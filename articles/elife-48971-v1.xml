<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">48971</article-id><article-id pub-id-type="doi">10.7554/eLife.48971</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural tracking of speech mental imagery during rhythmic inner counting</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-144754"><name><surname>Lu</surname><given-names>Lingxi</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-144755"><name><surname>Wang</surname><given-names>Qian</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-144756"><name><surname>Sheng</surname><given-names>Jingwei</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-144757"><name><surname>Liu</surname><given-names>Zhaowei</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-144758"><name><surname>Qin</surname><given-names>Lang</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-144759"><name><surname>Li</surname><given-names>Liang</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-143977"><name><surname>Gao</surname><given-names>Jia-Hong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9311-0297</contrib-id><email>jgao@pku.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund9"/><xref ref-type="other" rid="fund10"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">PKU-IDG/McGovern Institute for Brain Research</institution><institution>Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Center for MRI Research, Academy for Advanced Interdisciplinary Studies</institution><institution>Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Clinical Neuropsychology</institution><institution>Sanbo Brain Hospital, Capital Medical University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Linguistics</institution><institution>The University of Hong Kong</institution><addr-line><named-content content-type="city">Hong Kong</named-content></addr-line><country>China</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Speech and Hearing Research Center, School of Psychological and Cognitive Sciences</institution><institution>Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution content-type="dept">Beijing City Key Lab for Medical Physics and Engineering, Institution of Heavy Ion Physics, School of Physics</institution><institution>Peking University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewing Editor</role><aff><institution>Newcastle University</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>22</day><month>10</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e48971</elocation-id><history><date date-type="received" iso-8601-date="2019-06-01"><day>01</day><month>06</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-10-09"><day>09</day><month>10</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Lu et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Lu et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-48971-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.48971.001</object-id><p>The subjective inner experience of mental imagery is among the most ubiquitous human experiences in daily life. Elucidating the neural implementation underpinning the dynamic construction of mental imagery is critical to understanding high-order cognitive function in the human brain. Here, we applied a frequency-tagging method to isolate the top-down process of speech mental imagery from bottom-up sensory-driven activities and concurrently tracked the neural processing time scales corresponding to the two processes in human subjects. Notably, by estimating the source of the magnetoencephalography (MEG) signals, we identified isolated brain networks activated at the imagery-rate frequency. In contrast, more extensive brain regions in the auditory temporal cortex were activated at the stimulus-rate frequency. Furthermore, intracranial stereotactic electroencephalogram (sEEG) evidence confirmed the participation of the inferior frontal gyrus in generating speech mental imagery. Our results indicate that a disassociated neural network underlies the dynamic construction of speech mental imagery independent of auditory perception.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>speech mental imagery</kwd><kwd>MEG</kwd><kwd>sEEG</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>81790650</award-id><principal-award-recipient><name><surname>Gao</surname><given-names>Jia-Hong</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>81790651</award-id><principal-award-recipient><name><surname>Gao</surname><given-names>Jia-Hong</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>81727808</award-id><principal-award-recipient><name><surname>Gao</surname><given-names>Jia-Hong</given-names></name></principal-award-recipient></award-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>41830037</award-id><principal-award-recipient><name><surname>Gao</surname><given-names>Jia-Hong</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>National Key Research and Development Program of China</institution></institution-wrap></funding-source><award-id>2017YFC0108901</award-id><principal-award-recipient><name><surname>Gao</surname><given-names>Jia-Hong</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009592</institution-id><institution>Beijing Municipal Science and Technology Commission</institution></institution-wrap></funding-source><award-id>Z171100000117012</award-id><principal-award-recipient><name><surname>Gao</surname><given-names>Jia-Hong</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution>Shenzhen Peacock Plan</institution></institution-wrap></funding-source><award-id>KQTD2015033016104926</award-id><principal-award-recipient><name><surname>Gao</surname><given-names>Jia-Hong</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002858</institution-id><institution>China Postdoctoral Science Foundation</institution></institution-wrap></funding-source><award-id>2018M641046</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Lingxi</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31421003</award-id><principal-award-recipient><name><surname>Gao</surname><given-names>Jia-Hong</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A disassociated neural network underlies the dynamic construction of speech mental imagery independent of auditory perception.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>People can generate internal representations in the absence of external stimulation, for example, by speaking or singing in their minds. This experience is referred to as mental imagery, which reflects a higher cognitive function of the human brain (<xref ref-type="bibr" rid="bib23">Kosslyn et al., 2001</xref>). Speech mental imagery is among the most pervasive human experiences, and it has been estimated that people spend more than one-quarter of their lifetime engaged in such inner experiences (<xref ref-type="bibr" rid="bib13">Heavey and Hurlburt, 2008</xref>). Many functional magnetic resonance imaging (fMRI) studies have reported distributed neural networks involving the left pars opercularis in the inferior frontal cortex (Broca’s area) (<xref ref-type="bibr" rid="bib5">Aleman et al., 2005</xref>; <xref ref-type="bibr" rid="bib20">Kleber et al., 2007</xref>; <xref ref-type="bibr" rid="bib35">Papoutsi et al., 2009</xref>; <xref ref-type="bibr" rid="bib21">Koelsch et al., 2009</xref>; <xref ref-type="bibr" rid="bib38">Price, 2012</xref>; <xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>), the premotor cortex (<xref ref-type="bibr" rid="bib38">Price, 2012</xref>; <xref ref-type="bibr" rid="bib20">Kleber et al., 2007</xref>; <xref ref-type="bibr" rid="bib21">Koelsch et al., 2009</xref>; <xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>; <xref ref-type="bibr" rid="bib5">Aleman et al., 2005</xref>), the inferior parietal lobe (<xref ref-type="bibr" rid="bib20">Kleber et al., 2007</xref>; <xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>), the middle frontal cortex (<xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>), the anterior part of the insula (<xref ref-type="bibr" rid="bib21">Koelsch et al., 2009</xref>; <xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>; <xref ref-type="bibr" rid="bib5">Aleman et al., 2005</xref>; <xref ref-type="bibr" rid="bib2">Ackermann and Riecker, 2004</xref>), and the superior temporal gyrus (<xref ref-type="bibr" rid="bib5">Aleman et al., 2005</xref>) that can be activated by speech mental imagery. However, limited by the temporal resolution of the fMRI technique, the neural dynamics of speech mental imagery have been mostly overlooked.</p><p>Neuronal information processing is shaped by multiscale oscillatory activity (<xref ref-type="bibr" rid="bib7">Buzsáki and Draguhn, 2004</xref>), in which subjective experience in consciousness involves ongoing neural dynamics on the time scale of hundreds of milliseconds (<xref ref-type="bibr" rid="bib10">Dehaene and Changeux, 2011</xref>). Therefore, establishing a neurophysiological model of speech mental imagery is highly important for elucidating the spectral-temporal features of imagery-induced neural activity. However, in the experimental environment, tracking subjects’ internal subjective representations in milliseconds remains highly challenging due to the lack of external signals identifying imagery-related neural activity. Considering the limitations of the current methodologies, several neurophysiological studies have used the imagery-perception repetition paradigm to measure the sensory attenuation effect caused by speech mental imagery on subsequent auditory perception (<xref ref-type="bibr" rid="bib50">Tian and Poeppel, 2013</xref>; <xref ref-type="bibr" rid="bib51">Tian and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib53">Ylinen et al., 2015</xref>; <xref ref-type="bibr" rid="bib52">Whitford et al., 2017</xref>; <xref ref-type="bibr" rid="bib47">Tian et al., 2018</xref>). Preliminary magnetoencephalography (MEG) studies have shown that the modulatory effect of imagined speech on the amplitude of event-related responses to a subsequent auditory stimulus with matched pitch and/or content occurs as early as 100 to 200 ms (<xref ref-type="bibr" rid="bib50">Tian and Poeppel, 2013</xref>; <xref ref-type="bibr" rid="bib51">Tian and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib53">Ylinen et al., 2015</xref>), and in an electroencephalogram (EEG) study, N1 suppression was observed when a matched phoneme was generated covertly before an audible phoneme was presented (<xref ref-type="bibr" rid="bib52">Whitford et al., 2017</xref>). Recently, <xref ref-type="bibr" rid="bib47">Tian et al. (2018)</xref> examined the influence of imagined speech on the perceived loudness of a subsequent syllable and demonstrated the involvement of the sensory-driven cortex (i.e. the auditory cortex) in the early-stage interaction of speech imagery and perception. These neurophysiological findings implicate the early involvement of sensory-related neural substrates in the process of speech mental imagery, while in the adaptation paradigm, the neural pathway of top-down mental imagery cannot be disentangled from the bottom-up perception. To date, the neural dynamics of mental imagery independent of auditory perception remain puzzling.</p><p>Frequency-tagging is a promising approach for disentangling exogenous and endogenous processes when the corresponding neural activity changes periodically at different frequencies. Recently, frequency-tagging has been used to characterize higher-lever processes such as music rhythmic perception (<xref ref-type="bibr" rid="bib32">Nozaradan et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Tal et al., 2017</xref>; <xref ref-type="bibr" rid="bib26">Lenc et al., 2018</xref>; <xref ref-type="bibr" rid="bib33">Nozaradan et al., 2018</xref>) and speech linguistic construction (<xref ref-type="bibr" rid="bib12">Ding et al., 2016</xref>; <xref ref-type="bibr" rid="bib41">Sheng et al., 2019</xref>). For example, <xref ref-type="bibr" rid="bib32">Nozaradan et al. (2011)</xref> asked eight musicians to imagine a binary/ternary meter when listening to a constant beat and found spectral responses not only to the beat stimulation but also to the imagined meters of this beat. <xref ref-type="bibr" rid="bib12">Ding et al. (2016)</xref> reported cortical responses to internally constructed linguistic structures such as words, phrases and sentences at different frequencies during the comprehension of connected speech. Therefore, frequency-tagging is an excellent candidate as an approach for capturing the dynamic features during the internal construction of speech mental imagery by tagging the corresponding neural activity in the frequency domain.</p><p>Here, we apply the frequency-tagging method in a behavioral task involving speech mental imagery to experimentally isolate the neural manifestations of internally constructed speech from the stimulus-driven processes. In this way, we are able to directly track the neural processing time scales of speech mental imagery in the frequency domain (in our experiment, we manipulated the imagery-rate frequency to 0.8 Hz). Furthermore, we used the MEG source estimation method to localize the neural underpinnings of the isolated processes of speech imagery independent of the bottom-up auditory perception and confirmed the localization of the neural sources by intracranial stereotactic electroencephalogram (sEEG) recordings. We hypothesized that in addition to the sensory-driven auditory cortex, distinctive neural networks are responsible for generating speech mental imagery.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Cortical tracking of speech mental imagery</title><p>In the MEG study, we aimed to identify the cortical representations of speech mental imagery independent of stimulus-driven responses. We constructed a 4 Hz sequence of auditory stimuli that were mentally organized into 0.8 Hz groups by a speech mental imagery task (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Specifically, participants were asked to count in their minds following a sequence of 50 ms pure tones presented at 250 ms onset-to-onset intervals, and they were instructed to count from 1 to 10 in groups of 5 numbers (<bold>1</bold>, 2, 3, 4, 5, <bold>2</bold>, 2, 3, 4, 5, <bold>3</bold>, 2, 3, 4, 5…<bold>10</bold>, 2, 3, 4, 5) until the 50 pure tones terminated. As a result, when the subjects were performing the task, the spectral peaks were significant at the frequency of 0.8 Hz, corresponding to the grouping-rate rhythm of the mental imagery (<italic>t</italic><sub>19</sub> = 5.33, p=4.2 × 10<sup>−4</sup>, Cohen’s d = 1.19, one-tailed paired <italic>t</italic> test, false discovery rate [FDR] corrected), and at the frequency of 4 Hz, corresponding to the rate of the auditory stimuli (<italic>t</italic><sub>19</sub> = 18.45, corrected p=3.0 × 10<sup>−12</sup>, Cohen’s d = 4.13) (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). An additional harmonic peak at 1.6 Hz was found at the significance level of 0.05 (<italic>t</italic><sub>19</sub> = 3.18, corrected p=0.04, Cohen’s d = 0.96). No significant peaks were observed at the harmonic frequencies of 2.4 Hz (<italic>t</italic><sub>19</sub> = 2.46, corrected p=0.10, Cohen’s d = 0.55) and 3.2 Hz (<italic>t</italic><sub>19</sub> = 2.75, corrected p=0.07, marginally significant, Cohen’s d = 0.62). In contrast, under the control (baseline) condition, in which the participant received identical auditory stimuli without the imagery task, a significant peak was observed only at the stimulus-rate frequency of 4 Hz (<italic>t</italic><sub>19</sub> = 14.23, corrected p=3.1 × 10<sup>−10</sup>, Cohen’s d = 3.18). The topographic distribution of the grand averaged power at the imagery-rate frequency (0.8 Hz) and stimulus-rate frequency (4 Hz) displayed bilateral responses. Specifically, the two-tailed paired <italic>t</italic> test showed that the averaged power over the sensors in the right hemisphere was significantly stronger than that in the left hemisphere at 4 Hz under both the mental imagery condition (<italic>t</italic><sub>19</sub> = 3.78, p=0.001, Cohen’s d = 0.84) and the baseline condition (<italic>t</italic><sub>19</sub> = 3.21, p=0.004, Cohen’s d = 0.72) but not at the imagery-rate frequency of 0.8 Hz under the mental imagery condition (<italic>t</italic><sub>19</sub> = 0.34, p=0.72, Cohen’s d = 0.08).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.48971.002</object-id><label>Figure 1.</label><caption><title>Sensor-level responses in neural tracking of speech mental imagery.</title><p>(<bold>a</bold>) Under the mental imagery condition (left), a sequence of pure tones was presented with 250 ms onset-to-onset intervals. The participants were asked to loudly count in their minds by grouping every five numbers (with the imagery task), forming an imagery-rate frequency of 0.8 Hz and a stimulus-rate frequency of 4 Hz. Under the baseline condition (right), the participants passively listened to the same sounds without performing a task. (<bold>b</bold>) Significant spectral peaks were found at 0.8 Hz and its harmonics under the mental imagery condition, and significant 4 Hz peaks were found under both conditions (paired one-sided <italic>t</italic> test, FDR corrected). The topographic distribution was right-lateralized at 4 Hz but not at 0.8 Hz. The shaded area represents the standard error (SE). ***p&lt;0.001, *p&lt;0.05, <italic>m.s.</italic> represents marginally significant.</p><p><supplementary-material id="fig1sdata1"><object-id pub-id-type="doi">10.7554/eLife.48971.003</object-id><label>Figure 1—source data 1.</label><caption><title>MEG sensor-level responses data.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-48971-fig1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48971-fig1-v1.tif"/></fig><p>The comparison of the peak power between the two conditions confirmed the presence of imagery-induced cortical manifestations at the frequency of 0.8 Hz (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). The peak power at the frequencies of 0.8 Hz and 4 Hz was calculated by subtracting the average of two neighboring frequency bins from the power of the peak frequency bin. A two-tailed paired <italic>t</italic> test showed that the peak power at the imagery-rate frequency of 0.8 Hz when the participants induced rhythmic mental imagery was significantly larger than that under the baseline condition (<italic>t</italic><sub>19</sub> = 2.40, p=0.027, Cohen’s d = 0.54). No significant difference was observed between the two conditions at 4 Hz (<italic>t</italic><sub>19</sub> = −0.30, p=0.764, Cohen’s d = 0.07). Interestingly, the power of the neural responses at an imagery rate of 0.8 Hz was negatively correlated with that at a stimulus rate of 4 Hz when the participants induced the rhythmic mental imagery following the sound sequence (<italic>r</italic> = −0.68, p=0.001) but not when they passively listened to the auditory stimuli (<italic>r</italic> = −0.11, p=0.63) (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.48971.004</object-id><label>Figure 2.</label><caption><title>Power of spectral peaks at the imagery-rate frequency and stimulus-rate frequency.</title><p>(<bold>a</bold>) Mental imagery increased the peak power at the imagery-rate frequency of 0.8 Hz. No significant difference in peak power at 4 Hz was observed between the conditions. (<bold>b</bold>) A significant negative correlation was observed between the grand averaged power at 4 Hz and 0.8 Hz under the mental imagery condition but not under the baseline condition. **p&lt;0.01, *p&lt;0.05.</p><p><supplementary-material id="fig2sdata1"><object-id pub-id-type="doi">10.7554/eLife.48971.005</object-id><label>Figure 2—source data 1.</label><caption><title>Data for the power of spectral peaks.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-48971-fig2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48971-fig2-v1.tif"/></fig></sec><sec id="s2-2"><title>Brain regions activated by speech mental imagery</title><p>Using the frequency-tagging method, we identified the imagery-induced MEG manifestation in the frequency domain, but which neural networks generated such neural activity? Taking advantage of the previously developed minimum L1-norm source estimation methods (<xref ref-type="bibr" rid="bib14">Huang et al., 2006</xref>; <xref ref-type="bibr" rid="bib15">Huang et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Sheng et al., 2019</xref>), we addressed this question by evaluating the neural sources in the MEG data within the single-frequency bins at the target frequencies of 0.8 Hz and 4 Hz. As a result, we found the following significant clusters activated at the imagery-rate frequency of 0.8 Hz (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="table" rid="table1">Table 1</xref>): (1) the left central-frontal lobe, including the pars opercularis part of the inferior frontal gyrus (operIFG), the inferior part of the precentral gyrus (PrG) and the postcentral gyrus (PoG), (2) the left inferior occipital gyrus (IOG) and (3) the right inferior parietal lobe (IPL), including the supramarginal gyrus (SMG) and the inferior part of the PoG (cluster level p&lt;0.01, family-wise error [FWE] corrected with a voxel-level threshold of p&lt;0.001). Regarding the stimulus-rate frequency of 4 Hz, a more extensive brain network involving the bilateral Heschl’s gyrus (HG), right middle temporal gyrus (MTG), left SMG, bilateral superior temporal gyrus (STG), bilateral PrG, bilateral PoG and bilateral insula (INS) was significantly activated under both the mental imagery and the baseline conditions. An additional region in the right middle frontal gyrus (MFG) was also activated at 4 Hz under the mental imagery condition.</p><table-wrap id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.48971.006</object-id><label>Table 1.</label><caption><title>Major brain activity at the source level.</title></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Brain region</th><th colspan="3">Peak MNI coordinate</th><th rowspan="2"><italic>T</italic> value</th></tr><tr><th align="right"><italic>X</italic></th><th align="right"><italic>Y</italic></th><th align="right"><italic>Z</italic></th></tr></thead><tbody><tr><th colspan="5" valign="top"><bold>Imagery condition at 0.8 Hz</bold></th></tr><tr><td>R postcentral gyrus</td><td align="right">56</td><td align="right">−22</td><td align="right">29</td><td align="right">4.0</td></tr><tr><td>L postcentral gyrus</td><td align="right">−51</td><td align="right">-7</td><td align="right">23</td><td align="right">4.9</td></tr><tr><td>L precentral gyrus</td><td align="right">−50</td><td align="right">-6</td><td align="right">21</td><td align="right">4.7</td></tr><tr><td>L inferior frontal gyrus</td><td align="right">−46</td><td align="right">3</td><td align="right">28</td><td align="right">3.2</td></tr><tr><td>L inferior occipital gyrus</td><td align="right">−40</td><td align="right">−75</td><td align="right">-6</td><td align="right">5.3</td></tr><tr><td>R supramarginal gyrus</td><td align="right">60</td><td align="right">−31</td><td align="right">24</td><td align="right">6.0</td></tr><tr><th colspan="5" valign="top">Imagery condition at 4 <bold>Hz</bold></th></tr><tr><td>R superior temporal gyrus</td><td align="right">61</td><td align="right">−12</td><td align="right">8</td><td align="right">10.9</td></tr><tr><td>L superior temporal gyrus</td><td align="right">−51</td><td align="right">-5</td><td align="right">5</td><td align="right">5.7</td></tr><tr><td>R middle temporal gyrus</td><td align="right">67</td><td align="right">−19</td><td align="right">-3</td><td align="right">8.5</td></tr><tr><td>L Heschl’s gyrus</td><td align="right">−50</td><td align="right">-7</td><td align="right">5</td><td align="right">5.3</td></tr><tr><td>R Heschl’s gyrus</td><td align="right">58</td><td align="right">−10</td><td align="right">8</td><td align="right">10.3</td></tr><tr><td>R postcentral gyrus</td><td align="right">59</td><td align="right">−16</td><td align="right">17</td><td align="right">8.2</td></tr><tr><td>L postcentral gyrus</td><td align="right">−57</td><td align="right">−19</td><td align="right">17</td><td align="right">6.4</td></tr><tr><td>R precentral gyrus</td><td align="right">55</td><td align="right">0</td><td align="right">19</td><td align="right">7.3</td></tr><tr><td>L precentral gyrus</td><td align="right">−50</td><td align="right">-6</td><td align="right">21</td><td align="right">4.7</td></tr><tr><td>R insula</td><td align="right">46</td><td align="right">−10</td><td align="right">8</td><td align="right">6.9</td></tr><tr><td>L insula</td><td align="right">−46</td><td align="right">-9</td><td align="right">2</td><td align="right">3.8</td></tr><tr><td>R middle frontal gyrus</td><td align="right">6</td><td align="right">50</td><td align="right">−11</td><td align="right">4.0</td></tr><tr><td>L supramarginal gyrus</td><td align="right">−58</td><td align="right">−21</td><td align="right">17</td><td align="right">6.2</td></tr><tr><th colspan="5" valign="top">Baseline condition at 4 <bold>Hz</bold></th></tr><tr><td>R superior temporal gyrus</td><td align="right">56</td><td align="right">−16</td><td align="right">1</td><td align="right">11.0</td></tr><tr><td>L superior temporal gyrus</td><td align="right">−50</td><td align="right">−18</td><td align="right">12</td><td align="right">7.3</td></tr><tr><td>R middle temporal gyrus</td><td align="right">55</td><td align="right">−27</td><td align="right">0</td><td align="right">7.3</td></tr><tr><td>L Heschl’s gyrus</td><td align="right">−42</td><td align="right">−22</td><td align="right">12</td><td align="right">7.0</td></tr><tr><td>R Heschl’s gyrus</td><td align="right">54</td><td align="right">−13</td><td align="right">8</td><td align="right">10.8</td></tr><tr><td>R postcentral gyrus</td><td align="right">61</td><td align="right">−15</td><td align="right">15</td><td align="right">8.8</td></tr><tr><td>L postcentral gyrus</td><td align="right">−53</td><td align="right">−11</td><td align="right">18</td><td align="right">8.7</td></tr><tr><td>R precentral gyrus</td><td align="right">45</td><td align="right">-8</td><td align="right">29</td><td align="right">6.3</td></tr><tr><td>L precentral gyrus</td><td align="right">−54</td><td align="right">-4</td><td align="right">21</td><td align="right">5.9</td></tr><tr><td>R insula</td><td align="right">48</td><td align="right">−10</td><td align="right">4</td><td align="right">6.9</td></tr><tr><td>L insula</td><td align="right">−38</td><td align="right">−24</td><td align="right">22</td><td align="right">6.8</td></tr><tr><td>L supramarginal gyrus</td><td align="right">−43</td><td align="right">−27</td><td align="right">23</td><td align="right">8.3</td></tr></tbody></table></table-wrap><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.48971.007</object-id><label>Figure 3.</label><caption><title>Source-level brain activation at the imagery-rate frequency and stimulus-rate frequency.</title><p>Under the mental imagery condition (upper panel), the left central-frontal lobe, the left inferior occipital gyrus and the right inferior parietal lobe were activated at an imagery-rate frequency of 0.8 Hz, and a distributive brain network extending from the auditory cortex to the frontal and parietal regions was activated at a stimulus-rate frequency of 4 Hz (cluster level p&lt;0.01, FWE corrected with a voxel-level threshold of p&lt;0.001). Under the baseline condition (lower panel), stimulus-induced brain activation centring in the auditory temporal cortex was observed at 4 Hz.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48971-fig3-v1.tif"/></fig><p>Then, we compared the activation of regions of interest (ROIs) in different brain regions at the following three significant spectral peaks: (1) at 0.8 Hz under the mental imagery condition, (2) at 4 Hz under the mental imagery condition and (3) at 4 Hz under the baseline condition (<xref ref-type="fig" rid="fig4">Figure 4</xref>). For each brain region, the ROI centered at the peak voxel with the largest <italic>t</italic> value among the three levels was identified, and the amplitude of the activation was extracted from all 20 subjects. Two-way repeated measures ANOVAs (condition × brain region) for ROI activation showed a significant main effect of condition (F<sub>2, 38</sub> = 23.56, p&lt;0.001, <italic>ηp</italic><sup>2</sup> = 0.55), a significant main effect of brain region (F<sub>5.23, 99.41</sub> = 6.66, p&lt;0.001, <italic>ηp</italic><sup>2</sup> = 0.26), and a significant interaction between the two variables (F<sub>8.09, 153.67</sub> = 5.60, p&lt;0.001, <italic>ηp</italic><sup>2</sup> = 0.23). To untangle the interaction, we performed post hoc comparisons with Bonferroni correction for each brain region. The results showed that in the left hemisphere, the neural responses in the left IOG at 0.8 Hz under the mental imagery condition were significantly stronger than those at 4 Hz under the mental imagery condition and at 4 Hz under the baseline condition (both p&lt;0.05). In addition, the neural activation at 4 Hz under the imagery and baseline conditions was stronger than that at 0.8 Hz under the imagery condition in the left HG (p=0.058 and p&lt;0.001, respectively), the left INS (p=0.012 and p&lt;0.001, respectively) and the left SMG (p=0.032 and p&lt;0.001, respectively). In the left STG, the activation amplitude at 4 Hz under the baseline condition was larger than that at 0.8 Hz under the imagery condition (p=0.006). In the right hemisphere, the activation observed at 4 Hz under the imagery and baseline conditions was stronger than that observed at 0.8 Hz under the imagery condition in most regions (right HG, both p&lt;0.001; right STG, both p&lt;0.001; right INS, both p=0.003; right PrG, p=0.029 and 0.016, respectively; right PoG, p=0.025 and 0.010, respectively; right MTG, p=0.010 and 0.005, respectively; all with Bonferroni correction), reflecting robust auditory neural representations of the 4 Hz sequence of sound stimuli. No significant difference was observed in the brain activation levels at 4 Hz between the imagery condition and baseline condition, except in the right MFG, in which the activation at 4 Hz during mental imagery was stronger than that at baseline (p=0.014). These data indicate that the isolated neural networks underlying speech mental imagery involve the left central-frontal lobe, the left inferior occipital lobe and the right inferior parietal lobule, and that these networks are disassociated from the sensory-driven processes.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.48971.008</object-id><label>Figure 4.</label><caption><title>Comparisons of ROI activation among three significant spectral peaks.</title><p>The brain activation in the left IOG at 0.8 Hz under the mental imagery condition was stronger than that at 4 Hz. The brain activation in the right MFG at 4 Hz under the mental imagery condition was stronger than that under the baseline condition. In the left HG, left STG, left INS, left SMG, right HG, right STG, right INS, right PrG, right PoG and right MTG, the activation at 4 Hz under the mental imagery condition and/or the baseline condition was stronger than that at 0.8 Hz under the mental imagery condition. Multiple comparisons were Bonferroni corrected. ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05, <italic>m.s.</italic> represents marginally significant.</p><p><supplementary-material id="fig4sdata1"><object-id pub-id-type="doi">10.7554/eLife.48971.009</object-id><label>Figure 4—source data 1.</label><caption><title>ROI activation data.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-48971-fig4-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48971-fig4-v1.tif"/></fig></sec><sec id="s2-3"><title>Localization of neural tracking speech imagery using sEEG</title><p>To further confirm the localization of the neural sources in the MEG data, we recorded sEEG responses to the same auditory stimuli with and without the imagery task. The sEEG responses are neurophysiological signals with a high temporal and spatial resolution recorded from patients with intracranial electrode implantation for the clinical assessment of epilepsy. We analyzed the power of sEEG data in the high gamma band (60–100 Hz), which is highly correlated with the neural firing rate (<xref ref-type="bibr" rid="bib31">Mukamel et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Ray and Maunsell, 2011</xref>). We found responsive contacts in five subjects with significant 4 Hz spectral peaks in the left posterior INS, left anterior SMG, bilateral middle STG, left HG and right middle MTG under the mental imagery and/or baseline conditions. Most importantly, we found significant spectral responses at the imagery-rate frequency of 0.8 Hz when the participant was asked to perform a mental imagery task following the 4 Hz sound sequence (<italic>t</italic><sub>19</sub> = 3.61, p=0.021, Cohen’s d = 0.81, paired one-sided <italic>t</italic> test, FDR corrected) (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). The imagery-responsive contact was localized in the operIFG, which is consistent with the MEG source results in the Montreal Neurological Institute (MNI) space (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). Unlike in the baseline condition, under the mental imagery condition, the averaged time course of the high gamma activity in the operIFG periodically changed every 1.25 s (0.8 Hz), with an increasing amplitude near the last number of each mentally constructed group (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). Due to limited intracranial cases, we did not obtain sEEG data from the regions of the left IOG and right IPL, which were the other two neural clusters in the MEG source results.</p><fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.48971.010</object-id><label>Figure 5.</label><caption><title>SEEG responses to speech mental imagery.</title><p>(<bold>a</bold>) Subject one with contacts in the left operIFG, middle STG, posterior INS and anterior SMG. Significant spectral responses at 0.8 Hz were detected in the operIFG, and the remaining contacts were responsive to the 4 Hz frequency. (<bold>b</bold>) The cluster including the operIFG from MEG sources is presented in the upper panel. The electrode with contact in the operIFG in Subject one was projected in the MNI space and is shown in the lower panel. SEEG localization supports the MEG source estimation results. (<bold>c</bold>) Averaged time course of high gamma activity in the operIFG in Subject 1. Periodical changes presented every 1.25 s (0.8 Hz) with increasing amplitude near the onset of the last number (black arrow) in each mentally constructed group under the mental imagery condition (red line) but not the baseline condition (blue line). (<bold>d</bold>) Subject two with contacts in the left HG. Significant spectral peaks were observed at 4 Hz in the HG. (<bold>e</bold>) Subject three with contacts in the right middle STG. Significant spectral peaks were observed at 4 Hz in the middle STG. (<bold>f</bold>) Contact locations in the MNI space. Different colored marks represent each subject’s responsive contacts with significant spectral peaks at 0.8 Hz and/or 4 Hz. Gray marks represent non-responsive contacts with no significant spectral peaks. (<bold>g</bold>) In each brain region with contacts responsive at 4 Hz, the normalized peaks were significantly larger than zero under the baseline condition (blue stars). Among these peaks, the normalized peaks were significant in the left posterior INS, left anterior SMG, left HG and right middle STG (red stars). The peaks in the bilateral middle STG were smaller under the mental imagery condition than the baseline condition (gray stars). ***p&lt;0.001, **p&lt;0.01, *p&lt;0.05.</p><p><supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.48971.012</object-id><label>Figure 5—source data 1.</label><caption><title>sEEG response data.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-48971-fig5-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48971-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48971.011</object-id><label>Figure 5—figure supplement 1.</label><caption><title>SEEG results of subjects 4–5.</title><p>(<bold>a</bold>) Subject four with a contact in the left HG. A significant spectral peak was observed at 4 Hz in the imagery condition. (<bold>b</bold>) Subject five with contacts in the left anterior SMG and right middle MTG. Significant spectral peaks were observed at 4 Hz in the anterior SMG under both conditions and in the middle MTG under the baseline condition.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48971-fig5-figsupp1-v1.tif"/></fig></fig-group><p>In addition, we found significant spectral responses at the stimulus-rate frequency of 4 Hz (<xref ref-type="fig" rid="fig5">Figures 5a, d and e</xref> for subjects 1, 2, and 3; <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for subjects 4 and 5; <xref ref-type="table" rid="table2">Table 2</xref> for statistical results). In the left hemisphere, significant 4 Hz peaks were observed at the contacts localized in the posterior INS, anterior SMG and HG under the mental imagery condition and at the contacts localized in the middle STG, posterior INS, anterior SMG and HG under the baseline condition. In the right hemisphere, we found significant spectral responses at 4 Hz in the middle STG and middle MTG. Specifically, the right middle STG was responsive to the 4 Hz stimuli under the baseline condition and the mental imagery condition. The right middle MTG was responsive to the 4 Hz stimuli only under the baseline condition. These brain regions with significant spectral peaks at 4 Hz mainly agreed with the estimated neural sources based on the MEG data (<xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table2" position="float"><object-id pub-id-type="doi">10.7554/eLife.48971.013</object-id><label>Table 2.</label><caption><title>Statistical details for the sEEG results</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Contact localization</th><th>Subject no.</th><th>Df</th><th><italic>T</italic></th><th>Corrected <italic>P</italic></th><th>Cohen’s d</th></tr></thead><tbody><tr><th valign="top">Mental imagery condition at 4 <bold>Hz</bold></th><th/><th/><th/><th/><th/></tr><tr><td>L posterior insula</td><td>1</td><td>19</td><td>5.57</td><td>&lt;0.001</td><td>1.24</td></tr><tr><td>L anterior supramarginal gyrus</td><td>1</td><td>19</td><td>3.40</td><td>0.034</td><td>0.76</td></tr><tr><td/><td>1</td><td>19</td><td>2.99</td><td>0.084</td><td>0.67</td></tr><tr><td/><td>5</td><td>18</td><td>3.29</td><td>0.046</td><td>0.75</td></tr><tr><td>L Heschl’s gyrus</td><td>2</td><td>15</td><td>5.00</td><td>0.002</td><td>1.25</td></tr><tr><td/><td>2</td><td>15</td><td>4.64</td><td>0.004</td><td>1.16</td></tr><tr><td/><td>4</td><td>19</td><td>4.57</td><td>0.002</td><td>1.02</td></tr><tr><td>R middle superior temporal gyrus</td><td>3</td><td>19</td><td>4.08</td><td>0.007</td><td>0.91</td></tr><tr><td/><td>3</td><td>19</td><td>5.29</td><td>&lt;0.001</td><td>1.18</td></tr><tr><th valign="top">Baseline condition at 4 <bold>Hz</bold></th><th/><th/><th/><th/><th/></tr><tr><td>L middle superior temporal gyrus</td><td>1</td><td>19</td><td>4.47</td><td>0.003</td><td>1.00</td></tr><tr><td>L posterior insula</td><td>1</td><td>19</td><td>8.06</td><td>&lt;0.001</td><td>1.80</td></tr><tr><td>L anterior supramarginal gyrus</td><td>1</td><td>19</td><td>3.35</td><td>0.037</td><td>0.75</td></tr><tr><td/><td>5</td><td>18</td><td>3.43</td><td>0.034</td><td>0.79</td></tr><tr><td>L Heschl’s gyrus</td><td>2</td><td>19</td><td>4.41</td><td>0.003</td><td>0.99</td></tr><tr><td/><td>2</td><td>19</td><td>4.58</td><td>0.002</td><td>1.02</td></tr><tr><td/><td>2</td><td>19</td><td>4.60</td><td>0.002</td><td>1.03</td></tr><tr><td/><td>2</td><td>19</td><td>3.44</td><td>0.031</td><td>0.77</td></tr><tr><td>R middle superior temporal gyrus</td><td>3</td><td>19</td><td>5.29</td><td>&lt;0.001</td><td>1.18</td></tr><tr><td/><td>3</td><td>19</td><td>4.35</td><td>0.004</td><td>0.97</td></tr><tr><td/><td>3</td><td>19</td><td>3.81</td><td>0.013</td><td>0.85</td></tr><tr><td/><td>3</td><td>19</td><td>3.97</td><td>0.009</td><td>0.89</td></tr><tr><td/><td>3</td><td>19</td><td>3.79</td><td>0.014</td><td>0.85</td></tr><tr><td>R middle middle temporal gyrus</td><td>5</td><td>18</td><td>4.52</td><td>0.003</td><td>1.34</td></tr></tbody></table></table-wrap><p>Subsequently, we combined all the trials at the contacts localized in the same brain regions across different subjects. <xref ref-type="fig" rid="fig5">Figure 5f</xref> displays the localization of contacts in the MNI space from all five subjects. We calculated the normalized peak power in each brain region and examined the following two questions: (1) whether the 4 Hz peak responses were significant and (2) whether there were significant differences between the conditions (<xref ref-type="fig" rid="fig5">Figure 5g</xref>). The one-tailed paired <italic>t</italic> tests revealed significant 4 Hz peak powers under the baseline condition in several brain areas, including the left posterior INS (<italic>t</italic><sub>19</sub> = 24.82, p&lt;0.001, Cohen’s d = 3.05), left anterior SMG (<italic>t</italic><sub>58</sub> = 2.32, p=0.012, Cohen’s d = 0.30), left HG (<italic>t</italic><sub>99</sub> = 10.10, p&lt;0.001, Cohen’s d = 1.07), bilateral middle STG (left, <italic>t</italic><sub>19</sub> = 8.60, p&lt;0.001, Cohen’s d = 1.92; right, <italic>t</italic><sub>99</sub> = 9.28, p&lt;0.001, Cohen’s d = 0.93) and right middle MTG (<italic>t</italic><sub>18</sub> = 4.59, p&lt;0.001, Cohen’s d = 1.05). Among these regions, the 4 Hz peaks were also significantly larger than zero under the mental imagery condition in the brain areas of the left posterior INS (<italic>t</italic><sub>19</sub> = 6.71, p&lt;0.001, Cohen’s d = 1.50), left anterior SMG (<italic>t</italic><sub>58</sub> = 4.73, p&lt;0.001, Cohen’s d = 0.62), left HG (<italic>t</italic><sub>83</sub> = 8.97, p&lt;0.001, Cohen’s d = 0.98) and right middle STG (<italic>t</italic><sub>99</sub> = 2.69, p=0.004, Cohen’s d = 0.27).</p><p>Moreover, the comparisons between the conditions showed that the power at 4 Hz peaks in the bilateral middle STG under the mental imagery condition was reduced compared to that under the baseline condition (left middle STG, <italic>t</italic><sub>38</sub> = −2.93, p=0.006, Cohen’s d = 0.93; right middle STG, <italic>t</italic><sub>198</sub> = −4.05, p&lt;0.001, Cohen’s d = 0.57, two-tailed independent <italic>t</italic> test). Notably, the power at the 0.8 Hz peak in the operIFG during the imagery task was increased compared to that under the baseline condition (<italic>t</italic><sub>38</sub> = 3.52, p=0.001, Cohen’s d = 1.11). This result further supports the MEG results showing the reverse correlation between the power of the top-down neural activity induced by mental imagery and the power of the bottom-up stimulus-driven processes (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our data show that the neural activity induced by speech mental imagery can be tracked in the frequency domain and distinguished from stimulus-driven processing. The frequency-tagging method we applied in the imagery task casts light on the neural dynamics of the internally generated representations (i.e. speech mental imagery). More specifically, the left central-frontal lobe, the left inferior occipital lobe and the right inferior parietal lobe contribute to the spectral responses generated by speech mental imagery. Furthermore, the detection of imagery-induced neural signals in the left central-frontal lobe, particularly the region of the operIFG, in rare intracranial cases confirms the reliability of the minimum L1-norm estimation methods in solving the inverse problem for MEG data in our study.</p><p>Mental imagery has been proposed to be essentially an attention-guided memory retrieval process, for example, in the visual domain (<xref ref-type="bibr" rid="bib19">Ishai et al., 2002</xref>). Mental imagery of speech not only recruits the pathway by which phonological, lexical, or semantic details are retrieved from the memory system but also requires planned articulation that is simulated internally to enable the individuals to have the subjective experiences of their mental imagery (<xref ref-type="bibr" rid="bib49">Tian and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>). In our study, the task of rhythmic inner counting produced cyclical neural signals that were captured in the frequency domain, reflecting the top-down regulation of the internally constructed mental imagery. The operational definition of speech mental imagery condition, namely, the rhythmic inner counting task, is not only related to the internal representations of speech but also requires high levels of working memory. To count following a rapid sequence of sounds presented at a 250 ms onset-to-onset interval, the brain must maintain the mental representations of the last group of 5 numbers (or at least of the beginning number) and construct the ongoing imagery of the subsequent group of 5 numbers concurrently with the sound stimuli. Additionally, the participants needed to verbally name the numbers and count ‘loudly’ in their minds. It is worth mentioning that the rhythmic inner counting task in our study eliminated the influence of bottom-up sensory-driven processes by using the same pure tones as the stimuli and investigated the pure top-down processes of speech mental imagery independent of bottom-up perception. This is different from the typical silent counting task used in evoking P300 responses, which largely depends on the sensory perception and anticipation of the bottom-up physical features of the stimulus. The 0.8 Hz rhythmic mental operation activated the region of the left IOG, which is reported to be associated with verbal representation (<xref ref-type="bibr" rid="bib40">Saito et al., 2012</xref>) and memory maintenance in numeral processing (<xref ref-type="bibr" rid="bib9">Daitch et al., 2016</xref>), and the region of the right IPL, which is associated with auditory working memory (<xref ref-type="bibr" rid="bib37">Paulesu et al., 1993</xref>) and has also been proposed to be activated in imagined singing and imagined movement (<xref ref-type="bibr" rid="bib20">Kleber et al., 2007</xref>; <xref ref-type="bibr" rid="bib25">Lebon et al., 2018</xref>). In addition, our study shows that the ventral PrG and PoG are also activated during the construction of speech imagery, which is consistent with previous findings that stronger activation of the sensorimotor cortex induced by articulatory imagery than hearing imagery (<xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>) and the direct neural signal from the sensorimotor cortex contributed to the decoding of imagined speech and silent reading (<xref ref-type="bibr" rid="bib28">Martin et al., 2014</xref>; <xref ref-type="bibr" rid="bib29">Martin et al., 2016</xref>). Nevertheless, a firm conclusion on the contribution of the sensorimotor cortex to the construction of speech mental imagery will only be possible after controlling for any articulator movement in future studies. Furthermore, consistent with previous findings in fMRI studies suggesting that Broca’s area (operIFG) is activated during the silent articulation of speech (<xref ref-type="bibr" rid="bib5">Aleman et al., 2005</xref>; <xref ref-type="bibr" rid="bib20">Kleber et al., 2007</xref>; <xref ref-type="bibr" rid="bib35">Papoutsi et al., 2009</xref>; <xref ref-type="bibr" rid="bib21">Koelsch et al., 2009</xref>; <xref ref-type="bibr" rid="bib38">Price, 2012</xref>; <xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>), the imagery-enhanced spectral responses in the region of the operIFG are not only identified in the MEG neural sources but also detected in the intracranial focal recordings, confirming the involvement of the higher order frontal cortex in the dynamic construction of speech mental imagery. Taken together, these results show for the first time that the disassociated neural networks underlying the internal organization of speech mental imagery are independent of auditory perception. Because we identified only the operIFG as a neural source in the speech mental imagery due to the limited intracranial cases in the present sEEG experiment, more work is still needed to investigate the role of the left IOG and right IPL in the neural pathways underlying speech mental imagery.</p><p>Mixed evidence exists regarding the relationship between imagery and perception. One line of evidence suggests that mental imagery and sensory perception share common processes; for instance, the visual cortex can also be activated by visual imagery (<xref ref-type="bibr" rid="bib22">Kosslyn et al., 1999</xref>; <xref ref-type="bibr" rid="bib43">Stokes et al., 2009</xref>; <xref ref-type="bibr" rid="bib11">Dijkstra et al., 2017</xref>), and the auditory cortex is recruited during the imagination of music (<xref ref-type="bibr" rid="bib54">Zatorre and Halpern, 2005</xref>; <xref ref-type="bibr" rid="bib24">Kraemer et al., 2005</xref>). However, other studies have suggested that differentiated neural substrates underlie these two processes; for example, brain-damaged patients with impaired object recognition have normal visual imagery (<xref ref-type="bibr" rid="bib6">Behrmann et al., 1992</xref>), and the primary auditory cortex is activated by overt singing but not imagined singing (<xref ref-type="bibr" rid="bib20">Kleber et al., 2007</xref>). <xref ref-type="bibr" rid="bib47">Tian et al. (2018)</xref> shed light on this issue by investigating the imagery-perception adaptation effect in speech mental imagery and found that imagined speech modulates the perceived loudness of a subsequent speech stimulus, demonstrating the overlapping neural underpinnings of the auditory cortex underlying the two processes of imagery and perception. Limited by the imagery-perception repetition paradigm, Tian et al. did not illustrate the role of non-overlapping brain regions that might be involved in the modulatory effect of imagery on perceived loudness. In our study, we address this gap by disassociating the two processes of imagery and perception, which were precisely frequency-tagged, and identifying the specific neural sources of speech mental imagery independent of auditory perception. We show that the auditory cortex was not engaged in neural activation at an imagery-rate frequency of 0.8 Hz. Instead, some isolated neural clusters, including the higher order frontal cortex, were recruited at the imagery-rate frequency, reflecting the non-overlapping neural populations underlying the two distinctive processes of speech imagery and auditory perception. Notably, during the imagery task, the spectral power at the stimulus-rate frequency was weakened at the local contacts of the middle STG according to the sEEG recordings, indicating a potential imagery-driven suppression of auditory perception that fits well with the results of previous adaptation studies (<xref ref-type="bibr" rid="bib50">Tian and Poeppel, 2013</xref>; <xref ref-type="bibr" rid="bib51">Tian and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib53">Ylinen et al., 2015</xref>; <xref ref-type="bibr" rid="bib52">Whitford et al., 2017</xref>; <xref ref-type="bibr" rid="bib47">Tian et al., 2018</xref>). In addition, we uncovered a reverse correlation between the grand averaged power at the imagery-rate frequency and the stimulus-rate frequency (<xref ref-type="fig" rid="fig2">Figure 2b</xref>), which might have resulted from the reallocation of limited cognitive resources between the low-level auditory representation and high-level imagery construction. The crucial role of the (left-lateralized) frontal cortex in the top-down control mechanism has been well demonstrated in the speech perception literature with the account of predictive coding (<xref ref-type="bibr" rid="bib36">Park et al., 2015</xref>; <xref ref-type="bibr" rid="bib8">Cope et al., 2017</xref>). Interestingly, <xref ref-type="bibr" rid="bib42">Sohoglu et al. (2012)</xref> also found opposite effects of top-down expectations and bottom-up sensory details on the evoked response in the STG. Based on these findings, we propose that the higher order cortex (i.e. the inferior frontal cortex) is engaged in the dynamic organization of speech mental imagery, which might modulate neural activity in the lower-order auditory cortex. Determining the functional connectivity between the higher-order cortex and auditory cortex during the construction of speech mental imagery may be an important direction in future studies.</p><p>In our study, the same auditory stimuli (a 4 Hz sequence of pure tones) were played under both the mental imagery condition and baseline condition. As a result, strong spectral peaks with similar power amplitudes (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) and right-lateralized topographic distributions (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) at the sensor level were observed at a frequency of 4 Hz, reflecting robust stimulus-driven auditory responses. Therefore, we mainly refer to 4 Hz as the ‘stimulus-rate’ frequency, but we should be aware that the mental imagery of each number during the counting task was first produced at 4 Hz and then internally organized into groups at 0.8 Hz. When comparing the 4 Hz responses between conditions at the source level, we found greater activation of the right MFG in the mental imagery condition (<xref ref-type="fig" rid="fig4">Figure 4</xref>), which might reflect the additive effect of generating the speech mental imagery of each number during the counting task. Previous fMRI findings have suggested that the MFG is activated by the auditory imagery of music (<xref ref-type="bibr" rid="bib55">Zvyagintsev et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Lima et al., 2015</xref>) and can be activated by hearing imagery (HI) of speech compared to articulatory imagery (AI) (<xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>), but the results of our MEG experiment showed that the MFG also contributes to the processing of articulation imagery. From the perspective of inner speech theories (<xref ref-type="bibr" rid="bib4">Alderson-Day and Fernyhough, 2015</xref>), HI, which refers to the inner representation of another’s voice, is distinct from articulatory imagery, which refers to covert speech production, including motor preparation and verbal working memory maintenance (<xref ref-type="bibr" rid="bib1">Acheson et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Tian and Poeppel, 2013</xref>; <xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>). To ensure that the imagery task appeared to be natural and reflected subjects’ daily experience, we set the imagery task in our study as the mental operation of counting silently, which can be treated as a type of articulatory imagery, without asking the participants to strictly distinguish between HI and articulatory imagery. Therefore, care should be taken when comparing our results with other neuroimaging findings in the research area of HI of speech or auditory imagery of music. In the future, whether the neural mechanism of speech mental imagery uncovered in our paradigm can be extended to a richer account of inner speech must be considered.</p><p>Note that the auditory cortex was not significantly activated at the imagery-rate frequency. This result is different from the previous finding that similar neural responses were observed for overt and covert hearing tasks (<xref ref-type="bibr" rid="bib48">Tian and Poeppel, 2010</xref>), which can be explained from two aspects. Firstly, the neural pathways underlying speech mental imagery are strongly modulated by the specific imagery requirement (i.e. articulatory imagery or HI) and task demand (<xref ref-type="bibr" rid="bib48">Tian and Poeppel, 2010</xref>; <xref ref-type="bibr" rid="bib49">Tian and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib50">Tian and Poeppel, 2013</xref>; <xref ref-type="bibr" rid="bib46">Tian et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Tian et al., 2018</xref>). Unlike the detailed requirements for imagery used in <xref ref-type="bibr" rid="bib48">Tian and Poeppel (2010)</xref>, in our study, we instructed the participants to count in their minds without asking them to strictly distinguish between articulatory imagery and HI. Secondly, and most importantly, we for the first time applied frequency-tagging paradigm and disassociated the neural activity induced by rhythmic inner counting from that elicited by bottom-up auditory perception. The activation of auditory cortex was tagged and detected at the stimulus-rate frequency instead of imagery-rate frequency. These differences in the task requirements and experimental paradigm might have led to the different findings concerning the involvement of the auditory cortex (<xref ref-type="bibr" rid="bib48">Tian and Poeppel, 2010</xref>) in forming speech mental imagery.</p><p>Due to the absence of overt behavioral outcomes in mental imagery tasks, it is difficult or impossible to directly correlate the neural signals to the corresponding behavioral performance during the imagery task. To overcome this obstacle, some behavioral measurements have been developed in previous imagery studies, such as participant self-report assessments and judgement tasks regarding the content and nature of inner speech (<xref ref-type="bibr" rid="bib17">Hurlburt et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Hurlburt and Heavey, 2015</xref>; <xref ref-type="bibr" rid="bib27">Lima et al., 2015</xref>), but these approaches still largely rely on subjective experience without objective verification. Our study did not obtain behavioral data from the participants during the task but controlled for their behavioral performance via a preceding training session in which the participants were asked to report their performance of inner number generation and ensured that after training, they were able to perform the imagery task correctly. In addition, in the sEEG experiment, we recorded the participants’ subjective reports to exclude the incorrect trials from the analysis. A key issue that should be further addressed is advancing the paradigm to study speech imagery using behavioral measurements. Recently, an enlightening study conducted by <xref ref-type="bibr" rid="bib30">Martin et al. (2018)</xref> proposed a novel task that recorded both the output of a keyboard (without producing real sounds) and the neural activity when a musician imagined the music he or she was playing. Thus, the objective data from the recorded sound of the keyboard were simultaneously obtained and could be analyzed with the neural representations of the music imagery. In addition, intracranial neural signals from the auditory cortex and sensorimotor cortex can be used to reconstruct perceived and imagined speech (<xref ref-type="bibr" rid="bib28">Martin et al., 2014</xref>; <xref ref-type="bibr" rid="bib3">Akbari et al., 2019</xref>), providing a new account for decoding the neural signals induced by speech imagery. Inspired by these studies, more work needs to be performed to examine the correlations between possible behavioral manifestations and the neural substrates underlying mental imagery to deepen the understanding of speech mental imagery and advance its application in brain-machine interfaces.</p><p>In summary, our study applied a frequency-tagging paradigm to track the neural processing time scales in line with the generation of speech mental imagery and the stimulus-driven activity using both the neuronal measures (MEG) and direct recordings from the cortex (sEEG). We uncovered a distinctive neural network underlying the dynamic construction of speech mental imagery that was disentangled from sensory-driven auditory perception.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Twenty Mandarin-speaking young students (8 females and 12 males, 24.6 ± 3.6 years old, all right-handed) participated in the MEG experiment. The participants all had normal hearing abilities and reported having no history of mental disorders. Written informed consent was obtained from all participants. The participants were paid, and the experimental protocol was approved by the Peking University Institutional Review Board.</p></sec><sec id="s4-2"><title>Stimuli</title><p>The auditory signal was a 50 ms pure tone generated by Adobe Audition software (Adobe Systems Incorporated, San Jose, CA). The frequency of the pure tone was 440 Hz, and the sampling rate was 1.6 kHz.</p></sec><sec id="s4-3"><title>Procedures</title><p>During the experiment, each participant was seated in a dimly lit and magnetically shielded room (VACUUMSCHMELZE GmbH and Co. KG, Hanau, Germany) with a distance of 1 m from a projected screen. The acoustic signals were delivered to the participants using a MEG-compatible inserted ear-phone. Two blocks (a mental imagery block and a baseline block) were performed, and each block included 15 trials. The presentation order of the blocks was counterbalanced across subjects. Within each trial, the instruction was first presented on the screen. For the imagery block, the instructions were “Please count loudly in mind with five numbers in a group, <bold>1</bold>, 2, 3, 4, 5, <bold>2</bold>, 2, 3, 4, 5, <bold>3</bold>, 2, 3, 4, 5…<bold>10</bold>, 2, 3, 4, 5, following the rhythm of the sound until the sound is terminated’. In this counting task, the first number in each group increased from ‘1’ to ‘10’, and the remaining numbers in the group were always ‘2, 3, 4, 5’. For the baseline block, the instructions were ‘Please listen carefully to the sound until the sound is terminated’. The participants pressed the button when they were ready, and then a central fixation cross remained on the screen until the trial terminated. After a silent random interval ranging from 1 to 1.5 s, a pure tone was presented 50 times repeatedly with an onset-to-onset interval of 250 ms. Thus, the frequency of the presentation of the pure tones was 1/0.25 Hz, that is 4 Hz. More importantly, the frequency of the occurrence of the rhythmic imagery was 1/1.25 Hz, that is 0.8 Hz. The sound sequence in each trial lasted for 12.5 s.</p><p>Before the formal experiment, the subjects received a training session in which they were asked to orally report their inner performance after each trial. The training session was supervised to ensure that each subject could perform the task correctly. After the training, all the participants were able to generate the mental imagery of 50 numbers with every five numbers in a group following the 50 pure tones.</p></sec><sec id="s4-4"><title>MEG recording and structural MRI data acquisition</title><p>The continuous neuromagnetic signal was recorded using a 306-channel, whole-head MEG system (Elekta-Neuromag TRIUX, Helsinki, Finland) at Peking University. Before each block started, the head position of each subject was determined by four head-position indicator (HPI) coils. The electrooculogram (EOG) signal was simultaneously captured by two electrodes placed near the eyes as follows: one electrode was placed above the right eye, and one electrode was placed below the left eye. The continuous MEG data were on-line bandpass filtered at 0.1–330 Hz. The sampling rate was 1000 Hz.</p><p>The subjects’ structural MRI images were obtained with a 3T GE Discovery MR750 MRI scanner (GE Healthcare, Milwaukee, WI, USA). A three-dimensional (3D) fast gradient-recalled acquisition in the steady state with a radiofrequency spoiling (FSPGR) sequence was used to obtain 1 × 1 × 1 mm<sup>3</sup> resolution T1-weighted anatomical images. We co-registered the MEG data with the MRI data based on the location of three fiducial marks (the nasion and two pre-auricular points) and approximately 150 digitalization points on each subject’s scalp.</p></sec><sec id="s4-5"><title>Data pre-processing</title><p>The raw data were first processed using the temporal Signal Space Separation (tSSS) method (<xref ref-type="bibr" rid="bib45">Taulu and Simola, 2006</xref>) implanted in Maxfilter software (Elekta-Neuromag) for magnetic artefact suppression. Manually identified bad channels (3–5 channels per subject) with excessive noise were excluded from further analysis. The head position of the second block was co-registered to that of the first block using MaxMove software (Elekta-Neuromag). The independent component analysis (ICA) method was applied to remove the ocular artefacts using the MNE software (<ext-link ext-link-type="uri" xlink:href="http://www.martinos.org/mne/">http://www.martinos.org/mne/</ext-link>). Specifically, the ICA component with the highest correlation to the EOG signal was selected and rejected. Then, a bandpass filter (0.2 to 60 Hz) and a notch filter (50 Hz power-line interference) were applied to the de-noised data using the FieldTrip toolbox (<ext-link ext-link-type="uri" xlink:href="http://www.fieldtriptoolbox.org">http://www.fieldtriptoolbox.org</ext-link>), during which edge effects after filtering were eliminated by padding extra data on either side of each epoch. Finally, the event-related field (ERF) response was obtained by averaging the data with a time window of −0.5 to 12.5 s.</p><p>The cortical reconstruction and volume-based segmentation were completed based on each subject’s T1-weighted image using Freesurfer (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>). A realistic boundary element method (BEM) model was employed to solve the forward problem. A 5 mm cubic grid was prepared as the source space, resulting in approximately 10,000 nodes in the entire brain.</p></sec><sec id="s4-6"><title>Sensor-level analysis</title><p>The ERF response acquired during the pre-processing stage was modified by excluding the first 1.25 s of the stimulus to eliminate the transient response. Therefore, the stimulus length of the ERF was 11.25 s, leading to a frequency resolution of 1/11.25 Hz, that is 0.089 Hz. To pre-whiten all the magnetometers and gradiometers, a diagonal noise covariance matrix was calculated based on the 500 ms pre-stimulus baseline. Then, the pre-whitened data were transformed into the frequency domain using the fast Fourier transform (FFT). The average response power of all channels was calculated to compare the rhythmic response among the different conditions.</p></sec><sec id="s4-7"><title>Source estimation method</title><p>The present source estimation method was rooted in the previously developed minimum L1-norm estimation methods (<xref ref-type="bibr" rid="bib14">Huang et al., 2006</xref>; <xref ref-type="bibr" rid="bib15">Huang et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Sheng et al., 2019</xref>), which offer high spatial resolution for MEG analysis. The <italic>m</italic> × <italic>t</italic> sensor waveform matrix, where <italic>m</italic> and <italic>t</italic> represent the number of MEG sensors and the number of time points, respectively, was transformed using the FFT into frequency domain matrix <bold>K</bold>, which included both real and imaginary parts. To focus on the peak response at a given frequency (for example, 0.8 Hz in the imagery block during which the rhythm of mental imagery was manifested), we chose the single-frequency bin <bold>K</bold><italic><sub>f</sub></italic> for the source estimation, where <italic>f</italic> refers to the target frequency bin. Furthermore, a convex second-order cone programming (SOCP) method for solving the minimum L1-norm problem was applied to minimize the bias toward the coordinate axes (<xref ref-type="bibr" rid="bib34">Ou et al., 2009</xref>) as follows:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ2"><label>(1)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></disp-formula>where <bold><italic>w</italic></bold> refers to the depth weighting vector, <bold>G</bold> refers to the <italic>m</italic> × 2N gain matrix obtained from the BEM calculation, <italic>n</italic> is the number of the source grid, and <italic>i</italic> is the index of the source grid. <italic>θ</italic> and <italic>ϕ</italic> refer to the two dominant directions of the gain matrix (i.e. dominant source directions) using singular value decomposition. The solution is the <italic>n</italic> × 1 complex vector <bold>Ω</bold><italic><sub>f</sub></italic>.</p></sec><sec id="s4-8"><title>Statistical analysis</title><p>For the sensor-level analysis, a one-tailed paired <italic>t</italic> test was conducted to evaluate whether a significant spectral peak occurred at a frequency bin compared to the average of two neighboring frequency bins. This test was applied to all the frequency bins between 0.5 and 4.5 Hz, and the p values were adjusted by a false discovery rate (FDR) correction for multiple comparisons.</p><p>For the source-level analysis, each subject’s ERF response ranging from 1.25 to 12.5 s was first transformed into the frequency domain for the source estimation. The brain activation to the target frequency bin was obtained by calculating the root mean square (RMS) of the real and imaginary parts. A control state was created by averaging the sources of two neighbor frequency bins adjacent to the target frequency. Before the group analysis, individual brain result was projected to the MNI-152 template using FSL software (<ext-link ext-link-type="uri" xlink:href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/</ext-link>) and spatially smoothed with a Gaussian kernel of 5 mm full-width at half-maximum (FWHM). The brain results were further log-transformed to reduce the skewness of the distribution of the minimum L1-norm solutions (<xref ref-type="bibr" rid="bib16">Huang et al., 2016</xref>). A brain activation map at a certain frequency bin under each condition was obtained by comparing the brain responses with the average of their neighboring controls using a one-tailed paired <italic>t</italic> test (cluster level p&lt;0.01, family-wise error [FWE] corrected with a voxel-level threshold of p&lt;0.001). Furthermore, several regions of interest (ROIs) were identified by finding the local maximum of the significant neural clusters, and a 5 mm cube was used to extract the activation data from each ROI. Then, the brain activation was quantitatively compared among the following three levels: (1) at the frequency of 0.8 Hz under the imagery condition; (2) at the frequency of 4 Hz under the imagery condition; and (3) at the frequency of 4 Hz under the baseline condition. Repeated measures ANOVAs were used to examine the differences among the three levels. A Greenhouse-Geisser correction was applied for violation of sphericity. Post hoc comparisons were conducted using Bonferroni corrections. Here, non-independent ROI selection criteria were chosen to quantitatively compare the source-level neural activation between conditions; thus, we interpreted the results with caution to avoid overestimation of the ROI data.</p></sec><sec id="s4-9"><title>SEEG experiment</title><sec id="s4-9-1"><title>SEEG Participants</title><p>Five subjects (three females; 27.6 ± 12.0 years old, range 14–46 years old) who were undergoing clinical intracerebral evaluation for epilepsy participated in the sEEG experiment. All the participants were right-handed and had normal hearing abilities. Before the experiment, the participants gave their written consent for participation, including permission for scientific publication. As one subject was a minor, written informed consent from his/her parent was obtained. Intracerebral electrodes were stereotactically implanted within the participants’ brains solely for clinical purposes. All analyses were performed offline and did not interfere with the clinical management of the subjects. The sEEG study was approved by the Ethics Committee of the Sanbo Brain Hospital, Capital Medical University.</p></sec><sec id="s4-9-2"><title>SEEG recordings</title><p>Each 0.8 mm diameter intracerebral electrode contained 12–18 independent recording contacts of 2 mm in length and 1.5 mm apart from each other. The reference site was attached to the skin of each subject’s forehead. For each subject, the cortical reconstruction and segmentation were computed based on the T1-weighted image obtained before the electrode implantation using BrainSuite software (<ext-link ext-link-type="uri" xlink:href="http://brainsuite.org/">http://brainsuite.org/</ext-link>). Then, the CT with the electrode localizations and the anatomical images were co-registered and normalized to the MNI-152 template. The contact locations were identified and visualized with MNI coordinates using the Brainstorm toolbox (<ext-link ext-link-type="uri" xlink:href="http://neuroimgage.usc.edu/brainstorm/">http://neuroimgage.usc.edu/brainstorm/</ext-link>) and the BrainNetViewer toolbox (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/bnv/">https://www.nitrc.org/projects/bnv/</ext-link>) in the MATLAB environment. The intracranial neural responses were collected with the Nicolet clinical amplifier. The sampling rate was 512 Hz in four subjects (subjects 1 to 4) and 2048 Hz in one subject (subject 5).</p></sec><sec id="s4-9-3"><title>SEEG procedures</title><p>The subjects listened to the same stimuli and performed the same task as the healthy subjects in the previous MEG experiment. The participants were asked to count in their mind every five numbers in a group (imagery-rate frequency = 0.8 Hz) following a sequence of 50 pure tones (stimulus-rate frequency = 4 Hz). The auditory stimuli and trial structure were identical to those used in the MEG study, except that at the end of each trial under the mental imagery condition, the participants were instructed to orally report whether they had counted correctly following all the sounds. Then, the experimenter recorded their subjective report (correct/incorrect). Only correct trials were included in the subsequent analyses. In addition, the number of trials under each condition was dependent on each patient’s physical status, while a fixed number of trials was used in the MEG experiment. Furthermore, to reduce the confusion caused by the imagery task among the patients, the baseline condition was presented first, followed by the mental imagery condition. Before the formal mental imagery trials, the participants engaged in a practice session to ensure that they had fully understood the imagery task.</p></sec><sec id="s4-9-4"><title>SEEG data processing</title><p>The sEEG data from each contact were first analyzed separately, and the high gamma activity, which is related to the neural firing rate (<xref ref-type="bibr" rid="bib31">Mukamel et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Ray and Maunsell, 2011</xref>), was extracted by a bandpass filter of 60–100 Hz. The instantaneous amplitude envelope was obtained from the filtered data by applying the Hilbert transform and then converted to the frequency domain in each trial using the FFT. A significant spectrum peak was reported if the response power at the target frequency was significantly stronger than the average of its two neighboring frequencies using a one-tailed paired <italic>t</italic> test among all the trials under the same condition. This test was applied to all frequency bins between 0.5 and 4.5 Hz, and the p values were FDR corrected for multiple comparisons. The spectral power at each contact under both conditions was transformed to the <italic>Z</italic>-score for visual presentation (<xref ref-type="fig" rid="fig5">Figure 5a,c,d</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Specifically, the <italic>Z-</italic>score was computed as the difference between the power response in a frequency bin and the mean power response between 0.5 Hz and 4.5 Hz excluding the frequency bins of 0.8 Hz and 4 Hz divided by the standard deviation (SD) of the power response in these frequency bins.</p><p>To compare the responses among the contacts, the spectral peaks at each contact were first extracted and normalized as follows:<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where Power<sub>target</sub> refers to the response power at a target frequency (i.e. 4 Hz), and Power<sub>neighboring</sub> refers to the averaged response power at the two frequency bins neighboring the target frequency. For each brain region, a one-tailed paired <italic>t</italic> test was used to evaluate whether the normalized peak of all trials in that region was significantly larger than zero. Then, a two-tailed independent <italic>t</italic> test was used to identify the difference in the normalized peak responses between the imagery condition and the baseline condition. The significance level was set at p&lt;0.05.</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Data curation, Formal analysis, Supervision, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Formal analysis, Validation, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Software, Writing—review and editing</p></fn><fn fn-type="con" id="con5"><p>Visualization, Writing—review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Methodology</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Data curation, Formal analysis, Supervision, Funding acquisition, Validation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: For subjects in MEG experiment, written informed consents and consents to publish were obtained from all participants. The MEG experimental protocol was approved by the Peking University Institutional Review Board. For subjects in sEEG experiment, the written consent for participation was obtained, including permission for scientific publication. As one subject was a minor, written informed consent from his/her parent was obtained. The sEEG study was approved by the Ethics Committee of the Sanbo Brain Hospital, Capital Medical University.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.48971.014</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-48971-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Source data files have been provided for Figures 1, 2, 4 and 5. MEG and sEEG data have been provided on the Open Science Framework under the identifier bacge (<ext-link ext-link-type="uri" xlink:href="https://osf.io/bacge/">https://osf.io/bacge/</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Lingx</surname><given-names>Lu</given-names></name><name><surname>Qian</surname><given-names>Wang</given-names></name><name><surname>Jingwei</surname><given-names>Sheng</given-names></name><name><surname>Zhaowei</surname><given-names>Liu</given-names></name><name><surname>Lang</surname><given-names>Qin</given-names></name><name><surname>Liang</surname><given-names>Li</given-names></name><name><surname>Jia-Hong</surname><given-names>Gao</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Speech mental imagery</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="archive" xlink:href="https://osf.io/bacge/">bacge</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acheson</surname> <given-names>DJ</given-names></name><name><surname>Hamidi</surname> <given-names>M</given-names></name><name><surname>Binder</surname> <given-names>JR</given-names></name><name><surname>Postle</surname> <given-names>BR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A common neural substrate for language production and verbal working memory</article-title><source>Journal of Cognitive Neuroscience</source><volume>23</volume><fpage>1358</fpage><lpage>1367</lpage><pub-id pub-id-type="doi">10.1162/jocn.2010.21519</pub-id><pub-id pub-id-type="pmid">20617889</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackermann</surname> <given-names>H</given-names></name><name><surname>Riecker</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The contribution of the insula to motor aspects of speech production: a review and a hypothesis</article-title><source>Brain and Language</source><volume>89</volume><fpage>320</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1016/S0093-934X(03)00347-X</pub-id><pub-id pub-id-type="pmid">15068914</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akbari</surname> <given-names>H</given-names></name><name><surname>Khalighinejad</surname> <given-names>B</given-names></name><name><surname>Herrero</surname> <given-names>JL</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Towards reconstructing intelligible speech from the human auditory cortex</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>874</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-37359-z</pub-id><pub-id pub-id-type="pmid">30696881</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alderson-Day</surname> <given-names>B</given-names></name><name><surname>Fernyhough</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Inner speech: development, cognitive functions, phenomenology, and neurobiology</article-title><source>Psychological Bulletin</source><volume>141</volume><fpage>931</fpage><lpage>965</lpage><pub-id pub-id-type="doi">10.1037/bul0000021</pub-id><pub-id pub-id-type="pmid">26011789</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aleman</surname> <given-names>A</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name><name><surname>Koppenhagen</surname> <given-names>H</given-names></name><name><surname>Hagoort</surname> <given-names>P</given-names></name><name><surname>de Haan</surname> <given-names>EH</given-names></name><name><surname>Kahn</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The functional neuroanatomy of metrical stress evaluation of perceived and imagined spoken words</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>221</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh124</pub-id><pub-id pub-id-type="pmid">15269107</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrmann</surname> <given-names>M</given-names></name><name><surname>Winocur</surname> <given-names>G</given-names></name><name><surname>Moscovitch</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Dissociation between mental imagery and object recognition in a brain-damaged patient</article-title><source>Nature</source><volume>359</volume><fpage>636</fpage><lpage>637</lpage><pub-id pub-id-type="doi">10.1038/359636a0</pub-id><pub-id pub-id-type="pmid">1406994</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Draguhn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neuronal oscillations in cortical networks</article-title><source>Science</source><volume>304</volume><fpage>1926</fpage><lpage>1929</lpage><pub-id pub-id-type="doi">10.1126/science.1099745</pub-id><pub-id pub-id-type="pmid">15218136</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cope</surname> <given-names>TE</given-names></name><name><surname>Sohoglu</surname> <given-names>E</given-names></name><name><surname>Sedley</surname> <given-names>W</given-names></name><name><surname>Patterson</surname> <given-names>K</given-names></name><name><surname>Jones</surname> <given-names>PS</given-names></name><name><surname>Wiggins</surname> <given-names>J</given-names></name><name><surname>Dawson</surname> <given-names>C</given-names></name><name><surname>Grube</surname> <given-names>M</given-names></name><name><surname>Carlyon</surname> <given-names>RP</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name><name><surname>Rowe</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence for causal top-down frontal contributions to predictive processes in speech perception</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>2154</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-01958-7</pub-id><pub-id pub-id-type="pmid">29255275</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daitch</surname> <given-names>AL</given-names></name><name><surname>Foster</surname> <given-names>BL</given-names></name><name><surname>Schrouff</surname> <given-names>J</given-names></name><name><surname>Rangarajan</surname> <given-names>V</given-names></name><name><surname>Kaşikçi</surname> <given-names>I</given-names></name><name><surname>Gattas</surname> <given-names>S</given-names></name><name><surname>Parvizi</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mapping human temporal and parietal neuronal population activity and functional coupling during mathematical cognition</article-title><source>PNAS</source><volume>113</volume><fpage>E7277</fpage><lpage>E7286</lpage><pub-id pub-id-type="doi">10.1073/pnas.1608434113</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Changeux</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Experimental and theoretical approaches to conscious processing</article-title><source>Neuron</source><volume>70</volume><fpage>200</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.03.018</pub-id><pub-id pub-id-type="pmid">21521609</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname> <given-names>N</given-names></name><name><surname>Bosch</surname> <given-names>SE</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Vividness of visual imagery depends on the neural overlap with perception in visual Areas</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>1367</fpage><lpage>1373</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3022-16.2016</pub-id><pub-id pub-id-type="pmid">28073940</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Melloni</surname> <given-names>L</given-names></name><name><surname>Zhang</surname> <given-names>H</given-names></name><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>158</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1038/nn.4186</pub-id><pub-id pub-id-type="pmid">26642090</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heavey</surname> <given-names>CL</given-names></name><name><surname>Hurlburt</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The phenomena of inner experience</article-title><source>Consciousness and Cognition</source><volume>17</volume><fpage>798</fpage><lpage>810</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2007.12.006</pub-id><pub-id pub-id-type="pmid">18258456</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>MX</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Song</surname> <given-names>T</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name><name><surname>Harrington</surname> <given-names>DL</given-names></name><name><surname>Podgorny</surname> <given-names>I</given-names></name><name><surname>Canive</surname> <given-names>JM</given-names></name><name><surname>Lewis</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Vector-based spatial-temporal minimum L1-norm solution for MEG</article-title><source>NeuroImage</source><volume>31</volume><fpage>1025</fpage><lpage>1037</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.029</pub-id><pub-id pub-id-type="pmid">16542857</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>MX</given-names></name><name><surname>Nichols</surname> <given-names>S</given-names></name><name><surname>Robb</surname> <given-names>A</given-names></name><name><surname>Angeles</surname> <given-names>A</given-names></name><name><surname>Drake</surname> <given-names>A</given-names></name><name><surname>Holland</surname> <given-names>M</given-names></name><name><surname>Asmussen</surname> <given-names>S</given-names></name><name><surname>D'Andrea</surname> <given-names>J</given-names></name><name><surname>Chun</surname> <given-names>W</given-names></name><name><surname>Levy</surname> <given-names>M</given-names></name><name><surname>Cui</surname> <given-names>L</given-names></name><name><surname>Song</surname> <given-names>T</given-names></name><name><surname>Baker</surname> <given-names>DG</given-names></name><name><surname>Hammer</surname> <given-names>P</given-names></name><name><surname>McLay</surname> <given-names>R</given-names></name><name><surname>Theilmann</surname> <given-names>RJ</given-names></name><name><surname>Coimbra</surname> <given-names>R</given-names></name><name><surname>Diwakar</surname> <given-names>M</given-names></name><name><surname>Boyd</surname> <given-names>C</given-names></name><name><surname>Neff</surname> <given-names>J</given-names></name><name><surname>Liu</surname> <given-names>TT</given-names></name><name><surname>Webb-Murphy</surname> <given-names>J</given-names></name><name><surname>Farinpour</surname> <given-names>R</given-names></name><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Harrington</surname> <given-names>DL</given-names></name><name><surname>Heister</surname> <given-names>D</given-names></name><name><surname>Lee</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An automatic MEG low-frequency source imaging approach for detecting injuries in mild and moderate TBI patients with blast and non-blast causes</article-title><source>NeuroImage</source><volume>61</volume><fpage>1067</fpage><lpage>1082</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.029</pub-id><pub-id pub-id-type="pmid">22542638</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname> <given-names>CW</given-names></name><name><surname>Huang</surname> <given-names>MX</given-names></name><name><surname>Ji</surname> <given-names>Z</given-names></name><name><surname>Swan</surname> <given-names>AR</given-names></name><name><surname>Angeles</surname> <given-names>AM</given-names></name><name><surname>Song</surname> <given-names>T</given-names></name><name><surname>Huang</surname> <given-names>JW</given-names></name><name><surname>Lee</surname> <given-names>RR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>High-resolution MEG source imaging approach to accurately localize broca's area in patients with brain tumor or epilepsy</article-title><source>Clinical Neurophysiology</source><volume>127</volume><fpage>2308</fpage><lpage>2316</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2016.02.007</pub-id><pub-id pub-id-type="pmid">27072104</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hurlburt</surname> <given-names>RT</given-names></name><name><surname>Heavey</surname> <given-names>CL</given-names></name><name><surname>Kelsey</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Toward a phenomenology of inner speaking</article-title><source>Consciousness and Cognition</source><volume>22</volume><fpage>1477</fpage><lpage>1494</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2013.10.003</pub-id><pub-id pub-id-type="pmid">24184987</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hurlburt</surname> <given-names>RT</given-names></name><name><surname>Heavey</surname> <given-names>CL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Investigating pristine inner experience: implications for experience sampling and questionnaires</article-title><source>Consciousness and Cognition</source><volume>31</volume><fpage>148</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2014.11.002</pub-id><pub-id pub-id-type="pmid">25486341</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishai</surname> <given-names>A</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Ungerleider</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Visual imagery of famous faces: effects of memory and attention revealed by fMRI</article-title><source>NeuroImage</source><volume>17</volume><fpage>1729</fpage><lpage>1741</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1330</pub-id><pub-id pub-id-type="pmid">12498747</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleber</surname> <given-names>B</given-names></name><name><surname>Birbaumer</surname> <given-names>N</given-names></name><name><surname>Veit</surname> <given-names>R</given-names></name><name><surname>Trevorrow</surname> <given-names>T</given-names></name><name><surname>Lotze</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Overt and imagined singing of an italian Aria</article-title><source>NeuroImage</source><volume>36</volume><fpage>889</fpage><lpage>900</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.02.053</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Schulze</surname> <given-names>K</given-names></name><name><surname>Sammler</surname> <given-names>D</given-names></name><name><surname>Fritz</surname> <given-names>T</given-names></name><name><surname>Müller</surname> <given-names>K</given-names></name><name><surname>Gruber</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Functional architecture of verbal and tonal working memory: an FMRI study</article-title><source>Human Brain Mapping</source><volume>30</volume><fpage>859</fpage><lpage>873</lpage><pub-id pub-id-type="doi">10.1002/hbm.20550</pub-id><pub-id pub-id-type="pmid">18330870</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname> <given-names>SM</given-names></name><name><surname>Pascual-Leone</surname> <given-names>A</given-names></name><name><surname>Felician</surname> <given-names>O</given-names></name><name><surname>Camposano</surname> <given-names>S</given-names></name><name><surname>Keenan</surname> <given-names>JP</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name><name><surname>Ganis</surname> <given-names>G</given-names></name><name><surname>Sukel</surname> <given-names>KE</given-names></name><name><surname>Alpert</surname> <given-names>NM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The role of area 17 in visual imagery: convergent evidence from PET and rTMS</article-title><source>Science</source><volume>284</volume><fpage>167</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1126/science.284.5411.167</pub-id><pub-id pub-id-type="pmid">10102821</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname> <given-names>SM</given-names></name><name><surname>Ganis</surname> <given-names>G</given-names></name><name><surname>Thompson</surname> <given-names>WL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural foundations of imagery</article-title><source>Nature Reviews Neuroscience</source><volume>2</volume><fpage>635</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1038/35090055</pub-id><pub-id pub-id-type="pmid">11533731</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kraemer</surname> <given-names>DJ</given-names></name><name><surname>Macrae</surname> <given-names>CN</given-names></name><name><surname>Green</surname> <given-names>AE</given-names></name><name><surname>Kelley</surname> <given-names>WM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Musical imagery: sound of silence activates auditory cortex</article-title><source>Nature</source><volume>434</volume><elocation-id>158</elocation-id><pub-id pub-id-type="doi">10.1038/434158a</pub-id><pub-id pub-id-type="pmid">15758989</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebon</surname> <given-names>F</given-names></name><name><surname>Horn</surname> <given-names>U</given-names></name><name><surname>Domin</surname> <given-names>M</given-names></name><name><surname>Lotze</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Motor imagery training: kinesthetic imagery strategy and inferior parietal fMRI activation</article-title><source>Human Brain Mapping</source><volume>39</volume><fpage>1805</fpage><lpage>1813</lpage><pub-id pub-id-type="doi">10.1002/hbm.23956</pub-id><pub-id pub-id-type="pmid">29322583</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenc</surname> <given-names>T</given-names></name><name><surname>Keller</surname> <given-names>PE</given-names></name><name><surname>Varlet</surname> <given-names>M</given-names></name><name><surname>Nozaradan</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural tracking of the musical beat is enhanced by low-frequency sounds</article-title><source>PNAS</source><volume>115</volume><fpage>8221</fpage><lpage>8226</lpage><pub-id pub-id-type="doi">10.1073/pnas.1801421115</pub-id><pub-id pub-id-type="pmid">30037989</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lima</surname> <given-names>CF</given-names></name><name><surname>Lavan</surname> <given-names>N</given-names></name><name><surname>Evans</surname> <given-names>S</given-names></name><name><surname>Agnew</surname> <given-names>Z</given-names></name><name><surname>Halpern</surname> <given-names>AR</given-names></name><name><surname>Shanmugalingam</surname> <given-names>P</given-names></name><name><surname>Meekings</surname> <given-names>S</given-names></name><name><surname>Boebinger</surname> <given-names>D</given-names></name><name><surname>Ostarek</surname> <given-names>M</given-names></name><name><surname>McGettigan</surname> <given-names>C</given-names></name><name><surname>Warren</surname> <given-names>JE</given-names></name><name><surname>Scott</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Feel the noise: relating individual differences in auditory imagery to the structure and function of sensorimotor systems</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>4638</fpage><lpage>4650</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv134</pub-id><pub-id pub-id-type="pmid">26092220</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>S</given-names></name><name><surname>Brunner</surname> <given-names>P</given-names></name><name><surname>Holdgraf</surname> <given-names>C</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Crone</surname> <given-names>NE</given-names></name><name><surname>Rieger</surname> <given-names>J</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name><name><surname>Pasley</surname> <given-names>BN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decoding spectrotemporal features of overt and covert speech from the human cortex</article-title><source>Frontiers in Neuroengineering</source><volume>7</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fneng.2014.00014</pub-id><pub-id pub-id-type="pmid">24904404</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>S</given-names></name><name><surname>Brunner</surname> <given-names>P</given-names></name><name><surname>Iturrate</surname> <given-names>I</given-names></name><name><surname>Millán</surname> <given-names>JR</given-names></name><name><surname>Schalk</surname> <given-names>G</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name><name><surname>Pasley</surname> <given-names>BN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Word pair classification during imagined speech using direct brain recordings</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>25803</elocation-id><pub-id pub-id-type="doi">10.1038/srep25803</pub-id><pub-id pub-id-type="pmid">27165452</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>S</given-names></name><name><surname>Mikutta</surname> <given-names>C</given-names></name><name><surname>Leonard</surname> <given-names>MK</given-names></name><name><surname>Hungate</surname> <given-names>D</given-names></name><name><surname>Koelsch</surname> <given-names>S</given-names></name><name><surname>Shamma</surname> <given-names>S</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name><name><surname>Millán</surname> <given-names>JDR</given-names></name><name><surname>Knight</surname> <given-names>RT</given-names></name><name><surname>Pasley</surname> <given-names>BN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural encoding of auditory features during music perception and imagery</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>4222</fpage><lpage>4233</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx277</pub-id><pub-id pub-id-type="pmid">29088345</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukamel</surname> <given-names>R</given-names></name><name><surname>Nir</surname> <given-names>Y</given-names></name><name><surname>Harel</surname> <given-names>M</given-names></name><name><surname>Arieli</surname> <given-names>A</given-names></name><name><surname>Malach</surname> <given-names>R</given-names></name><name><surname>Fried</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Invariance of firing rate and field potential dynamics to stimulus modulation rate in human auditory cortex</article-title><source>Human Brain Mapping</source><volume>32</volume><fpage>1181</fpage><lpage>1193</lpage><pub-id pub-id-type="doi">10.1002/hbm.21100</pub-id><pub-id pub-id-type="pmid">20665720</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname> <given-names>S</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name><name><surname>Missal</surname> <given-names>M</given-names></name><name><surname>Mouraux</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Tagging the neuronal entrainment to beat and meter</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>10234</fpage><lpage>10240</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0411-11.2011</pub-id><pub-id pub-id-type="pmid">21753000</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname> <given-names>S</given-names></name><name><surname>Keller</surname> <given-names>PE</given-names></name><name><surname>Rossion</surname> <given-names>B</given-names></name><name><surname>Mouraux</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>EEG Frequency-Tagging and Input–Output Comparison in Rhythm Perception</article-title><source>Brain Topography</source><volume>31</volume><fpage>153</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1007/s10548-017-0605-8</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ou</surname> <given-names>W</given-names></name><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name><name><surname>Golland</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A distributed spatio-temporal EEG/MEG inverse solver</article-title><source>NeuroImage</source><volume>44</volume><fpage>932</fpage><lpage>946</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.05.063</pub-id><pub-id pub-id-type="pmid">18603008</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papoutsi</surname> <given-names>M</given-names></name><name><surname>de Zwart</surname> <given-names>JA</given-names></name><name><surname>Jansma</surname> <given-names>JM</given-names></name><name><surname>Pickering</surname> <given-names>MJ</given-names></name><name><surname>Bednar</surname> <given-names>JA</given-names></name><name><surname>Horwitz</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>From phonemes to articulatory codes: an fMRI study of the role of broca's Area in Speech Production</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>2156</fpage><lpage>2165</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn239</pub-id><pub-id pub-id-type="pmid">19181696</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname> <given-names>H</given-names></name><name><surname>Ince</surname> <given-names>RA</given-names></name><name><surname>Schyns</surname> <given-names>PG</given-names></name><name><surname>Thut</surname> <given-names>G</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title><source>Current Biology</source><volume>25</volume><fpage>1649</fpage><lpage>1653</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.04.049</pub-id><pub-id pub-id-type="pmid">26028433</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulesu</surname> <given-names>E</given-names></name><name><surname>Frith</surname> <given-names>CD</given-names></name><name><surname>Frackowiak</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The neural correlates of the verbal component of working memory</article-title><source>Nature</source><volume>362</volume><fpage>342</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1038/362342a0</pub-id><pub-id pub-id-type="pmid">8455719</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A review and synthesis of the first 20 years of PET and fMRI studies of heard speech, spoken language and reading</article-title><source>NeuroImage</source><volume>62</volume><fpage>816</fpage><lpage>847</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.062</pub-id><pub-id pub-id-type="pmid">22584224</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ray</surname> <given-names>S</given-names></name><name><surname>Maunsell</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Different origins of gamma rhythm and high-gamma activity in macaque visual cortex</article-title><source>PLOS Biology</source><volume>9</volume><elocation-id>e1000610</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000610</pub-id><pub-id pub-id-type="pmid">21532743</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saito</surname> <given-names>Y</given-names></name><name><surname>Ishii</surname> <given-names>K</given-names></name><name><surname>Sakuma</surname> <given-names>N</given-names></name><name><surname>Kawasaki</surname> <given-names>K</given-names></name><name><surname>Oda</surname> <given-names>K</given-names></name><name><surname>Mizusawa</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural substrates for semantic memory of familiar songs: is there an interface between lyrics and melodies?</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e46354</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0046354</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheng</surname> <given-names>J</given-names></name><name><surname>Zheng</surname> <given-names>L</given-names></name><name><surname>Lyu</surname> <given-names>B</given-names></name><name><surname>Cen</surname> <given-names>Z</given-names></name><name><surname>Qin</surname> <given-names>L</given-names></name><name><surname>Tan</surname> <given-names>LH</given-names></name><name><surname>Huang</surname> <given-names>M-X</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Gao</surname> <given-names>J-H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The cortical maps of hierarchical linguistic structures during speech perception</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>3232</fpage><lpage>3240</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy191</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname> <given-names>E</given-names></name><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Carlyon</surname> <given-names>RP</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Predictive top-down integration of prior knowledge during speech perception</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>8443</fpage><lpage>8453</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5069-11.2012</pub-id><pub-id pub-id-type="pmid">22723684</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname> <given-names>M</given-names></name><name><surname>Thompson</surname> <given-names>R</given-names></name><name><surname>Cusack</surname> <given-names>R</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Top-down activation of shape-specific population codes in visual cortex during mental imagery</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>1565</fpage><lpage>1572</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4657-08.2009</pub-id><pub-id pub-id-type="pmid">19193903</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tal</surname> <given-names>I</given-names></name><name><surname>Large</surname> <given-names>EW</given-names></name><name><surname>Rabinovitch</surname> <given-names>E</given-names></name><name><surname>Wei</surname> <given-names>Y</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Zion Golumbic</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural entrainment to the beat: the &quot;Missing-Pulse&quot; Phenomenon</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6331</fpage><lpage>6341</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2500-16.2017</pub-id><pub-id pub-id-type="pmid">28559379</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname> <given-names>S</given-names></name><name><surname>Simola</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements</article-title><source>Physics in Medicine and Biology</source><volume>51</volume><fpage>1759</fpage><lpage>1768</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/51/7/008</pub-id><pub-id pub-id-type="pmid">16552102</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Zarate</surname> <given-names>JM</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mental imagery of speech implicates two mechanisms of perceptual reactivation</article-title><source>Cortex</source><volume>77</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2016.01.002</pub-id><pub-id pub-id-type="pmid">26889603</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Teng</surname> <given-names>X</given-names></name><name><surname>Bai</surname> <given-names>F</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Imagined speech influences perceived loudness of sound</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>225</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0305-8</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Mental imagery of speech and movement implicates the dynamics of internal forward models</article-title><source>Frontiers in Psychology</source><volume>1</volume><elocation-id>166</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2010.00166</pub-id><pub-id pub-id-type="pmid">21897822</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mental imagery of speech: linking motor and perceptual systems through internal simulation and estimation</article-title><source>Frontiers in Human Neuroscience</source><volume>6</volume><elocation-id>314</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00314</pub-id><pub-id pub-id-type="pmid">23226121</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The effect of imagination on stimulation: the functional specificity of efference copies in speech processing</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1020</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00381</pub-id><pub-id pub-id-type="pmid">23469885</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamics of self-monitoring and error detection in speech production: evidence from mental imagery and MEG</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>352</fpage><lpage>364</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00692</pub-id><pub-id pub-id-type="pmid">25061925</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitford</surname> <given-names>TJ</given-names></name><name><surname>Jack</surname> <given-names>BN</given-names></name><name><surname>Pearson</surname> <given-names>D</given-names></name><name><surname>Griffiths</surname> <given-names>O</given-names></name><name><surname>Luque</surname> <given-names>D</given-names></name><name><surname>Harris</surname> <given-names>AW</given-names></name><name><surname>Spencer</surname> <given-names>KM</given-names></name><name><surname>Le Pelley</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neurophysiological evidence of efference copies to inner speech</article-title><source>eLife</source><volume>6</volume><elocation-id>e28197</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.28197</pub-id><pub-id pub-id-type="pmid">29199947</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ylinen</surname> <given-names>S</given-names></name><name><surname>Nora</surname> <given-names>A</given-names></name><name><surname>Leminen</surname> <given-names>A</given-names></name><name><surname>Hakala</surname> <given-names>T</given-names></name><name><surname>Huotilainen</surname> <given-names>M</given-names></name><name><surname>Shtyrov</surname> <given-names>Y</given-names></name><name><surname>Mäkelä</surname> <given-names>JP</given-names></name><name><surname>Service</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Two distinct auditory-motor circuits for monitoring speech production as revealed by content-specific suppression of auditory cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>1576</fpage><lpage>1586</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht351</pub-id><pub-id pub-id-type="pmid">24414279</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Halpern</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Mental concerts: musical imagery and auditory cortex</article-title><source>Neuron</source><volume>47</volume><fpage>9</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.06.013</pub-id><pub-id pub-id-type="pmid">15996544</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zvyagintsev</surname> <given-names>M</given-names></name><name><surname>Clemens</surname> <given-names>B</given-names></name><name><surname>Chechko</surname> <given-names>N</given-names></name><name><surname>Mathiak</surname> <given-names>KA</given-names></name><name><surname>Sack</surname> <given-names>AT</given-names></name><name><surname>Mathiak</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain networks underlying mental imagery of auditory and visual information</article-title><source>European Journal of Neuroscience</source><volume>37</volume><fpage>1421</fpage><lpage>1434</lpage><pub-id pub-id-type="doi">10.1111/ejn.12140</pub-id><pub-id pub-id-type="pmid">23383863</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48971.018</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Griffiths</surname><given-names>Timothy D</given-names></name><role>Reviewing Editor</role><aff><institution>Newcastle University</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Cope</surname><given-names>Thomas</given-names> </name><role>Reviewer</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Neural tracking of speech mental imagery&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Thomas Cope (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>The reviewers found the work interesting in demonstrating 'tagged' neural correlates using MEG and invasive recording of cyclical counting in premotor cortex, temporoparietal junction and occipitotemporal transition that could not be attributable to the stimulus.</p><p>Essential revisions:</p><p>1) The single most important comment is the attribution of the basis for the cyclical brain activity corresponding to the counting. This has a number of cognitive components including, for example, attention, working memory, phonological analysis, and (possibly) articulatory planning. To what extent do the authors feel that their use of the term 'speech mental imagery' is justified? The reviewers considered whether a more focused term was more appropriate. <italic>eLife</italic> does not encourage extensive further experiments or multiple cycles of revision that might address these components. What is required in a revision is a better justification of the term 'speech mental imagery' in the title and text or a reconsideration of this term.</p><p>2) The ANOVA with MEG is biased because the MEG activity focus was first defined by the contrast between the respective condition and baseline.</p><p>3) The independent repeated measures ANOVAs for each region is not the appropriate test here. Rather, an overall RM-ANOVA should be employed, with the test of interest being for a condition by region interaction. Post hoc tests should then be performed on marginal means, normalised for the overall effect of condition.</p><p>4) The activation of sensory-motor cortex is intriguing, but the claim that this area is involved in imagery speech as also claimed in (Bouchard et al., 2013) is not very compelling. For such a claim, it is necessary to ensure there was no articulatory movement (e.g. with EMG sensors). Otherwise, it could simply reflect a hardly noticeable yet existing muscle movement in the articulators as the subjects count the numbers.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48971.019</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The single most important comment is the attribution of the basis for the cyclical brain activity corresponding to the counting. This has a number of cognitive components including, for example, attention, working memory, phonological analysis, and (possibly) articulatory planning. To what extent do the authors feel that their use of the term 'speech mental imagery' is justified? The reviewers considered whether a more focused term was more appropriate. eLife does not encourage extensive further experiments or multiple cycles of revision that might address these components. What is required in a revision is a better justification of the term 'speech mental imagery' in the title and text or a reconsideration of this term.</p></disp-quote><p>We completely agree with the view that several cognitive components were involved in the rhythmic inner counting task when participants were asked to construct their speech mental imagery. Mental imagery has been proposed to be essentially an attention-guided memory retrieval process, for example, in the visual domain (Ishai et al., 2002). Mental imagery of speech not only recruits the pathway by which phonological, lexical, or semantic details are retrieved from the memory system but also requires planned articulation that is simulated internally to enable individuals to have subjective experiences of their mental imagery (Tian and Poeppel, 2012; Tian et al., 2016). Therefore, these cognitive components cannot be definitely isolated from the term speech mental imagery.</p><p>In our study, neural tracking of speech mental imagery refers to the detection of the cyclical neural signal induced by the task of rhythmic inner counting. This cyclical neural signal can be attributed to the top-down regulation of the generation of number imagery, specifically, the internal arrangement of numbers into groups. Such internal organization is primarily based on the details of the inner representation of number imagery. In this way, the imagery-rate neural activity is tagged in the frequency domain and can be captured using neurophysiological measurements. It is known that the activation patterns of the brain network underlying speech mental imagery can be modulated by the task requirements and contextual variation (Tian and Poeppel, 2012, 2013; Tian et al., 2018). Therefore, the most important point to note is the operational definition of the internal organization of mental imagery, which is the key factor that induces cyclical neural signals. Considering that the term “speech mental imagery” as used in the previous manuscript is not focused enough to provide sufficient information about the intention and methodology of our study, we have now revised the term by adding the operational definition of the internal organization of mental imagery, namely, the task of rhythmic inner counting. Correspondingly, in the revised manuscript, we have added relevant content to justify the use of “speech mental imagery” as follows:</p><p>[Title] “Neural tracking of speech mental imagery during rhythmic inner counting”</p><p>[Discussion] “Mental imagery has been proposed to be essentially an attention-guided memory retrieval process, for example, in the visual domain (Ishai et al., 2002). […] The operational definition of speech mental imagery condition, namely, the rhythmic inner counting task, is not only related to the internal representations of speech but also requires high levels of working memory.”</p><p>Correspondingly, we have added the following references to the revised manuscript:</p><p>Ishai, Haxby and Ungerleider, 2002; Tian and Poeppel, 2012.</p><disp-quote content-type="editor-comment"><p>2) The ANOVA with MEG is biased because the MEG activity focus was first defined by the contrast between the respective condition and baseline.</p></disp-quote><p>ROI analysis between conditions can be biased when the ROIs are selected by contrasting between those conditions, leading to circular analysis that can distort the results (Kriegeskorte et al., 2009, Nature Neuroscience). To control the influence of circular analysis in our study, different statistical comparisons based on different null hypotheses were applied in ROI selection and ROI analysis. Specifically, we selected the ROIs from the significant activation maps, which were obtained by contrasting the whole brain activity at the target frequency bin and the average of its two neighbouring frequency bins <italic>under each condition.</italic> These activation maps illustrate the neural sources behind the rhythmic signals induced by mental imagery and auditory perception, respectively. The null hypothesis of the ROI selection was that there were no significant differences between the source-level activation at the target frequency bins and their neighbouring controllers. Different from the ROI selection criteria, we applied ANOVA on the ROI activation among three levels (at 0.8 Hz under the mental imagery condition, at 4 Hz under the mental imagery condition and at 4 Hz under the baseline condition) and 16 brain regions to identify the significant differences <italic>between conditions</italic>, which was done to quantitatively examine the activation difference in these brain regions between top-down and bottom-up processes. The null hypothesis for the ANOVA was that there were no significant differences in the neural activation among different conditions and brain regions, which differed from the assumption of the previous ROI selection criteria. In this way, we avoided circular analysis and reduced the distortion of our results caused by the ROI selection bias.</p><p>An important point to note is that, to some extent, the overlapping data subset applied for ROI selection and analysis might still cause bias due to the noise effect within the same data subset (Kriegeskorte et al., 2009). From this view, we agree with the comment that selection bias might enter into the ANOVA, despite the different statistical methods we have applied for ROI selection and analysis. We note that this statistical bias is not restricted to the current study; it potentially applies to any ROI analysis that attempts to compare ROI activity after defining the ROIs in the same data subset using non-independent ROI selection instead of independent selection criteria such as the anatomy atlas. Selection bias may cause overestimation of the results when comparing ROI data between conditions (Kriegeskorte et al., 2009). Critically, it should be noted that in our study, the results of ROI comparisons between conditions were <italic>not</italic> overestimated, as no condition difference was observed in the regions of IFG and SMG, which were the main neural clusters responsive for the top-down imagery construction according to source estimation, and the crucial findings were not based on the results of ROI analysis. However, we urge caution in interpreting the ROI results in the revised manuscript by adding the relevant content:</p><p>[Materials and methods] “Here, non-independent ROI selection criteria were chosen to quantitatively compare the source-level neural activation between conditions; thus, we interpreted the results with caution to avoid overestimation of the ROI data.”</p><disp-quote content-type="editor-comment"><p>3) The independent repeated measures ANOVAs for each region is not the appropriate test here. Rather, an overall RM-ANOVA should be employed, with the test of interest being for a condition by region interaction. Post hoc tests should then be performed on marginal means, normalised for the overall effect of condition.</p></disp-quote><p>As per the suggestion, we have conducted an overall repeated measures ANOVA (condition by brain region) for the ROI data. As a result, we found significant main effects of both condition and brain region and a significant interaction between the two variables. Then, we performed post hoc comparisons with Bonferroni correction on estimated marginal means to illustrate the differences between conditions for each brain region. We have revised the relevant content in the Results section as listed below:</p><p>“Two-way repeated measures ANOVAs (condition × brain region) for ROI activation showed a significant main effect of condition (F<sub>2, 38</sub> = 23.56, <italic>p</italic> &lt; 0.001, <italic>ηp</italic><sup>2</sup> = 0.55), a significant main effect of brain region (F<sub>5.23, 99.41</sub> = 6.66, <italic>p</italic> &lt; 0.001, <italic>ηp</italic><sup>2</sup> = 0.26), and a significant interaction between the two variables (F<sub>8.09, 153.67</sub> = 5.60, <italic>p</italic> &lt; 0.001, <italic>ηp</italic><sup>2</sup> = 0.23). […] No significant difference was observed in the brain activation levels at 4 Hz between the imagery condition and baseline condition, except in the right MFG, in which the activation at 4 Hz during mental imagery was stronger than that at baseline (<italic>p</italic> = 0.014).”</p><disp-quote content-type="editor-comment"><p>4) The activation of sensory-motor cortex is intriguing, but the claim that this area is involved in imagery speech as also claimed in (Bouchard et al., 2013) is not very compelling. For such a claim, it is necessary to ensure there was no articulatory movement (e.g. with EMG sensors). Otherwise, it could simply reflect a hardly noticeable yet existing muscle movement in the articulators as the subjects count the numbers.</p></disp-quote><p>We fully agree with the comment on the point that it is necessary to exclude the influence of muscle movements to reach a compelling conclusion on the role of the sensorimotor cortex in speech mental imagery. We respond to two aspects of this helpful comment:</p><p>First, Bouchard et al. (2013) claimed that speech-articulator representations are arranged somatotopically in the ventral PrG and PoG during syllable production, while our study found that the ventral PrG and PoG were engaged in constructing speech imagery when participants were told to count numbers in their mind without moving their lips, jaw or tongue. Our result is consistent with previous findings concerning the involvement of the sensorimotor cortex in speech imagery. For example, Tian et al. (2016) reported that the sensorimotor cortex showed stronger activation in articulatory imagery in which participants generated kinaesthetic feelings of articulation than in hearing imagery in which motor-related imagery was discouraged, suggesting that speech mental imagery with motor-related feelings could activate the sensorimotor cortex. In addition, the neural signals directly recorded from the sensorimotor cortex contributed to the imagined-speech decoding in both hearing imagery without kinaesthetic representation (Martin et al., 2016) and silent reading of speech (Martin et al., 2014). We have now revised the statement and have drawn a more careful conclusion about the involvement of the sensorimotor cortex.</p><p>Second, an earlier study by Kleber et al. (2007) attempted to record the EMG sensor placed on the throat and ensured that the muscle movements in imagined singing were significantly weaker than those in overt singing and did not differ from those during the resting state. After taking EMG data into consideration, Kleber et al. (2007) still found activation in the sensorimotor cortex in imagined singing, which could not be accounted for by muscle movements in articulators. However, to our knowledge, most imagery studies have not strictly controlled for articulator movement using EMG recording (Tian et al., 2016; Martin et al., 2014, 2016) but primarily based the results on the assumption that participants followed the instructions on how to generate mental imagery. This valuable comment raises the importance of controlling the articulatory movement effect, which should be noted in future studies, especially when the activation of the sensorimotor cortex is observed in mental imagery studies.</p><p>Thus, to clarify the logic and to address the concerns about the effect of muscle movements, we have revised the relevant content in the Discussion section as follows:</p><p>“In addition, our study shows that the ventral PrG and PoG are also activated during the construction of speech imagery, which is consistent with previous findings that stronger activation of the sensorimotor cortex induced by articulatory imagery than hearing imagery (Tian et al., 2016) and the direct neural signal from the sensorimotor cortex contributed to the decoding of imagined speech and silent reading (Martin et al., 2014, 2016). Nevertheless, a firm conclusion on the contribution of the sensorimotor cortex to the construction of speech mental imagery will only be possible after controlling for any articulator movement in future studies.”</p><p>Correspondingly, we have added relevant references to the manuscript:</p><p>Martin et al., 2016; Martin et al., 2014.</p></body></sub-article></article>