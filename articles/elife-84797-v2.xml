<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">84797</article-id><article-id pub-id-type="doi">10.7554/eLife.84797</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Spatiotemporal neural dynamics of object recognition under uncertainty in humans</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-298913"><name><surname>Wu</surname><given-names>Yuan-hao</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5631-5082</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-298914"><name><surname>Podvalny</surname><given-names>Ella</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="pa1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-71932"><name><surname>He</surname><given-names>Biyu J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1549-1351</contrib-id><email>biyu.he@nyulangone.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Neuroscience Institute, New York University Grossman School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Neurology, New York University Grossman School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Neuroscience &amp; Physiology, New York University Grossman School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>Department of Radiology, New York University Grossman School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>University Medical Center Hamburg-Eppendorf</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Center for Molecular and Behavioral Neuroscience, Rutgers University, Newark, United States</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>15</day><month>05</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e84797</elocation-id><history><date date-type="received" iso-8601-date="2022-11-09"><day>09</day><month>11</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-05-12"><day>12</day><month>05</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-11-17"><day>17</day><month>11</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.11.17.516923"/></event></pub-history><permissions><copyright-statement>© 2023, Wu et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Wu et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-84797-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-84797-figures-v2.pdf"/><abstract><p>While there is a wealth of knowledge about core object recognition—our ability to recognize clear, high-contrast object images—how the brain accomplishes object recognition tasks under increased uncertainty remains poorly understood. We investigated the spatiotemporal neural dynamics underlying object recognition under increased uncertainty by combining MEG and 7 Tesla (7T) fMRI in humans during a threshold-level object recognition task. We observed an early, parallel rise of recognition-related signals across ventral visual and frontoparietal regions that preceded the emergence of category-related information. Recognition-related signals in ventral visual regions were best explained by a two-state representational format whereby brain activity bifurcated for recognized and unrecognized images. By contrast, recognition-related signals in frontoparietal regions exhibited a reduced representational space for recognized images, yet with sharper category information. These results provide a spatiotemporally resolved view of neural activity supporting object recognition under uncertainty, revealing a pattern distinct from that underlying core object recognition.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>object recognition</kwd><kwd>fMRI</kwd><kwd>MEG</kwd><kwd>decoding</kwd><kwd>representational similarity analysis</kwd><kwd>large-scale brain networks</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EY032085</award-id><principal-award-recipient><name><surname>He</surname><given-names>Biyu J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Combining 7 Tesla fMRI and MEG data collected during a challenging visual recognition task revealed distinct neural representational formats in ventral visual and frontoparietal regions, and the emergence of recognition-related signals prior to category-related information.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>We recognize objects in our environment rapidly and accurately across a variety of challenging viewing conditions, allowing us to successfully navigate everyday life. Consider driving a car, it is of utmost importance to our survival that we correctly differentiate between various traffic signs in low visibility conditions, such as during a rainstorm or on a foggy night.</p><p>In visual neuroscience, object recognition processing has commonly been viewed and studied as an automatic feedforward process within the ventral visual stream from the primary visual cortex to the inferior temporal cortex (<xref ref-type="bibr" rid="bib63">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib20">DiCarlo et al., 2012</xref>). A wealth of knowledge from neurophysiology and neuroimaging contributes to our understanding about how neural computations unfolding in this feedforward process support ‘core object recognition’—our ability to recognize, identify, and categorize objects presented in clear, high visibility conditions. However, how the brain carries out object recognition tasks when sensory input is noisy or ambiguous, as often is the case in the natural environment, remains poorly understood.</p><p>Recent accumulating evidence points to the intriguing possibility that object recognition under increased uncertainty (e.g. occlusion, clutter, shading, crowding, complexity) is supported by an interplay of feedforward and feedback signals between the ventral visual and higher-order frontoparietal regions. For instance, recent primate neurophysiology studies have shown that the ventrolateral prefrontal cortex actively participates in object recognition processing under challenging circumstances (due to occlusion, image complexity, or fast presentation) by sending feedback signals to ventral visual cortices (<xref ref-type="bibr" rid="bib25">Fyall et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Kar and DiCarlo, 2021</xref>; <xref ref-type="bibr" rid="bib9">Bellet et al., 2022</xref>).</p><p>In humans, early neuroimaging evidence using a visual masking paradigm revealed that the orbitofrontal cortex (OFC) might send top-down feedback signals to inferotemporal (IT) cortex conveying a fast initial analysis of the visual input (<xref ref-type="bibr" rid="bib5">Bar et al., 2006</xref>). Importantly, the extent of higher-order frontoparietal involvement in object recognition has been substantially broadened by later work (<xref ref-type="bibr" rid="bib34">Hegdé and Kersten, 2010</xref>; <xref ref-type="bibr" rid="bib22">Filimon et al., 2013</xref>; <xref ref-type="bibr" rid="bib72">Wang et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">González-García et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>; <xref ref-type="bibr" rid="bib52">Mei et al., 2022</xref>). For example, recognition of object images presented at liminal contrasts was associated with changes in the activation/deactivation level and object category information in widespread cortical networks (<xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>).</p><p>These recent findings suggest that object recognition processing under uncertainty is not confined to the ventral visual areas, but is jointly supported by regions at different cortical hierarchical levels. While these studies revealed which brain regions are involved in object recognition processing, they leave open the questions of how recognition-related signals dynamically evolve in each of these brain regions and how they coordinate with each other. For instance, some brain regions may exhibit transient signals while others may manifest sustained responses. Neither is it known how recognition-related signals propagate across the brain: They may evolve concurrently across multiple brain areas or flow sequentially from one area to another. Finally, different brain regions might have different representational formats related to how success or failure of objects recognition sculpts the neural activity patterns and the information carried by them.</p><p>In this study, we addressed these open questions by investigating the spatiotemporal dynamics of neural activity underlying object recognition under uncertainty. We combined MEG and ultra-high-field (7 Tesla) fMRI data recorded during a threshold-level object recognition task, and employed a model-driven MEG-fMRI fusion approach (<xref ref-type="bibr" rid="bib33">Hebart et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Flounders et al., 2019</xref>) to track neural dynamics associated with different aspects of object recognition processing with high temporal and spatial resolution. Our results provide a spatiotemporally resolved view of neural activity supporting object recognition under uncertainty in the human brain, and reveal a picture of rich and heterogeneous representational dynamics across large-scale brain networks.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Paradigm and behavior</title><p>To investigate the spatiotemporal dynamics of brain activity supporting object recognition under uncertainty, we combined MEG (<italic>N</italic>=24) and fMRI data (<italic>N</italic>=25) obtained from two separate experiments using the same stimuli set and experimental paradigm (with trial timing tailored to each imaging modality, see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The main datasets analyzed herein were previously published in <xref ref-type="bibr" rid="bib58">Podvalny et al., 2019</xref>, and <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental paradigm and behavioral results.</title><p>(<bold>A</bold>) Trial timeline. (<bold>B</bold>) An example real image from each category. (<bold>C</bold>) Percentage of real-image trials reported as recognized in the MEG and fMRI experiments. Recognition rates were very close to the intended threshold-level recognition rate of 50% in both experiments. (<bold>D</bold>) Categorization accuracy for real images in recognized (R) and unrecognized (U) trials. The black dashed line indicates chance level at 25%. ***: p&lt;0.001; significant differences in categorization behavior between recognized (R) and unrecognized (U) trials (one-sided Wilcoxon signed-rank test). (<bold>C–D</bold>) The horizontal gray lines of the violin plots indicate 25th percentile, median, and 75th percentile, respectively. Dots depict individual participants’ behavioral performance in the MEG (<italic>N</italic>=24) and fMRI experiments (<italic>N</italic>=25).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig1-v2.tif"/></fig><p>In each trial, participants viewed a briefly presented image containing an object presented at a liminal contrast. After a delay of variable length, participants reported the object category and their recognition experience (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). For the recognition question, participants were instructed to report ‘yes’ whenever they saw a meaningful object in it, and report ‘no’ if they saw nothing or low-level features only. In the case of an unrecognized image, participants were instructed to provide a genuine guess of the object category. The stimuli set included five unique exemplars from each of the following categories: human face, animal, house, and man-made objects (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). For each participant, the contrast of stimuli was titrated to reach an ~50% recognition rate using a staircase procedure before the main experiment. In addition, a phase-scrambled image from each category was also included in the experiments; the scrambled images were not included in the analyses reported below due to comparatively low trial numbers.</p><p>The behavioral results from the MEG and fMRI experiments showed consistent patterns: In both experiments, the recognition rates for real images did not differ from the intended rate of 50%: MEG participants reported 44.9±3.6% of the images as recognized (mean ± SEM, <italic>W</italic>=105, p = 0.21, two-sided Wilcoxon signed-rank test against 50%), and the mean recognition rate in fMRI participants was 48.0±2.6% (<italic>W</italic>=126.5, p=0.34, <xref ref-type="fig" rid="fig1">Figure 1C</xref>). Further, the object categorization behavior for recognized images was highly accurate across both experiments (MEG: 86.1±1.9%, <italic>W</italic>=300, p=5.96×10<sup>–8</sup>; fMRI: 78.8±2.2%, <italic>W</italic>=325, p=2.9×10<sup>–8</sup>; one-sided Wilcoxon signed-rank test against chance level of 25%) and significantly higher than those for unrecognized images (MEG: <italic>W</italic>=300, p=5.96×10<sup>–8</sup>; fMRI: <italic>W</italic>=325, p=2.9×10<sup>–8</sup>; one-sided Wilcoxon signed-rank test; <xref ref-type="fig" rid="fig1">Figure 1D</xref>). Interestingly, categorization accuracy in unrecognized trials remained above-chance level (MEG: 40.1±1.8%, <italic>W=</italic>300, p=5.96×10<sup>–8</sup>; fMRI: 32±1.9%, <italic>W=</italic>251, p=0.002), consistent with previous studies showing above-chance discrimination performance in conditions where the stimulus content did not reach awareness (<xref ref-type="bibr" rid="bib45">Lau and Passingham, 2006</xref>; <xref ref-type="bibr" rid="bib35">Hesselmann et al., 2011</xref>; <xref ref-type="bibr" rid="bib47">Li et al., 2014</xref>).</p><p>Together, this pattern of behavioral results suggests that the object images were successfully presented at subjective recognition threshold, and that participants performed the categorization task meaningfully in both recognized and unrecognized trials.</p></sec><sec id="s2-2"><title>Sequential emergence of recognition and object category information</title><p>We first investigated the timing of recognition state- and object category-related information in the MEG. The presence of both types of information in the current fMRI dataset was established in a previous study, which showed that brain activity in widely distributed cortical networks contains information about the subjective recognition outcome (recognized vs. unrecognized) and category (face/animal/house/man-made objects) of the images (<xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>). Here, utilizing the fine temporal resolution of MEG, we assessed <italic>when</italic> such information emerges.</p><p>To assess the temporal evolution of brain signals containing information about subjective recognition (recognized vs. unrecognized), we conducted multivariate decoding analyses on event-related fields (DC-35 Hz) at every time point from 500 ms before to 2000 ms after stimulus onset. Specifically, we applied binary support vector machine (SVM) classifiers (<xref ref-type="bibr" rid="bib14">Chang and Lin, 2011</xref>) to discriminate whole-brain sensor-level activity patterns associated with different recognition outcomes. Similarly, to temporally resolve the unfolding of MEG signals containing information about object categories, binary SVM classifiers were applied to decode object category in a pairwise manner (six pairs total) in recognized and unrecognized trials, respectively. For both analyses, decoding accuracy significantly above chance level (50%) indicated the presence of the information of interest in MEG signals.</p><p>Recognition outcome could be significantly decoded from 200 to 900 ms after stimulus onset (p&lt;0.05, cluster-based permutation test, <xref ref-type="fig" rid="fig2">Figure 2A</xref>), with peak decoding at 580 ms. Similar to previous fMRI findings (<xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>), the decodability of image category was highly dependent on subjective recognition (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). When images were reported as unrecognized, image category was undecodable throughout the epoch (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, red). By contrast, when the same identical images were recognized, decoding accuracy for category membership peaked at 290 ms the first time, though it did not survive cluster correction; decoding accuracy reached significance at 470 ms and remained significant until 1060 ms (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, green). These results reveal long-lasting neural processes elicited by brief stimuli, consistent with earlier studies using threshold-level visual stimuli (<xref ref-type="bibr" rid="bib19">Dehaene and Changeux, 2011</xref>; <xref ref-type="bibr" rid="bib47">Li et al., 2014</xref>; <xref ref-type="bibr" rid="bib8">Baria et al., 2017</xref>). At the same time, they reveal a rise of stimulus-evoked neural processes influencing recognition behavior at ~200 ms, preceding the emergence of category information at ~470 ms.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>MEG multivariate pattern analysis (<italic>N</italic>=24).</title><p>(<bold>A</bold>) Recognition outcome decoding. (<bold>B</bold>) Image category decoding in recognized (green) and unrecognized trials (red). (<bold>C</bold>) Group-average MEG representational dissimilarity matrices (RDMs) (40×40) at selected time points. (<bold>D</bold>) Visualization of representational dissimilarity structure at the same time points as in (<bold>C</bold>) using the first two dimensions of the multidimensional scaling (MDS). Each of the filled circles represents an exemplar image that was reported as recognized and is color-coded according to its category membership. The unfilled points represent the physically identical exemplar images that were not recognized by participants. (<bold>E</bold>) Mean between-image representational dissimilarity in recognized (green) and unrecognized trials (red). (<bold>F</bold>) Left: Correlation between category model RDM and MEG RDMs in recognized (green) and unrecognized (red) trials. Right: Category model RDM. Blue and yellow colors indicates low and high representational dissimilarities, respectively. In (<bold>A–B</bold>) and (<bold>E–F</bold>), shaded areas represent SEM across participants; horizontal dashed lines indicate the chance level, and vertical dashed lines indicate significance onset latencies. Statistical significance is indicated by the colored horizontal bars (p&lt;0.05, one-sided cluster-based sign permutation tests).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Object category decoding in the localizer task.</title><p>Same format as in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. The <italic>x</italic>-axis is truncated as compared to <xref ref-type="fig" rid="fig2">Figure 2B</xref> due to shorter trial length in the localizer task (1 s). In both the localizer and main task, images were presented for 66.7 ms.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig2-figsupp1-v2.tif"/></fig></fig-group><p>It is intriguing that category-related information emerged slowly over time compared to earlier studies using high-contrast images (e.g. <xref ref-type="bibr" rid="bib13">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib16">Cichy et al., 2014</xref>). To test whether the delayed responses were due to the low visibility (and increased uncertainty) of the present stimuli, we ran a separate MEG task block wherein participants viewed the same set of images presented at high contrasts (see Methods for details). Consistent with earlier studies, we found that when image visibility was high, there was a fast rise of category information within the first 200 ms after stimulus onset (reaching significance at 110 ms, peaking at 170 ms; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). This result confirms that the late onset of category information observed in the main task was due to the more challenging viewing condition, likely reflecting the more time-consuming recurrent processing involving corticocortical feedback activity (<xref ref-type="bibr" rid="bib44">Lamme and Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="bib25">Fyall et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Kar and DiCarlo, 2021</xref>).</p></sec><sec id="s2-3"><title>Dynamic change of representation format over time</title><p>The significant recognition outcome decoding emerging at 200 ms indicates that there was a dynamic change in how recognized images were represented relative to unrecognized images. To probe this further, we applied representational similarity analysis (RSA, <xref ref-type="bibr" rid="bib41">Kriegeskorte et al., 2008</xref>) to whole-head MEG signals. At each time point, we computed a representational dissimilarity matrix (RDM) for sensor-level activity patterns across unique images and recognition outcomes. Each RDM had dimensions of 40×40 (5 exemplars×4 object categories×2 recognition outcomes; <xref ref-type="fig" rid="fig2">Figure 2C</xref>), with each cell populated with a dissimilarity measure (quantified as 1-Pearson’s correlation) between activity patterns of an image/condition pair. We applied multidimensional scaling (MDS) to visualize the structure of RDMs by projecting them onto two-dimensional spaces (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). In the MDS plot, each dot represents an image associated with a particular recognition outcome, and distances between dots reflect the dissimilarities in neural activity patterns.</p><p>As expected, there was no discernable dissimilarity structure at stimulus onset: Dissimilarities between most image/condition pairs were close to one (as indicated by the yellow color in the RDM) and dots in the corresponding MDS plot were arbitrarily distributed. By 200 ms after stimulus onset, recognized images had started to cluster together, while unrecognized images remained relatively distributed across the MDS space. To test whether the clustering effect within the recognized images was indeed stronger than unrecognized images, we compared the mean dissimilarity between recognized images against that between unrecognized images at every time point (one-sided Wilcoxon signed-rank tests). As shown in <xref ref-type="fig" rid="fig2">Figure 2E</xref>, the mean dissimilarity between recognized images (green) was significantly lower than between unrecognized images (red) at most of the post-stimulus time points (130–1900 ms, blue horizontal bar), confirming the prominent clustering effect of recognized images over time.</p><p>Together with our report of image category being only decodable in recognized trials but not unrecognized trials (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), the strong clustering effect of recognized images indicates that neural activity associated with recognized images contained sharper category information compared to unrecognized images despite the overall smaller dissimilarities among each other. To validate the compatibility between category decoding and RDM results, we ran an additional RSA-based analysis to test for the presence of category information in brain responses to recognized and unrecognized images, respectively. At each time point, we compared the representational structure within the recognized images (cells in the upper-left quadrant) and unrecognized images (bottom-right quadrant) with a category model which predicted smaller dissimilarities between images from the same category than between images from different categories (<xref ref-type="fig" rid="fig2">Figure 2F</xref>, right). This yielded a time course of Spearman’s rhos, indexing the strength of category information available in brain responses to recognized and unrecognized images, respectively. As expected, the result was qualitatively similar to category decoding: Significant category information occurred shortly before ~500 ms and decayed at ~1000 ms in recognized trials, whereas no category information was evident in unrecognized trials (<xref ref-type="fig" rid="fig2">Figure 2F</xref>, left).</p><p>Taken together, these findings indicate that successful recognition is associated with a shrinking representational space occupied by cortical activity at 130 ms, which may have a facilitating effect on the formation of object category representation that reaches significance at ~470 ms. In the analyses presented below, we bring in the 7T fMRI data to shed light on the spatiotemporal evolution of this and related neural effects.</p></sec><sec id="s2-4"><title>Resolving the spatiotemporal dynamics of recognition-related processes using model-driven MEG-fMRI fusion</title><p>The MEG decoding analysis presented above focused on the temporal evolution of recognition-related information at the whole-brain level, leaving open the question of how the related signals unfold in individual brain regions over time. To provide a deeper understanding of the involved neural processes, we applied a model-driven MEG-fMRI fusion approach (<xref ref-type="bibr" rid="bib33">Hebart et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Flounders et al., 2019</xref>), which provides a spatiotemporally resolved view of different types of neural processes in the brain. This section provides a brief overview of our methodology.</p><p>First, we constructed model RDMs that capture two different types of recognition-related processing: (i) Consistent with our MEG results (<xref ref-type="fig" rid="fig2">Figure 2C–E</xref>) and related prior findings (<xref ref-type="bibr" rid="bib64">Schurger et al., 2015</xref>; <xref ref-type="bibr" rid="bib8">Baria et al., 2017</xref>), the ‘recognition model’ (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, left) hypothesizes that successful recognition is associated with a reduction of neural variability, resulting in clustering of activity patterns associated with recognized images (manifesting as low dissimilarity among them). (ii) The ‘two-state model’ (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, right), on the contrary, hypothesizes that subjective recognition is associated with a bifurcation in neural signals into two brain states corresponding to recognized and unrecognized outcomes, respectively (manifesting as low dissimilarity between trials with the same recognition outcome and high dissimilarity between trials with different recognition outcomes).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Model-based MEG-fMRI fusion procedure.</title><p>(<bold>A</bold>) Model representational dissimilarity matrices (RDMs) used for model-driven MEG-fMRI fusion. Blue and yellow colors represent low and high dissimilarities, respectively. (<bold>B</bold>) Schematic for the commonality analysis. Commonality coefficient at each time point reflects the shared variance between an MEG RDM of a given time point, fMRI RDM of a given region of interest (ROI), and a model RDM reflecting the expected dissimilarity structure based on a given hypothesis. By repeating the procedure for every single MEG time point, we obtained a commonality time course for a given ROI and a given model RDM. (<bold>C</bold>) fMRI ROIs (from <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>). Left: Early visual cortex (EVC) and ventral temporal cortex (VTC) ROIs from a representative subject, defined using a functional localizer. Right: ROIs outside the visual cortices, in the task-positive network (TPN, orange-yellow colors) and default mode network (DMN, blue), were yielded from the recognized vs. unrecognized general linear model (GLM) contrast. AG: angular gyrus, aIns: anterior insula, aPCC: anterior posterior-cingulate cortex, IFJ: inferior frontal junction, IPS: intraparietal sulcus, MFG: middle frontal gyrus, mPFC: medial prefrontal cortex, OFC: orbitofrontal cortex, PCC: posterior cingulate cortex, SFG: superior frontal gyrus, STG: superior temporal gyrus.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Correlation between MEG and fMRI representational dissimilarity matrices (RDMs) derived from early visual cortex (EVC) and ventral temporal cortex (VTC) regions of interest (ROIs).</title><p>For this analysis, we computed Spearman’s rank correlation between MEG RDM derived from each time point and fMRI RDM derived from each ROI. For all panels: the horizontal blue bar indicates statistically significant clusters of positive correlations (p&lt;0.05, cluster-based label permutation test). The permutation test was conducted using an analogous procedure as for model-driven MEG-fMRI fusion (see Methods).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Same as <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, with task-positive network (TPN) regions of interest (ROIs).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Same as <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, with default mode network (DMN) regions of interest (ROIs).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig3-figsupp3-v2.tif"/></fig></fig-group><p>We then used RSA to combine time-varying MEG data with spatially localized fMRI data based on the correspondence between their representational structures, under the assumption that a correspondence between neural measurements by different recording modalities reflects the same neural generators (<xref ref-type="bibr" rid="bib41">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib17">Cichy and Oliva, 2020</xref>). Specifically, we estimated how much of the RDM correspondence between a given fMRI location and a given MEG time point could be attributed to each process of interest, as captured by the model RDMs (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Analogous to the MEG data, we computed RDMs (dimensions: 40×40) based on the voxel-wise BOLD activity patterns in 25 regions of interest (ROIs) (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Four ROIs in the early visual cortices (bilateral V1, V2, V3, and V4) and four bilateral regions in the ventral temporal cortices (VTC) selective to faces, houses, animals, and man-made objects were defined using independent retinotopy and object category functional localizers, respectively. ROIs in the frontoparietal cortices were defined based on differences in activation magnitudes between recognized and unrecognized trials (<xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>) and covered nine unilateral regions in the task-positive network (TPN); and eight unilateral or midline regions in the default mode network (DMN). As shown in <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>, TPN regions had higher activation in recognized trials while DMN regions had stronger deactivation in recognized trials.</p><p>Relating an RDM derived from a given fMRI location to RDMs derived from all MEG time points yielded a time course of MEG-fMRI fusion (quantified as the squared Spearman’s rho) for that ROI (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>). For each ROI, we then conducted a commonality analysis to determine the portion of the shared variance between its RDM (from fMRI) and the MEG RDM at each time point that can be accounted for by each model RDM. This procedure yielded a time course of commonality coefficients for each ROI and each model RDM, indicating the temporal evolution of recognition-related information specific to the modeled process in that ROI. V4 was excluded from the analysis because its RDM did not have significant positive correlations with MEG RDMs, leaving 24 ROIs in the analysis presented below.</p></sec><sec id="s2-5"><title>Early, concurrent onsets of recognition-related information across large-scale brain networks</title><p>The results of the model-driven MEG-fMRI fusion are displayed in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Significant effects related to the recognition model were evident in all ROIs (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, left). The earliest significance onset was at 110 ms concurrently in multiple VTC (face, animal, and house), TPN (bilateral MFG, bilateral IPS, and right IFJ), and DMN (bilateral AG, left SFG) regions, all leading to a transient peak at ~150 ms (<xref ref-type="fig" rid="fig4">Figure 4</xref>, blue and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s3">3</xref>). This initial peak was followed by a dip and a cascade of the reestablishment of recognition model effects in TPN (190 ms onward), DMN regions (202.5 ms onward), and VTC regions (220 ms onward). In many of the TPN and DMN regions, the recognition model-related effects stayed significant at most time points during the 2 s period investigated. In contrast, recognition model-related effects in the VTC predominantly existed in the first 1000 ms but were largely undetectable from 1000 ms onward, and were characterized by a transient, intermittent occurrence (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, left; <xref ref-type="fig" rid="fig4">Figure 4B</xref>, ‘face’ ROI; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), potentially driven by feedback activity from frontoparietal regions. Notably, recognition model-related effects in the EVC were late and transient; they became statistically significant at ~600 ms and disappeared by 1000 ms after stimulus onset.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model-based MEG-fMRI fusion results.</title><p>(<bold>A</bold>) Commonality analysis results for all regions of interest (ROIs) are shown by plotting the statistically significant time points (p&lt;0.05, cluster-based label permutation tests). Left: Results based on the recognition model. Right: Results based on the two-state model. (<bold>B</bold>) Commonality coefficient time course for recognition model (blue) and two-state model (orange) effects in five selected ROIs. The vertical dashed lines denote onset latencies, and the horizontal bars at the top indicate statistically significant time points (p&lt;0.05, cluster-based label permutation tests). Gray-shaded areas denote the MEG-fMRI fusion, equivalent to the total variance shared by MEG and fMRI representational dissimilarity matrices (RDMs). Results from all ROIs are shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s3">3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Model-based MEG-fMRI fusion results for early visual cortex (EVC) and ventral temporal cortex (VTC) regions of interest (ROIs).</title><p>Same format as in <xref ref-type="fig" rid="fig4">Figure 4B</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Model-based MEG-fMRI fusion results for all task-positive network (TPN) regions of interest (ROIs).</title><p>Same format as in <xref ref-type="fig" rid="fig4">Figure 4B</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Model-based MEG-fMRI fusion results for all default mode network (DMN) regions of interest (ROIs).</title><p>Same format as in <xref ref-type="fig" rid="fig4">Figure 4B</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig4-figsupp3-v2.tif"/></fig></fig-group><p>Interestingly, the early, two-wave dynamics of recognition model-related effects did not occur uniformly across all ROIs within the same network. For instance, recognition model-related effects emerged relatively late in OFC and bilateral insulae (225–237.5 ms, <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>) compared to other TPN regions. Moreover, these three regions did not show the sustained, elevated effects observed in other TPN regions. Instead, their dynamics were characterized by continuous fluctuations, which resembled those observed in the VTC regions, consistent with the idea that these regions might have an especially close dialogue with the VTC (<xref ref-type="bibr" rid="bib5">Bar et al., 2006</xref>; <xref ref-type="bibr" rid="bib36">Huang et al., 2021</xref>).</p><p>We next sought to identify neural dynamics driven by the two-state model. Like the recognition model, we found significant two-state model activity in all ROIs (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, right). Similarly, we also observed early, two-wave dynamics in multiple ROIs, with two peaks occurring shortly after the initial onset latency at 120 ms. There are, however, noticeable differences between the results derived from these two models. For instance, while EVC regions had by far the latest onset time for recognition model effects (600 ms), they were among the ROIs wherein the two-state model-related effects had the shortest onset latency (120 ms, <xref ref-type="fig" rid="fig4">Figure 4B</xref>, ‘V1’; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Moreover, in stark contrast to the transient nature of recognition model effects in EVC, the two-state model-related effects were sustained within the first second after stimulus onset. Overall, two-state model-related effects showed a more stable temporal dynamic than recognition model-related effects. This difference was most prominent in the VTC regions, but similar differences were also observable in the OFC and bilateral insulae.</p><p>In sum, both recognition and two-state models are associated with early, concurrent onset of significant processes across widespread ROIs, but the two models capture different aspects of recognition-related processing and are associated with distinct network-level patterns (e.g. two-state model accounts for EVC activity better while recognition model accounts for TPN activity better; see <xref ref-type="fig" rid="fig4">Figure 4B</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s3">3</xref>) and dynamic characteristics (e.g. two-state model-related effects are more sustained over time than recognition model-related effects). Below, we systematically characterize these network-level differences.</p></sec><sec id="s2-6"><title>Differential representational formats between functional brain networks</title><p>To assess network-level effects, we first examined the functional network association of each ROI, using well-known resting-state network (RSN) parcellation (<xref ref-type="bibr" rid="bib73">Yeo et al., 2011</xref>). Because our TPN ROIs encompassed regions outside the occipitotemporal visual cortices that exhibited heighted activation in recognized than unrecognized trials (<xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>), they belonged to several RSNs. Therefore, these ROIs were assigned to the frontoparietal control network (FPCN, including the aPCC, bilateral MFG, and OFC), dorsal attentional network (DAN, including the right IFJ and bilateral IPS), and salience network (SAL, including bilateral insulae), according to the RSN that accounted for the highest percentage of voxels in each ROI (<xref ref-type="table" rid="table1">Table 1</xref>). For DMN ROIs, we excluded bilateral STG because &gt;50% of their voxels were located in the somatosensory network based on Yeo et al. parcellation, leaving six ROIs in the DMN in the following analysis (bilateral AG, mPFC, PCC, and bilateral SFG). The extents of EVC and VTC were unchanged.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Distribution of each regions of interest (ROI’s) voxels across different functional brain networks (as defined in <xref ref-type="bibr" rid="bib73">Yeo et al., 2011</xref>), presented as percentage of voxels located in each network.</title><p>Each ROI was assigned to the brain network with the highest voxel count.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top">Visual</th><th align="left" valign="top">Somato</th><th align="left" valign="top">SAL</th><th align="left" valign="top">DAN</th><th align="left" valign="top">Limbic</th><th align="left" valign="top">FCPN</th><th align="left" valign="top" colspan="2">DMN</th></tr></thead><tbody><tr><td align="left" valign="top" colspan="8"><italic><underline>SAL</underline></italic></td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">L aInsula</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">65.17</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">24.38</td><td align="char" char="." valign="top" colspan="2">3.09</td></tr><tr><td align="left" valign="top">R aInsula</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">60.02</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">30.59</td><td align="char" char="." valign="top" colspan="2">0.07</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top" colspan="2"/></tr><tr><td align="left" valign="top" colspan="8"><italic><underline>DAN</underline></italic></td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">R IFJ</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">79.43</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">12.18</td><td align="char" char="." valign="top" colspan="2">0.00</td></tr><tr><td align="left" valign="top">L IPS</td><td align="char" char="." valign="top">4.09</td><td align="char" char="." valign="top">2.80</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">39.69</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">34.45</td><td align="char" char="." valign="top" colspan="2">4.10</td></tr><tr><td align="left" valign="top">R IPS</td><td align="char" char="." valign="top">6.91</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">55.98</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">20.71</td><td align="char" char="." valign="top" colspan="2">0.01</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top" colspan="2"/></tr><tr><td align="left" valign="top" colspan="8"><italic><underline>FCPN</underline></italic></td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">L MFG</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.48</td><td align="char" char="." valign="top">8.50</td><td align="char" char="." valign="top">18.05</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">54.07</td><td align="char" char="." valign="top" colspan="2">1.55</td></tr><tr><td align="left" valign="top">R MFG</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">6.97</td><td align="char" char="." valign="top">0.01</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">78.21</td><td align="char" char="." valign="top" colspan="2">0.16</td></tr><tr><td align="left" valign="top">R OFC</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">31.08</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">2.94</td><td align="char" char="." valign="top">45.46</td><td align="char" char="." valign="top" colspan="2">2.22</td></tr><tr><td align="left" valign="top">aPCC</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.29</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">36.77</td><td align="char" char="." valign="top" colspan="2">20.53</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top" colspan="2"/></tr><tr><td align="left" valign="top" colspan="8"><italic><underline>DMN</underline></italic></td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">L AG</td><td align="char" char="." valign="top">6.96</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">1.11</td><td align="char" char="." valign="top">17.67</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">1.65</td><td align="char" char="." valign="top" colspan="2">67.83</td></tr><tr><td align="left" valign="top">R AG</td><td align="char" char="." valign="top">0.43</td><td align="char" char="." valign="top">8.78</td><td align="char" char="." valign="top">7.06</td><td align="char" char="." valign="top">15.91</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.20</td><td align="char" char="." valign="top" colspan="2">64.95</td></tr><tr><td align="left" valign="top">PCC</td><td align="char" char="." valign="top">5.68</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.18</td><td align="char" char="." valign="top" colspan="2">88.36</td></tr><tr><td align="left" valign="top">mPFC</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.06</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.06</td><td align="char" char="." valign="top">0.33</td><td align="char" char="." valign="top" colspan="2">91.73</td></tr><tr><td align="left" valign="top">L SFG</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">2.16</td><td align="char" char="." valign="top" colspan="2">80.88</td></tr><tr><td align="left" valign="top">R SFG</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.43</td><td align="char" char="." valign="top">16.62</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">21.78</td><td align="char" char="." valign="top" colspan="2">36.95</td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top" colspan="2"/></tr><tr><td align="left" valign="top" colspan="8"><italic>Somatosensory/motor network (excluded from the analysis</italic>)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">L STG</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">45.35</td><td align="char" char="." valign="top">7.34</td><td align="char" char="." valign="top">0.76</td><td align="char" char="." valign="top">0.35</td><td align="char" char="." valign="top">3.63</td><td align="char" char="." valign="top" colspan="2">38.46</td></tr><tr><td align="left" valign="top">R STG</td><td align="char" char="." valign="top">0.10</td><td align="char" char="." valign="top">69.18</td><td align="char" char="." valign="top">5.35</td><td align="char" char="." valign="top">1.77</td><td align="char" char="." valign="top">0.00</td><td align="char" char="." valign="top">1.51</td><td align="char" char="." valign="top" colspan="2">17.58</td></tr></tbody></table></table-wrap><p>Visual inspection of commonality coefficient time courses (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s3">3</xref>) suggests that functional networks differed in the predominance of model-related effects. We thus estimated the explanatory powers of each model RDM by computing how much of the shared variance between MEG and fMRI RDMs it can explain. This was conducted for each ROI at each time point in the post-stimulus epoch. We were further interested in how the explanatory power of RDM models changed as a function of time. To this end, we split the explanatory power estimates into an early (0–1000 ms following stimulus onset) and a late (1000–2000 ms) time window, reasoning that perceptual processing had relatively strong influences on brain responses in the early stage whereas those in the late stage were mainly involved in post-perceptual processing.</p><p>To quantify the effect of model (recognition vs. two-state) and time window (early vs. late) on the amount of explained MEG-fMRI covariance, we conducted a two-way mixed-design analysis of variance (ANOVA). This analysis revealed a significant main effect of model in all brain networks, with opposite effects between ventral visual and higher-order frontoparietal networks (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The two-state model was the superior model for explaining neural dynamics in EVC (<italic>F</italic><sub>1,799</sub> = 1021.34, p=5.14 × 10<sup>–145</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.32; <xref ref-type="fig" rid="fig5">Figure 5A</xref>) and VTC (<italic>F</italic><sub>1,799</sub> = 892.66, p=2.79 × 10<sup>–132</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.31; <xref ref-type="fig" rid="fig5">Figure 5B</xref>), while recognition model was superior for SAL (<italic>F</italic><sub>1,799</sub> = 12.33, p=4.72 × 10<sup>–4</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.01; <xref ref-type="fig" rid="fig5">Figure 5C</xref>), DAN (<italic>F</italic><sub>1,799</sub> = 5104.20, p=0, <italic>η</italic><sup>2</sup><sub>G</sub>=0.77; <xref ref-type="fig" rid="fig5">Figure 5D</xref>), FPCN (<italic>F</italic><sub>1,799</sub> = 740.60, p=6.42 × 10<sup>–116</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.36; <xref ref-type="fig" rid="fig5">Figure 5E</xref>), and DMN (<italic>F</italic><sub>1,799</sub> = 34.89, p=5.17 × 10<sup>–9</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.03; <xref ref-type="fig" rid="fig5">Figure 5F</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Percentage of shared variance between MEG and fMRI representational dissimilarity matrices (RDMs) explained by the recognition model (blue) and two-stage model (orange) in individual brain networks and two different post-stimulus time windows (early: 0–1000 ms; late: 1000–2000 ms).</title><p>In all panels: Line plots depict the median percentage of explained variance across all time points in the window. The dashed lines in the violin plots represent 25th percentile, median, and 75th percentile. Asterisks indicate significant differences between time windows (early vs. late, two-sided Mann-Whitney test) or models (recognition vs. two-state, two-sided Wilcoxon signed-rank test) at p&lt;0.001, Bonferroni-corrected across all pairwise tests. n.s.: not significant at a level of p&lt;0.05, Bonferroni-corrected. EVC: early visual cortex, VTC: ventral temporal cortex, SAL: salience network, DAN: dorsal attentional network, FPCN: frontoparietal control network, DMN: default mode network. Results for individual regions of interest (ROIs) are presented in <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Percentage of shared variance between MEG and fMRI representational dissimilarity matrices (RDMs) explained by the recognition model (blue) or the two-stage model (orange) in individual brain networks computed using 200 ms sliding windows in the post-stimulus period.</title><p>EVC: early visual cortices, VTC: ventral temporal cortices, SAL: salience network, DAN: dorsal attentional network, FCPN: frontoparietal control network, DMN: default mode network. In all panels, line plots depict the median percentage of explained variance across all time points within the window. Error bars indicate the interquartile range. *: p&lt;0.05, **: p&lt;0.01, ***: p&lt;0.001, all FDR-corrected for multiple comparisons across time windows.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Same as in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, with 100 ms sliding time windows.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Each model representational dissimilarity matrix (RDM’s) explanatory power in individual regions of interest (ROIs), grouped according to their brain network membership and the time window after stimulus onset (early vs. late).</title><p>In all panels, line plots depict the median percentage of explained variance across all time points in the window. Results from the recognition model are shown as solid lines; those from the two-state model are shown as dashed lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig5-figsupp3-v2.tif"/></fig></fig-group><p>There was a significant interaction effect between model and time window in all networks except DAN. In EVC, the significant interaction was driven by a sharp decrease in two-state model’s explanatory power over time (<italic>F</italic><sub>1,799</sub> = 155.62, p=9.27 × 10<sup>–33</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.07). In the remaining networks, the interaction was driven by an overall <italic>increase</italic> in two-state model’s explanatory power and a decrease in recognition model’s explanatory power over time (VTC: <italic>F</italic><sub>1,799</sub>=286.46, p=3.78×10<sup>–55</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.13; SAL: <italic>F</italic><sub>1,799</sub>=11.35, p=7.90×10<sup>–4</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.01; FPCN: <italic>F</italic><sub>1,799</sub>=97.65, p=8.38×10<sup>–22</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.07; DMN: <italic>F</italic><sub>1,799</sub>=316.67, p=6.31×10<sup>–60</sup>, <italic>η</italic><sup>2</sup><sub>G</sub>=0.20). Interestingly, this interaction effect manifested as a temporal shift in the dominant model in the DMN, with the recognition model prevailing in the early time window (<xref ref-type="fig" rid="fig5">Figure 5F</xref>, Wilcoxon signed-rank test, <italic>N</italic>=801, <italic>W</italic>=6944, p=3.66×10<sup>–46</sup>) and the two-state model prevailing in the late time window (<italic>N</italic>=800, <italic>W</italic>=23,937, p=1.56×10<sup>–11</sup>), suggesting a flexible adaption in DMN’s functional profile to different stages of object recognition processing.</p><p>Furthermore, we repeated the same analysis using different choices of time ranges (with 100 ms and 200 ms sliding windows). As shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s2">2</xref>, the results remained qualitatively similar, providing additional empirical support for the robustness of our results.</p><p>Together, these results reveal differential recognition-related representational dynamics between ventral visual and frontoparietal regions: Neural dynamics in the EVC and VTC were dominated by the two-state model effects, while the recognition model effects predominated in all other networks. In addition, compared to other networks, the salience (SAL) network had a relatively small main effect of model. Considering that SAL is situated between ventral visual and frontoparietal regions within the cortical hierarchy (<xref ref-type="bibr" rid="bib49">Margulies et al., 2016</xref>), these results imply a gradual change in the representational format along the cortical hierarchy, similar to that observed previously in a different visual task (<xref ref-type="bibr" rid="bib27">González-García et al., 2018</xref>). Importantly, the overall high percentages of the explained MEG-fMRI variance suggest that our choices of model RDMs were well suited for explaining MEG-fMRI fusion.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In the present study, we sought to provide a spatiotemporally resolved understanding of neural activity underlying object recognition under high uncertainty/low visibility conditions. To this end, we applied a model-based MEG-fMRI fusion approach to 7T fMRI data and MEG data collected using a threshold-level visual recognition task. Our results show that recognition-related information emerged simultaneously across multiple brain networks at 110 ms, but the representation format differed significantly between ventral visual and frontoparietal regions. While neural activity in visual regions is best captured by a state-switch process whereby recognized and unrecognized trials exhibit distinct activity patterns, neural activity in higher-order frontoparietal networks (including salience, dorsal attention, frontoparietal control, and default mode networks) are best described by a variability reduction process whereby recognized trials occupied a smaller region of the neural state space, yet with sharper representation of image category information (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Together, these results reveal rich and heterogenous neural dynamics supporting object recognition under increased uncertainty. Below, we discuss the details and implications of these findings.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Schematic illustration of the changes in representational geometry across functional brain networks and time.</title><p>Purple, orange, and blue colors rendered on the brain indicate the ventral visual regions, task-positive network (TPN), and default mode network (DMN). Squares in the upper panel display two-dimensional representational spaces in the corresponding brain areas, respectively. Green and red circles correspond to the representations of individual recognized and unrecognized image exemplars, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-fig6-v2.tif"/></fig><p>One common finding shared by the commonality analyses using different model RDMs was the early, parallel onsets (at 110 ms for the recognition model and 120 ms for the two-stage model) of recognition-related processes across multiple frontoparietal and ventral visual regions. The early onset observed in the higher-order frontoparietal regions is in accordance with previous MEG and primate neurophysiology findings, suggesting long-range recurrent processing between higher-order associative and ventral visual regions at an early stage during object recognition under challenging viewing conditions (<xref ref-type="bibr" rid="bib5">Bar et al., 2006</xref>; <xref ref-type="bibr" rid="bib38">Kar and DiCarlo, 2021</xref>; <xref ref-type="bibr" rid="bib9">Bellet et al., 2022</xref>). The observation of parallel onsets across multiple cortical regions is somewhat surprising and requires a mechanistic explanation, but similar findings were previously reported in intracranial recordings in both primates and humans (<xref ref-type="bibr" rid="bib7">Barbeau et al., 2008</xref>; <xref ref-type="bibr" rid="bib68">Siegel et al., 2015</xref>; <xref ref-type="bibr" rid="bib62">Regev et al., 2018</xref>). For instance, consistent with our results, Barbeau et al. showed that face-related signals onset simultaneously in fusiform gyrus and inferior frontal gyrus at 110 ms. These findings contrast with models positing a simple feedforward processing of visual inputs from primary sensory to high-order associative cortices. Instead, the simultaneity hints at the possibility of parallel processing of visual inputs across multiple cortical regions. For example, it has been suggested that the early frontoparietal recognition-related signals are triggered by the fast magnocellular inputs via the dorsal visual stream or projected from the pulvinar or mediodorsal nucleus of the thalamus (<xref ref-type="bibr" rid="bib11">Bullier, 2001</xref>; <xref ref-type="bibr" rid="bib4">Bar, 2003</xref>; <xref ref-type="bibr" rid="bib43">Kveraga et al., 2007</xref>). The recognition-related signals in the ventral visual regions, on the contrary, are likely to be triggered by inputs via the parvocellular pathway along the ventral visual stream, which may be facilitated by feedback signals from dorsal stream regions (<xref ref-type="bibr" rid="bib5">Bar et al., 2006</xref>).</p><p>The finding of differential model effects across large-scale functional networks suggests that recognition-related information in these networks differs in its representational format. Recognition-related information in visual regions (including EVC and VTC) is best characterized by a two-state representational format whereby activity patterns associated with both recognized and unrecognized exemplars clustered in two separated, relatively constrained representational subspaces. In contrast, recognition-related information carried by higher-order frontoparietal networks (including SAL, DAN, FPCN, and DMN) was dominated by the recognition model-related effects. They are linked with a representational format whereby activity patterns associated with recognized exemplars were clustered within a confined representational subspace, while activity patterns associated with unrecognized exemplars are distributed across a larger subspace. Importantly, despite the more confined representational subspace for recognized trials, category-level information is only present in recognized but not recognized trials (as shown by the decoding analysis), suggesting that the variability reduction in recognized trials might serve to reduce noise and facilitate the emergence of information related to the recognized stimulus content.</p><p>Previous monkey and human work associated a reduced neural variability with cortical engagement in stimulus and task processing (<xref ref-type="bibr" rid="bib48">Luczak et al., 2009</xref>; <xref ref-type="bibr" rid="bib61">Ratcliff et al., 2009</xref>; <xref ref-type="bibr" rid="bib15">Churchland et al., 2010</xref>; <xref ref-type="bibr" rid="bib30">He, 2013</xref>; <xref ref-type="bibr" rid="bib31">He and Zempel, 2013</xref>; <xref ref-type="bibr" rid="bib64">Schurger et al., 2015</xref>; <xref ref-type="bibr" rid="bib1">Arazi et al., 2017</xref>). In line with this interpretation, our finding of reduced neural variability in higher-order frontoparietal regions during recognized trials suggests that successful recognition is linked with enhanced stimulus processing in these regions (consistent with the emergence of category-level information mentioned above). On the other hand, the high neural variability in unrecognized trials suggests that failed recognition may partially result from limited frontoparietal engagement, at least in the context of challenging viewing conditions. Importantly, the unbalanced stimulus processing between recognized and unrecognized trials were limited to higher-order frontoparietal regions; EVC and VTC neural dynamics revealed a symmetrical, two-state representational format.</p><p>The asymmetrical reduction in neural variability was particularly strong in the FPCN and DAN, explaining 80% and 98.4% of total shared variance between fMRI and MEG RDMs in the 0–1000 ms time window. These areas are sources of attentional and executive control (<xref ref-type="bibr" rid="bib53">Miller and Cohen, 2001</xref>; <xref ref-type="bibr" rid="bib18">Corbetta and Shulman, 2002</xref>; <xref ref-type="bibr" rid="bib21">Duncan, 2010</xref>; <xref ref-type="bibr" rid="bib59">Ptak, 2012</xref>; <xref ref-type="bibr" rid="bib12">Buschman and Kastner, 2015</xref>). Accordingly, it is possible that the enhanced FCPN and DAN engagement during recognized trials reflect attention-related modulations that support object recognition (<xref ref-type="bibr" rid="bib66">Serences et al., 2004</xref>; <xref ref-type="bibr" rid="bib3">Baldauf and Desimone, 2014</xref>). In light of the increasing evidence that FCPN and DAN regions are actively involved in object recognition processing (<xref ref-type="bibr" rid="bib40">Konen and Kastner, 2008</xref>; <xref ref-type="bibr" rid="bib24">Freud et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>; <xref ref-type="bibr" rid="bib2">Ayzenberg and Behrmann, 2022</xref>; <xref ref-type="bibr" rid="bib52">Mei et al., 2022</xref>), the enhanced FCPN and DAN engagement may be directly related to perceptual processing per se. For example, the early recognition-related responses in the FPCN and DAN may reflect an initial, fast analysis of noisy visual input in the dorsal visual stream which facilitates fine-grained analysis happening in the ventral stream (<xref ref-type="bibr" rid="bib11">Bullier, 2001</xref>; <xref ref-type="bibr" rid="bib4">Bar, 2003</xref>; <xref ref-type="bibr" rid="bib5">Bar et al., 2006</xref>). Alternatively, they may reflect the accumulation of sensory evidence preceding the rise of a particular perceptual experience (<xref ref-type="bibr" rid="bib26">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib39">Kelly and O’Connell, 2015</xref>). These interpretations are not mutually exclusive and consistent with our empirical finding that the onset of recognition-related activity according to both models (significance at 110–120 ms) is earlier than the onset of category-level information (initial peak at 290 ms, significance at 470 ms).</p><p>Using whole-brain MEG decoding, we found that object category-related information reached significance at 470 ms. This is markedly later than those reported in previous studies using high-contrast, clear object images, where category decoding typically peaked within the first 200 ms (<xref ref-type="bibr" rid="bib20">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="bib13">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib16">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="bib37">Isik et al., 2014</xref>). Our control task and analysis confirmed that this latency difference was driven by a difference in stimulus visibility/uncertainty: In a localizer task (with the same image set as used in the main task but presented at suprathreshold contrasts), we found object category decoding reached significance at 110 ms and peaked at 170 ms. These results are consistent with the idea that more time-consuming recurrent neural processing is needed for object recognition under high uncertainty/low visibility. This interpretation is also consistent with recent human electrophysiology findings showing that the latency for the emergence of object category information increases as the viewing duration becomes shorter (<xref ref-type="bibr" rid="bib54">Mohsenzadeh et al., 2018</xref>) or the visible part of an object shrinks (<xref ref-type="bibr" rid="bib71">Tang et al., 2018</xref>; <xref ref-type="bibr" rid="bib60">Rajaei et al., 2019</xref>) and fits with the relatively long behavioral responses (up to &gt;1 s) for the categorization of degraded visual images (<xref ref-type="bibr" rid="bib56">Philiastides and Sajda, 2007</xref>; <xref ref-type="bibr" rid="bib61">Ratcliff et al., 2009</xref>; <xref ref-type="bibr" rid="bib34">Hegdé and Kersten, 2010</xref>; <xref ref-type="bibr" rid="bib57">Philiastides et al., 2011</xref>).</p><p>Finally, our study adds to a growing body of evidence showing the involvement of DMN in visual perceptual processing (<xref ref-type="bibr" rid="bib27">González-García et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">González-García and He, 2021</xref>; <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>; <xref ref-type="bibr" rid="bib69">Smallwood et al., 2021</xref>). For instance, a previous study using the 7T fMRI dataset investigated herein reported that despite stronger deactivation in recognized trials, DMN regions encoded the content of recognition within its (deactivated) activity patterns (<xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>). Here, using stimulus-induced reduction in neural variability as an indicator of cortical engagement, we found that successful recognition is associated with an enhanced DMN engagement despite its deactivation, and that neural activity in the DMN dynamically shifts from recognition model-dominated effects (variability reduction in recognized trials) to two-state model-dominated effects (bifurcation between recognized and unrecognized trials). The shift toward the two-state representational structure in the late stage may indicate an enhanced intercommunication with other areas that were also dominated by the two-state model effects, such as the ventral visual regions. Speculatively, given DMN’s prominent role in both associative processing (<xref ref-type="bibr" rid="bib6">Bar et al., 2007</xref>; <xref ref-type="bibr" rid="bib70">Stawarczyk et al., 2021</xref>) and prior-guided perception (<xref ref-type="bibr" rid="bib27">González-García et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Flounders et al., 2019</xref>), this effect might reflect interactions between the current visual inputs and the observer’s prior knowledge that guide subsequent perceptual decisions and related associations.</p><p>In summary, under a challenging viewing condition, we found that object recognition-related neural processes emerge early (110–120 ms) and simultaneously across multiple ventral visual and frontoparietal regions despite the late emergence (470 ms) of category-related information. Importantly, brain regions differ in the temporal evolution and representational format of their neural dynamics, in a manner that is roughly organized according to large-scale functional networks. Thus, object recognition under uncertainty engages a bidirectional coordination between multiple cortical networks with diverse functional roles at an early processing stage.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Twenty-five volunteers (15 females, mean age 26 years, range 22–34 years) participated in the MEG experiment, approved by the Institutional Review Board of the National Institute of Neurological Disorders and Stroke (protocol #14N-0002). One participant did not complete the MEG experiment due to discomfort and was removed from further procedures, leaving 24 participants in the MEG dataset. A total of 38 volunteers (26 females, mean age 27 years, range 20–38 years) participated in the fMRI experiment, following the study protocol (s15-01323) approved by the Institutional Board Review of New York University School of Medicine. Ten volunteers were excluded due to not completing the task and three were removed from data analysis due to poor behavioral performance, leaving 25 volunteers in the fMRI dataset. All participants were right-handed, neurologically healthy, and had a normal or corrected-to-normal vision. All experiments were conducted in accordance with the Declaration of Helsinki and written informed consent was obtained from each participant. Both the MEG and fMRI datasets analyzed herein have been used in previous publications (<xref ref-type="bibr" rid="bib58">Podvalny et al., 2019</xref>; <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>).</p></sec><sec id="s4-2"><title>Experimental stimuli and procedure</title><p>The same stimuli set was used in both the MEG and fMRI experiments. The stimuli set consisted of five unique exemplars from four categories, namely face, house, man-made object, and animal. These images were selected from public domain labeled photographs or from Psychological Image Collection at Stirling (PICS, <ext-link ext-link-type="uri" xlink:href="http://pics.psych.stir.co.uk">http://pics.psych.stir.co.uk</ext-link>). The images were resized to 300×300 pixels and converted to grayscale. In addition, the pixel intensities were normalized by subtracting the mean and dividing by the standard deviation, and image was filtered with a two-dimensional Gaussian kernel with a standard deviation of 1.5 pixels and 7×7 pixels size. We additionally generated scrambled images by shuffling the two-dimensional Fourier transformed phase of one randomly chosen exemplar from each category (data from scrambled image trials were not used in the present study due to low trial count). This yielded a stimuli set consisting of 20 unique real images and 4 scrambled images in total. In both MEG and fMRI experiment, stimulus size was ~8 degrees in diameter. Prior to the main task, participants underwent an adaptive staircase procedure ‘QUEST’ to identify image contrast yielding a recognition rate of 50%. Detailed procedures are available in <xref ref-type="bibr" rid="bib58">Podvalny et al., 2019</xref>; <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>.</p><p>In the main task, each trial started with a central fixation cross on a gray background for a variable duration (MEG: 2–4 s, fMRI: 6–20 s). This was followed by image presentation at central fixation (with a visible fixation cross) for 66.7 ms, during which the image intensity gradually increased from 0.01 to the threshold contrast intensity. After a variable delay period (MEG: 2–4 s, fMRI: 4 or 6 s) wherein the screen returned to the initial gray background with a fixation cross at the center, participants were prompted to answer two questions. The first was a four-alternative-forced-choice task, wherein participants reported the object category of the presented image with a button press. Participants were instructed to make a genuine guess if they did not recognize the object. The stimulus response mapping, indicated by the order of words ‘face’/‘house’/‘object’/‘animal’ on the screen, was randomized across trials. The second question asked about participants’ recognition experience. They were instructed to report ‘yes’ if they saw a meaningful content, and ‘no’ if they saw nothing or random noise patterns. In the MEG experiment, participants had up to 3 s to indicate their response to each question and the next screen was presented whenever a response was given, while in the fMRI experiment each question was presented for a fixed 2 s period.</p><p>Both the MEG and fMRI main tasks consisted of 300 real image and 60 scrambled image trials. Each unique image was presented 15 times throughout the main task. In the MEG experiment, trials were randomized and split into 10 experimental runs. In the fMRI experiment, trials were split into 15 experimental runs, each with the entire stimulus set presented once in a random order.</p><p>Participants in both the fMRI and MEG experiments additionally completed a ‘localizer’ task. The localizer task in the fMRI experiment was described in detail in <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>, and the data were used for the definition of VTC (face, house, object, animal) ROIs used in that paper and in the current study (for detailed method about localizer data analysis, see <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>). Here, we describe in detail the methods for the localizer task in the MEG experiment, as the associated data have not been previously published. In the localizer task block, participant viewed the same set of 20 real images as used in the main task (scrambled images were not included), with the crucial difference being that the images were presented at the original high contrast. This allowed us to directly compare the neural dynamics of object recognition triggered by the same set of images at different visibility/uncertainty levels. The localizer block consisted of 300 trials, with each unique image presented 15 times in a randomized order. Each trial lasted 1 s and started with an image presentation of 66.7 ms, during which the image intensity gradually increased from 0.01 to 1 (the original contrast). Participants were instructed to perform a one-back memory task and press a button whenever they saw an image presented twice consecutively.</p></sec><sec id="s4-3"><title>MEG data acquisition and preprocessing</title><p>Continuous MEG signals were recorded at a sampling rate of 1200 Hz using a 275-channel whole-head MEG system (CTF, VSM MedTech). Three electrodes were dysfunctional and excluded, leaving 272 channels for further procedures. MEG data were preprocessed with the MNE toolbox, Python (<xref ref-type="bibr" rid="bib29">Gramfort et al., 2013</xref>). Independent component analysis (ICA) was performed on continuous data from each run to remove artifacts related to eye movements, blinks, and heartbeats. The ICA-cleaned MEG data were detrended, demeaned, band-pass filtered between DC and 35 Hz, and down-sampled to 400 Hz (for commonality analysis) or 100 Hz (for decoding analysis). We epoched continuous MEG data into 2.5 s trials from –500 ms to 2000 ms relative to stimulus onset and applied baseline correction for each sensor using the pre-stimulus time window.</p></sec><sec id="s4-4"><title>Multivariate pattern analysis of MEG data</title><p>Decoding on MEG data was performed using The Decoding Toolbox (<xref ref-type="bibr" rid="bib32">Hebart et al., 2014</xref>). For each participant, the decoding analysis was performed using linear SVM (in LIBSVM implementation, <xref ref-type="bibr" rid="bib14">Chang and Lin, 2011</xref>) with a fixed cost parameter of <italic>c</italic>=1. Data associated with scrambled images were excluded from the analysis, leaving data associated with 40 conditions (5 unique real exemplars×4 categories×2 recognition outcomes).</p><p>For each time point (from –500 ms to 2000 ms relative to stimulus onset, in 10 ms increments), preprocessed MEG data derived from every single trial were arranged as a 272-dimensional vector (corresponding to 272 sensor responses) and served as a sample in the classification models. To decode recognition outcomes from MEG signals at a given time point, we employed a fivefold cross-validation scheme. We pseudo-randomly divided data into five subsets, each containing approximately the same percentage of samples from each condition as the complete dataset. We iteratively trained SVM classifiers to discriminate between samples from different conditions using data from four folds as the training set and tested for their generalization performance on the samples from the remaining fold. To minimize the potential classifier bias due to the unequal distribution of conditions in the training set, we employed a resampling method. For each cross-validation iteration, we built up to 100 different classification models, each trained on a random subset of samples from the more frequent class matched to the sample size of the less frequent class in the training set. The label predicted by the majority of the models was then selected as the predicted label for a particular test sample. This training test process was repeated until all folds had been used as the test set once. We computed balanced decoding accuracy across all cross-validation folds to evaluate the prediction performance. Balanced decoding accuracy of 50% indicates no predictive power, whereas a score of 100% indicates perfect predictive power.</p><p>An analogous procedure was employed to decode object category in recognized and unrecognized trials, respectively. For each time point, we performed the aforementioned binary classification strategy on all six possible pairwise combinations of the four object categories. Decoding scores derived from all pairwise classifications were then averaged to yield an estimate of the overall decoding score across all pairwise classifications. For the object category decoding in the localizer task block, we employed a similar procedure, but without the resampling procedure described above because the number of samples was already balanced across the four categories (75 each).</p><p>The group-level statistical significance for the balanced decoding accuracy at each time point was estimated using a one-tailed Wilcoxon signed-rank test against chance level (50%). To control for multiple comparisons, we performed cluster-based inference (<xref ref-type="bibr" rid="bib50">Maris and Oostenveld, 2007</xref>), whereby a cluster was defined as contiguous time points showing significant group-level decoding accuracy at a p&lt;0.05, uncorrected level (i.e. cluster-defining threshold), and a cluster-level statistic was computed by taking the sum of the <italic>W</italic> statistics of the Wilcoxon signed-rank test across all time points in a given cluster. The statistical significance of a given cluster was determined using a sign permutation test that randomly flipped the sign of subjects’ data (i.e. decoding accuracy minus chance) with 50% probability, 5000 times (<xref ref-type="bibr" rid="bib55">Nichols and Holmes, 2002</xref>; <xref ref-type="bibr" rid="bib51">Martin Cichy et al., 2017</xref>; <xref ref-type="bibr" rid="bib33">Hebart et al., 2018</xref>). For each permutation sample, we computed the <italic>W</italic> statistic for each time point and took the largest of the cluster-level statistics, resulting in an empirical distribution of maximum cluster-level statistics. We reported a cluster in the original data as significant if its cluster-level statistic exceeded the 95th percentile of the empirical distribution of maximum cluster-level statistics (corresponding to p&lt;0.05, cluster-corrected).</p><p>To construct an RDM for each time point (from –500 to 2000 ms relative to stimulus onset, in 2.5 ms increments) and each participant, we averaged the whole-brain sensor-level activity patterns for each of the 40 conditions and computed 1-Pearson’s correlation between each condition pair as the dissimilarity measure. Dissimilarities were then assembled in a 40×40 matrix. Dissimilarities between recognized exemplars were stored in the upper-left quadrant, and those between unrecognized exemplars were stored in the bottom-right quadrant. Values in the bottom-left and upper-right quadrants represented dissimilarities between recognized and unrecognized exemplars. The resulting RDM was symmetric around the diagonal, with the diagonal being undefined. We averaged RDMs across participants (<italic>N</italic>=24), yielding one RDM for each time point. To facilitate the visualization of relations between conditions, the mean RDMs at selected time points were projected onto a two-dimensional plot using MDS (<xref ref-type="bibr" rid="bib42">Kruskal and Wish, 1978</xref>; <xref ref-type="bibr" rid="bib67">Shepard, 1980</xref>).</p><p>We constructed a category model RDM to test for category-related processing in recognized and unrecognized image trials, respectively. The category model RDM had a size of 20×20 cells, corresponding to the total number of image exemplars for a given recognition outcome. It predicted that activity patterns evoked by images from the same category would have relatively small dissimilarities with each other, while dissimilarities between activity patterns evoked by different categories would be relatively large. For each participant, we compared the category model RDM with the parts of their MEG RDMs representing recognized and unrecognized trials (the upper-left and bottom-right quadrants), respectively. That is, we computed the Spearman’s rank correlation coefficient (rho) between the model RDM and MEG RDM derived from each time point. Statistical significance at the group-level was determined using procedures analogous to those employed for the decoding analysis. Spearman’s rho significant above zero indicated the presence of category information.</p></sec><sec id="s4-5"><title>MRI data acquisition, preprocessing, and ROI definition</title><p>Detailed methods related to the fMRI dataset have been reported in <xref ref-type="bibr" rid="bib46">Levinson et al., 2021</xref>. Here, we briefly summarize the procedures relevant to the present work. MRI data were acquired using a Siemens 7T scanner equipped with a 32-channel NOVA coil. A T1-weighted structural image was acquired using an MPRAGE sequence (TR=3000 ms, TE=4.49 ms, 192 saggital slices, flip angle=6, FOV=256×256 mm<sup>2</sup>, 1.0 mm isotropic). Functional volumes covering the whole brain were obtained using T2* weighted echo planar imaging sequence (TR=2000 ms, TE=25 ms, FOV=192×192 mm<sup>2</sup>, flip angle=50, acceleration factor/GRAPPA=2, multi-band factor 2). Each volume consisted of 54 oblique slices with an in-plane resolution of 2×2 mm and a slice thickness of 2 mm with a distance factor of 10%. fMRI data collected from each participant were preprocessed using FSL package (<ext-link ext-link-type="uri" xlink:href="http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL">http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL</ext-link>). In short, they were realigned, corrected for slice timing, spatially smoothed with a 3 mm FWHM kernel, and registered to the T1 anatomical scan. Artifacts related to motion, arteries, or CSF pulsation were removed using ICA.</p><p>A retinotopy functional localizer was used to define EVC ROIs in each participant. For each hemisphere, field maps in V1, V2d, V2v, V3d, V3v, and V4 were identified, and the dorsal and ventral portions of V2 and V3 within each hemisphere were subsequently merged into a single ROI, respectively. For the present study, we further merged the left and right hemisphere for each early visual area, resulting in four EVC ROIs including the bilateral V1, V2, V3, and V4. EVC ROIs could not be defined in several individuals due to noisy data, leaving 23 subjects’ data in analyses involving V1–V3 and 22 subjects’ data in analyses involving V4. The four category-selective ROIs in the VTC (face, house, object, animal) were defined individually using a separate functional localizer: each ROI consisted of up to 500 most responsive voxels to one of the four object categories within occipitotemporal visual cortex (thresholded at <italic>Z</italic>&gt;2.3, masked with the conjunction of inferior lateral occipital cortex, occipital fusiform gyrus, posterior parahippocampal gyrus, temporal fusiform cortex, and temporal occipital fusiform cortex from the Harvard-Oxford brain atlas). In addition, a category-selective ROI was only defined if more than 100 voxels surpassed the threshold within the mask. This yielded face-selective ROIs in 20 subjects, house-selective ROI in 15 subjects, object-selective ROI in 18 subjects, and animal-selective ROI in 18 subjects. TPN and DMN ROIs were defined based on significant clusters extracted from the group-level statistical maps for the recognized vs. unrecognized general linear model (GLM) contrast (cluster-corrected p&lt;0.05). These clusters were transformed back to each individual’s native space; thus, TPN and DMN ROIs were defined for all 25 participants.</p></sec><sec id="s4-6"><title>fMRI multivariate pattern analysis</title><p>We constructed a 40×40 RDM for each ROI based on the BOLD activity pattern elicited by each exemplar in each recognition condition. These activity patterns were estimated using a first-level GLM implemented in FSL. For each experimental run, we defined one regressor for each image exemplar associated with either recognition outcome aligned to the stimulus onsets. An additional nuisance regressor, aligned to the onset of the first question, was added to account for motor-related activation. This resulted in a total of 49 regressors per run ((20 real exemplars+4 scrambled exemplars)×2 recognition outcomes+1 nuisance regressor), each convolved with a gamma-shaped hemodynamic response function. Note that each exemplar was only presented once in an experimental run and could be only associated with a specific recognition outcome. As a consequence, only 25 regressors contained non-zero values in each run. The same GLM was estimated for all experimental runs, yielding trial-wise beta parameter estimates for activity patterns elicited by each of the presented stimuli in each recognition condition. As for the MEG RDM, we discarded data related to the scrambled images, leaving data associated with 40 conditions (20 real exemplars×2 recognition outcomes). We averaged the activity patterns across runs for each condition and then computed the dissimilarities between all condition pairs, before entering them into the RDM. Lastly, RDMs were averaged across participants, yielding a mean 40×40 RDM for each ROI.</p></sec><sec id="s4-7"><title>Model-driven MEG-fMRI fusion</title><p>To provide a spatiotemporally resolved view of object recognition processing under uncertainty, we performed a model-driven MEG-fMRI fusion based on RSA (<xref ref-type="bibr" rid="bib33">Hebart et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Flounders et al., 2019</xref>). First, we applied cross-modal RSA to combine the MEG and fMRI data acquired from independent participant groups. For a given MEG time point and a given fMRI ROI, we extracted the lower triangles of the corresponding 40×40 RDMs, respectively, and converted them to dissimilarity vectors. We then calculated the squared Spearman’ rank correlation coefficient between the MEG and fMRI dissimilarity vectors, which is equivalent to the variance shared by MEG and fMRI RDMs. Repeating this procedure for all MEG time points and fMRI ROIs, we obtained a time course of MEG-fMRI fusion that indexed the representational similarity between whole-brain MEG data and ROI-specific fMRI data. Since MEG and fMRI data were collected from different groups of participants, we conducted the described procedure using the group-average RDMs from each recording modality as the best estimate of the true RDM (<xref ref-type="bibr" rid="bib33">Hebart et al., 2018</xref>).</p><p>To isolate the effects related to a neural process of interest in the MEG-fMRI fusion, we constructed model RDMs that reflected the expected dissimilarity pattern related to a neural process of interest. The contribution of the neural process of interest to the MEG-fMRI fusion at a given time point and ROI can be estimated using commonality analysis (<xref ref-type="bibr" rid="bib65">Seibold and McPHEE, 1979</xref>; <xref ref-type="bibr" rid="bib33">Hebart et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Flounders et al., 2019</xref>). More specifically, a commonality coefficient <italic>C</italic> yielded from the analysis informed about how much of the shared variance between the MEG RDM at a given time point and the fMRI RDM from a given ROI can be attributed to the model RDM. Formally, the commonality coefficient at a given time point and a given location is calculated as follows:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>We applied a cluster-based label permutation test to determine the statistical significance of commonality coefficients (<xref ref-type="bibr" rid="bib33">Hebart et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Flounders et al., 2019</xref>). For each of the 5000 permutations, we randomly shuffled the labels of MEG RDM, computed the commonality coefficient time course for a given model, and extracted the maximum cluster-level statistic. The cluster-defining threshold at each time point was set to the 95th percentile of the null distribution yielded from the permutation test (equivalent to p&lt;0.05, one-tailed). Analogous to the statistical testing for MEG decoding, a cluster in the original data was considered significant if its cluster-level statistic exceeded the 95th percentile of the null distribution of maximum cluster-level statistics (corresponding to p&lt;0.05 cluster-corrected).</p></sec><sec id="s4-8"><title>Analysis on RDM models’ explanatory power</title><p>Model RDM’s explanatory power at a given time point was estimated by commonality coefficient divided by the shared variance between MEG and fMRI RDM, which quantifies the percentage of the shared variance between MEG and fMRI RDM that can be accounted for by the model. The explanatory power estimate was set to zero if the commonality coefficient was negative. The estimation was conducted for all 801 time points between 0 and 2000 ms relative to stimulus onset, yielding a time course of percentages of explained MEG-fMRI covariance per ROI. The time course was split into two halves, with the first half corresponding to 0–1000 ms after stimulus onset and the second half corresponding to 1001–2000 ms after stimulus onset.</p><p>To estimate each model’s effect at the network level, we averaged time courses of percentages of explained MEG-fMRI covariance across ROIs within each network. For each network, we conducted a two-by-two mixed-design ANOVA, with model (recognition model vs. two-state model) as the repeated-measures factor and time (early vs. late) as the between-group factor, and individual data points being percentages of explained MEG-fMRI covariance at each time point. Post hoc tests between models were conducted using a two-sided Wilcoxon signed-rank test, while post hoc tests between times were carried out using a two-sided Mann-Whitney test. Bonferroni correction was applied to control for multiple comparisons across post hoc tests. We reported statistical significance at a p&lt;0.05, Bonferroni-corrected level.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Data collection procedures followed protocols approved by the institutional review boards of the intramural research program of NINDS/NIH (protocol #14 N-0002) and NYU Grossman School of Medicine (protocol s15-01323). All participants provided written informed consent.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-84797-mdarchecklist1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The analysis code, data and code to reproduce all figures can be downloaded at <ext-link ext-link-type="uri" xlink:href="https://github.com/BiyuHeLab/eLife_Wu2023">https://github.com/BiyuHeLab/eLife_Wu2023</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib10">BiyuHeLab, 2023</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by an NIH grant to BJH (R01EY032085). The authors would like to thank Richard Hardstone and Max Levinson for useful discussions.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arazi</surname><given-names>A</given-names></name><name><surname>Censor</surname><given-names>N</given-names></name><name><surname>Dinstein</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural variability quenching predicts individual perceptual abilities</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>97</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1671-16.2016</pub-id><pub-id pub-id-type="pmid">28053033</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The dorsal visual pathway represents object-centered spatial relations for object recognition</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>4693</fpage><lpage>4710</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2257-21.2022</pub-id><pub-id pub-id-type="pmid">35508386</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldauf</surname><given-names>D</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of object-based attention</article-title><source>Science</source><volume>344</volume><fpage>424</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1126/science.1247003</pub-id><pub-id pub-id-type="pmid">24763592</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A cortical mechanism for triggering top-down Facilitation in visual object recognition</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>600</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1162/089892903321662976</pub-id><pub-id pub-id-type="pmid">12803970</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name><name><surname>Kassam</surname><given-names>KS</given-names></name><name><surname>Ghuman</surname><given-names>AS</given-names></name><name><surname>Boshyan</surname><given-names>J</given-names></name><name><surname>Schmid</surname><given-names>AM</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Hämäläinen</surname><given-names>MS</given-names></name><name><surname>Marinkovic</surname><given-names>K</given-names></name><name><surname>Schacter</surname><given-names>DL</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Top-down Facilitation of visual recognition</article-title><source>PNAS</source><volume>103</volume><fpage>449</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507062103</pub-id><pub-id pub-id-type="pmid">16407167</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name><name><surname>Aminoff</surname><given-names>E</given-names></name><name><surname>Mason</surname><given-names>M</given-names></name><name><surname>Fenske</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The units of thought</article-title><source>Hippocampus</source><volume>17</volume><fpage>420</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1002/hipo.20287</pub-id><pub-id pub-id-type="pmid">17455334</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbeau</surname><given-names>EJ</given-names></name><name><surname>Taylor</surname><given-names>MJ</given-names></name><name><surname>Regis</surname><given-names>J</given-names></name><name><surname>Marquis</surname><given-names>P</given-names></name><name><surname>Chauvel</surname><given-names>P</given-names></name><name><surname>Liégeois-Chauvel</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatio temporal Dynamics of face recognition</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>997</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm140</pub-id><pub-id pub-id-type="pmid">17716990</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baria</surname><given-names>AT</given-names></name><name><surname>Maniscalco</surname><given-names>B</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Initial-state-dependent, robust, transient neural Dynamics Encode conscious visual perception</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005806</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005806</pub-id><pub-id pub-id-type="pmid">29176808</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellet</surname><given-names>J</given-names></name><name><surname>Gay</surname><given-names>M</given-names></name><name><surname>Dwarakanath</surname><given-names>A</given-names></name><name><surname>Jarraya</surname><given-names>B</given-names></name><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Panagiotaropoulos</surname><given-names>TI</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Decoding rapidly presented visual stimuli from Prefrontal ensembles without report nor post-perceptual processing</article-title><source>Neuroscience of Consciousness</source><volume>2022</volume><elocation-id>niac005</elocation-id><pub-id pub-id-type="doi">10.1093/nc/niac005</pub-id><pub-id pub-id-type="pmid">35223085</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="software"><person-group person-group-type="author"><collab>BiyuHeLab</collab></person-group><year iso-8601-date="2023">2023</year><data-title>Elife_Wu2023</data-title><version designator="swh:1:rev:0917e4ff6bc59708a04e235c4f22af3c720bed3d">swh:1:rev:0917e4ff6bc59708a04e235c4f22af3c720bed3d</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7abe03c7604930a9965c7107ecc78c8b8a775dfb;origin=https://github.com/BiyuHeLab/eLife_Wu2023;visit=swh:1:snp:13530c6762727b03b955178a9e583c1005c54e37;anchor=swh:1:rev:0917e4ff6bc59708a04e235c4f22af3c720bed3d">https://archive.softwareheritage.org/swh:1:dir:7abe03c7604930a9965c7107ecc78c8b8a775dfb;origin=https://github.com/BiyuHeLab/eLife_Wu2023;visit=swh:1:snp:13530c6762727b03b955178a9e583c1005c54e37;anchor=swh:1:rev:0917e4ff6bc59708a04e235c4f22af3c720bed3d</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullier</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Integrated model of visual processing</article-title><source>Brain Research. Brain Research Reviews</source><volume>36</volume><fpage>96</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/s0165-0173(01)00085-6</pub-id><pub-id pub-id-type="pmid">11690606</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>From behavior to neural Dynamics: an integrated theory of attention</article-title><source>Neuron</source><volume>88</volume><fpage>127</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.017</pub-id><pub-id pub-id-type="pmid">26447577</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational dynamics of object vision: The first 1000 MS</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.1167/13.10.1</pub-id><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>CC</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: A library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Sugrue</surname><given-names>LP</given-names></name><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Clark</surname><given-names>AM</given-names></name><name><surname>Hosseini</surname><given-names>P</given-names></name><name><surname>Scott</surname><given-names>BB</given-names></name><name><surname>Bradley</surname><given-names>DC</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Armstrong</surname><given-names>KM</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Chang</surname><given-names>SW</given-names></name><name><surname>Snyder</surname><given-names>LH</given-names></name><name><surname>Lisberger</surname><given-names>SG</given-names></name><name><surname>Priebe</surname><given-names>NJ</given-names></name><name><surname>Finn</surname><given-names>IM</given-names></name><name><surname>Ferster</surname><given-names>D</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Stimulus onset Quenches neural variability: a widespread cortical phenomenon</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>369</fpage><lpage>378</lpage><pub-id pub-id-type="doi">10.1038/nn.2501</pub-id><pub-id pub-id-type="pmid">20173745</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A M/EEG-fMRI fusion Primer: resolving human brain responses in space and time</article-title><source>Neuron</source><volume>107</volume><fpage>772</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.001</pub-id><pub-id pub-id-type="pmid">32721379</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>3</volume><fpage>201</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/nrn755</pub-id><pub-id pub-id-type="pmid">11994752</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Changeux</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Experimental and theoretical approaches to conscious processing</article-title><source>Neuron</source><volume>70</volume><fpage>200</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.03.018</pub-id><pub-id pub-id-type="pmid">21521609</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The multiple-demand (MD) system of the Primate brain: mental programs for intelligent behaviour</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>172</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.004</pub-id><pub-id pub-id-type="pmid">20171926</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Filimon</surname><given-names>F</given-names></name><name><surname>Philiastides</surname><given-names>MG</given-names></name><name><surname>Nelson</surname><given-names>JD</given-names></name><name><surname>Kloosterman</surname><given-names>NA</given-names></name><name><surname>Heekeren</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>How embodied is perceptual decision making? evidence for separate processing of perceptual and motor decisions</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>2121</fpage><lpage>2136</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2334-12.2013</pub-id><pub-id pub-id-type="pmid">23365248</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flounders</surname><given-names>MW</given-names></name><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>Hardstone</surname><given-names>R</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural Dynamics of visual ambiguity resolution by perceptual prior</article-title><source>eLife</source><volume>8</volume><elocation-id>e41861</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.41861</pub-id><pub-id pub-id-type="pmid">30843519</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freud</surname><given-names>E</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The large-scale organization of shape processing in the ventral and dorsal pathways</article-title><source>eLife</source><volume>6</volume><elocation-id>e34464</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34464</pub-id><pub-id pub-id-type="pmid">29260709</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fyall</surname><given-names>AM</given-names></name><name><surname>El-Shamayleh</surname><given-names>Y</given-names></name><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Pasupathy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic representation of partially occluded objects in Primate Prefrontal and visual cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e25784</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.25784</pub-id><pub-id pub-id-type="pmid">28925354</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>Flounders</surname><given-names>MW</given-names></name><name><surname>Chang</surname><given-names>R</given-names></name><name><surname>Baria</surname><given-names>AT</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Content-specific activity in Frontoparietal and default-mode networks during prior-guided visual perception</article-title><source>eLife</source><volume>7</volume><elocation-id>e36068</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36068</pub-id><pub-id pub-id-type="pmid">30063006</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A gradient of sharpening effects by perceptual prior across the human cortical hierarchy</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>167</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2023-20.2020</pub-id><pub-id pub-id-type="pmid">33208472</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Goj</surname><given-names>R</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Brooks</surname><given-names>T</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MEG and EEG data analysis with MNE-python</article-title><source>Frontiers in Neuroscience</source><volume>7</volume><elocation-id>267</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id><pub-id pub-id-type="pmid">24431986</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Spontaneous and task-evoked brain activity negatively interact</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>4672</fpage><lpage>4682</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2922-12.2013</pub-id><pub-id pub-id-type="pmid">23486941</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>BJ</given-names></name><name><surname>Zempel</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Average is optimal: an inverted-U relationship between trial-to-trial brain activity and behavioral performance</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003348</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003348</pub-id><pub-id pub-id-type="pmid">24244146</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The decoding Toolbox (TDT): a Versatile software package for multivariate analyses of functional imaging data</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00088</pub-id><pub-id pub-id-type="pmid">25610393</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Harel</surname><given-names>A</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The representational Dynamics of task and object processing in humans</article-title><source>eLife</source><volume>7</volume><elocation-id>e32816</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id><pub-id pub-id-type="pmid">29384473</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hegdé</surname><given-names>J</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A link between visual Disambiguation and visual memory</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>15124</fpage><lpage>15133</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4415-09.2010</pub-id><pub-id pub-id-type="pmid">21068318</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hesselmann</surname><given-names>G</given-names></name><name><surname>Hebart</surname><given-names>M</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Differential BOLD activity associated with subjective and objective reports during &quot;Blindsight&quot; in normal observers</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>12936</fpage><lpage>12944</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1556-11.2011</pub-id><pub-id pub-id-type="pmid">21900572</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Tarnal</surname><given-names>V</given-names></name><name><surname>Vlisides</surname><given-names>PE</given-names></name><name><surname>Janke</surname><given-names>EL</given-names></name><name><surname>McKinney</surname><given-names>AM</given-names></name><name><surname>Picton</surname><given-names>P</given-names></name><name><surname>Mashour</surname><given-names>GA</given-names></name><name><surname>Hudetz</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Anterior insula regulates brain network transitions that gate conscious access</article-title><source>Cell Reports</source><volume>35</volume><elocation-id>109081</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109081</pub-id><pub-id pub-id-type="pmid">33951427</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The Dynamics of invariant object recognition in the human visual system</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1152/jn.00394.2013</pub-id><pub-id pub-id-type="pmid">24089402</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fast recurrent processing via ventrolateral Prefrontal cortex is needed by the Primate ventral stream for robust core visual object recognition</article-title><source>Neuron</source><volume>109</volume><fpage>164</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.035</pub-id><pub-id pub-id-type="pmid">33080226</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The neural processes underlying perceptual decision making in humans: recent progress and future directions</article-title><source>Journal of Physiology, Paris</source><volume>109</volume><fpage>27</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1016/j.jphysparis.2014.08.003</pub-id><pub-id pub-id-type="pmid">25204272</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konen</surname><given-names>CS</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Two Hierarchically organized neural systems for object information in human visual cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>224</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1038/nn2036</pub-id><pub-id pub-id-type="pmid">18193041</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems Neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kruskal</surname><given-names>JB</given-names></name><name><surname>Wish</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>Multidimensional Scaling</source><publisher-name>SAGE Publications</publisher-name><pub-id pub-id-type="doi">10.4135/9781412985130</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kveraga</surname><given-names>K</given-names></name><name><surname>Boshyan</surname><given-names>J</given-names></name><name><surname>Bar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Magnocellular projections as the trigger of top-down Facilitation in recognition</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>13232</fpage><lpage>13240</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3481-07.2007</pub-id><pub-id pub-id-type="pmid">18045917</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VAF</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The distinct modes of vision offered by Feedforward and recurrent processing</article-title><source>Trends in Neurosciences</source><volume>23</volume><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/s0166-2236(00)01657-x</pub-id><pub-id pub-id-type="pmid">11074267</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>HC</given-names></name><name><surname>Passingham</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Relative Blindsight in normal observers and the neural correlate of visual consciousness</article-title><source>PNAS</source><volume>103</volume><fpage>18763</fpage><lpage>18768</lpage><pub-id pub-id-type="doi">10.1073/pnas.0607716103</pub-id><pub-id pub-id-type="pmid">17124173</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levinson</surname><given-names>M</given-names></name><name><surname>Podvalny</surname><given-names>E</given-names></name><name><surname>Baete</surname><given-names>SH</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cortical and subcortical signatures of conscious object recognition</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2930</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-23266-x</pub-id><pub-id pub-id-type="pmid">34006884</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Hill</surname><given-names>Z</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spatiotemporal dissociation of brain activity underlying subjective awareness, objective performance and confidence</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4382</fpage><lpage>4395</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1820-13.2014</pub-id><pub-id pub-id-type="pmid">24647958</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luczak</surname><given-names>A</given-names></name><name><surname>Barthó</surname><given-names>P</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spontaneous events outline the realm of possible sensory responses in neocortical populations</article-title><source>Neuron</source><volume>62</volume><fpage>413</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.03.014</pub-id><pub-id pub-id-type="pmid">19447096</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margulies</surname><given-names>DS</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Goulas</surname><given-names>A</given-names></name><name><surname>Falkiewicz</surname><given-names>M</given-names></name><name><surname>Huntenburg</surname><given-names>JM</given-names></name><name><surname>Langs</surname><given-names>G</given-names></name><name><surname>Bezgin</surname><given-names>G</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Castellanos</surname><given-names>FX</given-names></name><name><surname>Petrides</surname><given-names>M</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Situating the default-mode network along a principal gradient of Macroscale cortical organization</article-title><source>PNAS</source><volume>113</volume><fpage>12574</fpage><lpage>12579</lpage><pub-id pub-id-type="doi">10.1073/pnas.1608282113</pub-id><pub-id pub-id-type="pmid">27791099</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG-and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin Cichy</surname><given-names>R</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamics of scene representations in the human brain revealed by Magnetoencephalography and deep neural networks</article-title><source>NeuroImage</source><volume>153</volume><fpage>346</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.03.063</pub-id><pub-id pub-id-type="pmid">27039703</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mei</surname><given-names>N</given-names></name><name><surname>Santana</surname><given-names>R</given-names></name><name><surname>Soto</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Informative neural representations of unseen contents during higher-order processing in human brains and deep artificial networks</article-title><source>Nature Human Behaviour</source><volume>6</volume><elocation-id>743</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-022-01362-2</pub-id><pub-id pub-id-type="pmid">35484210</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrative theory of Prefrontal cortex function</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id><pub-id pub-id-type="pmid">11283309</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Qin</surname><given-names>S</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ultra-rapid serial visual presentation reveals Dynamics of Feedforward and feedback processes in the ventral visual pathway</article-title><source>eLife</source><volume>7</volume><elocation-id>e36329</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36329</pub-id><pub-id pub-id-type="pmid">29927384</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Nonparametric Permutation tests for functional neuroimaging: a Primer with examples</article-title><source>Human Brain Mapping</source><volume>15</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id><pub-id pub-id-type="pmid">11747097</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Philiastides</surname><given-names>MG</given-names></name><name><surname>Sajda</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>EEG-informed fMRI reveals Spatiotemporal characteristics of perceptual decision making</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>13082</fpage><lpage>13091</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3540-07.2007</pub-id><pub-id pub-id-type="pmid">18045902</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Philiastides</surname><given-names>MG</given-names></name><name><surname>Auksztulewicz</surname><given-names>R</given-names></name><name><surname>Heekeren</surname><given-names>HR</given-names></name><name><surname>Blankenburg</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Causal role of Dorsolateral Prefrontal cortex in human perceptual decision making</article-title><source>Current Biology</source><volume>21</volume><fpage>980</fpage><lpage>983</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.04.034</pub-id><pub-id pub-id-type="pmid">21620706</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Podvalny</surname><given-names>E</given-names></name><name><surname>Flounders</surname><given-names>MW</given-names></name><name><surname>King</surname><given-names>LE</given-names></name><name><surname>Holroyd</surname><given-names>T</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A dual role of Prestimulus spontaneous neural activity in visual object recognition</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>3910</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-11877-4</pub-id><pub-id pub-id-type="pmid">31477706</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ptak</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The Frontoparietal attention network of the human brain: action, Saliency, and a priority map of the environment</article-title><source>The Neuroscientist</source><volume>18</volume><fpage>502</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1177/1073858411409051</pub-id><pub-id pub-id-type="pmid">21636849</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajaei</surname><given-names>K</given-names></name><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Ebrahimpour</surname><given-names>R</given-names></name><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Beyond core object recognition: recurrent processes account for object recognition under occlusion</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007001</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007001</pub-id><pub-id pub-id-type="pmid">31091234</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>Philiastides</surname><given-names>MG</given-names></name><name><surname>Sajda</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Quality of evidence for perceptual decision making is indexed by trial-to-trial variability of the EEG</article-title><source>PNAS</source><volume>106</volume><fpage>6539</fpage><lpage>6544</lpage><pub-id pub-id-type="doi">10.1073/pnas.0812589106</pub-id><pub-id pub-id-type="pmid">19342495</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Regev</surname><given-names>TI</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Gerber</surname><given-names>EM</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Deouell</surname><given-names>LY</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Human posterior Parietal cortex responds to visual stimuli as early as Peristriate occipital cortex</article-title><source>The European Journal of Neuroscience</source><volume>48</volume><fpage>3567</fpage><lpage>3582</lpage><pub-id pub-id-type="doi">10.1111/ejn.14164</pub-id><pub-id pub-id-type="pmid">30240547</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schurger</surname><given-names>A</given-names></name><name><surname>Sarigiannidis</surname><given-names>I</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Sitt</surname><given-names>JD</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical activity is more stable when sensory stimuli are consciously perceived</article-title><source>PNAS</source><volume>112</volume><fpage>E2083</fpage><lpage>E2092</lpage><pub-id pub-id-type="doi">10.1073/pnas.1418730112</pub-id><pub-id pub-id-type="pmid">25847997</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seibold</surname><given-names>DR</given-names></name><name><surname>McPHEE</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Commonality analysis: A method for decomposing explained variance in multiple regression analyses</article-title><source>Human Communication Research</source><volume>5</volume><fpage>355</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1111/j.1468-2958.1979.tb00649.x</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Schwarzbach</surname><given-names>J</given-names></name><name><surname>Courtney</surname><given-names>SM</given-names></name><name><surname>Golay</surname><given-names>X</given-names></name><name><surname>Yantis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Control of object-based attention in human cortex</article-title><source>Cerebral Cortex</source><volume>14</volume><fpage>1346</fpage><lpage>1357</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh095</pub-id><pub-id pub-id-type="pmid">15166105</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Multidimensional Scaling, tree-fitting, and clustering</article-title><source>Science</source><volume>210</volume><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1126/science.210.4468.390</pub-id><pub-id pub-id-type="pmid">17837406</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical information flow during flexible sensorimotor decisions</article-title><source>Science</source><volume>348</volume><fpage>1352</fpage><lpage>1355</lpage><pub-id pub-id-type="doi">10.1126/science.aab0551</pub-id><pub-id pub-id-type="pmid">26089513</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Bernhardt</surname><given-names>BC</given-names></name><name><surname>Leech</surname><given-names>R</given-names></name><name><surname>Bzdok</surname><given-names>D</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The default mode network in cognition: a Topographical perspective</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>503</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1038/s41583-021-00474-4</pub-id><pub-id pub-id-type="pmid">34226715</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stawarczyk</surname><given-names>D</given-names></name><name><surname>Bezdek</surname><given-names>MA</given-names></name><name><surname>Zacks</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Event representations and predictive processing: the role of the midline default network core</article-title><source>Topics in Cognitive Science</source><volume>13</volume><fpage>164</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1111/tops.12450</pub-id><pub-id pub-id-type="pmid">31486286</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>H</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Lotter</surname><given-names>W</given-names></name><name><surname>Moerman</surname><given-names>C</given-names></name><name><surname>Paredes</surname><given-names>A</given-names></name><name><surname>Ortega Caro</surname><given-names>J</given-names></name><name><surname>Hardesty</surname><given-names>W</given-names></name><name><surname>Cox</surname><given-names>D</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Recurrent computations for visual pattern completion</article-title><source>PNAS</source><volume>115</volume><fpage>8835</fpage><lpage>8840</lpage><pub-id pub-id-type="doi">10.1073/pnas.1719397115</pub-id><pub-id pub-id-type="pmid">30104363</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Arteaga</surname><given-names>D</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain mechanisms for simple perception and Bistable perception</article-title><source>PNAS</source><volume>110</volume><fpage>E3350</fpage><lpage>E3359</lpage><pub-id pub-id-type="doi">10.1073/pnas.1221945110</pub-id><pub-id pub-id-type="pmid">23942129</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeo</surname><given-names>BTT</given-names></name><name><surname>Krienen</surname><given-names>FM</given-names></name><name><surname>Sepulcre</surname><given-names>J</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name><name><surname>Lashkari</surname><given-names>D</given-names></name><name><surname>Hollinshead</surname><given-names>M</given-names></name><name><surname>Roffman</surname><given-names>JL</given-names></name><name><surname>Smoller</surname><given-names>JW</given-names></name><name><surname>Zöllei</surname><given-names>L</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The organization of the human cerebral cortex estimated by intrinsic functional Connectivity</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>1125</fpage><lpage>1165</lpage><pub-id pub-id-type="doi">10.1152/jn.00338.2011</pub-id><pub-id pub-id-type="pmid">21653723</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84797.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Luo</surname><given-names>Huan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.11.17.516923" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.11.17.516923"/></front-stub><body><p>This study presents valuable findings on the fine spatiotemporal profiles of object recognition in human brains under noisy and ambiguous conditions. The evidence supporting the conclusion is compelling, with state-of-the-art techniques and model-driven fusion of MEG and 7T fMRI. The work will be of broad interest to cognitive neuroscientists working on consciousness and related fields.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84797.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Huang</surname><given-names>Zirui</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/10.1101/2022.11.17.516923">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.11.17.516923v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Spatiotemporal neural dynamics of object recognition under uncertainty in humans&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Christian Büchel as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Zirui Huang (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission. Both reviewers acknowledge the importance of the questions addressed in the work and the state-of-the-art model-driven MEG-7T fusion analysis. In the meantime, the reviewer raised several crucial concerns about the validity of several methods and the underlying rationale, as well as the lack of statistical support for some conclusions.</p><p>Essential revisions:</p><p>(1) There are some confusions in the results, such as the inconsistencies between the decoding and RSA analysis, the strange outlier pattern that might confound the results, and the ROI selection that might bias model comparison. The authors should address the points by providing convincing and thorough evidence. (See comments by Reviewer 1)</p><p>(2) One of the key conclusions of the work, i.e., variability reduction, is not backed up by statistical evidence (reviewer 1).</p><p>(3) Several choices in the analysis seem arbitrary and lack verification, such as model comparison in different ROIs, and the use of two time ranges to test the hypothesis (see comments by Reviewer 1).</p><p>(4) Since 7T fMRI results provide detailed spatial information as well as early recognition activities that might arise through a subcortical pathway, it seems feasible and a valuable opportunity to delineate the underlying mechanism by including subcortical regions in the analysis (see Reviewer 2's comments).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. In Figure 2C, The RSA results seem to be not consistent with the decoding results shown in Figure 2B. If my understanding is correct, we would expect to see category decoding starting at 290 ms, but the RSA results are a bit uniform and do not display any clustering effect as would be the case for successful category decoding. Why is that?</p><p>2. Moreover, what do the horizontal and vertical yellow lines indicate? Do they represent some specific image exemplars that elicit abnormal MEG responses? I am concerned that they indeed contribute to the weak category decoding for unrecognized trials given that they are within the unrecognized quadrate.</p><p>3. In Figure 2D, the shrinking neural representation space of unrecognizable objects does not have statistical support. The authors should do a quantitative analysis to back up the claim.</p><p>4. The authors chose ROIs based on the difference between recognized and unrecognized trials for further analysis, i.e., testing the recognition model vs. the two-state model. What is the rationale behind the pre-selection? Would such ROI selection criteria be biased toward the recognition model?</p><p>5. The idea of using a &quot;recognition model&quot; and &quot;two-state model&quot; to test the representational neural format in different ROIs is interesting but unfortunately somewhat unconvincing. For example, the only difference between the two models seems to reside in the 4th quadrant, that is, the category RSA for unrecognized trials. In other words, I don't think the two models are good comparisons. Related to this point, Figure 4A is confusing since the two models work for all ROIs.</p><p>6. Related to the above point, if the major goal is to characterize the temporal evolvement of representational models in different regions, the authors should compare the two models directly at each time point and in each ROI. This seems to be a more straightforward way.</p><p>7. The authors selected 0-1000ms and 1000-2000ms as the perceptual and post-perceptual time range for model comparison. This division is quite arbitrary and lacks independent criteria. Moreover, 1000 ms is too long for perceptual analysis and would certainly involve post-perceptual and feedback signals, in contrast to the perceptual claim.</p><p>8. The major findings based on commonality analysis rely on decoding results of both MEG and fMRI. Figure 2B shows that unrecognizable trials in MEG signals do not contain category information. But how about fMRI decoding results? Do the ventral visual pathway regions show successful category decoding for unrecognizable trials? If yes, could the two-state results for VTC be simply due to fMRI results?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Wu and colleagues applied a novel MEG-fMRI fusion approach and studied the spatiotemporally resolved neural activity of object recognition under uncertainty through a reanalysis of MEG and fMRI datasets from the same research group (Podvalny et al., 2019; Levinson et al., 2021). The authors found that recognition-related brain activity occurs prior to category-related activity, and demonstrated differential representation formats (i.e., state-switch vs. variability-reduction) in ventral visual and frontoparietal regions. It was concluded that object recognition under uncertainty engages bidirectional coordination among multiple cortical networks with diverse functional roles at an early processing stage. The manuscript was very well-written. I have a few points to share as follows.</p><p>1. Perhaps many readers are not familiar with the MEG-fMRI fusion approach. A few sentences may be needed to unpack the rationale and basic assumptions of this approach. For example, measurements of neural activity by different imaging modalities are presumed to reflect the same generators when they correlate on a trial-by-trial or condition-by-condition basis.</p><p>2. One intriguing finding is that the onset of recognition-related activity (110-120ms according to both models) occurs much earlier than the onset of category-related activity. I wonder if the authors could discuss more about the early-stage activity. Does it suggest that the participants were aware of the content of the image at this stage? Or the neural process of early-stage activity remains under an unconscious processing stage, and the actual recognition experience occurred later at ~500ms (according to the peak decoding of recognition outcome)? A possible interpretation might be that the recognition-related activity at 110-120ms is an unconscious process associated with an initial assessment of the quality of sensory evidence, which could be accumulated over time (e.g., through corticocortical feedback activity) until the emergence of recognition experience.</p><p>3. The authors mentioned that &quot;the phase-scrambled images were not included in the analyses reported below due to comparatively low trial numbers&quot;. However, the scrambled conditions were analyzed by both Podvalny et al. 2019 and Levinson et al. 2021. Also considering that the MEG-fMRI fusion approach used in the current study was based on a condition-by-condition matrix instead of a trial-by-trial matrix, I wonder if false alarm vs. correct rejection (derived from scrambled conditions) would be information that is complementary to the current findings.</p><p>4. The early onset of frontoparietal recognition-related signals is of particular interest. The authors interpreted that they are &quot;triggered by the fast magnocellular inputs via the dorsal visual stream or projected from the pulvinar or mediodorsal nucleus of the thalamus.&quot; As the current study used 7T-fMRI data with high spatial resolution, studying subcortical regions should be feasible. In addition, Levinson et al. 2021 identified differences in activation magnitudes between recognized and unrecognized trials in a few subcortical regions including the thalamus and brainstem. It might be worth including these subcortical regions in the ROIs.</p><p>5. It was found that successful recognition was associated with enhanced DMN engagement despite its deactivation. There were dynamic shifts from recognition model-dominated effects to two-state model-dominated effects in the DMN. I wonder if the authors could elaborate a little more on the functional role of DMN in object recognition under uncertainty.</p><p>6. Please indicate recognized vs. unrecognized outcomes in Figure 3A and its legend. It wasn't very intuitive when first looking at the figure without reading the main text.</p><p>7. It may be easier for a general reader to quickly pick up the gist of the work, if a schematic illustration of the main findings is provided, along with putative neural mechanisms.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84797.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>(1) There are some confusions in the results, such as the inconsistencies between the decoding and RSA analysis, the strange outlier pattern that might confound the results, and the ROI selection that might bias model comparison. The authors should address the points by providing convincing and thorough evidence. (See comments by Reviewer 1)</p></disp-quote><p>We have now included additional results showing that the decoding and RSA results are highly consistent, in the newly added Figure 2F. We have also clarified the source of the outlier patterns observed in the RDM and performed additional control analyses demonstrating that these patterns are unlikely to confound the results. Finally, we have elaborated on our rationale for the ROI selection, the choice of models, and why our ROI selection does not bias model comparison. Please see below for detailed responses.</p><disp-quote content-type="editor-comment"><p>(2) One of the key conclusions of the work, i.e., variability reduction, is not backed up by statistical evidence (reviewer 1).</p></disp-quote><p>Statistical analysis regarding this effect is now provided in the newly added Figure 2E.</p><disp-quote content-type="editor-comment"><p>(3) Several choices in the analysis seem arbitrary and lack verification, such as model comparison in different ROIs, and the use of two time ranges to test the hypothesis (see comments by Reviewer 1).</p></disp-quote><p>We now better justify the selection of models and ROIs, and we have also carried out additional analyses which show that the results of model comparisons remain qualitatively similar regardless of specific choice of time ranges (Figure 5—figure supplement 1-2).</p><disp-quote content-type="editor-comment"><p>(4) Since 7T fMRI results provide detailed spatial information as well as early recognition activities that might arise through a subcortical pathway, it seems feasible and a valuable opportunity to delineate the underlying mechanism by including subcortical regions in the analysis (see Reviewer 2's comments).</p></disp-quote><p>While 7T fMRI has excellent sensitivity to subcortical regions, MEG does not. Because the present study requires aligning the spatial information from 7T fMRI with the temporal information from MEG, the lack of sufficient sensitivity of MEG to subcortical signals preclude robust analysis of subcortical processing. Please see our detailed response to this point below.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. In Figure 2C, The RSA results seem to be not consistent with the decoding results shown in Figure 2B. If my understanding is correct, we would expect to see category decoding starting at 290 ms, but the RSA results are a bit uniform and do not display any clustering effect as would be the case for successful category decoding. Why is that?</p></disp-quote><p>The reviewer is correct that clustering effect related to the category information is not immediately apparent in the RDMs via visual inspection (Figure 2C). To address the reviewer’s concern over the potential inconsistency between results shown in RDMs and category decoding, we included an additional RSA-based analysis to provide statistical support for the clustering effect in RDMs. The results are now depicted in the newly added Figure 2F, and described in detail on p.8-9 in the Results section (also see p.22 in the methods section):</p><p>“Together with our report of image category being only decodable in recognized trials but not unrecognized trials (Figure 2B), the strong clustering effect of recognized images indicates that neural activity associated with recognized images contained sharper category information compared to unrecognized images despite the overall smaller dissimilarities among each other. To validate the compatibility between category decoding and RDM results, we ran an additional RSA-based analysis to test for the presence of category information in brain responses to recognized and unrecognized images, respectively. At each time point, we compared the representational structure within the recognized images (cells in the upper-left quadrant) and unrecognized images (bottom-right quadrant) with a category model which predicted smaller dissimilarities between images from the same category than between images from different categories (Figure 2F, right). This yielded a time course of Spearman’s rhos, indexing the strength of category information available in brain responses to recognized and unrecognized images, respectively. As expected, the result was qualitatively similar to category decoding: Significant category information occurred shortly before ~500 ms and decayed at ~1000 ms in recognized trials, whereas no category information was evident in unrecognized trials (Figure 2F, left).”</p><p>We note that minor differences between the decoding and RSA-based results, such as the transient peaks before 500 ms, are expected because they were performed at different levels of the data. The decoding analysis was tailored at the image category level, while the RSA results were based on the betweenexemplar differences. Yet, the overall similarity between decoding and RSA-based analyses on category information corroborates our interpretation that brain activity contains sharper category information when images were recognized despite the reduced cortical activity space.</p><disp-quote content-type="editor-comment"><p>2. Moreover, what do the horizontal and vertical yellow lines indicate? Do they represent some specific image exemplars that elicit abnormal MEG responses? I am concerned that they indeed contribute to the weak category decoding for unrecognized trials given that they are within the unrecognized quadrate.</p></disp-quote><p>The horizontal and vertical yellow lines in the RDMs corresponded to an unrecognized face (#5) and house (#1) exemplar, respectively. They showed relatively high dissimilarities to all other conditions. These high dissimilarities were likely driven by the relatively low trial numbers in these two conditions. As shown in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>, the across-subject mean trial numbers for these two conditions were 2.83 (SEM: 0.52, 5<sup>th</sup> bar in ‘face U’) and 2.25 (0.43, 1<sup>st</sup> bar in ‘house U’), respectively.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Across-subject mean number of trials for individual image exemplars, grouped by recognition outcome and image category.</title><p>Green: recognized, red: unrecognized. Error bars represent SEM. Numbers above the bars represent across-subject mean trial number and SEM for a given image category and recognition outcome.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-sa2-fig1-v2.tif"/></fig><p>We do not think that the low trial numbers for these two individual conditions would have significantly contributed to the weak category decoding in unrecognized trials: In the category decoding analysis, samples for a given label (e.g., ‘face’) were pooled from trials corresponding to all five exemplars of the same category. That is, the sample size of a given label equaled the sum of trial numbers of all exemplars belonging to the that particular category. Thus, low trial number of a single exemplar had minimal effects on the overall sample size of that label which was relevant to the decoding performance. Indeed, as shown in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>, the sample sizes of both unrecognized face (mean:33.92, SEM: 2.98) and house (mean: 35.90, SEM: 2.98) lied well within the range of other recognized or unrecognized categories.</p><p>Moreover, we took additional measures to minimize potential bias due to unbalanced data. This included a resampling approach for the decoding analysis wherein label predictions were yielded from ensembles of classification models always fitted with balanced training data set, and the usage of balanced decoding accuracy to account for the unbalanced test data set (see <italic>Method: Multivariate pattern analysis of MEG data</italic> for details).</p><disp-quote content-type="editor-comment"><p>3. In Figure 2D, the shrinking neural representation space of unrecognizable objects does not have statistical support. The authors should do a quantitative analysis to back up the claim.</p></disp-quote><p>We thank the reviewer for pointing out this issue to our attention. We have now included additional results (Figure 2E) that provide statistical support for our claim of a shrinking neural representational space. These results on described on p.8 of the Results section:</p><p>“To test whether the clustering effect within the recognized images was indeed stronger than unrecognized images, we compared the mean dissimilarity between recognized images against that between unrecognized images at every time point (one-sided Wilcoxon sign rank tests). As shown in Figure 2E, the mean dissimilarity between recognized images (green) was significantly lower than between unrecognized images (red) at most of the poststimulus time points (130-1900 ms, blue horizontal bar), confirming the prominent clustering effect of recognized images over time.”</p><disp-quote content-type="editor-comment"><p>4. The authors chose ROIs based on the difference between recognized and unrecognized trials for further analysis, i.e., testing the recognition model vs. the two-state model. What is the rationale behind the pre-selection? Would such ROI selection criteria be biased toward the recognition model?</p></disp-quote><p>We thank the reviewer for this comment which revealed an inaccurate description about ROI selection in the previous version of the manuscript. We have now clarified that we only used the contrast between recognized and unrecognized trials to select ROIs in the higher-associative cortices (TPN and DMN regions). The basic rationale was to identify all brain regions engaged in the task, while excluding those not involved in the task. We do not think that this approach was biased towards the recognition model as the contrast itself was bidirectional and did not contain information about the expected representational structure. This was evidenced by the observation that brain activity in both the TPN and DMN regions were well explained by the recognition model despite having activation changes in the opposite directions (TPN regions showed higher <italic>activation</italic> in recognized trials while DMN regions showed higher <italic>deactivation</italic> in recognized trials). As such, in the early time window as shown in Figure 5, DAN and FPCN have higher activity in recognized trials, while DMN has higher activity in unrecognized trials, yet all three networks were better explained by the Recognition model. In addition, the best performing model for DMN flips from the Recognition model to the Two-state model from early to late time window, despite this network having a stronger deactivation for recognized trials throughout these time periods.</p><p>ROIs in the early visual cortices (EVC including bilateral V1, V2, V3, and V4) and object-selective ventral temporal cortices (VTC with face-, house-, object-, and animal-selective regions) were defined using separate retinotopic and functional localizers for each subject, independent of the main task, ruling out any potential bias towards a particular model. We have now revised p.9-10 of the manuscript regarding the ROI selection:</p><p>“Four ROIs in the early visual cortices (bilateral V1, V2, V3, and V4) and four bilateral regions in the ventral temporal cortices (VTC) selective to faces, houses, animals, and man-made objects were defined using independent retinotopy and object category functional localizers, respectively. ROIs in the frontoparietal cortices were defined based on differences in activation magnitudes between recognized and unrecognized trials (Levinson et al., 2021) and covered nine unilateral regions in the task-positive network (TPN); and eight unilateral or midline regions in the default mode network (DMN). As shown in Levinson et al. (2021), TPN regions had higher activation in recognized trials while DMN regions had stronger deactivation in recognized trials.”</p><disp-quote content-type="editor-comment"><p>5. The idea of using a &quot;recognition model&quot; and &quot;two-state model&quot; to test the representational neural format in different ROIs is interesting but unfortunately somewhat unconvincing. For example, the only difference between the two models seems to reside in the 4th quadrant, that is, the category RSA for unrecognized trials. In other words, I don't think the two models are good comparisons. Related to this point, Figure 4A is confusing since the two models work for all ROIs.</p></disp-quote><p>The reviewer is correct in that the recognition and two-state models differ only in the 4<sup>th</sup> quadrant. However, we think this difference is important as these models represent two fundamentally different hypotheses on the neural mechanisms underlying object recognition under uncertainty: The recognition model hypothesizes that object recognition under uncertainty is facilitated via the reduction in the variability of neural activity patterns, while the two-state model hypothesizes that it is achieved via a switch between two different brain states (now better summarized in the conceptual figure suggested by Reviewer #2, Figure 6).</p><p>While it is true that the effects of both recognition and two-state model were evident across all ROIs at particular time points, please also note that our results shown in Figure 4, Figure 4—figure supplement 1-3, Figure 5, and Figure5—figure supplement 1-3 demonstrate how these two models systematically differed in the timing, strength, and sustainability of their effects across different brain regions and networks. It is on these differences that our conclusions rest.</p><disp-quote content-type="editor-comment"><p>6. Related to the above point, if the major goal is to characterize the temporal evolvement of representational models in different regions, the authors should compare the two models directly at each time point and in each ROI. This seems to be a more straightforward way.</p></disp-quote><p>Our choice of comparing the models at the large-scale network level was based on the observation that ROIs within each network tended to have similar model-related temporal dynamics. Accordingly, we reasoned that model comparisons at the network level would provide a concise way to summarize the results without obscuring important information. Nevertheless, we agree with the reviewer that it is also important to examine model performances at the ROI level. For this reason, we also conducted analogous model comparisons for each individual ROI (Figure 5—figure supplement 3, previously Figure S8). The results at the ROI level were largely consistent with those at the network level. In addition, we also presented the quantitative model performance time courses for each individual ROI in Figure 4B and Figure 4—figure supplements 1-3, which allows a qualitative assessment of relative model performance at a very finegrained level (each ROI, each time point). However, we believe that doing statistical model comparison at the individual ROI-individual time point level would be highly noisy and would not result in much additional information than that already presented.</p><disp-quote content-type="editor-comment"><p>7. The authors selected 0-1000ms and 1000-2000ms as the perceptual and post-perceptual time range for model comparison. This division is quite arbitrary and lacks independent criteria. Moreover, 1000 ms is too long for perceptual analysis and would certainly involve post-perceptual and feedback signals, in contrast to the perceptual claim.</p></disp-quote><p>To address this concern, we repeated the analysis with different choices of time ranges for model comparisons. These results show qualitatively similar results and are presented in Figure 5—figure supplement 1-2 and described on p.14 of the Results section:</p><p>“Furthermore, we repeated the same analysis using different choices of time ranges (with 100 ms and 200 ms sliding windows). As shown in Figure 5—figure supplement 1-2, the results remained qualitatively similar, providing additional empirical support for the robustness of our results.”</p><p>We reasoned that neural activity during the first 1000 ms in the post-stimulus phase was mainly involved in the perceptual process based on multiple behavioral studies using similar degraded stimuli showing long reaction times of around or above 1000 ms (for specific references, see Discussion section, p.18, last sentence in the second paragraph). In addition, work by ourselves and others has shown that feedback signals are critical for perceptual resolution of degraded images such as those used herein (low-contrast, threshold-level images), and in such contexts can take well over 500 ms to unfold (see, e.g., Flounders et al., 2019). Nonetheless, we cannot completely exclude post-perceptual signals in the first 1000 ms after stimulus onset; therefore, we have tuned down the claim on p.13 of the manuscript:</p><p>“To this end, we split the explanatory power estimates into an early (0-1000 ms following stimulus onset) and a late (1000-2000 ms) time window, reasoning that perceptual processing had relatively strong influences on brain responses in the early stage whereas those in the late stage were mainly involved in post-perceptual processing.”</p><disp-quote content-type="editor-comment"><p>8. The major findings based on commonality analysis rely on decoding results of both MEG and fMRI. Figure 2B shows that unrecognizable trials in MEG signals do not contain category information. But how about fMRI decoding results? Do the ventral visual pathway regions show successful category decoding for unrecognizable trials? If yes, could the two-state results for VTC be simply due to fMRI results?</p></disp-quote><p>fMRI decoding results for ROIs in the ventral visual pathway were consistent with the MEG decoding results: Category information could be decoded from BOLD responses in the EVC (V1 – V4) and VTC (category-selective areas) when participants reported the stimuli as recognizable, but not when stimuli were unrecognizable. These results were previously reported Levinson et al. (2021, Figure 4b, Figure 5c and Figure S3b therein).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Wu and colleagues applied a novel MEG-fMRI fusion approach and studied the spatiotemporally resolved neural activity of object recognition under uncertainty through a reanalysis of MEG and fMRI datasets from the same research group (Podvalny et al., 2019; Levinson et al., 2021). The authors found that recognition-related brain activity occurs prior to category-related activity, and demonstrated differential representation formats (i.e., state-switch vs. variability-reduction) in ventral visual and frontoparietal regions. It was concluded that object recognition under uncertainty engages bidirectional coordination among multiple cortical networks with diverse functional roles at an early processing stage. The manuscript was very well-written. I have a few points to share as follows.</p><p>1. Perhaps many readers are not familiar with the MEG-fMRI fusion approach. A few sentences may be needed to unpack the rationale and basic assumptions of this approach. For example, measurements of neural activity by different imaging modalities are presumed to reflect the same generators when they correlate on a trial-by-trial or condition-by-condition basis.</p></disp-quote><p>We thank the reviewer for this suggestion and have included additional text on p.9:</p><p>“We then used RSA to combine time-varying MEG data with spatially-localized fMRI data based on the correspondence between their representational structures, under the assumption that a correspondence between neural measurements by different recording modalities reflects the same neural generators (Kriegeskorte et al., 2008; Cichy and Oliva, 2020)”</p><disp-quote content-type="editor-comment"><p>2. One intriguing finding is that the onset of recognition-related activity (110-120ms according to both models) occurs much earlier than the onset of category-related activity. I wonder if the authors could discuss more about the early-stage activity. Does it suggest that the participants were aware of the content of the image at this stage? Or the neural process of early-stage activity remains under an unconscious processing stage, and the actual recognition experience occurred later at ~500ms (according to the peak decoding of recognition outcome)? A possible interpretation might be that the recognition-related activity at 110-120ms is an unconscious process associated with an initial assessment of the quality of sensory evidence, which could be accumulated over time (e.g., through corticocortical feedback activity) until the emergence of recognition experience.</p></disp-quote><p>The interpretation about the early-stage activity brought up by the reviewer is highly intriguing. We have now added it to the Discussion section on p.18:</p><p>“Alternatively, they may reflect the accumulation of sensory evidence preceding the rise of a particular perceptual experience (Gold and Shadlen, 2007; Kelly and O’Connell, 2015). These interpretations are not mutually exclusive and consistent with our empirical finding that the onset of recognition-related activity according to both models (significance at 110-120 ms) is earlier than the onset of category-level information (initial peak at 290 ms, significance at 470 ms).”</p><disp-quote content-type="editor-comment"><p>3. The authors mentioned that &quot;the phase-scrambled images were not included in the analyses reported below due to comparatively low trial numbers&quot;. However, the scrambled conditions were analyzed by both Podvalny et al. 2019 and Levinson et al. 2021. Also considering that the MEG-fMRI fusion approach used in the current study was based on a condition-by-condition matrix instead of a trial-by-trial matrix, I wonder if false alarm vs. correct rejection (derived from scrambled conditions) would be information that is complementary to the current findings.</p></disp-quote><p>We agree with the reviewer that it would be intriguing to include scrambled image trials in the MEG-fMRI fusion. However, in contrast to real images wherein both hit and miss consisted of 20 conditions, respectively (4 categories x 5 exemplars), false alarm and correct rejection would only consist of 4 conditions (1 exemplar per category). Since the power of RSA-based analysis directly depends on the richness of condition set, the relatively small condition numbers for false alarm and correct rejection would make the comparisons with hit and miss derived from the real images or with each other difficult to interpret.</p><disp-quote content-type="editor-comment"><p>4. The early onset of frontoparietal recognition-related signals is of particular interest. The authors interpreted that they are &quot;triggered by the fast magnocellular inputs via the dorsal visual stream or projected from the pulvinar or mediodorsal nucleus of the thalamus.&quot; As the current study used 7T-fMRI data with high spatial resolution, studying subcortical regions should be feasible. In addition, Levinson et al. 2021 identified differences in activation magnitudes between recognized and unrecognized trials in a few subcortical regions including the thalamus and brainstem. It might be worth including these subcortical regions in the ROIs.</p></disp-quote><p>In fact, our initial MEG-fMRI fusion analysis included subcortical ROIs reported in Levinson et al. (2021) such as the brainstem and basal ganglia-thalamus. However, the RDMs derived from these ROIs did not show positive correlations with the MEG RDMs (see <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>). This is likely because MEG has poor sensitivity to subcortical sources. As such, subcortical regions were excluded from model-driven MEG-fMRI fusion analyses. The negative correlations observed between fMRI RDMs from subcortical regions and MEG RDMs (<xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>) are similar to that seen in V4 (Figure 3—figure supplement 1), and V4 was also excluded from our analyses for this reason.</p><p>In sum, because the present study requires aligning the spatial information from 7T fMRI with the temporal information from MEG, the lack of MEG sensitivity to subcortical signals preclude robust analysis of subcortical processing. Therefore, we excluded these regions from the analyses reported in the manuscript, as we cannot be confident about any result there.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Correlations between time-varying MEG RDMs and fMRI RDMs derived from two subcortical ROIs.</title><p>Format is the same as Figure 3—figure supplements 1-3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84797-sa2-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>5. It was found that successful recognition was associated with enhanced DMN engagement despite its deactivation. There were dynamic shifts from recognition model-dominated effects to two-state model-dominated effects in the DMN. I wonder if the authors could elaborate a little more on the functional role of DMN in object recognition under uncertainty.</p></disp-quote><p>We have now expanded our discussion regarding DMNs’ role in our task on p.18 of the Discussion section:</p><p>“The shift towards the two-state representational structure in the late stage may indicate an enhanced intercommunication with other areas that were also dominated by the two-state model effects, such as the ventral visual regions. Speculatively, given DMN’s prominent role in both associative processing (Bar et al., 2007; Stawarczyk et al., 2021) and prior-guided perception (Gonzalez-Garcia et al., 2018; Flounders et al., 2019), this effect might reflect interactions between the current visual inputs and the observer’s prior knowledge that guide subsequent perceptual decisions and related associations.”</p><disp-quote content-type="editor-comment"><p>6. Please indicate recognized vs. unrecognized outcomes in Figure 3A and its legend. It wasn't very intuitive when first looking at the figure without reading the main text.</p></disp-quote><p>Figure 3A has now been modified.</p><disp-quote content-type="editor-comment"><p>7. It may be easier for a general reader to quickly pick up the gist of the work, if a schematic illustration of the main findings is provided, along with putative neural mechanisms.</p></disp-quote><p>We thank the reviewer for this wonderful suggestion. A schematic illustration of our main findings is now added to the Discussion section (Figure 6 on p.16).</p></body></sub-article></article>