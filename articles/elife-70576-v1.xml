<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">70576</article-id><article-id pub-id-type="doi">10.7554/eLife.70576</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>PARROT is a flexible recurrent neural network framework for analysis of large protein datasets</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-240723"><name><surname>Griffith</surname><given-names>Daniel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9633-9601</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-169231"><name><surname>Holehouse</surname><given-names>Alex S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4155-5729</contrib-id><email>alex.holehouse@wustl.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>Department of Biochemistry and Molecular Biophysics, Washington University School of Medicine</institution><addr-line><named-content content-type="city">St Louis</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Center for Science and Engineering Living Systems, Washington University</institution><addr-line><named-content content-type="city">St Louis</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role>Reviewing Editor</role><aff><institution>Goethe University</institution><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role>Senior Editor</role><aff><institution>Goethe University</institution><country>Germany</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>09</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e70576</elocation-id><history><date date-type="received" iso-8601-date="2021-05-21"><day>21</day><month>05</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-09-06"><day>06</day><month>09</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-05-23"><day>23</day><month>05</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.05.21.445045"/></event></pub-history><permissions><copyright-statement>© 2021, Griffith and Holehouse</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Griffith and Holehouse</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-70576-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-70576-figures-v1.pdf"/><abstract><p>The rise of high-throughput experiments has transformed how scientists approach biological questions. The ubiquity of large-scale assays that can test thousands of samples in a day has necessitated the development of new computational approaches to interpret this data. Among these tools, machine learning approaches are increasingly being utilized due to their ability to infer complex nonlinear patterns from high-dimensional data. Despite their effectiveness, machine learning (and in particular deep learning) approaches are not always accessible or easy to implement for those with limited computational expertise. Here we present PARROT, a general framework for training and applying deep learning-based predictors on large protein datasets. Using an internal recurrent neural network architecture, PARROT is capable of tackling both classification and regression tasks while only requiring raw protein sequences as input. We showcase the potential uses of PARROT on three diverse machine learning tasks: predicting phosphorylation sites, predicting transcriptional activation function of peptides generated by high-throughput reporter assays, and predicting the fibrillization propensity of amyloid beta with data generated by deep mutational scanning. Through these examples, we demonstrate that PARROT is easy to use, performs comparably to state-of-the-art computational tools, and is applicable for a wide array of biological problems.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>machine learning</kwd><kwd>high-throughput methods</kwd><kwd>proteomics</kwd><kwd>bioinformatics</kwd><kwd>functional annotation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DGE-2139839</award-id><principal-award-recipient><name><surname>Griffith</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100003085</institution-id><institution>Longer Life Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Holehouse</surname><given-names>Alex S</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>PARROT makes it easy for anyone to train and use system- or data-specific deep learning models that map between protein sequence and arbitrary sequence annotations.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The past decade has seen an exponential increase in the rate at which biological data is generated (<xref ref-type="bibr" rid="bib33">Marx, 2013</xref>). Technological advances coupled with the falling costs of DNA synthesis and sequencing have made conducting high-throughput experiments accessible to most research labs (<xref ref-type="bibr" rid="bib25">Hughes and Ellington, 2017</xref>). The affordability of being able to sequence massive quantities of DNA is transforming how molecular biologists approach research. Protein functional assays and screens are seeing increasing library sizes, which allows researchers to investigate many different sequences and variants in a single experiment. In recently published studies, it is not uncommon to find deep mutational scanning (DMS) experiments that achieve nearly complete sequence coverage or assays that test tens of thousands of peptides (<xref ref-type="bibr" rid="bib5">Arnold et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Bolognesi et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Erijman et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Jones et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Livesey and Marsh, 2020</xref>; <xref ref-type="bibr" rid="bib47">Seuma et al., 2021</xref>; <xref ref-type="bibr" rid="bib44">Sanborn et al., 2021</xref>; <xref ref-type="bibr" rid="bib45">Schmiedel and Lehner, 2019</xref>). This abundance of data being generated has the potential to answer important biological questions; however, at the same time, it also significantly complicates experimental analysis.</p><p>Coinciding with the explosion of high-throughput omics experiments has been the development of computational methods for analyzing the resulting high-dimensional biological data. In particular, machine learning approaches have emerged as popular strategies in a wide range of biological applications (<xref ref-type="bibr" rid="bib51">Xu and Jackson, 2019</xref>; <xref ref-type="bibr" rid="bib15">Eraslan, 2019</xref>; <xref ref-type="bibr" rid="bib35">Moses, 2017</xref>). In general, machine learning approaches are effective at identifying patterns in complex datasets and extrapolating these learned patterns to make predictions on previously untested samples. Deep learning approaches, as opposed to ‘shallow’ machine learning approaches, such as logistic regression, are particularly well-suited for biological applications as they can implicitly capture relevant features in order to model complex, nonlinear, biological relationships (<xref ref-type="bibr" rid="bib34">Min et al., 2017</xref>; <xref ref-type="bibr" rid="bib39">Raimondi et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Xu et al., 2020</xref>). In the context of protein datasets, deep learning approaches offer the attractive quality of allowing researchers to simply input raw protein sequences into the model, rather than requiring an intermediate step where proteins are reduced into simplified representations (e.g., amino acid content or biophysical properties; <xref ref-type="bibr" rid="bib39">Raimondi et al., 2019</xref>).</p><p>However, despite their advantages over simpler models, deep learning approaches are still a relatively specialized form of data analysis. As a result, in many domains of biological sciences, there remains a technical and conceptual barrier for labs to apply deep learning approaches to their data. In some cases, this could be reasonably attributed to preference for more interpretable simple models, rather than more accurate, but often cryptic, deep learning models (<xref ref-type="bibr" rid="bib42">Rudin, 2019</xref>; <xref ref-type="bibr" rid="bib36">Murdoch et al., 2019</xref>). In other cases, this lack of adoption could be due to a general unfamiliarity and inexperience with deep learning. Indeed, the field of deep learning can appear daunting for those without extensive computational backgrounds. For an untrained scientist with amenable high-throughput datasets, it may be infeasible or too time-consuming to implement deep learning models into an analysis workflow.</p><p>Here, we aim to make cutting-edge deep learning accessible to a broad audience of biological researchers through our package PARROT (Protein Analysis using RecuRrent neural networks On Training data). PARROT is designed to be a general framework for training machine learning networks on large protein datasets, then using the trained network to make predictions on new protein sequences. The user side of PARROT is an easy-to-use command line tool that is flexible enough to handle a variety of data formats and machine learning tasks. In its implementation, PARROT carries out the computational heavy lifting through implementation of a recurrent neural network (RNN). RNNs are a class of deep learning architecture originally designed for language processing applications, but have since been employed with remarkable success in biology (<xref ref-type="bibr" rid="bib43">Rumelhart et al., 1986</xref>; <xref ref-type="bibr" rid="bib30">Lipton et al., 2021</xref>; <xref ref-type="bibr" rid="bib20">Hanson et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Heffernan et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Almagro Armenteros et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Li et al., 2017</xref>; <xref ref-type="bibr" rid="bib4">Angermueller et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Alley et al., 2019</xref>). Compared to other deep learning approaches, RNNs are unique in that they are designed to handle variable length sequences, which makes them well-suited for applications involving proteins. Using only raw protein sequences as input, RNNs can learn the relevant positional dependencies of amino acids needed to associate each sequence with a corresponding functional value or values. Through this architecture, PARROT is able to capture intrinsic patterns in large protein datasets in order to construct highly accurate predictive models.</p><p>In this paper, we introduce the underlying RNN architecture of PARROT and demonstrate its application to three different biological problems. First, we show that PARROT performs at a near state-of-the-art level on phosphorylation site prediction tasks, a well-characterized bioinformatics problem. Second, we use PARROT to train a predictor of transcriptional activation activity using the extensive peptide library from <xref ref-type="bibr" rid="bib16">Erijman et al., 2020</xref>. Third, we demonstrate how PARROT can be used in conjunction with DMS assays, using the amyloid beta-based dataset from <xref ref-type="bibr" rid="bib47">Seuma et al., 2021</xref>. Ultimately, we show that PARROT is an effective, generalizable, and easy-to-use machine learning tool that is applicable to a range of different protein datasets.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>PARROT is a general RNN framework</title><p>Our motivation behind PARROT was to develop a powerful deep learning tool that is easy to implement into any large-scale protein analysis workflows (&gt;1000s of sequences; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The general workflow involves the following steps. A user starts with a set of sequences of interest where each sequence (or each residue in each sequence) has some label associated with it, either a discrete class or a continuous value. PARROT uses this initial dataset to train, validate, and test a deep learning model. Training, validation, and testing are all performed automatically within PARROT using standard best practices for machine learning model generation. Once a model is built, the user can use that model to make predictions on new sequences for which there is no data associated.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>PARROT overview.</title><p>(<bold>A</bold>) A standard workflow that incorporates PARROT. Quantitative protein data is either obtained computationally or generated through experiment, then formatted such that each protein sequence or residue is linked to a particular value. PARROT allows users to train a predictor on this dataset. The trained network can then be applied on new sequences to make predictions. (<bold>B</bold>) The internal architecture of PARROT is a bidirectional long short-term memory (LSTM) network. (Top) Series of cells propagate information along the length of a protein sequence in both N-to-C and C-to-N directions and the final output is integrated from the deepest layers in each direction. (Bottom) A diagram of the LSTM cells used in PARROT. (<bold>C</bold>) Example data formats for the four kinds of machine learning problems PARROT can carry out on proteins: classification or regression tasks using per-sequence or per-residue output.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70576-fig1-v1.tif"/></fig><p>We used the PyTorch platform to implement the core RNN framework of PARROT (<xref ref-type="bibr" rid="bib37">Paszke, 2021</xref>). The serialized architecture of RNNs and their ability to handle variable length inputs makes them well-suited for learning information from protein sequences. In the context of protein analysis, each cell in an RNN integrates information from a particular amino acid with the output (‘hidden state vector’) of the preceding cell in the network. However, there are two main drawbacks of using the standard RNN architecture for protein analysis. First, standard RNNs require that information is propagated through the network in a single direction, which imposes an arbitrary constraint on the ability of a network to learn from protein sequences. Second, standard RNNs are susceptible to the ‘vanishing gradient problem,’ which arises due to the multiplication of many small values and can limit the ability of a network to learn long-range dependencies in the data (<xref ref-type="bibr" rid="bib6">Bengio et al., 1994</xref>). PARROT implements two common variants of RNN architecture in order to mitigate these factors (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). To address the first, the RNN implementation of PARROT is <italic>bidirectional</italic>, such that there are two parallel RNNs that propagate information from the protein sequence in opposite directions (N-to-C and C-to-N) simultaneously (<xref ref-type="bibr" rid="bib46">Schuster and Paliwal, 1997</xref>). To address the issue of vanishing gradients, PARROT employs long short-term memory (LSTM) cells, which have been shown to capture long-range dependencies in sequences more efficiently than standard RNNs (<xref ref-type="bibr" rid="bib23">Hochreiter and Schmidhuber, 1997</xref>). Combining bidirectionality with LSTM cells has been previously demonstrated to be effective at learning from biological sequences (<xref ref-type="bibr" rid="bib20">Hanson et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Heffernan et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Almagro Armenteros et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Li et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Alley et al., 2019</xref>). Taken together, PARROT’s underlying network architecture is specifically optimized for working with protein datasets.</p><p>PARROT was designed to conceal the inner workings of this RNN, such that only a limited set of information is required from the user. For the most basic usage, the user only needs to provide their data and specify the kind of machine learning task for which they are training the network (classification or regression, described below). User datasets are input as basic whitespace-delimited text files with each protein sequence and its corresponding data contained on a single line. This file can be prepared in any spreadsheet program (e.g., Excel) and saved as a tab-separated variable file. More detailed instructions for file preparation are provided in the PARROT documentation. One of the consequences of PARROT’s internal RNN is that the provided input sequences are not required to be the same length. Before training a PARROT network, users must specify whether their application qualifies as a <italic>classification</italic> or <italic>regression</italic> task. In classification tasks, the network is trained to assign discrete class labels to each input. For example, if one had a set of proteins where each protein localized to a specific organelle, this would lend itself to a classification task for predicting subcellular localization. For regression, the network outputs a continuous, real-number value for each input. For example, if one had a set of peptides where each sequence had an aggregation score between 0 and 1, this would lend itself to a regression task for predicting quantitative peptide aggregation. In addition to these two categories, users must also specify whether they want the PARROT network to produce <italic>per-sequence</italic> or <italic>per-residue</italic> output. Example data formats for each of these four categories are depicted in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. Beyond this core usage, advanced users may optionally specify network hyperparameters such as the number of layers in the network, size of the hidden state vectors, learning rate, batch size, number of training epochs, and various other optional arguments (see Materials and methods).</p><p>In the remaining sections, we demonstrate the effectiveness of PARROT in the context of three distinct protein applications. Our goal here is to illustrate the diverse types of biological questions PARROT is capable of interrogating and to inspire readers to apply PARROT in their own research.</p></sec><sec id="s2-2"><title>PARROT predicts phosphosites on par with established methods</title><p>We first benchmarked the performance of PARROT-derived networks on a commonly studied bioinformatics task: predicting phosphorylation sites in a protein sequence. We used the Phospho.ELM (P.ELM) version 9.0 (<xref ref-type="bibr" rid="bib11">Diella et al., 2007</xref>) and PhosPhAt (PPA) version 3.0 (<xref ref-type="bibr" rid="bib21">Heazlewood et al., 2008</xref>; <xref ref-type="bibr" rid="bib13">Durek et al., 2010</xref>) datasets for training and independent validation, similar to <xref ref-type="bibr" rid="bib12">Dou et al., 2014</xref>. P.ELM consists of literature-derived animal phosphorylation sites, and PPA consists of mass spectrometry-validated phosphosites in <italic>Arabidopsis thaliana</italic>. For both of these datasets, we extracted all 19-residue windows centered around serine, threonine, and tyrosine and divided each of these into phosphorylation-positive and -negative sets. After filtering out similar sequences with CD-HIT (<xref ref-type="bibr" rid="bib19">Fu et al., 2012</xref>), we then downsampled the larger phosphorylation-negative sets in order to create balanced datasets with identical numbers of phosphorylation-positive and phosphorylation-negative windows. Separate PARROT networks were trained on the serine, threonine, and tyrosine windows from the P.ELM dataset (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>PARROT’s performance on a phosphosite prediction task.</title><p>(<bold>A</bold>) Workflow for training PARROT networks for phosphosite prediction. Full-length, annotated sequences from the Phospho.ELM (P.ELM) dataset were split into phospho-positive and phospho-negative 19aa windows (11aa windows used in figure for clarity). PARROT predictors trained on these sequence windows and were used to make predictions on held out sequences and the PhosPhAt (PPA) dataset. (<bold>B</bold>) Matthew’s correlation coefficient (MCC), (<bold>C</bold>) sensitivity (<bold>D</bold>), and specificity scores for the PARROT predictors and three external predictors on the task of phosphosite prediction on the P.ELM and PPA datasets.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70576-fig2-v1.tif"/></fig><p>We first tested our PARROT phosphosite predictors for each of the three residues on the P.ELM dataset using 10-fold cross-validation. This involved randomly splitting each residue-specific dataset into 10 even subsets, then training on 9/10 of the data and testing on the held out 1/10 for each of the subsets. As a benchmark, we compared the performance of our PARROT networks against three established phosphosite predictors, PhosphoSVM, MusiteDeep, and PHOSFER, which each rely upon different methodologies (<xref ref-type="bibr" rid="bib12">Dou et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Trost and Kusalik, 2013</xref>; <xref ref-type="bibr" rid="bib50">Wang et al., 2017</xref>). As this was a binary classification problem, we focused our analysis on sensitivity, specificity, and Matthew’s correlation coefficient (MCC) as performance metrics. We chose MCC as a performance metric because it has been shown to be more informative for binary classification tasks than the more commonly used F1 score or accuracy (<xref ref-type="bibr" rid="bib10">Chicco and Jurman, 2020</xref>). Overall, the PARROT networks performed better than PhosphoSVM, and at a comparable level to PHOSFER and MusiteDeep (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Interestingly, there was variation in the relative performance of the top three methods across the three residue types, with PARROT performing best on pSer, second best on pThr, and third best on pTyr. This performance trend corresponds with the size of the training dataset available for each residue. The P.ELM cross-validation analysis also illuminated particular biases in each of the predictors. Notably, PHOSFER and MusiteDeep tended to predict with high sensitivity and low specificity, PhosphoSVM predicted with low sensitivity and high specificity (<xref ref-type="fig" rid="fig2">Figure 2C and D</xref>). However, PARROT’s predictions tended to be the most balanced, with comparable sensitivity and specificity across the three different residue types.</p><p>Overfitting to training data is a common problem in the field of machine learning, so to test for this we assessed the performance of our PARROT predictors on an independent test dataset. For each of the three residue types, we trained a PARROT predictor on the full P.ELM dataset and made predictions on Ser, Thr, and Tyr residues in the PPA dataset. Unsurprisingly, all of the PARROT predictors performed worse on the PPA data than they did on the P.ELM cross-validation analysis; however, the PARROT predictors still performed comparably or better than the three established phosphorylation site predictors (<xref ref-type="fig" rid="fig2">Figure 2B–D</xref>, <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). PARROT’s comparable performance to PHOSFER on the PPA dataset is particularly notable because PHOSFER was specifically designed for the prediction of plant phosphorylation sites (<xref ref-type="bibr" rid="bib49">Trost and Kusalik, 2013</xref>).</p><p>Ultimately, our intention behind these analyses is not to assert that our PARROT-based predictors are inherently superior to all other existing phosphorylation site predictors. Rather it is to demonstrate that PARROT, despite being a general framework for any type of protein analysis, can perform equivalently to methods that are specifically developed for a particular task. In doing so, we establish that PARROT-trained networks perform at a high level and that PARROT can confidently be extended to other less well-characterized protein applications.</p></sec><sec id="s2-3"><title>PARROT can integrate into high-throughput experiment workflows</title><p>Having established that predictors trained with PARROT can accurately learn patterns in large datasets, we next focused on showcasing PARROT’s ability to integrate into a typical high-throughput experiment workflow. To accomplish this, we turned to the data generated by <xref ref-type="bibr" rid="bib16">Erijman et al., 2020</xref>, in which the authors developed a high-throughput fluorescence assay for testing 30-mer peptides for activation domain (AD) function in yeast. Their assay measured ~37,000 sequences with AD function and ~1 million without, allowing them to train a convolutional neural network to predict AD function from sequence and secondary structure information (<italic>ADpred</italic>). This general workflow of (1) generating massive quantities of data using a high-throughput assay and (2) developing a computational predictor based on the assay data is becoming increasingly common in molecular biology. While ultimately the approach taken by Erijman et al. was computationally rigorous and successful, here we demonstrate that PARROT could readily be implemented in such a workflow without sacrificing performance (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Using PARROT in cases like this could save researchers valuable time from having to develop their own machine learning predictors from scratch.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>PARROT predicts functional yeast activation domains.</title><p>(<bold>A</bold>) Diagram of activation domain workflow. A PARROT network was trained on the yeast fluorescence activation assay data from Erijman et al. and used to make predictions on new protein sequences. (<bold>B</bold>) PARROT’s 10-fold cross-validation accuracy and area under the precision-recall curve (AUPRC) on the Erijman et al. dataset compared to the reported scores for two approaches employed in that paper: <italic>ADpred</italic> and a logistic regression based method. (<bold>C</bold>) Representative example of the correlation between PARROT’s predictions and the true activation scores of an independent yeast activation domain dataset. (<bold>D</bold>) PARROT’s performance on the tasks in (<bold>B</bold>) (top) and (<bold>C</bold>) (bottom) as a function of dataset size. For each specified dataset size, the actual number of sequences used for training and validation was 90% of the indicated value since networks were trained using 10-fold cross-validation. The dashed line is a hyperbola best-fit line. The reported performance of <italic>ADpred</italic> is shown for reference in gray.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70576-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Performance of PARROT networks trained on a multi-study dataset on the activation domain prediction task.</title><p>Red indicates PARROT networks trained on the combined datasets of Erijman et al. and Ravarani et al. Blue and gray markers are identical to <xref ref-type="fig" rid="fig3">Figure 3D</xref> in the main text and are included for comparison.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70576-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Analysis of PARROT networks on the test set data of Sanborn et al.</title><p>(Top row) Comparison of <italic>PADDLE</italic>’s predictions to the experimental values on three subsets of the sequences in the test set. <italic>PADDLE</italic> predictions and experimental values were obtained directly from Sanborn et al. and replotted in a manner similar to that study in order to facilitate side-by-side comparison. Refer to that published work for more information on the different subsets that compose the test set. (Middle row) Predictions made by a PARROT network trained on the Erijman et al. dataset. (Bottom row) Predictions made by a PARROT network trained on the Sanborn et al. training set. Denoted in parentheses are the 95% CI of the reported <italic>R</italic><sup>2</sup> value, determined using bootstrapping.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70576-fig3-figsupp2-v1.tif"/></fig></fig-group><p>Using 10-fold cross-validation, we trained PARROT networks on this AD dataset (see Materials and methods) and evaluated their performance and generalizability. First, we tested how well each of the networks predicted AD function on the held-out test set. PARROT networks predicted AD function with an accuracy of 93.1% (standard error ±0.1%) and an area under the precision-recall curve (AUPRC) of 0.973 (± 0.001), which were not significantly different from <italic>ADpred</italic>’s reported accuracy and AUPRC of 93.2% (± 0.1%) and 0.975 (± 0.001), respectively (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). However, the PARROT-based predictors did significantly outperform the simple logistic regression model also used in <xref ref-type="bibr" rid="bib16">Erijman et al., 2020</xref>, which had an accuracy of 89.1% (± 0.4%) and AUPRC of 0.942 (± 0.002). We also assessed the generalizability of the PARROT predictors through a similar approach as in the <italic>ADpred</italic> paper. Each cross-validation-trained network was also applied to an independent yeast AD dataset from <xref ref-type="bibr" rid="bib48">Staller et al., 2018</xref>. We found good correlation between our predicted AD values and the independent data with an average Pearson’s <italic>R</italic> = 0.586 (± 0.005), which was slightly higher than the reported performance of <italic>ADpred</italic> of <italic>R</italic> = 0.57 (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>To assess how the PARROT networks performed with fewer sequences to train on, we repeated both of these analyses on reduced datasets. Sampling from the complete dataset containing 75,846 30-mer peptides (50% displaying AD function), we created new 70K, 60K, 50K, 40K, 30K, 20K, 10K, and 5K peptide datasets. AUPRC began to plateau around 40K peptides, and generalizability to the Staller et al. data plateaued at around 30K, indicating that PARROT can robustly capture meaningful patterns in reduced datasets (<xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p><p>Although all of the peptides studied in this analysis were 30 residues in length, one of the benefits of PARROT over other deep learning approaches is that it is not limited to fixed length sequences. In theory, one could train a predictor with PARROT using the combined results from multiple independent assays that test for similar phenotypes. As a proof of concept for this idea, we combined the data from Erijman et al. with the results from a similar AD functional assay that tested 5–20 residue peptides (<xref ref-type="bibr" rid="bib40">Ravarani et al., 2018</xref>), trained new PARROT predictors on a variety of dataset sizes, and repeated the analyses described above. We found that 10-fold cross-validation accuracy and AUPRC slightly decreased using the combined datasets, possibly due to greater intra-dataset variance. However, performance on the independent test dataset was not significantly different (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Despite the modest dip in performance for this particular case, we posit that PARROT’s flexibility to incorporate multiple datasets while training could be useful in other contexts where a single, comprehensive dataset is not available.</p><p>As a final set of analyses, we compared our PARROT predictor to a recently published deep learning-based method for activation domain prediction, called <italic>PADDLE,</italic> developed by <xref ref-type="bibr" rid="bib44">Sanborn et al., 2021</xref>. Similar to <italic>ADpred</italic>, <italic>PADDLE</italic> is a deep convolutional neural network and was trained on data derived from a quantitative, high-throughput assay. When applying our PARROT predictor trained on the Erijman et al. data to the Sanborn et al. data., we obtained relatively poor predictive power (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). However, since <italic>ADpred</italic> had also been shown to be ineffective at predicting the data obtained by <xref ref-type="bibr" rid="bib44">Sanborn et al., 2021</xref>, we suspected that PARROT’s underperformance may reflect inherent system-specific limitations in transferability between the two datasets. To test this, we leveraged PARROT’s flexibility and trained a new predictor using the same training data as <italic>PADDLE</italic>. This new predictor saw substantially improved performance and was able to predict activation domain function comparably to <italic>PADDLE</italic>.</p></sec><sec id="s2-4"><title>PARROT can complement DMS experiments</title><p>For our final analysis, we demonstrate a unique usage for PARROT in tandem with DMS experiments. We conducted our training and testing of PARROT networks using a recent DMS dataset investigating amyloid beta (Aß42), a 42-residue peptide that can form plaques implicated in Alzheimer’s disease (<xref ref-type="bibr" rid="bib47">Seuma et al., 2021</xref>; <xref ref-type="bibr" rid="bib18">Findeis, 2007</xref>). In work by Seuma et al., the authors tested &gt;450 single and &gt;14,000 double mutants of Aß42 in an assay that measured each variant’s propensity to nucleate amyloid fibrils. Each of the variants they tested was assigned a log-ratio score (normalized to WT) with positive values indicating that that variant was nucleation-prone. While this scale of this experiment was massive, the sheer combinatorics of DMS makes it infeasible to truly capture all possible single and double mutations for a peptide of this size in a single assay. In our analysis, we show that PARROT can be employed to ‘fill in the gaps’ of a DMS experiment by training on the experimental variants and applying the network to predict the experimental outcome for variants that were not directly assayed.</p><p>We first validated PARROT’s ability to predict nucleation scores from DMS data. Unlike the previous applications, the peptides obtained from DMS experiments occupy a relatively limited region of sequence space given that each sequence differs by only a few point mutations. It was not initially clear to us if PARROT would be able to learn the general, underlying patterns within this more focused dataset rather than overfitting on specific observations. To test this, we set out to rigorously evaluate our PARROT networks by developing and applying a method of <italic>residue-wise cross-validation</italic>. For each cross-validation fold, the held-out test set consisted of the data of all variants (single and double) linked to a particular residue in the sequence, while the training set consisted of all other variants. For example, during the round of cross-validation for the fourth position of Aß42 (Phe-4), variants like F4G, F4S, F4S-H13N, etc., would be excluded from the training data and held in the test set. While this approach to training might seem excessive, it avoids the issue of overfitting that would arise using conventional cross-validation training. For example, if we were to naively divide our DMS data into 10 random subsets, we could have cases where the training set consists of double mutants like F4S-H13N and F4S-V36M while the single mutant F4S is in the held-out test set. In this kind of situation, our predictions would be more accurate, but this would be improperly overestimating PARROT’s performance.</p><p>Using residue-wise cross-validation, we trained and tested PARROT networks for all 42 positions of Aß42, taking the average predictions of double mutants since they were represented in the two separate test sets. Across all of the single and double mutants in the dataset, we see good correlation between PARROT’s predictions and the true assay scores (<italic>R</italic><sup>2</sup> = 0.593; <xref ref-type="fig" rid="fig4">Figure 4B</xref>). To provide context for this value, between multiple biological replicates of the DMS experiments an <italic>R</italic><sup>2</sup> of 0.72 was obtained, indicating to us that PARROT is effectively capturing much of the variation between sequences that are not due to biological noise (<xref ref-type="bibr" rid="bib47">Seuma et al., 2021</xref>). Within our entire set of predictions, the correlation was tighter among the double mutants in the dataset than the single mutants, likely due to the limited information that PARROT sees for the single mutants during training (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>PARROT can ‘fill in the gaps’ of deep mutational scanning experiments.</title><p>(<bold>A</bold>) Depiction of the residue-wise cross-validation workflow for predicting fibril nucleation scores using the Aß42 deep mutational scanning (DMS) assay from Seuma et al. (<bold>B</bold>) Correlation between the true assay scores and predictions made by PARROT networks trained using residue-wise cross-validation for &gt;14,000 single and double mutant variants. (<bold>C</bold>) Measurement of epistasis within the nucleation assay. (Left) Correlation between the nucleation scores of double mutants and the sum of nucleation scores of their composite single mutants. (Right) Correlation between the same double mutant nucleation scores and the predictions made by PARROT. (<bold>D</bold>) Receiver operator characteristic (ROC) curves for 12 familial Alzheimer’s disease (fAD) mutants versus all other single mutant variants in the dataset. Area under the curve (AUC) values are reported in the legend.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70576-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig4">Figure 4B</xref>, correlation between Aß42 nucleation scores and PARROT predictions divided into (<bold>A</bold>) single mutants and (<bold>B</bold>) double mutants.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70576-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Related to <xref ref-type="fig" rid="fig4">Figure 4C</xref>, measured epistasis between Aß42 double mutant nucleation scores and the average (top), maximum (middle), and minimum (bottom) of their composite single mutant scores.</title><p>The PARROT predictions have significantly tighter correlation than any of the other methods (p&lt;0.01).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-70576-fig4-figsupp2-v1.tif"/></fig></fig-group><p>We next sought to see if the PARROT networks could capture epistatic relationships between Aß42 residues in the set of double mutants. In assays that measure complex phenotypes such as the nucleation of amyloid fibrils, it is not clear a priori if independent mutations will work synergistically or antagonistically when combined. For this analysis, we were interested in how well PARROT could predict the impact of double mutations in the DMS dataset relative to simpler estimations, such as summing the assay score of the two single mutations. Looking at only the double mutants in our dataset for which both point mutations were represented in the set of single mutants, we found that PARROT’s predictions significantly outperformed this simple summing approach (p&lt;0.01; <xref ref-type="fig" rid="fig4">Figure 4C</xref>). We also tested PARROT against other approaches for predicting double mutants: averaging the single mutant scores, taking the minimum score, or taking the maximum score, and similarly found that PARROT’s predictions had significantly tighter correlation to the true values (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). While the effect size was relatively small, it is important to note that the PARROT networks making these epistatic predictions are training without key positional information due to the residue-wise cross-validation process. PARROT is not simply integrating information from the two single mutants, but rather it is making predictions based on general patterns it has learned from other variants.</p><p>Lastly, we wanted to see if PARROT was an effective tool for prioritizing untested candidate variants for follow-up study. Since it is infeasible for DMS experiments to test all possible point mutations in the protein sequence, we reasoned that PARROT might be an effective tool for making predictions on the mutants not covered by the assay. To test this idea, we assessed how effectively PARROT prioritized a set of 12 Aß42 variants linked to familial Alzheimer’s disease (fAD) within the entire collection of single mutants. This analysis was analogous to what was performed by Seuma and colleagues in the original DMS study (<xref ref-type="bibr" rid="bib47">Seuma et al., 2021</xref>). In addition to the predictions made by our residue-wise cross-validation networks (<italic>PARROT_ResCV</italic>), we trained an additional network using PARROT on the <italic>entire</italic> DMS dataset minus the 12 fAD-linked single mutants and all double mutants containing one or both of these mutations (<italic>PARROT_nofAD</italic>). We calculated area under the ROC curve (AUROC) to evaluate the predictions of these PARROT networks and compared PARROT’s performance to the original DMS assay and to TANGO (<xref ref-type="bibr" rid="bib17">Fernandez-Escamilla et al., 2004</xref>) and CADD (<xref ref-type="bibr" rid="bib41">Rentzsch et al., 2019</xref>), which are computational predictors of aggregation and variant effect, respectively (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). With the exception of the assay’s scores (which PARROT trained on), <italic>PARROT_nofAD</italic> and <italic>PARROT_ResCV</italic> outperformed all other predictors. In particular, the success of the <italic>PARROT_nofAD</italic> predictor demonstrates that PARROT can effectively ‘fill in the gaps’ of DMS experiments and help prioritize candidate variants for follow-up study. Essentially, researchers can use PARROT to construct their own variant effect predictor that is specific to their assay and protein of interest.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>When designing PARROT, we set out to develop a machine learning tool that effectively extracts patterns from protein sequence data, is generalizable to a wide array of regression and classification tasks, and is easy to use. There are a number of tools in recent years that satisfy some of these criteria, but not all three. For instance, deep learning-based predictors are becoming widely used in protein analysis, but these implementations tend to be designed for a single specific application rather than general use (<xref ref-type="bibr" rid="bib22">Heffernan et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Almagro Armenteros et al., 2017</xref>; <xref ref-type="bibr" rid="bib1">Alipanahi et al., 2015</xref>). Although general protein analysis tools do exist, these typically implement simpler techniques like linear or logistic regression, support vector machines or decision trees, and are not necessarily able to identify complex, nonlinear patterns in datasets (<xref ref-type="bibr" rid="bib8">Brandes et al., 2016</xref>; <xref ref-type="bibr" rid="bib31">Liu, 2019</xref>). Meanwhile, open-source software packages like PyTorch, Keras, and TensorFlow make general deep learning frameworks freely available, but implementing these requires significant computational expertise and time investment. PARROT offers a freely available deep learning tool that satisfies all three of these criteria. By creating a tool that is sufficiently flexible, straightforward, and computationally rigorous, we aim to make the advantages of deep learning accessible to all biologists.</p><p>Importantly, we have demonstrated that predictors built using PARROT perform comparably to existing machine learning predictors across multiple contexts. In the case of phosphorylation site prediction, PHOSFER, PhosphoSVM, and MusiteDeep have all been specifically designed for this task, while PARROT was not. Nonetheless, PARROT still predicts phosphorylation sites approximately equivalently to each of these methods. Likewise, PARROT also performs comparably to both <italic>ADpred</italic> and <italic>PADDLE</italic> after training on the same dataset as either of these predictors. In our analysis of Aß42, we saw that PARROT networks trained on the DMS dataset were more effective at identifying pathogenic, fibril forming variants than computational tools like TANGO or CADD. Collectively, these results demonstrate that PARROT’s flexibility across datasets does not come at the expense of performance. Moreover, while there has been a previous focus on the application of deep learning to understand folded protein stability, PARROT is demonstrably well-suited for working with intrinsically disordered protein sequences (<xref ref-type="bibr" rid="bib2">Alley et al., 2019</xref>; <xref ref-type="bibr" rid="bib9">Cao et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Hoie et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Lindorff-Larsen and Kragelund, 2021</xref>).</p><p>The three specific applications we used to showcase PARROT outline broader use cases in which it can be effective. For starters, PARROT can be used to create predictors from existing bioinformatic datasets; for example, we trained networks to predict phosphosites using the existing P.ELM dataset. Second, PARROT can easily be incorporated into the workflows of high-throughput protein experiments, as shown with the yeast activation domain predictor we created from Erijman et al.’s fluorescence assay data. DMS experiments are a special subset of this kind of usage. Our third example demonstrated how PARROT can train on DMS data and extrapolate predictions on variants that were not experimentally tested. In all three cases, PARROT can save researchers valuable time by eliminating the need to develop machine learning predictors de novo.</p><p>Beyond these applications, there are several other features built into PARROT that may increase its appeal to a wider scientific audience. Trained PARROT networks are fully portable into Python, which allows them to be easily integrated into stand-alone software tools, entirely independent of PARROT. As an example, we recently used PARROT to train a predictor of per-residue intrinsic disorder or predicted structure that offers a number of advantages in terms of performance and ease of use compared to the state of the art (<xref ref-type="bibr" rid="bib14">Emenecker et al., 2021</xref>). Additionally, while PARROT uses one-hot encoding to transform amino acid sequences into machine-readable numeric vectors by default, it can readily adopt other user-specified encoding schemes such as describing amino acids by their biophysical properties. As a consequence of this fact, PARROT is not specific to the canonical amino acid alphabet and can even be applied to nucleotide sequences. All of these features, and much more, are described in detail in the PARROT documentation.</p><p>As a final point, we would like to emphasize to prospective users of PARROT, or any similar tool, that predictions made by machine learning models should be interpreted with caution. Although deep learning methods are powerful at detecting patterns in data, this power also comes with increased susceptibility to overfitting and biased datasets. Proper data processing, not specific model architecture, is arguably the most critical factor for ensuring that deep learning is utilized accurately and meaningfully. While deep learning-based predictions can be instrumental in generating follow-up candidates and developing hypotheses, it is important to remember that these predictions do not replace the need for direct experimental validation.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>LSTM implementation</title><p>PARROT’s underlying bidirectional LSTM network is implemented using the PyTorch library in Python. Input protein sequences are converted to one-hot vectors and grouped into batches (default: 32 sequences per batch), then fed into both the first forward layer and first reverse layer of LSTM cells. By default, PARROT networks consist of two layers of LSTM cells, though this hyperparameter can be manually specified by the user. Information is propagated between adjacent LSTM cells and between layers through hidden state vectors, which can also have a manually specified size (default 10). Hidden state vectors from the final layer of LSTM cells are converted to the final output via a fully connected linear or softmax neuron (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). PARROT uses either a many-to-one or many-to-many architecture depending on whether the machine learning task at hand involves mapping protein sequences to single values (or class labels) or mapping <italic>each residue</italic> to a value/class label. The key implementation difference between these two architectures is in which hidden state vectors of the final layer of LSTM cells are input into the fully connected layer. For residue mapping, the hidden state vectors of the final forward and reverse cells <italic>at each position in the sequence</italic> are integrated into their own final connected layer (<xref ref-type="fig" rid="fig1">Figure 1C</xref><bold>,</bold> gray). In contrast for sequence mapping, only the hidden state vectors from the final forward and final reverse cells are integrated into the fully connected layer (<xref ref-type="fig" rid="fig1">Figure 1C</xref><bold>,</bold> green). For classification tasks, the fully connected layer outputs a vector with a size corresponding to the number of class labels. For regression tasks, this layer outputs a single value.</p><p>During training, weights in PARROT networks are updated using the Adam optimizer (<xref ref-type="bibr" rid="bib27">Kingma and Ba, 2014</xref>). By default, the initial learning rate is set at 0.001. Classification tasks employ a cross-entropy loss function, while regression tasks use L1 and L2 loss functions for sequence mapping and residue mapping tasks, respectively. PARROT splits input datasets 70-15-15 into training, validation, and testing datasets by default; however, these proportions can be manually specified via the ‘--set-fractions’ argument. The validation set is not trained on, but used to assess network performance after each epoch of training. The test set is completely held out until after training has concluded in order to give an estimate for how generalizable the trained network is on unseen data. Approximate training times for different hyperparameters and dataset sizes are listed in <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>. Further implementation details and information on additional run-time arguments can be found in the PARROT documentation.</p></sec><sec id="s4-2"><title>Evaluation metrics</title><p>In binary classification problems, each prediction falls into one of four cases: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). We compared our PARROT networks to other predictors using a variety of performance metrics that describe distribution of predictions across each of these categories. These metrics are calculated in the following ways:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mi> </mml:mi><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mn>0.5</mml:mn><mml:mi> </mml:mi><mml:mi>*</mml:mi><mml:mi> </mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mi> </mml:mi><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mi>*</mml:mi><mml:mi> </mml:mi><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi> </mml:mi><mml:mo>-</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mi>*</mml:mi><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi> </mml:mi><mml:mo>+</mml:mo><mml:mi> </mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:msqrt></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Alternatively, performance on classification tasks can be evaluated using precision-recall or receiver operator characteristic (ROC) curves. Instead of assigning each predicted sequence a discrete class label, sequences are assigned a continuous real number value corresponding to the confidence that it belongs to a particular class. We generated these non-discrete predictions using the optional ‘--probabilistic-classification’ command-line argument and calculated AUPRC and AUROC using the Python package scikit-learn (<xref ref-type="bibr" rid="bib38">Pedregosa et al., 2011</xref>).</p></sec><sec id="s4-3"><title>Phosphosite prediction</title><p>The same P.ELM and PPA datasets were used as by <xref ref-type="bibr" rid="bib12">Dou et al., 2014</xref>, each split into separate phospho-serine, -threonine, and -tyrosine subsets. Initially, sequences with &gt;30% similarity within each subset were removed using CD-HIT with default arguments (<xref ref-type="bibr" rid="bib19">Fu et al., 2012</xref>). We next extracted all 19-residue windows centered around all serine, threonine, and tyrosine residues in each of the respective datasets, dividing these into phosphorylation-positive and phosphorylation-negative sets. A subsequent round of filtering was performed and sequences within these subsets with &gt;20% similarity were removed. We then randomly downsampled the phosphorylation-negative sequences so that their number equaled the phosphorylation-positives and merged the two datasets into a single file for training by PARROT.</p><p>Our analysis proceeded by training and evaluating the networks on the P.ELM dataset using 10-fold cross-validation. The pSer, pThr, and pTyr datasets were each split randomly into 10 equal subsets. The PARROT script <italic>parrot-cvsplit</italic> facilitates this process of splitting a dataset into cross-validation subsets. Using the ‘--split’ flag, PARROT networks were subsequently trained on nine of these sets and the resulting network made predictions for the sequences in the held out test set. These networks were trained using the following arguments: two hidden layers; hidden vector size of 10; learning rate of 0.0001; batch size of 64; 500 training epochs. The reported performance metrics in <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="supplementary-material" rid="supp1 supp2">Supplementary files 1 and 2</xref> denote the average scores across the 10 cross-validation test sets. Predictions were also made by PHOSFER and MusiteDeep through their online web server on each of the cross-validation test sets and performance metrics were averaged. However, we opted not to test PhosphoSVM in this manner since this predictor was originally trained on the same P.ELM data and we wanted to avoid overfitting. Instead, we report the performance metrics taken directly from Dou et al. since these were calculated using a similar strategy of 10-fold cross-validation on the P.ELM dataset (<xref ref-type="bibr" rid="bib12">Dou et al., 2014</xref>).</p><p>Using the same training arguments, additional networks were trained on the full P.ELM dataset (separately for pSer, pThr, and pTyr) and used to make predictions on the PPA dataset. Predictions were also made by PHOSFER, MusiteDeep on the same PPA data, and performance metrics were calculated for each of these sets of predictions. As with the P.ELM data, the performance metrics of PhosphoSVM on the PPA data were taken directly from Dou et al.</p></sec><sec id="s4-4"><title>Activation domain function prediction</title><p>The quantitative fluorescence assay data of Erijman et al. was collected and processed in a manner identical to its source paper (<xref ref-type="bibr" rid="bib16">Erijman et al., 2020</xref>). Briefly, each 30-mer was assigned a real number score based on its distribution of reads across four fluorescence expression bins. These sequences were split into AD-positive and AD-negative sets and the negative set was sampled such that there were equal numbers of positive and negative sequences in the final dataset. This sampling process was repeated five times for the ‘full’ dataset (75,846 sequences), as well as for each of the reduced datasets (70K sequences, 60K sequences, etc.) in order to generate additional replicates.</p><p>Each dataset was split randomly into 10 cross-validation subsets, and PARROT networks were subsequently trained on nine and tested on the held-out subset. PARROT networks were trained using the following hyperparameters: two hidden layers; hidden vector size of 10; learning rate of 0.0005; batch size of 64; 300 training epochs. Although our input data was set up as a classification task, by using the ‘--probabilistic-classification’ argument, all of our predictions were output as real numbers between 0 and 1, which allowed us to conduct precision-recall curve analysis. In addition to assessing the performance on the held-out test set, each network was also used to make predictions on an independent dataset. This independent dataset was obtained from a similar yeast AD quantitative fluorescence assay from <xref ref-type="bibr" rid="bib48">Staller et al., 2018</xref>. We calculated the normalized expression value for each sequence in this dataset by dividing the raw AD activity (GFP) by the protein expression level (mCherry), and log-normalizing the data around the WT sequence. The performance metrics reported in <xref ref-type="fig" rid="fig3">Figure 3</xref> are the averages of 50 total replicates (five replicate datasets with 10-fold cross-validation for each).</p><p>We also created a combined training dataset using the results from a similar AD functional assay in <xref ref-type="bibr" rid="bib40">Ravarani et al., 2018</xref>. We extracted all sequences from this assay that were at least five residues in length and split into positive and negative sets as described using a cutoff of –0.14. These AD-positive and -negative sequences were then merged with the full Erijman et al. dataset, and PARROT networks were trained and evaluated in the same manner as before.</p><p>To perform comparisons against PADDLE (<xref ref-type="bibr" rid="bib44">Sanborn et al., 2021</xref>), we extracted the activation assay data from Sanborn et al. and split into training and test sets as specified by the ‘PADDLE split’ column. A PARROT regressive model was trained on the full training set using the following hyperparameters: two hidden layers; hidden vector size of twenty; learning rate of 0.001; batch size of 64; 300 training epochs. Predictions were made on all of the test set sequences with this new network, as well as with the PARROT predictor that trained on the Erijman et al. data. Sequences in the test set that belonged to the transcription factor tiling, scramble mutant, and Pdr1 variant subsets were split and graphed separately.</p></sec><sec id="s4-5"><title>Aß42 nucleation prediction</title><p>Data linking Aß42 nucleation propensity to sequence was obtained from <xref ref-type="bibr" rid="bib47">Seuma et al., 2021</xref>. Each single or double mutant variant was assigned a log-normalized (relative to WT) score with positive values reflecting that a variant is more prone to nucleating amyloid fibrils. For simplicity, we removed all nonsense variants from the dataset prior to training. The remaining variants were split into 42 different training-test set pairs, based on the position of the mutation(s) in that variant. Each test set contained all variants with mutations associated with a single residue, while the training sets consisted of all remaining variants. Accordingly, each double mutant was withheld in two separate test sets. Individual PARROT networks were trained on each of these unique training sets and the resulting network was used to make predictions on the corresponding test set. Networks were trained using the following hyperparameters: 3 hidden layers, hidden vector of size 8; learning rate of 0.0005; batch size of 64; and 250 training epochs. Predictions from the 42 test sets were combined, averaged (in the case of double mutants), and then analyzed.</p><p>We assessed the ability of PARROT to detect ‘epistasis’ by comparing the network’s prediction of double mutants to simpler approaches that estimated mutant effect by integrating nucleation scores of the associative single mutations. We determined statistical significance between correlations derived from these different approaches through bootstrapping. All data points were resampled with replacement 10,000 times, calculating Pearson’s <italic>R</italic> for each iteration, and the 99% confidence intervals were used as a threshold for significance (p&lt;0.01).</p><p>The 12 fAD-linked variants that we analyzed were H6R, D7N, D7H, E11K, K16N, A21G, E22G, E22K, E22Q, D23N, L34V, and A42T. <italic>PARROT_ResCV</italic> and <italic>PARROT_nofAD</italic> predictions for all single mutants were ordered in order to create ROC curves. The CADD and TANGO predictions used for ROC analysis were also obtained from Seuma et al. as they performed an identical analysis on this set of 12 variants.</p></sec><sec id="s4-6"><title>Implementation</title><p>The complete PARROT implementation consists of four command-line commands: <italic>parrot-train</italic>, <italic>parrot-predict, parrot-optimize and parrot-cvsplit</italic>. For the analysis described here, <italic>parrot-train</italic> was used to train the RNN predictors given a properly formatted dataset and <italic>parrot-predict</italic> was used to make predictions on new sequences using an existing trained network. We did not use <italic>parrot-optimize</italic> in these analyses, but can be used to automatically select network hyperparameters through Gaussian process optimization. <italic>parrot-cvsplit</italic> allows users to automatically split their datasets into k-folds for cross-validation. More details can be found in the PARROT documentation: <ext-link ext-link-type="uri" xlink:href="https://idptools-parrot.readthedocs.io/">https://idptools-parrot.readthedocs.io/</ext-link>. PARROT is optimized to run in a Mac or Linux environment, but can also work using Windows.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>none</p></fn><fn fn-type="COI-statement" id="conf2"><p>is a scientific consultant with Dewpoint Therapeutics</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Funding acquisition, Investigation, Methodology, Software, Supervision, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Complete table of performance metrics for phosphosite predictions on the Phospho.ELM (P.ELM) dataset.</title><p>Standard error, whenever possible, is reported in parentheses.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-70576-supp1-v1.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Complete table of performance metrics for phosphosite predictions on the PhosPhAt (PPA) datasets.</title><p>Standard error, whenever possible, is reported in parentheses.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-70576-supp2-v1.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Average PARROT network training times on different sizes of datasets and with variable hyperparameters.</title><p>Datasets were created by assigning random values in [–5, 5] to randomly generated protein sequences ~25–35 residues in length. Networks were trained using a NVIDIA TU116 GPU. Three replicate PARROT networks were trained for each specified set of hyperparameters and dataset.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-70576-supp3-v1.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-70576-transrepform1-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All code is fully open source and available here: <ext-link ext-link-type="uri" xlink:href="https://github.com/idptools/parrot">https://github.com/idptools/parrot</ext-link>. Documentation is available here: <ext-link ext-link-type="uri" xlink:href="https://idptools-parrot.readthedocs.io/">https://idptools-parrot.readthedocs.io/</ext-link>. Additional supporting data available here: <ext-link ext-link-type="uri" xlink:href="https://github.com/holehouse-lab/supportingdata/tree/master/2021/griffith_parrot_2021">https://github.com/holehouse-lab/supportingdata/tree/master/2021/griffith_parrot_2021</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:4bb48369891dc4416b6b176046846091d8cd9ddb">https://archive.softwareheritage.org/swh:1:rev:4bb48369891dc4416b6b176046846091d8cd9ddb</ext-link>). PhosPhat was taken from <ext-link ext-link-type="uri" xlink:href="http://phosphat.uni-hohenheim.de">http://phosphat.uni-hohenheim.de</ext-link> (specifically Phosphat_20200624.csv), while data for PhosphoElm where taken from <ext-link ext-link-type="uri" xlink:href="http://phospho.elm.eu.org/">http://phospho.elm.eu.org/</ext-link>. In both cases the entire dataset available at the time of analysis was used.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank the members of the Holehouse lab for helpful discussions and feedback. Special thanks to Shubhanjali Minhas for designing the PARROT logo. Funding for this work was provided by the National Science Foundation grant number DGE-2139839 and the Longer Life Foundation (an RGA/Washington University collaboration).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alipanahi</surname><given-names>B</given-names></name><name><surname>Delong</surname><given-names>A</given-names></name><name><surname>Weirauch</surname><given-names>MT</given-names></name><name><surname>Frey</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title><source>Nature Biotechnology</source><volume>33</volume><fpage>831</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id><pub-id pub-id-type="pmid">26213851</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alley</surname><given-names>EC</given-names></name><name><surname>Khimulya</surname><given-names>G</given-names></name><name><surname>Biswas</surname><given-names>S</given-names></name><name><surname>AlQuraishi</surname><given-names>M</given-names></name><name><surname>Church</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title><source>Nature Methods</source><volume>16</volume><fpage>1315</fpage><lpage>1322</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0598-1</pub-id><pub-id pub-id-type="pmid">31636460</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Almagro Armenteros</surname><given-names>JJ</given-names></name><name><surname>Sønderby</surname><given-names>CK</given-names></name><name><surname>Sønderby</surname><given-names>SK</given-names></name><name><surname>Nielsen</surname><given-names>H</given-names></name><name><surname>Winther</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>DeepLoc: prediction of protein subcellular localization using deep learning</article-title><source>Bioinformatics</source><volume>33</volume><fpage>3387</fpage><lpage>3395</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btx431</pub-id><pub-id pub-id-type="pmid">29036616</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angermueller</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>HJ</given-names></name><name><surname>Reik</surname><given-names>W</given-names></name><name><surname>Stegle</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning</article-title><source>Genome Biology</source><volume>18</volume><elocation-id>67</elocation-id><pub-id pub-id-type="doi">10.1186/s13059-017-1189-z</pub-id><pub-id pub-id-type="pmid">28395661</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnold</surname><given-names>CD</given-names></name><name><surname>Nemčko</surname><given-names>F</given-names></name><name><surname>Woodfin</surname><given-names>AR</given-names></name><name><surname>Wienerroither</surname><given-names>S</given-names></name><name><surname>Vlasova</surname><given-names>A</given-names></name><name><surname>Schleiffer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A high‐throughput method to identify trans‐activation domains within transcription factor sequences</article-title><source>The EMBO Journal</source><volume>37</volume><elocation-id>e98896</elocation-id><pub-id pub-id-type="doi">10.15252/embj.201798896</pub-id><pub-id pub-id-type="pmid">30006452</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Simard</surname><given-names>P</given-names></name><name><surname>Frasconi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Learning long-term dependencies with gradient descent is difficult</article-title><source>IEEE Transactions on Neural Networks</source><volume>5</volume><fpage>157</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1109/72.279181</pub-id><pub-id pub-id-type="pmid">18267787</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolognesi</surname><given-names>B</given-names></name><name><surname>Faure</surname><given-names>AJ</given-names></name><name><surname>Seuma</surname><given-names>M</given-names></name><name><surname>Schmiedel</surname><given-names>JM</given-names></name><name><surname>Tartaglia</surname><given-names>GG</given-names></name><name><surname>Lehner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The mutational landscape of a prion-like domain</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>4162</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-12101-z</pub-id><pub-id pub-id-type="pmid">31519910</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandes</surname><given-names>N</given-names></name><name><surname>Ofer</surname><given-names>D</given-names></name><name><surname>Linial</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>ASAP: a machine learning framework for local protein properties</article-title><source>Database</source><volume>2016</volume><elocation-id>baw133</elocation-id><pub-id pub-id-type="doi">10.1093/database/baw133</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>L</given-names></name><name><surname>Qi</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepDDG: Predicting the Stability Change of Protein Point Mutations Using Neural Networks</article-title><source>Journal of Chemical Information and Modeling</source><volume>59</volume><fpage>1508</fpage><lpage>1514</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.8b00697</pub-id><pub-id pub-id-type="pmid">30759982</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chicco</surname><given-names>D</given-names></name><name><surname>Jurman</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</article-title><source>BMC Genomics</source><volume>21</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.1186/s12864-019-6413-7</pub-id><pub-id pub-id-type="pmid">31898477</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diella</surname><given-names>F</given-names></name><name><surname>Gould</surname><given-names>CM</given-names></name><name><surname>Chica</surname><given-names>C</given-names></name><name><surname>Via</surname><given-names>A</given-names></name><name><surname>Gibson</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Phospho.Elm: A database of phosphorylation sites—update 2008</article-title><source>Nucleic Acids Research</source><volume>36</volume><fpage>D240</fpage><lpage>D244</lpage><pub-id pub-id-type="doi">10.1093/nar/gkm772</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dou</surname><given-names>Y</given-names></name><name><surname>Yao</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>PhosphoSVM: prediction of phosphorylation sites by integrating various protein sequence attributes with a support vector machine</article-title><source>Amino Acids</source><volume>46</volume><fpage>1459</fpage><lpage>1469</lpage><pub-id pub-id-type="doi">10.1007/s00726-014-1711-5</pub-id><pub-id pub-id-type="pmid">24623121</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durek</surname><given-names>P</given-names></name><name><surname>Schmidt</surname><given-names>R</given-names></name><name><surname>Heazlewood</surname><given-names>JL</given-names></name><name><surname>Jones</surname><given-names>A</given-names></name><name><surname>MacLean</surname><given-names>D</given-names></name><name><surname>Nagel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>PhosPhAt: the <italic>Arabidopsis thaliana</italic> phosphorylation site database</article-title><source>An Update. Nucleic Acids Res</source><volume>38</volume><fpage>D828</fpage><lpage>D834</lpage><pub-id pub-id-type="doi">10.1093/nar/gkp810</pub-id><pub-id pub-id-type="pmid">19880383</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Emenecker</surname><given-names>RJ</given-names></name><name><surname>Griffith</surname><given-names>D</given-names></name><name><surname>Holehouse</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Metapredict: A Fast, Accurate, and Easy-to-Use Cross-Platform Predictor of Consensus Disorder</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.05.30.446349</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eraslan</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning: new computational modelling techniques for genomics</article-title><source>Nature Reviews. Genetics</source><volume>20</volume><fpage>389</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1038/s41576-019-0122-6</pub-id><pub-id pub-id-type="pmid">30971806</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erijman</surname><given-names>A</given-names></name><name><surname>Kozlowski</surname><given-names>L</given-names></name><name><surname>Sohrabi-Jahromi</surname><given-names>S</given-names></name><name><surname>Fishburn</surname><given-names>J</given-names></name><name><surname>Warfield</surname><given-names>L</given-names></name><name><surname>Schreiber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A High-Throughput Screen for Transcription Activation Domains Reveals Their Sequence Features and Permits Prediction by Deep Learning</article-title><source>Molecular Cell</source><volume>78</volume><fpage>890</fpage><lpage>902</lpage><pub-id pub-id-type="doi">10.1016/j.molcel.2020.04.020</pub-id><pub-id pub-id-type="pmid">32416068</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez-Escamilla</surname><given-names>AM</given-names></name><name><surname>Rousseau</surname><given-names>F</given-names></name><name><surname>Schymkowitz</surname><given-names>J</given-names></name><name><surname>Serrano</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Prediction of sequence-dependent and mutational effects on the aggregation of peptides and proteins</article-title><source>Nature Biotechnology</source><volume>22</volume><fpage>1302</fpage><lpage>1306</lpage><pub-id pub-id-type="doi">10.1038/nbt1012</pub-id><pub-id pub-id-type="pmid">15361882</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Findeis</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The role of amyloid beta peptide 42 in Alzheimer’s disease</article-title><source>Pharmacology &amp; Therapeutics</source><volume>116</volume><fpage>266</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1016/j.pharmthera.2007.06.006</pub-id><pub-id pub-id-type="pmid">17716740</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>L</given-names></name><name><surname>Niu</surname><given-names>B</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title><source>Bioinformatics</source><volume>28</volume><fpage>3150</fpage><lpage>3152</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id><pub-id pub-id-type="pmid">23060610</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanson</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Paliwal</surname><given-names>K</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Improving protein disorder prediction by deep bidirectional long short-term memory recurrent neural networks</article-title><source>Bioinformatics</source><volume>33</volume><fpage>685</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btw678</pub-id><pub-id pub-id-type="pmid">28011771</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heazlewood</surname><given-names>JL</given-names></name><name><surname>Durek</surname><given-names>P</given-names></name><name><surname>Hummel</surname><given-names>J</given-names></name><name><surname>Selbig</surname><given-names>J</given-names></name><name><surname>Weckwerth</surname><given-names>W</given-names></name><name><surname>Walther</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>PhosPhAt: a database of phosphorylation sites in <italic>Arabidopsis thaliana</italic> and a plant-specific phosphorylation site predictor</article-title><source>Nucleic Acids Research</source><volume>36</volume><fpage>D1015</fpage><lpage>D1021</lpage><pub-id pub-id-type="doi">10.1093/nar/gkm812</pub-id><pub-id pub-id-type="pmid">17984086</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffernan</surname><given-names>R</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Paliwal</surname><given-names>K</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Capturing non-local interactions by long short-term memory bidirectional recurrent neural networks for improving prediction of protein secondary structure, backbone angles, contact numbers and solvent accessibility</article-title><source>Bioinformatics</source><volume>33</volume><fpage>2842</fpage><lpage>2849</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btx218</pub-id><pub-id pub-id-type="pmid">28430949</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Long short-term memory</article-title><source>Neural Computation</source><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hoie</surname><given-names>MH</given-names></name><name><surname>Cagiada</surname><given-names>M</given-names></name><name><surname>Frederiksen</surname><given-names>AHB</given-names></name><name><surname>Stein</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Predicting and Interpreting Large Scale Mutagenesis Data Using Analyses of Protein Stability and Conservation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.06.26.450037</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hughes</surname><given-names>RA</given-names></name><name><surname>Ellington</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Synthetic DNA Synthesis and Assembly: Putting the Synthetic in Synthetic Biology</article-title><source>Cold Spring Harbor Perspectives in Biology</source><volume>9</volume><elocation-id>ea023812</elocation-id><pub-id pub-id-type="doi">10.1101/cshperspect.a023812</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>EM</given-names></name><name><surname>Lubock</surname><given-names>NB</given-names></name><name><surname>Venkatakrishnan</surname><given-names>AJ</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Tseng</surname><given-names>AM</given-names></name><name><surname>Paggi</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Structural and functional characterization of G protein-coupled receptors with deep mutational scanning</article-title><source>eLife</source><volume>9</volume><elocation-id>e54895</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54895</pub-id><pub-id pub-id-type="pmid">33084570</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Protein remote homology detection based on bidirectional long short-term memory</article-title><source>BMC Bioinformatics</source><volume>18</volume><elocation-id>443</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-017-1842-2</pub-id><pub-id pub-id-type="pmid">29017445</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindorff-Larsen</surname><given-names>K</given-names></name><name><surname>Kragelund</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>On the potential of machine learning to examine the relationship between sequence, structure, dynamics and function of intrinsically disordered proteins</article-title><source>Journal of Molecular Biology</source><volume>10</volume><elocation-id>167196</elocation-id><pub-id pub-id-type="doi">10.1016/j.jmb.2021.167196</pub-id><pub-id pub-id-type="pmid">34390736</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lipton</surname><given-names>ZC</given-names></name><name><surname>Berkowitz</surname><given-names>J</given-names></name><name><surname>Elkan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A Critical Review of Recurrent Neural Networks for Sequence Learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.00019">https://arxiv.org/abs/1506.00019</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>BioSeq-Analysis: a platform for DNA, RNA and protein sequence analysis based on machine learning approaches</article-title><source>Brief Bioinform</source><volume>20</volume><fpage>1280</fpage><lpage>1294</lpage><pub-id pub-id-type="doi">10.1093/bib/bbx165</pub-id><pub-id pub-id-type="pmid">29272359</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livesey</surname><given-names>BJ</given-names></name><name><surname>Marsh</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Using deep mutational scanning to benchmark variant effect predictors and identify disease mutations</article-title><source>Molecular Systems Biology</source><volume>16</volume><elocation-id>e9380</elocation-id><pub-id pub-id-type="doi">10.15252/msb.20199380</pub-id><pub-id pub-id-type="pmid">32627955</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marx</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The big challenges of big data</article-title><source>Nature</source><volume>498</volume><fpage>255</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1038/498255a</pub-id><pub-id pub-id-type="pmid">23765498</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Min</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>B</given-names></name><name><surname>Yoon</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deep learning in bioinformatics</article-title><source>Brief Bioinform</source><volume>18</volume><fpage>851</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1093/bib/bbw068</pub-id><pub-id pub-id-type="pmid">27473064</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moses</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Statistical Modeling and Machine Learning for Molecular Biology</source><publisher-name>CRC Press</publisher-name><pub-id pub-id-type="doi">10.1201/9781315372266</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murdoch</surname><given-names>WJ</given-names></name><name><surname>Singh</surname><given-names>C</given-names></name><name><surname>Kumbier</surname><given-names>K</given-names></name><name><surname>Abbasi-Asl</surname><given-names>R</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Definitions, methods, and applications in interpretable machine learning</article-title><source>PNAS</source><volume>116</volume><fpage>22071</fpage><lpage>22080</lpage><pub-id pub-id-type="doi">10.1073/pnas.1900654116</pub-id><pub-id pub-id-type="pmid">31619572</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: Machine Learning in Python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raimondi</surname><given-names>D</given-names></name><name><surname>Orlando</surname><given-names>G</given-names></name><name><surname>Vranken</surname><given-names>WF</given-names></name><name><surname>Moreau</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Exploring the limitations of biophysical propensity scales coupled with machine learning for protein sequence analysis</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>16932</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-53324-w</pub-id><pub-id pub-id-type="pmid">31729443</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravarani</surname><given-names>CN</given-names></name><name><surname>Erkina</surname><given-names>TY</given-names></name><name><surname>De Baets</surname><given-names>G</given-names></name><name><surname>Dudman</surname><given-names>DC</given-names></name><name><surname>Erkine</surname><given-names>AM</given-names></name><name><surname>Babu</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High-throughput discovery of functional disordered regions: Investigation of transactivation domains</article-title><source>Molecular Systems Biology</source><volume>14</volume><elocation-id>e8190</elocation-id><pub-id pub-id-type="doi">10.15252/msb.20188190</pub-id><pub-id pub-id-type="pmid">29759983</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rentzsch</surname><given-names>P</given-names></name><name><surname>Witten</surname><given-names>D</given-names></name><name><surname>Cooper</surname><given-names>GM</given-names></name><name><surname>Shendure</surname><given-names>J</given-names></name><name><surname>Kircher</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>CADD: predicting the deleteriousness of variants throughout the human genome</article-title><source>Nucleic Acids Research</source><volume>47</volume><fpage>D886</fpage><lpage>D894</lpage><pub-id pub-id-type="doi">10.1093/nar/gky1016</pub-id><pub-id pub-id-type="pmid">30371827</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</article-title><source>Nature Machine Intelligence</source><volume>1</volume><fpage>206</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1038/s42256-019-0048-x</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Learning representations by back-propagating errors</article-title><source>Nature</source><volume>323</volume><fpage>533</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1038/323533a0</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanborn</surname><given-names>AL</given-names></name><name><surname>Yeh</surname><given-names>BT</given-names></name><name><surname>Feigerle</surname><given-names>JT</given-names></name><name><surname>Hao</surname><given-names>CV</given-names></name><name><surname>Townshend</surname><given-names>RJL</given-names></name><name><surname>Lieberman-Aiden</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Simple biochemical features underlie transcriptional activation domain diversity and dynamic, fuzzy binding to Mediator</article-title><source>eLife</source><volume>10</volume><elocation-id>e68068</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.68068</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmiedel</surname><given-names>JM</given-names></name><name><surname>Lehner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Determining protein structures using deep mutagenesis</article-title><source>Nature Genetics</source><volume>51</volume><fpage>1177</fpage><lpage>1186</lpage><pub-id pub-id-type="doi">10.1038/s41588-019-0431-x</pub-id><pub-id pub-id-type="pmid">31209395</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuster</surname><given-names>M</given-names></name><name><surname>Paliwal</surname><given-names>KK</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Bidirectional recurrent neural networks</article-title><source>IEEE Transactions on Signal Processing</source><volume>45</volume><fpage>2673</fpage><lpage>2681</lpage><pub-id pub-id-type="doi">10.1109/78.650093</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seuma</surname><given-names>M</given-names></name><name><surname>Faure</surname><given-names>A</given-names></name><name><surname>Badia</surname><given-names>M</given-names></name><name><surname>Lehner</surname><given-names>B</given-names></name><name><surname>Bolognesi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The genetic landscape for amyloid beta fibril nucleation accurately discriminates familial Alzheimer’s disease mutations</article-title><source>eLife</source><volume>10</volume><elocation-id>e63364</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63364</pub-id><pub-id pub-id-type="pmid">33522485</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staller</surname><given-names>MV</given-names></name><name><surname>Holehouse</surname><given-names>AS</given-names></name><name><surname>Swain-Lenz</surname><given-names>D</given-names></name><name><surname>Das</surname><given-names>RK</given-names></name><name><surname>Pappu</surname><given-names>RV</given-names></name><name><surname>Cohen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A High-Throughput Mutational Scan of an Intrinsically Disordered Acidic Transcriptional Activation Domain</article-title><source>Cell Systems</source><volume>6</volume><fpage>444</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2018.01.015</pub-id><pub-id pub-id-type="pmid">29525204</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trost</surname><given-names>B</given-names></name><name><surname>Kusalik</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Computational phosphorylation site prediction in plants using random forests and organism-specific instance weights</article-title><source>Bioinformatics</source><volume>29</volume><fpage>686</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt031</pub-id><pub-id pub-id-type="pmid">23341503</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Zeng</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>C</given-names></name><name><surname>Qiu</surname><given-names>W</given-names></name><name><surname>Liang</surname><given-names>Y</given-names></name><name><surname>Joshi</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>MusiteDeep: a deep-learning framework for general and kinase-specific phosphorylation site prediction</article-title><source>Bioinformatics</source><volume>33</volume><fpage>3909</fpage><lpage>3916</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btx496</pub-id><pub-id pub-id-type="pmid">29036382</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>C</given-names></name><name><surname>Jackson</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Machine learning and complex biological data</article-title><source>Genome Biology</source><volume>20</volume><elocation-id>76</elocation-id><pub-id pub-id-type="doi">10.1186/s13059-019-1689-0</pub-id><pub-id pub-id-type="pmid">30992073</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Verma</surname><given-names>D</given-names></name><name><surname>Sheridan</surname><given-names>RP</given-names></name><name><surname>Liaw</surname><given-names>A</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Marshall</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep Dive into Machine Learning Models for Protein Engineering</article-title><source>Journal of Chemical Information and Modeling</source><volume>60</volume><fpage>2773</fpage><lpage>2790</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.0c00073</pub-id><pub-id pub-id-type="pmid">32250622</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70576.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role>Reviewing Editor</role><aff><institution>Goethe University</institution><country>Germany</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Dötsch</surname><given-names>Volker</given-names></name><role>Reviewer</role><aff><institution>Goethe University</institution><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.05.21.445045">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.05.21.445045v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The analysis of large data sets obtained from omics or other approaches is often the most time consuming and difficult step of a study. Deep learning and related computational approaches offer the possibility to train a software on a certain data set and then analyze large new experimental data sets. The authors describe the software architecture and demonstrate the application of the system on three different topics: prediction of phosphorylation, prediction of transactivation potential of peptides and prediction of aggregation propensity. They compare the results of their new software PARROT with other existing software tools.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;PARROT: a flexible recurrent neural network framework for analysis of large protein datasets&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, including Volker Dötsch as the Senior and Reviewing Editor and Reviewer #2.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) A suggestion would be to provide a bit more information about how non-experts can validate the results from PARROT, both in the documentation and main text. Because the authors are targeting non machine learning experts--and providing many useful defaults--they should think about how a non-expert might use the tool. Would a non-expert know when to use a ROC curve versus some other metric? Obviously, the authors should not write a ML textbook here, but think carefully through ways to guide users to appropriate tests.</p><p>Two ideas:</p><p>1) Have PARROT spit out a stack of test results by default, rather than the few it defaults to now. You could put this behavior under a flag (e.g. --<monospace>testsoff</monospace>) so more advanced users would not get bombarded by spew, but otherwise confront the user with as many validation metrics as possible.</p><p>2) Encode some warning heuristics for common errors. For example, PARROT could warn the naive users that their training set was unbalanced.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.70576.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) A suggestion would be to provide a bit more information about how non-experts can validate the results from PARROT, both in the documentation and main text. Because the authors are targeting non machine learning experts--and providing many useful defaults--they should think about how a non-expert might use the tool. Would a non-expert know when to use a ROC curve versus some other metric? Obviously, the authors should not write a ML textbook here, but think carefully through ways to guide users to appropriate tests.</p><p>Two ideas:</p><p>1) Have PARROT spit out a stack of test results by default, rather than the few it defaults to now. You could put this behavior under a flag (e.g. --<monospace>testsoff</monospace>) so more advanced users would not get bombarded by spew, but otherwise confront the user with as many validation metrics as possible.</p><p>2) Encode some warning heuristics for common errors. For example, PARROT could warn the naive users that their training set was unbalanced.</p></disp-quote><p>We thank the reviewers for their thoughtful and constructive suggestions. As PARROT is ultimately designed with the non-ML expert in mind, we agree that we could provide more features and information to help naive users. To address this, we made extensive changes that we hope will be helpful for all prospective users of PARROT. We also made several smaller, quality-of-life changes in the newest version of PARROT. The major changes we made include:</p><p>1. Streamlining k-fold cross-validation</p><p>Since cross-validation is one of the most widely-used forms of validation in machine learning (including in our analyses for this manuscript), we wanted to facilitate this process in PARROT and provide examples to show users how to carry it out themselves. In particular, we added the script <italic>parrot-cvsplit</italic> into the PARROT distribution. This short script will randomly and automatically divide a dataset into a user-specified number of folds for training/testing a network. To demonstrate this, we have provided a complete example of 10-fold cross-validation in the documentation under the page “Evaluating a Network with Cross-Validation”. We have also provided more details on the general principles of validating trained networks on the “Machine Learning Resources” page (under “How can I validate that my trained network is performing well?”)</p><p>2. Providing users with more performance stats</p><p>As the reviewers recommended, we also increased the amount of information that PARROT provides to users regarding the performance of their trained network on the test set data. Previous iterations of PARROT only provided a single file containing the true and predicted values for each sequence in the test set and optional performance figures. The newest version of PARROT provides this information along with an additional “performance stats” file that describes network performance on a variety of metrics. A brief description of each of these metrics, and links to more detailed resources, are provided on the “Machine Learning Resources” page (under “What are the different performance metrics for evaluating ML networks?”). We hope the combination of these metrics and cogent, simple explanations of their meaning will help users better assess their trained models.</p><p>3. Providing users with warning heuristics</p><p>Like the reviewers suggested, we also added a few different warnings that inform users if their dataset does not meet particular heuristics. These warnings check for class imbalance (if classification task), dataset skew/imbalance or non-standardized data (if regression task), duplicate sequences, and minibatch size compared to dataset size.</p><p>4. Documentation overhaul</p><p>In concert with all of the major and minor code changes, we completely overhauled PARROT’s documentation (<ext-link ext-link-type="uri" xlink:href="https://idptools-parrot.readthedocs.io">https://idptools-parrot.readthedocs.io</ext-link>). The new docs reflect all of the additional features that have been added to PARROT. Additionally, we reworked the examples so that it is more organized and accessible. Now the examples are split into a “Basic Examples” and “Advanced Examples” page, and, as mentioned above, we added a full walkthrough of a cross-validation example.</p></body></sub-article></article>