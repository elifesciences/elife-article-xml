<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">92892</article-id><article-id pub-id-type="doi">10.7554/eLife.92892</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.92892.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The neural correlates of novelty and variability in human decision-making under an active inference framework</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Shuo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0002-1303-793X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Tian</surname><given-names>Yan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Liu</surname><given-names>Quanying</given-names></name><email>liuqy@sustech.edu.cn</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="other" rid="fund10"/><xref ref-type="other" rid="fund11"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Wu</surname><given-names>Haiyan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8869-6636</contrib-id><email>haiyanwu@um.edu.mo</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund12"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01r4q9n85</institution-id><institution>Centre for Cognitive and Brain Sciences and Department of Psychology, University of Macau</institution></institution-wrap><addr-line><named-content content-type="city">Macau</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/049tv2d57</institution-id><institution>Department of Biomedical Engineering, Southern University of Science and Technology</institution></institution-wrap><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Chait</surname><given-names>Maria</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>21</day><month>03</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP92892</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-10-17"><day>17</day><month>10</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-09-18"><day>18</day><month>09</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.09.18.558250"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-22"><day>22</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92892.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-08-23"><day>23</day><month>08</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92892.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-02-28"><day>28</day><month>02</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.92892.3"/></event></pub-history><permissions><copyright-statement>Â© 2024, Zhang et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Zhang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-92892-v1.pdf"/><abstract><p>Active inference integrates perception, decision-making, and learning into a united theoretical framework, providing an efficient way to trade off exploration and exploitation by minimizing (expected) free energy. In this study, we asked how the brain represents values and uncertainties (novelty and variability), and resolves these uncertainties under the active inference framework in the exploration-exploitation trade-off. Twenty-five participants performed a contextual two-armed bandit task, with electroencephalogram (EEG) recordings. By comparing the model evidence for active inference and reinforcement learning models of choice behavior, we show that active inference better explains human decision-making under novelty and variability, which entails exploration or information seeking. The EEG sensor-level results show that the activity in the frontal, central, and parietal regions is associated with novelty, while the activity in the frontal and central brain regions is associated with variability. The EEG source-level results indicate that the expected free energy is encoded in the frontal pole and middle frontal gyrus and uncertainties are encoded in different brain regions but with overlap. Our study dissociates the expected free energy and uncertainties in active inference theory and their neural correlates, speaking to the construct validity of active inference in characterizing cognitive processes of human decisions. It provides behavioral and neural evidence of active inference in decision processes and insights into the neural mechanism of human decisions under uncertainties.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>active inference</kwd><kwd>the exploration-exploitation trade-off</kwd><kwd>uncertainty</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>The Science and Technology Development Fund (FDCT) of Macau</institution></institution-wrap></funding-source><award-id>FDCT 0127/2020/A3</award-id><principal-award-recipient><name><surname>Wu</surname><given-names>Haiyan</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003453</institution-id><institution>The Natural Science Foundation of Guangdong Province</institution></institution-wrap></funding-source><award-id>2021A1515012509</award-id><principal-award-recipient><name><surname>Wu</surname><given-names>Haiyan</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Shenzhen-Hong Kong-Macao Science and Technology Innovation Project</institution></institution-wrap></funding-source><award-id>SGDX2020110309280100</award-id><principal-award-recipient><name><surname>Wu</surname><given-names>Haiyan</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>MYRG of University of Macau</institution></institution-wrap></funding-source><award-id>MYRG2022-00188-ICI</award-id><principal-award-recipient><name><surname>Wu</surname><given-names>Haiyan</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>NSFC-FDCT Joint Program</institution></institution-wrap></funding-source><award-id>0095/2022/AFJ</award-id><principal-award-recipient><name><surname>Wu</surname><given-names>Haiyan</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>The SRG of University of Macau</institution></institution-wrap></funding-source><award-id>SRG202000027-ICI</award-id><principal-award-recipient><name><surname>Wu</surname><given-names>Haiyan</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012166</institution-id><institution>The National Key R&amp;D Program of China</institution></institution-wrap></funding-source><award-id>2021YFF1200804</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Quanying</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>62001205</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Quanying</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution>Shenzhen Science and Technology Innovation Committee</institution></institution-wrap></funding-source><award-id>2022410129</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Quanying</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution>Guangdong Province Key Laboratory of Advanced Biomaterials</institution></institution-wrap></funding-source><award-id>2022B1212010003</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Quanying</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution>Shenzhen Science and Technology Innovation Committee</institution></institution-wrap></funding-source><award-id>KCXFZ2020122117340001</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Quanying</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution>The Science and Technology Development Fund (FDCT) of Macau</institution></institution-wrap></funding-source><award-id>FDCT 0041/2022/A</award-id><principal-award-recipient><name><surname>Wu</surname><given-names>Haiyan</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neurobehavioral data combined with computational models shows the superiority of active inference models in explaining human decisions under uncertainty.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Active inference from the free energy principle provides a powerful explanatory tool for understanding the dynamic relationship between an agent and its environment (<xref ref-type="bibr" rid="bib20">Friston, 2010</xref>). Free energy is a measure of an agentâs uncertainty about the environment, which can be understood as the difference between the real environment state and the agentâs estimated environment state (<xref ref-type="bibr" rid="bib18">Friston and Stephan, 2007</xref>). In addition, expected free energy is the free energy about the future and can be used to guide the optimization process of decision-making. Under the active inference framework, perception, action, and learning are all driven by the minimization of free energy (<xref ref-type="fig" rid="fig1">Figure 1</xref>). By minimizing free energy, people can optimize decisions, which encompasses both the reduction of uncertainty about the environment (through <italic>exploration</italic>) and the maximization of rewards (through <italic>exploitation</italic>). Active inference (<xref ref-type="bibr" rid="bib23">Friston et al., 2016</xref>) is a pragmatic implementation of the free energy principle to action, proposing that agents not only minimize free energy through perception but also through actions that enable them to reach preferable states. Briefly, in active inference, the agent has an internal cognitive model to approximate the hidden states of the environment (perception) and actively acts to enable oneself to reach preferable states (action) (see âContextual two-armed bandit taskâ).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Active inference.</title><p>(<bold>a</bold>) Qualitatively, agents receive observations from the environment and use these observations to optimize Bayesian beliefs under an internal cognitive (a.k.a., world or generative) model of the environment. Then agents actively sample the environment states by action, choosing actions that would make them in more favorable states. The environment changes its state according to agentsâ policies (action sequences) and transition functions. Then again, agents receive new observations from the environment. (<bold>b</bold>) From a quantitative perspective, agents optimize the Bayesian beliefs under an internal cognitive (a.k.a., world or generative) model of the environment by minimizing the variational free energy. Then agents select policies minimizing the expected free energy, namely, the surprise expected in the future under a particular policy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-fig1-v1.tif"/></fig><p>In recent years, the active inference framework has been applied to understanding cognitive processes and behavioral policies in human decisions. Many works provide support for the potential of the active inference framework to describe complex cognitive processes and give theoretical insights into behavioral dynamics (<xref ref-type="bibr" rid="bib24">Friston et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Kirchhoff et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Parr et al., 2022</xref>; <xref ref-type="bibr" rid="bib19">Friston et al., 2009</xref>). For instance, it is theoretically deduced in the active inference framework on the exploration and exploitation trade-off (<xref ref-type="bibr" rid="bib55">Schwartenbeck et al., 2019</xref>; <xref ref-type="bibr" rid="bib23">Friston et al., 2016</xref>), which trade-off is essential to the functioning of cognitive agents in many decision contexts (<xref ref-type="bibr" rid="bib44">OâReilly and Tushman, 2011</xref>; <xref ref-type="bibr" rid="bib66">Wilson et al., 2014</xref>). Specifically, <italic>exploration</italic> is to take actions that offer extra information about the current environment, actions with higher uncertainty, while <italic>exploitation</italic> is to take actions to maximize immediate rewards given the current belief, actions with higher expected reward. The exploration-exploitation trade-off refers to an inherent tension between information (resolving uncertainty) and goal-seeking, particularly when the agent is confronted with incomplete information about the environment (<xref ref-type="bibr" rid="bib28">Gershman, 2019</xref>). However, these theoretical studies have rarely been confirmed experimentally with lab empirical evidence from both behavioral and neural responses (<xref ref-type="bibr" rid="bib20">Friston, 2010</xref>; <xref ref-type="bibr" rid="bib18">Friston and Stephan, 2007</xref>). We aimed to validate the active framework in a decision-making task with electroencephalogram (EEG) neural recordings.</p><p>The decision-making process frequently involves grappling with varying forms of uncertainty, such as novelty â the kind of uncertainty that can be reduced through sampling, and variability â the inherent uncertainty (variance) presented by a stable environment. Studies have investigated these different forms of uncertainty in decision-making, focusing on their neural correlates (<xref ref-type="bibr" rid="bib13">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib1">Badre et al., 2012</xref>; <xref ref-type="bibr" rid="bib8">Cavanagh et al., 2012</xref>; <xref ref-type="bibr" rid="bib49">Payzan-LeNestour et al., 2013</xref>). These studies utilized different forms of multi-armed bandit tasks, for example, the restless multi-armed bandit tasks (<xref ref-type="bibr" rid="bib13">Daw et al., 2006</xref>; <xref ref-type="bibr" rid="bib30">Guha et al., 2010</xref>), risky/safe bandit tasks (<xref ref-type="bibr" rid="bib60">Tomov et al., 2020</xref>; <xref ref-type="bibr" rid="bib15">Fan et al., 2023</xref>; <xref ref-type="bibr" rid="bib49">Payzan-LeNestour et al., 2013</xref>), and contextual multi-armed bandit tasks (<xref ref-type="bibr" rid="bib10">Collins and Frank, 2018</xref>; <xref ref-type="bibr" rid="bib54">Schulz et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Collins and Frank, 2012</xref>). However, these tasks only separate either variability from novelty in uncertainty or actions from states (perception). In our work, we develop a contextual multi-armed bandit task to enable participants to actively reduce novelty, avoid variability, and maximize rewards using various policies (see âBehavioral resultsâ and Figure 4a). Our task makes it possible to study whether the brain represents these different types of uncertainty distinctly (<xref ref-type="bibr" rid="bib39">Levy et al., 2010</xref>) and whether the brain represents both the value of reducing uncertainty and the degree of uncertainty. The active inference framework presents a theoretical approach to investigate these questions. Within this framework, uncertainties can be reduced to novelty and variability. Novelty is represented by the uncertainty about model parameters associated with choosing a particular action, while variability is signified by the variance of the environmentâs hidden states. The value of reducing novelty, the value of reducing variability, and extrinsic value together constitute expected free energy (see âContextual two-armed bandit taskâ).</p><p>Our study aims to utilize the active inference framework to investigate how the brain represents the decision-making process and how the brain disassociates the representations of novelty and variability (the degree of uncertainty and the value of reducing uncertainty). To achieve these aims, we utilize the active inference framework to examine the exploration-exploitation trade-off, with behavioral and EEG data (see âMaterials and methodsâ). Our study provides results of (1) how participants trade off the exploration and exploitation in the contextual two-armed bandit task (behavioral evidence), (2) how brain signals differ under different levels of ambiguities and risks (sensor-level EEG evidence), (3) how our brain encodes the trade-off of exploration and exploitation, evaluates the values of reducing novelty and reducing variability during action selection, and (4) updates the information about the environment during belief update (source-level EEG evidence).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Contextual two-armed bandit task</title><p>In this study, we developed a âcontextual two-armed bandit taskâ (<xref ref-type="fig" rid="fig2">Figure 2</xref>), which was based on the conventional multi-armed bandit task (<xref ref-type="bibr" rid="bib42">Lu et al., 2010</xref>; <xref ref-type="bibr" rid="bib55">Schwartenbeck et al., 2019</xref>). Participants were instructed to explore two paths that offer rewards with the aim of maximizing cumulative rewards. One path provided constant rewards in each trial, labeled the âSafe&quot;, while the other, referred to as the âRiskyâ, probabilistically offered varying amounts of rewards. The risky path had two different contexts, âContext 1â and âContext 2â, each corresponding to different reward distributions. The risky path would give more rewards in âContext 1â and give fewer rewards in âContext 2â. The context of the risky path changed randomly in each trial, and agents could only know the specific context of the current trialâs risky path by accessing the âCueâ option, although this comes with a cost. The actual reward distribution of the risky path in âContext 1â was [+12 (55%), +9 (25%), +6 (10%), + 3(5%), + 0(5%)] and the actual reward distribution of the risky path in âContext 2â was [+12 (5%), +9 (5%), +6 (10%) + 3(25%) + 0 (55%)]. For a comprehensive overview of the specific settings, refer to <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The contextual two-armed bandit task.</title><p>(<bold>a</bold>) In this task, agents need to make two choices in each trial. The first choice is âStayâ and âCueâ. The âStayâ option gives you nothing while the âCueâ option gives you a â1 reward and the context information about the âRiskyâ option in the current trial. The second choice is âSafeâ and âRiskyâ. The âSafeâ option always gives you a +6 reward and the âRiskyâ option gives you a reward probabilistically, ranging from 0 to +12 depending on the current context (context 1 or context 2). (<bold>b</bold>) The four policies in this task are âCueâ and âSafeâ, âStayâ and âSafeâ, âCueâ and âRiskyâ, and âStayâ and âRiskyâ. (<bold>c</bold>) The likelihood matrix maps from 8 hidden states (columns) to 7 observations (rows).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-fig2-v1.tif"/></fig><p>We ran some simulation experiments to demonstrate how active inference agents performed the âcontextual two-armed bandit taskâ (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="app1fig1">Appendix 1âfigures 1</xref> and <xref ref-type="fig" rid="app1fig2">2</xref>). Active inference agents with different parameter configurations could exhibit different decision-making policies, as demonstrated in the simulation experiment. By adjusting parameters such as <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>I</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mi>X</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>), <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>), and Î± (<xref ref-type="disp-formula" rid="equ1">Equation 11</xref>), agents could operate under different policies. Agents with a low learning rate would initially incur a cost to access the cue, enabling them to thoroughly explore and understand the reward distributions of different contexts. Once sufficient environmental information was obtained, the agent would evaluate the actual values of various policies and select the optimal policy for exploitation. In the experimental setup, the optimal policy required selecting the risky path in a high-reward context and the safe path in a low-reward context after accessing the cue. However, in particularly difficult circumstances, an agent with a high learning rate might become trapped in a local optimum and consistently opt for the safe path, especially if the initial high-reward scenarios encountered yield minimal rewards.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The simulation experiment results.</title><p>This figure demonstrates how an agent selects actions and updates beliefs over 60 trials in the active inference framework. The first two panels (<bold>a, b</bold>) display the agentâs policy and depict how the policy probabilities are updated (choosing between the stay or cue option in the first choice, and selecting between the safe or risky option in the second choice). The scatter plot indicates the agentâs actions, with green representing the cue option when the context of the risky path is âContext 1â (high-reward context), orange representing the cue option when the context of the risky path is âContext 2â (low-reward context), purple representing the stay option when the agent is uncertain about the context of the risky path, and blue indicating the safe-risky choice. The shaded region represents the agentâs confidence, with darker shaded regions indicating greater confidence. The third panel (<bold>c</bold>) displays the rewards obtained by the agent in each trial. The fourth panel (<bold>d</bold>) shows the prediction error of the agent in each trial, which decreases over time. Finally, the fifth panel (<bold>e</bold>) illustrates the expected rewards of the âRisky Pathâ in the two contexts of the agent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-fig3-v1.tif"/></fig><p><xref ref-type="fig" rid="fig3">Figure 3</xref> shows how an active inference agent with <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> performs our task. We can see the active inference agent exhibits human-like policies and efficiency in completing tasks. In the early stages of the simulation, the agent tended to prefer the âCueâ option, as it provided more information, reducing novelty and reducing variability. Similarly, in the second choice, the agent favored the âRiskyâ option, even though initially the expected rewards for the âSafeâ and âRiskyâ options were the same, but the âRiskyâ option offered greater informational value and reduced novelty. In the latter half of the experiment, the agent again preferred the âCueâ option due to its higher expected reward. For the second choice, the agent made decisions based on specific contexts, opting for the âRiskyâ option in âContext 1â for a higher expected reward, and the âSafeâ option in âContext 2â where the informational value of the âRiskyâ option was outweighed by the difference in expected rewards between the âSafeâ option and the âRiskyâ option in âContext 2â.</p><p>In the behavioral experiment, in order to enrich the behavioral data of participants, a âyou can askâ stage was added at the beginning of each trial. When the participants see âyou can askâ, they know that they can choose whether to ask for cue information in the next stage; when the participants see âyou canât askâ, they know that they canât choose whether to ask and it defaults that participants choose the âStayâ option. Additionally, to make the experiment more realistic, we added a background story of âfinding applesâ to the experiment. Specifically, participants were presented with the following instructions: <italic>âYou are on a quest for apples in a forest, beginning with 5 apples. You encounter two paths: (1) The left path offers a fixed yield of 6 apples per excursion. (2) The right path offers a probabilistic reward of 0/3/6/9/12 apples, and it has two distinct contexts, labeled âContext 1â and âContext 2,â each with a different reward distribution. Note that the context associated with the right path will randomly change in each trial. Before selecting a path, a ranger will provide information about the context of the right path (âContext 1â or âContext 2â) in exchange for an apple. The more apples you collect, the greater your monetary reward will be.&quot;</italic></p><p>The participants were provided with the task instructions (i.e., prior beliefs) above and asked to press a space bar to proceed. They were told that the total number of apples collected would determine the monetary reward they would receive. For each trial, the experimental procedure is illustrated in <xref ref-type="fig" rid="fig4">Figure 4a</xref> and comprises five stages:</p><list list-type="order"><list-item><p>âYou can askâ stage: Participants are informed if they can choose to ask in the âFirst choiceâ stage. If they canât ask, it defaults that participants choose to not ask. This stage lasts for 2 seconds.</p></list-item><list-item><p>âFirst choiceâ stage: Participants decide whether to press the right or left button to ask the ranger for information, at the cost of an apple. In this stage, participants have 2 seconds to decide which option to choose, and they cannot press buttons within these 2 seconds. Then, they need to respond by pressing a button within another 2 seconds. This stage corresponds to the action selection in active inference.</p></list-item><list-item><p>âFirst resultâ stage: Participants either receive information about the context of the right path for the current trial or gain no additional information based on their choices. This stage lasts for 2 seconds and corresponds to the belief update in active inference.</p></list-item><list-item><p>âSecond choiceâ stage: Participants decide whether to select the RIGHT or LEFT key to choose the safe path or risky path. In this stage, participants have two seconds to decide which option to choose, and they cannot press buttons within these 2 seconds. Then, they need to respond by pressing a button within another 2 seconds. This stage corresponds to the action selection in active inference.</p></list-item><list-item><p>âSecond resultâ stage: Participants are informed about the number of apples rewarded in the current trial and their total apple count, which lasts for 2 seconds. This stage corresponds to the belief update in active inference.</p></list-item></list><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>The experiment task and behavioral result.</title><p>(<bold>a</bold>) The five stages of the experiment, which include the âYou can askâ stage to prompt the participants to decide whether to request information from the Ranger, the âFirst choiceâ stage to decide whether to ask the ranger for information, the âFirst resultâ stage to display the result of the âFirst choiceâ stage, the âSecond choiceâ stage to choose between left and right paths under different uncertainties and the âSecond resultâ stage to show the result of the âSecond choiceâ stage. The error bars show the 95% confidence interval. (<bold>b</bold>) The number of times each option was selected. The error bar indicates the variance among participants. (<bold>c</bold>) The Bayesian information criterion of active inference, model-free reinforcement learning, and model-based reinforcement learning.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-fig4-v1.tif"/></fig><p>Each stage was separated by a jitter ranging from 0.6 to 1.0 seconds. The entire experiment consisted of a single block with a total of 120 trials. The participants were required to use any two fingers of one hand to press the buttons (left arrow and right arrow on the keyboard).</p></sec><sec id="s2-2"><title>Behavioral results</title><p>To assess the evidence for active inference over reinforcement learning, we fitted active inference (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>), model-free reinforcement learning, and model-based reinforcement learning models to the behavioral data of each participant. This involved optimizing the free parameters of active inference and reinforcement learning models. The resulting likelihood was used to calculate the Bayesian information criterion (BIC) (<xref ref-type="bibr" rid="bib61">Vrieze, 2012</xref>) as the evidence for each model. The free parameters for the active inference model [<inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>I</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mi>X</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>) and Î± <xref ref-type="disp-formula" rid="equ1">Equation 11</xref>] scaled the contribution of the three terms that constituted the expected free energy in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>. These coefficients could be regarded as precisions that characterized each participantâs prior beliefs about contingencies and rewards. For example, increasing Î± meant participants would update their beliefs about reward contingencies more quickly, increasing <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> meant participants would like to reduce novelty more, and increasing <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>I</mml:mi></mml:mstyle></mml:math></inline-formula> meant participants would like to learn the hidden state of the environment and reduce variability more. The free parameters for the model-free reinforcement learning model were the learning rate Î± and the temperature parameter Î³, and the free parameters for the model-based were the learning rate Î±, the temperature parameter Î³ and prior (the details for the model-free reinforcement learning model can be found in <xref ref-type="disp-formula" rid="equ12 equ13 equ14 equ15 equ16 equ17 equ18 equ19 equ20 equ21 equ22">Equations 12â22</xref>, and the details for the model-based reinforcement learning model can be found in <xref ref-type="disp-formula" rid="equ23 equ24 equ25 equ26 equ27 equ28 equ29 equ30 equ31 equ32 equ33 equ34">Equations 23â34</xref> in the supplementary method). The parameter fitting for these three models was conducted using the âBayesianOptimizationâ package (<xref ref-type="bibr" rid="bib17">Frazier, 2018</xref>) in Python, first randomly sampling 1000 times and then iterating for an additional 1000 times.</p><p>The model comparison results demonstrated that active inference provided a better performance to fit participantsâ behavioral data compared to the basic model-free reinforcement learning and model-based reinforcement learning (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). Notably, the active inference could better capture the participantsâ exploratory inclinations (<xref ref-type="bibr" rid="bib59">Sutton and Barto, 2018</xref>; <xref ref-type="bibr" rid="bib22">Friston et al., 2015</xref>). This was evident in our experimental observations (<xref ref-type="fig" rid="fig4">Figure 4b</xref>) where participants significantly favored asking the ranger over opting to stay. Asking the ranger, which provided environmental information, emerged as a more beneficial policy within the context of this task.</p><p>Moreover, participantsâ preferences for information gain (i.e., epistemic value) were found to vary depending on the context. When participants lacked information about the context and the risky path had the same average rewards as the safe path but with greater variability, they showed an equal preference for both options (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, âNot askâ). However, in âContext 1â (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, high-reward context), where the risky path offered greater rewards than the safe path, participants strongly favored the risky path, which not only provided higher rewards but also had more epistemic value. In contrast, in âContext 2â (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, low-reward context), where the risky path had fewer rewards than the safe path, participants mostly chose the safe path but occasionally opted for the risky path, recognizing that despite its fewer rewards, it offered epistemic value.</p><p><xref ref-type="fig" rid="fig5">Figure 5</xref> shows the comparison between the active inference model and the behavioral data, where we can see that the model can fit the participantsâ behavioral strategies well. In the âStay-Cueâ choice, participants always tend to choose to ask the ranger and rarely choose not to ask. When the context was unknown, participants chose the âSafeâ option or the âRiskyâ option very randomly, and they did not show any aversion to variability. When given âContext 1â, where the âRiskyâ option gave participants a high average reward, participants almost exclusively chose the âRiskyâ option, which provided more information in the early trials and was found to provide more rewards in the later rounds. When given âContext 2â, where the âRiskyâ option gave participants a low average reward, participants initially chose the âRiskyâ option and then tended to choose the âSafeâ option. We can see that participants still occasionally chose the âRiskyâ option in the later trials of the experiment, which the model does not capture. This may be due to the influence of forgetting. Participants chose the âRiskyâ option again to establish an estimate of the reward distribution.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The comparison between the active inference model and the behavioral data in (<bold>a</bold>) the âFirst choiceâ stage, and the âSecond choiceâ stage; (<bold>b</bold>) context unknown, (<bold>c</bold>) âContext 1â, and (<bold>d</bold>) âContext 2â.</title><p>The bar graphs show participantsâ behavior data in each trial, and the height shows the proportion of participants who chose a certain option in each trial. The scatter plots show the modelâs fitting results for the two choices of the participants. The closer the point is to the bar graph on both sides, the higher the fitting accuracy. The line graphs show the trend of the model fitting accuracies with the trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-fig5-v1.tif"/></fig></sec><sec id="s2-3"><title>EEG results at sensor level</title><p>As depicted in <xref ref-type="fig" rid="fig6">Figure 6a</xref>, we divided electrodes into five clusters: left frontal, right frontal, central, left parietal, and right parietal. Within the âSecond choiceâ stage, participants were required to make decisions under varying degrees of uncertainty (the uncertainty about the hidden states and the uncertainty about the model parameters). Thus, we investigated whether distinct brain regions exhibited different responses under such uncertainty.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>EEG results at the sensor level.</title><p>(<bold>a</bold>) The electrode distribution. (<bold>b</bold>) The signal amplitude of different brain regions in the first and second half of the experiment in the âSecond choiceâ stage. The error bar indicates the amplitude variance in each region. The right panel shows the visualization of the evoked data and spectrum data. (<bold>c</bold>) The signal amplitude of different brain areas in the âSecond choiceâ stage where participants know the context or do not know the context of the right path. The error bar indicates the amplitude variance in each region. The error bars show the 95% confidence interval. The right panel shows the visualization of the evoked data and spectrum data. FL: frontal-left; FR: frontal-right; C: central; PL: parietal-left; PR: parietal-right.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-fig6-v1.tif"/></fig><p>In the first half of the experimental trials, participants would have greater uncertainty about model parameters compared to the latter half of the trials (<xref ref-type="bibr" rid="bib55">Schwartenbeck et al., 2019</xref>). We thus analyzed data from the first half and latter half trials and identified statistically significant differences in the signal amplitude of the left frontal region (p&lt;0.01), the right frontal region (p&lt;0.05), the central region (p&lt;0.01), and the left parietal region (p&lt;0.05), suggesting a role for these areas in encoding the statistical structure of the environment (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). We postulated that when participants had constructed the statistical model of the environment during the second half of the trials, brains could effectively utilize the statistical model to make more confident decisions and exhibit greater neural responses.</p><p>To investigate whether distinct brain regions exhibited differential responses under the uncertainty about the hidden states, we divided all trials into two groups: the âaskedâ trials and the ânot-askedâ trials based on whether participants chose to ask in the âFirst choiceâ stage. In the <italic>not-asked</italic> (<xref ref-type="fig" rid="fig6">Figure 6c</xref>), participants had greater uncertainty about the hidden states of the environment compared to the <italic>asked trials</italic>. We identified statistically significant differences in the signal amplitude of the left frontal region (p&lt;0.01), the right frontal region (p&lt;0.05), and the central region (p&lt;0.001), suggesting a role for these areas in encoding the hidden states of the environment. It might suggest that when participants knew the hidden states, they could effectively integrate the information with the environmental statistical structure to make more precise or confident decisions and exhibit greater neural response. The right panel of <xref ref-type="fig" rid="fig6">Figure 6c</xref> revealed a higher signal in the theta band during not-asked trials, suggesting a correlation between theta band signal and uncertainty about the hidden states (<xref ref-type="bibr" rid="bib32">Harper et al., 2017</xref>).</p></sec><sec id="s2-4"><title>EEG results at source level</title><p>In the final analysis of the neural correlates of the decision-making process, as quantified by the epistemic and intrinsic values of expected free energy, we presented a series of linear regressions in source space. These analyses tested for correlations over trials between constituent terms in expected free energy (the value of reducing variability, the value of reducing novelty, extrinsic value, and expected free energy itself) and neural responses in source space. Additionally, we also investigated the neural correlate of (the degree of) variability, (the degree of) novelty, and prediction error. Because we were dealing with a 2-second time series, we were able to identify the periods of time during decision-making when the correlates were expressed. The linear regression was run by the âmne.stats.linear regressionâ function in the MNE package (<inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>â¼</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). <italic>Activity</italic> is the activity amplitude of the EEG signal in the source space and <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> is one of the regressors that we mentioned (e.g., expected free energy, the value of reducing novelty, etc.).</p><p>In these analyses, we focused on the induced power of neural activity at each time point, in the brain source space. To illustrate the functional specialization of these neural correlates, we presented whole-brain maps of correlation coefficients and picked out the brain region with the most significant correlation for reporting fluctuations in selected correlations over 2-second periods. These analyses were presented in a descriptive fashion to highlight the nature and variety of the neural correlates, which we unpacked in relation to the existing EEG literature in the discussion. The significant results after false discovery rate (FDR) (<xref ref-type="bibr" rid="bib3">Benjamini and Hochberg, 1995</xref>; <xref ref-type="bibr" rid="bib26">Gershman et al., 2014</xref>) correction are shown in shaded regions. Additional regression results can be found in supplementary materials.</p><sec id="s2-4-1"><title>âFirst choiceâ stage: Action selection</title><p>During the âFirst choiceâ stage, participants were presented with the choice of either choosing to stay or ask the ranger to get information regarding the present context of the risky path, the latter choice coming at a cost. Here, we examined âexpected free energyâ (<inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>), âvalue of reducing variabilityâ (<inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>), and âextrinsic valueâ <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mi>X</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We found a robust correlation (p&lt;0.05) between the âexpected free energyâ regressor and the frontal pole (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). In addition, the superior temporal gyrus also displayed strong correlations with expected free energy. With respect to the âvalue of reducing variabilityâ regressor, we identified a strong correlation (p&lt;0.05) with the medial orbitofrontal cortex (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). In addition, the postcentral gyrus and precentral gyrus also displayed strong correlations with the value of reducing variability. For the âextrinsic valueâ regressor, we observed a strong correlation (p&lt;0.05) with the middle temporal gyrus (see <xref ref-type="fig" rid="app1fig4">Appendix 1âfigure 4a</xref>). In addition, the inferior temporal gyrus, and superior temporal gyrus also exhibited strong correlations with extrinsic value. Interestingly, we observed that during the âFirst choiceâ stage, expected free energy and extrinsic value regressors were both strongly correlated. However, expected free energy correlations appeared later than those of extrinsic value at the beginning, suggesting that the brain initially encoded reward values before integrating these values with information values for decision-making.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>The source estimation results of expected free energy and active inference in the âFirst choiceâ stage.</title><p>(<bold>a</bold>) The regression intensity (<inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula>) of expected free energy. The right panel indicates the regression intensity between the frontal pole (1, right half) and the expected free energy. The green-shaded regions indicate p&lt;0.05 after false discovery rate (FDR) correction (the average <italic>t</italic>-value during these significant periods equals â3.228). (<bold>b</bold>) The regression intensity (<inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula>) of the value of reducing variability. The right panel indicates the regression intensity between the medial orbitofrontal cortex (5, left half) and the value of reducing variability. The green-shaded regions indicate p&lt;0.05 after FDR correction (the average <italic>t</italic>-value during these significant periods equals â3.081). The black lines indicate the average intensities, and the gray-shaded regions indicate the ranges of variations (the 95% confidence interval). The gray lines indicate p&lt;0.05 before FDR.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-fig7-v1.tif"/></fig></sec><sec id="s2-4-2"><title>âFirst resultâ stage: Belief update</title><p>During the âFirst resultâ stage, participants were presented with the outcome of their first choice, which informed them of the current context: either âContext 1â or âContext 2â for the risky path, or no additional information if they opted not to ask. This process correlated with the âreducing variabilityâ regressor, as it corresponded to resolving uncertainties about hidden states. We assumed that the brain learning hidden states (reducing variability) corresponded to the value of reducing variability. Thus, the âreducing variabilityâ regressor could be <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p><p>For âreducing variabilityâ, we observed a robust correlation (p&lt;0.05) within the medial orbitofrontal cortex (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). In addition, the rostral middle frontal gyrus, the lateral orbitofrontal cortex, and the superior temporal gyrus also displayed strong correlations with reducing variability.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>The source estimation results of reducing variability and reducing novelty in the two result stages.</title><p>(<bold>a</bold>) The regression intensity (Î²) of reducing variability in the âFirst resultâ stage. The right panel indicates the regression intensity between the medial orbitofrontal cortex (5, left half) and reducing variability. The green-shaded regions indicate p&lt;0.05 after false discovery rate (FDR) correction (the average <italic>t</italic>-value during these significant periods equals â3.001). (<bold>b</bold>) The regression intensity (<inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula>) of reducing novelty in the âSecond resultâ stage. The right panel indicates the regression intensity between the precentral gyrus (15, right half) and reducing novelty. The green-shaded regions indicate p&lt;0.05 after FDR correction (the average <italic>t</italic>-value during these significant periods equals <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>â</mml:mo><mml:mn>3.278</mml:mn></mml:mstyle></mml:math></inline-formula>). The black lines indicate the average intensities, and the gray-shaded regions indicate the ranges of variations (the 95% confidence interval). The gray lines indicate p&lt;0.05 before FDR.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-fig8-v1.tif"/></fig></sec><sec id="s2-4-3"><title>âSecond choiceâ stage: Action selection</title><p>During the âSecond choiceâ stage, participants chose between the risky path and the safe path based on the current information, with the aim of maximizing rewards. This required a balance between exploration and exploitation. Here, we examined âexpected free energyâ (<inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>), âvalue of reducing noveltyâ (<inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>L</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>), âextrinsic valueâ (<inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mi>X</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>), and ânoveltyâ (<inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>).</p><p>For âexpected free energyâ (<xref ref-type="fig" rid="fig9">Figure 9a</xref>), we identified strong correlations (p&lt;0.001) in the rostral middle frontal gyrus. In addition, the caudal middle frontal gyrus, middle temporal gyrus, pars triangularis, and superior temporal gyrus also displayed strong correlations with expected free energy. Regarding the âvalue of reducing noveltyâ, we found that the rostral middle frontal gyrus showed strong correlations (p&lt;0.05). In addition, the superior frontal gyrus, insula, and lateral orbitofrontal cortex also displayed strong correlations with the value of reducing novelty. For âextrinsic valueâ, strong correlations (p&lt;0.001) were evident in the rostral middle frontal gyrus (see <xref ref-type="fig" rid="app1fig4">Appendix 1âfigure 4b</xref>). In addition, the middle temporal gyrus, pars opercularis, and precentral gyrus also displayed strong correlations with extrinsic values. In the âSecond choiceâ stage, participants made choices under different degrees of novelty. For âthe degree of noveltyâ, we found no significant correlations after FDR correction (see <xref ref-type="fig" rid="app1fig6">Appendix 1âfigure 6</xref>). Generally, the correlations between regressors and brain signals were more pronounced in the âSecond choiceâ stage compared to the âFirst choiceâ stage.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>The source estimation results of expected free energy and the value of reducing novelty in the âSecond choiceâ stage.</title><p>(a) The regression intensity (<inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula>) of expected free energy. The right panel indicates the regression intensity between the rostral middle frontal gyrus (1, left half) and expected free energy, the black line indicates the average intensity of this region, and the gray-shaded region indicates the range of variation. The yellow-shaded regions indicate p&lt;0.001 after false discovery rate (FDR) (the average <italic>t</italic>-value during these significant periods equals â4.819) and the gray lines indicate p&lt;0.001 before FDR. (b) The regression intensity (Î²) of the value of reducing novelty. The right panel indicates the regression intensity between the rostral middle frontal gyrus (6, left half) and the value of reducing novelty, the black line indicates the average intensity of this region, and the gray-shaded region indicates the range of variation (the 95% confidence interval). The green-shaded regions indicate p&lt;0.05 after FDR (the average <italic>t</italic>-value during these significant periods equals â3.067) and the gray lines indicate p&lt;0.05 before FDR.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-fig9-v1.tif"/></fig></sec><sec id="s2-4-4"><title>âSecond resultâ stage: Belief update</title><p>During the âSecond resultâ stage, participants obtained specific rewards based on their second choice: selecting the safe path yields a fixed reward, whereas choosing the risky path results in variable rewards, contingent upon the context. Here we examined âextrinsic valueâ (<inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>), âprediction errorâ (<inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>), and âreducing noveltyâ (<inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>). Here, we also assumed that learning model parameters (reducing novelty) corresponded to the value of reducing novelty.</p><p>For âextrinsic valueâ, we observed strong correlations (p&lt;0.05) in the lateral occipital gyrus, inferior parietal gyrus, and superior parietal gyrus (see <xref ref-type="fig" rid="app1fig5">Appendix 1âfigure 5a</xref>). For âprediction errorâ, we observed strong correlations (p&lt;0.05) in the bank of the superior temporal sulcus, inferior temporal gyrus, and lateral occipital gyrus (see <xref ref-type="fig" rid="app1fig5">Appendix 1âfigure 5b</xref>). For âreducing noveltyâ, we observed strong correlations (p&lt;0.05) in the precentral gyrus (see <xref ref-type="fig" rid="fig8">Figure 8</xref>).</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we utilized active inference to explore the neural correlates involved in the human decision-making process under novelty and variability. By employing a contextual two-bandit task, we demonstrated that the active inference framework effectively describes real-world decision-making. Our findings indicate that active inference not only provides explanations for decision-making under different kinds of uncertainty but also reveals the common and unique neural correlates associated with different types of uncertainties and decision-making policies. This was supported by evidence from both sensor-level and source-level EEG.</p><sec id="s3-1"><title>The varieties of human exploration strategies in active inference</title><p>In the diverse realm of human behavior, it has been observed that human exploration strategies vary significantly depending on the current situation. Such strategies can be viewed as a blend of <italic>directed exploration</italic>, where actions with higher levels of uncertainty are favored, and <italic>random exploration</italic>, where actions are chosen at random (<xref ref-type="bibr" rid="bib27">Gershman, 2018</xref>). In the framework of active inference, the randomness in exploration is derived from the precision parameter employed during policy selection. As the precision parameter increases, the randomness in agentsâ actions also increases. On the other hand, the directed exploration stems from the computation of expected free energy. Policies that lead to the exploration of more disambiguating options, hence yielding higher information gain, are assigned increased expected free energy by the model (<xref ref-type="bibr" rid="bib23">Friston et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Friston et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Gershman, 2019</xref>).</p><p>Our model-fitting results indicate that people show high variance in their exploration strategies (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). Exploration strategies, from a model-based perspective, incorporate a fusion of model-free learning and model-based learning. Intriguingly, these two learning ways exhibit both competition and cooperation within the human brain (<xref ref-type="bibr" rid="bib29">GlÃ¤scher et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Daw et al., 2005</xref>). The simplicity and effectiveness of model-free learning contrast with its inflexibility and data inefficiency. Conversely, model-based learning, although flexible and capable of forward planning, demands substantial cognitive resources. The active inference model tends to lean more toward model-based learning as this model incorporates a cognitive model of the environment to guide the agentâs actions. Our simulation results showed these model-based behaviors in which the agent constructs an environment model and uses the model to maximize rewards (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Active inference can integrate model-free learning through adding a habitual term (<xref ref-type="bibr" rid="bib23">Friston et al., 2016</xref>). This allows the active inference agent to exploit the cognitive model (model-based) for planning in the initial task stages and utilize habits for increased accuracy and efficiency in later stages.</p></sec><sec id="s3-2"><title>The strength of the active inference framework in decision-making</title><p>Active inference is a comprehensive framework elucidating neurocognitive processes (<xref ref-type="fig" rid="fig1">Figure 1</xref>). It unifies perception, decision-making, and learning within a single framework centered around the minimization of free energy. One of the primary strengths of the active inference model lies in its robust statistical (<xref ref-type="bibr" rid="bib11">Crooks, 1998</xref>) and neuroscientific underpinnings (<xref ref-type="bibr" rid="bib38">Lehmann et al., 2022</xref>), allowing for a lucid understanding of an agentâs interaction within its environment.</p><p>Active inference offers a superior exploration mechanism compared with basic model-free reinforcement learning (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). Since traditional reinforcement learning models determine their policies solely on the state, this setting leads to difficulty in extracting temporal information (<xref ref-type="bibr" rid="bib37">Laskin et al., 2020</xref>) and increases the likelihood of entrapment within local minima. In contrast, the policies in active inference are determined by both time and state. This dependence on time (<xref ref-type="bibr" rid="bib64">Wang et al., 2016</xref>) enables policies to adapt efficiently, such as emphasizing exploration in the initial stages and exploitation later on. Moreover, this mechanism prompts more exploratory behavior in instances of state novelty. A further advantage of active inference lies in its adaptability to different task environments (<xref ref-type="bibr" rid="bib24">Friston et al., 2017</xref>). It can configure different generative models to address distinct tasks and compute varied forms of free energy and expected free energy.</p><p>Despite these strengths, the active inference framework also has its limitations (<xref ref-type="bibr" rid="bib51">Raja et al., 2021</xref>). One notable limitation pertains to its computational complexity (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), resulting from its model-based architecture, restricting the traditional active inference modelâs application within continuous state-action spaces. Additionally, the model heavily relies on the selection of priors, meaning that poorly chosen priors could adversely affect decision-making, learning, and other processes (<xref ref-type="bibr" rid="bib55">Schwartenbeck et al., 2019</xref>). However, sometimes it is just the opposite. As illustrated in the model comparison, priors can be a strength of Bayesian approaches. Under the complete class theorem (<xref ref-type="bibr" rid="bib62">Wald, 1947</xref>; <xref ref-type="bibr" rid="bib6">Brown, 1981</xref>), any pair of behavioral data and reward functions can be described in terms of ideal Bayesian decision-making with particular priors. In other words, there always exists a description of behavioral data in terms of some priors. This means that one can, in principle, characterize any given behavioral data in terms of the priors that explain that behavior. In our example, these were effectively priors over the precision of various preferences or beliefs about contingencies that underwrite expected free energy.</p></sec><sec id="s3-3"><title>Representing uncertainties at the sensor level</title><p>The employment of EEG signals in decision-making processes under uncertainty has largely concentrated on event-related potential (ERP) and spectral features at the sensor level (<xref ref-type="bibr" rid="bib63">Wang et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Lin et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Bland and Schaefer, 2011</xref>; <xref ref-type="bibr" rid="bib5">Botelho et al., 2023</xref>). In our study, the sensor-level results reveal greater neural responses in multiple brain regions during the second half trials compared to the first half, and similarly, during not-asked trials as opposed to asked trials (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>In our setting, after the first half of the trials, participants had learned some information about the environmental statistical structure, thus experiencing less novelty in the latter half of the trials. This increased understanding enabled them to better utilize the statistical structure for decision-making than they did in the first half of the trials. In contrast, during the not-asked trials, the lack of knowledge of the environmentâs hidden states led to higher-variability actions. This elevated variability was reflected in increased positive brain activities.</p><p>Novelty and variability, two pivotal factors in decision-making, are often misinterpreted and can vary in meaning depending on the context. Regarding the sensor level results, we find an overall greater neural response for the second half of the trials than the first half of the trials (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). It may indicate a generally greater neural response for the lower novelty trials, which may contrast with previous studies showing greater neural response for higher novelty trials in previous studies (<xref ref-type="bibr" rid="bib57">Sun et al., 2017</xref>; <xref ref-type="bibr" rid="bib5">Botelho et al., 2023</xref>). For example, a late positive potential (LPP) was identified in their work, which differentiated levels of novelty, with the amplitude of the LPP serving as an index for perceptual novelty levels. However, the novelty in their task was defined as the perceptual difficulty of distinguishing, while our definition of novelty corresponds to the information gained from certain policies. Furthermore, <xref ref-type="bibr" rid="bib68">Zheng et al., 2020</xref> used a wheel-of-fortune task to examine the ERP and oscillatory correlations of neural feedback processing under conditions of variability and novelty. Their findings suggest that risky gambling enhanced cognitive control signals, as evidenced by theta oscillation. In contrast, ambiguous gambling heightened affective and motivational salience during feedback processing, as indicated by positive activity and delta oscillation. Future work may focus on this oscillation level analysis and reveal more evidence on it.</p></sec><sec id="s3-4"><title>Representation of decision-making process in human brain</title><p>In our experiment, each stage corresponded to distinct phases of the decision-making process. Participants made decisions to optimize cumulative rewards based on current information about the environment during the two choice stages while acquiring information about the environment during the two result stages.</p><p>During the âFirst choiceâ stage, participants had to decide whether to pay an additional cost in exchange for information regarding the environmentâs hidden states. Here, the epistemic value stemmed from resolving the uncertainty about the hidden states and reducing variability. The frontal pole appears to play a critical role in this process by combining extrinsic value with epistemic value, expected free energy, to guide decision-making (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Our results also showed the medial orbitofrontal cortex, postcentral gyrus, and precentral gyrus were correlated with the value of reducing variability. A previous study <xref ref-type="bibr" rid="bib31">Guo et al., 2013</xref> demonstrated that the frontal pole was strongly activated in the risky condition and the ambiguous condition during decision-making. Another study also demonstrated that the frontal pole played an important role in the interaction between beliefs (variability and novelty) and payoffs (gains and losses) (<xref ref-type="bibr" rid="bib56">Smith et al., 2002</xref>).</p><p>As for the âFirst resultâ stage, participants learned about the environmentâs hidden states and avoided risks in the environment. Our results indicated that the medial orbitofrontal cortex, rostral middle frontal gyrus, and lateral orbitofrontal cortex played a crucial role in both valuing the uncertainty about hidden states and learning information about these hidden states (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). A previous study <xref ref-type="bibr" rid="bib40">Li et al., 2016</xref> found that both the medial and lateral orbitofrontal cortex encoded variability and reward probability while the lateral orbitofrontal cortex played a dominant role in coding experienced value. Another study (<xref ref-type="bibr" rid="bib52">Rolls et al., 2022</xref>) indicated that the medial orbitofrontal cortex was related to risk-taking and risk-taking was driven by specific orbitofrontal cortex reward systems. Throughout the âFirst resultâ stage, participants are processing the state information relevant to the current trial. The orbitofrontal cortex is postulated to play a key role in processing this state information and employing it to construct an environmental model.</p><p>In the âSecond choiceâ stage, participants chose between a safe path and a risky path based on their current information. When knowing the environmentâs hidden states, participants tended to resolve the uncertainty about model parameters by opting for the risky path. Conversely, without knowledge of the hidden states, Participants leaned toward variability reduction by choosing the safe path. Expected free energy is also correlated with brain signals but in different regions, such as the rostral middle frontal gyrus, caudal middle frontal gyrus, and middle temporal gyrus. Our results also highlighted the significance of the rostral middle frontal gyrus, superior frontal gyrus, insula, and lateral orbitofrontal cortex, in evaluating the value of reducing novelty. These results suggest that some brain regions may evaluate both the value of reducing novelty and the value of reducing variability (<xref ref-type="bibr" rid="bib36">Krain et al., 2006</xref>).</p><p>For the âSecond resultâ stage, participants got rewards according to their actions, constructing the value function and the state transition function. Our results highlighted the role of the precentral gyrus and superior parietal gyrus in learning the state transition function and reducing novelty (<xref ref-type="fig" rid="fig8">Figure 8b</xref>). Participants made their decisions in different contexts and there were multiple studies emphasizing the role of the superior parietal gyrus in uncertain decision-making (<xref ref-type="bibr" rid="bib48">Paulus et al., 2001</xref>; <xref ref-type="bibr" rid="bib33">Huettel et al., 2005</xref>; <xref ref-type="bibr" rid="bib50">Ragni et al., 2016</xref>).</p><p>In the two âchoiceâ stages, we observed stronger correlations for the expected free energy compared to the extrinsic value, suggesting that the expected free energy could better represent the actual value of the brain used to guide actions (<xref ref-type="bibr" rid="bib65">Williams et al., 2021</xref>). Compared with the âFirst choiceâ stage, the correlations in the âSecond choiceâ stage were more significant. This may indicate that the brain is activated more when making decisions for rewards than when making decisions for information. We found neural correlates for the value of reducing variability and the value of reducing novelty, but not the degree of variability and novelty (after FDR correction). Future work should design a task that highlights different degrees of variability and ambiguities. In the two result stages, the regression results of the âSecond resultâ stage were not very reliable. This may be due to our discrete reward structure. Participants may not be good at remembering specific probabilities, but only the mean reward.</p><p>It should be acknowledged that only a subset of the regions identified without correction exhibit model parameter correlations that are robust enough to remain significant after correction for multiple comparisons. In future work, we should collect more precise neural data to make the results more robust, for example, collecting a head model for each subject instead of using an average model.</p></sec><sec id="s3-5"><title>Conclusion</title><p>In the current study, we introduce the active inference framework to investigate the neural mechanisms underlying an exploration and exploitation decision-making task. Compared to model-free reinforcement learning, active inference provides a superior exploration bonus and offers a better fit to the participantsâ behavioral data. Given that the behavioral task in our study only involved variables from a limited number of states and rewards, future research should strive to apply the active inference framework to more complex tasks. Specific brain regions may play key roles in balancing exploration and exploitation. The frontal pole and middle frontal gyrus were primarily involved in action selection (expected free energy). The precentral gyrus was mainly engaged in evaluating the value of reducing variability, and the rostral middle frontal cortex was also engaged in evaluating the value of reducing novelty. Furthermore, the medial orbitofrontal cortex participated in learning the hidden states of the environment (reducing variability) and the precentral gyrus participated in learning the model parameters of the environment (reducing novelty). In essence, our findings suggest that active inference is capable of investigating human behaviors in decision-making under uncertainty. Overall, this research presents evidence from both behavioral and neural perspectives that support the concept of active inference in decision-making processes. We also offer insights into the neural mechanisms involved in human decision-making under various forms of uncertainty.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>The free energy principle and active inference</title><p>The free energy principle (<xref ref-type="bibr" rid="bib20">Friston, 2010</xref>) is a theoretical framework that proposes that both biological and non-biological systems tend to minimize their (variational) free energy to maintain a non-equilibrium steady state. In the context of the brain, the free energy principle suggests that the brain functions as an âinference machineâ that aims to minimize the difference between its internal cognitive model about the environment and the true causes (hidden states) of perceived sensory inputs. This minimization is achieved through active inference.</p><p>Active inference can be regarded as a form of planning as inference, in which an agent samples the environment to maximize the evidence for its internal cognitive model of how sensory samples are generated. This is sometimes known as self-evidencing (<xref ref-type="bibr" rid="bib23">Friston et al., 2016</xref>). Under the active inference framework, variational free energy can be viewed as the objective function that underwrites belief updating; namely, inference and learning. By minimizing the free energy expected following an action (i.e., expected free energy), we can optimize decisions and resolve uncertainty.</p><p>Mathematically, the minimization of free energy is formally related to variational Bayesian methods (<xref ref-type="bibr" rid="bib25">Galdo et al., 2020</xref>). Variational inference is used to estimate both hidden states of the environment and the parameters of the cognitive model. This process can be viewed as an optimization problem that seeks to find the best model parameters and action policy to maximize the sensory evidence. By minimizing variational free energy and expected free energy, optimal model parameters can be estimated and better decisions can be made (<xref ref-type="bibr" rid="bib21">Friston, 2013</xref>). Active inference bridges the sensory input, cognitive processes, and action output, enabling us to quantitatively describe the neural processes of learning about the environment. The brain receives sensory input <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>o</mml:mi></mml:mstyle></mml:math></inline-formula> from the environment, and the cognitive model encoded by the brain <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> makes an inference on the cause of sensory input <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>o</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> (a.k.a., the hidden state of the environment). In the free energy principle, minimizing free energy refers to minimizing the difference (e.g., KL divergence) between the cognitive model encoded by the brain and the causes of the sensory input. Thus, free energy is an information-theoretic quantity that bounds the evidence for the data model. Free energy can be minimized by the following two means (<xref ref-type="bibr" rid="bib7">Buckley et al., 2017</xref>):</p><list list-type="bullet"><list-item><p>Minimize free energy through <italic>perception</italic>. Based on existing observations, by maximizing model evidence, the brain improves its internal cognitive model, reducing the gap between the true cause of the sensory input and the estimated distribution of the internal cognitive model.</p></list-item><list-item><p>Minimize free energy through <italic>action</italic>. The agent actively samples the environment, making the sensory input more in line with the cognitive model by sampling preferred states (i.e., prior preferences over observations). Minimizing free energy through action is one of the generalizations afforded by the free energy principle over Bayesian formulations that only address perception.</p></list-item></list><p>Active inference formulates the necessary cognitive processing as a process of belief updating, where choices depend on agentsâ expected free energy. Expected free energy serves as a universal objective function, guiding both perception and action. In brief, expected free energy can be seen as the expected surprise following some policies. The expected surprise can be reduced by resolving uncertainty, and one can select policies with lower expected free energy that can encourage information-seeking and resolve uncertainty. Additionally, one can minimize expected surprise by avoiding surprising or aversive outcomes (<xref ref-type="bibr" rid="bib45">Oudeyer and Kaplan, 2007</xref>; <xref ref-type="bibr" rid="bib53">Schmidhuber, 2010</xref>). This leads to goal-seeking behavior, where goals can be viewed as prior preferences or rewarding outcomes.</p><p>Technically, expected free energy can be expressed as risk plus ambiguity or in terms of expected information gain and expected value, where the value corresponds to (log) prior preferences. We will refer to both formulations in what follows. Resolving ambiguity (and maximizing information gain) has epistemic value, while avoiding risk (and maximizing expected value) has pragmatic or instrumental value. These two types of values can be referred to in terms of intrinsic and extrinsic value, respectively (<xref ref-type="bibr" rid="bib2">Barto et al., 2013</xref>; <xref ref-type="bibr" rid="bib55">Schwartenbeck et al., 2019</xref>).</p><sec id="s4-1-1"><title>The generative model</title><p>Active inference builds on partially observable Markov decision processes: (O, S, U, T, R, P, Q) (see <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Ingredients for computational modeling of active inference.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Notations</th><th align="left" valign="bottom">Definition</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>O</italic></td><td align="left" valign="bottom">A finite set of observations (outcomes)</td><td align="left" valign="bottom">Sensory input that brains receive.</td></tr><tr><td align="left" valign="bottom"><italic>S</italic></td><td align="left" valign="bottom">A finite set of hidden states</td><td align="left" valign="bottom">The true hidden states of the environment that generate sensory inputs to brains.</td></tr><tr><td align="left" valign="bottom"><italic>U</italic></td><td align="left" valign="bottom">A finite set of actions</td><td align="left" valign="bottom">Agent performs actions that change the environment.</td></tr><tr><td align="left" valign="bottom"><italic>T</italic></td><td align="left" valign="bottom">A finite set of time-sensitive policies</td><td align="left" valign="bottom">A policy is an action sequence over time.</td></tr><tr><td align="left" valign="bottom"><italic>R</italic></td><td align="left" valign="bottom">A generative process<inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">The generative process generates observations and next states (state transitions of the environment) based on current states and actions.</td></tr><tr><td align="left" valign="bottom"><italic>P</italic></td><td align="left" valign="bottom">A generative model<inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>s</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Î·</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">The generative model describes what the agent believes about the environment (how observations are generated).</td></tr><tr><td align="left" valign="bottom"><italic>Q</italic></td><td align="left" valign="bottom">An approximate posterior</td><td align="left" valign="bottom">The Bayesian beliefs under the generative model that is optimized to minimize variational free energy. By definition, these beliefs correspond to approximate posteriors.</td></tr></tbody></table></table-wrap><p>In this model, the generative model <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi></mml:mstyle></mml:math></inline-formula> is parameterized as follows and the model parameters are <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î·</mml:mi><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>Î²</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>(<xref ref-type="bibr" rid="bib23">Friston et al., 2016</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Î·</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Î·</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Î³</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:mi>Î³</mml:mi><mml:mo>â</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Î³</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Î</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>Î²</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>o</mml:mi></mml:mstyle></mml:math></inline-formula> is observations or sensory inputs (<inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> is the history of observations), <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> is the hidden states of the environment (<inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>s</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> is the history of hidden states), <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Ï</mml:mi></mml:mstyle></mml:math></inline-formula> is agentâs policies, <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi></mml:mstyle></mml:math></inline-formula> is the likelihood matrix mapping from hidden states to observations, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi></mml:mstyle></mml:math></inline-formula> is the transition function for hidden states under the policy in time <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>d</mml:mi></mml:mstyle></mml:math></inline-formula> is the prior expectation of each state at the beginning of each trial, <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î³</mml:mi></mml:mstyle></mml:math></inline-formula> is the inverse temperature of beliefs about policies, <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula> is the prior expectation of policiesâ temperature parameters, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> is the concentration parameters of the likelihood matrix, <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Ï</mml:mi></mml:mstyle></mml:math></inline-formula> is the softmax function, <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is the categorical distribution, <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is the Dirichlet distribution, and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Î</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is the Gamma distribution.</p><p>The posterior probability of the corresponding hidden states and parameters (<inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>s</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>Î²</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) is as <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Î³</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow class="MJX-TeXAtom-OP MJX-fixedlimits"><mml:mi>arg</mml:mi><mml:mo>â¡</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mo>â¡</mml:mo><mml:mi>F</mml:mi><mml:mo>â</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>The generative model is a conceptual representation of how agents understand their environment. This model fundamentally posits that agentsâ observations are contingent upon states, and the transitions of these states inherently depend on both the state itself and the chosen policy. It is crucial to note that within this model the policy is considered a stochastic variable requiring inference, thus considering planning as a form of inference. This inference process involves deducing the optimal policy from the agentsâ observations. All the conditional abilities rest on likelihood and state transition models that are parameterized using a Dirichlet distribution (<xref ref-type="bibr" rid="bib16">FitzGerald et al., 2015</xref>). The Dirichlet distributionâs sufficient statistic is its concentration parameter, which is equivalently interpreted as the cumulative frequency of previous occurrences. In essence, this means that the agents incorporate the frequency of past combinations of states and observations into the generative model. Therefore, the generative model plays a pivotal role in inferring the probabilities and uncertainties related to the hidden states and observations.</p></sec><sec id="s4-1-2"><title>Variational free energy and expected free energy</title><p>Perception, decision-making, and learning in active inference are all achieved by minimizing the variational and expected free energy with respect to the model parameters and hidden states. The variational free energy can be expressed in various forms with respect to the reduced posterior as <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>s</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>Î²</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, including the hidden states and parameters. These forms of free energy are consistent with the variational inference in statistics. Minimizing free energy is equal to maximizing model evidence, that is, minimizing surprise. In addition, free energy can also be written in other forms as <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="1"><mml:mtable rowspacing=".2em" columnspacing="0.333em" displaystyle="false"><mml:mtr><mml:mtd><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â</mml:mo></mml:munder></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mstyle><mml:mo>â</mml:mo><mml:mstyle scriptlevel="1"><mml:mtable rowspacing=".2em" columnspacing="0.333em" displaystyle="false"><mml:mtr><mml:mtd><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â</mml:mo></mml:munder></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The initial term, denoted as <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>, is conventionally referred to as âcomplexityâ. This term, reflecting the divergence between <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, quantifies the volume of information intended to be encoded within <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> that is not inherent in <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The subsequent term, <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>E</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>, designated as âaccuracyâ, represents the likelihood of an observation expected under approximate posterior (Bayesian) beliefs about hidden states.</p><p>The minimization of variational free energy facilitates a progressive alignment between the approximate posterior distribution of hidden states, as encoded by the brainâs cognitive function, and the actual posterior distribution of the environment. However, it is noteworthy that our policy beliefs are future-oriented. We want policies that possess the potential to effectively guide us toward achieving the future state that we desire. It follows that these policies should aim to minimize the free energy in the future, or in other words, expected free energy. Thus, expected free energy depends on future time points <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Ï</mml:mi></mml:mstyle></mml:math></inline-formula> and policies <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Ï</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> can be replaced by the possible hidden state <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and the likelihood matrix <italic>A</italic>. The relationship between policy selection and expected free energy is inversely proportional: a lower expected free energy under a given policy heightens the probability of that policyâs selection. Hence, expected free energy emerges as a crucial ccice.<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>â</mml:mo><mml:mi>Î³</mml:mi><mml:mo>â</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:munder><mml:mo>â</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Next, we can derive the expected free energy in the same way as the variational free energy:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, it is important to note that we anticipate observations that have not yet occurred. Consequently, we designate <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. If we establish a relationship between <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and the prior preference, it enables us to express expected free energy in terms of epistemic value and extrinsic value. The implications of such a relationship offer a new lens to understand the interplay between cognitive processes and their environmental consequences, thereby enriching our understanding of decision-making under the active inference framework.<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="1"><mml:mtable rowspacing=".2em" columnspacing="0.333em" displaystyle="false"><mml:mtr><mml:mtd><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mstyle scriptlevel="1"><mml:mtable rowspacing=".2em" columnspacing="0.333em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle scriptlevel="1"><mml:mtable rowspacing=".2em" columnspacing="0.333em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mstyle><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â</mml:mo></mml:munder></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>E</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mtext>Â </mml:mtext><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mstyle><mml:mo>â</mml:mo><mml:mstyle scriptlevel="1"><mml:mtable rowspacing=".2em" columnspacing="0.333em" displaystyle="false"><mml:mtr><mml:mtd><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â</mml:mo></mml:munder></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mtext>Â </mml:mtext><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In this context, extrinsic value aligns with the concept of expected utility. On the other hand, epistemic value corresponds to the anticipated information gain or the value of reducing uncertainty, encapsulating the exploration of both model parameters (novelty) and the hidden states (salience), which are to be illuminated by future observations. We can add coefficients (<inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>I</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mi>X</mml:mi></mml:mstyle></mml:math></inline-formula>) before these three terms of <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> to better simulate the diverse exploration strategies of agents:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>A</mml:mi><mml:mi>L</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>Value of reducing novelty</mml:mtext></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>Value of reducing variability</mml:mtext></mml:mrow></mml:munder><mml:mo>â</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>E</mml:mi><mml:mi>X</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>â</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>Extrinsic value</mml:mtext></mml:mrow></mml:munder></mml:mrow></mml:math></disp-formula></p><p>To align with different types of uncertainties and avoid conflicts with active inference terminology, the first two terms in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> are referred to as the value of reducing novelty and variability, respectively, while the corresponding terms in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> are termed novelty and variability.</p><p>Belief updates play a dual role by facilitating both inference and learning processes. The inference is here understood as the optimization of expectations about the hidden states. Learning, on the other hand, involves the optimization of model parameters. This optimization necessitates the finding of sufficient statistics of the approximate posterior that minimize the variational free energy. Active inference employs the technique of gradient descent to identify the optimal update method (<xref ref-type="bibr" rid="bib23">Friston et al., 2016</xref>). In the present work, our focus is primarily centered on the updated methodology related to the likelihood mapping <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi></mml:mstyle></mml:math></inline-formula> and the concentration parameter <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula> (rows correspond to observations, and columns correspond to hidden states):<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>Î±</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="s4-2"><title>Participants</title><p>Participants were recruited via an online recruitment advertisement. We recruited 25 participants (male: 14, female: 11, mean age: 20.82 Â± 2.12 years old), concurrently collecting EEG and behavioral data. All participants signed an informed consent form before the experiments. This study was approved by the local ethics committee of the University of Macau (BSERE22-APP006-ICI).</p></sec><sec id="s4-3"><title>EEG processing</title><p>The processing of EEG signals was conducted using the EEGLAB toolbox (<xref ref-type="bibr" rid="bib43">MartÃ­nez-Cancino et al., 2021</xref>) in the MATLAB and the MNE package (<xref ref-type="bibr" rid="bib14">Esch et al., 2019</xref>). The preprocessing of EEG data involved multiple steps, including data selection, downsampling, high- and low-pass filtering, and independent component analysis (ICA) decomposition. Two-second data segments were selected at various stages during each trial in <xref ref-type="fig" rid="fig4">Figure 4a</xref>. Subsequently, the data was downsampled to a frequency of 250 Hz and subjected to high- and low filtering within the 1â30 Hz frequency range. In instances where channels exhibited abnormal data, these were resolved using interpolation and average values. Following this, ICA was applied to identify and discard components flagged as noise.</p><p>After obtaining the preprocessed data, our objective was to gain a more comprehensive understanding of the specific functions associated with each brain region, mapping EEG signals from the sensor level to the source level. To accomplish this, we employed the head model and source space available in the âfsaverageâ of the MNE package. We utilized eLORETA (<xref ref-type="bibr" rid="bib47">Pascual-Marqui, 2007</xref>) for mapping the EEG data to the source space and used the âaparc subâ parcellation for annotation (<xref ref-type="bibr" rid="bib34">Khan et al., 2018</xref>).</p><p>We segmented the data into five intervals that corresponded to the five stages of the experiment. The first stage, known as the âYou can askâ stage, informed participants whether they could ask the ranger. In the second stage, referred to as the âFirst choiceâ stage, participants decided whether to seek cues. The third stage, called the âFirst resultâ stage, revealed the results of participantsâ first choices. The fourth stage, known as the âSecond choiceâ stage, involved choosing between choosing the safe or risky path. Finally, the fifth stage, named the âSecond resultâ stage, encompassed receiving rewards. The 2 seconds in the two choosing stages when participants were thinking about which option to choose and the 2 seconds in the two result stages when the results were presented were used for analysis. Each interval lasted 2 seconds, and this categorization allowed us to investigate brain activity responses to different phases of the decision-making process. Specifically, we examined the processes of action selection and belief update within the framework of active inference.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Methodology, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Writing â original draft</p></fn><fn fn-type="con" id="con3"><p>Supervision, Methodology, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Writing â original draft, Project administration, Writing â review and editing, Funding acquisition, Resources, Methodology</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants signed an informed consent form before the experiments. This study was approved by the local ethics committee of the University of Macau (BSERE22-APP006-ICI).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-92892-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All experiment data and analysis codes are available at GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/andlab-um/FreeEnergyEEG">https://github.com/andlab-um/FreeEnergyEEG</ext-link> (copy archived at <xref ref-type="bibr" rid="bib67">Zhang, 2025</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was mainly supported by the Science and Technology Development Fund (FDCT) of Macau [0127/2020/A3, 0041/2022/A], the Natural Science Foundation of Guangdong Province (2021A1515012509), Shenzhen-Hong Kong-Macao Science and Technology Innovation Project (Category C) (SGDX2020110309280100), MYRG of University of Macau (MYRG2022-00188-ICI), NSFC-FDCT Joint Program 0095/2022/AFJ, the SRG of University of Macau (SRG202000027-ICI), the National Key R&amp;D Program of China (2021YFF1200804), National Natural Science Foundation of China (62001205), Shenzhen Science and Technology Innovation Committee (2022410129, KCXFZ2020122117340001), and Guangdong Provincial Key Laboratory of Advanced Biomaterials (2022B1212010003).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname><given-names>D</given-names></name><name><surname>Doll</surname><given-names>BB</given-names></name><name><surname>Long</surname><given-names>NM</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rostrolateral prefrontal cortex and individual differences in uncertainty-driven exploration</article-title><source>Neuron</source><volume>73</volume><fpage>595</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.025</pub-id><pub-id pub-id-type="pmid">22325209</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barto</surname><given-names>A</given-names></name><name><surname>Mirolli</surname><given-names>M</given-names></name><name><surname>Baldassarre</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Novelty or surprise?</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>907</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00907</pub-id><pub-id pub-id-type="pmid">24376428</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bland</surname><given-names>AR</given-names></name><name><surname>Schaefer</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Electrophysiological correlates of decision making under varying levels of uncertainty</article-title><source>Brain Research</source><volume>1417</volume><fpage>55</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2011.08.031</pub-id><pub-id pub-id-type="pmid">21911213</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botelho</surname><given-names>C</given-names></name><name><surname>Fernandes</surname><given-names>C</given-names></name><name><surname>Campos</surname><given-names>C</given-names></name><name><surname>Seixas</surname><given-names>C</given-names></name><name><surname>Pasion</surname><given-names>R</given-names></name><name><surname>Garcez</surname><given-names>H</given-names></name><name><surname>Ferreira-Santos</surname><given-names>F</given-names></name><name><surname>Barbosa</surname><given-names>F</given-names></name><name><surname>Maques-Teixeira</surname><given-names>J</given-names></name><name><surname>Paiva</surname><given-names>TO</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Uncertainty deconstructed: conceptual analysis and state-of-the-art review of the ERP correlates of risk and ambiguity in decision-making</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>23</volume><fpage>522</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.3758/s13415-023-01101-8</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>LD</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>A complete class theorem for statistical problems with finite sample spaces</article-title><source>The Annals of Statistics</source><volume>9</volume><fpage>1289</fpage><lpage>1300</lpage><pub-id pub-id-type="doi">10.1214/aos/1176345645</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckley</surname><given-names>CL</given-names></name><name><surname>Kim</surname><given-names>CS</given-names></name><name><surname>McGregor</surname><given-names>S</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The free energy principle for action and perception: A mathematical review</article-title><source>Journal of Mathematical Psychology</source><volume>81</volume><fpage>55</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2017.09.004</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>JF</given-names></name><name><surname>Figueroa</surname><given-names>CM</given-names></name><name><surname>Cohen</surname><given-names>MX</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Frontal theta reflects uncertainty and unexpectedness during exploration and exploitation</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>2575</fpage><lpage>2586</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr332</pub-id><pub-id pub-id-type="pmid">22120491</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis</article-title><source>The European Journal of Neuroscience</source><volume>35</volume><fpage>1024</fpage><lpage>1035</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07980.x</pub-id><pub-id pub-id-type="pmid">22487033</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Within- and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory</article-title><source>PNAS</source><volume>115</volume><fpage>2502</fpage><lpage>2507</lpage><pub-id pub-id-type="doi">10.1073/pnas.1720963115</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crooks</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Nonequilibrium measurements of free energy differences for microscopically reversible markovian systems</article-title><source>Journal of Statistical Physics</source><volume>90</volume><fpage>1481</fpage><lpage>1487</lpage><pub-id pub-id-type="doi">10.1023/A:1023208217925</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1704</fpage><lpage>1711</lpage><pub-id pub-id-type="doi">10.1038/nn1560</pub-id><pub-id pub-id-type="pmid">16286932</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>OâDoherty</surname><given-names>JP</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical substrates for exploratory decisions in humans</article-title><source>Nature</source><volume>441</volume><fpage>876</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1038/nature04766</pub-id><pub-id pub-id-type="pmid">16778890</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esch</surname><given-names>L</given-names></name><name><surname>Dinh</surname><given-names>C</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>D</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Khan</surname><given-names>S</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>HÃ¤mÃ¤lÃ¤inen</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>MNE: software for acquiring, processing, and visualizing meg/eeg data</article-title><source>Magnetoencephalography: From Signals to Dynamic Cortical Networks</source><volume>01</volume><fpage>355</fpage><lpage>371</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>H</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Trait somatic anxiety is associated with reduced directed exploration and underestimation of uncertainty</article-title><source>Nature Human Behaviour</source><volume>7</volume><fpage>102</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1038/s41562-022-01455-y</pub-id><pub-id pub-id-type="pmid">36192493</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>FitzGerald</surname><given-names>THB</given-names></name><name><surname>Schwartenbeck</surname><given-names>P</given-names></name><name><surname>Moutoussis</surname><given-names>M</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Active inference, evidence accumulation, and the urn task</article-title><source>Neural Computation</source><volume>27</volume><fpage>306</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00699</pub-id><pub-id pub-id-type="pmid">25514108</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Frazier</surname><given-names>PI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A tutorial on bayesian optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.02811">https://arxiv.org/abs/1807.02811</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Free-energy and the brain</article-title><source>Synthese</source><volume>159</volume><fpage>417</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1007/s11229-007-9237-y</pub-id><pub-id pub-id-type="pmid">19325932</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reinforcement learning or active inference?</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e6421</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0006421</pub-id><pub-id pub-id-type="pmid">19641614</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The free-energy principle: a unified brain theory?</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nrn2787</pub-id><pub-id pub-id-type="pmid">20068583</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Active inference and free energy</article-title><source>The Behavioral and Brain Sciences</source><volume>36</volume><fpage>212</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12002142</pub-id><pub-id pub-id-type="pmid">23663424</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Ognibene</surname><given-names>D</given-names></name><name><surname>Mathys</surname><given-names>C</given-names></name><name><surname>Fitzgerald</surname><given-names>T</given-names></name><name><surname>Pezzulo</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Active inference and epistemic value</article-title><source>Cognitive Neuroscience</source><volume>6</volume><fpage>187</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1080/17588928.2015.1020053</pub-id><pub-id pub-id-type="pmid">25689102</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>FitzGerald</surname><given-names>T</given-names></name><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Schwartenbeck</surname><given-names>P</given-names></name><name><surname>O Doherty</surname><given-names>J</given-names></name><name><surname>Pezzulo</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Active inference and learning</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>68</volume><fpage>862</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.06.022</pub-id><pub-id pub-id-type="pmid">27375276</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>FitzGerald</surname><given-names>T</given-names></name><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Schwartenbeck</surname><given-names>P</given-names></name><name><surname>Pezzulo</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Active Inference: a process theory</article-title><source>Neural Computation</source><volume>29</volume><fpage>1</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00912</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galdo</surname><given-names>M</given-names></name><name><surname>Bahg</surname><given-names>G</given-names></name><name><surname>Turner</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Variational Bayesian methods for cognitive science</article-title><source>Psychological Methods</source><volume>25</volume><fpage>535</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1037/met0000242</pub-id><pub-id pub-id-type="pmid">31599616</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Blei</surname><given-names>DM</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decomposing spatiotemporal brain patterns into topographic latent sources</article-title><source>NeuroImage</source><volume>98</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.04.055</pub-id><pub-id pub-id-type="pmid">24791745</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deconstructing the human algorithms for exploration</article-title><source>Cognition</source><volume>173</volume><fpage>34</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2017.12.014</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Uncertainty and Exploration</article-title><source>Decision</source><volume>6</volume><fpage>277</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1037/dec0000101</pub-id><pub-id pub-id-type="pmid">33768122</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>GlÃ¤scher</surname><given-names>J</given-names></name><name><surname>Daw</surname><given-names>N</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>OâDoherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title><source>Neuron</source><volume>66</volume><fpage>585</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.016</pub-id><pub-id pub-id-type="pmid">20510862</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guha</surname><given-names>S</given-names></name><name><surname>Munagala</surname><given-names>K</given-names></name><name><surname>Shi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Approximation algorithms for restless bandit problems</article-title><source>Journal of the ACM</source><volume>58</volume><fpage>1</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1145/1870103.1870106</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Sun</surname><given-names>B</given-names></name><name><surname>Gao</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain areas activated by uncertain reward-based decision-making in healthy volunteers</article-title><source>Neural Regeneration Research</source><volume>8</volume><fpage>3344</fpage><lpage>3352</lpage><pub-id pub-id-type="doi">10.3969/j.issn.1673-5374.2013.35.009</pub-id><pub-id pub-id-type="pmid">25206656</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harper</surname><given-names>J</given-names></name><name><surname>Malone</surname><given-names>SM</given-names></name><name><surname>Iacono</surname><given-names>WG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Theta- and delta-band EEG network dynamics during a novelty oddball task</article-title><source>Psychophysiology</source><volume>54</volume><fpage>1590</fpage><lpage>1605</lpage><pub-id pub-id-type="doi">10.1111/psyp.12906</pub-id><pub-id pub-id-type="pmid">28580687</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huettel</surname><given-names>SA</given-names></name><name><surname>Song</surname><given-names>AW</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Decisions under uncertainty: probabilistic context influences activation of prefrontal and parietal cortices</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>3304</fpage><lpage>3311</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5070-04.2005</pub-id><pub-id pub-id-type="pmid">15800185</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>S</given-names></name><name><surname>Hashmi</surname><given-names>JA</given-names></name><name><surname>Mamashli</surname><given-names>F</given-names></name><name><surname>Michmizos</surname><given-names>K</given-names></name><name><surname>Kitzbichler</surname><given-names>MG</given-names></name><name><surname>Bharadwaj</surname><given-names>H</given-names></name><name><surname>Bekhti</surname><given-names>Y</given-names></name><name><surname>Ganesan</surname><given-names>S</given-names></name><name><surname>Garel</surname><given-names>K-LA</given-names></name><name><surname>Whitfield-Gabrieli</surname><given-names>S</given-names></name><name><surname>Gollub</surname><given-names>RL</given-names></name><name><surname>Kong</surname><given-names>J</given-names></name><name><surname>Vaina</surname><given-names>LM</given-names></name><name><surname>Rana</surname><given-names>KD</given-names></name><name><surname>Stufflebeam</surname><given-names>SM</given-names></name><name><surname>HÃ¤mÃ¤lÃ¤inen</surname><given-names>MS</given-names></name><name><surname>Kenet</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Maturation trajectories of cortical resting-state networks depend on the mediating frequency band</article-title><source>NeuroImage</source><volume>174</volume><fpage>57</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.02.018</pub-id><pub-id pub-id-type="pmid">29462724</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirchhoff</surname><given-names>M</given-names></name><name><surname>Parr</surname><given-names>T</given-names></name><name><surname>Palacios</surname><given-names>E</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Kiverstein</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Markov blankets of life: autonomy, active inference and the free energy principle</article-title><source>Journal of the Royal Society, Interface</source><volume>15</volume><elocation-id>20170792</elocation-id><pub-id pub-id-type="doi">10.1098/rsif.2017.0792</pub-id><pub-id pub-id-type="pmid">29343629</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krain</surname><given-names>AL</given-names></name><name><surname>Wilson</surname><given-names>AM</given-names></name><name><surname>Arbuckle</surname><given-names>R</given-names></name><name><surname>Castellanos</surname><given-names>FX</given-names></name><name><surname>Milham</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Distinct neural mechanisms of risk and ambiguity: a meta-analysis of decision-making</article-title><source>NeuroImage</source><volume>32</volume><fpage>477</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.02.047</pub-id><pub-id pub-id-type="pmid">16632383</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Laskin</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Stooke</surname><given-names>A</given-names></name><name><surname>Pinto</surname><given-names>L</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name><name><surname>Srinivas</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reinforcement learning with augmented data</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>19884</fpage><lpage>19895</lpage></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lehmann</surname><given-names>K</given-names></name><name><surname>Bolis</surname><given-names>D</given-names></name><name><surname>Ramstead</surname><given-names>MJ</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Kanske</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>An active inference approach to second-person neuroscience</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/6y5ve</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>Nelson</surname><given-names>AJ</given-names></name><name><surname>Rustichini</surname><given-names>A</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural representation of subjective value under risk and ambiguity</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>1036</fpage><lpage>1047</lpage><pub-id pub-id-type="doi">10.1152/jn.00853.2009</pub-id><pub-id pub-id-type="pmid">20032238</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Vanni-Mercier</surname><given-names>G</given-names></name><name><surname>Isnard</surname><given-names>J</given-names></name><name><surname>MauguiÃ¨re</surname><given-names>F</given-names></name><name><surname>Dreher</surname><given-names>J-C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The neural dynamics of reward value and risk coding in the human orbitofrontal cortex</article-title><source>Brain</source><volume>139</volume><fpage>1295</fpage><lpage>1309</lpage><pub-id pub-id-type="doi">10.1093/brain/awv409</pub-id><pub-id pub-id-type="pmid">26811252</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Duan</surname><given-names>L</given-names></name><name><surname>Xu</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Gu</surname><given-names>R</given-names></name><name><surname>Luo</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Electrophysiological indexes of option characteristic processing</article-title><source>Psychophysiology</source><volume>56</volume><elocation-id>e13403</elocation-id><pub-id pub-id-type="doi">10.1111/psyp.13403</pub-id><pub-id pub-id-type="pmid">31134663</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>T</given-names></name><name><surname>PÃ¡l</surname><given-names>D</given-names></name><name><surname>PÃ¡l</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Contextual multi-armed bandits</article-title><conf-name>In Proceedings of the Thirteenth international conference on Artificial Intelligence and Statistics</conf-name><fpage>485</fpage><lpage>492</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MartÃ­nez-Cancino</surname><given-names>R</given-names></name><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Truong</surname><given-names>D</given-names></name><name><surname>Artoni</surname><given-names>F</given-names></name><name><surname>Kreutz-Delgado</surname><given-names>K</given-names></name><name><surname>Sivagnanam</surname><given-names>S</given-names></name><name><surname>Yoshimoto</surname><given-names>K</given-names></name><name><surname>Majumdar</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The open EEGLAB portal interface: high-performance computing with EEGLAB</article-title><source>NeuroImage</source><volume>224</volume><elocation-id>116778</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116778</pub-id><pub-id pub-id-type="pmid">32289453</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>OâReilly</surname><given-names>CA</given-names><suffix>III</suffix></name><name><surname>Tushman</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Organizational ambidexterity in action: how managers explore and exploit</article-title><source>California Management Review</source><volume>53</volume><fpage>5</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1525/cmr.2011.53.4.5</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oudeyer</surname><given-names>P-Y</given-names></name><name><surname>Kaplan</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What is intrinsic motivation? a typology of computational approaches</article-title><source>Frontiers in Neurorobotics</source><volume>1</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.12.006.2007</pub-id><pub-id pub-id-type="pmid">18958277</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Parr</surname><given-names>T</given-names></name><name><surname>Pezzulo</surname><given-names>G</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><source>Active Inference: The Free Energy Principle in Mind, Brain, and Behavior</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/12441.001.0001</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pascual-Marqui</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Discrete, 3d distributed, linear imaging methods of electric neuronal activity. Part 1: Exact, Zero Error Localization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/0710.3341">https://arxiv.org/abs/0710.3341</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulus</surname><given-names>MP</given-names></name><name><surname>Hozack</surname><given-names>N</given-names></name><name><surname>Zauscher</surname><given-names>B</given-names></name><name><surname>McDowell</surname><given-names>JE</given-names></name><name><surname>Frank</surname><given-names>L</given-names></name><name><surname>Brown</surname><given-names>GG</given-names></name><name><surname>Braff</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Prefrontal, parietal, and temporal cortex networks underlie decision-making in the presence of uncertainty</article-title><source>NeuroImage</source><volume>13</volume><fpage>91</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1006/nimg.2000.0667</pub-id><pub-id pub-id-type="pmid">11133312</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payzan-LeNestour</surname><given-names>E</given-names></name><name><surname>Dunne</surname><given-names>S</given-names></name><name><surname>Bossaerts</surname><given-names>P</given-names></name><name><surname>OâDoherty</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The neural representation of unexpected uncertainty during value-based decision making</article-title><source>Neuron</source><volume>79</volume><fpage>191</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.037</pub-id><pub-id pub-id-type="pmid">23849203</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ragni</surname><given-names>M</given-names></name><name><surname>Franzmeier</surname><given-names>I</given-names></name><name><surname>Maier</surname><given-names>S</given-names></name><name><surname>Knauff</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Uncertain relational reasoning in the parietal cortex</article-title><source>Brain and Cognition</source><volume>104</volume><fpage>72</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2016.02.006</pub-id><pub-id pub-id-type="pmid">26970943</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raja</surname><given-names>V</given-names></name><name><surname>Valluri</surname><given-names>D</given-names></name><name><surname>Baggs</surname><given-names>E</given-names></name><name><surname>Chemero</surname><given-names>A</given-names></name><name><surname>Anderson</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The Markov blanket trick: On the scope of the free energy principle and active inference</article-title><source>Physics of Life Reviews</source><volume>39</volume><fpage>49</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.plrev.2021.09.001</pub-id><pub-id pub-id-type="pmid">34563472</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Wan</surname><given-names>Z</given-names></name><name><surname>Cheng</surname><given-names>W</given-names></name><name><surname>Feng</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Risk-taking in humans and the medial orbitofrontal cortex reward system</article-title><source>NeuroImage</source><volume>249</volume><elocation-id>118893</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.118893</pub-id><pub-id pub-id-type="pmid">35007715</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Formal theory of creativity, fun, and intrinsic motivation (1990â2010)</article-title><source>IEEE Transactions on Autonomous Mental Development</source><volume>2</volume><fpage>230</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1109/TAMD.2010.2056368</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schulz</surname><given-names>E</given-names></name><name><surname>Konstantinidis</surname><given-names>E</given-names></name><name><surname>Speekenbrink</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Exploration-exploitation in a contextual multi-armed bandit task</article-title><conf-name>In: International conference on cognitive modeling</conf-name><fpage>118</fpage><lpage>123</lpage></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartenbeck</surname><given-names>P</given-names></name><name><surname>Passecker</surname><given-names>J</given-names></name><name><surname>Hauser</surname><given-names>TU</given-names></name><name><surname>FitzGerald</surname><given-names>TH</given-names></name><name><surname>Kronbichler</surname><given-names>M</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Computational mechanisms of curiosity and goal-directed exploration</article-title><source>eLife</source><volume>8</volume><elocation-id>e41703</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.41703</pub-id><pub-id pub-id-type="pmid">31074743</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Dickhaut</surname><given-names>J</given-names></name><name><surname>McCabe</surname><given-names>K</given-names></name><name><surname>Pardo</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Neuronal substrates for choice under ambiguity, risk, gains, and losses</article-title><source>Management Science</source><volume>48</volume><fpage>711</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1287/mnsc.48.6.711.194</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>S</given-names></name><name><surname>Zhen</surname><given-names>S</given-names></name><name><surname>Fu</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>D-A</given-names></name><name><surname>Shimojo</surname><given-names>S</given-names></name><name><surname>Adolphs</surname><given-names>R</given-names></name><name><surname>Yu</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decision ambiguity is mediated by a late positive potential originating from cingulate cortex</article-title><source>NeuroImage</source><volume>157</volume><fpage>400</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.003</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>The reinforcement learning problem</chapter-title><person-group person-group-type="editor"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><source>Reinforcement Learning: An Introduction</source><publisher-name>Udemy</publisher-name><fpage>51</fpage><lpage>85</lpage></element-citation></ref><ref id="bib59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement learning: an introduction</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomov</surname><given-names>MS</given-names></name><name><surname>Truong</surname><given-names>VQ</given-names></name><name><surname>Hundia</surname><given-names>RA</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dissociable neural correlates of uncertainty underlie different exploration strategies</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>2371</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-15766-z</pub-id><pub-id pub-id-type="pmid">32398675</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vrieze</surname><given-names>SI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Model selection and psychological theory: a discussion of the differences between the Akaike information criterion (AIC) and the Bayesian information criterion (BIC)</article-title><source>Psychological Methods</source><volume>17</volume><fpage>228</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1037/a0027127</pub-id><pub-id pub-id-type="pmid">22309957</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wald</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1947">1947</year><article-title>An essentially complete class of admissible decision functions</article-title><source>The Annals of Mathematical Statistics</source><volume>18</volume><fpage>549</fpage><lpage>555</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177730345</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Zheng</surname><given-names>J</given-names></name><name><surname>Huang</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>P300 and decision making under risk and ambiguity</article-title><source>Computational Intelligence and Neuroscience</source><volume>2015</volume><elocation-id>108417</elocation-id><pub-id pub-id-type="doi">10.1155/2015/108417</pub-id><pub-id pub-id-type="pmid">26539213</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>JX</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Tirumala</surname><given-names>D</given-names></name><name><surname>Soyer</surname><given-names>H</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Munos</surname><given-names>R</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Learning to Reinforcement Learn</source><publisher-name>Udemy</publisher-name></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>TB</given-names></name><name><surname>Burke</surname><given-names>CJ</given-names></name><name><surname>Nebe</surname><given-names>S</given-names></name><name><surname>Preuschoff</surname><given-names>K</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name><name><surname>Tobler</surname><given-names>PN</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Testing models at the neural level reveals how the brain computes subjective value</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2106237118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2106237118</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Geana</surname><given-names>A</given-names></name><name><surname>White</surname><given-names>JM</given-names></name><name><surname>Ludvig</surname><given-names>EA</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Humans use directed and random exploration to solve the explore-exploit dilemma</article-title><source>Journal of Experimental Psychology. General</source><volume>143</volume><fpage>2074</fpage><lpage>2081</lpage><pub-id pub-id-type="doi">10.1037/a0038199</pub-id><pub-id pub-id-type="pmid">25347535</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>FreeEnergyEEG</data-title><version designator="swh:1:rev:4e0953016e65961327ba5be1075f5aad52f3ba40">swh:1:rev:4e0953016e65961327ba5be1075f5aad52f3ba40</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:6f2be3dbfbf29fd71797cefdc297e56e742f1136;origin=https://github.com/andlab-um/FreeEnergyEEG;visit=swh:1:snp:366f2a8d69b21472365b7e87cfd7bafba10ffbb8;anchor=swh:1:rev:4e0953016e65961327ba5be1075f5aad52f3ba40">https://archive.softwareheritage.org/swh:1:dir:6f2be3dbfbf29fd71797cefdc297e56e742f1136;origin=https://github.com/andlab-um/FreeEnergyEEG;visit=swh:1:snp:366f2a8d69b21472365b7e87cfd7bafba10ffbb8;anchor=swh:1:rev:4e0953016e65961327ba5be1075f5aad52f3ba40</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Yi</surname><given-names>W</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Common and distinct electrophysiological correlates of feedback processing during risky and ambiguous decision making</article-title><source>Neuropsychologia</source><volume>146</volume><elocation-id>107526</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2020.107526</pub-id><pub-id pub-id-type="pmid">32535129</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Supplementary method</title><sec sec-type="appendix" id="s8-1"><title>Model-free reinforcement learning</title><p>We used a model-free reinforcement learning approach as a baseline model. It involves two value functions, <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, which respectively describe the expected values of different options for the participants:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> equals 6, corresponding to the fixed reward obtained for choosing the safe option. <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> represents the total expected reward for choosing the âStayâ option in this round for the participant, while <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> represents the total expected reward for choosing the âCueâ option in this round. <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> respectively denote the expected rewards for choosing the âSafeâ option in âContext 1â and âContext 2â, whereas <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> represent the expected rewards for choosing the âRiskâ option in âContext 1â and âContext 2â.<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> corresponds to choosing the âStayâ option, and <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula> corresponds to choosing the âCueâ option.</p><p>If participants decide whether to choose the âSafeâ option or the âRiskâ option when knowing the context (âContext 1â):<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:msub><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>If participants decide whether to choose the âafeâ option or the âRiskâ option when knowing the context (âContext 2â):<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>If participants decide whether to choose the âSafeâ option or the âRiskâ option when without knowing the context:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>After participants make two choices and receive rewards <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are updated using delta learning rule (<xref ref-type="bibr" rid="bib58">Sutton and Barto, 1998</xref>).</p><p>If participants chose the âStayâ option:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>If participants chose the âCueâ option:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo></mml:mrow></mml:math></disp-formula></p><p>(â1) is the cost of choosing the âCueâ option and Î± is the learning rate. <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> always equal 6 because choosing the âSafeâ option consistently results in a fixed reward of 6.</p><p>If participants chose the âRiskâ option when knowing the context (âContext 1â):<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo></mml:math></disp-formula></p><p>If participants chose the âRiskâ option when knowing the context (âContext 2â):<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo></mml:math></disp-formula></p><p>If participants chose the âRiskâ option when without knowing the context:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mtable rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>â</mml:mo><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>â</mml:mo><mml:mi>Î±</mml:mi><mml:mo>â</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>â</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s8-2"><title>Model-based reinforcement learning</title><p>Here, we used a model-based reinforcement learning model as another baseline model. It involves the same likelihood matrix mapping from hidden states to outcomes <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi></mml:mstyle></mml:math></inline-formula> and the concentration parameters of likelihood <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p>When participants decide whether to choose the âStayâ option or the âCueâ option, they consider the expected values of the âStayâ option, the âCueâ option, the âSafeâ option, and the âRiskâ option under both âContext 1â and âContext 2â:<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>â</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>â</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula><disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>6.</mml:mn></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mstyle></mml:math></inline-formula> equals [6, 12, 9, 3, 0, 0, â1, â1]. If participants choose the âStayâ option, they will not know the context and make the decision based on the expected values of the âSafeâ option and the âRiskâ option:<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>If participants choose the âCueâ option, they can then decide whether to choose the âSafeâ option or the âRiskâ option based on the context:<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>6</mml:mn><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>6</mml:mn><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Here, (â1) is the cost of choosing the âCueâ option. The probabilities of choosing the âStayâ option and the âCueâ option are, respectively:<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>If participants decide whether to choose the âSafeâ option or the âRiskâ option when knowing the context (âContext 1â):<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>If participants decide whether to choose the âSafeâ option or the âRiskâ option when knowing the context (âContext 2â):<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>If participants decide whether to choose the âSafeâ option or the âRiskâ option when without knowing the context:<disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>Î³</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The process of value updating is the same as the process of value updating in active inference:<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>Î±</mml:mi><mml:munder><mml:mo>â</mml:mo><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>â</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, Î± is the learning rate.</p></sec></sec><sec sec-type="appendix" id="s9"><title>Supplementary results</title><sec sec-type="appendix" id="s9-1"><title>Simulation results</title><p>The simulation experiments primarily focus on the utility of AI, AL, and EX in the active inference models, comparing the differences in decision-making among agents with varying AI, AL, and EX parameter values, considering their varying tendencies to avoid risk, reduce ambiguity, and maximize extrinsic rewards. The simulation experiments in the supplementary materials considered two situations <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>.</p><fig id="app1fig1" position="float"><label>Appendix 1âfigure 1.</label><caption><title>The simulation experiment results.</title><p>This figure demonstrates how an agent selects actions and updates beliefs over 60 trials in the active inference framework. The first two panels (<bold>a, b</bold>) display the agentâs policy and depict how the policy probabilities are updated (choosing between the stay or cue option in the first choice, and selecting between the safe or risky option in the second choice). The scatter plot indicates the agentâs actions, with green representing the cue option when the context of the risky path is âContext 1â (high-reward context), orange representing the cue option when the context of the risky path is âContext 2â (low-reward context), purple representing the stay option when the agent is uncertain about the context of the risky path, and blue indicating the safe-risky choice. The shaded region represents the agentâs confidence, with darker shaded regions indicating greater confidence. The third panel (<bold>c</bold>) displays the rewards obtained by the agent in each trial. The fourth panel (<bold>d</bold>) shows the prediction error of the agent in each trial. Finally, the fifth panel (<bold>e</bold>) illustrates the expected rewards of the âRisky Pathâ in the two contexts of the agent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-app1-fig1-v1.tif"/></fig><p>When AI and AL are set to 0 and EX is set to 10, it is expected that the agent will solely maximize extrinsic rewards without engaging in exploration. <xref ref-type="fig" rid="app1fig2">Appendix 1âfigure 2</xref> depicts the decisions of such an agent. In the initial trials, the agent almost exclusively selects the âStayâ option, choosing the âCueâ option only occasionally, as the expected rewards for the âRiskâ and âSafeâ options are both 6 initially, and choosing the âCueâ option involves a cost. Without a bias toward either the âSafeâ or âRiskâ option, the agentâs second choice is random. In the latter half, as the agent occasionally selects the âCueâ and âRiskâ options, updating its expectations of the âRiskâ optionâs reward, it increases the probability of choosing the âCueâ option. For the second choice, the agent eventually learns the optimal strategy, selecting the âRiskâ option in âContext 1â and the âSafeâ option in âContext 2â. While the agent with AI and AL set to 0 can eventually learn the optimal strategy, its rate of strategy optimization is significantly slower compared to an agent with non-zero values for all three parameters.</p><fig id="app1fig2" position="float"><label>Appendix 1âfigure 2.</label><caption><title>The simulation experiment results.</title><p>This figure demonstrates how an agent selects actions and updates beliefs over 60 trials in the active inference framework. The first two panels (<bold>a, b</bold>) display the agentâs policy and depict how the policy probabilities are updated (choosing between the stay or cue option in the first choice, and selecting between the safe or risky option in the second choice). The scatter plot indicates the agentâs actions, with green representing the cue option when the context of the risky path is âContext 1â (high-reward context), orange representing the cue option when the context of the risky path is âContext 2â (low-reward context), purple representing the stay option when the agent is uncertain about the context of the risky path, and blue indicating the safe-risky choice. The shaded region represents the agentâs confidence, with darker shaded regions indicating greater confidence. The third panel (<bold>c</bold>) displays the rewards obtained by the agent in each trial. The fourth panel (<bold>d</bold>) shows the prediction error of the agent in each trial. Finally, the fifth panel (<bold>e</bold>) illustrates the expected rewards of the âRisky Pathâ in the two contexts of the agent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-app1-fig2-v1.tif"/></fig><p>If EX is set to 0, and AI and AL are set to 10, the agent is expected to solely maximize information gain without pursuing higher extrinsic rewards. <xref ref-type="fig" rid="app1fig1">Appendix 1âfigure 1</xref> illustrates the decisions of such an agent. Regardless of the experimentâs stage, the agent overwhelmingly prefers the âCueâ option in the first choice, as it consistently offers the same value in avoiding risk. Similarly, in the second choice, the agent favors the âRiskâ option due to its ability to reduce ambiguity. The agent explores the âRiskâ option equally in both âContext 1â and âContext 2â. However, it is noteworthy that in the latter half of the experiment, the agentâs probability of selecting the âSafeâ option increases, as continued exploration of the âRiskâ option decreases its perceived ambiguity, rendering its informational value negligible.</p><p>In comparison, the active inference model exhibits an excellent exploration mechanism, enabling the agent to quickly learn optimal strategies in uncertain environments. Disabling the modelâs AL and AI parameters by setting them to 0 undermines this exploration mechanism, forcing the agent to optimize its strategy slowly through random exploration.</p></sec><sec sec-type="appendix" id="s9-2"><title>Model recovery</title><p>To demonstrate how reliable our models are (the active inference model, model-free reinforcement learning model, and model-based reinforcement learning model), we run some simulation experiments for model recovery. We use these three models, with their own fitting parameters, to generate some simulated data. Then we will fit all three sets of data using these three models. The model recovery results are shown in <xref ref-type="fig" rid="app1fig3">Appendix 1âfigure 3</xref>. This is the confusion matrix of models: the percentage of all subjects simulated based on a certain model that is fitted best by a certain model. The goodness of fit was compared using the Bayesian information criterion. We can see that the result of model recovery is very good, and the simulated data generated by a model can be best explained by this model.</p><fig id="app1fig3" position="float"><label>Appendix 1âfigure 3.</label><caption><title>Model recovery results.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-app1-fig3-v1.tif"/></fig></sec><sec sec-type="appendix" id="s9-3"><title>Source estimate results</title><p>In our results analysis, different criteria lead to different outcomes. For instance, requiring a certain proportion of brain regions to exhibit significant correlations, specifying whether the significance level should be p&lt;0.05 or p&lt;0.01, or mandating a minimum duration for significant correlations in a certain proportion of brain regions all yield distinct results. Therefore, we compiled all brain regions that meet various criteria in the supplementary materials.</p><fig id="app1fig4" position="float"><label>Appendix 1âfigure 4.</label><caption><title>The source estimation results of extrinsic value in the two choosing stages.</title><p>(<bold>a</bold>) The regression intensity (<inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula>) of extrinsic value in the âFirst choiceâ stage. The right panel indicates the regression intensity between the middle temporal gyrus (6, right half) and extrinsic value. The green-shaded regions indicate p&lt;0.05 after false discovery rate (FDR) correction (the average <italic>t</italic>-value during these significant periods equals 3.673). (<bold>b</bold>) The regression intensity (<inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula>) of extrinsic value in the âSecond choiceâ stage. The right panel indicates the regression intensity between the rostral middle frontal gyrus (6, left half) and extrinsic value. The yellow-shaded regions indicate p&lt;0.001 after FDR correction (the average <italic>t</italic>-value during these significant periods equals <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>4.740</mml:mn></mml:mstyle></mml:math></inline-formula>). The black lines indicate the average intensities, and the gray-shaded regions indicate the ranges of variations. The gray lines indicate p&lt;0.05 before FDR.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-app1-fig4-v1.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1âfigure 5.</label><caption><title>The source estimation results of extrinsic value and prediction error in the âSecond resultâ stage.</title><p>(<bold>a</bold>) The regression intensity (<inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula>) of extrinsic value. The right panel indicates the regression intensity between the lateral occipital cortex (3, right half) and extrinsic value. The green-shaded regions indicate p&lt;0.05 after false discovery rate (FDR) correction (the average <italic>t</italic>-value during these significant periods equals 2.875). (<bold>b</bold>) The regression intensity (<inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Î²</mml:mi></mml:mstyle></mml:math></inline-formula>) of prediction error. The right panel indicates the regression intensity between the lateral occipital cortex (3, right half) and prediction error. The green-shaded regions indicate p&lt;0.05 after FDR correction (the average <italic>t</italic>-value during these significant periods equals â2.716). The black lines indicate the average intensities, and the gray-shaded regions indicate the ranges of variations. The gray lines indicate p&lt;0.05 before FDR.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-app1-fig5-v1.tif"/></fig><fig id="app1fig6" position="float"><label>Appendix 1âfigure 6.</label><caption><title>The source estimation results of ambiguity in the âSecond choiceâ stage.</title><p>The right panel indicates the regression intensity between the frontal pole (1, left half) and ambiguity. The black line indicates the average intensities, and the gray-shaded regions indicate the ranges of variations. The gray lines indicate p&lt;0.05 before FDR.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-92892-app1-fig6-v1.tif"/></fig><table-wrap id="app1table1" position="float"><label>Appendix 1âtable 1.</label><caption><title>In the âStay/Cueâ choice, we require that the activity of more than 50% of brain regions remains significantly correlated with the expected free energy for over 0.32 seconds, with a significance p&lt;0.05 (after false discovery rate correction).</title><p>The brain regions are delineated according to the &quot;aparc sub&quot; parcellation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Regressor</th><th align="left" valign="bottom">p-Value</th><th align="left" valign="bottom">Proportion</th><th align="left" valign="bottom">Duration</th><th align="left" valign="bottom"/></tr></thead><tbody><tr><td align="left" valign="bottom">Expected free energy</td><td align="left" valign="bottom">0.01</td><td align="left" valign="bottom">0.5</td><td align="left" valign="bottom">0.32 seconds</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>Brain region</bold></td><td align="left" valign="bottom"><bold>Duration</bold></td><td align="left" valign="bottom"><bold>Proportion</bold></td><td align="left" valign="bottom"><bold>Regression coefficient</bold></td><td align="left" valign="bottom"><bold><italic>t</italic>-Value</bold></td></tr><tr><td align="left" valign="bottom">Frontalpole 1-lh</td><td align="left" valign="bottom">0.34</td><td align="left" valign="bottom">0.8965</td><td align="left" valign="bottom">1.342 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.136</td></tr><tr><td align="left" valign="bottom">Frontalpole 1-rh</td><td align="left" valign="bottom">0.372</td><td align="left" valign="bottom">0.9086</td><td align="left" valign="bottom">1.357 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.228</td></tr><tr><td align="left" valign="bottom">Lateralorbitofrontal 2-lh</td><td align="left" valign="bottom">0.364</td><td align="left" valign="bottom">0.8608</td><td align="left" valign="bottom">1.526 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.235</td></tr><tr><td align="left" valign="bottom">Lateralorbitofrontal 3-lh</td><td align="left" valign="bottom">0.332</td><td align="left" valign="bottom">0.7691</td><td align="left" valign="bottom">1.362 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.195</td></tr><tr><td align="left" valign="bottom">Lateralorbitofrontal 5-lh</td><td align="left" valign="bottom">0.336</td><td align="left" valign="bottom">0.7778</td><td align="left" valign="bottom">1.365 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.126</td></tr><tr><td align="left" valign="bottom">Lateralorbitofrontal 5-rh</td><td align="left" valign="bottom">0.332</td><td align="left" valign="bottom">0.8193</td><td align="left" valign="bottom">1.333 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â2.984</td></tr><tr><td align="left" valign="bottom">Lateralorbitofrontal 6-lh</td><td align="left" valign="bottom">0.372</td><td align="left" valign="bottom">0.9086</td><td align="left" valign="bottom">1.550 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.335</td></tr><tr><td align="left" valign="bottom">Lateralorbitofrontal 7-lh</td><td align="left" valign="bottom">0.332</td><td align="left" valign="bottom">0.8614</td><td align="left" valign="bottom">1.584 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.062</td></tr><tr><td align="left" valign="bottom">Lateralorbitofrontal 7-rh</td><td align="left" valign="bottom">0.34</td><td align="left" valign="bottom">0.8902</td><td align="left" valign="bottom">1.530 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.059</td></tr><tr><td align="left" valign="bottom">Medialorbitofrontal 1-lh</td><td align="left" valign="bottom">0.356</td><td align="left" valign="bottom">0.9711</td><td align="left" valign="bottom">1.624 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.068</td></tr><tr><td align="left" valign="bottom">Medialorbitofrontal 1-rh</td><td align="left" valign="bottom">0.348</td><td align="left" valign="bottom">0.9234</td><td align="left" valign="bottom">1.563 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.049</td></tr><tr><td align="left" valign="bottom">Medialorbitofrontal 2-lh</td><td align="left" valign="bottom">0.348</td><td align="left" valign="bottom">0.9163</td><td align="left" valign="bottom">1.360 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.038</td></tr><tr><td align="left" valign="bottom">Medialorbitofrontal 3-rh</td><td align="left" valign="bottom">0.336</td><td align="left" valign="bottom">0.9048</td><td align="left" valign="bottom">1.402 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.016</td></tr><tr><td align="left" valign="bottom">Medialorbitofrontal 4-lh</td><td align="left" valign="bottom">0.364</td><td align="left" valign="bottom">0.8767</td><td align="left" valign="bottom">1.436 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.165</td></tr><tr><td align="left" valign="bottom">Medialorbitofrontal 5-lh</td><td align="left" valign="bottom">0.38</td><td align="left" valign="bottom">0.8579</td><td align="left" valign="bottom">1.502 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.420</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 11-lh</td><td align="left" valign="bottom">0.364</td><td align="left" valign="bottom">0.8773</td><td align="left" valign="bottom">1.604 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.200</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 11-rh</td><td align="left" valign="bottom">0.344</td><td align="left" valign="bottom">0.8469</td><td align="left" valign="bottom">1.486 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.112</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 12-lh</td><td align="left" valign="bottom">0.344</td><td align="left" valign="bottom">0.8857</td><td align="left" valign="bottom">1.494 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.120</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 12-rh</td><td align="left" valign="bottom">0.336</td><td align="left" valign="bottom">0.7840</td><td align="left" valign="bottom">1.189 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.210</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 13-rh</td><td align="left" valign="bottom">0.34</td><td align="left" valign="bottom">0.8510</td><td align="left" valign="bottom">1.428 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.220</td></tr><tr><td align="left" valign="bottom">Superiorfrontal 1-lh</td><td align="left" valign="bottom">0.336</td><td align="left" valign="bottom">0.9122</td><td align="left" valign="bottom">1.420 Ã 10<sup>â11</sup></td><td align="left" valign="bottom">â3.101</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1âtable 2.</label><caption><title>In the âStay/Cueâ choice, we require that the activity of more than 50% of brain regions remains significantly correlated with the value of reducing risk for over 0.152 seconds, with a significance p&lt;0.05 (after false discovery rate correction).</title><p>The brain regions are delineated according to the &quot;aparc sub&quot; parcellation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Regressor</th><th align="left" valign="top">p-Value</th><th align="left" valign="top">Proportion</th><th align="left" valign="top">Duration</th><th align="left" valign="top"/></tr></thead><tbody><tr><td align="left" valign="top">Value of avoiding risk</td><td align="char" char="." valign="top">0.05</td><td align="char" char="." valign="top">0.5</td><td align="char" char="." valign="top">0.152 seconds</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top"><bold>Brain region</bold></td><td align="left" valign="top"><bold>Duration</bold></td><td align="left" valign="top"><bold>Proportion</bold></td><td align="left" valign="top"><bold>Regression coefficient</bold></td><td align="left" valign="top"><italic><bold>t</bold></italic><bold>-Value</bold></td></tr><tr><td align="left" valign="top">Caudalmiddlefrontal 5-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.9676</td><td align="left" valign="top">1.151 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.363</td></tr><tr><td align="left" valign="top">Caudalmiddlefrontal 6-lh</td><td align="char" char="." valign="top">0.156</td><td align="char" char="." valign="top">0.9837</td><td align="left" valign="top">1.006 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.307</td></tr><tr><td align="left" valign="top">Insula 2-lh</td><td align="char" char="." valign="top">0.156</td><td align="char" char="." valign="top">0.9597</td><td align="left" valign="top">1.270 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.350</td></tr><tr><td align="left" valign="top">Insula 3-lh</td><td align="char" char="." valign="top">0.156</td><td align="char" char="." valign="top">0.9359</td><td align="left" valign="top">1.229 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.251</td></tr><tr><td align="left" valign="top">Medialorbitofrontal 5-lh</td><td align="char" char="." valign="top">0.164</td><td align="char" char="." valign="top">0.8659</td><td align="left" valign="top">1.386 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.081</td></tr><tr><td align="left" valign="top">Parsopercularis 3-lh</td><td align="char" char="." valign="top">0.160</td><td align="char" char="." valign="top">0.9479</td><td align="left" valign="top">1.450 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.334</td></tr><tr><td align="left" valign="top">Postcentral 10-lh</td><td align="char" char="." valign="top">0.160</td><td align="char" char="." valign="top">0.9450</td><td align="left" valign="top">1.071 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.054</td></tr><tr><td align="left" valign="top">Postcentral 11-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.9737</td><td align="left" valign="top">1.111 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.171</td></tr><tr><td align="left" valign="top">Postcentral 13-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.8991</td><td align="left" valign="top">1.190 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â2.951</td></tr><tr><td align="left" valign="top">Precentral 11-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.9799</td><td align="left" valign="top">1.033 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.376</td></tr><tr><td align="left" valign="top">Precentral 12-lh</td><td align="char" char="." valign="top">0.156</td><td align="char" char="." valign="top">0.9630</td><td align="left" valign="top">1.170 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.280</td></tr><tr><td align="left" valign="top">Precentral 13-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.9868</td><td align="left" valign="top">1.033 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.347</td></tr><tr><td align="left" valign="top">Precentral 8-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.9757</td><td align="left" valign="top">1.028 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.434</td></tr><tr><td align="left" valign="top">Precentral 9-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.9649</td><td align="left" valign="top">9.692 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.323</td></tr><tr><td align="left" valign="top">Rostralanteriorcingulate 2-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.8816</td><td align="left" valign="top">1.007 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â2.970</td></tr><tr><td align="left" valign="top">Superiorfrontal 17-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.9868</td><td align="left" valign="top">1.001 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.371</td></tr><tr><td align="left" valign="top">Supramarginal 3-lh</td><td align="char" char="." valign="top">0.156</td><td align="char" char="." valign="top">0.8521</td><td align="left" valign="top">1.260 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.068</td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1âtable 3.</label><caption><title>In the âStay/Cueâ choice, we require that the activity of more than 50% of brain regions remains significantly correlated with the extrinsic value for over 0.128 seconds, with a significance p&lt;0.05 (after false discovery rate correction).</title><p>The brain regions are delineated according to the &quot;aparc sub&quot; parcellation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Regressor</th><th align="left" valign="top">p-Value</th><th align="left" valign="top">Proportion</th><th align="left" valign="top">Duration</th><th align="left" valign="top"/></tr></thead><tbody><tr><td align="left" valign="top">Extrinsic value</td><td align="char" char="." valign="top">0.05</td><td align="char" char="." valign="top">0.5</td><td align="char" char="." valign="top">0.128 seconds</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top"><bold>Brain region</bold></td><td align="left" valign="top"><bold>Duration</bold></td><td align="left" valign="top"><bold>Proportion</bold></td><td align="left" valign="top"><bold>regression coefficient</bold></td><td align="left" valign="top"><italic><bold>t</bold></italic><bold>-Value</bold></td></tr><tr><td align="left" valign="top">Bankssts 1-rh</td><td align="char" char="." valign="top">0.136</td><td align="char" char="." valign="top">0.9632</td><td align="left" valign="top">4.173 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.547</td></tr><tr><td align="left" valign="top">Bankssts 2-rh</td><td align="char" char="." valign="top">0.128</td><td align="char" char="." valign="top">0.9583</td><td align="left" valign="top">3.646 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.432</td></tr><tr><td align="left" valign="top">Fusiform 7-rh</td><td align="char" char="." valign="top">0.136</td><td align="char" char="." valign="top">0.9647</td><td align="left" valign="top">4.727 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.806</td></tr><tr><td align="left" valign="top">Inferiorparietal 9-rh</td><td align="char" char="." valign="top">0.132</td><td align="char" char="." valign="top">0.9532</td><td align="left" valign="top">3.946 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.560</td></tr><tr><td align="left" valign="top">Inferiortemporal 4-rh</td><td align="char" char="." valign="top">0.132</td><td align="char" char="." valign="top">0.9731</td><td align="left" valign="top">5.742 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.786</td></tr><tr><td align="left" valign="top">Inferiortemporal 5-rh</td><td align="char" char="." valign="top">0.14</td><td align="char" char="." valign="top">0.9771</td><td align="left" valign="top">5.648 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.917</td></tr><tr><td align="left" valign="top">Inferiortemporal 6-rh</td><td align="char" char="." valign="top">0.136</td><td align="char" char="." valign="top">1.0000</td><td align="left" valign="top">5.635 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.651</td></tr><tr><td align="left" valign="top">Inferiortemporal 7-rh</td><td align="char" char="." valign="top">0.136</td><td align="char" char="." valign="top">0.9559</td><td align="left" valign="top">4.850 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.510</td></tr><tr><td align="left" valign="top">Middletemporal 1-rh</td><td align="char" char="." valign="top">0.132</td><td align="char" char="." valign="top">0.9495</td><td align="left" valign="top">4.260 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.326</td></tr><tr><td align="left" valign="top">Middletemporal 3-rh</td><td align="char" char="." valign="top">0.14</td><td align="char" char="." valign="top">0.9679</td><td align="left" valign="top">5.060 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.521</td></tr><tr><td align="left" valign="top">Middletemporal 4-rh</td><td align="char" char="." valign="top">0.128</td><td align="char" char="." valign="top">0.9688</td><td align="left" valign="top">4.636 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.564</td></tr><tr><td align="left" valign="top">Middletemporal 5-rh</td><td align="char" char="." valign="top">0.132</td><td align="char" char="." valign="top">0.9610</td><td align="left" valign="top">5.078 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.490</td></tr><tr><td align="left" valign="top">Middletemporal 6-rh</td><td align="char" char="." valign="top">0.164</td><td align="char" char="." valign="top">0.9338</td><td align="left" valign="top">5.983 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.673</td></tr><tr><td align="left" valign="top">Middletemporal 7-rh</td><td align="char" char="." valign="top">0.128</td><td align="char" char="." valign="top">0.8880</td><td align="left" valign="top">4.359 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.242</td></tr><tr><td align="left" valign="top">Superiortemporal 3-rh</td><td align="char" char="." valign="top">0.128</td><td align="char" char="." valign="top">0.9886</td><td align="left" valign="top">3.652 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.454</td></tr><tr><td align="left" valign="top">Superiortemporal 4-rh</td><td align="char" char="." valign="top">0.14</td><td align="char" char="." valign="top">0.9629</td><td align="left" valign="top">4.585 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.494</td></tr><tr><td align="left" valign="top">Superiortemporal 5-rh</td><td align="char" char="." valign="top">0.132</td><td align="char" char="." valign="top">0.9865</td><td align="left" valign="top">4.22 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.299</td></tr><tr><td align="left" valign="top">Superiortemporal 6-rh</td><td align="char" char="." valign="top">0.132</td><td align="char" char="." valign="top">0.9449</td><td align="left" valign="top">3.658 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.325</td></tr><tr><td align="left" valign="top">Superiortemporal 9-rh</td><td align="char" char="." valign="top">0.132</td><td align="char" char="." valign="top">0.9596</td><td align="left" valign="top">3.435 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">3.429</td></tr></tbody></table></table-wrap><table-wrap id="app1table4" position="float"><label>Appendix 1âtable 4.</label><caption><title>In the result stage after the &quot;Stay/Cue&quot; choice, we require that the activity of more than 50% of brain regions remains significantly correlated with (the value of) avoiding risk for over 0.32 seconds, with a significance p&lt;0.05 (after false discovery rate correction).</title><p>The brain regions are delineated according to the âaparc subâ parcellation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Regressor</th><th align="left" valign="bottom">p-value</th><th align="left" valign="bottom">Proportion</th><th align="left" valign="bottom">Duration</th><th align="left" valign="bottom"/></tr></thead><tbody><tr><td align="left" valign="bottom">(The value of) avoiding risk</td><td align="char" char="." valign="bottom">0.05</td><td align="char" char="." valign="bottom">0.5</td><td align="char" char="." valign="bottom">0.32 seconds</td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom"><bold>Brain region</bold></td><td align="left" valign="bottom"><bold>Duration</bold></td><td align="left" valign="bottom"><bold>Proportion</bold></td><td align="left" valign="bottom"><bold>regression coefficient</bold></td><td align="left" valign="bottom"><italic><bold>t</bold></italic><bold>-Value</bold></td></tr><tr><td align="left" valign="bottom">Caudalanteriorcingulate 1-lh</td><td align="char" char="." valign="bottom">0.344</td><td align="char" char="." valign="bottom">0.9341</td><td align="left" valign="bottom">1.523 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.023</td></tr><tr><td align="left" valign="bottom">Caudalanteriorcingulate 2-lh</td><td align="char" char="." valign="bottom">0.332</td><td align="char" char="." valign="bottom">0.9375</td><td align="left" valign="bottom">1.458 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â2.985</td></tr><tr><td align="left" valign="bottom">Lateralorbitofrontal 1-rh</td><td align="char" char="." valign="bottom">0.328</td><td align="char" char="." valign="bottom">0.8659</td><td align="left" valign="bottom">1.733 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â2.869</td></tr><tr><td align="left" valign="bottom">Medialorbitofrontal 5-lh</td><td align="char" char="." valign="bottom">0.376</td><td align="char" char="." valign="bottom">0.9109</td><td align="left" valign="bottom">1.839 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.001</td></tr><tr><td align="left" valign="bottom">Middletemporal 1-lh</td><td align="char" char="." valign="bottom">0.324</td><td align="char" char="." valign="bottom">0.8642</td><td align="left" valign="bottom">2.245 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â2.944</td></tr><tr><td align="left" valign="bottom">Middletemporal 5-lh</td><td align="char" char="." valign="bottom">0.344</td><td align="char" char="." valign="bottom">0.9201</td><td align="left" valign="bottom">2.466 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.098</td></tr><tr><td align="left" valign="bottom">Parstriangularis 1-rh</td><td align="char" char="." valign="bottom">0.364</td><td align="char" char="." valign="bottom">0.9194</td><td align="left" valign="bottom">1.804 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.103</td></tr><tr><td align="left" valign="bottom">Parstriangularis 2-rh</td><td align="char" char="." valign="bottom">0.344</td><td align="char" char="." valign="bottom">0.8709</td><td align="left" valign="bottom">1.936 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.038</td></tr><tr><td align="left" valign="bottom">Parstriangularis 3-rh</td><td align="char" char="." valign="bottom">0.328</td><td align="char" char="." valign="bottom">0.9079</td><td align="left" valign="bottom">1.893 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.117</td></tr><tr><td align="left" valign="bottom">Parstriangularis 4-rh</td><td align="char" char="." valign="bottom">0.364</td><td align="char" char="." valign="bottom">0.9011</td><td align="left" valign="bottom">2.057 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.278</td></tr><tr><td align="left" valign="bottom">Rostralanteriorcingulate 2-lh</td><td align="char" char="." valign="bottom">0.368</td><td align="char" char="." valign="bottom">0.9348</td><td align="left" valign="bottom">1.553 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.095</td></tr><tr><td align="left" valign="bottom">Rostralanteriorcingulate 2-rh</td><td align="char" char="." valign="bottom">0.328</td><td align="char" char="." valign="bottom">0.9321</td><td align="left" valign="bottom">1.469 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â2.847</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 1-rh</td><td align="char" char="." valign="bottom">0.344</td><td align="char" char="." valign="bottom">0.9120</td><td align="left" valign="bottom">1.571 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.043</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 10-rh</td><td align="char" char="." valign="bottom">0.328</td><td align="char" char="." valign="bottom">0.9011</td><td align="left" valign="bottom">1.772 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.080</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 12-rh</td><td align="char" char="." valign="bottom">0.36</td><td align="char" char="." valign="bottom">0.9016</td><td align="left" valign="bottom">1.722 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.102</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 4-rh</td><td align="char" char="." valign="bottom">0.348</td><td align="char" char="." valign="bottom">0.8931</td><td align="left" valign="bottom">1.825 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â2.994</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 5-rh</td><td align="char" char="." valign="bottom">0.328</td><td align="char" char="." valign="bottom">0.9463</td><td align="left" valign="bottom">1.664 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.000</td></tr><tr><td align="left" valign="bottom">Rostralmiddlefrontal 8-rh</td><td align="char" char="." valign="bottom">0.336</td><td align="char" char="." valign="bottom">0.9365</td><td align="left" valign="bottom">1.730 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.070</td></tr><tr><td align="left" valign="bottom">Superiorfrontal 5-rh</td><td align="char" char="." valign="bottom">0.332</td><td align="char" char="." valign="bottom">0.9277</td><td align="left" valign="bottom">1.548 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â2.948</td></tr><tr><td align="left" valign="bottom">Superiorfrontal 6-rh</td><td align="char" char="." valign="bottom">0.328</td><td align="char" char="." valign="bottom">0.9268</td><td align="left" valign="bottom">1.585 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â2.966</td></tr><tr><td align="left" valign="bottom">Superiortemporal 7-lh</td><td align="char" char="." valign="bottom">0.34</td><td align="char" char="." valign="bottom">0.8882</td><td align="left" valign="bottom">2.055 Ã 10<sup>â11</sup></td><td align="char" char="." valign="bottom">â3.017</td></tr></tbody></table></table-wrap><table-wrap id="app1table5" position="float"><label>Appendix 1âtable 5.</label><caption><title>In the âSafe/Riskâ choice, we require that the activity of more than 90% of brain regions remains significantly correlated with the expected free energy for over 1.88 seconds, with a significance p&lt;0.001 (after false discovery rate correction).</title><p>The brain regions are delineated according to the âaparc subâ parcellation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Regressor</th><th align="left" valign="top">p-Value</th><th align="left" valign="top">Proportion</th><th align="left" valign="top">Duration</th><th align="left" valign="top"/></tr></thead><tbody><tr><td align="left" valign="top">Expected free energy</td><td align="char" char="." valign="top">0.001</td><td align="char" char="." valign="top">0.5</td><td align="char" char="." valign="top">1.88 seconds</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top"><bold>Brain region</bold></td><td align="left" valign="top"><bold>Duration</bold></td><td align="left" valign="top"><bold>Proportion</bold></td><td align="left" valign="top"><bold>Regression coefficient</bold></td><td align="left" valign="top"><bold><italic>t</italic>-Value</bold></td></tr><tr><td align="left" valign="top">Caudalmiddlefrontal 2-lh</td><td align="char" char="." valign="top">1.908</td><td align="char" char="." valign="top">0.9783</td><td align="left" valign="top">2.030 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.746</td></tr><tr><td align="left" valign="top">Insula 6-lh</td><td align="char" char="." valign="top">1.896</td><td align="char" char="." valign="top">0.9579</td><td align="left" valign="top">2.223 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.693</td></tr><tr><td align="left" valign="top">Middletemporal 5-lh</td><td align="char" char="." valign="top">1.904</td><td align="char" char="." valign="top">0.9850</td><td align="left" valign="top">3.350 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â5.115</td></tr><tr><td align="left" valign="top">Middletemporal 6-lh</td><td align="char" char="." valign="top">1.92</td><td align="char" char="." valign="top">0.9753</td><td align="left" valign="top">3.676 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.988</td></tr><tr><td align="left" valign="top">Parsorbitalis 2-lh</td><td align="char" char="." valign="top">1.912</td><td align="char" char="." valign="top">0.9187</td><td align="left" valign="top">2.694 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.803</td></tr><tr><td align="left" valign="top">Parstriangularis 1-lh</td><td align="char" char="." valign="top">1.884</td><td align="char" char="." valign="top">0.9581</td><td align="left" valign="top">2.580 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.717</td></tr><tr><td align="left" valign="top">Parstriangularis 2-lh</td><td align="char" char="." valign="top">1.896</td><td align="char" char="." valign="top">0.9768</td><td align="left" valign="top">2.619 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.814</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 1-lh</td><td align="char" char="." valign="top">1.908</td><td align="char" char="." valign="top">0.9830</td><td align="left" valign="top">2.243 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.819</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 2-lh</td><td align="char" char="." valign="top">1.896</td><td align="char" char="." valign="top">0.9783</td><td align="left" valign="top">2.216 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.789</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 4-lh</td><td align="char" char="." valign="top">1.884</td><td align="char" char="." valign="top">0.9662</td><td align="left" valign="top">2.082 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.716</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 6-lh</td><td align="char" char="." valign="top">1.904</td><td align="char" char="." valign="top">0.9898</td><td align="left" valign="top">2.470 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â4.901</td></tr></tbody></table></table-wrap><table-wrap id="app1table6" position="float"><label>Appendix 1âtable 6.</label><caption><title>In the âSafe/Riskâ choice, we require that the activity of more than 50% of brain regions remains significantly correlated with the value of reducing ambiguity for over 0.14 seconds, with a significance p&lt;0.05 (after false discovery rate correction).</title><p>The brain regions are delineated according to the âaparc subâ parcellation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Regressor</th><th align="left" valign="top">p-Value</th><th align="left" valign="top">Proportion</th><th align="left" valign="top">Duration</th><th align="left" valign="top"/></tr></thead><tbody><tr><td align="left" valign="top">Value of reducing ambiguity</td><td align="char" char="." valign="top">0.05</td><td align="char" char="." valign="top">0.5</td><td align="char" char="." valign="top">0.14 seconds</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top"><bold>Brain region</bold></td><td align="left" valign="top"><bold>Duration</bold></td><td align="left" valign="top"><bold>Proportion</bold></td><td align="left" valign="top"><bold>Regression coefficient</bold></td><td align="left" valign="top"><bold><italic>t</italic>-Value</bold></td></tr><tr><td align="left" valign="top">Insula 6-lh</td><td align="char" char="." valign="top">0.144</td><td align="char" char="." valign="top">0.9037</td><td align="left" valign="top">5.112 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.165</td></tr><tr><td align="left" valign="top">Lateralorbitofrontal 4-lh</td><td align="char" char="." valign="top">0.144</td><td align="char" char="." valign="top">0.8782</td><td align="left" valign="top">4.612 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â2.971</td></tr><tr><td align="left" valign="top">Parsorbitalis 2-lh</td><td align="char" char="." valign="top">0.144</td><td align="char" char="." valign="top">0.7381</td><td align="left" valign="top">5.591 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.059</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 1-lh</td><td align="char" char="." valign="top">0.148</td><td align="char" char="." valign="top">0.8730</td><td align="left" valign="top">4.907 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.107</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 6-lh</td><td align="char" char="." valign="top">0.16</td><td align="char" char="." valign="top">0.8679</td><td align="left" valign="top">5.244 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.067</td></tr><tr><td align="left" valign="top">Superiorfrontal 10-lh</td><td align="char" char="." valign="top">0.152</td><td align="char" char="." valign="top">0.9046</td><td align="left" valign="top">4.974 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.065</td></tr><tr><td align="left" valign="top">Superiorfrontal 6-lh</td><td align="char" char="." valign="top">0.148</td><td align="char" char="." valign="top">0.8845</td><td align="left" valign="top">4.779 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â2.946</td></tr></tbody></table></table-wrap><table-wrap id="app1table7" position="float"><label>Appendix 1âtable 7.</label><caption><title>In the âSafe/Riskâ choice, we require that the activity of more than 50% of brain regions remains significantly correlated with the extrinsic value for over 1.68 seconds, with a significance p&lt;0.001 (after false discovery rate correction).</title><p>The brain regions are delineated according to the âaparc subâ parcellation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Regressor</th><th align="left" valign="top">p-Value</th><th align="left" valign="top">Proportion</th><th align="left" valign="top">Duration</th><th align="left" valign="top"/></tr></thead><tbody><tr><td align="left" valign="top">Extrinsic value</td><td align="char" char="." valign="top">0.001</td><td align="char" char="." valign="top">0.5</td><td align="char" char="." valign="top">1.68 seconds</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top"><bold>Brain region</bold></td><td align="left" valign="top"><bold>Duration</bold></td><td align="left" valign="top"><bold>Proportion</bold></td><td align="left" valign="top"><bold>Regression coefficient</bold></td><td align="left" valign="top"><italic><bold>t</bold></italic><bold>-Value</bold></td></tr><tr><td align="left" valign="top">Caudalmiddlefrontal 2-lh</td><td align="char" char="." valign="top">1.712</td><td align="char" char="." valign="top">0.9626</td><td align="left" valign="top">2.131 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.607</td></tr><tr><td align="left" valign="top">Caudalmiddlefrontal 3-lh</td><td align="char" char="." valign="top">1.692</td><td align="char" char="." valign="top">0.9485</td><td align="left" valign="top">2.081 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.523</td></tr><tr><td align="left" valign="top">Lateralorbitofrontal 4-lh</td><td align="char" char="." valign="top">1.688</td><td align="char" char="." valign="top">0.9437</td><td align="left" valign="top">2.271 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.595</td></tr><tr><td align="left" valign="top">Middletemporal 4-lh</td><td align="char" char="." valign="top">1.692</td><td align="char" char="." valign="top">0.9063</td><td align="left" valign="top">3.402 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.727</td></tr><tr><td align="left" valign="top">Middletemporal 5-lh</td><td align="char" char="." valign="top">1.78</td><td align="char" char="." valign="top">0.9559</td><td align="left" valign="top">3.490 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.905</td></tr><tr><td align="left" valign="top">Middletemporal 6-lh</td><td align="char" char="." valign="top">1.764</td><td align="char" char="." valign="top">0.9558</td><td align="left" valign="top">3.889 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.847</td></tr><tr><td align="left" valign="top">Middletemporal 6-rh</td><td align="char" char="." valign="top">1.732</td><td align="char" char="." valign="top">0.9301</td><td align="left" valign="top">2.865 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.687</td></tr><tr><td align="left" valign="top">Parsopercularis 2-lh</td><td align="char" char="." valign="top">1.74</td><td align="char" char="." valign="top">0.9478</td><td align="left" valign="top">2.697 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.695</td></tr><tr><td align="left" valign="top">Parsopercularis 4-lh</td><td align="char" char="." valign="top">1.712</td><td align="char" char="." valign="top">0.9528</td><td align="left" valign="top">2.647 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.654</td></tr><tr><td align="left" valign="top">Parsorbitalis 2-lh</td><td align="char" char="." valign="top">1.748</td><td align="char" char="." valign="top">0.9059</td><td align="left" valign="top">2.849 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.675</td></tr><tr><td align="left" valign="top">Parstriangularis 1-lh</td><td align="char" char="." valign="top">1.732</td><td align="char" char="." valign="top">0.9310</td><td align="left" valign="top">2.759 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.655</td></tr><tr><td align="left" valign="top">Parstriangularis 2-lh</td><td align="char" char="." valign="top">1.76</td><td align="char" char="." valign="top">0.9591</td><td align="left" valign="top">2.798 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.739</td></tr><tr><td align="left" valign="top">Parstriangularis 3-lh</td><td align="char" char="." valign="top">1.704</td><td align="char" char="." valign="top">0.9577</td><td align="left" valign="top">2.565 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.673</td></tr><tr><td align="left" valign="top">Precentral 13-lh</td><td align="char" char="." valign="top">1.696</td><td align="char" char="." valign="top">0.9682</td><td align="left" valign="top">1.950 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.744</td></tr><tr><td align="left" valign="top">Precentral 14-lh</td><td align="char" char="." valign="top">1.74</td><td align="char" char="." valign="top">0.9688</td><td align="left" valign="top">2.066 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.799</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 1-lh</td><td align="char" char="." valign="top">1.74</td><td align="char" char="." valign="top">0.9664</td><td align="left" valign="top">2.351 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.653</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 2-lh</td><td align="char" char="." valign="top">1.716</td><td align="char" char="." valign="top">0.9693</td><td align="left" valign="top">2.357 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.692</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 4-lh</td><td align="char" char="." valign="top">1.712</td><td align="char" char="." valign="top">0.9541</td><td align="left" valign="top">2.217 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.634</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 5-lh</td><td align="char" char="." valign="top">1.708</td><td align="char" char="." valign="top">0.9653</td><td align="left" valign="top">2.333 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.663</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 6-lh</td><td align="char" char="." valign="top">1.72</td><td align="char" char="." valign="top">0.9867</td><td align="left" valign="top">2.598 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.74</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 7-lh</td><td align="char" char="." valign="top">1.7</td><td align="char" char="." valign="top">0.9277</td><td align="left" valign="top">2.221 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.602</td></tr><tr><td align="left" valign="top">Rostralmiddlefrontal 8-lh</td><td align="char" char="." valign="top">1.7</td><td align="char" char="." valign="top">0.9788</td><td align="left" valign="top">2.520 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">4.703</td></tr></tbody></table></table-wrap><table-wrap id="app1table8" position="float"><label>Appendix 1âtable 8.</label><caption><title>In the result stage of the âSafe/Riskâ choice, we require that the activity of more than 50% of brain regions remains significantly correlated with the extrinsic value for over 0.248 seconds, with a significance p&lt;0.05 (after false discovery rate correction).</title><p>The brain regions are delineated according to the &quot;aparc sub&quot; parcellation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Regressor</th><th align="left" valign="top">p-Value</th><th align="left" valign="top">Proportion</th><th align="left" valign="top">Duration</th><th align="left" valign="top"/></tr></thead><tbody><tr><td align="left" valign="top">Extrinsic value</td><td align="char" char="." valign="top">0.05</td><td align="char" char="." valign="top">0.5</td><td align="char" char="." valign="top">0.248 seconds</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top"><bold>Brain region</bold></td><td align="left" valign="top"><bold>Duration</bold></td><td align="left" valign="top"><bold>Proportion</bold></td><td align="left" valign="top"><bold>Regression coefficient</bold></td><td align="left" valign="top"><italic><bold>t</bold></italic><bold>-Value</bold></td></tr><tr><td align="left" valign="top">Fusiform 3-rh</td><td align="char" char="." valign="top">0.252</td><td align="char" char="." valign="top">0.9714</td><td align="left" valign="top">1.090 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.202</td></tr><tr><td align="left" valign="top">Fusiform 5-rh</td><td align="char" char="." valign="top">0.256</td><td align="char" char="." valign="top">0.9375</td><td align="left" valign="top">9.409 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â3.063</td></tr><tr><td align="left" valign="top">Inferiorparietal 11-rh</td><td align="char" char="." valign="top">0.252</td><td align="char" char="." valign="top">0.9538</td><td align="left" valign="top">8.761 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â3.029</td></tr><tr><td align="left" valign="top">Inferiorparietal 5-rh</td><td align="char" char="." valign="top">0.256</td><td align="char" char="." valign="top">0.9473</td><td align="left" valign="top">9.393 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â3.067</td></tr><tr><td align="left" valign="top">Inferiortemporal 7-rh</td><td align="char" char="." valign="top">0.256</td><td align="char" char="." valign="top">0.9703</td><td align="left" valign="top">1.157 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.357</td></tr><tr><td align="left" valign="top">Lateraloccipital 3-rh</td><td align="char" char="." valign="top">0.268</td><td align="char" char="." valign="top">0.8806</td><td align="left" valign="top">9.523 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â2.875</td></tr><tr><td align="left" valign="top">Lateraloccipital 4-rh</td><td align="char" char="." valign="top">0.26</td><td align="char" char="." valign="top">0.9538</td><td align="left" valign="top">8.228 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â2.902</td></tr><tr><td align="left" valign="top">Lateraloccipital 5-rh</td><td align="char" char="." valign="top">0.252</td><td align="char" char="." valign="top">0.9224</td><td align="left" valign="top">9.960 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â2.908</td></tr><tr><td align="left" valign="top">Lateraloccipital 8-rh</td><td align="char" char="." valign="top">0.26</td><td align="char" char="." valign="top">0.9790</td><td align="left" valign="top">9.966 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â3.092</td></tr><tr><td align="left" valign="top">Lateraloccipital 9-rh</td><td align="char" char="." valign="top">0.252</td><td align="char" char="." valign="top">0.9302</td><td align="left" valign="top">9.413 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â3.026</td></tr><tr><td align="left" valign="top">Lingual 5-rh</td><td align="char" char="." valign="top">0.256</td><td align="char" char="." valign="top">0.9792</td><td align="left" valign="top">1.097 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.155</td></tr><tr><td align="left" valign="top">Paracentral 2-rh</td><td align="char" char="." valign="top">0.252</td><td align="char" char="." valign="top">0.9762</td><td align="left" valign="top">5.791 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â3.065</td></tr><tr><td align="left" valign="top">Parahippocampal 2-rh</td><td align="char" char="." valign="top">0.256</td><td align="char" char="." valign="top">0.9896</td><td align="left" valign="top">1.031 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.225</td></tr><tr><td align="left" valign="top">Postcentral 10-rh</td><td align="char" char="." valign="top">0.252</td><td align="char" char="." valign="top">0.9707</td><td align="left" valign="top">7.185 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â2.997</td></tr><tr><td align="left" valign="top">Precuneus 8-rh</td><td align="char" char="." valign="top">0.256</td><td align="char" char="." valign="top">0.9398</td><td align="left" valign="top">6.078 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â2.904</td></tr><tr><td align="left" valign="top">Superiorparietal 11-rh</td><td align="char" char="." valign="top">0.26</td><td align="char" char="." valign="top">0.9303</td><td align="left" valign="top">8.428 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â2.895</td></tr><tr><td align="left" valign="top">Superiorparietal 3-rh</td><td align="char" char="." valign="top">0.268</td><td align="char" char="." valign="top">0.9091</td><td align="left" valign="top">7.188 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â3.0</td></tr><tr><td align="left" valign="top">Superiorparietal 6-rh</td><td align="char" char="." valign="top">0.264</td><td align="char" char="." valign="top">0.9318</td><td align="left" valign="top">8.186 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â3.01</td></tr><tr><td align="left" valign="top">Superiorparietal 7-rh</td><td align="char" char="." valign="top">0.256</td><td align="char" char="." valign="top">0.9420</td><td align="left" valign="top">7.757 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â2.958</td></tr><tr><td align="left" valign="top">Superiorparietal 8-rh</td><td align="char" char="." valign="top">0.264</td><td align="char" char="." valign="top">0.9674</td><td align="left" valign="top">7.950 Ã 10<sup>â12</sup></td><td align="char" char="." valign="top">â2.989</td></tr></tbody></table></table-wrap><table-wrap id="app1table9" position="float"><label>Appendix 1âtable 9.</label><caption><title>In the result stage of the âSafe/Riskâ choice, we require that the activity of more than 50% of brain regions remains significantly correlated with (the value of) reducing ambiguity for over 0.072 seconds, with a significance p&lt;0.05.</title><p>The brain regions are delineated according to the âaparc subâ parcellation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Regressor</th><th align="left" valign="top">p-Value</th><th align="left" valign="top">Proportion</th><th align="left" valign="top">Duration</th><th align="left" valign="top"/></tr></thead><tbody><tr><td align="left" valign="top">(The value of) reducing ambiguity</td><td align="char" char="." valign="top">0.05</td><td align="char" char="." valign="top">0.5</td><td align="char" char="." valign="top">0.072 seconds</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top"><bold>Brain region</bold></td><td align="left" valign="top"><bold>Duration</bold></td><td align="left" valign="top"><bold>Proportion</bold></td><td align="left" valign="top"><bold>Regression coefficient</bold></td><td align="left" valign="top"><italic><bold>t</bold></italic><bold>-Value</bold></td></tr><tr><td align="left" valign="top">Paracentral 4-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9697</td><td align="left" valign="top">2.809 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.321</td></tr><tr><td align="left" valign="top">Paracentral 5-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9506</td><td align="left" valign="top">2.803 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.156</td></tr><tr><td align="left" valign="top">Paracentral 6-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9778</td><td align="left" valign="top">2.420 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.145</td></tr><tr><td align="left" valign="top">Precentral 11-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9861</td><td align="left" valign="top">3.115 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.316</td></tr><tr><td align="left" valign="top">Precentral 15-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9596</td><td align="left" valign="top">2.952 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.278</td></tr><tr><td align="left" valign="top">Precentral 16-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9293</td><td align="left" valign="top">3.079 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.23</td></tr><tr><td align="left" valign="top">Precentral 7-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9242</td><td align="left" valign="top">2.994 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.27</td></tr><tr><td align="left" valign="top">Superiorparietal 3-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9343</td><td align="left" valign="top">2.940 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.132</td></tr><tr><td align="left" valign="top">Superiorparietal 6-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9611</td><td align="left" valign="top">3.410 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.19</td></tr><tr><td align="left" valign="top">Supramarginal 1-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9615</td><td align="left" valign="top">3.942 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.41</td></tr><tr><td align="left" valign="top">Supramarginal 9-rh</td><td align="char" char="." valign="top">0.072</td><td align="char" char="." valign="top">0.9213</td><td align="left" valign="top">3.731 Ã 10<sup>â11</sup></td><td align="char" char="." valign="top">â3.452</td></tr></tbody></table></table-wrap></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92892.4.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chait</surname><given-names>Maria</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study addresses a central question in systems neuroscience (validation of active inference models of exploration) using a combination of behavior, neuroimaging, and modeling. The data provided offers <bold>solid</bold> evidence that humans do perceive, choose, and learn in a manner consistent with the essential ingredients of active inference, and that quantities that correlate with relevant parameters of this active inference scheme are encoded in different regions of the brain.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92892.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper presents a compelling and comprehensive study of decision-making under uncertainty. It addresses a fundamental distinction between belief-based (cognitive neuroscience) formulations of choice behavior with reward-based (behavioral psychology) accounts. Specifically, it asks whether active inference provides a better account of planning and decision making, relative to reinforcement learning. To do this, the authors use a simple but elegant paradigm that includes choices about whether to seek both information and rewards. They then assess the evidence for active inference and reinforcement learning models of choice behavior, respectively. After demonstrating that active inference provides a better explanation of behavioral responses, the neuronal correlates of epistemic and instrumental value (under an optimized active inference model) are characterized using EEG. Significant neuronal correlates of both kinds of value were found in sensor and source space. The source space correlates are then discussed sensibly, in relation to the existing literature on the functional anatomy of perceptual and instrumental decision-making under uncertainty.</p><p>Comments on revisions:</p><p>Many thanks for attending to my previous comments. I think your manuscript is now easier to read - and your new (Bayesian) analyses are described clearly.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92892.4.sa2</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper aims to investigate how the human brain represents different forms of value and uncertainty that participate in active inference within a free-energy framework, in a two-stage decision task involving contextual information sampling, and choices between safe and risky rewards, which promotes shifting between exploration and exploitation. They examine neural correlates by recording EEG and comparing activity in the first vs second half of trials and between trials in which subjects did and did not sample contextual information, and perform a regression with free-energy-related regressors against data &quot;mapped to source space.&quot;</p><p>Strengths:</p><p>This two-stage paradigm is cleverly designed to incorporate several important processes of learning, exploration/exploitation and information sampling that pertain to active inference. Although scalp/brain regions showing sensitivity to the active-inference related quantities do not necessarily suggest what role they play, they are illuminating and useful as candidate regions for further investigation. The aims are ambitious, and the methodologies are impressive. The paper lays out an extensive introduction to the free energy principle and active inference to make the findings accessible to a broad readership.</p><p>Weaknesses:</p><p>It is worth noting that the high lower-cutoff of 1 Hz in the bandpass filter, included to reduce the impact of EEG noise, would remove from the EEG any sustained, iteratively updated representation that evolves with learning across trials, or choice-related processes that unfold slowly over the course of the 2-second task windows. It is thus possible there are additional processes related to the active inference quantities that are missed here. This is not a flaw as one must always try to balance noise removal against signal removal in filter settings - it is just a caveat. As the authors also note, the regions showing up as correlated with model parameters change depending on source modelling method and correction for multiple comparisons, warranting some caution around the localisation aspect.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.92892.4.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Shuo</given-names></name><role specific-use="author">Author</role><aff><institution>University of Macau</institution><addr-line><named-content content-type="city">macau</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Tian</surname><given-names>Yan</given-names></name><role specific-use="author">Author</role><aff><institution>University of Macau</institution><addr-line><named-content content-type="city">macau</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Quanying</given-names></name><role specific-use="author">Author</role><aff><institution>Southern University of Science and Technology</institution><addr-line><named-content content-type="city">Shenzhen</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Haiyan</given-names></name><role specific-use="author">Author</role><aff><institution>University of Macau</institution><addr-line><named-content content-type="city">macau</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>This paper presents a compelling and comprehensive study of decision-making under uncertainty. It addresses a fundamental distinction between belief-based (cognitive neuroscience) formulations of choice behavior with reward-based (behavioral psychology) accounts. Specifically, it asks whether active inference provides a better account of planning and decision making, relative to reinforcement learning. To do this, the authors use a simple but elegant paradigm that includes choices about whether to seek both information and rewards. They then assess the evidence for active inference and reinforcement learning models of choice behavior, respectively. After demonstrating that active inference provides a better explanation of behavioral responses, the neuronal correlates of epistemic and instrumental value (under an optimized active inference model) are characterized using EEG. Significant neuronal correlates of both kinds of value were found in sensor and source space. The source space correlates are then discussed sensibly, in relation to the existing literature on the functional anatomy of perceptual and instrumental decision-making under uncertainty.</p></disp-quote><p>We are deeply grateful for your careful review of our work and your suggestions. Your insights have helped us identify areas where we can strengthen the arguments and clarify the methodology. We hope to apply the idea of active inference to our future work, emphasizing the integrity of perception and action.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Many thanks for attending to my previous suggestions. I think your presentation is now much clearer and nicely aligned with the active inference literature.</p><p>There is one outstanding issue. I think you have overinterpreted the two components of epistemic value in Equation 8. The two components that you have called the value of reducing risk and the value of reducing ambiguity are not consistent with the normal interpretation. These two components are KL divergences that measure the expected information gain about parameters and states respectively.</p><p>If you read the Schwartenbeck et al paper carefully, you will see that the first (expected information gain about parameters) is usually called novelty, while the second (expected information gain about states) is usually called salience.</p><p>This means you can replace &quot;the value of reducing ambiguity&quot; with &quot;novelty&quot; and &quot;the value of reducing risk&quot; with &quot;salience&quot;.</p><p>For your interest, &quot;risk&quot; and &quot;ambiguity&quot; are alternative ways of decomposing expected free energy. In other words, you can decompose expected free energy into (negative) expected information gain and expected value (as you have done). Alternatively, you can rearrange the terms and express expected free energy as risk and ambiguity. Look at the top panel of Figure 4 in:</p><p><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0022249620300857">https://www.sciencedirect.com/science/article/pii/S0022249620300857</ext-link></p><p>I hope that this helps.</p></disp-quote><p>We deeply thank you for your recommendations about the interpretation of the epistemic value in Equation 8. We have now corrected them to Novelty and Salience:</p><p><inline-formula><mml:math id="sa3m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Ï</mml:mi><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>â£</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>Â noveltyÂ </mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:msub><mml:mi>ln</mml:mi><mml:mrow><mml:mstyle scriptlevel="1"><mml:mtable rowspacing="0.1em" columnspacing="0em" displaystyle="false"><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>â£</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>â£</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>Â salienceÂ </mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>â</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>Epistemic valueÂ </mml:mtext></mml:mrow></mml:munder><mml:mo>â</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mo>â¡</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>â</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>Extrinsic valueÂ </mml:mtext></mml:mrow></mml:munder></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>In addition, in order to avoid terminology conflicts with active inference and to describe these two different uncertainties, we replaced Ambiguity in the article with Novelty, referring to the uncertainty that can be reduced by sampling, and replaced Risk with Variability, referring to the uncertainty inherent in the environment (variance).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer # 2 (Public Review):</bold></p><p>Summary<bold>:</bold></p><p>Zhang and colleagues use a combination of behavioral, neural, and computational analyses to test an active inference model of exploration in a novel reinforcement learning task..</p><p>Strengths:</p><p>The paper addresses an important question (validation of active inference models of exploration). The combination of behavior, neuroimaging, and modeling is potentially powerful for answering this question.</p><p>I appreciate the addition of details about model fitting, comparison, and recovery, as well as the change in some of the methods.</p></disp-quote><p>We are deeply grateful for your careful review of our work and your suggestions. And we are also very sorry that in our last responses, there were a few suggestions from you that we did not respond them appropriately in our manuscript. We hope to be able to respond to these suggestions well in this revision. Thank you for your contribution to ensuring the scientificity and reproducibility of the work.</p><disp-quote content-type="editor-comment"><p>The authors do not cite what is probably the most relevant contextual bandit study, by Collins &amp; Frank (2018, PNAS), which uses EEG.</p><p>The authors cite Collins &amp; Molinaro as a form of contextual bandit, but that's not the case (what they call &quot;context&quot; is just the choice set). They should look at the earlier work from Collins, starting with Collins &amp; Frank (2012, EJN).</p></disp-quote><p>We deeply thank you for your comments. Now we add the relevant citations in the manuscript (line 46):</p><p>âThese studies utilized different forms of multi-armed bandit tasks, e.g the restless multi-armed bandit tasks (Daw et al., 2006; Guha et al., 2010), risky/safe bandit tasks (Tomov et al., 2020; Fan et al., 2022; Payzan et al., 2013), contextual multi-armed bandit tasks (Collins &amp; Frank, 2018; Schulz et al., 2015; Collins &amp; Frank, 2012)â</p><p>Daw, N. D., O'doherty, J. P., Dayan, P., Seymour, B., &amp; Dolan, R. J. (2006). Cortical substrates for exploratory decisions in humans. <italic>Nature</italic>, <italic>441</italic>(7095), 876-879.</p><p>Guha, S., Munagala, K., &amp; Shi, P. (2010). Approximation algorithms for restless bandit problems. <italic>Journal of the ACM (JACM)</italic>, <italic>58</italic>(1), 1-50.</p><p>Tomov, M. S., Truong, V. Q., Hundia, R. A., &amp; Gershman, S. J. (2020). Dissociable neural correlates of uncertainty underlie different exploration strategies. <italic>Nature communications</italic>, <italic>11</italic>(1), 2371.</p><p>Fan, H., Gershman, S. J., &amp; Phelps, E. A. (2023). Trait somatic anxiety is associated with reduced directed exploration and underestimation of uncertainty. <italic>Nature Human Behaviour</italic>, <italic>7</italic>(1), 102-113.</p><p>Payzan-LeNestour, E., Dunne, S., Bossaerts, P., &amp; OâDoherty, J. P. (2013). The neural representation of unexpected uncertainty during value-based decision making. <italic>Neuron</italic>, <italic>79</italic>(1), 191-201.</p><p>Collins, A. G., &amp; Frank, M. J. (2018). Within-and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory. <italic>Proceedings of the National Academy of Sciences</italic>, <italic>115</italic>(10), 2502-2507.</p><p>Schulz, E., Konstantinidis, E., &amp; Speekenbrink, M. (2015, April). Exploration-exploitation in a contextual multi-armed bandit task. In <italic>International conference on cognitive modeling</italic> (pp. 118-123).</p><p>Collins, A. G., &amp; Frank, M. J. (2012). How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis. <italic>European Journal of Neuroscience</italic>, <italic>35</italic>(7), 1024-1035.</p><disp-quote content-type="editor-comment"><p>Placing statistical information in a GitHub repository is not appropriate. This needs to be in the main text of the paper. I don't understand why the authors refer to space limitations; there are none for eLife, as far as I'm aware.</p></disp-quote><p>We deeply thank you for your comments. We calculated the average t-value of the brain regions with significant results over the significant time, and added the t-value results to the main text and supplementary materials.</p><disp-quote content-type="editor-comment"><p>In answer to my question about multiple comparisons, the authors have added the following: &quot;Note that we did not attempt to correct for multiple comparisons; largely, because the correlations observed were sustained over considerable time periods, which would be almost impossible under the null hypothesis of no correlations.&quot; I'm sorry, but this does not make sense. Either the authors are doing multiple comparisons, in which case multiple comparison correction is relevant, or they are doing a single test on the extended timeseries, in which case they need to report that. There exist tools for this kind of analysis (e.g., Gershman et al., 2014, NeuroImage). I'm not suggesting that the authors should necessarily do this, only that their statistical approach should be coherent. As a reference point, the authors might look at the aforementioned Collins &amp; Frank (2018) study.</p></disp-quote><p>We deeply thank you for your comments. We have now replaced all our results with the results after false discovery rate correction and added relevant descriptions (line 357,358):</p><p>âThe significant results after false discovery rate (FDR) (Benjamini et al., 1995, Gershman et al., 2014) correction were shown in shaded regions. Additional regression results can be found in Supplementary Materials.â</p><p>Benjamini, Y., &amp; Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. <italic>Journal of the Royal statistical society: series B (Methodological)</italic>, <italic>57</italic>(1), 289-300.</p><p>Gershman, S. J., Blei, D. M., Norman, K. A., &amp; Sederberg, P. B. (2014). Decomposing spatiotemporal brain patterns into topographic latent sources. <italic>NeuroImage</italic>, <italic>98</italic>, 91-102.</p><p>After FDR correction, our results have changed slightly. We have updated our Results and Discussion section.</p><p>It should be acknowledged that the changes in these results may represent a certain degree of error in our data (perhaps because the EEG data is too noisy or because of the average template we used, âfsaverageâ). Therefore, we added relevant discussion in the Discussion section (line527-529):</p><p>âIt should be acknowledged that our EEG-based regression results are somewhat unstable, and the brain regions with significant regression are inconsistent before and after FDR correction. In future work, we should collect more precise neural data to reduce this instability.â</p><disp-quote content-type="editor-comment"><p>I asked the authors to show more descriptive comparison between the model and the data. Their response was that this is not possible, which I find odd given that they are able to use the model to define a probability distribution on choices. All I'm asking about here is to show predictive checks which build confidence in the model fit. The additional simulations do not address this. The authors refer to figures 3 and 4, but these do not show any direct comparison between human data and the model beyond model comparison metrics.</p></disp-quote><p>We deeply thank you for your comments. We now compare the participantsâ behavioral data and the modelâs predictions trial by trial (Figure 5). We can clearly see the participantsâ behavioral strategies in different states and trials and the modelâs prediction accuracy. We have added the discussion related to Figure 5 (line 309-318):</p><p>âFigure 5 shows the comparison between the active inference model and the behavioral data, where we can see that the model can fit the participants behavioral strategies well. In the âStay-Cue&quot; choice, participants always tend to choose to ask the ranger and rarely choose not to ask. When the context was unknown, participants chose the âSafe&quot; option or the âRisky&quot; option very randomly, and they did not show any aversion to variability. When given âContext 1&quot;, where the âRisky&quot; option gave participants a high average reward, participants almost exclusively chose the âRisky&quot; option, which provided more information in the early trials and was found to provide more rewards in the later rounds. When given âContext 2&quot;, where the âRisky&quot; option gave participants a low average reward, participants initially chose the âRisky&quot; option and then tended to choose the âSafe&quot; option. We can see that participants still occasionally chose the âRisky&quot; option in the later trials of the experiment, which the model does not capture. This may be due to the influence of forgetting. Participants chose the âRisky&quot; option again to establish an estimate of the reward distribution.â</p><disp-quote content-type="editor-comment"><p><bold>Reviewer # 2 (Recommendations For The Authors):</bold></p><p>In the supplement, there are missing references (&quot;[?]&quot;).</p></disp-quote><p>Thank you very much for pointing out this. We have now fixed this error.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer # 3 (Public review):</bold></p><p>Summary:</p><p>This paper aims to investigate how the human brain represents different forms of value and uncertainty that participate in active inference within a free-energy framework, in a two-stage decision task involving contextual information sampling, and choices between safe and risky rewards, which promotes shifting between exploration and exploitation. They examine neural correlates by recording EEG and comparing activity in the first vs second half of trials and between trials in which subjects did and did not sample contextual information, and perform a regression with free-energy-related regressors against data &quot;mapped to source space.&quot;</p><p>Strengths:</p><p>This two-stage paradigm is cleverly designed to incorporate several important processes of learning, exploration/exploitation and information sampling that pertain to active inference. Although scalp/brain regions showing sensitivity to the active-inference related quantities do not necessary suggest what role they play, they are illuminating and useful as candidate regions for further investigation. The aims are ambitious, and the methodologies impressive. The paper lays out an extensive introduction to the free energy principle and active inference to make the findings accessible to a broad readership.</p><p>Weaknesses:</p><p>In its revised form the paper is complete in providing the important details. Though not a serious weakness, it is important to note that the high lower-cutoff of 1 Hz in the bandpass filter, included to reduce the impact of EEG noise, would remove from the EEG any sustained, iteratively updated representation that evolves with learning across trials, or choice-related processes that unfold slowly over the course of the 2-second task windows.</p></disp-quote><p>We are deeply grateful for your careful review of our work and your suggestions. We are very sorry that we did not modify our filter frequency (it would be a lot of work to modify it). Thank you very much for pointing this out. We noticed the shortcoming of the high lower-cutoff of 1 Hz in the bandpass filter. We will carefully consider the filter frequency when preprocessing data in future work. Thank you very much!</p></body></sub-article></article>