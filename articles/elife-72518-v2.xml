<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">72518</article-id><article-id pub-id-type="doi">10.7554/eLife.72518</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Review Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Ecology</subject></subj-group></article-categories><title-group><article-title>Data-driven causal analysis of observational biological time series</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-247374"><name><surname>Yuan</surname><given-names>Alex Eric</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8972-7497</contrib-id><email>alexericyuan@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-2203"><name><surname>Shou</surname><given-names>Wenying</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5693-381X</contrib-id><email>wenying.shou@gmail.com</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>Molecular and Cellular Biology PhD program, University of Washington</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/007ps6h72</institution-id><institution>Basic Sciences Division, Fred Hutchinson Cancer Research Center</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Centre for Life’s Origins and Evolution, Department of Genetics, Evolution and Environment, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schuman</surname><given-names>Meredith C</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>University of Zurich</institution></institution-wrap><country>Switzerland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Schuman</surname><given-names>Meredith C</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>University of Zurich</institution></institution-wrap><country>Switzerland</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>19</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e72518</elocation-id><history><date date-type="received" iso-8601-date="2021-08-09"><day>09</day><month>08</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-01-23"><day>23</day><month>01</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-08-04"><day>04</day><month>08</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.08.03.233692"/></event></pub-history><permissions><copyright-statement>© 2022, Yuan and Shou</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Yuan and Shou</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-72518-v2.pdf"/><abstract><p>Complex systems are challenging to understand, especially when they defy manipulative experiments for practical or ethical reasons. Several fields have developed parallel approaches to infer causal relations from observational time series. Yet, these methods are easy to misunderstand and often controversial. Here, we provide an accessible and critical review of three statistical causal discovery approaches (pairwise correlation, Granger causality, and state space reconstruction), using examples inspired by ecological processes. For each approach, we ask what it tests for, what causal statement it might imply, and when it could lead us astray. We devise new ways of visualizing key concepts, describe some novel pathologies of existing methods, and point out how so-called ‘model-free’ causality tests are not assumption-free. We hope that our synthesis will facilitate thoughtful application of methods, promote communication across different fields, and encourage explicit statements of assumptions. A video walkthrough is available (Video 1 or <ext-link ext-link-type="uri" xlink:href="https://youtu.be/AlV0ttQrjK8">https://youtu.be/AlV0ttQrjK8</ext-link>).</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>time series</kwd><kwd>causality</kwd><kwd>model-free</kwd><kwd>surrogate data</kwd><kwd>convergent cross-mapping</kwd><kwd>Granger causality</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01GM124128</award-id><principal-award-recipient><name><surname>Shou</surname><given-names>Wenying</given-names></name><name><surname>Yuan</surname><given-names>Alex Eric</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000691</institution-id><institution>Academy of Medical Sciences</institution></institution-wrap></funding-source><award-id>AMS Professorship</award-id><principal-award-recipient><name><surname>Shou</surname><given-names>Wenying</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Wolfson Foundation and Royal Society</institution></institution-wrap></funding-source><award-id>Wolfson Fellowship</award-id><principal-award-recipient><name><surname>Shou</surname><given-names>Wenying</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>1917258</award-id><principal-award-recipient><name><surname>Shou</surname><given-names>Wenying</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Visualizations, simulations, and examples are used to provide an accessible synthesis of the reasoning and assumptions behind commonly used causal discovery approaches.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Ecological communities perform important activities, from facilitating digestion in the human gut to driving biogeochemical cycles. Communities are often highly complex, with many species engaging in diverse interactions. To control communities, it helps to know causal relationships between variables (e.g. whether perturbing the abundance of one species might alter the abundance of another species). We can express these relationships either explicitly by proposing causal networks (<xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref>; <xref ref-type="bibr" rid="bib20">Chattopadhyay et al., 2019</xref>; <xref ref-type="bibr" rid="bib38">Glymour et al., 2019</xref>; <xref ref-type="bibr" rid="bib94">Runge et al., 2019b</xref>; <xref ref-type="bibr" rid="bib93">Runge et al., 2019a</xref>; <xref ref-type="bibr" rid="bib95">Sanchez-Romero et al., 2019</xref>; <xref ref-type="bibr" rid="bib63">Leng et al., 2020</xref>), or implicitly by simply predicting the effects of new perturbations (<xref ref-type="bibr" rid="bib29">Daniels and Nemenman, 2015</xref>; <xref ref-type="bibr" rid="bib70">Mangan et al., 2016</xref>).</p><p>Ideally, biologists discover such causal relations from manipulative experiments. However, manipulative experiments can be infeasible or inappropriate: Natural ecosystems may not offer enough replicates for comprehensive manipulative experiments, and perturbations can be impractical at large scales and may have unanticipated negative consequences. On the other hand, there exists an ever-growing abundance of observational time series (i.e. without intentional perturbations). The goal of obtaining accurate causal predictions from these or similar data sets has motivated several complementary lines of investigation.</p><p>Determining causal relationships can become more straightforward if one already knows, or is willing to assume, a model that captures key aspects of the underlying process. For example, the Lotka-Volterra model popular in mathematical ecology assumes that species interact in a pairwise fashion, that the fitness effects from different interactions are additive, and that all pairwise interactions can be represented by a single equation form where parameters can vary to reflect signs and strengths of fitness effects. By fitting such a model to time series of species abundances and environmental factors, one can predict, for instance, which species interact or how a community might respond to certain perturbations (<xref ref-type="bibr" rid="bib104">Stein et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Fisher and Mehta, 2014</xref>; <xref ref-type="bibr" rid="bib15">Bucci et al., 2016</xref>). However, the Lotka-Volterra equations often fail to describe complex ecosystems and chemically mediated interactions (<xref ref-type="bibr" rid="bib64">Levine, 1976</xref>; <xref ref-type="bibr" rid="bib118">Wootton, 2002</xref>; <xref ref-type="bibr" rid="bib72">Momeni et al., 2017</xref>).</p><p>When our understanding is insufficient to support knowledge-based modeling, how might we formulate causal hypotheses? A large and rapidly growing literature attempts to infer causal relations from time series data without using a mechanistic model. Such methods are sometimes called ‘model-free’ (<xref ref-type="bibr" rid="bib24">Coenen et al., 2020</xref>), although they typically rely on <italic>statistical</italic> models. Some of these methods avoid any equation-based description of the dynamics and instead examine some notion of ‘information flow’ between time series (<xref ref-type="bibr" rid="bib42">Granger, 1980</xref>; <xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>). Others deploy highly flexible equations that are not necessarily mechanistic (<xref ref-type="bibr" rid="bib41">Granger, 1969</xref>; <xref ref-type="bibr" rid="bib6">Barnett and Seth, 2014</xref>).</p><p>Here, we focus on three model-free approaches that have been commonly used to make causal claims in ecology research: pairwise correlation, Granger causality, and state space reconstruction. For each, we ask (1) what information does the method give us, (2) what causal statement might that information imply, and (3) when might the method lead us astray?</p><p>We found that answering these seemingly basic questions was at first surprisingly challenging for several reasons. First, modern causal discovery approaches have intellectual roots in several communities including philosophy, statistics, econometrics, and chaos theory, which sometimes use different words for the same idea, and the same word for different ideas. The word causality itself is a prime example: Many philosophers (and scientists) would say that <inline-formula><mml:math id="inf1"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf2"><mml:mi>Y</mml:mi></mml:math></inline-formula> if an intervention upon <inline-formula><mml:math id="inf3"><mml:mi>X</mml:mi></mml:math></inline-formula> would result in a change in <inline-formula><mml:math id="inf4"><mml:mi>Y</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib117">Woodward, 2016</xref>; <xref ref-type="bibr" rid="bib82">Pearl, 2000</xref>). Granger’s original works instead defined causality to be about how important the history of <inline-formula><mml:math id="inf5"><mml:mi>X</mml:mi></mml:math></inline-formula> is in predicting <inline-formula><mml:math id="inf6"><mml:mi>Y</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib41">Granger, 1969</xref>; <xref ref-type="bibr" rid="bib42">Granger, 1980</xref>), and in the nonlinear dynamics field, causality is sometimes used to mean that the trajectories of <inline-formula><mml:math id="inf7"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf8"><mml:mi>Y</mml:mi></mml:math></inline-formula> have certain shared geometric or topological properties (<xref ref-type="bibr" rid="bib45">Harnack et al., 2017</xref>). Such language, while unproblematic when confined to a single community, can nevertheless obscure important differences between methods from different communities. A second challenge is that in methodological articles, key assumptions are sometimes hidden in algorithmic details, or simply not mentioned. Finally, some methods deal with nuanced or advanced mathematical ideas that can be difficult even for those with quantitative training. Given these challenges, it is no surprise that efforts to infer causal relationships from observational time series have sometimes been highly controversial, with an abundance of ‘letters to the editor’, sometimes followed by impassioned dialogue (<xref ref-type="bibr" rid="bib66">Luo et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Baskerville and Cobey, 2017</xref>; <xref ref-type="bibr" rid="bib110">Tiokhin and Hruschka, 2017</xref>; <xref ref-type="bibr" rid="bib97">Schaller et al., 2017</xref>; <xref ref-type="bibr" rid="bib7">Barnett et al., 2018</xref>).</p><p>We have tried to balance precision and readability in this review. To accomplish this, we devised new ways to visualize key concepts. We also compare all methods to a common definition of causality that is useful to experimental scientists. We provide refreshers and discussions of mathematical notions in the Appendices. Lastly, a video walkthrough covering many of the key concepts and takeaway messages is available at <ext-link ext-link-type="uri" xlink:href="https://youtu.be/AlV0ttQrjK8">https://youtu.be/AlV0ttQrjK8</ext-link>; and as <xref ref-type="video" rid="video1">Video 1</xref>. Our goals are to inform, to facilitate communication across different fields, and to encourage explicit statements of methodological assumptions and caveats. For a broad overview of time series causal methods in Earth sciences or more technical reviews, see <xref ref-type="bibr" rid="bib94">Runge et al., 2019b</xref> and <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>; <xref ref-type="bibr" rid="bib92">Runge, 2018b</xref> respectively.</p><media mimetype="video" mime-subtype="mp4" id="video1" xlink:href="elife-72518-video1.mp4"><label>Video 1.</label><caption><title>Video walkthrough.</title></caption></media></sec><sec id="s2"><title>Dependence, correlation, and causality</title><sec id="s2-1"><title>Causality</title><p>In this article, we use the definition of ‘causality’ that is common in statistics and intuitive to scientists: <inline-formula><mml:math id="inf9"><mml:mi>X</mml:mi></mml:math></inline-formula> has a causal effect on <inline-formula><mml:math id="inf10"><mml:mi>Y</mml:mi></mml:math></inline-formula> (‘<inline-formula><mml:math id="inf11"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf12"><mml:mi>Y</mml:mi></mml:math></inline-formula>’ or ‘<inline-formula><mml:math id="inf13"><mml:mi>X</mml:mi></mml:math></inline-formula> is a causer; <inline-formula><mml:math id="inf14"><mml:mi>Y</mml:mi></mml:math></inline-formula> is a causee’ or ‘<inline-formula><mml:math id="inf15"><mml:mi>X</mml:mi></mml:math></inline-formula> is a cause; <inline-formula><mml:math id="inf16"><mml:mi>Y</mml:mi></mml:math></inline-formula> is an effect’) if some externally applied perturbation of <inline-formula><mml:math id="inf17"><mml:mi>X</mml:mi></mml:math></inline-formula> can result in a perturbation in <inline-formula><mml:math id="inf18"><mml:mi>Y</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We say that <inline-formula><mml:math id="inf19"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:mi>Y</mml:mi></mml:math></inline-formula> are <italic>causally related</italic> if <inline-formula><mml:math id="inf21"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf22"><mml:mi>Y</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf23"><mml:mi>Y</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf24"><mml:mi>X</mml:mi></mml:math></inline-formula>, or some other variable causes both. Otherwise, <inline-formula><mml:math id="inf25"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mi>Y</mml:mi></mml:math></inline-formula> are <italic>causally unrelated</italic>. Additionally, one can talk about direct versus indirect causality (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; see legend for definitions). A surprising result from past several decades of causality research is that there are in fact some conditions under which directional causal structures can be correctly inferred (‘identified’) from purely observational data (<xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref>; <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>; <xref ref-type="bibr" rid="bib53">Hitchcock, 2020b</xref>) (e.g. <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2</xref>, last row). However, empirical time series often do not contain enough information for easy causal identifiability (<xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref>; <xref ref-type="bibr" rid="bib38">Glymour et al., 2019</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Causality.</title><p>(<bold>A</bold>) Definition. If a perturbation in <inline-formula><mml:math id="inf27"><mml:mi>X</mml:mi></mml:math></inline-formula> can result in a change in future values of <inline-formula><mml:math id="inf28"><mml:mi>Y</mml:mi></mml:math></inline-formula>, then <inline-formula><mml:math id="inf29"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf30"><mml:mi>Y</mml:mi></mml:math></inline-formula>. This definition does not require that <italic>any</italic> perturbation in <inline-formula><mml:math id="inf31"><mml:mi>X</mml:mi></mml:math></inline-formula> will perturb <inline-formula><mml:math id="inf32"><mml:mi>Y</mml:mi></mml:math></inline-formula>. For example, if the effect of <inline-formula><mml:math id="inf33"><mml:mi>X</mml:mi></mml:math></inline-formula> on <inline-formula><mml:math id="inf34"><mml:mi>Y</mml:mi></mml:math></inline-formula> has saturated, then a further increase in <inline-formula><mml:math id="inf35"><mml:mi>X</mml:mi></mml:math></inline-formula> will not affect <inline-formula><mml:math id="inf36"><mml:mi>Y</mml:mi></mml:math></inline-formula>. In this article, causality is represented by a hollow arrow. To embody probabilistic thinking (e.g. drunk driving increases the chance of car accidents; <xref ref-type="bibr" rid="bib82">Pearl, 2000</xref>), <inline-formula><mml:math id="inf37"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf38"><mml:mi>Y</mml:mi></mml:math></inline-formula> are depicted as histograms. Sometimes, perturbations in one variable can change the current value of another variable if, for example, the two variables are linked by a conservation law (e.g. conservation of energy). Some have argued that these are also causal relationships (<xref ref-type="bibr" rid="bib117">Woodward, 2016</xref>). (<bold>B</bold>) Direct versus indirect causality. The direct causers of <inline-formula><mml:math id="inf39"><mml:mi>Y</mml:mi></mml:math></inline-formula> are given by the minimal set of variables such that once the entire set is fixed, no other variables can cause <inline-formula><mml:math id="inf40"><mml:mi>Y</mml:mi></mml:math></inline-formula>. Here, three variables <inline-formula><mml:math id="inf41"><mml:mi>X</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf42"><mml:mi>Z</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf43"><mml:mi>U</mml:mi></mml:math></inline-formula> activate <inline-formula><mml:math id="inf44"><mml:mi>Y</mml:mi></mml:math></inline-formula>. The set <inline-formula><mml:math id="inf45"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> constitutes the direct causers of <inline-formula><mml:math id="inf46"><mml:mi>Y</mml:mi></mml:math></inline-formula> (or <inline-formula><mml:math id="inf47"><mml:mi>Y</mml:mi></mml:math></inline-formula>’s ‘parents‘ [<xref ref-type="bibr" rid="bib49">Hausman and Woodward, 1999</xref>; <xref ref-type="bibr" rid="bib82">Pearl, 2000</xref>]), since if we fix both <inline-formula><mml:math id="inf48"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf49"><mml:mi>Z</mml:mi></mml:math></inline-formula>, then <inline-formula><mml:math id="inf50"><mml:mi>Y</mml:mi></mml:math></inline-formula> becomes independent of <inline-formula><mml:math id="inf51"><mml:mi>U</mml:mi></mml:math></inline-formula>. If a causer is not direct, we say that it is indirect. Whether a causer is direct or indirect can depend on the scope of included variables. For example, suppose that yeast releases acetate, and acetate inhibits the growth of bacteria. If acetate is not in our scope, then yeast density has a direct causal effect on bacterial density. Conversely, if acetate is included in our scope, then acetate (but not yeast) is the direct causer of bacterial density (since fixing acetate concentration would fix bacterial growth regardless of yeast density). When we draw interaction networks with more than two variables, hollow arrows between variables denote direct causation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-fig1-v2.tif"/></fig></sec><sec id="s2-2"><title>Correlation versus dependence</title><p>The adage ‘correlation is not causality’ is well-known to the point of being cliché (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>; <xref ref-type="bibr" rid="bib23">Coenen and Weitz, 2018</xref>; <xref ref-type="bibr" rid="bib16">Carr et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Mainali et al., 2019</xref>). Yet, to dismiss correlative evidence altogether seems too extreme. To make use of correlative evidence without being reckless, it helps to distinguish between the terms ‘correlation’ and ‘dependence’. When applied to ecological time series, the term ‘correlation’ is often used to describe some statistic that quantifies the similarity between two observed time series (<xref ref-type="bibr" rid="bib115">Weiss et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Coenen and Weitz, 2018</xref>). Examples include Pearson’s correlation coefficient and local similarity (<xref ref-type="bibr" rid="bib90">Ruan et al., 2006</xref>). In contrast, statistical dependence is a hypothesis about the probability distributions that produced those time series, and has close connections to causality.</p><p>Dependence has a precise definition in statistics, and is most easily described for two binary events. For instance, if the incidence of vision loss is higher among diabetics than among the general population, then vision loss and diabetes are statistically dependent. In general, events <inline-formula><mml:math id="inf52"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf53"><mml:mi>B</mml:mi></mml:math></inline-formula> are dependent if across many independent trials (e.g. patients), the probability that <inline-formula><mml:math id="inf54"><mml:mi>A</mml:mi></mml:math></inline-formula> occurs given that <inline-formula><mml:math id="inf55"><mml:mi>B</mml:mi></mml:math></inline-formula> has occurred (e.g. incidence of vision loss among diabetics only) is different from the background probability that <inline-formula><mml:math id="inf56"><mml:mi>A</mml:mi></mml:math></inline-formula> occurs (e.g. background incidence of vision loss). If <inline-formula><mml:math id="inf57"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf58"><mml:mi>B</mml:mi></mml:math></inline-formula> are not dependent, then they are called independent. The concept of dependence is readily generalized from binary events to numerical variables, and also to vectors such as time series (Appendix 1).</p><p>Dependence is connected to causation by the widely accepted ‘Common Cause Principle’: <italic>if two variables are dependent, then they are causally related</italic> (i.e. one causes the other, or both share a common cause; <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>; <xref ref-type="bibr" rid="bib93">Runge et al., 2019a</xref>; <xref ref-type="bibr" rid="bib53">Hitchcock, 2020b</xref>; <xref ref-type="bibr" rid="bib52">Hitchcock and Rédei, 2020a</xref>). Note however that if one mistakenly introduces selection bias, then two independent variables can appear to be dependent (<xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3</xref>). The closely related property of conditional dependence (i.e. whether two variables are dependent after statistically controlling for certain other variables; Appendix 1) can be even more causally informative. In fact, when conditional dependence (and conditional independence) relationships are known, it is sometimes possible to infer most or all of the direct causal relationships at play, even without manipulative experiments or temporal information. Many of the algorithms that accomplish this rely on two technical but often reasonable assumptions: the ‘causal Markov condition’, which allows one to infer causal information from conditional <italic>dependence</italic>, and the ‘causal faithfulness condition’, which allows one to infer causal information from conditional <italic>independence</italic> (Appendix 2; <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Glymour et al., 2019</xref>; <xref ref-type="bibr" rid="bib53">Hitchcock, 2020b</xref>).</p><p>In sum, whereas a correlation is a statistical description of data, statistical dependence is a hypothesis about the relationship between the underlying probability distributions. Dependence is in turn linked to causality. Below, we discuss tests that use correlation to detect dependence in time series.</p></sec><sec id="s2-3"><title>Testing for dependence between time series using surrogate data</title><p>Despite its scientific usefulness, dependence between time series can be treacherous to test for. This is because time series are often autocorrelated (e.g. what occurs today influences what occurs tomorrow), so that a single pair of time series contains information from only a single trial. If one has many trials that are independent and free of systematic differences (e.g. <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi/><mml:mo>≥</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> as in some laboratory microcosm experiments), the task is relatively easy: One can test whether the abundances of species <inline-formula><mml:math id="inf60"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf61"><mml:mi>Y</mml:mi></mml:math></inline-formula> are statistically dependent by comparing the correlation between <inline-formula><mml:math id="inf62"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mi>Y</mml:mi></mml:math></inline-formula> abundance series from the same trial with those between <inline-formula><mml:math id="inf64"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf65"><mml:mi>Y</mml:mi></mml:math></inline-formula> abundance series from different trials (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>; see also <xref ref-type="bibr" rid="bib75">Moulder et al., 2018</xref>). However, a large trial number is generally a luxury and often only one trial is available. In such cases, attempting to discern whether two time series are statistically dependent is like attempting to divine whether diabetes and vision loss are dependent with only a single patient (i.e. we have an ‘<inline-formula><mml:math id="inf66"><mml:mi>n</mml:mi></mml:math></inline-formula>-of-one problem’). As one possible remedy, there are parametric tests using the Pearson correlation coefficient that account for autocorrelation. In these tests, one estimates the correlation coefficient between time series, and evaluates its statistical significance using the variance of the null distribution (<xref ref-type="bibr" rid="bib1">Afyouni et al., 2019</xref>). However, the calculation of this variance relies on estimates of the autocorrelation at each lag for both time series, which can be highly uncertain (<xref ref-type="bibr" rid="bib86">Pyper and Peterman, 1998</xref>; <xref ref-type="bibr" rid="bib33">Ebisuzaki, 1997</xref>). Furthermore, after estimating the variance, one must also assume the shape of the null distribution before a p-value can be assigned to the correlation.</p><p>Alternatively, the <inline-formula><mml:math id="inf67"><mml:mi>n</mml:mi></mml:math></inline-formula>-of-one problem is often addressed by a technique called surrogate data testing. Specifically, one computes some measure of correlation between two time series <inline-formula><mml:math id="inf68"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf69"><mml:mi>Y</mml:mi></mml:math></inline-formula>. Next, one uses a computer to simulate replicates of <inline-formula><mml:math id="inf70"><mml:mi>Y</mml:mi></mml:math></inline-formula> that might have been obtained if <inline-formula><mml:math id="inf71"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mi>Y</mml:mi></mml:math></inline-formula> were independent (see below). Each simulated replicate is called a ‘surrogate’ <inline-formula><mml:math id="inf73"><mml:mi>Y</mml:mi></mml:math></inline-formula>. Finally, one computes the correlation between <inline-formula><mml:math id="inf74"><mml:mi>X</mml:mi></mml:math></inline-formula> and each surrogate <inline-formula><mml:math id="inf75"><mml:mi>Y</mml:mi></mml:math></inline-formula>. A p-value (representing evidence against the null hypothesis that <inline-formula><mml:math id="inf76"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf77"><mml:mi>Y</mml:mi></mml:math></inline-formula> are independent) is then determined by counting how many of the surrogate <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>s produce a correlation at least as strong as the real <inline-formula><mml:math id="inf79"><mml:mi>Y</mml:mi></mml:math></inline-formula>. For example, if we produced 19 surrogates and found the real correlation to be stronger than all 19 surrogate correlations, then we would write down a p-value of <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>19</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>. Ideally, if two time series are independent, then we should register a a p-value of 0.05 (or less) in only 5% of cases.</p><p>Several procedures can be used to produce surrogate time series, each corresponding to an assumption about how the original time series was generated (<xref ref-type="bibr" rid="bib62">Lancaster et al., 2018</xref>). One popular procedure is to simply shuffle the values of a time series (<xref ref-type="bibr" rid="bib90">Ruan et al., 2006</xref>; <xref ref-type="bibr" rid="bib34">Eiler et al., 2012</xref>; <xref ref-type="bibr" rid="bib101">Shade et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Cyriaque et al., 2020</xref>). This procedure, often called permutation, assumes that all possible orderings of the time points in the series are equally likely. This assumption is commonly violated in time series due to autocorrelation, and thus the test is often invalid. For example, for independent time series in <xref ref-type="fig" rid="fig2">Figure 2B–C</xref>, this test returns <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> at rates of <inline-formula><mml:math id="inf82"><mml:mrow><mml:mn>30</mml:mn><mml:mo>∼</mml:mo><mml:mrow><mml:mn>92</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, much higher than 5%. Nevertheless, permutation testing has appeared in many applied works, perhaps because it has been the default option in some popular software packages. Another procedure for generating surrogates is called phase randomization. It first uses the Fourier transform to represent a time series as a sum of sine waves, then randomly shifts each of the component sine waves in time, and finally sums the phase-shifted components (<xref ref-type="bibr" rid="bib33">Ebisuzaki, 1997</xref>; <xref ref-type="bibr" rid="bib99">Schreiber and Schmitz, 2000</xref>; <xref ref-type="bibr" rid="bib3">Andrzejak et al., 2003</xref>; <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>). This procedure is considered appropriate when the original time series is obtained from a linear, Gaussian, and stationary process (<xref ref-type="bibr" rid="bib3">Andrzejak et al., 2003</xref>; <xref ref-type="bibr" rid="bib62">Lancaster et al., 2018</xref>), where ‘linear’ means that future values depend linearly on past values, ‘Gaussian’ means that any subsequence follows a multivariate Gaussian distribution, and ‘stationary’ means that this distribution does not change over time. See <xref ref-type="bibr" rid="bib18">Chan, 1997</xref> for a discussion of exact requirements. Indeed, this test performed well (with a false positive rate of 4%) when time series satisfied its assumptions (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), and poorly when the stationarity assumption was violated (with a false positive rate of 21%; <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Other surrogate data procedures include time shifting (<xref ref-type="bibr" rid="bib3">Andrzejak et al., 2003</xref>), the block bootstrap (<xref ref-type="bibr" rid="bib81">Papana et al., 2017</xref>), and the twin method (<xref ref-type="bibr" rid="bib109">Thiel et al., 2006</xref>). Some surrogate data tests have been shown to perform reasonably well even when the exact theoretical requirements are unmet or unknown (<xref ref-type="bibr" rid="bib109">Thiel et al., 2006</xref>; <xref ref-type="bibr" rid="bib81">Papana et al., 2017</xref>), but a more comprehensive benchmarking effort is needed to map out each method’s valid domain in practice.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Two independent temporal processes can appear significantly correlated when compared to an inappropriate null model.</title><p>(<bold>A</bold>) Densities of independent yeast and bacteria cultures growing exponentially are correlated. (<bold>B, C</bold>) Correlation between time series of two independent island populations can appear significant if inappropriate tests are used. (<bold>B</bold>) In an island (“isl”), individuals stochastically migrate to and from the mainland in roughly equal numbers so that total island biomass follows a random walk. At each time step, the net change in island biomass is drawn from a standard normal distribution (mean = 0; standard deviation = 1 biomass unit). (<bold>C</bold>) An island population receives cells through migration and loses cells via death. Observations are made after 1000 steps, so that the population size has reached an equilibrium. For both (<bold>B</bold>) and (<bold>C</bold>), we performed 1000 simulations in which we calculated the Pearson correlation coefficient of a pair of independent islands populations. Both panels contain: example time series (upper right), a scatterplot comparing two independent islands (lower left), the distribution of Pearson correlation coefficient strength (blue shading), and the proportion of simulations in which the correlation was deemed significant (<inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>) by surrogate data tests using either permutation or phase randomization (see main text). Ideally, the proportion of correlations that are significant (false positives) should not exceed 5%. The strength of correlation is weaker in (<bold>C</bold>) compared to (<bold>B</bold>), yet still often significant according to the permutation test. See Appendix 5 for more details.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-fig2-v2.tif"/></fig><p>In sum, surrogate data allow a researcher to use an observed correlation statistic to test for dependence under some assumption about the data-generating process. Dependence indicates the presence of a causal relationship, and conditional dependence can sometimes even indicate the direction (<xref ref-type="bibr" rid="bib53">Hitchcock, 2020b</xref>; <xref ref-type="bibr" rid="bib38">Glymour et al., 2019</xref>; <xref ref-type="bibr" rid="bib51">Heinze-Deml et al., 2018</xref>; <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2</xref>). Below we consider Granger causality and state space reconstruction, two approaches that can be used to directly infer the direction of causality from time series.</p></sec></sec><sec id="s3"><title>Granger causality: intuition, pitfalls, and implementations</title><sec id="s3-1"><title>Intuition and formal definitions</title><p>In simple language, <inline-formula><mml:math id="inf84"><mml:mi>X</mml:mi></mml:math></inline-formula> is said to Granger-cause <inline-formula><mml:math id="inf85"><mml:mi>Y</mml:mi></mml:math></inline-formula> if a collection of time series containing all historical measurements predicts <inline-formula><mml:math id="inf86"><mml:mi>Y</mml:mi></mml:math></inline-formula>’s future behavior better than a similar collection that excludes the history of <inline-formula><mml:math id="inf87"><mml:mi>X</mml:mi></mml:math></inline-formula>. An important consequence of this definition is that Granger causality excludes indirect causes, as illustrated in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. In practice, whether a causal relationship is direct or indirect depends on which variables are observed. For instance, in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, if <inline-formula><mml:math id="inf88"><mml:mi>Y</mml:mi></mml:math></inline-formula> were not observed, then <inline-formula><mml:math id="inf89"><mml:mi>X</mml:mi></mml:math></inline-formula> would “directly” cause (and Granger-cause) <inline-formula><mml:math id="inf90"><mml:mi>Z</mml:mi></mml:math></inline-formula>.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Causality versus Granger causality.</title><p>(<bold>A</bold>) Granger causality is designed to reveal direct causes, not indirect causes. Although <inline-formula><mml:math id="inf91"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf92"><mml:mi>Z</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf93"><mml:mi>X</mml:mi></mml:math></inline-formula> does not Granger-cause <inline-formula><mml:math id="inf94"><mml:mi>Z</mml:mi></mml:math></inline-formula> because with the history of <inline-formula><mml:math id="inf95"><mml:mi>Y</mml:mi></mml:math></inline-formula> available, the history of <inline-formula><mml:math id="inf96"><mml:mi>X</mml:mi></mml:math></inline-formula> no longer adds value for predicting <inline-formula><mml:math id="inf97"><mml:mi>Z</mml:mi></mml:math></inline-formula>. This also shows that Granger causality is not transitive: <inline-formula><mml:math id="inf98"><mml:mi>X</mml:mi></mml:math></inline-formula> Granger-causes <inline-formula><mml:math id="inf99"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf100"><mml:mi>Y</mml:mi></mml:math></inline-formula> Granger-causes <inline-formula><mml:math id="inf101"><mml:mi>Z</mml:mi></mml:math></inline-formula>, but <inline-formula><mml:math id="inf102"><mml:mi>X</mml:mi></mml:math></inline-formula> does not Granger-cause <inline-formula><mml:math id="inf103"><mml:mi>Z</mml:mi></mml:math></inline-formula>. (<bold>B</bold>) Failure modes of Granger causality when inferring direct causality. (<bold>i</bold>) False negative due to lack of stochasticity. <inline-formula><mml:math id="inf104"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf105"><mml:mi>Y</mml:mi></mml:math></inline-formula> mutually and deterministically cause one another through a copy operation (<xref ref-type="bibr" rid="bib5">Ay and Polani, 2011</xref>; <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>): <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> copies <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and vice versa. Since <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> already contains sufficient information to know <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> exactly, the history of <inline-formula><mml:math id="inf110"><mml:mi>Y</mml:mi></mml:math></inline-formula> cannot improve prediction of <inline-formula><mml:math id="inf111"><mml:mi>X</mml:mi></mml:math></inline-formula>, and so <inline-formula><mml:math id="inf112"><mml:mi>Y</mml:mi></mml:math></inline-formula> does not Granger-cause <inline-formula><mml:math id="inf113"><mml:mi>X</mml:mi></mml:math></inline-formula>. By symmetry, <inline-formula><mml:math id="inf114"><mml:mi>X</mml:mi></mml:math></inline-formula> does not Granger-cause <inline-formula><mml:math id="inf115"><mml:mi>Y</mml:mi></mml:math></inline-formula>. (<bold>ii</bold>) False positive due to unobserved common cause. <inline-formula><mml:math id="inf116"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf117"><mml:mi>Y</mml:mi></mml:math></inline-formula> with a delay of 1, and causes <inline-formula><mml:math id="inf118"><mml:mi>Z</mml:mi></mml:math></inline-formula> with a delay of 2. We only observe <inline-formula><mml:math id="inf119"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf120"><mml:mi>Z</mml:mi></mml:math></inline-formula>. Since <inline-formula><mml:math id="inf121"><mml:mi>Y</mml:mi></mml:math></inline-formula> receives the same “information” before <inline-formula><mml:math id="inf122"><mml:mi>Z</mml:mi></mml:math></inline-formula>, the history of <inline-formula><mml:math id="inf123"><mml:mi>Y</mml:mi></mml:math></inline-formula> helps to predict <inline-formula><mml:math id="inf124"><mml:mi>Z</mml:mi></mml:math></inline-formula>, and thus <inline-formula><mml:math id="inf125"><mml:mi>Y</mml:mi></mml:math></inline-formula> Granger-causes <inline-formula><mml:math id="inf126"><mml:mi>Z</mml:mi></mml:math></inline-formula>, resulting in a false positive. (<bold>iii</bold>) Infrequent sampling can induce false negatives. Although there is a Granger causality signal when we sample once per time step, the signal is lost when we sample only once per two steps (<xref ref-type="bibr" rid="bib39">Gong et al., 2015</xref>). (<bold>iv</bold>) Measurement noise can lead Granger causality to suffer both false positives and false negatives. <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> represent process noise and are normal random variables with mean of 0 and variance of 1. All process noise terms are independent of one another.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-fig3-v2.tif"/></fig><p>Granger causality has many related but nonequivalent quantitative incarnations in the literature, including several that were proposed by Granger himself (<xref ref-type="bibr" rid="bib41">Granger, 1969</xref>; <xref ref-type="bibr" rid="bib42">Granger, 1980</xref>). <xref ref-type="box" rid="box1">Box 1</xref> presents two definitions: one based on a linear regression which we call ‘linear Granger causality’ (<xref ref-type="bibr" rid="bib37">Gibbons et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Ai et al., 2019</xref>; <xref ref-type="bibr" rid="bib8">Barraquand et al., 2020</xref>; <xref ref-type="bibr" rid="bib69">Mainali et al., 2019</xref>) and another more general definition which we call ‘general Granger causality’ (also sometimes called nonlinear Granger causality; <xref ref-type="bibr" rid="bib42">Granger, 1980</xref>; <xref ref-type="bibr" rid="bib31">Diks and Panchenko, 2006</xref>; <xref ref-type="bibr" rid="bib11">Bekiros and Diks, 2008</xref>; <xref ref-type="bibr" rid="bib112">Vicente et al., 2011</xref>; <xref ref-type="bibr" rid="bib89">Roux et al., 2013</xref>; <xref ref-type="bibr" rid="bib81">Papana et al., 2017</xref>). See theorem 10.3 of <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref> for a discussion of the theoretical relationship between general Granger causality and (true) causality.</p><boxed-text id="box1"><label>Box 1.</label><caption><title>Granger causality</title></caption><p><bold>1. Linear Granger causality:</bold></p><p>Under linear Granger causality, <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> Granger-causes <inline-formula><mml:math id="inf131"><mml:mi>Y</mml:mi></mml:math></inline-formula> if including the history of <inline-formula><mml:math id="inf132"><mml:mi>X</mml:mi></mml:math></inline-formula> in a linear autoregressive model (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) allows for a better prediction of future <inline-formula><mml:math id="inf133"><mml:mi>Y</mml:mi></mml:math></inline-formula> than not including the history of <inline-formula><mml:math id="inf134"><mml:mi>X</mml:mi></mml:math></inline-formula> (i.e. setting all <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>α</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> coefficients to zero). By “linear autoregressive model”, we mean that the future value of variable <inline-formula><mml:math id="inf136"><mml:mi>Y</mml:mi></mml:math></inline-formula> is modeled as a linear combination of historical values of <inline-formula><mml:math id="inf137"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf138"><mml:mi>Y</mml:mi></mml:math></inline-formula> and all other observed variables that might help predict <inline-formula><mml:math id="inf139"><mml:mi>Y</mml:mi></mml:math></inline-formula> (“...”):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf140"><mml:mi>t</mml:mi></mml:math></inline-formula> is the time index, <inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a time lag index, <inline-formula><mml:math id="inf142"><mml:mi>c</mml:mi></mml:math></inline-formula> is a constant, coefficients such as <inline-formula><mml:math id="inf143"><mml:msub><mml:mi>α</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf144"><mml:msub><mml:mi>β</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> represent the strength of contributions from their respective terms, and <inline-formula><mml:math id="inf145"><mml:msub><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> represents independent and identically-distributed (IID, Appendix 1) process noise (Figure 7A).</p><p><bold>2. General Granger causality (</bold><xref ref-type="bibr" rid="bib42">Granger, 1980</xref><bold>):</bold></p><p>Let <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> be series of random variables indexed by time <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> Granger-causes <inline-formula><mml:math id="inf151"><mml:mi>Y</mml:mi></mml:math></inline-formula> with respect to the information set <inline-formula><mml:math id="inf152"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> if:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>≠</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>at one or more times <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the probability distribution of <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> conditional on the variable set <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note that <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> may include multiple variables and thus plays the same role as “. . .” in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>.</p></boxed-text></sec><sec id="s3-2"><title>Granger causality failure modes</title><p>We discuss four important instances where Granger causality can fail as an indicator of direct causality (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). These pathologies can be understood intuitively and can apply to both linear and general Granger causality. First, if a system has deterministic dynamics (see Appendix 3), then Granger causality may fail to detect causal relations (<xref ref-type="fig" rid="fig3">Figure 3Bi</xref>). More generally, if dynamics have a low degree of randomness, Granger causality signals can be very weak (e.g. knowing <inline-formula><mml:math id="inf158"><mml:mi>X</mml:mi></mml:math></inline-formula>’s past improves predictions of <inline-formula><mml:math id="inf159"><mml:mi>Y</mml:mi></mml:math></inline-formula>’s future only slightly; <xref ref-type="bibr" rid="bib58">Janzing et al., 2013</xref>; <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>). Moreover, as we will discuss later, this limitation has motivated other methods that take a primarily deterministic view (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>). Second, Granger causality may erroneously assign a direct causal relation between a pair of variables that have an unobserved common cause (<xref ref-type="fig" rid="fig3">Figure 3Bii</xref>). Third, recording data at a frequency below that of the original process by ‘subsampling’ (e.g. taking weekly measurements of a daily process) or by ‘temporal aggregation’ (e.g. taking weekly averages of a daily process) can alter the inferred causal structure (<xref ref-type="fig" rid="fig3">Figure 3Biii</xref>), although recent techniques can help with these issues (<xref ref-type="bibr" rid="bib39">Gong et al., 2015</xref>; <xref ref-type="bibr" rid="bib56">Hyttinen et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Gong et al., 2017</xref>). Lastly, when measurements are noisy (<xref ref-type="fig" rid="fig3">Figure 3Biv</xref>), Granger causality can assign false interactions and also fail to detect true causality (<xref ref-type="bibr" rid="bib78">Newbold, 1978</xref>), although some progress has been made on this front (<xref ref-type="bibr" rid="bib77">Nalatore et al., 2007</xref>).</p></sec><sec id="s3-3"><title>Practical testing for linear and general Granger causality</title><p>One might still attempt to infer Granger causality despite the above caveats, especially in situations where caveats can be largely avoided. Linear Granger causality has standard parametric tests: if any of the <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>α</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> terms in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is nonzero, then <inline-formula><mml:math id="inf161"><mml:mi>X</mml:mi></mml:math></inline-formula> linearly Granger-causes <inline-formula><mml:math id="inf162"><mml:mi>Y</mml:mi></mml:math></inline-formula>. Parametric tests are computationally inexpensive and available in multiple free and well-documented software packages (<xref ref-type="bibr" rid="bib100">Seabold and Perktold, 2010</xref>; <xref ref-type="bibr" rid="bib6">Barnett and Seth, 2014</xref>). These tests assume that time series are ‘covariance-stationary’, which means that certain statistical properties of the series are time-independent (<xref ref-type="bibr" rid="bib6">Barnett and Seth, 2014</xref>; see also Appendix 3), and can fail when this assumption is violated (<xref ref-type="bibr" rid="bib111">Toda and Phillips, 1993</xref>; <xref ref-type="bibr" rid="bib79">Ohanian, 1988</xref>; <xref ref-type="bibr" rid="bib50">He and Maekawa, 2001</xref>). Additionally, applying linear Granger causality to nonlinear systems can lead to incorrect causal conclusions (<xref ref-type="bibr" rid="bib65">Li et al., 2018</xref>). One can assess whether the linear model (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) is a reasonable approximation, for instance by checking whether the model residuals <inline-formula><mml:math id="inf163"><mml:msub><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> are uncorrelated across time (<xref ref-type="bibr" rid="bib35">Feige and Pearce, 1979</xref>) as is assumed by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>.</p><p>Tests for general Granger causality often use a statistic known as transfer entropy (<xref ref-type="bibr" rid="bib80">Papana et al., 2012</xref>). Roughly, the transfer entropy from <inline-formula><mml:math id="inf164"><mml:mi>X</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf165"><mml:mi>Y</mml:mi></mml:math></inline-formula> is the extent to which the entropy (a measurement of uncertainty) of <inline-formula><mml:math id="inf166"><mml:mi>Y</mml:mi></mml:math></inline-formula>’s future is reduced when we account for (specifically, condition on) the past of <inline-formula><mml:math id="inf167"><mml:mi>X</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib98">Schreiber, 2000</xref>; <xref ref-type="bibr" rid="bib25">Cover and Thomas, 2006</xref>; <xref ref-type="bibr" rid="bib74">Montalto et al., 2014</xref>; <xref ref-type="bibr" rid="bib81">Papana et al., 2017</xref>). A significant transfer entropy thus indicates the presence of general Granger causality. Surrogate data are typically used to evaluate significance (<xref ref-type="bibr" rid="bib74">Montalto et al., 2014</xref>; <xref ref-type="bibr" rid="bib81">Papana et al., 2017</xref>; <xref ref-type="bibr" rid="bib102">Shorten et al., 2021</xref>). However, the previously discussed surrogate data procedures are designed to test the null hypothesis of independence, which is different from the null hypothesis of general Granger non-causality (i.e. <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, but replace ‘≠’ with ‘=’). More recent surrogate procedures have been proposed to address this issue (<xref ref-type="bibr" rid="bib91">Runge, 2018a</xref>; <xref ref-type="bibr" rid="bib102">Shorten et al., 2021</xref>). Several software implementations of Granger causality tests based on transfer entropy statistics are available (e.g. <xref ref-type="bibr" rid="bib74">Montalto et al., 2014</xref>; <xref ref-type="bibr" rid="bib10">Behrendt et al., 2019</xref>; <xref ref-type="bibr" rid="bib116">Wollstadt et al., 2019</xref>).</p><p>Granger causality methods face challenges when datasets have a large number of variables (e.g. in microbial ecology). In this case, the summation in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> will contain a large number of terms, and so a regression procedure may fail to detect many true interactions (<xref ref-type="bibr" rid="bib93">Runge et al., 2019a</xref>; <xref ref-type="bibr" rid="bib94">Runge et al., 2019b</xref>). To handle systems with many variables, one can impose the assumption that only a small number of causal links exist (<xref ref-type="bibr" rid="bib37">Gibbons et al., 2017</xref>; <xref ref-type="bibr" rid="bib69">Mainali et al., 2019</xref>). This is sometimes called sparse regression or regularization. Additionally, under certain technical assumptions, it is possible to use a series of logical rules to remove unnecessary terms in a purely data-driven way (<xref ref-type="bibr" rid="bib94">Runge et al., 2019b</xref>; <xref ref-type="bibr" rid="bib93">Runge et al., 2019a</xref>). As an example, suppose that we wish to test whether pH is a Granger-cause of chlorophyll concentration in some aquatic environment and we infer based on a prior analysis that chlorophyll concentration is always independent of fluctuations in salinity. Then, most likely, salinity is irrelevant to the pH-chlorophyll relationship and can be safely omitted from our Granger causality analysis. As an aside, this reasoning could theoretically fail in pathological cases where, for instance, the ‘faithfulness’ condition (Appendix 2) is violated (see Example 7 of <xref ref-type="bibr" rid="bib92">Runge, 2018b</xref> for a worked counterexample). These rules and their associated assumptions are formalized in ‘constraint-based’ causal discovery algorithms (Appendix 2; <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Glymour et al., 2019</xref>). The development of new causal discovery algorithms, and their application to time series, is a very active area of research (<xref ref-type="bibr" rid="bib57">Hyvärinen et al., 2010</xref>; <xref ref-type="bibr" rid="bib94">Runge et al., 2019b</xref>; <xref ref-type="bibr" rid="bib93">Runge et al., 2019a</xref>; <xref ref-type="bibr" rid="bib95">Sanchez-Romero et al., 2019</xref>).</p></sec></sec><sec id="s4"><title>State space reconstruction (SSR): intuition, pitfalls, and implementations</title><p>The term ‘state space reconstruction’ (SSR) refers to a broad swath of techniques for prediction, inference, and estimation in time series analysis (<xref ref-type="bibr" rid="bib17">Casdagli et al., 1991</xref>; <xref ref-type="bibr" rid="bib61">Kugiumtzis et al., 1994</xref>; <xref ref-type="bibr" rid="bib4">Asefa et al., 2005</xref>; <xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Cummins et al., 2015</xref>). In this article, when we use the term SSR, we refer only to SSR methods for causality detection. The SSR approach is especially popular in empirical ecology (<xref ref-type="bibr" rid="bib12">Brookshire and Weaver, 2015</xref>; <xref ref-type="bibr" rid="bib26">Cramer et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Hannisdal et al., 2017</xref>; <xref ref-type="bibr" rid="bib71">Matsuzaki et al., 2018</xref>; <xref ref-type="bibr" rid="bib114">Wang et al., 2019</xref>). SSR methods are intended to complement Granger causality: Whereas Granger causality has trouble with deterministic dynamics (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), the SSR approach is explicitly designed for systems that are primarily deterministic (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>). Since SSR is less intuitive than correlation or Granger causality, we introduce it with an example rather than a definition.</p><sec id="s4-1"><title>Visualizing SSR causal discovery</title><p>Consider the deterministic dynamical system in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Here, <inline-formula><mml:math id="inf168"><mml:mi>Z</mml:mi></mml:math></inline-formula> is causally driven by <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, but not by <inline-formula><mml:math id="inf171"><mml:mi>W</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf172"><mml:mi>V</mml:mi></mml:math></inline-formula>. We can make a vector out of the current value <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and two past values <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where τ is the time delay and <inline-formula><mml:math id="inf176"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is called a ‘delay vector’ (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, red dots). The delay vector can be represented as a single point in the three-dimensional <inline-formula><mml:math id="inf177"><mml:mi>Z</mml:mi></mml:math></inline-formula> ‘delay space’ (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, red dot). We then shade each point of the trajectory in <inline-formula><mml:math id="inf178"><mml:mi>Z</mml:mi></mml:math></inline-formula> delay space according to the contemporaneous value of <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which causally influences <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Since in this example each point of the trajectory in <inline-formula><mml:math id="inf181"><mml:mi>Z</mml:mi></mml:math></inline-formula> delay space corresponds to one and only one <inline-formula><mml:math id="inf182"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> value, we call this a ‘delay map’ from <inline-formula><mml:math id="inf183"><mml:mi>Z</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf184"><mml:mi>Y</mml:mi></mml:math></inline-formula>. Notice that the <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> gradient in this plot looks gradual in the sense that if two points are nearby in the delay space of <inline-formula><mml:math id="inf186"><mml:mi>Z</mml:mi></mml:math></inline-formula>, then their corresponding <inline-formula><mml:math id="inf187"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> shades are also similar. This property is called ‘continuity’ (<xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>). Overall, there is a continuous map from the <inline-formula><mml:math id="inf188"><mml:mi>Z</mml:mi></mml:math></inline-formula> delay space to <inline-formula><mml:math id="inf189"><mml:mi>Y</mml:mi></mml:math></inline-formula>, or more concisely, a ‘continuous delay map’ from <inline-formula><mml:math id="inf190"><mml:mi>Z</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf191"><mml:mi>Y</mml:mi></mml:math></inline-formula>. A similar continuous delay map also exists from <inline-formula><mml:math id="inf192"><mml:mi>Z</mml:mi></mml:math></inline-formula> to its other causer <inline-formula><mml:math id="inf193"><mml:mi>X</mml:mi></mml:math></inline-formula>. On the other hand, if we shade the delay space of <inline-formula><mml:math id="inf194"><mml:mi>Z</mml:mi></mml:math></inline-formula> by <inline-formula><mml:math id="inf195"><mml:mi>W</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf196"><mml:mi>V</mml:mi></mml:math></inline-formula> (neither of which causes <inline-formula><mml:math id="inf197"><mml:mi>Z</mml:mi></mml:math></inline-formula>), we do not get a continuous delay map (<xref ref-type="fig" rid="fig4">Figure 4D–E</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>SSR causal methods look for a continuous map from the delay space of a causee to the causer, and this approach becomes more difficult in the presence of noise.</title><p>(<bold>A</bold>) A toy 5-variable linear system. (<bold>B</bold>) Time series. The delay vector <inline-formula><mml:math id="inf198"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> (shown as three red dots) can be represented as a single point in the 3-dimensional <inline-formula><mml:math id="inf199"><mml:mi>Z</mml:mi></mml:math></inline-formula> delay space (C, red dot). (<bold>C</bold>) We then shade each point of the <inline-formula><mml:math id="inf200"><mml:mi>Z</mml:mi></mml:math></inline-formula> delay space trajectory by its corresponding contemporaneous value of <inline-formula><mml:math id="inf201"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (without measurement noise). The shading is continuous (with gradual transitions in shade), which the SSR approach interprets as indicating that <inline-formula><mml:math id="inf202"><mml:mi>Y</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf203"><mml:mi>Z</mml:mi></mml:math></inline-formula> (correctly in this case). (<bold>D</bold>) When we repeat this procedure, but now shade the <inline-formula><mml:math id="inf204"><mml:mi>Z</mml:mi></mml:math></inline-formula> delay space trajectory by <inline-formula><mml:math id="inf205"><mml:mrow><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the shading is bumpy, which the SSR approach correctly interprets to indicate that <inline-formula><mml:math id="inf206"><mml:mi>W</mml:mi></mml:math></inline-formula> does not cause <inline-formula><mml:math id="inf207"><mml:mi>Z</mml:mi></mml:math></inline-formula>. (<bold>E</bold>) Shading the delay space trajectory of <inline-formula><mml:math id="inf208"><mml:mi>Z</mml:mi></mml:math></inline-formula> by the causally unrelated <inline-formula><mml:math id="inf209"><mml:mi>V</mml:mi></mml:math></inline-formula> also gives a bumpy result. (<bold>F</bold>) Dynamics as in (<bold>C</bold>), but now with noisy measurements of <inline-formula><mml:math id="inf210"><mml:mi>Y</mml:mi></mml:math></inline-formula> (purple in B). The shading is no longer gradual. Thus with noisy data, inferring causal relationships becomes more difficult.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-fig4-v2.tif"/></fig><p>In this example, there is a continuous delay map from a causee to a causer, but not the other way around, and also no continuous delay map between causally unrelated variables. If this behavior reflects a broader principle, then perhaps continuous delay maps can be used to infer the presence and direction of causation. Is there in fact a broader principle?</p><p>In fact, there is a sort of broader principle, but it may not be fully satisfying for causality testing. The principle stems from a classic theorem due to <xref ref-type="bibr" rid="bib108">Takens, 1980</xref>. A rough translation of Takens’ theorem is the following: If a particle follows a deterministic trajectory which forms a surface (e.g. an ant crawling all over a doughnut), and if we take one-dimensional measurements of that particle’s position over time (e.g. the distance from the ant’s starting position), then we are almost guaranteed to find a continuous delay map from our measurements (of current distance) to the original surface (the donut), as long as we use enough delays. (We walk through visual examples of these ideas in detail in Appendix 4.) A key result that follows from this theorem is that we can typically (‘generically’) expect to find continuous delay maps from ‘dynamically driven’ variables to ‘dynamically driving’ variables in a coupled deterministic dynamical system, as long as certain technical requirements are met (<xref ref-type="bibr" rid="bib27">Cummins et al., 2015</xref>). Although the notion of ‘dynamic driving’ (<xref ref-type="bibr" rid="bib27">Cummins et al., 2015</xref>) differs from our definition of causation, the two are related and we will still use the standard notion of causation when evaluating the performance of SSR methods. In theory, Takens’ theorem says that almost any choice of delay vector should work as long as it contains enough delays. However in practice, with finite noisy data, the behavior of SSR methods can depend on the delay vector selection procedure (<xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>; see also Appendix 4). Overall, Takens’ theorem and later results (<xref ref-type="bibr" rid="bib96">Sauer et al., 1991</xref>; <xref ref-type="bibr" rid="bib27">Cummins et al., 2015</xref>) form the theoretical basis of SSR techniques.</p><p>SSR techniques attempt to detect a continuous delay map (or a related feature) between two variables and use this to infer the presence and direction of causation (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>; <xref ref-type="bibr" rid="bib68">Ma et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Harnack et al., 2017</xref>): A continuous delay map from <inline-formula><mml:math id="inf211"><mml:mi>Y</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf212"><mml:mi>X</mml:mi></mml:math></inline-formula> is taken as an indication that <inline-formula><mml:math id="inf213"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf214"><mml:mi>Y</mml:mi></mml:math></inline-formula>. The fact that the map points in the opposite direction as the expected causation is potentially counterintuitive. One informal explanation is that the delay vectors of the causee can contain a record of past influence from the causer (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>). As a word of warning, while causation is one possible explanation for a continuous delay map, it is not the only possible explanation. Indeed, we now illustrate scenarios where a causal relationship and a continuous delay map do not coincide.</p></sec><sec id="s4-2"><title>SSR failure modes</title><p><xref ref-type="fig" rid="fig5">Figure 5</xref> illustrates four failure modes of SSR. In the first failure mode, which we refer to as ‘nonreverting continuous dynamics’ (top row of <xref ref-type="fig" rid="fig5">Figure 5</xref>; see also Appendix 4), a continuous map arises from the delay space of <inline-formula><mml:math id="inf215"><mml:mi>X</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf216"><mml:mi>Z</mml:mi></mml:math></inline-formula> because a continuous map can be found from the delay space of <inline-formula><mml:math id="inf217"><mml:mi>X</mml:mi></mml:math></inline-formula> to time (‘nonreverting <inline-formula><mml:math id="inf218"><mml:mi>X</mml:mi></mml:math></inline-formula>’) and from time to <inline-formula><mml:math id="inf219"><mml:mi>Z</mml:mi></mml:math></inline-formula> (‘continuous <inline-formula><mml:math id="inf220"><mml:mi>Z</mml:mi></mml:math></inline-formula>’). This pathology leads to false causal conclusions and may explain apparently causal results in some early works where SSR methods were applied to data with a clear temporal trend. We are not aware of statistical tests for this problem, but <xref ref-type="bibr" rid="bib21">Clark et al., 2015</xref> recommend shading points in the delay space with their corresponding time to visually check for a time trend. In the second failure mode (<xref ref-type="fig" rid="fig5">Figure 5</xref>, second row; see also <xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>), one variable drives another variable in such a way that the dynamics of the two variables are synchronized. Consequently, although the true causal relationship is unidirectional, bidirectional causality is inferred. Although the ‘prediction lag test’ (<xref ref-type="fig" rid="fig6">Figure 6B</xref> right panel) can sometimes alleviate this problem (<xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>), it is not foolproof as we demonstrate in Appendix 4. In the third failure mode (<xref ref-type="fig" rid="fig5">Figure 5</xref> third row), <inline-formula><mml:math id="inf221"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf222"><mml:mi>Z</mml:mi></mml:math></inline-formula> both oscillate and <inline-formula><mml:math id="inf223"><mml:mi>X</mml:mi></mml:math></inline-formula>’s period is an integer multiple of <inline-formula><mml:math id="inf224"><mml:mi>Z</mml:mi></mml:math></inline-formula>’s period. In this case, <inline-formula><mml:math id="inf225"><mml:mi>Z</mml:mi></mml:math></inline-formula> is inferred to cause <inline-formula><mml:math id="inf226"><mml:mi>X</mml:mi></mml:math></inline-formula> even though they are causally unrelated (see also <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>). In the fourth failure mode (<xref ref-type="fig" rid="fig5">Figure 5</xref>, bottom row), SSR gives a false negative error due to ‘pathological symmetry’, although this may be rare in practice.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Failure modes associated with SSR-based causal discovery.</title><p>Top row: Nonreverting continuous dynamics may lead SSR to infer causality where there is none. This example consists of two time series: a wavy linear increase and a parabolic trajectory. Although they are causally unrelated, we can find continuous delay maps between them. This is because there is (i) a continuous map from the delay vector <inline-formula><mml:math id="inf227"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf228"><mml:mi>t</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf229"><mml:mi>X</mml:mi></mml:math></inline-formula> is ‘nonreverting’), and (ii) a continuous map from <inline-formula><mml:math id="inf230"><mml:mi>t</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf231"><mml:mi>Z</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf232"><mml:mi>Z</mml:mi></mml:math></inline-formula> is ‘continuous’), and thus there is a continuous delay map from <inline-formula><mml:math id="inf233"><mml:mi>X</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf234"><mml:mi>Z</mml:mi></mml:math></inline-formula> (‘nonreverting continuous dynamics’; <xref ref-type="fig" rid="app4fig3">Appendix 4—figure 3</xref>). Thus, one falsely infers that <inline-formula><mml:math id="inf235"><mml:mi>Z</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf236"><mml:mi>X</mml:mi></mml:math></inline-formula>, and with similar reasoning that <inline-formula><mml:math id="inf237"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf238"><mml:mi>Z</mml:mi></mml:math></inline-formula>. Second row: <inline-formula><mml:math id="inf239"><mml:mi>X</mml:mi></mml:math></inline-formula> drives <inline-formula><mml:math id="inf240"><mml:mi>Z</mml:mi></mml:math></inline-formula> such that their dynamics are ‘synchronized’, and consequently, we find a continuous delay map also from <inline-formula><mml:math id="inf241"><mml:mi>X</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf242"><mml:mi>Z</mml:mi></mml:math></inline-formula> even though <inline-formula><mml:math id="inf243"><mml:mi>Z</mml:mi></mml:math></inline-formula> does not drive <inline-formula><mml:math id="inf244"><mml:mi>X</mml:mi></mml:math></inline-formula>. Note that the extent of synchronization is not always apparent from inspecting equations (e.g. Figure 12 of <xref ref-type="bibr" rid="bib73">Mønster et al., 2017</xref>) or dynamics (row 5 of <xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref>). Third row: <inline-formula><mml:math id="inf245"><mml:mi>X</mml:mi></mml:math></inline-formula> oscillates at a period that is five times the oscillatory period of <inline-formula><mml:math id="inf246"><mml:mi>Z</mml:mi></mml:math></inline-formula>. There is a continuous delay map from <inline-formula><mml:math id="inf247"><mml:mi>X</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf248"><mml:mi>Z</mml:mi></mml:math></inline-formula> even through <inline-formula><mml:math id="inf249"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf250"><mml:mi>Z</mml:mi></mml:math></inline-formula> are causally unrelated. Note that true causality sometimes also induces oscillations where the period of one variable is an integer multiple of the period of another (e.g. in <xref ref-type="fig" rid="fig4">Figure 4</xref>, the period of <inline-formula><mml:math id="inf251"><mml:mi>Z</mml:mi></mml:math></inline-formula> is three times the period of <inline-formula><mml:math id="inf252"><mml:mi>X</mml:mi></mml:math></inline-formula>). Bottom row: In the classic chaotic Lorenz attractor, <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> cause one another, but we do not see a continuous map from the delay space of <inline-formula><mml:math id="inf255"><mml:mi>Z</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf256"><mml:mi>X</mml:mi></mml:math></inline-formula>. This is because, as mentioned earlier, satisfying the conditions in Takens’ theorem makes a continuous mapping likely but not guaranteed (Appendix 4). Here, <inline-formula><mml:math id="inf257"><mml:mi>Z</mml:mi></mml:math></inline-formula> is an example of this lack of guarantee (<xref ref-type="bibr" rid="bib30">Deyle and Sugihara, 2011</xref>) due to a symmetry in the system (see ‘Background definitions for causation in dynamic systems’ in the supplementary information of <xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-fig5-v2.tif"/></fig><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Illustration of the convergent cross mapping (CCM) procedure for testing whether <inline-formula><mml:math id="inf258"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf259"><mml:mi>Y</mml:mi></mml:math></inline-formula>.</title><p>(<bold>A</bold>) Computing cross map skill. Consider the point <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denoted by the red dot (“actual <inline-formula><mml:math id="inf261"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>” in ①), which we want to predict from <inline-formula><mml:math id="inf262"><mml:mi>Y</mml:mi></mml:math></inline-formula> delay vectors. We first look up the contemporaneous <inline-formula><mml:math id="inf263"><mml:mi>Y</mml:mi></mml:math></inline-formula> delay vector <inline-formula><mml:math id="inf264"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> (②, red dynamics), and identify times within our training data when delay vectors of <inline-formula><mml:math id="inf265"><mml:mi>Y</mml:mi></mml:math></inline-formula> were the most similar (i.e. least Euclidean distance) to our red delay vector (③, blue segments). We then look up their contemporaneous values of <inline-formula><mml:math id="inf266"><mml:mi>X</mml:mi></mml:math></inline-formula> (④, blue crosses), and use their weighted average to predict <inline-formula><mml:math id="inf267"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (⑤, open magenta circle; weights are given as equations S2 and S3 in the supplement of <xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>). We repeat this procedure for many choices of <inline-formula><mml:math id="inf268"><mml:mi>T</mml:mi></mml:math></inline-formula> and calculate the Pearson correlation coefficient between the actual <inline-formula><mml:math id="inf269"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and predicted <inline-formula><mml:math id="inf270"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (⑥). This correlation is called the “cross map skill”. While other measures of cross map skill, such as mean squared error, may also be used (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>), here we follow the convention of <xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>. (<bold>B</bold>) Four criteria for inferring causality from the cross map skill. Data points in (<bold>A</bold>) are marked by dots and connecting lines are visual aids.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-fig6-v2.tif"/></fig></sec><sec id="s4-3"><title>Convergent cross mapping: detecting SSR causal signals from real data</title><p>SSR causal discovery methods require testing for the existence of continuous delay maps between variables. However, testing for continuity in real data is complicated by noise and discrete sampling (<xref ref-type="fig" rid="fig4">Figure 4</xref>, compare panels C and F; see also <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>).</p><p>Several methods have been used to detect SSR causal signals by detecting approximate continuity (<xref ref-type="bibr" rid="bib27">Cummins et al., 2015</xref>) or related properties (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>; <xref ref-type="bibr" rid="bib68">Ma et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Harnack et al., 2017</xref>). The most popular is convergent cross mapping (CCM), which has been applied to nonlinear (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>) or linear deterministic systems (<xref ref-type="bibr" rid="bib8">Barraquand et al., 2020</xref>). CCM is based on a statistic called ‘cross map skill’ that quantifies how well a causer can be predicted from delay vectors of its causee (<xref ref-type="fig" rid="fig6">Figure 6A</xref>), conceptually similar to checking for gradual transitions when shading the causee delay space by causer values (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Four criteria have been proposed to infer causality (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>; <xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>; <xref ref-type="fig" rid="fig6">Figure 6B</xref>): First, the cross map skill must be positive. Second, the cross map skill must be significant according to some surrogate data test. Third, the cross map skill must increase with an increasing amount of training data. Lastly, the cross map skill must be greater when predicting past values of the causer than when predicting future values of the causer (the prediction lag test [<xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref>; <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>] in the right panel of <xref ref-type="fig" rid="fig6">Figure 6B</xref>, but see Appendix 4 for caveats of this test). In practice, many if not most CCM analyses use only a subset of these four criteria (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>; <xref ref-type="bibr" rid="bib12">Brookshire and Weaver, 2015</xref>; <xref ref-type="bibr" rid="bib26">Cramer et al., 2017</xref>; <xref ref-type="bibr" rid="bib113">Wang et al., 2018</xref>). Other approaches to detect various aspects of continuous delay maps have also been proposed (<xref ref-type="bibr" rid="bib68">Ma et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Cummins et al., 2015</xref>; <xref ref-type="bibr" rid="bib45">Harnack et al., 2017</xref>; <xref ref-type="bibr" rid="bib63">Leng et al., 2020</xref>). We do not know of a systematic comparison of these alternatives.</p></sec></sec><sec id="s5"><title>Simulation examples: external drivers and noise jointly influence causal discovery performance</title><p>In this section, we examine how environmental drivers, process noise, and measurement noise can influence the performance of Granger causality and CCM, using computer simulations. We constructed a toy ecological system with a known causal structure, obtained its dynamics (with noise) through simulations, and applied a linear Granger causality test (using the MVGC package of <xref ref-type="bibr" rid="bib6">Barnett and Seth, 2014</xref>) and CCM (using the R language package rEDM) to test how well we could infer causal relationships.</p><p>We simulated a two-species community in which one species (<italic>S</italic><sub>1</sub>) causally influences the other species (<italic>S</italic><sub>2</sub>) but <italic>S</italic><sub>2</sub> has no influence on <italic>S</italic><sub>1</sub> (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Additionally, <italic>S</italic><sub>1</sub> is causally influenced by an unobserved periodic external driver and <italic>S</italic><sub>2</sub> either is (<xref ref-type="fig" rid="fig7">Figure 7D</xref>) or is not (<xref ref-type="fig" rid="fig7">Figure 7E</xref>) causally influenced by its own (also unobserved) periodic external driver. In an ecosystem, external drivers might appear as changes in temperature, light, or water levels, for example. We also added process noise to model the stochastic nature of natural ecosystems and added measurement noise to model measurement uncertainty. Process noise propagates to future times and can result from, for instance, stochastic migration and death (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). In contrast, measurement noise does not propagate over time, and includes instrument noise as well as ecological processes that occur during sampling. Since tests for CCM causality criteria have varied widely (<xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>; <xref ref-type="bibr" rid="bib19">Chang et al., 2017</xref>; <xref ref-type="bibr" rid="bib8">Barraquand et al., 2020</xref>), we tested for CCM criteria using two different procedures (<xref ref-type="fig" rid="fig7">Figure 7</xref> legend and Appendix 5).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Performance of Granger causality and convergent cross mapping in a toy model with noise.</title><p>(<bold>A</bold>) The effect of a time points’s process noise, but not its measurement noise, propagates to subsequent time points. (<bold>B</bold>) We simulated a two-species community. The process noise terms <inline-formula><mml:math id="inf271"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf272"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as well as the measurement noise terms <inline-formula><mml:math id="inf273"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf274"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, are IID normal random variables with a mean of zero and a standard deviation whose value we vary. (<bold>C</bold>) Five possible outcomes of the causal analysis. (<bold>D, E</bold>) Community dynamics and causal analysis outcomes. We varied the level (i.e. standard deviation) of process noise and measurement noise. For Granger causality, we used the MVGC package (Appendix 5). For convergent cross mapping, we used the rEDM package to calculate cross map skill and to construct surrogate data, and custom codes for other tasks (Appendix 5). Each pie chart shows the distribution of inference outcomes from 1,000 independent replicates. Note that the MVGC package does not necessarily flag data corrupted by a problematic level of measurement noise (<xref ref-type="bibr" rid="bib67">Lusch et al., 2016</xref>). In both the main and alternative CCM procedures, criterion 1 (positive ρ) was checked directly and random phase surrogate data were used to test criterion 2 (significance of ρ). Criterion 4 (prediction lag test) was not used, because the test is difficult to interpret for periodic dynamics where cross map skill can oscillate as a function of prediction lag length (<xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref>). The two procedures differ only in how they test criterion 3 (ρ increases with more training data): the main procedure uses bootstrap testing following <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref> while the alternative procedure uses a Kendall’s τ as suggested by <xref ref-type="bibr" rid="bib19">Chang et al., 2017</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-fig7-v2.tif"/></fig><p>Granger causality and CCM can perform well when their respective requirements are met, but both are fairly sensitive to the levels of process and measurement noise (<xref ref-type="fig" rid="fig7">Figure 7D and E</xref>, correct inferences colored as green in pie charts) and to details of the ecosystem (whether or not <italic>S</italic><sub>2</sub> has its own external driver; compare <xref ref-type="fig" rid="fig7">Figure 7D and E</xref>). In both methods, detection of the true causal link is disrupted by either the strongest measurement noise (standard deviation of 1) or the strongest process noise (standard deviation of 8) used here.</p><p>For Granger causality (<xref ref-type="fig" rid="fig7">Figure 7D and E</xref>, left panels), the MVGC package correctly rejects the data as inappropriate in the deterministic setting (lower left corner). When process and/or measurement noise is present, their relative amount is important: As measurement noise increases (from bottom to top), process noise often needs to increase (from left to right) for Granger causality to perform well. Indeed, prior analytical results (<xref ref-type="bibr" rid="bib78">Newbold, 1978</xref>; <xref ref-type="bibr" rid="bib77">Nalatore et al., 2007</xref>) show that measurement noise can induce false positives (e.g. red slices in row 2, column 2) and hide true positives (e.g. grey slices in row 1). Surprisingly, increasing measurement noise can sometimes improve performance (in column 3 of both panels, row two has a larger green slice than row 3).</p><p>To understand the CCM results (<xref ref-type="fig" rid="fig7">Figure 7D and E</xref>, right panels), recall that CCM is designed for deterministic systems, and fails when dynamics of variables are synchronized. When <italic>S</italic><sub>2</sub> has its own external driver (<xref ref-type="fig" rid="fig7">Figure 7D</xref>), there is no synchrony, and CCM performs admirably in the deterministic setting (lower left corner). CCM performs less well when measurement or process noise is introduced. Strikingly, when we remove the external driver of <italic>S</italic><sub>1</sub> (<xref ref-type="fig" rid="fig7">Figure 7E</xref>), CCM performs poorly. This is likely because the two species are now synchronized in the absence of noise (violating the ‘no synchrony’ requirement of CCM). However, adding noise, which removes the synchrony problem, violates the determinism requirement. So CCM is frustrated either way. Note that unlike CCM, Granger causality is less sensitive to the presence of underlying synchrony as long as this synchrony is disrupted by process noise. Additionally, the performance of CCM (<xref ref-type="fig" rid="fig7">Figure 7D and E</xref>, right panels) is sensitive to the test procedure (olive brackets).</p><p>In reality, where a system lies in the spectrum of process versus measurement noise is often unknown, and we are not aware of any method that reliably distinguishes between process noise and measurement noise without knowing the functional form of the system. Furthermore, how might one tell if a time series is stochastic or deterministic so that one can choose between Granger causality versus CCM? One idea is that deterministic processes tend to be more predictable than stochastic processes, at least in the short term (<xref ref-type="bibr" rid="bib48">Hastings et al., 1993</xref>). Indeed, the inventors of CCM have recommended checking whether historical values of a time series can be used to accurately predict future values (<xref ref-type="bibr" rid="bib106">Sugihara and May, 1990</xref>) before applying CCM (i.e. <xref ref-type="bibr" rid="bib21">Clark et al., 2015</xref>). However, practical time series found in nature are most likely somewhere between the extremes of ‘fully deterministic’ (i.e. no measurement or process noise) and ‘fully stochastic’ (i.e. IID). Time series are often partly deterministic due to autocorrelation and partly stochastic due to random fluctuations. Indeed, simulations have found that SSR-based and Granger causality-based methods can both potentially succeed for such systems (<xref ref-type="bibr" rid="bib8">Barraquand et al., 2020</xref>). Future work is needed to flesh out the nuances of when and why methods from these two classes provide similar or different performance (<xref ref-type="bibr" rid="bib8">Barraquand et al., 2020</xref>).</p></sec><sec id="s6"><title>Summary: model-free causality tests are not assumption-free</title><p>We have described three causal discovery approaches for observational time series (<xref ref-type="table" rid="table1">Table 1</xref>). Although the techniques explored in this article have been called model-free and do not depend on prior mechanistic knowledge, they are by no means free from assumptions (<xref ref-type="bibr" rid="bib24">Coenen et al., 2020</xref>). The danger that arises when we replace knowledge-based modeling with model-free inference is that we can replace explicitly stated assumptions with unstated and unscrutinized assumptions. Too frequently, both methodological and applied works fall into this trap. Nevertheless, when assumptions are clearly articulated and shown to be reasonable, model-free causal discovery techniques have the potential to jump-start the discovery process where little mechanistic information is known. Still, experimental follow-up (when possible) remains valuable since any technique that seeks to infer causality from observational measurements will typically require at least some assumptions that are difficult to fully verify.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>A comparison of three statistical causal discovery approaches.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">What does it mean if the method detects a link?</th><th align="left" valign="bottom">Implied causal statement</th><th align="left" valign="bottom">What are some possible failure modes?</th></tr></thead><tbody><tr><td align="left" valign="bottom">Correlation</td><td align="left" valign="bottom"><italic>X</italic> and <italic>Y</italic> are statistically dependent.</td><td align="left" valign="bottom"><italic>X</italic> causes <italic>Y</italic>, <italic>Y</italic> causes <italic>X</italic>, or <italic>Z</italic> causes both.</td><td align="left" valign="bottom">Surrogate null model may make incorrect assumptions about the data-generating process.</td></tr><tr><td align="left" valign="bottom">Granger causality</td><td align="left" valign="bottom">The history of <italic>X</italic> contains unique information that is useful for predicting the future of <italic>Y</italic>.</td><td align="left" valign="bottom"><italic>X</italic> directly causes <italic>Y</italic>.</td><td align="left" valign="bottom">Hidden common cause; infrequent sampling; deterministic system (no process noise); excessive process noise; measurement noise</td></tr><tr><td align="left" valign="bottom">State space reconstruction</td><td align="left" valign="bottom">The delay space of <italic>X</italic> can be used to estimate <italic>Y</italic>.</td><td align="left" valign="bottom"><italic>Y</italic> causes <italic>X.</italic></td><td align="left" valign="bottom">Nonreverting continuous dynamics; synchrony; integer multiple periods; pathological symmetry; measurement or process noise</td></tr></tbody></table></table-wrap><p>We have discussed several failure modes of various causal discovery approaches (<xref ref-type="table" rid="table1">Table 1</xref>). Among these failure modes, measurement noise and nonstationarity have been repeatedly singled out as crucial considerations for real data (<xref ref-type="bibr" rid="bib105">Stokes and Purdon, 2017</xref>; <xref ref-type="bibr" rid="bib7">Barnett et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Munch et al., 2020</xref>). While the deleterious effect of excessive measurement noise is intuitive, the pernicious effect of nonstationarity is not always appreciated. This is perhaps because the stationarity requirement, although ubiquitous, is sometimes hidden in the analysis pipeline. For example, when testing whether cross map skill (or correlation) is significant, surrogate data tests are commonly used (e.g. <xref ref-type="bibr" rid="bib62">Lancaster et al., 2018</xref>), and nearly all of them require stationary data. Granger causality tests also typically require data to be stationary.</p><p>What comes next? We cannot cover all open fronts in data-driven causal discovery from time series, but do note a few directions that we think are important. First, given that practical ecological time series can rarely be shown to satisfy the assumptions of tests with mathematical exactness, we would benefit from a more complete understanding of how well tests for dependence and/or causality tolerate moderate deviations from assumptions. In a different direction, one may sometimes possess not a complete mathematical model, but instead some pieces of a model, such as the knowledge that nutrients influence the growth of organisms according to largely monotonic saturable functions. Techniques that attempt to make use of such partial models have recently obtained intriguing results (<xref ref-type="bibr" rid="bib29">Daniels and Nemenman, 2015</xref>; <xref ref-type="bibr" rid="bib13">Brunton et al., 2016</xref>; <xref ref-type="bibr" rid="bib70">Mangan et al., 2016</xref>), and more would be welcome. Moreover, natural experiments often involve known external perturbations that are random or whose effects are poorly understood. An important question is how inference techniques might best take advantage of such perturbations (<xref ref-type="bibr" rid="bib32">Eaton and Murphy, 2007</xref>; <xref ref-type="bibr" rid="bib88">Rothenhäusler et al., 2015</xref>).</p><p>Perhaps most importantly, how can method developers best communicate their assumptions and caveats to method users who are potentially unfamiliar with technical terms or concepts? One effective strategy is to provide simulation examples of how applying techniques to pathological data may give incorrect results (<xref ref-type="bibr" rid="bib21">Clark et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Brunton et al., 2016</xref>). Video walkthroughs (e.g. <xref ref-type="video" rid="video1">Video 1</xref>; <xref ref-type="bibr" rid="bib14">Brunton et al., 2017</xref>; <xref ref-type="bibr" rid="bib119">Xie and Shou, 2021</xref>) may be another useful way to communicate how a method works as well as method assumptions. Finally, we recommend that editors and reviewers work with authors to ensure that failure modes and caveats are clearly articulated in the main text, along with accessible explanations of any necessary technical terms or concepts.</p></sec></body><back><sec sec-type="additional-information" id="s7"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Funding acquisition, Investigation, Supervision, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s8"><title>Additional files</title><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>Code for simulations.</title></caption><media xlink:href="elife-72518-code1-v2.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></sec><ack id="ack"><title>Acknowledgements</title><p>We are grateful to two reviewers (Bree Cummins and James Boedicker) for superb feedback. We thank David Fredricks and Sujatha Srinivasan (Fred Hutch) for biological discussions that inspired this effort, and members of the Shou group for helpful comments. We consulted Tim Sauer (George Mason University) on topology and SSR and Fang Han (University of Washington) on probability. Kun Zhang (Carnegie Mellon University), Sean Gibbons (Institute for Systems Biology), Nathan Kutz (University of Washington), Peng Ding (UC Berkeley), Chris Barnes (University College London), Bianca De Stavola (University College London), Ricardo Silva (University College London), and Neville Kenneth Kitson (Queen Mary University of London) also generously gave us feedback on various versions of our manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afyouni</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Effective degrees of freedom of the Pearson’s correlation coefficient under autocorrelation</article-title><source>NeuroImage</source><volume>199</volume><fpage>609</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.011</pub-id><pub-id pub-id-type="pmid">31158478</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ai</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>G</given-names></name><name><surname>Liang</surname><given-names>X</given-names></name><name><surname>Xia</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Constructing the microbial association network from large-scale time series data using granger causality</article-title><source>Genes</source><volume>10</volume><elocation-id>E216</elocation-id><pub-id pub-id-type="doi">10.3390/genes10030216</pub-id><pub-id pub-id-type="pmid">30875820</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrzejak</surname><given-names>RG</given-names></name><name><surname>Kraskov</surname><given-names>A</given-names></name><name><surname>Stögbauer</surname><given-names>H</given-names></name><name><surname>Mormann</surname><given-names>F</given-names></name><name><surname>Kreuz</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Bivariate surrogate techniques: necessity, strengths, and caveats</article-title><source>Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics</source><volume>68</volume><elocation-id>066202</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.68.066202</pub-id><pub-id pub-id-type="pmid">14754292</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asefa</surname><given-names>T</given-names></name><name><surname>Kemblowski</surname><given-names>M</given-names></name><name><surname>Lall</surname><given-names>U</given-names></name><name><surname>Urroz</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Support vector machines for nonlinear state space reconstruction: Application to the Great Salt Lake time series</article-title><source>Water Resources Research</source><volume>41</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.1029/2004WR003785</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ay</surname><given-names>N</given-names></name><name><surname>Polani</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Information flows in causal networks</article-title><source>Advances in Complex Systems</source><volume>11</volume><fpage>17</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1142/S0219525908001465</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The MVGC multivariate Granger causality toolbox: a new approach to Granger-causal inference</article-title><source>Journal of Neuroscience Methods</source><volume>223</volume><fpage>50</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.10.018</pub-id><pub-id pub-id-type="pmid">24200508</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Barrett</surname><given-names>AB</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Misunderstandings regarding the application of Granger causality in neuroscience</article-title><source>PNAS</source><volume>115</volume><fpage>E6676</fpage><lpage>E6677</lpage><pub-id pub-id-type="doi">10.1073/pnas.1714497115</pub-id><pub-id pub-id-type="pmid">29991604</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barraquand</surname><given-names>F</given-names></name><name><surname>Picoche</surname><given-names>C</given-names></name><name><surname>Detto</surname><given-names>M</given-names></name><name><surname>Hartig</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Inferring species interactions using Granger causality and convergent cross mapping</article-title><source>Theoretical Ecology</source><volume>14</volume><fpage>87</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1007/s12080-020-00482-7</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baskerville</surname><given-names>EB</given-names></name><name><surname>Cobey</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Does influenza drive absolute humidity?</article-title><source>PNAS</source><volume>114</volume><fpage>E2270</fpage><lpage>E2271</lpage><pub-id pub-id-type="doi">10.1073/pnas.1700369114</pub-id><pub-id pub-id-type="pmid">28298534</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrendt</surname><given-names>S</given-names></name><name><surname>Dimpfl</surname><given-names>T</given-names></name><name><surname>Peter</surname><given-names>FJ</given-names></name><name><surname>Zimmermann</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>RTransferEntropy — Quantifying information flow between different time series using effective transfer entropy</article-title><source>SoftwareX</source><volume>10</volume><elocation-id>100265</elocation-id><pub-id pub-id-type="doi">10.1016/j.softx.2019.100265</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bekiros</surname><given-names>SD</given-names></name><name><surname>Diks</surname><given-names>CGH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The nonlinear dynamic relationship of exchange rates: Parametric and nonparametric causality testing</article-title><source>Journal of Macroeconomics</source><volume>30</volume><fpage>1641</fpage><lpage>1650</lpage><pub-id pub-id-type="doi">10.1016/j.jmacro.2008.04.001</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brookshire</surname><given-names>ENJ</given-names></name><name><surname>Weaver</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Long-term decline in grassland productivity driven by increasing dryness</article-title><source>Nature Communications</source><volume>6</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/ncomms8148</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunton</surname><given-names>SL</given-names></name><name><surname>Proctor</surname><given-names>JL</given-names></name><name><surname>Kutz</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Discovering governing equations from data by sparse identification of nonlinear dynamical systems</article-title><source>PNAS</source><volume>113</volume><fpage>3932</fpage><lpage>3937</lpage><pub-id pub-id-type="doi">10.1073/pnas.1517384113</pub-id><pub-id pub-id-type="pmid">27035946</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunton</surname><given-names>SL</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Proctor</surname><given-names>JL</given-names></name><name><surname>Kaiser</surname><given-names>E</given-names></name><name><surname>Kutz</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Chaos as an intermittently forced linear system</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00030-8</pub-id><pub-id pub-id-type="pmid">28559566</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bucci</surname><given-names>V</given-names></name><name><surname>Tzen</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Simmons</surname><given-names>M</given-names></name><name><surname>Tanoue</surname><given-names>T</given-names></name><name><surname>Bogart</surname><given-names>E</given-names></name><name><surname>Deng</surname><given-names>L</given-names></name><name><surname>Yeliseyev</surname><given-names>V</given-names></name><name><surname>Delaney</surname><given-names>ML</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Olle</surname><given-names>B</given-names></name><name><surname>Stein</surname><given-names>RR</given-names></name><name><surname>Honda</surname><given-names>K</given-names></name><name><surname>Bry</surname><given-names>L</given-names></name><name><surname>Gerber</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>MDSINE: Microbial Dynamical Systems INference Engine for microbiome time-series analyses</article-title><source>Genome Biology</source><volume>17</volume><elocation-id>121</elocation-id><pub-id pub-id-type="doi">10.1186/s13059-016-0980-6</pub-id><pub-id pub-id-type="pmid">27259475</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname><given-names>A</given-names></name><name><surname>Diener</surname><given-names>C</given-names></name><name><surname>Baliga</surname><given-names>NS</given-names></name><name><surname>Gibbons</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Use and abuse of correlation analyses in microbial ecology</article-title><source>The ISME Journal</source><volume>13</volume><fpage>2647</fpage><lpage>2655</lpage><pub-id pub-id-type="doi">10.1038/s41396-019-0459-z</pub-id><pub-id pub-id-type="pmid">31253856</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casdagli</surname><given-names>M</given-names></name><name><surname>Eubank</surname><given-names>S</given-names></name><name><surname>Farmer</surname><given-names>JD</given-names></name><name><surname>Gibson</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>State space reconstruction in the presence of noise</article-title><source>Physica D</source><volume>51</volume><fpage>52</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/0167-2789(91)90222-U</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>On the validity of the method of surrogate data</article-title><source>Fields Inst. Commun</source><volume>11</volume><fpage>77</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1090/fic/011/06</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>CW</given-names></name><name><surname>Ushio</surname><given-names>M</given-names></name><name><surname>Hsieh</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Empirical dynamic modeling for beginners</article-title><source>Ecological Research</source><volume>32</volume><fpage>785</fpage><lpage>796</lpage><pub-id pub-id-type="doi">10.1007/s11284-017-1469-9</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chattopadhyay</surname><given-names>A</given-names></name><name><surname>Manupriya</surname><given-names>P</given-names></name><name><surname>Sarkar</surname><given-names>A</given-names></name><name><surname>Balasubramanian</surname><given-names>VN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural network attributions: A causal perspective.</article-title><conf-name>International Conference on Machine Learning.</conf-name><fpage>981</fpage><lpage>990</lpage></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>AT</given-names></name><name><surname>Ye</surname><given-names>H</given-names></name><name><surname>Isbell</surname><given-names>F</given-names></name><name><surname>Deyle</surname><given-names>ER</given-names></name><name><surname>Cowles</surname><given-names>J</given-names></name><name><surname>Tilman</surname><given-names>GD</given-names></name><name><surname>Sugihara</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Spatial convergent cross mapping to detect causal relationships from short time series</article-title><source>Ecology</source><volume>96</volume><fpage>1174</fpage><lpage>1181</lpage><pub-id pub-id-type="doi">10.1890/14-1479.1</pub-id><pub-id pub-id-type="pmid">26236832</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cobey</surname><given-names>S</given-names></name><name><surname>Baskerville</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Limits to causal inference with state-space reconstruction for infectious disease</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0169050</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0169050</pub-id><pub-id pub-id-type="pmid">28030639</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coenen</surname><given-names>AR</given-names></name><name><surname>Weitz</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Limitations of correlation-based inference in complex virus-microbe communities</article-title><source>MSystems</source><volume>3</volume><elocation-id>e00084-18</elocation-id><pub-id pub-id-type="doi">10.1128/mSystems.00084-18</pub-id><pub-id pub-id-type="pmid">30175237</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coenen</surname><given-names>AR</given-names></name><name><surname>Hu</surname><given-names>SK</given-names></name><name><surname>Luo</surname><given-names>E</given-names></name><name><surname>Muratore</surname><given-names>D</given-names></name><name><surname>Weitz</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A primer for microbiome time-series analysis</article-title><source>Frontiers in Genetics</source><volume>11</volume><elocation-id>310</elocation-id><pub-id pub-id-type="doi">10.3389/fgene.2020.00310</pub-id><pub-id pub-id-type="pmid">32373155</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cover</surname><given-names>TM</given-names></name><name><surname>Thomas</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Elements of Information Theory</source><publisher-loc>Wiley</publisher-loc><publisher-name>Elements of Information Theory</publisher-name><pub-id pub-id-type="doi">10.1002/047174882X</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cramer</surname><given-names>KL</given-names></name><name><surname>O’Dea</surname><given-names>A</given-names></name><name><surname>Clark</surname><given-names>TR</given-names></name><name><surname>Zhao</surname><given-names>J-X</given-names></name><name><surname>Norris</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Prehistorical and historical declines in Caribbean coral reef accretion rates driven by loss of parrotfish</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>14160</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14160</pub-id><pub-id pub-id-type="pmid">28112169</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cummins</surname><given-names>B</given-names></name><name><surname>Gedeon</surname><given-names>T</given-names></name><name><surname>Spendlove</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>On the efficacy of state space reconstruction methods in determining causality</article-title><source>SIAM Journal on Applied Dynamical Systems</source><volume>14</volume><fpage>335</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1137/130946344</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cyriaque</surname><given-names>V</given-names></name><name><surname>Géron</surname><given-names>A</given-names></name><name><surname>Billon</surname><given-names>G</given-names></name><name><surname>Nesme</surname><given-names>J</given-names></name><name><surname>Werner</surname><given-names>J</given-names></name><name><surname>Gillan</surname><given-names>DC</given-names></name><name><surname>Sørensen</surname><given-names>SJ</given-names></name><name><surname>Wattiez</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Metal-induced bacterial interactions promote diversity in river-sediment microbiomes</article-title><source>FEMS Microbiology Ecology</source><volume>96</volume><elocation-id>fiaa076</elocation-id><pub-id pub-id-type="doi">10.1093/femsec/fiaa076</pub-id><pub-id pub-id-type="pmid">32343356</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daniels</surname><given-names>BC</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automated adaptive inference of phenomenological dynamical models</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8133</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9133</pub-id><pub-id pub-id-type="pmid">26293508</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deyle</surname><given-names>ER</given-names></name><name><surname>Sugihara</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Generalized theorems for nonlinear state space reconstruction</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e18295</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0018295</pub-id><pub-id pub-id-type="pmid">21483839</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diks</surname><given-names>C</given-names></name><name><surname>Panchenko</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A new statistic and practical guidelines for nonparametric Granger causality testing</article-title><source>Journal of Economic Dynamics and Control</source><volume>30</volume><fpage>1647</fpage><lpage>1669</lpage><pub-id pub-id-type="doi">10.1016/j.jedc.2005.08.008</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Eaton</surname><given-names>D</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Exact bayesian structure learning from uncertain interventions</article-title><conf-name>Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics, PMLR</conf-name><fpage>107</fpage><lpage>114</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebisuzaki</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A method to estimate the statistical significance of a correlation when the data are serially correlated</article-title><source>Journal of Climate</source><volume>10</volume><fpage>2147</fpage><lpage>2153</lpage><pub-id pub-id-type="doi">10.1175/1520-0442(1997)010&lt;2147:AMTETS&gt;2.0.CO;2</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eiler</surname><given-names>A</given-names></name><name><surname>Heinrich</surname><given-names>F</given-names></name><name><surname>Bertilsson</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Coherent dynamics and association networks among lake bacterioplankton taxa</article-title><source>The ISME Journal</source><volume>6</volume><fpage>330</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.1038/ismej.2011.113</pub-id><pub-id pub-id-type="pmid">21881616</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feige</surname><given-names>EL</given-names></name><name><surname>Pearce</surname><given-names>DK</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>The casual causal relationship between money and income: Some caveats for time series analysis</article-title><source>The Review of Economics and Statistics</source><volume>61</volume><elocation-id>521</elocation-id><pub-id pub-id-type="doi">10.2307/1935784</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>CK</given-names></name><name><surname>Mehta</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Identifying keystone species in the human gut microbiome from metagenomic timeseries using sparse linear regression</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e102451</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0102451</pub-id><pub-id pub-id-type="pmid">25054627</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibbons</surname><given-names>SM</given-names></name><name><surname>Kearney</surname><given-names>SM</given-names></name><name><surname>Smillie</surname><given-names>CS</given-names></name><name><surname>Alm</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Two dynamic regimes in the human gut microbiome</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005364</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005364</pub-id><pub-id pub-id-type="pmid">28222117</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glymour</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Spirtes</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Review of causal discovery methods based on graphical models</article-title><source>Frontiers in Genetics</source><volume>10</volume><elocation-id>524</elocation-id><pub-id pub-id-type="doi">10.3389/fgene.2019.00524</pub-id><pub-id pub-id-type="pmid">31214249</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Schoelkopf</surname><given-names>B</given-names></name><name><surname>Tao</surname><given-names>D</given-names></name><name><surname>Geiger</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Discovering temporal causal relations from subsampled data.</article-title><conf-name>International Conference on Machine Learning.</conf-name><fpage>1898</fpage><lpage>1906</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Glymour</surname><given-names>C</given-names></name><name><surname>Tao</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Causal discovery from temporally aggregated time series.</article-title><conf-name>Uncertainty in artificial intelligence: proceedings of the… conference. Conference on Uncertainty in Artificial Intelligence. NIH Public Access.</conf-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granger</surname><given-names>CWJ</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Investigating causal relations by econometric models and cross-spectral methods</article-title><source>Econometrica: Journal of the Econometric Society</source><volume>37</volume><elocation-id>424</elocation-id><pub-id pub-id-type="doi">10.2307/1912791</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granger</surname><given-names>CW</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Testing for causality: a personal viewpoint</article-title><source>Journal of Economic Dynamics and Control</source><volume>2</volume><fpage>329</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1016/0165-1889(80)90069-X</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Greene</surname><given-names>WH</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Econometric Analysis</source><publisher-name>Pearson</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hannisdal</surname><given-names>B</given-names></name><name><surname>Haaga</surname><given-names>KA</given-names></name><name><surname>Reitan</surname><given-names>T</given-names></name><name><surname>Diego</surname><given-names>D</given-names></name><name><surname>Liow</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Common species link global ecosystems to climate change: dynamical evidence in the planktonic fossil record</article-title><source>Proceedings. Biological Sciences</source><volume>284</volume><elocation-id>20170722</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2017.0722</pub-id><pub-id pub-id-type="pmid">28701561</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harnack</surname><given-names>D</given-names></name><name><surname>Laminski</surname><given-names>E</given-names></name><name><surname>Schünemann</surname><given-names>M</given-names></name><name><surname>Pawelzik</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Topological causality in dynamical systems</article-title><source>Physical Review Letters</source><volume>119</volume><elocation-id>098301</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.119.098301</pub-id><pub-id pub-id-type="pmid">28949567</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>SFM</given-names></name><name><surname>Pineda</surname><given-names>JMB</given-names></name><name><surname>Chen</surname><given-names>CC</given-names></name><name><surname>Green</surname><given-names>R</given-names></name><name><surname>Shou</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Disentangling strictly self-serving mutations from win-win mutations in a mutualistic microbial community</article-title><source>eLife</source><volume>8</volume><elocation-id>e44812</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.44812</pub-id><pub-id pub-id-type="pmid">31162049</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>SFM</given-names></name><name><surname>Chen</surname><given-names>CC</given-names></name><name><surname>Shou</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Pleiotropic mutations can rapidly evolve to directly benefit self and cooperative partner despite unfavorable conditions</article-title><source>eLife</source><volume>10</volume><elocation-id>e57838</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57838</pub-id><pub-id pub-id-type="pmid">33501915</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hastings</surname><given-names>A</given-names></name><name><surname>Hom</surname><given-names>CL</given-names></name><name><surname>Ellner</surname><given-names>S</given-names></name><name><surname>Turchin</surname><given-names>P</given-names></name><name><surname>Godfray</surname><given-names>HCJ</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Chaos in ecology: Is mother nature a strange attractor?</article-title><source>Annual Review of Ecology and Systematics</source><volume>24</volume><fpage>1</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1146/annurev.es.24.110193.000245</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hausman</surname><given-names>DM</given-names></name><name><surname>Woodward</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Independence, invariance and the causal markov condition</article-title><source>The British Journal for the Philosophy of Science</source><volume>50</volume><fpage>521</fpage><lpage>583</lpage><pub-id pub-id-type="doi">10.1093/bjps/50.4.521</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>Z</given-names></name><name><surname>Maekawa</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>On spurious Granger causality</article-title><source>Economics Letters</source><volume>73</volume><fpage>307</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/S0165-1765(01)00498-0</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinze-Deml</surname><given-names>C</given-names></name><name><surname>Maathuis</surname><given-names>MH</given-names></name><name><surname>Meinshausen</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Causal Structure Learning</article-title><source>Annual Review of Statistics and Its Application</source><volume>5</volume><fpage>371</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1146/annurev-statistics-031017-100630</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hitchcock</surname><given-names>C</given-names></name><name><surname>Rédei</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020a</year><chapter-title>Reichenbach’s common cause principle</chapter-title><person-group person-group-type="editor"><name><surname>Zalta</surname><given-names>EN</given-names></name></person-group><source>The Stanford Encyclopedia of Philosophy</source><publisher-name>Springer</publisher-name><fpage>259</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1007/978-94-010-0385-8_17</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hitchcock</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020b</year><chapter-title>Causal Models</chapter-title><person-group person-group-type="editor"><name><surname>Zalta</surname><given-names>EN</given-names></name></person-group><source>The Stanford Encyclopedia of Philosophy</source><publisher-name>Columbia University</publisher-name><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hoyer</surname><given-names>PO</given-names></name><name><surname>Janzing</surname><given-names>D</given-names></name><name><surname>Mooij</surname><given-names>JM</given-names></name><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Nonlinear causal discovery with additive noise models</article-title><conf-name>NIPS</conf-name><fpage>689</fpage><lpage>696</lpage></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huke</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Embedding Nonlinear Dynamical Systems: A Guide to Takens’ Theorem</source><publisher-name>MIMS EPrint</publisher-name></element-citation></ref><ref id="bib56"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hyttinen</surname><given-names>A</given-names></name><name><surname>Plis</surname><given-names>S</given-names></name><name><surname>Järvisalo</surname><given-names>M</given-names></name><name><surname>Eberhardt</surname><given-names>F</given-names></name><name><surname>Danks</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Causal discovery from subsampled time series data by constraint optimization.</article-title><conf-name>Conference on Probabilistic Graphical Models.</conf-name><fpage>216</fpage><lpage>227</lpage></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Shimizu</surname><given-names>S</given-names></name><name><surname>Hoyer</surname><given-names>PO</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Estimation of a structural vector autoregression model using non-gaussianity</article-title><source>Journal of Machine Learning Research</source><volume>11</volume><elocation-id>5</elocation-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janzing</surname><given-names>D</given-names></name><name><surname>Balduzzi</surname><given-names>D</given-names></name><name><surname>Grosse-Wentrup</surname><given-names>M</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Quantifying causal influences</article-title><source>The Annals of Statistics</source><volume>41</volume><fpage>2324</fpage><lpage>2358</lpage><pub-id pub-id-type="doi">10.1214/13-AOS1145</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>Z</given-names></name><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Jiao</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Refined nonuniform embedding for coupling detection in multivariate time series</article-title><source>Physical Review. E</source><volume>101</volume><elocation-id>062113</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.101.062113</pub-id><pub-id pub-id-type="pmid">32688603</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koplenig</surname><given-names>A</given-names></name><name><surname>Müller-Spitzer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Population size predicts lexical diversity, but so does the mean sea level --why it is important to correctly account for the structure of temporal data</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0150771</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0150771</pub-id><pub-id pub-id-type="pmid">26938719</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kugiumtzis</surname><given-names>D</given-names></name><name><surname>Lillekjendlie</surname><given-names>B</given-names></name><name><surname>Christophersen</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Chaotic time series. Part I. Estimation of some invariant properties in state-space</article-title><source>Modeling, Identification and Control</source><volume>15</volume><fpage>205</fpage><lpage>224</lpage><pub-id pub-id-type="doi">10.4173/mic.1994.4.1</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lancaster</surname><given-names>G</given-names></name><name><surname>Iatsenko</surname><given-names>D</given-names></name><name><surname>Pidde</surname><given-names>A</given-names></name><name><surname>Ticcinelli</surname><given-names>V</given-names></name><name><surname>Stefanovska</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Surrogate data for hypothesis testing of physical systems</article-title><source>Physics Reports</source><volume>748</volume><fpage>1</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.physrep.2018.06.001</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leng</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>H</given-names></name><name><surname>Kurths</surname><given-names>J</given-names></name><name><surname>Lai</surname><given-names>YC</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Partial cross mapping eliminates indirect causal influences</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-16238-0</pub-id><pub-id pub-id-type="pmid">32457301</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levine</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Competitive interactions in ecosystems</article-title><source>The American Naturalist</source><volume>110</volume><fpage>903</fpage><lpage>910</lpage><pub-id pub-id-type="doi">10.1086/283116</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Xiao</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>D</given-names></name><name><surname>Cai</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Causal inference in nonlinear systems: Granger causality versus time-delayed mutual information</article-title><source>Physical Review. E</source><volume>97</volume><elocation-id>052216</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.97.052216</pub-id><pub-id pub-id-type="pmid">29906860</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>M</given-names></name><name><surname>Kantz</surname><given-names>H</given-names></name><name><surname>Lau</surname><given-names>NC</given-names></name><name><surname>Huang</surname><given-names>W</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Questionable dynamical evidence for causality between galactic cosmic rays and interannual variation in global temperature</article-title><source>PNAS</source><volume>112</volume><fpage>3253</fpage><lpage>3256</lpage><pub-id pub-id-type="doi">10.1073/pnas.1510571112</pub-id><pub-id pub-id-type="pmid">26283407</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lusch</surname><given-names>B</given-names></name><name><surname>Maia</surname><given-names>PD</given-names></name><name><surname>Kutz</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inferring connectivity in networked dynamical systems: Challenges using Granger causality</article-title><source>Physical Review E</source><volume>94</volume><elocation-id>032220</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.94.032220</pub-id><pub-id pub-id-type="pmid">27739857</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>H</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Detecting causality from nonlinear dynamics with short-term time series</article-title><source>Scientific Reports</source><volume>4</volume><elocation-id>7464</elocation-id><pub-id pub-id-type="doi">10.1038/srep07464</pub-id><pub-id pub-id-type="pmid">25501646</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mainali</surname><given-names>K</given-names></name><name><surname>Bewick</surname><given-names>S</given-names></name><name><surname>Vecchio-Pagan</surname><given-names>B</given-names></name><name><surname>Karig</surname><given-names>D</given-names></name><name><surname>Fagan</surname><given-names>WF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Detecting interaction networks in the human microbiome with conditional Granger causality</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007037</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007037</pub-id><pub-id pub-id-type="pmid">31107866</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mangan</surname><given-names>NM</given-names></name><name><surname>Brunton</surname><given-names>SL</given-names></name><name><surname>Proctor</surname><given-names>JL</given-names></name><name><surname>Kutz</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inferring biological networks by sparse identification of nonlinear dynamics</article-title><source>IEEE Transactions on Molecular, Biological and Multi-Scale Communications</source><volume>2</volume><fpage>52</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1109/TMBMC.2016.2633265</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsuzaki</surname><given-names>S-IS</given-names></name><name><surname>Suzuki</surname><given-names>K</given-names></name><name><surname>Kadoya</surname><given-names>T</given-names></name><name><surname>Nakagawa</surname><given-names>M</given-names></name><name><surname>Takamura</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bottom-up linkages between primary production, zooplankton, and fish in a shallow, hypereutrophic lake</article-title><source>Ecology</source><volume>99</volume><fpage>2025</fpage><lpage>2036</lpage><pub-id pub-id-type="doi">10.1002/ecy.2414</pub-id><pub-id pub-id-type="pmid">29884987</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momeni</surname><given-names>B</given-names></name><name><surname>Xie</surname><given-names>L</given-names></name><name><surname>Shou</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Lotka-Volterra pairwise modeling fails to capture diverse pairwise microbial interactions</article-title><source>eLife</source><volume>6</volume><elocation-id>e25051</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.25051</pub-id><pub-id pub-id-type="pmid">28350295</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mønster</surname><given-names>D</given-names></name><name><surname>Fusaroli</surname><given-names>R</given-names></name><name><surname>Tylén</surname><given-names>K</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Sherson</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Causal inference from noisy time-series data — Testing the Convergent Cross-Mapping algorithm in the presence of noise and external influence</article-title><source>Future Generation Computer Systems</source><volume>73</volume><fpage>52</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.future.2016.12.009</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montalto</surname><given-names>A</given-names></name><name><surname>Faes</surname><given-names>L</given-names></name><name><surname>Marinazzo</surname><given-names>D</given-names></name><name><surname>Baumert</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>MuTE: A matlab toolbox to compare established and novel estimators of the multivariate transfer entropy</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e109462</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0109462</pub-id><pub-id pub-id-type="pmid">25314003</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moulder</surname><given-names>RG</given-names></name><name><surname>Boker</surname><given-names>SM</given-names></name><name><surname>Ramseyer</surname><given-names>F</given-names></name><name><surname>Tschacher</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Determining synchrony between behavioral time series: An application of surrogate data generation for establishing falsifiable null-hypotheses</article-title><source>Psychological Methods</source><volume>23</volume><fpage>757</fpage><lpage>773</lpage><pub-id pub-id-type="doi">10.1037/met0000172</pub-id><pub-id pub-id-type="pmid">29595296</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munch</surname><given-names>SB</given-names></name><name><surname>Brias</surname><given-names>A</given-names></name><name><surname>Sugihara</surname><given-names>G</given-names></name><name><surname>Rogers</surname><given-names>TL</given-names></name><name><surname>Griffith</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Frequently asked questions about nonlinear dynamics and empirical dynamic modelling</article-title><source>ICES Journal of Marine Science</source><volume>77</volume><fpage>1463</fpage><lpage>1479</lpage><pub-id pub-id-type="doi">10.1093/icesjms/fsz209</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nalatore</surname><given-names>H</given-names></name><name><surname>Ding</surname><given-names>M</given-names></name><name><surname>Rangarajan</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Mitigating the effects of measurement noise on Granger causality</article-title><source>Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics</source><volume>75</volume><elocation-id>031123</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.75.031123</pub-id><pub-id pub-id-type="pmid">17500684</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newbold</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Feedback induced by measurement errors</article-title><source>International Economic Review</source><volume>19</volume><elocation-id>787</elocation-id><pub-id pub-id-type="doi">10.2307/2526341</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohanian</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>The spurious effects of unit roots on vector autoregressions</article-title><source>Journal of Econometrics</source><volume>39</volume><fpage>251</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1016/0304-4076(88)90058-9</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papana</surname><given-names>A</given-names></name><name><surname>Kugiumtzis</surname><given-names>D</given-names></name><name><surname>Larsson</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Detection of direct causal effects and application to epileptic electroencephalogram analysis</article-title><source>International Journal of Bifurcation and Chaos</source><volume>22</volume><elocation-id>1250222</elocation-id><pub-id pub-id-type="doi">10.1142/S0218127412502227</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papana</surname><given-names>A</given-names></name><name><surname>Kyrtsou</surname><given-names>C</given-names></name><name><surname>Kugiumtzis</surname><given-names>D</given-names></name><name><surname>Diks</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Assessment of resampling methods for causality testing: A note on the US inflation behavior</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0180852</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0180852</pub-id><pub-id pub-id-type="pmid">28708870</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pearl</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Causality</source><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perretti</surname><given-names>CT</given-names></name><name><surname>Munch</surname><given-names>SB</given-names></name><name><surname>Sugihara</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Model-free forecasting outperforms the correct mechanistic model for simulated and experimental data</article-title><source>PNAS</source><volume>110</volume><fpage>5253</fpage><lpage>5257</lpage><pub-id pub-id-type="doi">10.1073/pnas.1216076110</pub-id><pub-id pub-id-type="pmid">23440207</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Mooij</surname><given-names>J</given-names></name><name><surname>Janzing</surname><given-names>D</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Identifiability of causal graphs using functional models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1202.3757">https://arxiv.org/abs/1202.3757</ext-link></element-citation></ref><ref id="bib85"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Janzing</surname><given-names>D</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Elements of Causal Inference: Foundations and Learning Algorithms</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyper</surname><given-names>BJ</given-names></name><name><surname>Peterman</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Comparison of methods to account for autocorrelation in correlation analyses of fish data</article-title><source>Canadian Journal of Fisheries and Aquatic Sciences</source><volume>55</volume><fpage>2127</fpage><lpage>2140</lpage><pub-id pub-id-type="doi">10.1139/f98-104</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenfeld</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sex-dependent differences in voluntary physical activity</article-title><source>Journal of Neuroscience Research</source><volume>95</volume><fpage>279</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1002/jnr.23896</pub-id><pub-id pub-id-type="pmid">27870424</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rothenhäusler</surname><given-names>D</given-names></name><name><surname>Heinze</surname><given-names>C</given-names></name><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Meinshausen</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Backshift: Learning Causal Cyclic Graphs from Unknown Shift Interventions</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1506.02494">https://arxiv.org/abs/1506.02494</ext-link></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roux</surname><given-names>F</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name><name><surname>Aru</surname><given-names>J</given-names></name><name><surname>Uhlhaas</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The phase of thalamic alpha activity modulates cortical gamma-band activity: evidence from resting-state MEG recordings</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>17827</fpage><lpage>17835</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5778-12.2013</pub-id><pub-id pub-id-type="pmid">24198372</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruan</surname><given-names>Q</given-names></name><name><surname>Dutta</surname><given-names>D</given-names></name><name><surname>Schwalbach</surname><given-names>MS</given-names></name><name><surname>Steele</surname><given-names>JA</given-names></name><name><surname>Fuhrman</surname><given-names>JA</given-names></name><name><surname>Sun</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Local similarity analysis reveals unique associations among marine bacterioplankton species and environmental factors</article-title><source>Bioinformatics</source><volume>22</volume><fpage>2532</fpage><lpage>2538</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btl417</pub-id><pub-id pub-id-type="pmid">16882654</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Runge</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information</article-title><conf-name>International Conference on Artificial Intelligence and Statistics</conf-name><fpage>938</fpage><lpage>947</lpage></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Runge</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Causal network reconstruction from time series: From theoretical assumptions to practical estimation</article-title><source>Chaos</source><volume>28</volume><elocation-id>075310</elocation-id><pub-id pub-id-type="doi">10.1063/1.5025050</pub-id><pub-id pub-id-type="pmid">30070533</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Runge</surname><given-names>J</given-names></name><name><surname>Bathiany</surname><given-names>S</given-names></name><name><surname>Bollt</surname><given-names>E</given-names></name><name><surname>Camps-Valls</surname><given-names>G</given-names></name><name><surname>Coumou</surname><given-names>D</given-names></name><name><surname>Deyle</surname><given-names>E</given-names></name><name><surname>Glymour</surname><given-names>C</given-names></name><name><surname>Kretschmer</surname><given-names>M</given-names></name><name><surname>Mahecha</surname><given-names>MD</given-names></name><name><surname>Muñoz-Marí</surname><given-names>J</given-names></name><name><surname>van Nes</surname><given-names>EH</given-names></name><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Quax</surname><given-names>R</given-names></name><name><surname>Reichstein</surname><given-names>M</given-names></name><name><surname>Scheffer</surname><given-names>M</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Spirtes</surname><given-names>P</given-names></name><name><surname>Sugihara</surname><given-names>G</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Zscheischler</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Inferring causation from time series in Earth system sciences</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2553</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10105-3</pub-id><pub-id pub-id-type="pmid">31201306</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Runge</surname><given-names>J</given-names></name><name><surname>Nowack</surname><given-names>P</given-names></name><name><surname>Kretschmer</surname><given-names>M</given-names></name><name><surname>Flaxman</surname><given-names>S</given-names></name><name><surname>Sejdinovic</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Detecting and quantifying causal associations in large nonlinear time series datasets</article-title><source>Science Advances</source><volume>5</volume><elocation-id>eaau4996</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aau4996</pub-id><pub-id pub-id-type="pmid">31807692</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanchez-Romero</surname><given-names>R</given-names></name><name><surname>Ramsey</surname><given-names>JD</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Glymour</surname><given-names>MRK</given-names></name><name><surname>Huang</surname><given-names>B</given-names></name><name><surname>Glymour</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Estimating feedforward and feedback effective connections from fMRI time series: Assessments of statistical methods</article-title><source>Network Neuroscience</source><volume>3</volume><fpage>274</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1162/netn_a_00061</pub-id><pub-id pub-id-type="pmid">30793083</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sauer</surname><given-names>T</given-names></name><name><surname>Yorke</surname><given-names>JA</given-names></name><name><surname>Casdagli</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Embedology</article-title><source>Journal of Statistical Physics</source><volume>65</volume><fpage>579</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1007/BF01053745</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaller</surname><given-names>M</given-names></name><name><surname>Hofer</surname><given-names>MK</given-names></name><name><surname>Beall</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Evidence that an ebola outbreak influenced voting preferences, even after controlling (mindfully) for autocorrelation: Reply to Tiokhin and Hruschka (2017)</article-title><source>Psychological Science</source><volume>28</volume><fpage>1361</fpage><lpage>1363</lpage><pub-id pub-id-type="doi">10.1177/0956797617718183</pub-id><pub-id pub-id-type="pmid">28708035</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiber</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Measuring information transfer</article-title><source>Physical Review Letters</source><volume>85</volume><fpage>461</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.85.461</pub-id><pub-id pub-id-type="pmid">10991308</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiber</surname><given-names>T</given-names></name><name><surname>Schmitz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Surrogate time series</article-title><source>Physica D</source><volume>142</volume><fpage>346</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1016/S0167-2789(00)00043-9</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Seabold</surname><given-names>S</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><conf-name>Statsmodels: Econometric and Statistical Modeling with Python</conf-name><article-title>Python in Science Conference.</article-title><conf-loc>Austin, Texas</conf-loc><pub-id pub-id-type="doi">10.25080/Majora-92bf1922-011</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shade</surname><given-names>A</given-names></name><name><surname>McManus</surname><given-names>PS</given-names></name><name><surname>Handelsman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Unexpected diversity during community succession in the apple flower microbiome</article-title><source>MBio</source><volume>4</volume><elocation-id>e00602-12</elocation-id><pub-id pub-id-type="doi">10.1128/mBio.00602-12</pub-id><pub-id pub-id-type="pmid">23443006</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shorten</surname><given-names>DP</given-names></name><name><surname>Spinney</surname><given-names>RE</given-names></name><name><surname>Lizier</surname><given-names>JT</given-names></name><name><surname>Marinazzo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Estimating transfer entropy in continuous time between neural spike trains or other event-based data</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008054</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008054</pub-id><pub-id pub-id-type="pmid">33872296</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spirtes</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Causal discovery and inference: concepts and recent methodological advances</article-title><source>Applied Informatics</source><volume>3</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1186/s40535-016-0018-x</pub-id><pub-id pub-id-type="pmid">27195202</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>RR</given-names></name><name><surname>Bucci</surname><given-names>V</given-names></name><name><surname>Toussaint</surname><given-names>NC</given-names></name><name><surname>Buffie</surname><given-names>CG</given-names></name><name><surname>Rätsch</surname><given-names>G</given-names></name><name><surname>Pamer</surname><given-names>EG</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Xavier</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Ecological modeling from time-series inference: insight into dynamics and stability of intestinal microbiota</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003388</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003388</pub-id><pub-id pub-id-type="pmid">24348232</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>PA</given-names></name><name><surname>Purdon</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A study of problems encountered in Granger causality analysis from a neuroscience perspective</article-title><source>PNAS</source><volume>114</volume><fpage>E7063</fpage><lpage>E7072</lpage><pub-id pub-id-type="doi">10.1073/pnas.1704663114</pub-id><pub-id pub-id-type="pmid">28778996</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugihara</surname><given-names>G</given-names></name><name><surname>May</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Nonlinear forecasting as a way of distinguishing chaos from measurement error in time series</article-title><source>Nature</source><volume>344</volume><fpage>734</fpage><lpage>741</lpage><pub-id pub-id-type="doi">10.1038/344734a0</pub-id><pub-id pub-id-type="pmid">2330029</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugihara</surname><given-names>G</given-names></name><name><surname>May</surname><given-names>R</given-names></name><name><surname>Ye</surname><given-names>H</given-names></name><name><surname>Hsieh</surname><given-names>C</given-names></name><name><surname>Deyle</surname><given-names>E</given-names></name><name><surname>Fogarty</surname><given-names>M</given-names></name><name><surname>Munch</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Detecting causality in complex ecosystems</article-title><source>Science</source><volume>338</volume><fpage>496</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1126/science.1227079</pub-id><pub-id pub-id-type="pmid">22997134</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Takens</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1980">1980</year><chapter-title>Detecting strange attractors in turbulence</chapter-title><person-group person-group-type="editor"><name><surname>Rand</surname><given-names>D</given-names></name><name><surname>Young</surname><given-names>LS</given-names></name></person-group><source>Dynamical Systems and Turbulence, Warwick</source><publisher-name>Springer</publisher-name><fpage>366</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1007/BFb0091903</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thiel</surname><given-names>M</given-names></name><name><surname>Romano</surname><given-names>MC</given-names></name><name><surname>Kurths</surname><given-names>J</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Twin surrogates to test for complex synchronisation</article-title><source>Europhysics Letters</source><volume>75</volume><fpage>535</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1209/epl/i2006-10147-0</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tiokhin</surname><given-names>L</given-names></name><name><surname>Hruschka</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>No evidence that an ebola outbreak influenced voting preferences in the 2014 elections after controlling for time-series autocorrelation: A commentary on Beall, Hofer, and Schaller (2016)</article-title><source>Psychological Science</source><volume>28</volume><fpage>1358</fpage><lpage>1360</lpage><pub-id pub-id-type="doi">10.1177/0956797616680396</pub-id><pub-id pub-id-type="pmid">28548899</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toda</surname><given-names>HY</given-names></name><name><surname>Phillips</surname><given-names>PCB</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>The spurious effect of unit roots on vector autoregressions</article-title><source>Journal of Econometrics</source><volume>59</volume><fpage>229</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/0304-4076(93)90024-Y</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vicente</surname><given-names>R</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Lindner</surname><given-names>M</given-names></name><name><surname>Pipa</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Transfer entropy--a model-free measure of effective connectivity for the neurosciences</article-title><source>Journal of Computational Neuroscience</source><volume>30</volume><fpage>45</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1007/s10827-010-0262-3</pub-id><pub-id pub-id-type="pmid">20706781</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>De Maeyer</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Duan</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Detecting the causal effect of soil moisture on precipitation using convergent cross mapping</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>12171</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-30669-2</pub-id><pub-id pub-id-type="pmid">30111861</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Yoshimura</surname><given-names>C</given-names></name><name><surname>Allam</surname><given-names>A</given-names></name><name><surname>Kimura</surname><given-names>F</given-names></name><name><surname>Honma</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causality analysis and prediction of 2-methylisoborneol production in a reservoir using empirical dynamic modeling</article-title><source>Water Research</source><volume>163</volume><elocation-id>114864</elocation-id><pub-id pub-id-type="doi">10.1016/j.watres.2019.114864</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>S</given-names></name><name><surname>Van Treuren</surname><given-names>W</given-names></name><name><surname>Lozupone</surname><given-names>C</given-names></name><name><surname>Faust</surname><given-names>K</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Deng</surname><given-names>Y</given-names></name><name><surname>Xia</surname><given-names>LC</given-names></name><name><surname>Xu</surname><given-names>ZZ</given-names></name><name><surname>Ursell</surname><given-names>L</given-names></name><name><surname>Alm</surname><given-names>EJ</given-names></name><name><surname>Birmingham</surname><given-names>A</given-names></name><name><surname>Cram</surname><given-names>JA</given-names></name><name><surname>Fuhrman</surname><given-names>JA</given-names></name><name><surname>Raes</surname><given-names>J</given-names></name><name><surname>Sun</surname><given-names>F</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Correlation detection strategies in microbial data sets vary widely in sensitivity and precision</article-title><source>The ISME Journal</source><volume>10</volume><fpage>1669</fpage><lpage>1681</lpage><pub-id pub-id-type="doi">10.1038/ismej.2015.235</pub-id><pub-id pub-id-type="pmid">26905627</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wollstadt</surname><given-names>P</given-names></name><name><surname>Lizier</surname><given-names>J</given-names></name><name><surname>Vicente</surname><given-names>R</given-names></name><name><surname>Finn</surname><given-names>C</given-names></name><name><surname>Martinez-Zarzuela</surname><given-names>M</given-names></name><name><surname>Mediano</surname><given-names>P</given-names></name><name><surname>Novelli</surname><given-names>L</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>IDTxl: The Information Dynamics Toolkit xl: a Python package for the efficient analysis of multivariate information dynamics in networks</article-title><source>Journal of Open Source Software</source><volume>4</volume><elocation-id>1081</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01081</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Woodward</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Causation and manipulability</chapter-title><person-group person-group-type="editor"><name><surname>Zalta</surname><given-names>EN</given-names></name></person-group><source>In The Stanford Encyclopedia of Philosophy</source><publisher-name>Stanford University</publisher-name><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wootton</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Indirect effects in complex ecosystems: recent progress and future challenges</article-title><source>Journal of Sea Research</source><volume>48</volume><fpage>157</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/S1385-1101(02)00149-1</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>L</given-names></name><name><surname>Shou</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Steering ecological-evolutionary dynamics to improve artificial selection of microbial communities</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6799</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26647-4</pub-id><pub-id pub-id-type="pmid">34815384</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>H</given-names></name><name><surname>Deyle</surname><given-names>ER</given-names></name><name><surname>Gilarranz</surname><given-names>LJ</given-names></name><name><surname>Sugihara</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinguishing time-delayed causal interactions using convergent cross mapping</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>14750</elocation-id><pub-id pub-id-type="doi">10.1038/srep14750</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Spirtes</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Detection of unfaithfulness and robust causal inference</article-title><source>Minds and Machines</source><volume>18</volume><fpage>239</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1007/s11023-008-9096-4</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s9"><title>Random variables and their relationships</title><sec sec-type="appendix" id="s9-1"><title>Dependence between random variables and between vectors of random variables</title><p>The concepts of dependence and independence between random variables are central to many statistical methods, including those that concern causality. A random variable is a variable whose values or experimental measurements depend on outcomes of a random phenomenon and follow a particular probability distribution. Reichenbach’s common cause principle states that if <inline-formula><mml:math id="inf275"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf276"><mml:mi>Y</mml:mi></mml:math></inline-formula> are random variables with a statistical dependence (such as a nonzero covariance), then one or more of three statements is true: <inline-formula><mml:math id="inf277"><mml:mi>X</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf278"><mml:mi>Y</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf279"><mml:mi>Y</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf280"><mml:mi>X</mml:mi></mml:math></inline-formula>, or a third variable <inline-formula><mml:math id="inf281"><mml:mi>Z</mml:mi></mml:math></inline-formula> causes both <inline-formula><mml:math id="inf282"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf283"><mml:mi>Y</mml:mi></mml:math></inline-formula>. The common cause principle cannot be proven from the axioms of probability; rather, the principle is itself a fundamental assumption that supports much of the modern statistical theory of causality (Section 1.4.2 of <xref ref-type="bibr" rid="bib82">Pearl, 2000</xref>).</p><p>As an example, consider the size and length of a bacterial cell. If a larger cell tends to be longer, then cell volume and cell length covary and are thus dependent. A mathematical definition of dependence (and its opposite, independence) is presented in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Joint distribution, marginal distributions, and dependence between two random variables.</title><p>(<bold>A</bold>) A scatterplot of data associated with random variables <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents a ‘joint distribution’ (black). Histograms for data associated with <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and for data associated with <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represent ‘marginal distributions’ (green). Strictly speaking, joint and marginal distributions must be normalized so that probabilities (here represented as ‘counts’) sum to 1. Graphically, marginal distributions are projections of the joint distribution on the axes. Two random variables are identically distributed if their marginal distributions are identical. (<bold>B</bold>) Independence between two random variables. Gray box: a mathematical definition of independence, where ‘<inline-formula><mml:math id="inf288"><mml:mi>P</mml:mi></mml:math></inline-formula>‘ means probability. Two random variables are dependent if and only if they are not independent. Visually, if two random variables are independent, then different values of one random variable will not change our knowledge about another random variable. In (<bold>i</bold>), <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> increases as  increases (so that different  values imply different expectations about <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), and thus, <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are not independent (i.e. they are dependent). In (<bold>ii</bold>), <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>  and <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>  are independent. One might argue that when <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> values become extreme, <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> values tend to land in the middle. However, this is a visual artifact caused by fewer data points at the more extreme <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> values. If we had plotted histograms of <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> at various <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> values, we would see that <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is always normally distributed with the same mean and variance. (<bold>iii</bold>) Indeed, when we plotted the difference between the observed probability <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the probability expected from <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> being independent <inline-formula><mml:math id="inf304"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, (<bold>ii</bold>) showed a near-zero difference (blue), while (<bold>i</bold>) showed deviation from zero (red). This is consistent with <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> being independent in (<bold>ii</bold>) but not in (<bold>i</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app1-fig1-v2.tif"/></fig><p>Dependence can be readily generalized from the definition in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> to become a property between two vectors of random variables. (Note that a time series can be viewed as a vector of random variables.) For example, suppose that we measure two variables <inline-formula><mml:math id="inf307"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf308"><mml:mi>Y</mml:mi></mml:math></inline-formula> over two days. Our (very short) time series are then <inline-formula><mml:math id="inf309"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf310"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> where the subscript index denotes the day of measurement. Similar to <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>, we would say that our two time series are independent if<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>for all choices of <inline-formula><mml:math id="inf311"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s9-2"><title>When are two random variables independent and identically distributed (IID)?</title><p>Many statistical techniques require repeated measurements that can be modeled as independent and identically distributed (IID) random variables, and passing non-IID data (such as time series) into such techniques can lead to spurious results (e.g. <xref ref-type="fig" rid="fig2">Figure 2</xref>; see also <xref ref-type="bibr" rid="bib60">Koplenig and Müller-Spitzer, 2016</xref>). Random variables are IID if they have the same probability distribution and are independent (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). In <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref> we give examples of pairs of random variables that are (or are not) identically distributed, and that are (or are not) independent. Note that two dependent random variables can be linearly correlated (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>, 3rd column), or not (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>, 4th column).</p><p>Random sampling from a population with replacement is one way to produce “IID data” (which we use as a shorthand for “data which can be modeled as IID random variables”). For example, repeatedly rolling a standard die can be thought of as randomly sampling from the set <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with replacement: if the first trial registers 1, then the second trial can register one as well. Otherwise, if sampling was done <italic>without</italic> replacement, then the second trial must not register 1, which means that the outcome of the second trial would depend on the outcome of the first trial.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Examples of random variables that are identically distributed or not identically distributed, and independent or not independent.</title><p>In the top row, <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are identically distributed (projections of the scatter plot on both axes would have the same shape, as in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). Note that in the top row of the rightmost column, the scatter plot is not symmetric along the diagonal line, yet projections on both axes yield identical marginal distributions: three segments of equal densities. Thus, the two random variables are identically distributed. In the bottom row, <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are not identically distributed. In the leftmost two columns, the two random variables are independent (for more details about independence, see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>). In the last three columns, the two random variables are dependent: different <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> values alter our knowledge of <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app1-fig2-v2.tif"/></fig></sec><sec sec-type="appendix" id="s9-3"><title>A sample drawn from a mixed population can still be IID, as long as sample members are chosen randomly and independently</title><p>Since the IID concept is so central to statistical analysis, we wish to further clarify one conceptual difficulty that may arise. To set the stage, suppose that a scientist measures the levels of voluntary physical activity in a collection of mice that includes both males and females. Also suppose that female mice tend to be more physically active than male mice (<xref ref-type="bibr" rid="bib87">Rosenfeld, 2017</xref>). Since this dataset now contains measurements from both the less active males and the more active females, we might naively think that these data cannot be IID.</p><p>In fact, such a dataset still might be IID, but this depends on how the scientist chooses which mice to measure. To illustrate this fact, consider the highly simplified scenario in which only two mice are assayed for physical activity. Let <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> be random variables that describe the activity levels of these two mice. We consider three different ways that the scientist might select which mice to assay. Only one of these ways will result in an IID dataset.</p><p>First, suppose that the scientist chooses to measure <italic>X</italic><sub>1</sub> from a male mouse and <italic>X</italic><sub>2</sub> from a female mouse. In this case, to see whether <italic>X</italic><sub>1</sub> and <italic>X</italic><sub>2</sub> are IID, we can use the same visualization strategy as in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>. That is, we imagine many possible ‘parallel universes’, each with a different possible two-mouse dataset (left panel of <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>). This allows us to visualize the joint distribution of <italic>X</italic><sub>1</sub> and <italic>X</italic><sub>2</sub>. We can then see that <italic>X</italic><sub>1</sub> and <italic>X</italic><sub>2</sub> are independent, but not identically distributed.</p><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Measurements taken from a mixed population may still be IID, as long as sampling is independent and random.</title><p>Consider a study in which physical activity is measured from a mixed population of low-activity male mice and high-activity female mice. For simplicity, suppose that the study uses only two mice. To see whether this could be an IID dataset, we imagine drawing many possible versions of that sample, and ask whether our first measurement <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and second measurement <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are identically distributed and independent. We could collect this sample in three different ways (3 sets of charts). On the left, we take our first measurement <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from a male and second measurement <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from a female. In this case, our two measurements are independent, but not identically distributed, and thus not IID. In the middle, we choose one male and one female per sample, but choose the first measurement randomly from a male or female. Now, our measurements are identically distributed but not independent (so also not IID). On the right, the sex of each measurement is randomly and independently chosen so that, for example, a sample might have two measurements from the same sex. In this case our sample is an IID dataset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app1-fig3-v2.tif"/></fig><p>Second, suppose that the scientist again selects exactly one mouse of each sex, but randomizes the order so that both <italic>X</italic><sub>1</sub> and <italic>X</italic><sub>2</sub> have an equal chance of being measured from a male or female mouse (middle panel of <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>). We can now see that <italic>X</italic><sub>1</sub> and <italic>X</italic><sub>2</sub> are identically distributed, but not independent.</p><p>Lastly, suppose that the scientist selects mice randomly, and without any information about whether a mouse is male or female. In this case, the two-mouse sample might be all male, all female, or have one of each. Once again we plot the joint distribution of <italic>X</italic><sub>1</sub> and <italic>X</italic><sub>2</sub> by imagining their values across many different parallel universes (right panel of <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>). We then see that that <italic>X</italic><sub>1</sub> and <italic>X</italic><sub>2</sub> are finally independent identically distributed. Overall, a set of measurements can be IID even if they are taken from a mixed population, as long as they are sampled randomly from among different subpopulations.</p></sec><sec sec-type="appendix" id="s9-4"><title>Independence and statistical conditioning</title><p>Here, we first restate the concept of independence in terms of statistical conditioning, and then introduce the related concept of conditional independence.</p><p>It is intuitive that two variables are independent if knowledge of one variable tells us nothing about the other. The statistical notion of independence captures this intuition: Random variables <inline-formula><mml:math id="inf325"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf326"><mml:mi>Y</mml:mi></mml:math></inline-formula> are independent if the conditional distribution of <inline-formula><mml:math id="inf327"><mml:mi>X</mml:mi></mml:math></inline-formula> given <inline-formula><mml:math id="inf328"><mml:mi>Y</mml:mi></mml:math></inline-formula> is always equal to the marginal distribution of <inline-formula><mml:math id="inf329"><mml:mi>X</mml:mi></mml:math></inline-formula>. For discrete random variables, this condition can be written<disp-formula id="equ4"><label>(3)</label><mml:math id="m4"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>or equivalently written <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf331"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf332"><mml:mi>y</mml:mi></mml:math></inline-formula>. For continuous random variables, independence can be written in terms of probability density functions as <inline-formula><mml:math id="inf333"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> or equivalently, <inline-formula><mml:math id="inf334"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf335"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the conditional density of <inline-formula><mml:math id="inf336"><mml:mi>X</mml:mi></mml:math></inline-formula> given <inline-formula><mml:math id="inf337"><mml:mi>Y</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf338"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the joint density of <inline-formula><mml:math id="inf339"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf340"><mml:mi>Y</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf341"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf342"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the marginal densities of <inline-formula><mml:math id="inf343"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf344"><mml:mi>Y</mml:mi></mml:math></inline-formula>, respectively.</p><p>The statement “<inline-formula><mml:math id="inf345"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf346"><mml:mi>Y</mml:mi></mml:math></inline-formula> are conditionally independent given <inline-formula><mml:math id="inf347"><mml:mi>Z</mml:mi></mml:math></inline-formula>” intuitively means that <inline-formula><mml:math id="inf348"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf349"><mml:mi>Y</mml:mi></mml:math></inline-formula> are independent when we only analyze outcomes where <inline-formula><mml:math id="inf350"><mml:mi>Z</mml:mi></mml:math></inline-formula> has a certain value. For discrete random variables, this condition is written <inline-formula><mml:math id="inf351"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, or equivalently, <inline-formula><mml:math id="inf352"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for all <inline-formula><mml:math id="inf353"><mml:mi>x</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf354"><mml:mi>y</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf355"><mml:mi>z</mml:mi></mml:math></inline-formula>. For continuous random variables, we have a similar formulation except that the probability <inline-formula><mml:math id="inf356"><mml:mi>P</mml:mi></mml:math></inline-formula> is replaced by the probability density <inline-formula><mml:math id="inf357"><mml:mi>f</mml:mi></mml:math></inline-formula> (i.e. <inline-formula><mml:math id="inf358"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf359"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula>). If <inline-formula><mml:math id="inf360"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf361"><mml:mi>Y</mml:mi></mml:math></inline-formula> are not conditionally independent given <inline-formula><mml:math id="inf362"><mml:mi>Z</mml:mi></mml:math></inline-formula>, then <inline-formula><mml:math id="inf363"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf364"><mml:mi>Y</mml:mi></mml:math></inline-formula> are conditionally dependent given <inline-formula><mml:math id="inf365"><mml:mi>Z</mml:mi></mml:math></inline-formula>.</p><p>One could be forgiven for worrying about the feasibility of testing for dependence between long time series. This is because as a time series grows longer, the amount of data needed to get a sense of its probability distribution would seem to grow extremely rapidly. Thus, when <inline-formula><mml:math id="inf366"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf367"><mml:mi>Y</mml:mi></mml:math></inline-formula> are vectors that represent long time series, estimating the distributions in <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> seems unrealistic. However, establishing that two time series are dependent only requires that we show that the distributions on the left and right sides of <xref ref-type="disp-formula" rid="equ4">Equation 3</xref> differ. Showing that two distributions differ can be much easier than actually estimating those distributions. For instance, if we know that the averages of two univariate distributions are different, then we immediately know that the two distributions are not the same, even if we know nothing about their shapes. Indeed, <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref> demonstrates a way to test for dependence between time series with only a moderate number of replicates, and without any assumptions about the shapes of the distributions. Additionally, surrogate data methods can be used to test for dependence with only one replicate of each time series, as discussed in the main text.</p></sec><sec sec-type="appendix" id="s9-5"><title>When multiple trials exist, the significance of a correlation between time series can be assessed by swapping time series among trials</title><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>When multiple identical and independent trials are available, the significance of a correlation between time series within a trial can be assessed by comparing it to correlations between trials.</title><p>(<bold>A</bold>) A thought experiment in which yeast and bacteria are grown in the same test tube, but follow independent dynamics. We imagine collecting growth curves from 25 independent replicate trials. (<bold>B</bold>) Correlations within and between trials. The Pearson correlation coefficient between yeast and bacteria growth curves from trial one is a seemingly impressive ∼0.98 (pink dot). But does this result really indicate that the two growth curves are dependent? To answer this question, notice that the yeast curves from other trials are similarly highly correlated to the bacteria curve from trial 1, even though they all come from independent trials (grey dots). Therefore, the pink dot cannot be used as evidence that the yeast and bacteria growth are dependent. If the within-trial correlation (pink dot) were stronger than, for instance, 95% of the between-trial correlations (grey dots), we would have evidence of dependence.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app1-fig4-v2.tif"/></fig></sec></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s10"><title>Causal discovery with directed acyclic graphs</title><sec sec-type="appendix" id="s10-1"><title>Discovering causal relationships and their associated directed acyclic graphs (DAGs)</title><p>Many theoretical results and data-driven methods for causal analysis begin by representing causal relationships as a directed acyclic graph (DAG). That is, one makes a graph by represeting random variables as nodes and by drawing a directed edge from each direct cause (or parent) to its causee (or child), as in <xref ref-type="fig" rid="fig1">Figure 1B</xref>; additionally, the graph is acyclic, meaning that that it does not contain any directed paths from any variable back to itself. The acyclicity condition is often required for nice theoretical properties and ease of analysis (<xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref>). Additionally, when data are temporal, a particular node in the graph commonly refers to a particular variable measured at a particular time (e.g. chapter 10 of <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>). If we follow this convention and note that causation cannot flow backward in time, and if we additionally exclude instantaneous causation, then our causal graph will be acyclic, even for systems with feedback (<xref ref-type="fig" rid="app2fig4">Appendix 2—figure 4</xref>).</p><p>DAGs are useful visual tools in their own right, but for many purposes we need to be more mathematically precise about what we mean when we draw an edge from one variable to another. Thus, often one interprets a causal DAG as corresponding to a set of equations with the following two conditions: First, each variable can be written as a function of (only) the variable’s direct causers and a random process noise term unique to the variable. Models that satisfy this condition are called structural equation models (SEMs) (<xref ref-type="bibr" rid="bib53">Hitchcock, 2020b</xref>). Second, all process noise terms are (jointly) independent of one another. SEMs that satisfy this second condition are called Markovian and have a useful property called the ‘causal Markov condition’ (<xref ref-type="bibr" rid="bib82">Pearl, 2000</xref>). (Some authors [<xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>], but not all [<xref ref-type="bibr" rid="bib53">Hitchcock, 2020b</xref>], require that all SEMs be Markovian by definition.) The causal Markov condition, along with the related ‘causal faithfulness condition’ are key assumptions that allow one to connect statistical structure to causal structure and infer aspects of causal structure from data, even in observational settings.</p><p>The causal Markov condition states that if there is no path from <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf369"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in a DAG (i.e. we cannot go from <inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf371"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> by following a sequence of edges in the forward direction), then <inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are conditionally independent given <inline-formula><mml:math id="inf374"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>'s parents (<xref ref-type="bibr" rid="bib82">Pearl, 2000</xref>; <xref ref-type="bibr" rid="bib121">Zhang and Spirtes, 2008</xref>). In this context <inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> can be either a variable or a set of variables. As an example, consider the boxed DAG in <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2B</xref>. Here, <inline-formula><mml:math id="inf376"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> share the common cause <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Each variable depends on its parents, and on its own process noise term. Although <inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are dependent, the causal Markov condition expresses the intuitive idea that if we were to control for <inline-formula><mml:math id="inf381"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, then <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> would become independent. Note that if <inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> does not have any parents, then the statement “<inline-formula><mml:math id="inf385"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are conditionally independent given <inline-formula><mml:math id="inf387"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>'s parents” reduces to “<inline-formula><mml:math id="inf388"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf389"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are independent”.</p><p>The causal faithfulness condition is, like the Markov condition, very useful in causal discovery and often quite reasonable. However, faithfulness is more difficult to state precisely and concisely without first introducing technical notation such as ‘<inline-formula><mml:math id="inf390"><mml:mi>d</mml:mi></mml:math></inline-formula>-separation’ (as in definition 6.33 of <xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>). We attempt to give the gist of the idea here and direct readers to other sources (<xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>; <xref ref-type="bibr" rid="bib121">Zhang and Spirtes, 2008</xref>) for more precise definitions. The causal faithfulness condition is a kind of converse to the causal Markov condition. Recall that the causal Markov condition requires certain conditional (or unconditional) independence relationships based on the causal graph structure. Let us call any other independence relationships (i.e. those not required directly or indirectly by the causal Markov condition) ‘extra’ independence relationships. The (joint) probability distribution of random variables is causally faithful to the DAG if no ‘extra’ independence relationships exist (<xref ref-type="bibr" rid="bib53">Hitchcock, 2020b</xref>). An imprecise shorthand for the faithfulness condition is ‘independence relationships indicate the absence of certain causal relationships’. The faithfulness condition can be violated when two effects precisely cancel each other (<xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref>).</p><p>Existing observational causal discovery methods for the IID (e.g. non-temporal) setting are diverse. Such methods can differ greatly in the assumptions they make (e.g. whether there are hidden variables or ‘unknown shift interventions’), the reasoning they employ, and the resolution of causal detail they provide (e.g. a unique causal graph versus a set of several plausible graphs) (<xref ref-type="bibr" rid="bib51">Heinze-Deml et al., 2018</xref>). We will briefly introduce two classes of causal methods: (1) constraint-based search and (2) structural equation models (SEMs) with assumptions about the functional forms of equations (<xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref>). However, these two classes, while illustrative of different modes of causal discovery, are far from an exhaustive list (<xref ref-type="bibr" rid="bib51">Heinze-Deml et al., 2018</xref>).</p><p>Constraint-based search uses independence and dependence relationships (and their conditional counterparts) to narrow down the scope of possible causal graphs without exhaustively checking all possibilities (which can be enormous in number even for a handful of variables). The PC algorithm (named after its inventors Peter Spirtes and Clark Glymour) and the fast causal inference algorithm are examples of constraint-based search methods (<xref ref-type="bibr" rid="bib38">Glymour et al., 2019</xref>). However, constraint-based methods often find multiple graphs that are consistent with the same set of data (e.g. <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2Biii</xref>, see legend; see also <xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref>).</p><p>Functional form-based (or SEM-based) approaches to causal discovery begin by assuming a particular functional form for causal relationships, and then assess a given causal hypothesis by inspecting the joint distribution between a potential causer and its potential causee (<xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref>). These methods rely on the fact that in a Markovian SEM, each variable has a noise term that is independent of the noise terms of all other variables (<xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>). Given two dependent variables with no hidden common causes, one can use an appropriate regression to estimate values of a proposed causee based on the proposed causer (<xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref>). If the residuals of this regression are independent from the proposed causer, then the proposed causal direction is consistent with the data (<xref ref-type="bibr" rid="bib54">Hoyer et al., 2008</xref>). Crucially, theoretical results indicate that for a fairly wide variety of scenarios (e.g. linear non-Gaussian and post-nonlinear models), we can expect the data to be consistent with only one causal direction, thus enabling unambiguous identification of the causal direction (<xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref>). An illustrative graphic example is given in Figure 3 of <xref ref-type="bibr" rid="bib103">Spirtes and Zhang, 2016</xref> and also in Figure 3 of <xref ref-type="bibr" rid="bib38">Glymour et al., 2019</xref>. Similar ideas can be applied to multivariate systems (<xref ref-type="bibr" rid="bib54">Hoyer et al., 2008</xref>; <xref ref-type="bibr" rid="bib84">Peters et al., 2012</xref>).</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Violation of faithfulness condition due to precise cancellation of causal effects.</title><p>Although <inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> has a direct causal effect on <inline-formula><mml:math id="inf392"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we assume here that this is exactly canceled out by an opposing influence via the indirect path of <inline-formula><mml:math id="inf393"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, although the Markov condition does not require that <inline-formula><mml:math id="inf394"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf395"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> be independent, <inline-formula><mml:math id="inf396"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf397"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are actually independent. We thus say that the joint probability distribution of the variables <inline-formula><mml:math id="inf398"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is not faithful to the graph.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app2-fig1-v2.tif"/></fig><fig id="app2fig2" position="float"><label>Appendix 2—figure 2.</label><caption><title>Probability distributions alone can specify causal structure to varying degrees of resolution.</title><p>Consider a system of three and only three random variables <inline-formula><mml:math id="inf399"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf400"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Between each pair of variables, there are three possible unidirectional relationships: causation in one direction, causation in the opposite direction, and no causation. With three pairs of variables and three types of relationships, there are 3<sup>3</sup> = 27 possible graphs. (<bold>A</bold>) Two of these graphs are cyclic, while the rest are DAGs. (<bold>B</bold>) If our system is described by a Markovian and causally faithful SEM, we can infer some aspects of causal structure from probability distributions alone. We demonstrate this by using the dependence relationships between <inline-formula><mml:math id="inf402"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf403"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (blue) to infer causal relationships. (<bold>Bi</bold>): <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf405"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are always independent. <inline-formula><mml:math id="inf406"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf407"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are not causally related. (<bold>Bii</bold>): <inline-formula><mml:math id="inf408"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are dependent, implying that they are causally related. (Recall that in this article, two variables are “causally related” if one causes the other, or they share a common cause.) Furthermore, <inline-formula><mml:math id="inf410"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf411"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are conditionally dependent given <inline-formula><mml:math id="inf412"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For example, in the circled graph, variation in <inline-formula><mml:math id="inf413"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will affect <inline-formula><mml:math id="inf414"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, resulting in dependence between <inline-formula><mml:math id="inf415"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf416"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, even if we control for <inline-formula><mml:math id="inf417"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>Biii</bold>): <inline-formula><mml:math id="inf418"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf419"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are dependent, but are conditionally independent given <inline-formula><mml:math id="inf420"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. There is no direct link between <inline-formula><mml:math id="inf421"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf422"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, but they are causally related. Note that all three graphs are consistent with the following observations: <inline-formula><mml:math id="inf423"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf424"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are dependent and conditionally independent given <inline-formula><mml:math id="inf425"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf426"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf427"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are dependent and conditionally dependent given <inline-formula><mml:math id="inf428"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf429"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf430"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are dependent and conditionally dependent given <inline-formula><mml:math id="inf431"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, we cannot uniquely identify the causal structure from dependence relationships alone. (<bold>Biv</bold>): <inline-formula><mml:math id="inf432"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf433"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are independent, but are conditionally dependent given <inline-formula><mml:math id="inf434"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>; see <xref ref-type="fig" rid="app2fig3">Appendix 2—figure 3</xref> for an example of this scenario. This case corresponds to one and only one possible DAG.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app2-fig2-v2.tif"/></fig><fig id="app2fig3" position="float"><label>Appendix 2—figure 3.</label><caption><title>Selection bias creates the false impression of dependence.</title><p>(<bold>A</bold>) DAG depicting the assumed causal relationship between math scores, writing scores, and admission to a certain college. (<bold>B</bold>) Math and writing scores in a fictitious student population are independent of each other, and take on random values distributed uniformly between 0 and 100. (<bold>C</bold>) A college admits a student if and only if their combined score exceeds 100. It is apparent that when we condition on college admission (by plotting only the scores of admitted students), math and writing scores show a negative association, indicating that they are dependent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app2-fig3-v2.tif"/></fig><fig id="app2fig4" position="float"><label>Appendix 2—figure 4.</label><caption><title>Causal discovery approaches designed for directed acyclic graphs (DAGs) can be applied to time series from systems with feedback.</title><p>(<bold>i</bold>) Consider a mutualistic system where <inline-formula><mml:math id="inf435"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf436"><mml:mi>B</mml:mi></mml:math></inline-formula> represent the population sizes of two species that mutually facilitate each other’s growth. (ii) When the role of time is ignored, the causal graph is cyclic and thus not a DAG. (iii) For time series data where <inline-formula><mml:math id="inf437"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, … represent the population size of <inline-formula><mml:math id="inf438"><mml:mi>A</mml:mi></mml:math></inline-formula> at times <inline-formula><mml:math id="inf439"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi></mml:mrow></mml:math></inline-formula>, the causal graph is no longer cyclic since <inline-formula><mml:math id="inf440"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> causes <inline-formula><mml:math id="inf441"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf442"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> causes <inline-formula><mml:math id="inf443"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and so on. Note that <inline-formula><mml:math id="inf444"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> causes <inline-formula><mml:math id="inf445"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (and similarly <inline-formula><mml:math id="inf446"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> causes <inline-formula><mml:math id="inf447"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). This framework (<xref ref-type="bibr" rid="bib85">Peters et al., 2017</xref>) has helped one of the authors classify mutations in ecological communities with feedback interactions (<xref ref-type="bibr" rid="bib46">Hart et al., 2019</xref>; <xref ref-type="bibr" rid="bib47">Hart et al., 2021</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app2-fig4-v2.tif"/></fig></sec></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s11"><title>Mathematical concepts for stochastic time series</title><sec sec-type="appendix" id="s11-1"><title>Intuition for random phase surrogate data</title><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Intuition for random phase surrogate data methods.</title><p>Random phase surrogate data methods generate <inline-formula><mml:math id="inf448"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> by representing <inline-formula><mml:math id="inf449"><mml:mi>Y</mml:mi></mml:math></inline-formula> as a sum of sine waves (1), randomly shifting the phases of the component sine waves (2), and summing up the shifted sine waves (3).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app3-fig1-v2.tif"/></fig></sec><sec sec-type="appendix" id="s11-2"><title>Stationarity</title><p>Many methods for time series analysis require that data satisfy some stationarity condition, meaning that certain statistical properties of the data must remain constant across time. Two important stationarity conditions are strong stationarity and covariance-stationarity. A stochastic process <inline-formula><mml:math id="inf450"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is called strongly stationary if the joint distribution of any <inline-formula><mml:math id="inf451"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> consecutive values <inline-formula><mml:math id="inf452"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is independent of time <inline-formula><mml:math id="inf453"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>(Definition 20.1 of <xref ref-type="bibr" rid="bib43">Greene, 2012</xref>). A stochastic process <inline-formula><mml:math id="inf454"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is covariance-stationary (or weakly stationary) if: (1) the ensemble mean <inline-formula><mml:math id="inf455"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is finite and does not depend on <inline-formula><mml:math id="inf456"><mml:mi>t</mml:mi></mml:math></inline-formula>; (2) the variance <inline-formula><mml:math id="inf457"><mml:mrow><mml:mi>Var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is finite and does not depend on <inline-formula><mml:math id="inf458"><mml:mi>t</mml:mi></mml:math></inline-formula>; (3) for all choices of <inline-formula><mml:math id="inf459"><mml:mi>h</mml:mi></mml:math></inline-formula>, the covariance <inline-formula><mml:math id="inf460"><mml:mrow><mml:mi>Cov</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is finite and does not depend on <inline-formula><mml:math id="inf461"><mml:mi>t</mml:mi></mml:math></inline-formula> (Definition 20.2 of <xref ref-type="bibr" rid="bib43">Greene, 2012</xref>).</p><p>As an illustrated example of a covariance-stationary process, consider a population whose dynamics are governed by death and stochastic migration (similar to <xref ref-type="fig" rid="fig2">Figure 2C</xref>):<disp-formula id="equ5"><label>(4)</label><mml:math id="m5"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic>X</italic><sub><italic>t</italic></sub> is the population size at time <inline-formula><mml:math id="inf462"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf463"><mml:mi>a</mml:mi></mml:math></inline-formula> is the proportion of the population lost due to death during one time step, <inline-formula><mml:math id="inf464"><mml:mi>c</mml:mi></mml:math></inline-formula> is the average number of individuals migrating into the population during one time step, and <inline-formula><mml:math id="inf465"><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is a random variable with a mean of zero which represents temporal fluctuations in the number of migrants. Suppose that we observed the dynamics of 10 populations governed by <xref ref-type="disp-formula" rid="equ5">Equation 4</xref> such that the populations all have the same parameters, but are independent (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2A</xref>). Then, at each time point <inline-formula><mml:math id="inf466"><mml:mi>t</mml:mi></mml:math></inline-formula>, we will have some distribution of values of <italic>X</italic><sub><italic>t</italic></sub>. In fact, if we have not just 10, but 1,200 replicates, we can see that the distribution of values of <italic>X</italic><sub><italic>t</italic></sub> does not appear to depend on time (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2B</xref>, top). Furthermore, the covariance between <italic>X</italic><sub><italic>t</italic></sub> and <inline-formula><mml:math id="inf467"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> does not appear to depend on time either (<xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2B</xref>, bottom).</p><fig id="app3fig2" position="float"><label>Appendix 3—figure 2.</label><caption><title>Example of a covariance-stationary process.</title><p>(<bold>A</bold>) Ten replicate runs of the stochastic process described in <xref ref-type="disp-formula" rid="equ5">Equation 4</xref> with parameter choices <inline-formula><mml:math id="inf468"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf469"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>. The noise term <inline-formula><mml:math id="inf470"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a normal random variable with mean of zero and standard deviation of 1. To illustrate the behavior of a single replicate, we highlight one representative trajectory in black. (<bold>B</bold>) The distribution of <inline-formula><mml:math id="inf471"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> values is shown for 1200 replicate runs of the same stochastic process as in (<bold>A</bold>). The mean of <inline-formula><mml:math id="inf472"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is given as a solid red line and the mean ± the standard deviation of <inline-formula><mml:math id="inf473"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by dashed blue lines. Bottom: <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is plotted against <inline-formula><mml:math id="inf475"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> for two values of <inline-formula><mml:math id="inf476"><mml:mi>t</mml:mi></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app3-fig2-v2.tif"/></fig><p>Although it is common to talk about a time series being stationary or nonstationary, this is technically a slight abuse of language. Just as the mean and variance are properties of a random variable (and not of any single data point obtained from that random variable), stationarity is a property of a stochastic process (and not of any single time series produced by that process). This fact is illustrated by comparing the middle and bottom rows of <xref ref-type="fig" rid="app3fig3">Appendix 3—figure 3</xref>. If we examine any one time series from the middle or bottom rows (e.g. the black curves in each), we see that they have essentially the same dynamics (i.e. they are sine waves with the same frequency). However, the process shown in the middle row is covariance-stationary (as shown below), whereas the process shown in the bottom row is not since its mean changes over time.</p><fig id="app3fig3" position="float"><label>Appendix 3—figure 3.</label><caption><title>Whether a stochastic process is stationary depends on its entire ensemble of time series.</title><p>The top panel shows IID standard normal noise. The middle and bottom panels both show sinusoidal curves. Although an individual time series from the middle panel looks similar to that from the bottom panel, only the middle panel shows a covariance-stationary process.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app3-fig3-v2.tif"/></fig><p>To see that the middle row of <xref ref-type="fig" rid="app3fig3">Appendix 3—figure 3</xref> shows a covariance-stationary process, we can show that the mean, variance, and covariance of the process are independent of time:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>h</mml:mi><mml:mn>3</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s11-3"><title>Deterministic processes with many variables may appear stochastic</title><p>A deterministic time series from a system with many variables can be approximated as stochastic. This is illustrated below in <xref ref-type="fig" rid="app3fig4">Appendix 3—figure 4</xref>. When we track the trajectory of a particle in a box with 99 other particles (<xref ref-type="fig" rid="app3fig4">Appendix 3—figure 4</xref> bottom row), the observed trajectory appears random, even though the governing equations of motion are deterministic. In particular, the motion of our particle over each time step can be approximated as having a random component. Note that this flavor of randomness is in general different from the phenomenon called chaos. In chaotic dynamics, each time step needs not be random, but small changes in initial conditions lead to large changes at later times.</p><fig id="app3fig4" position="float"><label>Appendix 3—figure 4.</label><caption><title>A many-variable deterministic system can be approximated as a stochastic system.</title><p>The position of a particle in a system of particles bouncing in a one-dimensional box is plotted over time. In each simulation, particles with radius 0 bounce around in a box with walls of infinite mass placed at positions 0 and 1. Each particle has a mass of 1 and is initialized at a random position between 0 and 1 according to a uniform distribution. Initial velocities are chosen in the following way: The initial velocity of each particle in a box is first randomly chosen from between –1 and 1 according to a uniform distribution. Then, all initial velocities in a given box are multiplied by the same constant to ensure that the total kinetic energy of each box is 0.5. Kinetic energy is conserved throughout the simulation. The simulation then follows the particles as they experience momentum-conserving collisions with one another and with the walls.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app3-fig4-v2.tif"/></fig></sec></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s12"><title>State space reconstruction</title><sec sec-type="appendix" id="s12-1"><title>Difficulty of evaluating the continuity or smoothness of a function with finite or noisy data</title><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>Continuity, smoothness, and the difficulty of evaluating the continuity or smoothness of a function with finite or noisy data.</title><p>(<bold>A</bold>) <inline-formula><mml:math id="inf477"><mml:mi>y</mml:mi></mml:math></inline-formula> is not a function of <inline-formula><mml:math id="inf478"><mml:mi>x</mml:mi></mml:math></inline-formula> because a single <inline-formula><mml:math id="inf479"><mml:mi>x</mml:mi></mml:math></inline-formula> value can correspond to more than one <inline-formula><mml:math id="inf480"><mml:mi>y</mml:mi></mml:math></inline-formula> value. Here, when we shade <inline-formula><mml:math id="inf481"><mml:mi>x</mml:mi></mml:math></inline-formula> with the value of <inline-formula><mml:math id="inf482"><mml:mi>y</mml:mi></mml:math></inline-formula>, we randomly choose the upper or the lower <inline-formula><mml:math id="inf483"><mml:mi>y</mml:mi></mml:math></inline-formula> value, leading to bumpy shading. (<bold>B</bold>) <inline-formula><mml:math id="inf484"><mml:mi>y</mml:mi></mml:math></inline-formula> is a discontinuous function of <inline-formula><mml:math id="inf485"><mml:mi>x</mml:mi></mml:math></inline-formula>. This is because at any “breakpoint” (circle) between two adjacent segments, the limit taken from the left side is different from the limit taken from the right side. Shading <inline-formula><mml:math id="inf486"><mml:mi>x</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf487"><mml:mi>y</mml:mi></mml:math></inline-formula> generates a ‘bumpy’ pattern. (<bold>C</bold>) <inline-formula><mml:math id="inf488"><mml:mi>y</mml:mi></mml:math></inline-formula> is a continuous function of <inline-formula><mml:math id="inf489"><mml:mi>x</mml:mi></mml:math></inline-formula>, and shading <inline-formula><mml:math id="inf490"><mml:mi>x</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf491"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> generates a gradual pattern. (<bold>D</bold>) <inline-formula><mml:math id="inf492"><mml:mi>y</mml:mi></mml:math></inline-formula> is a continuous and smooth function of <inline-formula><mml:math id="inf493"><mml:mi>x</mml:mi></mml:math></inline-formula>. Consider any point on the curve (call it <inline-formula><mml:math id="inf494"><mml:mi>p</mml:mi></mml:math></inline-formula>). We can draw a line between <inline-formula><mml:math id="inf495"><mml:mi>p</mml:mi></mml:math></inline-formula> and a neighboring point to the left (<italic>p</italic><sub><italic>L</italic></sub>). We can also draw a line between <inline-formula><mml:math id="inf496"><mml:mi>p</mml:mi></mml:math></inline-formula> and a neighbor to the right (<italic>p</italic><sub><italic>R</italic></sub>). If the slopes of these two lines become equal as <italic>p</italic><sub><italic>L</italic></sub> and <italic>p</italic><sub><italic>R</italic></sub> become infinitesimally close to <inline-formula><mml:math id="inf497"><mml:mi>p</mml:mi></mml:math></inline-formula>, then the function is smooth. Although the function in (<bold>C</bold>) is continuous, it is not smooth since at the maximum point, the slope taken from the left-hand side is different from the slope taken from the right-hand side. A smooth function is always continuous. (<bold>E</bold>) With finite and noisy data, shading <inline-formula><mml:math id="inf498"><mml:mi>x</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf499"><mml:mi>y</mml:mi></mml:math></inline-formula> often generates a bumpy pattern. It is unclear whether <inline-formula><mml:math id="inf500"><mml:mi>y</mml:mi></mml:math></inline-formula> is a function of <inline-formula><mml:math id="inf501"><mml:mi>x</mml:mi></mml:math></inline-formula>, and if yes, whether the function is continuous and/or smooth.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app4-fig1-v2.tif"/></fig><sec sec-type="appendix" id="s12-1-1"><title>Considerations for selecting delay vector parameters for SSR</title><p>To construct delay vectors for SSR, one must choose the delay vector length <inline-formula><mml:math id="inf502"><mml:mi>E</mml:mi></mml:math></inline-formula> and the time delay τ. How does one choose <inline-formula><mml:math id="inf503"><mml:mi>E</mml:mi></mml:math></inline-formula> and τ? In general, detecting a continuous delay map requires that the delay vector length <inline-formula><mml:math id="inf504"><mml:mi>E</mml:mi></mml:math></inline-formula> be large enough so that no two parts of the delay space cross. For example, using <inline-formula><mml:math id="inf505"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> (instead of <inline-formula><mml:math id="inf506"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>) to make <xref ref-type="fig" rid="fig4">Figure 4C</xref> would have projected the delay space onto two dimensions. This would introduce line crossings, which would in turn produce artifactual discontinuities in the shading. On the other hand, the amount of data required to perform SSR inference is said to grow with the delay vector length (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>). SSR is less sensitive to τ, although it is possible to mask a continuous delay map by choosing a “bad” τ. For example, consider what would happen to <xref ref-type="fig" rid="fig4">Figure 4C</xref> if we set τ to the period of <inline-formula><mml:math id="inf507"><mml:mi>Z</mml:mi></mml:math></inline-formula>. Since the delay vector is <inline-formula><mml:math id="inf508"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, setting set τ to the period of <inline-formula><mml:math id="inf509"><mml:mi>Z</mml:mi></mml:math></inline-formula> would force all 3 elements of the delay vector to always be equal. In geometric terms, this would compress the delay space onto a line, destroying the continuous delay map. However, bad choices of τ such as this are rare. Various practical methods are available for systematically choosing <inline-formula><mml:math id="inf510"><mml:mi>E</mml:mi></mml:math></inline-formula> and τ, and delay vectors with variable delays (e.g. <inline-formula><mml:math id="inf511"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>) have also been used (<xref ref-type="bibr" rid="bib45">Harnack et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>; <xref ref-type="bibr" rid="bib59">Jia et al., 2020</xref>).</p></sec><sec sec-type="appendix" id="s12-1-2"><title>Historical notes on the basis of SSR</title><p>Takens’ celebrated paper (<xref ref-type="bibr" rid="bib108">Takens, 1980</xref>) was a major theoretical advance that has inspired a variety of data-driven methods for both causality detection and forecasting (e.g. <xref ref-type="bibr" rid="bib83">Perretti et al., 2013</xref>). Theorem 1 of <xref ref-type="bibr" rid="bib108">Takens, 1980</xref> is reproduced below. Here, the term ‘map’ can be used interchangeably with ‘function’: a map from <inline-formula><mml:math id="inf512"><mml:mi>X</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf513"><mml:mi>Y</mml:mi></mml:math></inline-formula> sends each point in <inline-formula><mml:math id="inf514"><mml:mi>X</mml:mi></mml:math></inline-formula> to exactly one point in <inline-formula><mml:math id="inf515"><mml:mi>Y</mml:mi></mml:math></inline-formula>.</p><boxed-text id="box2"><label>Appendix 4—box 1.</label><caption><title>Takens’ theorem.</title></caption><p>Takens’ theorem (theorem 1 of <xref ref-type="bibr" rid="bib108">Takens, 1980</xref>): Let <inline-formula><mml:math id="inf516"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> be a compact manifold of dimension <inline-formula><mml:math id="inf517"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For pairs <inline-formula><mml:math id="inf518"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>ϕ</mml:mi><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> a smooth diffeomorphism and <inline-formula><mml:math id="inf519"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> a smooth function, it is a generic property that the map <inline-formula><mml:math id="inf520"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> defined by<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>is an embedding; by “smooth” we mean at least <inline-formula><mml:math id="inf521"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Authors’ note: A function is in the class “<inline-formula><mml:math id="inf522"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>” if its <italic>k</italic>th derivative is continuous.</p></boxed-text><p>We will attempt to illustrate Takens’ theorem using the example in <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref>. This example consists of a deterministic dynamical system with three variables <inline-formula><mml:math id="inf523"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf524"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf525"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> . To begin, we visualize the state space in <inline-formula><mml:math id="inf526"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> coordinates (<xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2C</xref>), and color the trajectory with time (a colored clock-like ring in <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2D</xref> to highlight the periodic nature of system dynamics). This trajectory is the manifold <inline-formula><mml:math id="inf527"><mml:mi>M</mml:mi></mml:math></inline-formula> in Takens’ theorem and is 1-dimensional (<inline-formula><mml:math id="inf528"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) since it is a loop. Takens’ theorem then asks us to choose a function <inline-formula><mml:math id="inf529"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, which we will define as a function that “points into the past”. Specifically, <inline-formula><mml:math id="inf530"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is a function that maps a point <inline-formula><mml:math id="inf531"><mml:mi>p</mml:mi></mml:math></inline-formula> on the manifold <inline-formula><mml:math id="inf532"><mml:mi>M</mml:mi></mml:math></inline-formula> at the current time <inline-formula><mml:math id="inf533"><mml:mi>t</mml:mi></mml:math></inline-formula> to the point <inline-formula><mml:math id="inf534"><mml:mi>q</mml:mi></mml:math></inline-formula> at a previous time <inline-formula><mml:math id="inf535"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula>. Similarly, <inline-formula><mml:math id="inf536"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> would apply <inline-formula><mml:math id="inf537"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> twice and map <inline-formula><mml:math id="inf538"><mml:mi>p</mml:mi></mml:math></inline-formula> at the current time <inline-formula><mml:math id="inf539"><mml:mi>t</mml:mi></mml:math></inline-formula> to the point <inline-formula><mml:math id="inf540"><mml:mi>r</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf541"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (olive in <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2B</xref>), and <inline-formula><mml:math id="inf542"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> would map point <inline-formula><mml:math id="inf543"><mml:mi>q</mml:mi></mml:math></inline-formula> at the past time <inline-formula><mml:math id="inf544"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula> to the point <inline-formula><mml:math id="inf545"><mml:mi>p</mml:mi></mml:math></inline-formula> at the current time <inline-formula><mml:math id="inf546"><mml:mi>t</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2C</xref>). Note that <inline-formula><mml:math id="inf547"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf548"><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, which are ‘discrete-time’ mappings, are distinct from the differential equations that generated the system dynamics (which are continuous in time; <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2A</xref>). The term ‘diffeomorphism’ in the theorem means that both this function <inline-formula><mml:math id="inf549"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> and its inverse function (the map from past to present) are smooth (<xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>).</p><fig id="app4fig2" position="float"><label>Appendix 4—figure 2.</label><caption><title>Illustration of Takens’ theorem.</title><p>(<bold>A</bold>) We consider a 3-variable toy system in which <inline-formula><mml:math id="inf550"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf551"><mml:mi>Y</mml:mi></mml:math></inline-formula> causally influence <inline-formula><mml:math id="inf552"><mml:mi>Z</mml:mi></mml:math></inline-formula>, but <inline-formula><mml:math id="inf553"><mml:mi>Z</mml:mi></mml:math></inline-formula> does not influence <inline-formula><mml:math id="inf554"><mml:mi>X</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf555"><mml:mi>Y</mml:mi></mml:math></inline-formula>. (<bold>B</bold>) Time series of the three variables. (<bold>C</bold>) We can represent the time series as the state space manifold <inline-formula><mml:math id="inf556"><mml:mi>M</mml:mi></mml:math></inline-formula>. Takens’ theorem requires that <inline-formula><mml:math id="inf557"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> (a function that maps a point <inline-formula><mml:math id="inf558"><mml:mi>p</mml:mi></mml:math></inline-formula> at current time <inline-formula><mml:math id="inf559"><mml:mi>t</mml:mi></mml:math></inline-formula> to the point <inline-formula><mml:math id="inf560"><mml:mi>q</mml:mi></mml:math></inline-formula> at a previous time <inline-formula><mml:math id="inf561"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula>) and its inverse <inline-formula><mml:math id="inf562"><mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> (from past to current time) are both smooth (<inline-formula><mml:math id="inf563"><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>: the first and second derivatives of the function exist and are continuous everywhere on the manifold). (<bold>D</bold>) To mark time progression, we color each point along the trajectory with its corresponding time value where time is represented as a color ring similar to a clock to reflect the periodic nature of system dynamics. (<bold>E, G, I</bold>) Shading the state space manifold with three different observation functions (<inline-formula><mml:math id="inf564"><mml:mi>f</mml:mi></mml:math></inline-formula> in Takens’ theorem) as indicated above each plot. (<bold>F, H, J</bold>) Delay space based on the observation function, colored with time. The map <inline-formula><mml:math id="inf565"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula> in Takens’ theorem maps, for example, point <inline-formula><mml:math id="inf566"><mml:mi>p</mml:mi></mml:math></inline-formula> in panel E to point <inline-formula><mml:math id="inf567"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>Z</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>Z</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> in panel F. The theorem tells us that for ‘generic’ choices of <inline-formula><mml:math id="inf568"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf569"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the function <inline-formula><mml:math id="inf570"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula> and its inverse <inline-formula><mml:math id="inf571"><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> provide a continuous mapping from the state space manifold to the delay space manifold, and vice versa. In this example <inline-formula><mml:math id="inf572"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>3.6</mml:mn></mml:mrow></mml:math></inline-formula>. In panel J, multiple colors in a region are due to one period wrapping around the delay space multiple times (inset), but the color shading transition is continuous (similar to panel F).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app4-fig2-v2.tif"/></fig><p>The next symbol in the theorem is <inline-formula><mml:math id="inf573"><mml:mi>f</mml:mi></mml:math></inline-formula>, which can be viewed as an ‘observation’ function that maps each point on the manifold to a single real number (e.g. in <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2E</xref>, <inline-formula><mml:math id="inf574"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>Z</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> so that <inline-formula><mml:math id="inf575"><mml:mi>f</mml:mi></mml:math></inline-formula> simply returns the <inline-formula><mml:math id="inf576"><mml:mi>Z</mml:mi></mml:math></inline-formula> coordinate of point <inline-formula><mml:math id="inf577"><mml:mi>p</mml:mi></mml:math></inline-formula>). Takens’ theorem then asks us to consider a function <inline-formula><mml:math id="inf578"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula> that maps a point <inline-formula><mml:math id="inf579"><mml:mi>p</mml:mi></mml:math></inline-formula> at time <inline-formula><mml:math id="inf580"><mml:mi>t</mml:mi></mml:math></inline-formula> on our state space manifold (<xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2E</xref>) to a point in the ‘delay space’. The coordinates of the delay space are given by applying the observation function to point <inline-formula><mml:math id="inf581"><mml:mi>p</mml:mi></mml:math></inline-formula> (which occurs at at time <inline-formula><mml:math id="inf582"><mml:mi>t</mml:mi></mml:math></inline-formula>), point <inline-formula><mml:math id="inf583"><mml:mi>q</mml:mi></mml:math></inline-formula> (at time <inline-formula><mml:math id="inf584"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula>), and point <inline-formula><mml:math id="inf585"><mml:mi>r</mml:mi></mml:math></inline-formula> (at time <inline-formula><mml:math id="inf586"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), so that a single point in the delay space is <inline-formula><mml:math id="inf587"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>Z</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>Z</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>Z</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> with respect to a particular time <inline-formula><mml:math id="inf588"><mml:mi>t</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2F</xref>). This choice of delay space comes from three earlier choices: First, we consider delayed values of <inline-formula><mml:math id="inf589"><mml:mi>Z</mml:mi></mml:math></inline-formula> since <inline-formula><mml:math id="inf590"><mml:mi>Z</mml:mi></mml:math></inline-formula> is what our observation function <inline-formula><mml:math id="inf591"><mml:mi>f</mml:mi></mml:math></inline-formula> returns; second, since <inline-formula><mml:math id="inf592"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (the dimension of the manifold) is 1, the delay space should be of dimension 3 (<inline-formula><mml:math id="inf593"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) per Takens’ theorem; third, the delay length of τ comes from our diffeomorphism <inline-formula><mml:math id="inf594"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>. Then, Takens’ theorem states that for ‘most’ (technically, ‘generic’) choices of <inline-formula><mml:math id="inf595"><mml:mi>f</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf596"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, the function <inline-formula><mml:math id="inf597"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula> is an embedding. This means that <inline-formula><mml:math id="inf598"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula> is diffeomorphic to its image. That is, the curve in delay space will map smoothly (and thus continuously) to the manifold <inline-formula><mml:math id="inf599"><mml:mi>M</mml:mi></mml:math></inline-formula> and vice versa (<xref ref-type="bibr" rid="bib55">Huke, 2006</xref>).</p><p>Indeed from <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2C-F</xref>, we can see that for our choice of observation function (i.e. <inline-formula><mml:math id="inf600"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>), there is a map from the state space manifold <inline-formula><mml:math id="inf601"><mml:mi>M</mml:mi></mml:math></inline-formula> to the delay space manifold. This is because each dot in the state space manifold corresponds to a single time color (i.e. a point within a period), and each time color corresponds to a single dot in the delay space manifold, and thus, each point in the state space manifold corresponds to a single point in the delay space manifold. Moreover, <inline-formula><mml:math id="inf602"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula> is continuous because the maps from state space to the time ring and from the time ring to delay space are both continuous. Similarly, we can see that the inverse of <inline-formula><mml:math id="inf603"><mml:mi mathvariant="normal">Φ</mml:mi></mml:math></inline-formula>, which points from the delay space manifold to the state space manifold is also a continuous map, as guaranteed (generically) by Takens’ theorem.</p><p>Strikingly, if the observation function is <inline-formula><mml:math id="inf604"><mml:mi>Y</mml:mi></mml:math></inline-formula>, we will no longer have a continuous map from the delay space trajectory (now of <inline-formula><mml:math id="inf605"><mml:mi>Y</mml:mi></mml:math></inline-formula>) to the state space trajectory. This is visualized as ‘bumpy coloring’ in <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2H</xref>. In fact, we cannot even map the delay space to the time ring or the state space: <inline-formula><mml:math id="inf606"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf607"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> occupy the same point in the <inline-formula><mml:math id="inf608"><mml:mi>Y</mml:mi></mml:math></inline-formula> delay space, yet correspond to different times within a period (<xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2B</xref>) and thus they correspond to different locations in the state space. Takens’ theorem took care of this pathology using the word ‘generic’. That is, <inline-formula><mml:math id="inf609"><mml:mi>Y</mml:mi></mml:math></inline-formula> is not considered a generic observation function here. On the other hand, if we use an observation function based on 95% <inline-formula><mml:math id="inf610"><mml:mi>Y</mml:mi></mml:math></inline-formula> mixed with 5% <inline-formula><mml:math id="inf611"><mml:mi>Z</mml:mi></mml:math></inline-formula>, we get an embedding from the state space to the delay space (<xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2I–J</xref>). This is essentially what the term ‘generic’ means in the context of topology: Although some observation functions do not give you an embedding, these ‘bad’ observation functions can be tweaked just a tiny bit to become ‘good’ ones. Similarly, some choices of <inline-formula><mml:math id="inf612"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> do not work (i.e. <inline-formula><mml:math id="inf613"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> for this system), but these are exceptions (see Theorem 2 of <xref ref-type="bibr" rid="bib55">Huke, 2006</xref> for what makes a <inline-formula><mml:math id="inf614"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> “generic”).</p><p>At a conceptual level, SSR causality inference can be performed by shading the delay space of one variable (the potential causee) with the contemporaneous value of another variable (the potential causer), and inferring a causal link if this shading is continuous. In the example of <xref ref-type="fig" rid="fig4">Figure 4</xref> in the main text, shading the delay space of <inline-formula><mml:math id="inf615"><mml:mi>Z</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf616"><mml:mi>Y</mml:mi></mml:math></inline-formula> generates a continuous pattern, consistent with <inline-formula><mml:math id="inf617"><mml:mi>Y</mml:mi></mml:math></inline-formula> causing <inline-formula><mml:math id="inf618"><mml:mi>Z</mml:mi></mml:math></inline-formula>. On the other hand, shading the delay space of <inline-formula><mml:math id="inf619"><mml:mi>Z</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf620"><mml:mi>W</mml:mi></mml:math></inline-formula> shows a bumpy pattern, consistent with <inline-formula><mml:math id="inf621"><mml:mi>W</mml:mi></mml:math></inline-formula> not causing <inline-formula><mml:math id="inf622"><mml:mi>Z</mml:mi></mml:math></inline-formula>.</p><p>Sauer and colleagues (<xref ref-type="bibr" rid="bib96">Sauer et al., 1991</xref>) later extended Takens’ theorem by proving a similar result that is in some ways more general. Theorem 2.5 in <xref ref-type="bibr" rid="bib96">Sauer et al., 1991</xref> is distinct but related to Takens’ theorem, and applies to cases that Takens’ theorem does not cover, such as fractal spaces. Additionally, (<xref ref-type="bibr" rid="bib96">Sauer et al., 1991</xref>) replaces the concept of ‘generic’ functions with a different notion (‘prevalence’), which is closer to a probabilistic statement. <xref ref-type="bibr" rid="bib27">Cummins et al., 2015</xref> then formally connected these results to a notion of potentially causal coupling between dynamic variables.</p></sec><sec sec-type="appendix" id="s12-1-3"><title>Nonreverting continuous dynamics: criteria and effects on convergent cross mapping</title><p>We first illustrate ‘nonreverting continuous dynamics’, which reflects a nonstationarity pathology for SSR techniques. We then discuss how nonreverting continuous dynamics affects CCM.</p><p>We use the phrase ‘nonreverting continuous dynamics’ to describe the following idea: If the <inline-formula><mml:math id="inf623"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space maps continuously to <inline-formula><mml:math id="inf624"><mml:mi>t</mml:mi></mml:math></inline-formula> (‘nonreverting’ <inline-formula><mml:math id="inf625"><mml:mi>X</mml:mi></mml:math></inline-formula>), and <inline-formula><mml:math id="inf626"><mml:mi>t</mml:mi></mml:math></inline-formula> maps continuously to <inline-formula><mml:math id="inf627"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (‘continuous’ <inline-formula><mml:math id="inf628"><mml:mi>Y</mml:mi></mml:math></inline-formula>), then the <inline-formula><mml:math id="inf629"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space will map continuously to <inline-formula><mml:math id="inf630"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, even if <inline-formula><mml:math id="inf631"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf632"><mml:mi>Y</mml:mi></mml:math></inline-formula> are causally unrelated (<xref ref-type="fig" rid="app4fig3">Appendix 4—figure 3A</xref>). <xref ref-type="fig" rid="app4fig3">Appendix 4—figure 3B</xref> illustrates this with three scenarios in which <inline-formula><mml:math id="inf633"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf634"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are causally <italic>independent</italic> time series. In the top row, the <inline-formula><mml:math id="inf635"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space maps continuously to <inline-formula><mml:math id="inf636"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf637"><mml:mi>t</mml:mi></mml:math></inline-formula> maps continuously to <inline-formula><mml:math id="inf638"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> , so we get nonreverting continuous dynamics and a continuous delay map from <inline-formula><mml:math id="inf639"><mml:mi>X</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf640"><mml:mi>Y</mml:mi></mml:math></inline-formula> even though <inline-formula><mml:math id="inf641"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf642"><mml:mi>Y</mml:mi></mml:math></inline-formula> are independent. In the middle row, the <inline-formula><mml:math id="inf643"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space maps continuously to <inline-formula><mml:math id="inf644"><mml:mi>t</mml:mi></mml:math></inline-formula>, but <inline-formula><mml:math id="inf645"><mml:mi>t</mml:mi></mml:math></inline-formula> does not map continuously to <inline-formula><mml:math id="inf646"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, so we do not have nonreverting continuous dynamics (i.e. no continuous map from the <inline-formula><mml:math id="inf647"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space to <inline-formula><mml:math id="inf648"><mml:mi>Y</mml:mi></mml:math></inline-formula>). In the bottom row, the <inline-formula><mml:math id="inf649"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space does not map continuously to <inline-formula><mml:math id="inf650"><mml:mi>t</mml:mi></mml:math></inline-formula>. This is because a single delay vector (shown as a cyan dot in the delay space) occurs at multiple times (shown as repeated cyan line segments whose starting and ending points denote the two values of the delay vector), generating a bumpy pattern similar to <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1A</xref>. In this case, even though <inline-formula><mml:math id="inf651"><mml:mi>t</mml:mi></mml:math></inline-formula> maps continuously to <inline-formula><mml:math id="inf652"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we do not have nonreverting continuous dynamics and we do not get a spurious continuous map from the <inline-formula><mml:math id="inf653"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space to <inline-formula><mml:math id="inf654"><mml:mi>Y</mml:mi></mml:math></inline-formula>.</p><fig id="app4fig3" position="float"><label>Appendix 4—figure 3.</label><caption><title>Nonreverting continuous dynamics.</title><p>(<bold>A</bold>) Definition of nonreverting continuous dynamics. We call <inline-formula><mml:math id="inf655"><mml:mi>X</mml:mi></mml:math></inline-formula> nonreverting if the delay space of <inline-formula><mml:math id="inf656"><mml:mi>X</mml:mi></mml:math></inline-formula> maps continuously to <inline-formula><mml:math id="inf657"><mml:mi>t</mml:mi></mml:math></inline-formula> (time). We call <inline-formula><mml:math id="inf658"><mml:mi>Y</mml:mi></mml:math></inline-formula> ‘continuous‘ if <inline-formula><mml:math id="inf659"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a continuous function of <inline-formula><mml:math id="inf660"><mml:mi>t</mml:mi></mml:math></inline-formula>. If <inline-formula><mml:math id="inf661"><mml:mi>X</mml:mi></mml:math></inline-formula> is nonreverting and <inline-formula><mml:math id="inf662"><mml:mi>Y</mml:mi></mml:math></inline-formula> is continuous then we say that the pair of time series <inline-formula><mml:math id="inf663"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> has nonreverting continuous dynamics. (<bold>B</bold>) Examples. In each row, <inline-formula><mml:math id="inf664"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf665"><mml:mi>Y</mml:mi></mml:math></inline-formula> are causally independent. Leftmost column: Dynamics. Each red or blue dot (visible upon zooming in on some of the charts) represents a single time point. Second column: Looking for a continuous map from the delay vectors of <inline-formula><mml:math id="inf666"><mml:mi>X</mml:mi></mml:math></inline-formula> (the <inline-formula><mml:math id="inf667"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space) to <inline-formula><mml:math id="inf668"><mml:mi>t</mml:mi></mml:math></inline-formula>, i.e. nonreverting <inline-formula><mml:math id="inf669"><mml:mi>X</mml:mi></mml:math></inline-formula> dynamics. Third column: Looking for a continuous map from <inline-formula><mml:math id="inf670"><mml:mi>t</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf671"><mml:mi>Y</mml:mi></mml:math></inline-formula> by assessing whether <inline-formula><mml:math id="inf672"><mml:mi>Y</mml:mi></mml:math></inline-formula> at nearby times share similar values. Since the data occur at discrete times, the standard definition of continuity does not naturally apply, so ‘continuous <inline-formula><mml:math id="inf673"><mml:mi>Y</mml:mi></mml:math></inline-formula>’ really means ‘highly autocorrelated’. Fourth and final column: the presence or absence of ‘nonreverting continuous dynamics’. With nonreverting continuous dynamics, there is a continuous map from the <inline-formula><mml:math id="inf674"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space to <inline-formula><mml:math id="inf675"><mml:mi>Y</mml:mi></mml:math></inline-formula>, and thus <inline-formula><mml:math id="inf676"><mml:mi>Y</mml:mi></mml:math></inline-formula> appears to cause <inline-formula><mml:math id="inf677"><mml:mi>X</mml:mi></mml:math></inline-formula> even though <inline-formula><mml:math id="inf678"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf679"><mml:mi>Y</mml:mi></mml:math></inline-formula> are causally independent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app4-fig3-v2.tif"/></fig><p>Nonreverting continuous dynamics interferes with CCM causal discovery. Although one could attempt to mitigate the nonstationarity problem by interspersing training and testing data before quantifying cross map skill (<xref ref-type="bibr" rid="bib66">Luo et al., 2015</xref>; <xref ref-type="fig" rid="app4fig4">Appendix 4—figure 4</xref>, Column 4), we find that this approach leads to false positive errors (<xref ref-type="fig" rid="app4fig4">Appendix 4—figure 4</xref>, bottom row). In contrast, the alternative (not interspersing training and testing data) can lead to false negative errors (<xref ref-type="fig" rid="app4fig4">Appendix 4—figure 4</xref>, third row). Thus, the ability to correctly infer causality with CCM is vastly reduced when data exhibit nonreverting continuous dynamics.</p><fig id="app4fig4" position="float"><label>Appendix 4—figure 4.</label><caption><title>Nonreverting continuous dynamics impair the ability of CCM to correctly infer causality.</title><p>Each row represents a system where <inline-formula><mml:math id="inf680"><mml:mi>Y</mml:mi></mml:math></inline-formula> does or does not causally influence <inline-formula><mml:math id="inf681"><mml:mi>X</mml:mi></mml:math></inline-formula> (Column 1). Column 2: Governing equations. Column 3: Checking for nonreverting continuous dynamics as in <xref ref-type="fig" rid="app4fig3">Appendix 4—figure 3</xref>. The top two rows do not have nonreverting continuous dynamics since there is no continuous map from the delay space of <inline-formula><mml:math id="inf682"><mml:mi>X</mml:mi></mml:math></inline-formula> to time. The bottom two rows have nonreverting continuous dynamics. Columns 4 and 5: Results of CCM where training and testing data are interspersed or when we train on the past and test on future. In the bottom two rows, CCM suffers false negative or false positive errors depending on the analysis details (e.g. whether training and testing data are interspersed).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app4-fig4-v2.tif"/></fig></sec><sec sec-type="appendix" id="s12-1-4"><title>The prediction lag test: intuition and some failure modes</title><p>State space reconstruction methods suffer false positive errors in the presence of synchrony (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>). This occurs when “the dependence of the dynamics of the forced variable on its own state is no longer significant” (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>). Ye et al. proposed a test in an effort to solve this problem (<xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref>). Their procedure relies on finding mappings from the delay vector <inline-formula><mml:math id="inf683"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf684"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf685"><mml:mi>E</mml:mi></mml:math></inline-formula> is the delay vector length, τ is the time lag, and <inline-formula><mml:math id="inf686"><mml:mi>l</mml:mi></mml:math></inline-formula> is a key variable known as the “prediction lag”. They then examine how the cross map skill (<xref ref-type="fig" rid="fig6">Figure 6B</xref>) varies with the prediction lag. According to this technique, if the cross map skill is maximized at a positive prediction lag (<inline-formula><mml:math id="inf687"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), then the putative causality is spurious and arose from, for example, strong unidirectional forcing. The reasoning is that if the causee were to predict the future of the causer, then causation would appear to flow backward in time, which is nonsensical. On the other hand, if the highest quality mapping occurs at a non-positive prediction lag (<inline-formula><mml:math id="inf688"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), then we have further evidence that the detected causality is real and not spurious (<xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref>).</p><p>We find that while this test correctly distinguishes between real and spurious causal signals at some times, at other times it does not. Within each row of <xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref>, we examine a different system and ask whether <inline-formula><mml:math id="inf689"><mml:mi>Y</mml:mi></mml:math></inline-formula> causes <inline-formula><mml:math id="inf690"><mml:mi>X</mml:mi></mml:math></inline-formula> according to: (1) the ground truth model, (2) our visual continuity test, (3) a CCM cross map skill test (without the prediction lag test), and (4) the prediction lag test.</p><p>In rows 1 and 2 of <xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref>, the prediction lag test performs well, overturning the results of the visual continuity and CCM tests when apparent causality is spurious (row 1), and agreeing with the continuity and CCM tests (row 2) when apparent causality is real (modified from <xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref> <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). However in row 3, the prediction lag test dismisses a true causal link as spurious. Moreover, when we apply the prediction lag test to a system with a periodic putative driver (row 4), we find that cross map skill is a periodic function of the prediction lag. While this result is what we would expect mathematically, its causal interpretation is unclear. The fifth row of <xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref> is an extreme case of strong forcing, where <inline-formula><mml:math id="inf691"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a function of <inline-formula><mml:math id="inf692"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but not <inline-formula><mml:math id="inf693"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Here the prediction lag test gives a false positive error. In the bottom row, <inline-formula><mml:math id="inf694"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf695"><mml:mi>Y</mml:mi></mml:math></inline-formula> do not interact, but are both driven by a common cause <inline-formula><mml:math id="inf696"><mml:mi>W</mml:mi></mml:math></inline-formula> with different lags. Specifically, <inline-formula><mml:math id="inf697"><mml:mrow><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> exerts a direct effect on <inline-formula><mml:math id="inf698"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and on <inline-formula><mml:math id="inf699"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, <inline-formula><mml:math id="inf700"><mml:mi>Y</mml:mi></mml:math></inline-formula> receives the same information as <inline-formula><mml:math id="inf701"><mml:mi>X</mml:mi></mml:math></inline-formula>, but at an earlier time, analogous to <xref ref-type="fig" rid="fig3">Figure 3ii</xref>. Consistent with this, delay vectors of <inline-formula><mml:math id="inf702"><mml:mi>X</mml:mi></mml:math></inline-formula> predict past values of <inline-formula><mml:math id="inf703"><mml:mi>Y</mml:mi></mml:math></inline-formula> better than future values of <inline-formula><mml:math id="inf704"><mml:mi>Y</mml:mi></mml:math></inline-formula>. Thus, the prediction lag test produces a false positive error.</p><fig id="app4fig5" position="float"><label>Appendix 4—figure 5.</label><caption><title>Comparison of visual continuity testing, cross map skill testing, and prediction lag testing in causal discovery.</title><p>Each row represents a two-variable or three-variable system where <inline-formula><mml:math id="inf705"><mml:mi>Y</mml:mi></mml:math></inline-formula> does or does not causally influence <inline-formula><mml:math id="inf706"><mml:mi>X</mml:mi></mml:math></inline-formula>. The leftmost column shows the equations and ground truth causality. The second column shows a sample of <inline-formula><mml:math id="inf707"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf708"><mml:mi>Y</mml:mi></mml:math></inline-formula> dynamics. Red and blue dots represent <inline-formula><mml:math id="inf709"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf710"><mml:mi>Y</mml:mi></mml:math></inline-formula> values, respectively; black lines connecting the dots serve as a visual aid. The third column shows visual continuity testing and causal interpretation. We write ‘likely’ in the top row because the map from <inline-formula><mml:math id="inf711"><mml:mi>X</mml:mi></mml:math></inline-formula> delay space to <inline-formula><mml:math id="inf712"><mml:mi>Y</mml:mi></mml:math></inline-formula> appears to have some small bumps on the right side of the plot. The fourth column shows cross map skill testing (without the prediction lag test) and causal interpretation. Black dots show cross map skill. Open purple dots show the 5% chance cutoff at the maximum library size according to random phase surrogate data testing (see Appendix 5), or are placed below the horizontal axis if the 5% chance cutoff is below the plot. In all systems <inline-formula><mml:math id="inf713"><mml:mi>Y</mml:mi></mml:math></inline-formula> appears to cause <inline-formula><mml:math id="inf714"><mml:mi>X</mml:mi></mml:math></inline-formula> according to cross map skill testing since cross map skill is positive, increases with training data size, and is significant according to the surrogate data test. The rightmost column shows the prediction lag test and causal interpretation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app4-fig5-v2.tif"/></fig><p><xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref> applied the prediction lag test to 500 systems with the same form as in the third row of <xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref> but with randomly chosen parameters. They found that within the parameter range they sampled, false negative errors as in <xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref> do occur, but such errors are rare. We repeated the randomized numerical experiment from <xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref> for both the original parameter range of <xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref>; <xref ref-type="fig" rid="app4fig6">Appendix 4—figure 6B</xref>, ‘friendly’ parameter regime and a second parameter range of the same volume in parameter space (<xref ref-type="fig" rid="app4fig6">Appendix 4—figure 6B</xref>, ‘pathological’ parameter regime). In this pathological parameter regime, false negative errors occur in the overwhelming majority of cases.</p><fig id="app4fig6" position="float"><label>Appendix 4—figure 6.</label><caption><title>Parameters within a ‘pathological’ regime almost always cause the prediction lag test to erroneously reject a true causal link.</title><p>(<bold>A</bold>) System equations. For both ‘friendly’ and ‘pathological’ regimes, initial conditions <inline-formula><mml:math id="inf715"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf716"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were independently and randomly drawn from the uniform distribution between 0.01 and 0.99 (“<inline-formula><mml:math id="inf717"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>0.99</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>”), and <italic>R</italic><sub><italic>X</italic></sub> was drawn from <inline-formula><mml:math id="inf718"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3.7</mml:mn><mml:mo>,</mml:mo><mml:mn>3.9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. <italic>R</italic><sub><italic>Y</italic></sub> was drawn from <inline-formula><mml:math id="inf719"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3.7</mml:mn><mml:mo>,</mml:mo><mml:mn>3.9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (‘friendly’) or <inline-formula><mml:math id="inf720"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3.1</mml:mn><mml:mo>,</mml:mo><mml:mn>3.3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (‘pathological’). <inline-formula><mml:math id="inf721"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf722"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were independently drawn from <inline-formula><mml:math id="inf723"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (‘friendly’) or <inline-formula><mml:math id="inf724"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.15</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (‘pathological’). (<bold>B</bold>) Box plots show the optimal prediction lag when using delay vectors made from <inline-formula><mml:math id="inf725"><mml:mi>X</mml:mi></mml:math></inline-formula> to predict values of <inline-formula><mml:math id="inf726"><mml:mi>Y</mml:mi></mml:math></inline-formula> in 250 systems with parameters selected randomly as described just now. In the ground truth model for this system, <inline-formula><mml:math id="inf727"><mml:mi>Y</mml:mi></mml:math></inline-formula> exerts a causal influence on <inline-formula><mml:math id="inf728"><mml:mi>X</mml:mi></mml:math></inline-formula>. In the ‘friendly’ parameter regime, the optimal prediction horizon is negative, correctly indicating that <inline-formula><mml:math id="inf729"><mml:mi>Y</mml:mi></mml:math></inline-formula> does indeed cause <inline-formula><mml:math id="inf730"><mml:mi>X</mml:mi></mml:math></inline-formula>. In the ‘pathological’ regime, the optimal prediction horizon is positive, and so the prediction lag test would wrongly conclude that <inline-formula><mml:math id="inf731"><mml:mi>Y</mml:mi></mml:math></inline-formula> does not cause <inline-formula><mml:math id="inf732"><mml:mi>X</mml:mi></mml:math></inline-formula>. In the friendly regime the ‘box’ is shown as a line because the vast majority of trials had the same optimal prediction lag of -1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-72518-app4-fig6-v2.tif"/></fig></sec></sec></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s13"><title>Detailed methods</title><sec sec-type="appendix" id="s13-1"><title>Methodological details for <xref ref-type="fig" rid="fig2">Figure 2</xref></title><p>For panel B, we simulated the random walk system<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where the <inline-formula><mml:math id="inf733"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> terms were drawn independently from a normal distribution with mean of 0 and standard deviation of 1. We simulated this system from the initial condition of <inline-formula><mml:math id="inf734"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> through 999 subsequent steps. For panel C, we simulated the autoregressive system<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>0.75</mml:mn><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>10</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where the <inline-formula><mml:math id="inf735"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> terms were again drawn independently from a normal distribution with mean of 0 and standard deviation of 1. We simulated this system from the initial condition of <inline-formula><mml:math id="inf736"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula> for 1999 subsequent steps. We only used the final 1000 steps for computing the correlation between two time series.</p><p>To compute the significance of the Pearson correlation between two time series, we used surrogate data generated by either permutation or the random phase procedure. Permutation surrogate time series were generated by randomly shuffling data. Random phase surrogate time series were generated by Ebisuzaki’s random phase method (<xref ref-type="bibr" rid="bib33">Ebisuzaki, 1997</xref>) as implemented in the rEDM (version 1.5) function make_surrogate_data. For a pair of time series <inline-formula><mml:math id="inf737"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1000</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1000</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we first computed the Pearson correlation <inline-formula><mml:math id="inf738"><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> between the two time series. We then replaced the <italic>X</italic><sub>2</sub> values with surrogate time series and recomputed the Pearson correlation as <inline-formula><mml:math id="inf739"><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula>. We computed this shuffled correlation 9,999 times (permutation) or 499 times (random phase) to get a null distribution <inline-formula><mml:math id="inf740"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. Following <xref ref-type="bibr" rid="bib99">Schreiber and Schmitz, 2000</xref>, we computed the <inline-formula><mml:math id="inf741"><mml:mi>p</mml:mi></mml:math></inline-formula> value as<disp-formula id="equ12"><label>(5)</label><mml:math id="m12"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf742"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of surrogates, <inline-formula><mml:math id="inf743"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of surrogate correlations <inline-formula><mml:math id="inf744"><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> whose magnitude was greater than or equal to the magnitude of the original correlation <inline-formula><mml:math id="inf745"><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>, and the “<inline-formula><mml:math id="inf746"><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>” terms account for the original correlation <inline-formula><mml:math id="inf747"><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s13-2"><title>Methodological details for <xref ref-type="fig" rid="fig4">Figure 4</xref></title><p>The system of equations was numerically integrated using the ode45 method in Matlab from <inline-formula><mml:math id="inf748"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf749"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> in time steps of 0.03, and plotted in the delay space <inline-formula><mml:math id="inf750"><mml:mi>Z</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf751"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>3.6</mml:mn></mml:mrow></mml:math></inline-formula>. The initial condition for all state variables (<inline-formula><mml:math id="inf752"><mml:mi>V</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf753"><mml:mi>W</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf754"><mml:mi>X</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf755"><mml:mi>Y</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf756"><mml:mi>Z</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf757"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>, <inline-formula><mml:math id="inf758"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>, and <inline-formula><mml:math id="inf759"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>) was 1. For panel F, measurement noise was added to <inline-formula><mml:math id="inf760"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Specifically, noisy data were generated as:<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.15</mml:mn><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mn>3</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.15</mml:mn><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf761"><mml:mrow><mml:mi>Unif</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a uniform random variable bounded by <inline-formula><mml:math id="inf762"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf763"><mml:mi>b</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf764"><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:math></inline-formula> is the difference between the maximum and minimum values of <inline-formula><mml:math id="inf765"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between <inline-formula><mml:math id="inf766"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf767"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula>. These noise parameters are chosen so that <inline-formula><mml:math id="inf768"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is centered at <inline-formula><mml:math id="inf769"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and has a standard deviation of <inline-formula><mml:math id="inf770"><mml:mrow><mml:mn>0.15</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> .</p></sec><sec sec-type="appendix" id="s13-3"><title>Methodological details for <xref ref-type="fig" rid="fig5">Figure 5</xref></title><p>The dynamics in the top row of <xref ref-type="fig" rid="fig5">Figure 5</xref> were generated from the equations:<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This continuous-time system was discretized from <inline-formula><mml:math id="inf771"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf772"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> on an evenly spaced grid of 400 data points for visualizing delay spaces where the time delay is 50 time points (i.e. <inline-formula><mml:math id="inf773"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>400</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>).</p><p>The dynamics in the second row of <xref ref-type="fig" rid="fig5">Figure 5</xref> were generated from the equations:<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3.61</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mn>3.61</mml:mn><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>3.61</mml:mn><mml:mo>−</mml:mo><mml:mn>3.61</mml:mn><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with initial conditions of <inline-formula><mml:math id="inf774"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf775"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula>. For this system, <inline-formula><mml:math id="inf776"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf777"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> were used to make the plots of delay spaces.</p><p>The dynamics in the third row of <xref ref-type="fig" rid="fig5">Figure 5</xref> were generated from the equations:<disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>25</mml:mn><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>with initial conditions of <inline-formula><mml:math id="inf778"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. For this system, <inline-formula><mml:math id="inf779"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> was used for delay spaces. This continuous-time system was numerically integrated using the ode45 method in Matlab from <inline-formula><mml:math id="inf780"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf781"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>13.998</mml:mn></mml:mrow></mml:math></inline-formula> on a grid of 4,667 evenly-spaced time points for plotting dynamics, and time points <inline-formula><mml:math id="inf782"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.003</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf783"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>7.698</mml:mn></mml:mrow></mml:math></inline-formula> were used for visualizing delay spaces.</p><p>The dynamics in the bottom row of <xref ref-type="fig" rid="fig5">Figure 5</xref> were generated from the classic Lorenz attractor equations:<disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>10</mml:mn><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>10</mml:mn><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>28</mml:mn><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>8</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>with initial conditions of <inline-formula><mml:math id="inf784"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. A delay of <inline-formula><mml:math id="inf785"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.14</mml:mn></mml:mrow></mml:math></inline-formula> was used to make delay spaces. This continuous-time system was numerically integrated using the ode45 method in Matlab from <inline-formula><mml:math id="inf786"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf787"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>399.98</mml:mn></mml:mrow></mml:math></inline-formula> on an evenly spaced grid of 5715 data points for visualizing delay spaces.</p></sec><sec sec-type="appendix" id="s13-4"><title>Methodological details for <xref ref-type="fig" rid="fig7">Figure 7</xref></title><sec sec-type="appendix" id="s13-4-1"><title>Ground truth model and data generation</title><p>We used the ground truth model:<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1.2</mml:mn><mml:mo>−</mml:mo><mml:mn>0.1</mml:mn><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1.1</mml:mn><mml:mo>−</mml:mo><mml:mn>0.2</mml:mn><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.3</mml:mn><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2.5</mml:mn><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf788"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf789"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the population sizes of species 1 and 2 at time <italic>t</italic>.  and <inline-formula><mml:math id="inf790"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are the values of periodic drivers at time <inline-formula><mml:math id="inf791"><mml:mi>t</mml:mi></mml:math></inline-formula>. Specifically, in both the two-driver and one-driver cases:<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>0.05</mml:mn><mml:mo>⁢</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>0.05</mml:mn><mml:mo>⁢</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mn>6</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the two-driver case:<disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>t</mml:mi><mml:msqrt><mml:mn>10</mml:mn></mml:msqrt></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Conversely, in the one-driver case <inline-formula><mml:math id="inf792"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The process noise terms <inline-formula><mml:math id="inf793"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf794"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are both IID normal random variables with mean of 0 and with shared standard deviation <inline-formula><mml:math id="inf795"><mml:msub><mml:mi>σ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>. Specifically, for any pair of times <inline-formula><mml:math id="inf796"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf797"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf798"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are independent, and similarly for <inline-formula><mml:math id="inf799"><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. Also, all values <inline-formula><mml:math id="inf800"><mml:mrow><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi></mml:mrow></mml:math></inline-formula> are independent of all values <inline-formula><mml:math id="inf801"><mml:mrow><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi></mml:mrow></mml:math></inline-formula>. At the beginning of each replicate simulation, the phases <inline-formula><mml:math id="inf802"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf803"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> are independently assigned a random number from a uniform distribution between 0 and <inline-formula><mml:math id="inf804"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula>, and do not change with time.</p><p>To generate data without measurement noise, we simulated this system for <inline-formula><mml:math id="inf805"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>400</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> with the initial conditions <inline-formula><mml:math id="inf806"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>;</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>4.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. We used the final 200 time points for inference to help ensure that the system had reached equilibrium behavior.</p><p>We also introduced additive measurement noise to simulate instrument uncertainty:<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>1.5</mml:mn></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf807"><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf808"><mml:msubsup><mml:mi>S</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> represent the observed values (i.e. noisy measurements) of <italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub>. <inline-formula><mml:math id="inf809"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf810"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are also IID normal random variables with mean of 0 and standard deviation <inline-formula><mml:math id="inf811"><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>. The tables in <xref ref-type="fig" rid="fig7">Figure 7D</xref> are generated by varying <inline-formula><mml:math id="inf812"><mml:msub><mml:mi>σ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> from 0 to 8 and varying <inline-formula><mml:math id="inf813"><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> from 0 to 1.</p></sec></sec><sec sec-type="appendix" id="s13-5"><title>Causal analysis using Granger causality and CCM</title><p>For each combination of <inline-formula><mml:math id="inf814"><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf815"><mml:msub><mml:mi>σ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> (the standard deviation of measurement noise and process noise, respectively), we generated 1000 time series for <italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub> as described above. For each replicate pair of time series, we used Granger causality and CCM to infer whether <italic>S</italic><sub>1</sub> causes <italic>S</italic><sub>2</sub> (it does) and whether <italic>S</italic><sub>2</sub> causes <italic>S</italic><sub>1</sub> (it does not).</p></sec><sec sec-type="appendix" id="s13-6"><title>Granger causality inference</title><p>We used the multivariate Granger causality Matlab package (MVGC, <xref ref-type="bibr" rid="bib6">Barnett and Seth, 2014</xref>). We used the following settings:</p><list list-type="bullet"><list-item><p>regmode = ’OLS’ (We fit the autoregressive model by the ordinary least squares method).</p></list-item><list-item><p>icregmode = ’LWR’ (We determined the information criterion using the LWR algorithm. This is the default setting).</p></list-item><list-item><p>morder = ’AIC’ (We used Akaike information criterion to determine the number of lags in the autoregressive model).</p></list-item><list-item><p>momax = 50 (We used a maximum of 50 lags in the autoregressive model).</p></list-item><list-item><p>tstat = ” (We used Granger’s F-test for statistical significance. This is the default setting).</p></list-item></list><p>We inferred the presence of a causal link if the p-value was less than or equal to 0.05. We inferred no causal link otherwise. When <inline-formula><mml:math id="inf816"><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf817"><mml:msub><mml:mi>σ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> were both 0, the MVGC package (correctly) exited with an error on most trials. We reported this as ‘unsuitable data’ in <xref ref-type="fig" rid="fig7">Figure 7D &amp; E</xref>.</p><p>When <inline-formula><mml:math id="inf818"><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf819"><mml:msub><mml:mi>σ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> are both 0, the inferred spectral radius of the stochastic process is close to 1, and the MVGC routines can be prohibitively slow (i.e. when running 1,000 trials, the program would hang at an early stage for hours). In this case, the authors note that switching from the package’s default single-regression mode to an alternative dual-regression mode may improve runtime (<xref ref-type="bibr" rid="bib6">Barnett and Seth, 2014</xref>). We thus switched to the dual-regression mode when the spectral radius was between 0.9999 and 1 (a spectral radius of 1 or more causes an error). This fix had no effect on benchmark results as long as at least one of <inline-formula><mml:math id="inf820"><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf821"><mml:msub><mml:mi>σ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> was not 0.</p></sec><sec sec-type="appendix" id="s13-7"><title>Convergent cross mapping</title><p>Convergent cross mapping looks for a delay map from <inline-formula><mml:math id="inf822"><mml:mi>X</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf823"><mml:mi>Y</mml:mi></mml:math></inline-formula>. That is, CCM looks for a map from <inline-formula><mml:math id="inf824"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf825"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus in order to apply CCM one needs to choose the delay τ and the vector length (dimension of the delay space) <italic>E.</italic> The parameters <inline-formula><mml:math id="inf826"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and τ should ideally be ‘generic’ in the sense of Takens’ theorem: we want to avoid line-crossing (such as the symbol ‘<inline-formula><mml:math id="inf827"><mml:mi mathvariant="normal">∞</mml:mi></mml:math></inline-formula>’) in the delay space, because otherwise, <inline-formula><mml:math id="inf828"><mml:msup><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> in <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref> does not exist. There are different ways to do this, but no method is obviously the best (<xref ref-type="bibr" rid="bib45">Harnack et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>).</p><p>Following <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref> and <xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref> we chose τ and <inline-formula><mml:math id="inf829"><mml:mi>E</mml:mi></mml:math></inline-formula> to maximize univariate one-step-ahead forecast of the putative causee <inline-formula><mml:math id="inf830"><mml:mrow><mml:mi>X</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> That is, for <inline-formula><mml:math id="inf831"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we try to predict <inline-formula><mml:math id="inf832"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using the simplex projection method by finding delay vectors in the training data of <inline-formula><mml:math id="inf833"><mml:mi>X</mml:mi></mml:math></inline-formula> that are most similar to <inline-formula><mml:math id="inf834"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, and take weighed average of their <inline-formula><mml:math id="inf835"><mml:mi>X</mml:mi></mml:math></inline-formula> values one step in the future (i.e. <xref ref-type="fig" rid="fig6">Figure 6A</xref> where <inline-formula><mml:math id="inf836"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> and the prediction lag is 1). If the delay space has a line crossing, then at the cross-point, a one-step-ahead forecast may have more than one possible outcome and thus perform poorly. In more detail, we made one-step-ahead forecasts within the time range 201–400 (we did not use time range 1–200 to avoid transient dynamics). As per the field standard, we used leave-one-out cross-validation to do simplex projection. That is, when making a forecast for a time <inline-formula><mml:math id="inf837"><mml:mi>t</mml:mi></mml:math></inline-formula>, we used all times within 201–400 other than <inline-formula><mml:math id="inf838"><mml:mi>t</mml:mi></mml:math></inline-formula> as training data (200 time points). We performed a grid search, varying τ from 1 to 6 and varying <inline-formula><mml:math id="inf839"><mml:mi>E</mml:mi></mml:math></inline-formula> from 1 to 6. We then used the combination of τ and <inline-formula><mml:math id="inf840"><mml:mi>E</mml:mi></mml:math></inline-formula> that maximized the forecast skill (the Pearson correlation between forecasts and true values) for subsequent CCM analysis. Additionally, following (<xref ref-type="bibr" rid="bib107">Sugihara et al., 2012</xref>), if the optimal combination of τ and <inline-formula><mml:math id="inf841"><mml:mi>E</mml:mi></mml:math></inline-formula> failed to give a significantly positive forecast skill, we did not report CCM results for that trial and reported the trial as “unsuitable data”. To test whether forecast skill is “significantly positive”, we ask whether it is robust to small changes in the training dataset. To do so, we used a naive bootstrap approach to create different versions of training libraries composed of randomly chosen delay vectors (sampling with replacement: some vectors may not be sampled and others may be sampled more than once) from the original training data using the ’random_libs’ setting in the rEDM (version 1.5) ccm method. The training library size (the number of delay vectors in the library) was chosen to be 200. We then calculated forecast skills with 300 such libraries and considered the forecast skill “significant” if at least least 95% gave a forecast skill greater than 0.</p><p>Having chosen τ and <inline-formula><mml:math id="inf842"><mml:mi>E</mml:mi></mml:math></inline-formula>, we checked three CCM criteria to infer causality (criteria 1–3 in <xref ref-type="fig" rid="fig6">Figure 6</xref>) using rEDM version 1.5. We did not use the fourth criterion (the prediction lag test) since its interpretation is unclear for periodic systems (<xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref>). For all three criteria, we used the same cross-validation setting that we used to choose τ and <inline-formula><mml:math id="inf843"><mml:mi>E</mml:mi></mml:math></inline-formula>. The first CCM criterion is that cross map skill is greater than 0. Thus, we computed cross map skill using the maximum possible number of distinct delay vectors (<inline-formula><mml:math id="inf844"><mml:mrow><mml:mn>200</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and compared this value to 0.</p><p>The second CCM criterion is that the cross map skill from causee to causer with real data must be greater than the cross map skill when the putative causer is replaced with surrogate data. To test this criterion, we first computed cross map skill using the same training and testing time points as before to obtain a single cross map skill value. We then repeatedly (1000 times) computed cross map skill in the same way, but now with the putative causer time series replaced with random phase surrogate data. Random phase surrogate data were generated by Ebisuzaki’s method as implemented in the rEDM function make_surrogate_data. We then computed the p-value according to <xref ref-type="disp-formula" rid="equ12">Equation 5</xref>. A putative causal link would pass this criterion if the p-value was less than or equal to 0.05.</p><p>The third CCM criterion is that cross map skill increases with more training data. Following <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>, we again used a naive bootstrap approach to test for this criterion. Specifically, we computed the cross map skill with a training library composed of randomly chosen delay vectors sampled with replacement from the original training data time points. We used either a large library with <inline-formula><mml:math id="inf845"><mml:mrow><mml:mn>200</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> available training vectors as used previously, or a small library with 15 training vectors. For each of 1000 bootstrap trials, we compared the cross map skill from a randomly chosen small library to the cross map skill from a randomly chosen large library. We said that the cross map skill increased with training data if the cross map skill of the large library was greater than that of the small library in at least 95% of the 1000 bootstrap trials.</p><p>For ‘alternative’ CCM testing, we only changed how the third CCM criterion (cross map skill increases with more training data) were tested. Here, instead of using the bootstrap test of <xref ref-type="bibr" rid="bib22">Cobey and Baskerville, 2016</xref>, we tested the third CCM criterion using Kendall’s τ test as suggested in <xref ref-type="bibr" rid="bib19">Chang et al., 2017</xref>. To do this, we varied the library size from a minimum of 15 vectors to the the maximum library size (<inline-formula><mml:math id="inf846"><mml:mrow><mml:mn>200</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), in increments of 3 vectors. For each library size, we computed cross map skill using 50 libraries randomly sampled without replacement (e.g. the 50 libraries would be identical at the maximal library size). We then computed the median cross map skill for each library size. Finally we ran a 1-tailed Kendall’s τ test for a positive association between library size and median cross map skill. We used the function stats.kendalltau from the Python package SciPy to compute a 2-tailed p-value, and then divided this p-value by two to get a 1-tailed p-value. We said that cross map skill increased with training data if the τ statistic was positive and the 1-tailed p-value was <inline-formula><mml:math id="inf847"><mml:mrow><mml:mi/><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s13-8"><title>Methodological details for <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref></title><p>The original subpopulation distributions are normal distributions with standard deviation of 10 and mean of 100 (male) or 130 (female). Each sampling plot shows 300 random samples.</p></sec><sec sec-type="appendix" id="s13-9"><title>Methodological details for <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref></title><p>To generate data for panels C-J, the system of panel A was numerically integrated using the ode45 method in Matlab with a time step of 0.005 and with the initial condition that <inline-formula><mml:math id="inf848"><mml:mi>X</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf849"><mml:mi>Y</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf850"><mml:mi>Z</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf851"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>, <inline-formula><mml:math id="inf852"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> were all set to one at <inline-formula><mml:math id="inf853"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Panels C, E, F, G, and I show data from a single period. For panel H the system was integrated for about five periods to more clearly visualize the lack of a continuous delay map. For panel J, the system was integrated for 1 period for the main panel and about 12 periods (to increase the sampling density) for the inset. This allows us to better see the separated legs of the curve upon zooming in. Panels C, D, F, H, and J were colored <inline-formula><mml:math id="inf854"><mml:mrow><mml:mi>mod</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. That is, they were colored by the remainder of <inline-formula><mml:math id="inf855"><mml:mi>t</mml:mi></mml:math></inline-formula> (time) after dividing by <inline-formula><mml:math id="inf856"><mml:mi>T</mml:mi></mml:math></inline-formula> (here <inline-formula><mml:math id="inf857"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>). <inline-formula><mml:math id="inf858"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>3.6</mml:mn></mml:mrow></mml:math></inline-formula> was used for all delay spaces.</p></sec><sec sec-type="appendix" id="s13-10"><title>Methodological details for <xref ref-type="fig" rid="app4fig3">Appendix 4—figure 3</xref></title><p>All systems were discretized from <inline-formula><mml:math id="inf859"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf860"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> on an evenly spaced grid of 200 points for visualizing delay spaces.</p><p>The dynamics in the top row were generated from the equations:<disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1.3</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A delay time of 12 time indices (i.e. <inline-formula><mml:math id="inf861"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>12</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>200</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) was used for constructing delay spaces.</p><p>The dynamics in the second row were generated from the equations:<disp-formula id="equ26"><mml:math id="m26"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1.3</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>3.77</mml:mn><mml:mo>−</mml:mo><mml:mn>3.77</mml:mn><mml:mo>∗</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf862"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>200</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and the initial condition <inline-formula><mml:math id="inf863"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>. A delay time of 25 time indices (i.e. <inline-formula><mml:math id="inf864"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>25</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>200</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) was used for constructing delay spaces.</p><p>In the third row the dynamics are identical to the first row, except that <inline-formula><mml:math id="inf865"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf866"><mml:mi>Y</mml:mi></mml:math></inline-formula> are switched, and <inline-formula><mml:math id="inf867"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula> time indices was used for constructing delay spaces.</p></sec><sec sec-type="appendix" id="s13-11"><title>Methodological details for <xref ref-type="fig" rid="app4fig4">Appendix 4—figure 4</xref></title><p>Top row: For this system, we used the initial conditions <inline-formula><mml:math id="inf868"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. We numerically integrated this system using ode45 in Matlab with a time step of 0.03. We composed delay vectors of length <inline-formula><mml:math id="inf869"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> with a delay of <inline-formula><mml:math id="inf870"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>3.6</mml:mn></mml:mrow></mml:math></inline-formula>. We visualized the delay space using data from <inline-formula><mml:math id="inf871"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf872"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>29.97</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 1–1000). For CCM with temporally separate training and testing sets, we used data from <inline-formula><mml:math id="inf873"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf874"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>14.97</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 1–500) for training data and data from <inline-formula><mml:math id="inf875"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf876"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>29.97</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 501–1000) for testing. Specifically, in the rEDM (version 0.7.2) ccm method we set the “lib” argument to “c(1, 500)” and set the “pred” argument to “c(501, 1000)”. We used rEDM version 0.7.2 for this analysis because we found that it more easily produced distinct training and test sets than later versions (on a computer running MacOS 11.6 and R version 4.0.2). For CCM with temporally interspersed training and testing sets, we set both the lib and pred arguments to “c(1,1000)”. This setting instructs rEDM to use leave-one-out cross-validation.</p><p>Second row: Ground truth data generation and analysis were the same as in the top row, except that the roles of <inline-formula><mml:math id="inf877"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf878"><mml:mi>Y</mml:mi></mml:math></inline-formula> were swapped.</p><p>Third row: For this system, we used the initial conditions <inline-formula><mml:math id="inf879"><mml:mrow><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>1.6</mml:mn></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. We numerically integrated this system using ode45 in Matlab with a time step of 0.1. We visualized the delay space using data from <inline-formula><mml:math id="inf880"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf881"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 1–401). We used the delay vector parameters <inline-formula><mml:math id="inf882"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>3.0</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. For CCM with temporally separate training and testing sets, we used data from <inline-formula><mml:math id="inf883"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf884"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>19.9</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 1–200) for training data and data from <inline-formula><mml:math id="inf885"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf886"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>39.9</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 201–400) for testing. For CCM with temporally interspersed training and testing sets, we used cross-validation over the entire range <inline-formula><mml:math id="inf887"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf888"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>39.9</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>Bottom row: We discretized this system with a time step of 0.05. We visualized the delay space using data from <inline-formula><mml:math id="inf889"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf890"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 1–401). We used the delay vector parameters <inline-formula><mml:math id="inf891"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. For CCM with temporally separate training and testing sets, we used data from <inline-formula><mml:math id="inf892"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf893"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>9.95</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 1–200) for training data and data from <inline-formula><mml:math id="inf894"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf895"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>19.95</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 201–400) for testing. For CCM with temporally interspersed training and testing sets, we used cross-validation over the entire range <inline-formula><mml:math id="inf896"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf897"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>19.95</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>For convergent cross mapping, we used the same τ and <inline-formula><mml:math id="inf898"><mml:mi>E</mml:mi></mml:math></inline-formula> as for visualizing delay spaces (see above). “Training data size” on the horizontal axis is the number of delay vectors in the training library. Each dot in these CCM plots represents the average forecast skill over 300 randomly chosen libraries. Error bars represent the 95% confidence interval as calculated by the bias-corrected and accelerated bootstrap (1,000 bootstraps) as implemented in Matlab’s bootci function. Error bars are the same color as the dots and so are not visible when they fit inside the dots.</p></sec><sec sec-type="appendix" id="s13-12"><title>Methodological details for <xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref></title><p>Top row: For this system, we used the initial conditions <inline-formula><mml:math id="inf899"><mml:mrow><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and composed delay vectors of length <inline-formula><mml:math id="inf900"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> with a delay of <inline-formula><mml:math id="inf901"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. We visualized the delay space using data from time points 501–2000. We used points 801–1000 for training data and points 1001–2000 for testing cross map predictions.</p><p>Second row: For this system, we used the initial conditions <inline-formula><mml:math id="inf902"><mml:mrow><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and the delay vector parameters <inline-formula><mml:math id="inf903"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We visualized the delay space using data from time points 501–2000. We used points 801–1000 for training data and points 1001–2000 for testing cross map predictions.</p><p>Third row: For this system, we used the initial conditions <inline-formula><mml:math id="inf904"><mml:mrow><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and the delay vector parameters <inline-formula><mml:math id="inf905"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We visualized the delay space using data from time points 501–2000 (time points 1-<inline-formula><mml:math id="inf906"><mml:mrow><mml:mn>6</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> for the zoomed-in inset). We used points 801–1000 for training data and points 1001–2000 for testing cross map predictions.</p><p>Fourth row: For this system, we used the initial conditions <inline-formula><mml:math id="inf907"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf908"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. We numerically integrated this system using ode45 in Matlab with a time step of 0.1. We visualized the delay space using data from <inline-formula><mml:math id="inf909"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>50.1</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf910"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 501–2000). We used the delay vector parameters <inline-formula><mml:math id="inf911"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>7.2</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We used data from <inline-formula><mml:math id="inf912"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>70.1</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf913"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 701–1000) for training data and data from <inline-formula><mml:math id="inf914"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>100.1</mml:mn></mml:mrow></mml:math></inline-formula> through <inline-formula><mml:math id="inf915"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> (time indices 1001–2000) for testing cross map predictions.</p><p>Fifth row: For this system, we used the initial conditions <inline-formula><mml:math id="inf916"><mml:mrow><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and composed delay vectors of length <inline-formula><mml:math id="inf917"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> with a delay of <inline-formula><mml:math id="inf918"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. We visualized the delay space using data from time points 501–2000. We used points 801–1000 for training data and points 1001–2000 for testing cross map predictions.</p><p>Sixth row: For this system, the “initial” conditions specified the first three time points since we included a lag of 3. Thus, for <inline-formula><mml:math id="inf919"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf920"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf921"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf922"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>. We composed delay vectors of length <inline-formula><mml:math id="inf923"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> with a delay of <inline-formula><mml:math id="inf924"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. We visualized the delay space using data from time points 501–2000. We used points 801–1000 for training data and points 1001–2000 for testing cross map predictions.</p><p>For convergent cross mapping (in rEDM version 0.7.2), we used the same τ and <inline-formula><mml:math id="inf925"><mml:mi>E</mml:mi></mml:math></inline-formula> as for visualizing delay spaces. The training data size is the number of delay vectors in the training library. For the plots in the fourth column, we chose 300 random libraries of training delay vectors with variable training data size, and used the standard prediction lag of 0. Delay vectors were chosen without replacement. Note that at large training data size, some or all of the 300 random libraries can be identical. Each dot in these CCM plots represents the average forecast skill over all 300 randomly-chosen libraries. Error bars represent the 95% confidence interval as calculated by the bias-corrected and accelerated bootstrap (1,000 bootstraps) as implemented in Matlab’s bootci function. Error bars are the same color as the dots and so are not visible when they fit inside the dots.</p><p>In all rows, the cross map skill for the putative causer <inline-formula><mml:math id="inf926"><mml:mi>Y</mml:mi></mml:math></inline-formula> was greater than for at least 95% of random phase surrogate time series (purple dot). The 5% cutoff value was computed for the maximum library size (156 for row four and ∼200 for all other rows) by running the CCM procedure after replacing the putative causer <inline-formula><mml:math id="inf927"><mml:mi>Y</mml:mi></mml:math></inline-formula> with 500 random phase surrogate time series generated using the rEDM function make_surrogate_data.</p><p>For the plots in the fifth column we used the full library contained within the training data window (156 delay vectors for row four and ∼200 for all other rows) and varied the prediction lag. We did not use random libraries for these plots.</p></sec><sec sec-type="appendix" id="s13-13"><title>Methodological details for <xref ref-type="fig" rid="app4fig6">Appendix 4—figure 6</xref></title><p>To generate randomized parameter sets, we randomly selected <italic>R</italic><sub><italic>X</italic></sub>, <italic>R</italic><sub><italic>Y</italic></sub>, <inline-formula><mml:math id="inf928"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf929"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from uniform distributions. We also randomly selected the initial conditions <inline-formula><mml:math id="inf930"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf931"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from uniform distributions. To make systems in the ‘friendly’ parameter regime, we drew <italic>R</italic><sub><italic>X</italic></sub> and <italic>R</italic><sub><italic>Y</italic></sub> independently from the range 3.7–3.9, we drew <inline-formula><mml:math id="inf932"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf933"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> independently from the range 0.05–0.1, and we drew <inline-formula><mml:math id="inf934"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf935"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> independently from the range 0.01–0.99. These are the same parameters used in the randomized numerical simulations of <xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref>. Next, to make systems in the ‘pathological’ parameter regime, we drew <italic>R</italic><sub><italic>X</italic></sub> from the range 3.7–3.9, we drew <italic>R</italic><sub><italic>Y</italic></sub> from the range 3.1–3.3, we drew <inline-formula><mml:math id="inf936"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf937"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> independently from the range 0.15–0.2, and we drew <inline-formula><mml:math id="inf938"><mml:mrow><mml:mi>X</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf939"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> independently from the range 0.01–0.99. For both parameter regimes we randomly chose 250 sets of parameters and ran the system for 3,000 time points. Occasionally a randomly chosen system would leave the basin of attraction and reach large values, represented on the computer as positive or negative infinity, or ‘not a number’. When this occurred, we discarded the data and resampled parameters.</p><p>To apply CCM (in rEDM version 0.7.2) on each system, we generated a training library of delay vectors of <inline-formula><mml:math id="inf940"><mml:mi>X</mml:mi></mml:math></inline-formula> by randomly selecting 200 vectors from among time points 100–2000. We then evaluated cross map skill from delay vectors of <inline-formula><mml:math id="inf941"><mml:mi>X</mml:mi></mml:math></inline-formula> to values of <inline-formula><mml:math id="inf942"><mml:mi>Y</mml:mi></mml:math></inline-formula> at points 2001–3000. Following <xref ref-type="bibr" rid="bib120">Ye et al., 2015</xref>, we used delay vectors of length <inline-formula><mml:math id="inf943"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and a delay duration of <inline-formula><mml:math id="inf944"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. We evaluated cross map skill with a prediction horizon of -8 through 8.</p></sec></sec></app></app-group></back></article>