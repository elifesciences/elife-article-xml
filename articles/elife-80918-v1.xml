<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">80918</article-id><article-id pub-id-type="doi">10.7554/eLife.80918</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Developmental Biology</subject></subj-group></article-categories><title-group><article-title>MorphoFeatures for unsupervised exploration of cell types, tissues, and organs in volume electron microscopy</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-284457"><name><surname>Zinchenko</surname><given-names>Valentyna</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6900-0656</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-284458"><name><surname>Hugger</surname><given-names>Johannes</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-233126"><name><surname>Uhlmann</surname><given-names>Virginie</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-62167"><name><surname>Arendt</surname><given-names>Detlev</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7833-050X</contrib-id><email>arendt@embl.de</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-133512"><name><surname>Kreshuk</surname><given-names>Anna</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1334-6388</contrib-id><email>anna.kreshuk@embl.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03mstc592</institution-id><institution>Cell Biology and Biophysics Unit, European Molecular Biology Laboratory (EMBL)</institution></institution-wrap><addr-line><named-content content-type="city">Heidelberg</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02catss52</institution-id><institution>European Bioinformatics Institute, European Molecular Biology Laboratory (EMBL)</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03mstc592</institution-id><institution>Developmental Biology Unit, European Molecular Biology Laboratory (EMBL)</institution></institution-wrap><addr-line><named-content content-type="city">Heidelberg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Scheffer</surname><given-names>Louis K</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Desplan</surname><given-names>Claude</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>02</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e80918</elocation-id><history><date date-type="received" iso-8601-date="2022-06-09"><day>09</day><month>06</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-01-06"><day>06</day><month>01</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-05-07"><day>07</day><month>05</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.05.07.490949"/></event></pub-history><permissions><copyright-statement>© 2023, Zinchenko, Hugger et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Zinchenko, Hugger et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-80918-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-80918-figures-v1.pdf"/><related-article related-article-type="commentary" ext-link-type="doi" xlink:href="10.7554/eLife.86172" id="ra1"/><abstract><p>Electron microscopy (EM) provides a uniquely detailed view of cellular morphology, including organelles and fine subcellular ultrastructure. While the acquisition and (semi-)automatic segmentation of multicellular EM volumes are now becoming routine, large-scale analysis remains severely limited by the lack of generally applicable pipelines for automatic extraction of comprehensive morphological descriptors. Here, we present a novel unsupervised method for learning cellular morphology features directly from 3D EM data: a neural network delivers a representation of cells by shape and ultrastructure. Applied to the full volume of an entire three-segmented worm of the annelid <italic>Platynereis dumerilii</italic>, it yields a visually consistent grouping of cells supported by specific gene expression profiles. Integration of features across spatial neighbours can retrieve tissues and organs, revealing, for example, a detailed organisation of the animal foregut. We envision that the unbiased nature of the proposed morphological descriptors will enable rapid exploration of very different biological questions in large EM volumes, greatly increasing the impact of these invaluable, but costly resources.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>machine learning</kwd><kwd>morphology</kwd><kwd>representation learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>P. dumerilii</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>788921</award-id><principal-award-recipient><name><surname>Arendt</surname><given-names>Detlev</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Unsupervised machine learning on the ultrastructure and shape of cells in volume electron microscopy yields a compact representation of cellular morphology that complements genetics-based cell type characterisation.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Development of multicellular organisms progressively gives rise to a variety of cell types, with differential gene expression resulting in different cellular structures that enable diverse cellular functions. Structures and functions together represent the phenotype of an organism. The characterisation of cell types is important to understand how organisms are built and function in general (<xref ref-type="bibr" rid="bib1">Arendt, 2008</xref>). Recent advances in single-cell sequencing have allowed the monitoring of differential gene expression in the cell types that make up the tissues and organs of entire multicellular organisms and led to the recognition of distinct regulatory programmes driving the expression of unique sets of cell type-specific effector genes (<xref ref-type="bibr" rid="bib31">Hobert and Kratsios, 2019</xref>; <xref ref-type="bibr" rid="bib64">Sebé-Pedrós et al., 2018a</xref>; <xref ref-type="bibr" rid="bib65">Sebé-Pedrós et al., 2018b</xref>; <xref ref-type="bibr" rid="bib49">Musser et al., 2021</xref>; <xref ref-type="bibr" rid="bib23">Fincher et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Siebert et al., 2019</xref>). These distinct genetic programmes make the identity and thus define cell types (<xref ref-type="bibr" rid="bib2">Arendt et al., 2016</xref>; <xref ref-type="bibr" rid="bib72">Tanay and Sebé-Pedrós, 2021</xref>). The genetic individuation of cell types then translates into phenotypic differences between cells, and one of the most intriguing questions is to determine this causation for each cell type. Only then will we understand their structural and functional peculiarities and intricacies and learn how evolutionary cell type diversification has sustained organismal morphology and physiology. As a prerequisite towards this aim, we need a comprehensive monitoring and comparison of cellular phenotypes across the entire multicellular body, which can then be linked to the genetically defined cell types.</p><p>Recent efforts have focused on large-scale high-resolution imaging to capture the morphological uniqueness of cell types. For the first time, the rapid progress in volume EM allows this for increasingly large samples, including organs and entire smaller animals, with nanometer resolution (<xref ref-type="bibr" rid="bib6">Bae et al., 2021</xref>; <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>; <xref ref-type="bibr" rid="bib85">Zheng et al., 2018</xref>; <xref ref-type="bibr" rid="bib60">Scheffer et al., 2020</xref>; <xref ref-type="bibr" rid="bib17">Cook et al., 2019</xref>; <xref ref-type="bibr" rid="bib79">Verasztó et al., 2020</xref>). Supported by automated cell segmentation pipelines (<xref ref-type="bibr" rid="bib30">Heinrich et al., 2021</xref>; <xref ref-type="bibr" rid="bib45">Macrina et al., 2021</xref>; <xref ref-type="bibr" rid="bib52">Pape et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Müller et al., 2021</xref>), such datasets allow characterisation and comparison of cellular morphologies, including membrane-bound and membraneless organelles and inclusions, at an unprecedented level of detail for thousands of cells simultaneously (<xref ref-type="bibr" rid="bib75">Turner et al., 2022</xref>; <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>). A notable challenge of such large-scale studies is the analysis of massive amounts of imaging data that can no longer be inspected manually. The latest tools for visual exploration of multi-terabyte 3D data allow for seamless browsing through the regions of interest (<xref ref-type="bibr" rid="bib57">Pietzsch et al., 2015</xref>; <xref ref-type="bibr" rid="bib46">Maitin-Shepard et al., 2021</xref>), given such regions are specified and contain a limited number of cells. However, if the goal is to characterise and/or compare morphologies of most or all cells in the dataset, one has to be able to automatically assign each cell an unbiased comprehensive morphological description - a feature vector with values representing the morphological properties of the cell in the most parsimonious manner.</p><p>The need for automation and quantitative evaluation has triggered the design of complex engineered features that capture specific cell characteristics (<xref ref-type="bibr" rid="bib7">Barad et al., 2022</xref>). However, such features often have to be manually chosen and mostly provide only a limited, targeted view of the overall morphology. To alleviate this problem, intricate modelling pipelines for cellular morphology quantification have been developed (<xref ref-type="bibr" rid="bib59">Ruan et al., 2020</xref>; <xref ref-type="bibr" rid="bib19">Driscoll et al., 2019</xref>; <xref ref-type="bibr" rid="bib56">Phillip et al., 2021</xref>); still, these are often data- and task-specific. For unbiased monitoring and comparison of cellular morphologies, broadly applicable automated pipelines are missing. They should, in addition, be capable of extracting comprehensive representations that are not biased by manual feature selection, the end task, or the type of data used. We expect that developing such techniques will not only speed up the process of scientific discovery in big volume imaging datasets but also facilitate detecting unusual and underrepresented morphologies.</p><p>In the last decade, artificial neural networks have been shown to exceed manually engineered pipelines in extracting comprehensive descriptors for various types of data and to be particularly suited for the extraction of visual features (<xref ref-type="bibr" rid="bib38">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Falk et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Simonyan and Zisserman, 2014</xref>; <xref ref-type="bibr" rid="bib24">Girshick et al., 2014</xref>). Moreover, the rise of so-called self-supervised training methods has removed the necessity for generating massive amounts of manual annotations previously needed to train a network. Besides the obvious advantage of being less labour-intensive, self-supervised methods do not optimise features for a specific end-task but instead produce descriptors that have been shown to be useful in a variety of downstream tasks, such as image classification, segmentation, and object detection (<xref ref-type="bibr" rid="bib29">He et al., 2020</xref>; <xref ref-type="bibr" rid="bib76">Van den Oord et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Tian et al., 2020</xref>). Such methods have already shown to be successful for phenotyping cells in image-based profiling (<xref ref-type="bibr" rid="bib44">Lu et al., 2019</xref>; <xref ref-type="bibr" rid="bib40">Lafarge et al., 2019</xref>) and for describing local morphology of patches or subcompartments within neural cells in 3D electron microscopy (EM) of brain volumes (<xref ref-type="bibr" rid="bib33">Huang et al., 2020</xref>; <xref ref-type="bibr" rid="bib63">Schubert et al., 2019</xref>). Building on the latest achievements, we aim to expand the feature extraction methods to automated characterisation of cellular morphology at the whole animal level.</p><p>Here, we present the first framework for the fully unsupervised characterisation of cellular shapes and ultrastructures in a whole-body dataset for an entire animal. We apply this new tool to the fully segmented serial block-face EM (SBEM) volume of the 6 days post fertilisation young worm of the nereid <italic>Platynereis dumerilii</italic>, comprising 11,402 mostly differentiated cells with distinct morphological properties (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>). We show that our method yields morphological descriptors - <italic>MorphoFeatures</italic> - that are in strong agreement with human perception of morphological similarity and quantitatively outperform manually defined features on cell classification and symmetric partner detection tasks. We further illustrate how our features can facilitate the detection of cell types by morphological means, via similarity-based clustering in the MorphoFeature vector space, sorting the cells into morphologically meaningful groups that show high correlation with genetically defined types such as muscle cells and neurons. Our pipeline also allows for the characterisation of rare cell types, such as enteric neurons and rhabdomeric photoreceptors, and detects distinct cell populations within the developing midgut. Finally, defining feature vectors that also represent the morphology of immediate neighbours, we group cells into tissues and also obtain larger groupings that represent entire organs. We show that such neighbourhood-based <italic>MorphoContextFeatures</italic> clustering reproduces manually annotated ganglionic nuclei in the annelid brain and represents a powerful tool to automatically and comprehensively detect the distinct tissues that belong to and make up the foregut of the nereid worm - including highly specialised and intricate structures such as the neurosecretory infracerebral gland (<xref ref-type="bibr" rid="bib8">Baskin, 1974</xref>; <xref ref-type="bibr" rid="bib32">Hofmann, 1976</xref>; <xref ref-type="bibr" rid="bib25">Golding, 1970</xref>) and the axochord that has been likened to the chordate notochord (<xref ref-type="bibr" rid="bib41">Lauri et al., 2014</xref>). We further show that such morphologically defined tissues and organs correlate with cell type and tissue-specific gene expression. Our work thus sets the stage for linking genetic identity and structure-function of cell types, tissues, and organs across an entire animal.</p><p>The MorphoFeatures for the <italic>Platynereis</italic> dataset, as well as the code to generate and analyse them are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/kreshuklab/MorphoFeatures">https://github.com/kreshuklab/MorphoFeatures</ext-link>, (<xref ref-type="bibr" rid="bib86">Zinchenko, 2023</xref> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:dc0982d6a278139517e94977a48b50119419b1a5;origin=https://github.com/kreshuklab/MorphoFeatures;visit=swh:1:snp:01b134d03a405dd5ad419a9931676bd43b6e0714;anchor=swh:1:rev:f13d505f68e0dc08bd5fed9121ee56e45b4bd6ac">swh:1:rev:f13d505f68e0dc08bd5fed9121ee56e45b4bd6ac</ext-link>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Unsupervised deep-learning extracts extensive morphological features</title><p>Our pipeline has been designed to extract morphological descriptors of cells (MorphoFeatures) from EM data. It requires prior segmentation of all the cells and nuclei of interest. The pipeline utilises segmentation masks to extract single cells/nuclei and represents their morphology as a combination of three essential components: shape, coarse, and fine texture (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We train neural networks to represent these components independently, using point clouds as input for shape, low-resolution raw data for coarse texture with sufficient context and small, high-resolution crops of raw data for fine texture. Separating these components ensures they are all represented in the final features and facilitates further interpretation. The features are combined to form one feature vector for each cell of the animal.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Deep-learning pipeline for extracting MorphoFeatures.</title><p>(<bold>A</bold>) Cell segmentation is used to mask the volume of a specific cell (and its nucleus) in the raw data. Neural networks are trained to represent shape, coarse, and fine texture from the cell volume (separately for cytoplasm and nuclei). The resulting features are combined in one MorphoFeatures vector that is used for the subsequent analysis. (<bold>B</bold>) Training procedure for the shape features. A contrastive loss is used to decrease the distance between the feature vectors of two augmented views of the same cell and increase the distance to another cell. (<bold>C</bold>) Training procedure for the coarse and fine texture features (here illustrated by coarse texture). Besides the contrastive loss, an autoencoder loss is used that drives the network to reconstruct the original cell from the feature vector.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig1-v1.tif"/></fig><p>A common way to train a neural network for extracting relevant features is to use some prior information about samples in the dataset as a source of supervision. For example, to extract morphological representations of cells, one could use a complementary task of predicting cell lines (<xref ref-type="bibr" rid="bib83">Yao et al., 2019</xref>; <xref ref-type="bibr" rid="bib18">Doan et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Eulenberg et al., 2017</xref>), experimental conditions the cells are coming from <xref ref-type="bibr" rid="bib12">Caicedo et al., 2018</xref>, or classification of proteins fluorescently labelled in the cells (<xref ref-type="bibr" rid="bib37">Kobayashi et al., 2021</xref>). However, such metadata is mostly unavailable for EM volumes, and manual annotation might become infeasible with increasing data size. Moreover, exploratory studies are often aimed at discovering unusual morphology, new cell types, or tissues. In this case, defining supervision is not only difficult but might also bias the exploration towards the ‘proxy’ groups used for supervision. To enable immediate data exploration even for datasets where direct supervision is hard to define and the ground truth cannot easily be obtained, we developed a fully unsupervised training pipeline that is based on two complementary objectives (<xref ref-type="fig" rid="fig1">Figure 1B and C</xref>). The first is an autoencoder reconstruction loss, where a network extracts a low-dimensional representation of each cell and then uses this representation to reconstruct back the original cell volume. This loss encourages the network to extract the most comprehensive description. The second objective is a contrastive loss that ensures the feature vectors extracted from two similarly looking cells (positive samples) are closer to each other than to feature vectors extracted from more dissimilar cells (negative samples). Since we do not know in advance which samples can be considered positive for our dataset, we are using slightly different views of the same cell, generated by applying realistic transformations to cell volumes (see Materials and methods). The combination of these two losses encourages the learned features to retain the maximal amount of information about the cells, while enforcing distances between feature vectors to reflect morphological similarity of cells.</p><p>The pipeline was trained and applied on the cellular atlas of the marine annelid <italic>P. dumerilii</italic> (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>). It comprises a 3D SBEM volume of the whole animal that has sufficient resolution to distinguish ultrastructural elements (organelles and inclusions, nuclear and cytoplasm texture, etc.) and an automated segmentation of 11,402 cells and nuclei (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Additionally, whole-animal gene expression maps are available that cover many differentiation genes and transcription factors.</p><p>Such a high number of morphologically and genetically diverse cells can make the initial exploratory analysis difficult. We designed MorphoFeatures to enable unbiased exploration of morphological variability in the animal on both cellular and tissue level; and indeed our training procedure yields 480 features that extensively describe each cell in terms of its cytoplasm and nucleus shape, coarse texture, and fine texture (80 features for each category). The features can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/kreshuklab/MorphoFeatures.git">https://github.com/kreshuklab/MorphoFeatures.git</ext-link>; the feature table can be directly integrated with the <italic>Platynereis</italic> atlas of <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref> through the MoBIE plugin (<xref ref-type="bibr" rid="bib53">Pape et al., 2022</xref>) in Fiji (<xref ref-type="bibr" rid="bib61">Schindelin et al., 2012</xref>).</p></sec><sec id="s2-2"><title>MorphoFeatures allow for accurate morphological class prediction</title><p>Good morphological features should distinguish visibly separate cell groups present in the data. To estimate the representation quality of our MorphoFeatures, we quantified how well they can be used to tell such groups apart. We took the morphological cell class labels available in the dataset, proofread, corrected, and expanded them. These labels include seven general classes of cells: neurons, epithelial cells, midgut cells, muscle cells, secretory cells, dark neurosecretory cells, and ciliary band cells (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) with ~60 annotated cells per class in the whole volume. It is worth noting that these cell classes were chosen based on visual morphological traits and do not necessarily represent genetically defined cell type families (<xref ref-type="bibr" rid="bib3">Arendt et al., 2019</xref>). For example, while ciliary band cells are a small narrow group of morphologically homogeneous cells that likely constitute one cell type family, secretory cells were defined as cells with abundant organelles/inclusions in their cytoplasm that could indicate secretion, spanning a wider range of morphologies and, potentially, genetically defined cell types.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Morphological class prediction.</title><p>(<bold>A</bold>) Examples of cells from seven manually defined morphological classes used for evaluation. (<bold>B</bold>) Confusion matrix of class prediction for the logistic regression model. Rows are the labels, and columns are the predictions. Scale bars: 5 μm. (<bold>C</bold>) The predicted probability of the epithelial class in the whole animal. Note that while a few cells on the animal surface have been used for training of the logistic regression, no labels were given in the foregut opening or chaetae which are still correctly recognised as epithelial. (<bold>D</bold>) The predicted probability of neural cells in the whole animal.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Morphological class predictions.</title><p>(<bold>A</bold>) All the errors made during any of the cross-validation runs of the logistic regression model (L – true label, P – prediction, scale bars: 5 µm). B. Whole-animal prediction of (from left to right) dark neurosecretory, muscle, secretory, ciliary band, and midgut cells (scale bars: 25 µm).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Confusion matrix of class predictions for the logistic regression model using separate morphological components.</title><p>Rows are the labels, and columns are the predictions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig2-figsupp2-v1.tif"/></fig></fig-group><p>We extracted MorphoFeatures by passing all the cells through the neural network trained as described above (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) and used them as an input to train a logistic regression classifier to predict the above-mentioned cell classes. This classifier has few parameters and can be trained even on the modest number of labels we have available. If any subset of the features strongly correlates with a given cell class, the model will show high performance. A classifier with MorphoFeatures achieved an accuracy of 96%, showing that the learned features are sufficient to distinguish broad morphological classes. To better understand remaining inaccuracies, we show the errors made by the classifier in <xref ref-type="fig" rid="fig2">Figure 2B</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>. This reveals, as expected, that while morphologically homogeneous classes can be easily separated, the classifier tends to confuse some secretory cells with other types (midgut and epithelial cells) that are also difficult to discern by eye and might well be mis-annotated. It also confuses the general neuron class with neurosecretory cells that represent a subclass of neurons.</p><p>To illustrate large-scale performance of the broad cell type classification model, we show its predictions on the whole animal in <xref ref-type="fig" rid="fig2">Figure 2C and D</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>. These results demonstrate that MorphoFeatures are sufficiently expressive to enable training a generalisable model from a small number of labels, taking epithelial cells as a showcase: while the model was trained exclusively on the cells of the outer animal surface, it successfully predicted the cells outlining the gut opening and the chaetae cells as epithelial as well. Even more convincingly, the model predicted some cells in the midgut region as neurons, which after careful visual examination were indeed confirmed to be enteric neurons.</p><p>To explore to which extent each of the morphology components (e.g. cell shape or fine nuclear texture) contributes to predicting the manually labelled cell classes, we ran a similar classification pipeline using these components separately (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). The results show that, for example, cytoplasm texture is sufficient to tell some classes apart with high precision. However, both coarse and fine cytoplasm texture perform slightly worse on distinguishing neurosecretory cells, in which nuclei occupy almost the whole cell volume. Cell shape is satisfactory to characterise neurons and muscle cells but shows inferior performance on epithelial and secretory cells, which display a great variety of cell shapes. Surprisingly, nuclear fine texture features correctly distinguish, among other classes, muscle cells, suggesting that this class has characteristic chromatin texture.</p><p>In <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>, morphological classification of cells was performed based on manually defined features, including descriptors of shape (e.g. volume, sphericity, and major and minor axes), intensity (e.g. intensity mean, median and SD), and texture (Haralick features) extracted from the cell, nucleus, and chromatin segmentations, with 140 features in total. To compare these explicitly defined features with implicitly learned MorphoFeatures, we passed them through the same evaluation pipeline yielding a classifier which achieves the accuracy of 94%, in comparison to 96% achieved by MorphoFeatures. The classifier mostly made similar mistakes but performed worse on the classes of muscle and epithelial cells. Both sets of features demonstrate high accuracy on the broad morphological type classification task. For a more detailed comparison, we turn to a different task which allows us to implicitly, but quantitatively, evaluate how well the features group similar cells together. We use the metric introduced in <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref> that is based on the fact that the animal is bilaterally symmetric, thus almost all the cells on one side of the animal will have a symmetric partner. The metric estimates how close a given cell is to its potential symmetric partner in the morphological feature space in comparison to all the other cells in the animal (see Materials and methods). According to this metric, our MorphoFeatures are 38% more precise in locating a symmetric partner than the ones from <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>, showing the widely accepted superiority of neural network extracted features to explicitly defined ones.</p></sec><sec id="s2-3"><title>MorphoFeatures uncover groups of morphologically similar cells</title><p>Comprehensive analysis of morphological features should enable the grouping of cells with similar morphology, which may represent cell types. We investigated the grouping induced by MorphoFeatures by projecting the features of all the animal cells onto 2D space using the dimensionality reduction technique UMAP (<xref ref-type="bibr" rid="bib47">McInnes et al., 2018</xref>). To explore the quality of our representations, we repeatedly took a random cell from the animal and visualised its three closest neighbours (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). This shows that cells in close proximity in the feature space appear highly visually similar and might represent cell types at the morphological level. For example, a group of secretory cells in which the cytoplasm is filled with round electron-transparent vesicles (upper green panel) corresponds to the ‘bright droplets’ cells from <xref ref-type="bibr" rid="bib79">Verasztó et al., 2020</xref>. Another group of secretory cells has an extensive endoplasmic reticulum (ER) surrounding the nucleus with a prominent nucleolus (lower green panel) and represents parapodial spinning gland cells (<xref ref-type="bibr" rid="bib79">Verasztó et al., 2020</xref>). One can also see clear groups of flat developing midgut cells (brown panel), short muscles (red panel), ciliary band cells on the outer animal surface (yellow panel) as well as extended flat and short round epithelial cells (orange panels). Another interesting observation is that even though the neurite morphology is not taken into account, neurons show remarkable diversity based on soma morphology only (blue panels). Careful examination of a group of cells, initially labelled as neurons, that are located next to the epithelial cells on the UMAP projection (upper right blue panel) revealed that these cells are sensory cells that extend their processes to the epithelial surface.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Visual analysis of MorphoFeatures representations.</title><p>(<bold>A</bold>) Finding visually similar cells using MorphoFeatures. Multidimensional features are visualised in 2D using UMAP. Each point represents a feature vector of a cell from the dataset. The cells for which annotations are available are visualised in respective colours. For a random cell, the cell and its three closest neighbours in the UMAP space are visualised in the electron microscopy (EM) volume. Scale bars: 5 μm. (<bold>B</bold>) Visualising morphological clusters. Clustering results are visualised on the MorphoFeatures UMAP representation. For some clusters, the cells comprising the cluster are shown in the animal volume to visualise the cell type. For example, cluster 6 precisely picks out the dark neurosecretory cells, while cluster 14 corresponds to the midgut cells (see Text for more details). Scale bars: 10μm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Cluster of split segmentation errors.</title><p>(<bold>A</bold>) The cluster visualised on the UMAP representation. (<bold>B</bold>) Examples of the cluster cell shapes. Cytoplasm is shown in grey, nuclei - in yellow.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Cells with segmentation errors.</title><p>Midgut cells (dark green), long muscles (dark orange), foregut muscles (light orange), and epithelial cell (light green) still get assigned to the correct cluster. Neuron (dark yellow) get wrongly assigned to epithelial cells cluster. Scale bars: 5 μm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig3-figsupp2-v1.tif"/></fig></fig-group><p>To explore the structure of the MorphoFeatures space, we performed hierarchical clustering of all cells. In the first clustering round, we split cells into 15 broad morphological groups (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Cluster content could be revealed through available annotations and visualisation with the PlatyBrowser (<xref ref-type="bibr" rid="bib53">Pape et al., 2022</xref>; <xref ref-type="fig" rid="fig3">Figure 3B</xref>): Clusters 1–8 represent different groups of neurons; 9–11 epithelial cells; 12 and 13 are muscles; 14 midgut cells; while cluster 15 was further subclustered into secretory cells (subcluster 1) and ciliary band cells (subcluster 2). MorphoFeatures can thus guide the morphological exploration of a whole-animal volume, facilitating the discovery and annotation of morphologically coherent groups of cells (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). For example, the foregut epithelial cells are distinct from the epithelial cells of the outer animal surface and chaetae cells (clusters 9, 11, and 10, respectively). Separate groups are also formed by different types of neurons, for example, foregut neurons (cluster 4).</p></sec><sec id="s2-4"><title>Morphological clusters have distinct gene expression profiles</title><p>To further describe the morphological clusters, we took advantage of the whole-animal cellular gene expression atlas available for <italic>Platynereis</italic> (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>), containing more than 200 genes mapped onto the EM volume. We looked for genes that are highly expressed in a given cluster, while also being specific for this cluster - having low expression in the other cells of the animal.</p><p>Many of our clusters show a clear genetic signature (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Among the neurons, cluster 5 shows the most specific gene expression including the Transient receptor potential (Trp) channels <italic>pkd2, trpV4</italic>, and <italic>trpV5</italic>, and the bHLH transcription factor <italic>asci</italic>, which demarcate sensory cells of the palpal, antennal, and cirral ganglia (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>). Cluster 1 shows specific expression of the homeodomain transcription factors <italic>lhx6</italic> and <italic>phox2</italic> and an <italic>ap2</italic> family member, while cluster 6 composed of dark neurosecretory cells shows specific expression of two neurosecretion markers, atrial natriuretic peptide receptor <italic>anpra</italic> and prohormone convertase <italic>phc2</italic>, identifying these as brain parts of the circular and apical nervous system, respectively (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Arendt, 2021</xref>). Cluster 8, another neural cluster, is enriched for the oxidative stress marker <italic>cytoglobin</italic> (<italic>globin-like</italic>; <xref ref-type="bibr" rid="bib68">Song et al., 2020</xref>) and the bHLH transcription factor <italic>mitf</italic>. Subclustering revealed three subclusters, one of which represents rhabdomeric photoreceptors of the adult eye (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, subcluster 8.1) that specifically express <italic>mitf</italic> and <italic>globin-like</italic>. An average cell in this subcluster (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) shows abundant black inclusions. The second subcluster (8.2) contains pigment cells with black pigment granules. Cells in the third subcluster (8.3) are located in the midgut, with a smooth oval shape, neurite extensions and nuclei taking up the most cell volume (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>) - in striking contrast to other midgut cells but resembling neurons, making it likely that these cells represent enteric neurons previously postulated on the basis of serotonin staining (<xref ref-type="bibr" rid="bib11">Brunet et al., 2016</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Clustering and gene analysis.</title><p>(<bold>A</bold>) Gene expression dot plot. The size of the dots shows how much of a cluster expresses a gene; the colour shows how much of the expression of a gene is confined to a cluster (see Materials and methods). The genes mentioned in the text are enboxed. The clusters lacking highly specific gene expression were not included. (<bold>B-C</bold>) The average shape and texture (see Materials and methods) of (<bold>B</bold>) rhabdomeric photoreceptors (cluster 8.1) and (<bold>C</bold>) the enteric neurons (cluster 8.3). (<bold>D</bold>) Localisation of the enteric neurons (pink) in the midgut volume (grey). Left: frontal view and right: side view.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Clustering and gene analysis.</title><p>Some of the genes shown to be differentially expressed in neuron, midgut, muscle, and epithelial clusters and in the photoreceptor cells are visualised on the UMAP representation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Comparing MorphoFeatures to a set of manually defined features from <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>.</title><p>(<bold>A</bold>) The set of manually defined features is visualised in 2D using UMAP. The cells for which annotations are available are visualised in respective colours. (<bold>B and C</bold>) The MorphoFeatures cluster of (<bold>B</bold>) the rhabdomeric photoreceptors and (<bold>C</bold>) enteric neurons is visualised on the UMAP representation of the manually defined features, showing a bigger spread of cells across the morphological space. (<bold>D</bold>) The MorphoFeatures cluster of the foregut muscles is split into multiple groups on the UMAP representation of the manually defined features. For two groups, cells are visualised in the animal volume, showing similarity of the cells comprising them. Scale bars: 10μm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig4-figsupp2-v1.tif"/></fig></fig-group><p>Among the epidermal cells, the outer cluster 11 is enriched for <italic>protocadherin</italic> (<italic>pcdh15</italic>) and <italic>neurotrypsin</italic> (<italic>ntrps</italic>), whereas cluster 9 representing foregut epidermis shows high specificity for the bHLH transcription factor <italic>sim1</italic> and the homeodomain factor <italic>pou3</italic> (<italic>brn124</italic>, known to be expressed in stomodeal ectoderm in sea urchin; <xref ref-type="bibr" rid="bib16">Cole and Arnone, 2009</xref>). For the muscles, the specific expression of striated muscle-specific myosin heavy chain <italic>mhcl4</italic> in cluster 12 denotes the group of striated somatic muscles, while its absence in cluster 13 indicates that the foregut muscles have not yet switched to expressing striated markers (<xref ref-type="bibr" rid="bib11">Brunet et al., 2016</xref>). Instead, the high and specific expression of the calexcitin-like sarcoplasmic calcium-binding protein <italic>Scp2</italic> (<italic>calexcitin2;</italic> <xref ref-type="bibr" rid="bib82">White et al., 2011</xref>), <italic>calmodulin</italic>, and of the Boule homologue <italic>Boll</italic> (<italic>boule-like</italic>) in cluster 13 suggests a calcium excitation and/or sequestration mechanism specific for foregut muscles. Finally, the cluster of midgut cells (cluster 14) specifically expresses the forkhead domain transcription factor <italic>foxA</italic> (<italic>for</italic>), a gut developmental marker (<xref ref-type="bibr" rid="bib10">Boyle and Seaver, 2008</xref>) and <italic>TrpV-c</italic>, another Trp channel paralogue of the vanilloid subtype with presumed mechanosensory function.</p><p>Beyond that, we noted considerable genetic heterogeneity in the midgut cluster. Finer clustering (<xref ref-type="fig" rid="fig5">Figure 5</xref>) revealed one subcluster with strong expression of the smooth muscle markers <italic>non-muscle-mhc</italic> and <italic>mrlc2</italic>, and another expressing a chitinase related to chitotriosidase (<italic>nov2</italic>) and the zinc finger transcription factor <italic>Prdm16/mecom</italic>, known to maintain homeostasis in intestinal epithelia (<xref ref-type="bibr" rid="bib70">Stine et al., 2019</xref>). Visualising the cells belonging to each subcluster and the specifically expressed genes in the PlatyBrowser (<xref ref-type="fig" rid="fig5">Figure 5E</xref>) revealed distinct territories in the differentiating midgut, which we interpret as midgut smooth musculature and digestive epithelia. We also detected an enigmatic third cluster located outside of the midgut, in the animal parapodia, comprising cells that resemble midgut cells morphologically (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). For this subcluster, the current gene repertoire of the cellular expression atlas did not reveal any specifically expressed gene.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Midgut cell types with defining genes.</title><p>(<bold>A</bold>) Finer clustering of the midgut cluster results in three subclusters. (<bold>B</bold>) Gene expression dot plot of the two midgut subclusters that are presumably developing smooth muscles and digestive cells of the midgut. The size of the dots shows how much of a cluster expresses a gene; the colour shows how much of the expression of a gene is confined to a cluster (see Materials and methods). (<bold>C</bold>) Some of the genes shown to be differentially expressed in the two subclusters, plotted on the UMAP representation. (<bold>D</bold>) The location (upper panel) and an example cell (lower panel) of the subcluster located in the animal parapodia. Scale bars: upper panel - 25μm, lower panel - 5μm. (<bold>E</bold>) Cells belonging to the two cell types (left panels) and the genes differentiating them (centre and right panels) are visualised in the animal volume, with colour representing gene expression overlayed on the electron microscopy (EM) plane view. Scale bar: 10μm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig5-v1.tif"/></fig></sec><sec id="s2-5"><title>MorphoFeatures correspond to visually interpretable morphological properties</title><p>Neural networks are often referred to as ‘black boxes’ to signify that it is not straightforward to trace learned features or decisions back to particular input properties. To examine whether it is possible to understand which properties of cells learned by our network distinguish the discovered morphological groups, we first identified MorphoFeatures that have high values specific to each cluster (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Then for a set of characteristic features, we visualised cells that correspond to the maximum and minimum value of the corresponding feature (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Visual inspection showed that many of them can be matched to visually comprehensible properties.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Visualisation of the learned features.</title><p>For each feature, four cells with a minimal (blue) and four cells with a maximal (red) value of the feature are shown, see text for detailed analysis. Shown are cytoplasm coarse texture feature 21, nuclear coarse texture feature 4, cytoplasm fine texture feature 50, nuclear fine texture feature 7, cytoplasm shape feature 14, and nuclear shape feature 66. Scale bars: coarse texture - 2μm, fine texture - 1μm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Dot plot of MorphoFeatures specific to clusters (<bold>A</bold>) 10–14, 8.1, 8.2, and 3, (<bold>B</bold>) 1, 4, 8.3, 15.1, and 7, and (<bold>C</bold>) 6 and 15.2.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig6-figsupp1-v1.tif"/></fig></fig-group><p>For example, cytoplasm coarse texture feature 21 (<xref ref-type="fig" rid="fig6">Figure 6</xref>, upper left), which distinguishes the outer epithelial cells (cluster 11), shows its minimal value in secretory cells, which contain multiple highly distinct types of texture, and its maximal value in the cells crowded with small vesicles and cisternae on the surface of the animal. The cluster of enteric neurons (cluster 8.3) strongly differs from other neurons by nuclear coarse texture feature 4 (<xref ref-type="fig" rid="fig6">Figure 6</xref>, upper right), which contrasts nuclei with large amount of heterochromatin often attached to the nuclear lamina to nuclei with a high amount of euchromatin and prominent nucleoli. Cytoplasm fine texture feature 50 (<xref ref-type="fig" rid="fig6">Figure 6</xref>, middle left), characteristic to the midgut cells (cluster 14), is the lowest in cells with a small stretch of homogeneous cytoplasm with mitochondria and has its peak in cells with abundant Golgi cisternae and medium-sized mitochondria. Rhabdomeric photoreceptors of the adult eye (cluster 8.1) display specific nuclear fine texture feature 7 (<xref ref-type="fig" rid="fig6">Figure 6</xref>, middle right) that differentiates between nuclei with smooth and rather grainy texture. Cell shape feature 14 (<xref ref-type="fig" rid="fig6">Figure 6</xref>, lower left) has its minimal value found in beaker-shaped cells with smooth surface and its maximal value in rather round cells with extremely rugged surface and is specific for ciliary band cells (cluster 15.2). Foregut muscles (cluster 13) can be described using nuclear shape feature 66 (<xref ref-type="fig" rid="fig6">Figure 6</xref>, lower right), having one extreme in nuclei with a small compartment with a rough surface (as a result of a segmentation error) and the other extreme in elongated flat nuclei.</p></sec><sec id="s2-6"><title>Adding neighbour morphological information helps to identify tissues and organs</title><p>Most cells do not function in isolation but rather form tissues, i.e., groups of structurally and functionally similar cells with interspersed extracellular matrix. We set out to systematically identify tissues with a new feature vector assigned to each cell that takes into account information about the morphology of neighbouring cells. For this, we combined the MorphoFeatures of a cell with the average MorphoFeatures of its immediate neighbours, yielding a feature vector that represents the morphology of both the cell and its surrounding, which we refer to as MorphoContextFeatures.</p><p>Applying the representation analysis described before, we find that proximity in MorphoContextFeature space no longer reflects morphological similarity only but rather identifies neighbouring groups of morphologically similar cells (whereby the neighbouring groups can be morphologically dissimilar). In other words, we now identify different tissues that together constitute organs composed of various tissues (<xref ref-type="fig" rid="fig7">Figure 7</xref>). For example, a separate assembly of groups of cells in the lower part of the UMAP projection represents the animal foregut subdivided into its constituting tissues comprising foregut neurons (lower blue panel), foregut muscles (lower beige panel), and foregut epithelium (left yellow-green panel). Next to this, we identify a group of muscles surrounding the foregut (left red panel).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Characterising neighbourhoods with MorphoContextFeatures.</title><p>Clustering results are visualised on the MorphoContextFeatures UMAP representation. For some clusters, the cells comprising the cluster are shown in the animal volume to visualise the cell type. Upper panels (from left to right): secretory (blue) and epithelial (green) cells of parapodia, epithelial, and secretory cells of the head, cirral, palpal, and dorsal ganglia. Lower panels (from left to right): foregut muscles, foregut neurons, infracerebral gland, and ventral nerve cord. Left panels (from top to bottom): muscles surrounding the foregut and foregut epithelium. Right panels (from top to bottom): epithelial-sensory circumpalpal cells and peripheral ganglia. Scale bars: 10μm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig7-v1.tif"/></fig><p>For the nervous system, adding neighbourhood information helps distinguishing cirral, palpal, and dorsal ganglia (three upper right panels, from left to right), as well as the ventral nerve cord and peripheral ganglia (two lower right panels). To benchmark the quality of this grouping, we clustered all the cells in the MorphoContextFeature space (see Materials and methods) and compared the neuron clusters to both manually segmented ganglionic nuclei and to the genetically defined neuronal clusters taking into account available gene expression in the cellular atlas (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>; <xref ref-type="fig" rid="fig8">Figure 8A and B</xref>). Notably, manual segmentation of brain tissues relied on visible tissue boundaries and thus has lower quality in the areas where such boundaries are not sufficiently distinct.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>MorphoContextFeatures define ganglionic nuclei.</title><p>(<bold>A and B</bold>) The animal ganglia as defined by manual segmentation (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>), our MorphoContextFeature clustering and gene expression clustering displayed on (<bold>A</bold>) the UMAP representation and (<bold>B</bold>) in the animal volume. PG, palpal ganglia; CpG, circumpalpal ganglia; AG, antennal ganglia; MB, mushroom bodies; DG, dorsal ganglion; CG, cirral ganglia; VNC, ventral nerve cord. Scale bars: 10μm. (<bold>C</bold>) Gene expression dot plot of the ganglia defined by MorphoContextFeature clustering. The size of the dots shows how much of a cluster expresses a gene; the colour shows how much of the expression of a gene is confined to a cluster (see Materials and methods). (<bold>D</bold>) Some of the genes shown to be differentially expressed in the ganglia defined by MorphoContextFeature clustering, plotted on the UMAP representation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Ganglia clusters on the MorphoFeatures representation.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig8-figsupp1-v1.tif"/></fig></fig-group><p>In essence, all three ways of defining ganglia lead to very similar results (<xref ref-type="fig" rid="fig8">Figure 8B</xref>), yet MorphoContextFeature clustering appeared to be the most powerful. Gene clustering failed to distinguish the circumpalpal ganglion, defined both by manual segmentation and MorphoContextFeature clustering, due to the lack of distinct gene expression in the atlas. Manual segmentation failed to distinguish between the dorso-posterior and dorsal-anterior ganglionic nuclei due to the lack of distinct tissue boundaries. These are well defined by gene clustering and subdivided even further by MorphoContextFeature clustering. Unaware of the physical tissue boundaries, MorphoContextFeatures sometimes led to ambiguous assignment of cells neighbouring other tissues, especially cells with strongly different morphology. Such noisy assignment can be noticed, for example, in the neurons bordering muscle cells on the boundary of the cirral ganglia and in the centre of the brain.</p><p>To further examine the contribution of incorporating cellular neighbourhood morphology, we visualise the discovered foregut tissues and ganglia on the MorphoFeatures representation (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> and <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>). This shows that while some of the tissues, such as palpal ganglia or foregut epithelium, are composed of cells of one morphological type, others, e.g., circumpalpal ganglia or infracerebral gland, comprise cells with different morphology and could only be detected when taking into account cellular context as well.</p></sec><sec id="s2-7"><title>Unbiased exploration of foregut tissues</title><p>To further illustrate how MorphoContextFeatures can assist an unbiased exploration of an animal EM volume, we focused on the foregut. Subclustering whenever suitable (<xref ref-type="fig" rid="fig9">Figure 9A</xref>; see Materials and methods), the resulting clusters revealed various foregut tissues including foregut epidermis, ganglia, inner musculature, and a prominent muscle surrounding the foregut. We also found a tissue of secretory neuron-like cells that surround the foregut like a collar (<xref ref-type="fig" rid="fig9">Figure 9B and C</xref>, in green).</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Detailed characterisation of the foregut.</title><p>(<bold>A</bold>) Foregut region clusters plotted on the UMAP representation. (<bold>B</bold>) The foregut tissues, as defined by the MorphoContextFeatures clustering, shown in the animal volume. Scale bar: 10μm. (<bold>C</bold>) A 3D visualisation of the foregut tissues, as defined by the MorphoContextFeatures clustering: all tissues (upper left panel), surrounding muscles removed (upper right panel), both muscle groups removed (lower panel). (<bold>D</bold>) Some of the genes shown to be differentially expressed in the foregut region clusters, plotted on the UMAP representation. (<bold>E</bold>) Gene expression dot plot of the foregut region clustering. The size of the dots shows how much of a cluster expresses a gene; the colour shows how much of the expression of a gene is confined to a cluster (see Materials and methods). The genes mentioned in the text are enboxed. (<bold>F</bold>) The genes specific to the axocord muscles, visualised on the animal volume. Scale bars: 10μm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig9-v1.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Foregut clusters on the MorphoFeatures representation.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig9-figsupp1-v1.tif"/></fig><fig id="fig9s2" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 2.</label><caption><title>Infracerebral gland.</title><p>(<bold>A</bold>) The location of the gland in the head. The neuropil and the secretory cells are pointed up by black arrows, the surrounding muscle layers - by red arrows. (<bold>B</bold>) The shape of the gland and its position relative to the posterior pair of adult eyes (black arrows). (<bold>C</bold>) A cavity likely to be a developing blood vessel (black arrow) on top of the gland.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80918-fig9-figsupp2-v1.tif"/></fig></fig-group><p>Further inspection revealed that the latter structure is located underneath the brain neuropil and close to the neurosecretory endings of the neurosecretory plexus (<xref ref-type="bibr" rid="bib79">Verasztó et al., 2020</xref>) and is bounded by thin layers of longitudinal muscle fibres on both the outer and the inner surface (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2A</xref>). The location of the structure indicated it might be the differentiating infracerebral gland - a neurosecretory gland located beneath the brain assumed to play a role in sensing blood glucose levels (<xref ref-type="bibr" rid="bib5">Backfisch, 2013</xref>; <xref ref-type="bibr" rid="bib8">Baskin, 1974</xref>; <xref ref-type="bibr" rid="bib32">Hofmann, 1976</xref>; <xref ref-type="bibr" rid="bib25">Golding, 1970</xref>). The organ is leaf-shaped and lays between the posterior pair of adult eyes (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2B</xref>) as also described in <xref ref-type="bibr" rid="bib5">Backfisch, 2013</xref> and borders a cavity likely to be a developing blood vessel (<xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2C</xref>). In comparison to the foregut neurons that express sodium channel <italic>scn8aa</italic>, the infracerebral gland cells specifically express homeobox protein <italic>onecut/hnf6</italic> (<xref ref-type="fig" rid="fig9">Figure 9D and E</xref>). We also noted that the tissue stains positive for EdU applied between 3 and 5 dpf, indicating that it is still proliferating.</p><p>We then identified the prominent muscles surrounding the foregut as the anterior extension of the axochord (<xref ref-type="bibr" rid="bib41">Lauri et al., 2014</xref>). Visualising the genes specifically expressed in these muscles, we noticed specific expression of the type I collagen gene <italic>col1a1</italic>, a marker for axochordal tissue, and of muscle junctional gene <italic>junctophilin1</italic> (<xref ref-type="fig" rid="fig9">Figure 9F</xref>). An anterior extension of the axochord around the foregut has been observed in several species and is described in <xref ref-type="bibr" rid="bib41">Lauri et al., 2014</xref>; <xref ref-type="bibr" rid="bib11">Brunet et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Nielsen et al., 2018</xref>.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We presented an automated method of extracting comprehensive representations of cellular morphology from segmented whole-animal EM data. In stark contrast to the currently available methods, ours refrains both from specifying features explicitly and from predefining a concrete task that might skew the method towards extracting specific features: putting emphasis on some features may bias descriptions towards specific morphologies or cell types, hindering exploratory analysis. To obtain extensive generally useful representations, we rely on the latest progress in self-supervised deep learning and set only two criteria for the extracted features: (1) they have to be sufficiently detailed to allow reconstructing the original cell volume and (2) they have to bring visually similar cells in feature space closer to each other than dissimilar ones. We train a deep-learning pipeline optimised for these two conditions and show that it produces rich cellular representations, combining shape-, ultrastructure-, and texture-based features. More precisely, we show that the obtained representations (MorphoFeatures) capture fine morphological peculiarities and group together cells with similar visual appearance and localisation in the animal. Clustering of cells in the MorphoFeature space and taking advantage of an existing gene expression atlas with cellular resolution, we show that the obtained clusters represent units with differential and coherent gene expression.</p><p>Both for morphological type and for tissue detection, we illustrate how MorphoFeatures can facilitate unbiased exploratory analysis aimed at detecting and characterising unusual phenotypes or unusual cell neighbourhoods. Moreover, we show quantitative superiority of MorphoFeatures over a broad range of manually defined features on the tasks of cell classification and symmetric partner detection. We also show that, despite being learned implicitly, MorphoFeatures still correspond to visually understandable properties and can often be interpreted. Finally, adding morphological information from the neighbouring cells (MorphoContextFeatures), we reveal the power of our approach to detect local groups of cells with similar morphology that represent tissues.</p><p>Since our pipeline is based on the fast-evolving field of self-supervised deep learning, we expect it to further improve with its general advancement. For example, a promising direction are models, such as <xref ref-type="bibr" rid="bib26">Grill et al., 2020</xref>, that only rely on positive samples and remove the need to define which cells belong to a different type. Besides MorphoFeatures, we also believe that MorphoContextFeatures, which incorporate neighbourhood information, will potentially benefit from state-of-the-art self-supervised techniques such as contrastive-learning techniques on graphs (<xref ref-type="bibr" rid="bib28">Hassani and Khasahmadi, 2020</xref>; <xref ref-type="bibr" rid="bib78">Velickovic et al., 2019</xref>). Additionally, we expect hardware improvement, i.e., GPU RAM size, to allow training bigger models on larger texture patches of higher resolution that might further refine the quality of morphological representations. Another interesting direction enabled by the modularity of our pipeline is to integrate data from other modalities. Our features currently integrate shape and texture information of cells, but one could also envision enriching cellular descriptions by adding different types of data, e.g., transcriptomics, proteomics, or metabolomics, in a similar manner.</p><sec id="s3-1"><title>Versatility of the MorphoFeatures toolbox</title><p>We envision our pipeline to enable, for the first time, fast systematic investigation of large EM volumes that involves consistent grouping and characterisation of morphological types, discovering particular phenotypes and neighbourhoods, as well as localising cells or tissues in an animal volume that are visually similar to cells or groups of cells of interest. Moreover, we expect MorphoFeatures to be useful for studies where several image volumes of the same species are acquired, be it another sample, a varying phenotype, or a different developmental stage. In such cases, training the pipeline on multiple volumes simultaneously to produce sample-independent features would result in morphological representations that can be used to map cells to their corresponding partners in another volume, enabling unbiased comparative analysis at scale. In addition, in case the other volumes are not segmented, one could use the fine texture part of MorphoFeatures to locate patches with phenotypic characteristic signature or cells with distinct ultrastructure (e.g. muscles and secretory cells). Using fine texture descriptors in such cases can also facilitate further segmentation since the segmentation of the previous volume can be used to define constraints of which textures can co-occur in the cells of the animal. However, given the high sensitivity of neural networks to intensity variations, it will be essential to avoid non-biological systematic change in the visual appearance of cells such as intensity shifts, both within and between the EM volumes.</p><p>The presented pipeline can also be adjusted for other types of data. For example, since our analysis revealed a high variability in neuronal soma morphologies, it appears reasonable to also apply the pipeline to brain EM volumes, where MorphoFeatures could supplement neuron skeleton features for more precise neuronal morphology description. We also expect the approach to contribute to the analysis of cells in other volumetric modalities, such as X-ray holographic nano-tomography (<xref ref-type="bibr" rid="bib39">Kuan et al., 2020</xref>). For light microscopy with membrane staining, we expect the shape part of the MorphoFeatures pipeline to be directly applicable and a useful addition to existing analysis tools. Generally, the versatility of the pipeline is enhanced by its modularity that makes it possible to adapt different parts (e.g. cell shape or nucleus fine texture) to datasets where not all the morphological components are desired or can be extracted.</p></sec><sec id="s3-2"><title>Identification of cell types and cell type families with MorphoFeatures</title><p>Any attempt to define cell types morphologically has so far been hampered by the selection of relevant properties. What are the morphological features that are best suited for recognising cell types? Regarding neurons, the shapes of dendrites and axons, soma size, and spine density have been measured, in combination with physiological properties including resting potential, biophysical properties, and firing rate (<xref ref-type="bibr" rid="bib84">Zeng and Sanes, 2017</xref>). Volume EM has recently allowed considerable expansion of morphometric parameters, including various shape, intensity, and texture features (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>). Using these numerous parameters, it is possible to discern general categories such as neurons, muscle, or midgut cells; yet they do not provide further resolution within these groups. Comparing the MorphoFeatures representations to the predefined features (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>) hints at supreme resolution power that appears to distinguish morphological groupings down to the cell type level (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). For example, the subclusters of rhabdomeric photoreceptors of the adult eye and enteric neurons can not be identified in the explicit features representation. Furthermore, the cluster of foregut muscles revealed by the MorphoFeatures appears to be split, with some cells being found closer to striated muscles and some residing between the groups of neurons and dark neurosecretory cells.</p><p>Most importantly, the cell clusters identified in MorphoFeature space appear to come closest to genetically defined cell types, as demonstrated here, among others, for different types of mechano-sensory cells or cell types of the apical nervous system. Interestingly, the MorphoFeatures also unravel morphological cell types without a genetic match in the cell type atlas (such as the enteric neurons, see above). This is most likely due to the yet limited coverage of the atlas and expected to improve with the continuous addition of genes to the atlas. In particular, the integration of other multimodal omics datasets such as single cell transcriptomics data will be highly rewarding to improve the match between morphological and molecularly defined cell types, which is the ultimate aim of cell type characterisation and categorisation (<xref ref-type="bibr" rid="bib84">Zeng and Sanes, 2017</xref>).</p></sec><sec id="s3-3"><title>Towards an unbiased whole-body identification of tissues and organs</title><p>The current revolution in the generation of giant volume EM datasets including representations of entire bodies calls for new strategies in the automated processing and comparison of such datasets. This not only involves segmentation and characterisation at the cellular level but also requires the automated identification of discernable structures at the level of tissues and organs. So far, the recognition of tissues in larger volumes was achieved by manual segmentation. We now present a powerful new algorithm to recognise tissues and organs via their co-clustering in the MorphoContextFeature space, which takes into account similarities between cell neighbourhoods. Using this algorithm, we reproduce and partially outperform tissue recognition via manual segmentation - as exemplified for the ganglionic nuclei of the nereid brain. Beyond that, we present a comprehensive account of foregut tissues, without prior manual segmentation. This analysis reveals a rich collection of tissues including an anterior extension of the axochord surrounding the foregut (<xref ref-type="bibr" rid="bib41">Lauri et al., 2014</xref>) and the infracerebral gland (<xref ref-type="bibr" rid="bib8">Baskin, 1974</xref>; <xref ref-type="bibr" rid="bib25">Golding, 1970</xref>), an adenohypophysis-like neurosecretory tissue that we identify as anterior bilateral extensions of the foregut roof.</p><p>While the manual segmentation of tissues mostly relies on the detection of tissue boundaries, our automated tissue detection equally processes the morphological similarities of its constituting cells. This enables tissue separation even in cases where morphological boundaries appear to be absent, as is the case for the anterior and posterior dorsal ganglionic nuclei, which are lacking a clear boundary but are composed of morphologically different cell types that are also genetically distinct (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>).</p></sec><sec id="s3-4"><title>Conclusion</title><p>The revolution in multi-omics techniques and volume EM has unleashed enormous potential in generating multimodal atlases for tissues as well as entire organs and animals. The main unit of reference in these atlases is the cell type, which has been defined, and is manifest, both molecularly and morphologically. Our work now provides a versatile new tool to recognise cell types in an automated, unbiased, and comprehensive manner solely based on ultrastructure and shape. For the first time, the MorphoFeatures will allow us to compare and identify corresponding cell types in datasets of the same species, but also between species, on purely morphological grounds.</p><p>Furthermore, the MorphoContextFeatures automatically recognise tissues and organs across entire bodies, opening up new perspectives in comparative anatomy. Without previous knowledge, we can obtain the full complement of tissues and organs for a given specimen and quantitatively compare them to those of other specimens of the same or other species. In combination with cell type-, tissue-, and organ-specific expression data, this will enable us to identify corresponding traits in an unbiased manner and with unprecedented accuracy.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Data and preprocessing</title><sec id="s4-1-1"><title>Data</title><p>We used the dataset of the marine annelid <italic>P. dumerilii</italic> at late nectochaete stage (6 dpf) from <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>. The dataset comprised an SBEM volume of the complete worm in 10 × 10 × 25 nm resolution, 3D cell and nucleus segmentation masks as well as Whole Mount In-Situ Hybridisation (WMISH) gene expression maps. The segmentation contains 11,402 cells with nuclei that belong to multiple genetically and morphologically distinct cell types. It is important to note that for neurons only somata were segmented since the imaging resolution did not allow for neurite tracing. The gene expression maps cover mainly differentiation genes and transcription factors, which were registered onto the EM volume, exploiting high stereotypy of the <italic>Platynereis</italic> at this stage of development. Gene expression values were also assigned to the EM segmented cells. The maps contain 201 genes and 4 EdU proliferation stainings. We refer to <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref> for further details. Since the intensity correction algorithm that was used in the EM volume performed unreliably in regions close to the boundaries of the volume in the z-axis, we excluded all cells located within the region of uncertain intensity assignment quality. These regions correspond to parts of the animal’s head and the pygidium.</p></sec><sec id="s4-1-2"><title>Preprocessing</title><p>The EM dataset is used to extract the three morphological components separately for cytoplasm and nuclei as follows. Due to the limitations of the GPU memory, for the coarse texture, a central crop is taken from a downsampled version of the data (80 × 80 × 100 nm). The centre of mass of the nucleus is determined, and a bounding box of size 144 × 144 ×144 pixels (cytoplasm) or 104 × 104 × 104 (nuclei) is taken around it. During prediction, the bounding box is increased to 320 × 320 × 320 (cytoplasm) and 160 × 160 × 160 (nuclei) to fit more information. Fine texture patches have a higher resolution (20 × 20 × 25 nm) and are taken separately from cytoplasm and nucleus. For this, the bounding box surrounding cytoplasm or nucleus is split into cubes of 32 × 32 × 32 pixels, and only those cubes are considered which are less than 50% empty. For both coarse and fine texture, the data is normalised to the range of [0, 1], and for cytoplasm crops, the nuclei are masked out and vice versa.</p><p>In order to obtain point clouds and normals of the membrane surfaces, we first converted the segmented 3D voxel volumes into triangular meshes. More specifically, we first applied the marching cubes algorithm <xref ref-type="bibr" rid="bib43">Lorensen and Cline, 1987</xref>; <xref ref-type="bibr" rid="bib77">van der Walt et al., 2014</xref> followed by Laplacian smoothing (<xref ref-type="bibr" rid="bib62">Schroeder et al., 1998</xref>) to obtain smooth triangular surface meshes. These meshes were then simplified and made watertight using <xref ref-type="bibr" rid="bib71">Stutz and Geiger, 2020</xref>; <xref ref-type="bibr" rid="bib14">Cignoni et al., 2008</xref>. Having obtained smooth triangular meshes enables us to compute consistent normals and to apply complex deformations to the underlying surface. During training and inference, the meshes and their surface normals are sampled uniformly to be then processed by the neural network.</p></sec></sec><sec id="s4-2"><title>Unsupervised deep-learning pipeline</title><p>This section describes our proposed algorithm in more detail. The training and prediction pipeline is implemented in Pytorch (<xref ref-type="bibr" rid="bib54">Paszke et al., 2019</xref>). On a high level, our model consists of six encoder networks which leverage graph convolutional as well as regular convolutional layers in order to learn shape and texture aspects of a cell’s morphology. We are using self-supervised learning in combination with stochastic gradient descent in order to obtain good network parameters. The objective that is optimised is a weighted sum of the NT-Xent loss (<xref ref-type="bibr" rid="bib13">Chen et al., 2020</xref>) and the mean squared error and can be interpreted as an unbiased proxy task. We will detail and elaborate on these aspects in the following sections.</p><sec id="s4-2-1"><title>Neural network model</title><p>The proposed network, as depicted in (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), contains six encoders which are encoding shape, coarse, and fine-grained texture of cells and nuclei.</p><p>The shape encoder takes as input a set of points in the form of 3D Euclidean coordinates as well as the corresponding normals (see Data and preprocessing) and outputs an N-dimensional shape feature vector. The network itself is a DeepGCN model (<xref ref-type="bibr" rid="bib42">Li et al., 2019</xref>), which is a deep neural network for processing unstructured sets such as point clouds. Conceptually, it consists of three building blocks. In the first block, k-nearest neighbour graphs are built dynamically in order to apply graph convolutions. This can be interpreted as pointwise passing and local aggregation of geometric information which is repeated multiple times. In the second block, the pointwise aggregated geometric information is fused by passing it through a convolutional layer followed by a max pooling operation that condenses the information into a high-dimensional latent vector. Finally, the obtained latent vector is passed through a multilayer perceptron to obtain the shape component of the overall feature vector.</p><p>The encoding of fine texture details and coarser contextual information is done using the encoder of a U-Net (<xref ref-type="bibr" rid="bib58">Ronneberger et al., 2015</xref>). The fully convolutional network reduces spatial information gradually by employing three blocks that consist of 3D convolutional layers with ELU activations (<xref ref-type="bibr" rid="bib15">Clevert et al., 2015</xref>) separated by 3DMaxPooling layers. The number of feature maps starts with 64, increases twofold in each block, and afterwards gets reduced to 80 by an additional convolutional layer followed by an ELU activation. In the end, a global average pooling operation is applied to reduce feature dimensionality from 3D to 2D. To ensure that empty regions of the data are not affecting the final feature vector, they are excluded for this final pooling.</p></sec><sec id="s4-2-2"><title>Training</title><p>In order to find suitable representations, we leverage self-supervised contrastive learning (CL; <xref ref-type="bibr" rid="bib13">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Hadsell et al., 2006</xref>). The idea is to create a representation space in which morphologically similar cells are embedded close to each other, while dissimilar ones are embedded far apart. For learning representations of shape, we follow a purely contrastive approach, whereas for texture representations, we combine the CL objective with an autoencoder reconstruction loss. Both are described in more detail below.</p><p>The training procedure for CL is the same for shape and texture representations. First, we randomly sample a mini batch of cells or nuclei. Two augmented views are then created for each sample in the batch (see Data augmentations) and concatenated to form an augmented batch of twice the size. The augmented batch is passed through the neural network encoder to compute representations. Those representations that share the same underlying sample are considered as positive pairs, whereas the others are considered as negative pairs. The NT-Xent loss (<xref ref-type="bibr" rid="bib13">Chen et al., 2020</xref>) is then used to score the representations based on their similarity and, therefore, encourages the network to embed positive pairs closer to each other than negative pairs.</p><p>For learning shape representations, we first augmented all cell and nucleus meshes using biharmonic deformation fields and as-rigid-as-possible transformations in a preprocessing step (see Data augmentation). More specifically, we computed 10 random views for each membrane surface. The augmented meshes are then used for training, i.e., we compute surface normals and sample 1024 points and their corresponding normals which form the inputs of the shape encoders. In each training iteration, we sampled 48 cells/nuclei, which resulted in a batch size of 96. Adam (<xref ref-type="bibr" rid="bib36">Kingma and Ba, 2014</xref>) with a learning rate of 0.0002 and a weight decay of 0.0004 was used to update the network parameters. The hyperparameters were obtained by a random search.</p><p>In order to learn representations of texture, we combine the NT-Xent loss with an autoencoder reconstruction objective. The latent representations, taken before the dimensionality reduction step of global average pooling, are further processed by a symmetric decoder part of the UNet network with only one block, which aims to reconstruct the network inputs. The reconstructions are then compared to the original inputs and scored by a mean squared error loss. Additionally, an L2-Norm loss was applied to flattened bottleneck features to restrict the range of possible values. The final loss is thus a weighted sum of the NT-Xent, the mean squared error loss, and the L2-Norm loss. The batch size was set to 12 and 16 for cell and nuclei coarse texture and to 32 for fine texture. For training, we used Adam (<xref ref-type="bibr" rid="bib36">Kingma and Ba, 2014</xref>) as optimizer with a learning rate of 0.0001 and a weight decay of 0.00005. The model was evaluated every 100 iterations, and the best result was saved with a patience of 0.95. The learning rate was reduced by 0.98 whenever the validation score did not show improvement with a patience of 0.95.</p></sec><sec id="s4-2-3"><title>Data augmentation</title><p>A key component for successful contrastive representation learning are suitable data augmentations. In our context, applying augmentations to a cell can be interpreted as creating morphological variations of a cell. By creating positive and negative pairs (cf. previous section), the network learns to distinguish true variations from noise and non-essential ones. It is therefore important to find a suitable set of transformations that distort the right features of a cell and do not change its morphological identity.</p><p>As augmentations for shape, we leveraged biharmonic and as-rigid-as-possible transformations (<xref ref-type="bibr" rid="bib9">Botsch and Kobbelt, 2004</xref>; <xref ref-type="bibr" rid="bib69">Sorkine and Alexa, 2007</xref>, <xref ref-type="bibr" rid="bib35">Jacobson et al., 2018</xref>) as they deform cellular and nuclear membranes in a way that retains most characteristic geometric features of the original surface. In order to compute these deformations, we sample so-called handle regions on the membrane surface. These regions are translated in a random, normal, or negative normal direction. The deformation constraint is then propagated to the rest of the surface either via a biharmonic deformation field or an as-rigid-as-possible deformation. We combine these deformations with random anisotropic scaling, rotations, and/or symmetry transformations.</p><p>To create positive samples for the contrastive training of texture, we used random flips and rotations and elastic transformations of the data.</p></sec><sec id="s4-2-4"><title>Inference</title><p>In order to compute the MorphoFeatures representation for the whole animal, we processed each cell’s cellular and nuclear shape and texture, as described in previous sections, with our trained networks. For the fine texture patches to get one feature vector per cell, the feature vectors of all the patches from a cell were averaged. Each cell is represented by a 480-dimensional vector that consists of a nuclear and cellular part. Each of these parts consists of a shape, coarse, and fine-grained texture 80-dimensional feature vector. This vector size was found empirically using the bilateral distance as a metric. Vectors of bigger size showed similar performance, but contained more redundant features.</p></sec></sec><sec id="s4-3"><title>Quantitative feature evaluation</title><p>In this section, we describe our feature evaluation experiments in more detail. In order to assess the quality of MorphoFeatures quantitatively, we conducted cell-type prediction and bilateral pair analysis experiments. We use the same set of explicit features that were used in <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref> for analysing cellular morphology as a baseline for comparisons.</p><sec id="s4-3-1"><title>Explicit feature baseline</title><p>The explicitly defined representation includes shape features (volume in microns, extent, equivariant diameter, major and minor axes, surface area, sphericity, and max radius), intensity features (different quantiles of intensity mean, median, and SD), and Haralick texture features. These features were calculated, whenever appropriate, for cell cytoplasm, nucleus, and chromatin segmentation. The calculated features were taken from <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>: <ext-link ext-link-type="uri" xlink:href="https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/morphology.tsv">https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/morphology.tsv</ext-link> for cellular segmentation and from <ext-link ext-link-type="uri" xlink:href="https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-nuclei/morphology.tsv">https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-nuclei/morphology.tsv</ext-link> for nucleus and chromatin segmentation. The UMAP of these features was taken from <ext-link ext-link-type="uri" xlink:href="https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/morphology_umap.tsv">https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/morphology_umap.tsv</ext-link>.</p></sec><sec id="s4-3-2"><title>Classification of visibly distinct groups of cells</title><p>In order to verify that MorphoFeatures can distinguish visibly different groups of cells, we evaluated the prediction accuracy of a logistic regression model that was trained on a small dataset of 390 annotated cells taken from <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The original annotation contained 571 neurons, 141 epithelial cells, 55 midgut cells, 53 muscle cells, 51 secretory cells, 41 dark cells, and 25 ciliated cells. We first corrected the assignments deemed incorrect by the annotators with expertise in <italic>Platynereis</italic> cell types. Then for the classes that contained less than 60 cells, we manually annotated more cells. From bigger classes, we selected 60 cells, selecting the most morphologically diverse ones when possible. For ciliated cells, we were only able to locate 30 cells in the whole animal.</p><p>First, we computed the MorphoFeatures representation for each cell in the animal (excluding cells with severe segmentation errors) and standardised features across the whole dataset. We then trained and validated a logistic regression model using a stratified k-folds cross validator with fivefolds. We used the implementations of logistic regression, stratified k-fold from Scikit-learn (<xref ref-type="bibr" rid="bib55">Pedregosa et al., 2011</xref>) with the following arguments: C=1, solver = ‘lbfgs’. To prevent potential overfitting due to the high number of features, we used feature agglomeration (<xref ref-type="bibr" rid="bib55">Pedregosa et al., 2011</xref>; n_clusters = 100) to reduce the number of similar features.</p></sec><sec id="s4-3-3"><title>Bilateral pair analysis</title><p>The criterion of classifying distinct groups of cells in the previous section can be seen as a coarse and global way of evaluating MorphoFeatures. In this section, we describe how we used the animal’s bilateral symmetry to have a more fine-grained measure for quantitatively evaluating nuanced morphological differences. The measure, which we will refer to as bilateral distance, was introduced in <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref> and leverages the fact that most cells in the animal have a symmetric partner cell on the other side of the mediolateral axis. The bilateral distance is computed in the following way: first, we determine for each cell <inline-formula><mml:math id="inf1"><mml:mi>c</mml:mi></mml:math></inline-formula> a group of N potential partner cells <inline-formula><mml:math id="inf2"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> on the opposite side. We then compute a ranking based on the Euclidean distances <inline-formula><mml:math id="inf3"><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">’</mml:mi></mml:mrow></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:math></inline-formula> in feature space between cell <inline-formula><mml:math id="inf4"><mml:mi>c</mml:mi></mml:math></inline-formula> and all other cells <inline-formula><mml:math id="inf5"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">’</mml:mi></mml:mrow><mml:mo>≠</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula>. Since <inline-formula><mml:math id="inf6"><mml:mi>c</mml:mi></mml:math></inline-formula>’s symmetric partner has the same morphology, up to natural variability, it should be among the closest cells in the ranking. We then determine the highest ranked partner cell as <inline-formula><mml:math id="inf7"><mml:mrow><mml:mtext>rank</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In doing this for all cells, we create a list containing the ranking positions of cells to their symmetric partner. The bilateral distance is then defined as the median of this list and can be interpreted as a measure of how close two morphologically very similar cells are on average in feature space.</p></sec></sec><sec id="s4-4"><title>Adding neighbour morphological information</title><p>This section describes how neighbourhood information was incorporated into MorphoFeatures in order to obtain MorphoContextFeatures. First, we built a region adjacency graph from the cell segmentation masks using scikit-image (<xref ref-type="bibr" rid="bib77">van der Walt et al., 2014</xref>). The obtained graph reflects the animal’s cellular connectivity, i.e., cells are represented by nodes in the graph, and nodes are connected by an edge if the corresponding cells are neighbours. We then assign node attributes by mapping the MorphoFeatures representation of a cell to the corresponding node. Neighbourhood features are then obtained by computing the mean over a node’s MorphoFeatures vector and the MorphoFeatures vectors of its surrounding neighbours. Finally, a cell’s neighbourhood features are concatenated with the cell’s MorphoFeatures to obtain the MorphoContextFeatures representation.</p><p>We benchmarked multiple methods for aggregating feature vectors of neighbouring cells, including taking the mean, the median, and approaches based on graph neural networks. Among these methods, taking the mean showed to be the best tradeoff between performance and simplicity. Compared to taking the median, the mean features showed to give a higher accuracy on predicting morphotypes and have a lower bilateral distance. The features extracted by graph neural networks exhibited unstable behaviour, both in predictiveness of morphotypes as well as bilateral distance.</p></sec><sec id="s4-5"><title>Feature analysis</title><sec id="s4-5-1"><title>UMAP representation analysis</title><p>Visualisation was done using the UMAP package (<xref ref-type="bibr" rid="bib47">McInnes et al., 2018</xref>) with the following parameters: Euclidean metric, 15 neighbours, and 0 minimal distance. Cell metadata, such as annotated types, animal regions, or gene expression, was plotted on the resulting UMAP.</p></sec><sec id="s4-5-2"><title>Clustering</title><p>Cell clustering was done according to the following procedure. First, the features were standardised to zero mean and unit variance. Then, following <xref ref-type="bibr" rid="bib20">Dubourg-Felonneau et al., 2021</xref>, a weighted k-neighbour graph was constructed from MorphoFeatures representations of all the animal cells using the UMAP algorithm (<xref ref-type="bibr" rid="bib47">McInnes et al., 2018</xref>) with the following arguments: n_neighbours = 20, metric=‘Euclidean’, and min_dist = 0. Afterwards the Leiden algorithm (<xref ref-type="bibr" rid="bib74">Traag et al., 2019</xref>) for community detection was used to partition the resulting graph, using the CPMVertexPartition method and a resolution parameter of 0.004. For more fine-grained clustering, the resolution was increased to 0.1 to split apart secretory and ciliary band cells, 0.2 to split photoreceptors and enteric neurons, 0.07 to split the midgut cluster, 0.005 for muscle neighbourhood cluster (n_neighbours for the UMAP clustering was adjusted to 10), and 0.01 for splitting foregut neurons and epithelial cells.</p><p>One of the clusters was discarded since visual examination showed it contained only cells with split segmentation errors that ‘cut’ through the nucleus area along one of the axes (examples shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Other types of segmentation errors in the dataset mostly did not influence the clustering quality for multiple reasons. First, most errors are rather randomly distributed and not consistent, meaning the size of falsely attached fragments is highly variable, and the location of segmentation splits is not consistent. Secondly, many of the errors are cell type specific. For example, midgut cells often got merged with midgut lumen, (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, dark green) and muscle cells often experienced splits or merges of other muscle pieces (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, dark and light orange). Small merges that are not cell type specific (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, light green) also minimally affected the assignment. However, substantial merges could lead to a cell being grouped with a different cell type, for example, a neuron that got merged with a piece of epithelial cell bigger than the neuron size (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, yellow-green) got grouped together with epithelial cells, not neurons.</p><p>To visualise average shape and texture of a cluster (4B,C), a median feature vector was computed across the cluster, and the cell with the feature vector closest to the median one was taken as the average cell.</p></sec><sec id="s4-5-3"><title>Gene expression analysis</title><p>To produce the cluster gene expression ‘dot plots’ (<xref ref-type="fig" rid="fig4">Figures 4A</xref>, <xref ref-type="fig" rid="fig5">5B</xref>, <xref ref-type="fig" rid="fig8">8C</xref> and <xref ref-type="fig" rid="fig9">9E</xref>), the gene expression values for each cell were taken from <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref> at <ext-link ext-link-type="uri" xlink:href="https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/genes.tsv">https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/genes.tsv</ext-link>. Similar to <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>, for each cluster and for each gene, we calculated three values: (A) the mean expression in the cluster, (B) the fraction of the total animal/region expression within this cluster (the sum of expression in this cluster divided by the sum of the expression in the whole animal or the selected region), and (C) the specificity defined as <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Intuitively, <inline-formula><mml:math id="inf9"><mml:mi>A</mml:mi></mml:math></inline-formula> indicates how much of the cells in a given cluster express a given gene, <inline-formula><mml:math id="inf10"><mml:mi>B</mml:mi></mml:math></inline-formula> shows how selective this gene for this cluster is, and <inline-formula><mml:math id="inf11"><mml:mi>C</mml:mi></mml:math></inline-formula> is a harmonic mean of the two values. To create a dot plot, only the genes with <inline-formula><mml:math id="inf12"><mml:mi>C</mml:mi></mml:math></inline-formula> value above 0.15 were used. For the neuron clusters 1, 3, and 5 (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), the threshold was increased to 0.2 due to a high number of specifically expressed genes available. For the midgut clusters (<xref ref-type="fig" rid="fig5">Figure 5</xref>) to show only the genes differing between the two clusters, the genes were removed, where the specificity was above 0.15 in both clusters but differed by less than 0.4. The size of the dots in the plots reflects <inline-formula><mml:math id="inf13"><mml:mi>A</mml:mi></mml:math></inline-formula>, and the colour corresponds to <inline-formula><mml:math id="inf14"><mml:mi>B</mml:mi></mml:math></inline-formula>. The cluster with the most gene expression was determined, and the remaining clusters were sorted by their similarity to it. To illustrate some of the genes showing differential expression in these groups (<xref ref-type="fig" rid="fig5">Figures 5</xref>, <xref ref-type="fig" rid="fig8">8</xref> and <xref ref-type="fig" rid="fig9">9</xref>), the corresponding regions of the UMAP representation were cut out, and these genes were plotted on top.</p></sec><sec id="s4-5-4"><title>Feature specificity analysis</title><p>The feature specificity ‘dot plots’ were generated as follows. The MorphoFeatures of all cells were processed by first clipping the values below –2 and above +2 (less than 5% of values affected) and standardised to the range of [0, 1]. Then three values <inline-formula><mml:math id="inf15"><mml:mi>A</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf16"><mml:mi>B</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf17"><mml:mi>C</mml:mi></mml:math></inline-formula> were calculated similarly to gene expression dot plots. However, since smaller clusters would have smaller sum of any feature, we additionally normalised the value <inline-formula><mml:math id="inf18"><mml:mi>B</mml:mi></mml:math></inline-formula> by diving it by the relative cluster size. To create a dot plot, only the features with <inline-formula><mml:math id="inf19"><mml:mi>C</mml:mi></mml:math></inline-formula> value above 1 were used.</p><p>To illustrate some features specific to clusters (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), we first selected a set of features from the dot plot with high specificity in a given cluster and low specificity in other clusters. We then showed their extremes with cells that have the respective minimal/maximal value of this feature (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The cells with merge errors of more than half of a cell size were ignored. In order to visualise the 3D cells and nuclei, we selected a 2D view that we presume shows the shape or texture feature common between all four selected cells with minimal or maximal value of the feature.</p><p>Visualising the extremes of various features from the dot plot, we found most features (80–90%) to be relatively easy to interpret. More specifically, 82% shape features, 80% fine texture features, and 90% coarse texture features could be visually mapped to explainable properties in less than 2 min of observing the data. However, when selecting a random subset (10–20 random features) from each morphological component, the number of clearly interpretable features was lower, 65% for fine, 70% for coarse texture, and 45% for shape features.</p></sec><sec id="s4-5-5"><title>Ganglionic nuclei</title><p>To compare ganglionic nuclei (<xref ref-type="fig" rid="fig8">Figure 8</xref>), we used manually segmented regions from <ext-link ext-link-type="uri" xlink:href="https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/ganglia_ids.tsv">https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/ganglia_ids.tsv</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/regions.tsv">https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/regions.tsv</ext-link> and gene clusters from <ext-link ext-link-type="uri" xlink:href="https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/gene_clusters.tsv">https://github.com/mobie/platybrowser-project/blob/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/gene_clusters.tsv</ext-link> (<xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>).</p></sec></sec><sec id="s4-6"><title>Data visualisation</title><p>We used matplotlib (<xref ref-type="bibr" rid="bib34">Hunter, 2007</xref>) and seaborn (<xref ref-type="bibr" rid="bib81">Waskom, 2021</xref>) for plotting the data, MoBIE (<xref ref-type="bibr" rid="bib53">Pape et al., 2022</xref>) for visualising cells in the EM volume, and vedo (<xref ref-type="bibr" rid="bib50">Musy et al., 2022</xref>) for visualising meshed cells and tissues.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Validation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-80918-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The study relies on the publicly available data from <xref ref-type="bibr" rid="bib80">Vergara et al., 2021</xref>. The generated cell representations are available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/kreshuklab/MorphoFeatures">https://github.com/kreshuklab/MorphoFeatures</ext-link>.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Vergara</surname><given-names>P</given-names></name><name><surname>Meechan</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Whole-body integration of gene expression and single-cell morphology</data-title><source>EMPIAR</source><pub-id pub-id-type="accession" xlink:href="https://www.ebi.ac.uk/empiar/EMPIAR-10365/">10365</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank all the members of the Kreshuk and Uhlmann group, especially Constantin Pape, for helpful comments. We further thank Christian Tischer for implementing features essential for biological analysis and exploration in MoBIE. We thank Hernando M Vergara for assistance in cell type recognition and Kimberly I Meechan for help with comparison to explicit features. We would also like to acknowledge the support of the EMBL IT Services, especially Juri Pecar for maintaining the GPU resources. The work was supported by EMBL internal funds and by a grant from the European Research Council (NeuralCellTypeEvo 788921) to DA.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arendt</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The evolution of cell types in animals: emerging principles from molecular studies</article-title><source>Nature Reviews. Genetics</source><volume>9</volume><fpage>868</fpage><lpage>882</lpage><pub-id pub-id-type="doi">10.1038/nrg2416</pub-id><pub-id pub-id-type="pmid">18927580</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arendt</surname><given-names>D</given-names></name><name><surname>Musser</surname><given-names>JM</given-names></name><name><surname>Baker</surname><given-names>CVH</given-names></name><name><surname>Bergman</surname><given-names>A</given-names></name><name><surname>Cepko</surname><given-names>C</given-names></name><name><surname>Erwin</surname><given-names>DH</given-names></name><name><surname>Pavlicev</surname><given-names>M</given-names></name><name><surname>Schlosser</surname><given-names>G</given-names></name><name><surname>Widder</surname><given-names>S</given-names></name><name><surname>Laubichler</surname><given-names>MD</given-names></name><name><surname>Wagner</surname><given-names>GP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The origin and evolution of cell types</article-title><source>Nature Reviews. Genetics</source><volume>17</volume><fpage>744</fpage><lpage>757</lpage><pub-id pub-id-type="doi">10.1038/nrg.2016.127</pub-id><pub-id pub-id-type="pmid">27818507</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arendt</surname><given-names>D</given-names></name><name><surname>Bertucci</surname><given-names>PY</given-names></name><name><surname>Achim</surname><given-names>K</given-names></name><name><surname>Musser</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Evolution of neuronal types and families</article-title><source>Current Opinion in Neurobiology</source><volume>56</volume><fpage>144</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.01.022</pub-id><pub-id pub-id-type="pmid">30826503</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arendt</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Elementary nervous systems</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>376</volume><elocation-id>20200347</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2020.0347</pub-id><pub-id pub-id-type="pmid">33550948</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Backfisch</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Regulatory tools and the characterization of insulinergic cells in the annelid Platynereis dumerilii</article-title><publisher-name>Zentrum für Molekulare Biologie</publisher-name></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bae</surname><given-names>JA</given-names></name><name><surname>Baptiste</surname><given-names>M</given-names></name><name><surname>Bodor</surname><given-names>AL</given-names></name><name><surname>Brittain</surname><given-names>D</given-names></name><name><surname>Buchanan</surname><given-names>J</given-names></name><name><surname>Bumbarger</surname><given-names>DJ</given-names></name><name><surname>Castro</surname><given-names>MA</given-names></name><name><surname>Celii</surname><given-names>B</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Collman</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Functional Connectomics Spanning Multiple Areas of Mouse Visual Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.07.28.454025</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Barad</surname><given-names>BA</given-names></name><name><surname>Medina</surname><given-names>M</given-names></name><name><surname>Fuentes</surname><given-names>D</given-names></name><name><surname>Wiseman</surname><given-names>RL</given-names></name><name><surname>Grotjahn</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A Surface Morphometrics Toolkit to Quantify Organellar Membrane Ultrastructure Using Cryo-Electron Tomography</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.01.23.477440</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baskin</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Further observations on the fine structure and development of the infracerebral complex (“ infracerebral gland ”) of Nereis limnicola (Annelida, Polychaeta)</article-title><source>Cell and Tissue Research</source><volume>154</volume><fpage>519</fpage><lpage>531</lpage><pub-id pub-id-type="doi">10.1007/BF00219671</pub-id><pub-id pub-id-type="pmid">4442111</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botsch</surname><given-names>M</given-names></name><name><surname>Kobbelt</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>An intuitive framework for real-time freeform modeling</article-title><source>ACM Transactions on Graphics</source><volume>23</volume><fpage>630</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1145/1015706.1015772</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyle</surname><given-names>MJ</given-names></name><name><surname>Seaver</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Developmental expression of FOXA and GATA genes during gut formation in the polychaete annelid, Capitella sp. I</article-title><source>Evolution &amp; Development</source><volume>10</volume><fpage>89</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1111/j.1525-142X.2007.00216.x</pub-id><pub-id pub-id-type="pmid">18184360</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunet</surname><given-names>T</given-names></name><name><surname>Fischer</surname><given-names>AH</given-names></name><name><surname>Steinmetz</surname><given-names>PR</given-names></name><name><surname>Lauri</surname><given-names>A</given-names></name><name><surname>Bertucci</surname><given-names>P</given-names></name><name><surname>Arendt</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The evolutionary origin of bilaterian smooth and striated myocytes</article-title><source>eLife</source><volume>5</volume><elocation-id>e19607</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19607</pub-id><pub-id pub-id-type="pmid">27906129</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Caicedo</surname><given-names>JC</given-names></name><name><surname>McQuin</surname><given-names>C</given-names></name><name><surname>Goodman</surname><given-names>A</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Carpenter</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Weakly supervised learning of single-cell feature embeddings</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>9309</fpage><lpage>9318</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00970</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A simple framework for contrastive learning of visual representations</article-title><conf-name>In International conference on machine learning PMLR</conf-name><fpage>1597</fpage><lpage>1607</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cignoni</surname><given-names>P</given-names></name><name><surname>Callieri</surname><given-names>M</given-names></name><name><surname>Corsini</surname><given-names>M</given-names></name><name><surname>Dellepiane</surname><given-names>M</given-names></name><name><surname>Ganovelli</surname><given-names>F</given-names></name><name><surname>Ranzuglia</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Meshlab: an open-source mesh processing tool</article-title><conf-name>In Eurographics Italian chapter conference</conf-name><fpage>129</fpage><lpage>136</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Clevert</surname><given-names>DA</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Hochreiter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fast and Accurate Deep Network Learning by Exponential Linear Units (Elus)</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.07289">https://arxiv.org/abs/1511.07289</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>AG</given-names></name><name><surname>Arnone</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Fluorescent in situ hybridization reveals multiple expression domains for spbrn1/2/4 and identifies a unique ectodermal cell type that Co-expresses the ParaHox gene splox</article-title><source>Gene Expression Patterns</source><volume>9</volume><fpage>324</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1016/j.gep.2009.02.005</pub-id><pub-id pub-id-type="pmid">19250980</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cook</surname><given-names>SJ</given-names></name><name><surname>Jarrell</surname><given-names>TA</given-names></name><name><surname>Brittin</surname><given-names>CA</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Bloniarz</surname><given-names>AE</given-names></name><name><surname>Yakovlev</surname><given-names>MA</given-names></name><name><surname>Nguyen</surname><given-names>KCQ</given-names></name><name><surname>Tang</surname><given-names>LT-H</given-names></name><name><surname>Bayer</surname><given-names>EA</given-names></name><name><surname>Duerr</surname><given-names>JS</given-names></name><name><surname>Bülow</surname><given-names>HE</given-names></name><name><surname>Hobert</surname><given-names>O</given-names></name><name><surname>Hall</surname><given-names>DH</given-names></name><name><surname>Emmons</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Whole-animal connectomes of both <italic>Caenorhabditis elegans</italic> sexes</article-title><source>Nature</source><volume>571</volume><fpage>63</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1352-7</pub-id><pub-id pub-id-type="pmid">31270481</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doan</surname><given-names>M</given-names></name><name><surname>Sebastian</surname><given-names>JA</given-names></name><name><surname>Caicedo</surname><given-names>JC</given-names></name><name><surname>Siegert</surname><given-names>S</given-names></name><name><surname>Roch</surname><given-names>A</given-names></name><name><surname>Turner</surname><given-names>TR</given-names></name><name><surname>Mykhailova</surname><given-names>O</given-names></name><name><surname>Pinto</surname><given-names>RN</given-names></name><name><surname>McQuin</surname><given-names>C</given-names></name><name><surname>Goodman</surname><given-names>A</given-names></name><name><surname>Parsons</surname><given-names>MJ</given-names></name><name><surname>Wolkenhauer</surname><given-names>O</given-names></name><name><surname>Hennig</surname><given-names>H</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Wilson</surname><given-names>A</given-names></name><name><surname>Acker</surname><given-names>JP</given-names></name><name><surname>Rees</surname><given-names>P</given-names></name><name><surname>Kolios</surname><given-names>MC</given-names></name><name><surname>Carpenter</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Objective assessment of stored blood quality by deep learning</article-title><source>PNAS</source><volume>117</volume><fpage>21381</fpage><lpage>21390</lpage><pub-id pub-id-type="doi">10.1073/pnas.2001227117</pub-id><pub-id pub-id-type="pmid">32839303</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driscoll</surname><given-names>MK</given-names></name><name><surname>Welf</surname><given-names>ES</given-names></name><name><surname>Jamieson</surname><given-names>AR</given-names></name><name><surname>Dean</surname><given-names>KM</given-names></name><name><surname>Isogai</surname><given-names>T</given-names></name><name><surname>Fiolka</surname><given-names>R</given-names></name><name><surname>Danuser</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Robust and automated detection of subcellular morphological motifs in 3D microscopy images</article-title><source>Nature Methods</source><volume>16</volume><fpage>1037</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0539-z</pub-id><pub-id pub-id-type="pmid">31501548</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dubourg-Felonneau</surname><given-names>G</given-names></name><name><surname>Shams</surname><given-names>S</given-names></name><name><surname>Akiva</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Protein Organization with Manifold Exploration and Spectral Clustering</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.12.08.471858</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eulenberg</surname><given-names>P</given-names></name><name><surname>Köhler</surname><given-names>N</given-names></name><name><surname>Blasi</surname><given-names>T</given-names></name><name><surname>Filby</surname><given-names>A</given-names></name><name><surname>Carpenter</surname><given-names>AE</given-names></name><name><surname>Rees</surname><given-names>P</given-names></name><name><surname>Theis</surname><given-names>FJ</given-names></name><name><surname>Wolf</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reconstructing cell cycle and disease progression using deep learning</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>463</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-00623-3</pub-id><pub-id pub-id-type="pmid">28878212</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falk</surname><given-names>T</given-names></name><name><surname>Mai</surname><given-names>D</given-names></name><name><surname>Bensch</surname><given-names>R</given-names></name><name><surname>Çiçek</surname><given-names>Ö</given-names></name><name><surname>Abdulkadir</surname><given-names>A</given-names></name><name><surname>Marrakchi</surname><given-names>Y</given-names></name><name><surname>Böhm</surname><given-names>A</given-names></name><name><surname>Deubner</surname><given-names>J</given-names></name><name><surname>Jäckel</surname><given-names>Z</given-names></name><name><surname>Seiwald</surname><given-names>K</given-names></name><name><surname>Dovzhenko</surname><given-names>A</given-names></name><name><surname>Tietz</surname><given-names>O</given-names></name><name><surname>Dal Bosco</surname><given-names>C</given-names></name><name><surname>Walsh</surname><given-names>S</given-names></name><name><surname>Saltukoglu</surname><given-names>D</given-names></name><name><surname>Tay</surname><given-names>TL</given-names></name><name><surname>Prinz</surname><given-names>M</given-names></name><name><surname>Palme</surname><given-names>K</given-names></name><name><surname>Simons</surname><given-names>M</given-names></name><name><surname>Diester</surname><given-names>I</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>U-net: deep learning for cell counting, detection, and morphometry</article-title><source>Nature Methods</source><volume>16</volume><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id><pub-id pub-id-type="pmid">30559429</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fincher</surname><given-names>CT</given-names></name><name><surname>Wurtzel</surname><given-names>O</given-names></name><name><surname>de Hoog</surname><given-names>T</given-names></name><name><surname>Kravarik</surname><given-names>KM</given-names></name><name><surname>Reddien</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cell type transcriptome atlas for the planarian <italic>Schmidtea mediterranea</italic></article-title><source>Science</source><volume>360</volume><elocation-id>eaaq1736</elocation-id><pub-id pub-id-type="doi">10.1126/science.aaq1736</pub-id><pub-id pub-id-type="pmid">29674431</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Malik</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</article-title><conf-name>2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2014.81</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golding</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>The infracerebral gland in nephtys -- a possible neuroendocrine complex</article-title><source>General and Comparative Endocrinology</source><volume>14</volume><fpage>114</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/0016-6480(70)90013-4</pub-id><pub-id pub-id-type="pmid">5435505</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Grill</surname><given-names>JB</given-names></name><name><surname>Strub</surname><given-names>F</given-names></name><name><surname>Altché</surname><given-names>F</given-names></name><name><surname>Tallec</surname><given-names>C</given-names></name><name><surname>Richemond</surname><given-names>P</given-names></name><name><surname>Buchatskaya</surname><given-names>E</given-names></name><name><surname>Doersch</surname><given-names>C</given-names></name><name><surname>Avila Pires</surname><given-names>B</given-names></name><name><surname>Guo</surname><given-names>Z</given-names></name><name><surname>Gheshlaghi Azar</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Bootstrap your own latent-a new approach to self-supervised learning</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>21271</fpage><lpage>21284</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hadsell</surname><given-names>R</given-names></name><name><surname>Chopra</surname><given-names>S</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dimensionality reduction by learning an invariant mapping</article-title><conf-name>In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2006.100</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hassani</surname><given-names>K</given-names></name><name><surname>Khasahmadi</surname><given-names>AH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Contrastive multi-view representation learning on graphs</article-title><conf-name>In International Conference on Machine Learning</conf-name><fpage>4116</fpage><lpage>4126</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Fan</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Momentum Contrast for Unsupervised Visual Representation Learning</article-title><conf-name>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00975</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinrich</surname><given-names>L</given-names></name><name><surname>Bennett</surname><given-names>D</given-names></name><name><surname>Ackerman</surname><given-names>D</given-names></name><name><surname>Park</surname><given-names>W</given-names></name><name><surname>Bogovic</surname><given-names>J</given-names></name><name><surname>Eckstein</surname><given-names>N</given-names></name><name><surname>Petruncio</surname><given-names>A</given-names></name><name><surname>Clements</surname><given-names>J</given-names></name><name><surname>Pang</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>CS</given-names></name><name><surname>Funke</surname><given-names>J</given-names></name><name><surname>Korff</surname><given-names>W</given-names></name><name><surname>Hess</surname><given-names>HF</given-names></name><name><surname>Lippincott-Schwartz</surname><given-names>J</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Weigel</surname><given-names>AV</given-names></name><collab>COSEM Project Team</collab></person-group><year iso-8601-date="2021">2021</year><article-title>Whole-Cell organelle segmentation in volume electron microscopy</article-title><source>Nature</source><volume>599</volume><fpage>141</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03977-3</pub-id><pub-id pub-id-type="pmid">34616042</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobert</surname><given-names>O</given-names></name><name><surname>Kratsios</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neuronal identity control by terminal selectors in worms, flies, and chordates</article-title><source>Current Opinion in Neurobiology</source><volume>56</volume><fpage>97</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.12.006</pub-id><pub-id pub-id-type="pmid">30665084</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hofmann</surname><given-names>DK</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Regeneration and endocrinology in the polychaete <italic>Platynereis dumerilii</italic></article-title><source>Wilhelm Roux’s Archives of Developmental Biology</source><volume>180</volume><fpage>47</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1007/BF00848884</pub-id><pub-id pub-id-type="pmid">28304895</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>GB</given-names></name><name><surname>Yang</surname><given-names>HF</given-names></name><name><surname>Takemura</surname><given-names>S</given-names></name><name><surname>Rivlin</surname><given-names>P</given-names></name><name><surname>Plaza</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Latent Feature Representation via Unsupervised Learning for Pattern Discovery in Massive Electron Microscopy Image Volumes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2012.12175">https://arxiv.org/abs/2012.12175</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: a 2D graphics environment</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jacobson</surname><given-names>A</given-names></name><name><surname>Panozzo</surname><given-names>D</given-names></name><name><surname>Schüller</surname><given-names>C</given-names></name><name><surname>Diamanti</surname><given-names>O</given-names></name><name><surname>Zhou</surname><given-names>Q</given-names></name><name><surname>Pietroni</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Libigl: A simple c++ geometry processing library</data-title><version designator="MPL2">MPL2</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://libigl.github.io/">https://libigl.github.io/</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>H</given-names></name><name><surname>Cheveralls</surname><given-names>KC</given-names></name><name><surname>Leonetti</surname><given-names>MD</given-names></name><name><surname>Royer</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Self-Supervised Deep Learning Encodes High-Resolution Features of Protein Subcellular Localization</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.29.437595</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep convolutional neural networks</article-title><conf-name>Advances in neural information processing systems</conf-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuan</surname><given-names>AT</given-names></name><name><surname>Phelps</surname><given-names>JS</given-names></name><name><surname>Thomas</surname><given-names>LA</given-names></name><name><surname>Nguyen</surname><given-names>TM</given-names></name><name><surname>Han</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>C-L</given-names></name><name><surname>Azevedo</surname><given-names>AW</given-names></name><name><surname>Tuthill</surname><given-names>JC</given-names></name><name><surname>Funke</surname><given-names>J</given-names></name><name><surname>Cloetens</surname><given-names>P</given-names></name><name><surname>Pacureanu</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>W-CA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dense neuronal reconstruction through X-ray holographic nano-tomography</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1637</fpage><lpage>1643</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0704-9</pub-id><pub-id pub-id-type="pmid">32929244</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lafarge</surname><given-names>MW</given-names></name><name><surname>Caicedo</surname><given-names>JC</given-names></name><name><surname>Carpenter</surname><given-names>AE</given-names></name><name><surname>Pluim</surname><given-names>JP</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Veta</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Capturing single-cell phenotypic variation via unsupervised representation learning</article-title><conf-name>In International Conference on Medical Imaging with Deep Learning</conf-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lauri</surname><given-names>A</given-names></name><name><surname>Brunet</surname><given-names>T</given-names></name><name><surname>Handberg-Thorsager</surname><given-names>M</given-names></name><name><surname>Fischer</surname><given-names>AHL</given-names></name><name><surname>Simakov</surname><given-names>O</given-names></name><name><surname>Steinmetz</surname><given-names>PRH</given-names></name><name><surname>Tomer</surname><given-names>R</given-names></name><name><surname>Keller</surname><given-names>PJ</given-names></name><name><surname>Arendt</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Development of the annelid axochord: insights into notochord evolution</article-title><source>Science</source><volume>345</volume><fpage>1365</fpage><lpage>1368</lpage><pub-id pub-id-type="doi">10.1126/science.1253396</pub-id><pub-id pub-id-type="pmid">25214631</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>G</given-names></name><name><surname>Muller</surname><given-names>M</given-names></name><name><surname>Thabet</surname><given-names>A</given-names></name><name><surname>Ghanem</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>DeepGCNs: Can GCNs Go As Deep As CNNs?</article-title><conf-name>2019 IEEE/CVF International Conference on Computer Vision (ICCV</conf-name><pub-id pub-id-type="doi">10.1109/ICCV.2019.00936</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorensen</surname><given-names>WE</given-names></name><name><surname>Cline</surname><given-names>HE</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Marching cubes: a high resolution 3D surface construction algorithm</article-title><source>ACM SIGGRAPH Computer Graphics</source><volume>21</volume><fpage>163</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1145/37402.37422</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>AX</given-names></name><name><surname>Kraus</surname><given-names>OZ</given-names></name><name><surname>Cooper</surname><given-names>S</given-names></name><name><surname>Moses</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning unsupervised feature representations for single cell microscopy images with paired cell inpainting</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007348</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007348</pub-id><pub-id pub-id-type="pmid">31479439</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Macrina</surname><given-names>T</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Lu</surname><given-names>R</given-names></name><name><surname>Turner</surname><given-names>NL</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Popovych</surname><given-names>S</given-names></name><name><surname>Silversmith</surname><given-names>W</given-names></name><name><surname>Kemnitz</surname><given-names>N</given-names></name><name><surname>Bae</surname><given-names>JA</given-names></name><name><surname>Castro</surname><given-names>MA</given-names></name><name><surname>Dorkenwald</surname><given-names>S</given-names></name><name><surname>Halageri</surname><given-names>A</given-names></name><name><surname>Jia</surname><given-names>Z</given-names></name><name><surname>Jordan</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Mitchell</surname><given-names>E</given-names></name><name><surname>Mondal</surname><given-names>SS</given-names></name><name><surname>Mu</surname><given-names>S</given-names></name><name><surname>Nehoran</surname><given-names>B</given-names></name><name><surname>Wong</surname><given-names>W</given-names></name><name><surname>Yu</surname><given-names>S</given-names></name><name><surname>Bodor</surname><given-names>AL</given-names></name><name><surname>Brittain</surname><given-names>D</given-names></name><name><surname>Buchanan</surname><given-names>J</given-names></name><name><surname>Bumbarger</surname><given-names>DJ</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Collman</surname><given-names>F</given-names></name><name><surname>Elabbady</surname><given-names>L</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Kapner</surname><given-names>D</given-names></name><name><surname>Kinn</surname><given-names>S</given-names></name><name><surname>Mahalingam</surname><given-names>G</given-names></name><name><surname>Papadopoulos</surname><given-names>S</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Schneider-Mizell</surname><given-names>CM</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Takeno</surname><given-names>M</given-names></name><name><surname>Torres</surname><given-names>R</given-names></name><name><surname>Yin</surname><given-names>W</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Costa</surname><given-names>N da</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Petascale Neural Circuit Reconstruction: Automated Methods</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.08.04.455162</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Maitin-Shepard</surname><given-names>J</given-names></name><name><surname>Baden</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Google/neuroglancer</data-title><version designator="1244b6e">1244b6e</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/google/neuroglancer">https://github.com/google/neuroglancer</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Saul</surname><given-names>N</given-names></name><name><surname>Großberger</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>UMAP: uniform manifold approximation and projection</article-title><source>Journal of Open Source Software</source><volume>3</volume><elocation-id>861</elocation-id><pub-id pub-id-type="doi">10.21105/joss.00861</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>A</given-names></name><name><surname>Schmidt</surname><given-names>D</given-names></name><name><surname>Xu</surname><given-names>CS</given-names></name><name><surname>Pang</surname><given-names>S</given-names></name><name><surname>D’Costa</surname><given-names>JV</given-names></name><name><surname>Kretschmar</surname><given-names>S</given-names></name><name><surname>Münster</surname><given-names>C</given-names></name><name><surname>Kurth</surname><given-names>T</given-names></name><name><surname>Jug</surname><given-names>F</given-names></name><name><surname>Weigert</surname><given-names>M</given-names></name><name><surname>Hess</surname><given-names>HF</given-names></name><name><surname>Solimena</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>3D fib-sem reconstruction of microtubule-organelle interaction in whole primary mouse β cells</article-title><source>The Journal of Cell Biology</source><volume>220</volume><elocation-id>e202010039</elocation-id><pub-id pub-id-type="doi">10.1083/jcb.202010039</pub-id><pub-id pub-id-type="pmid">33326005</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musser</surname><given-names>JM</given-names></name><name><surname>Schippers</surname><given-names>KJ</given-names></name><name><surname>Nickel</surname><given-names>M</given-names></name><name><surname>Mizzon</surname><given-names>G</given-names></name><name><surname>Kohn</surname><given-names>AB</given-names></name><name><surname>Pape</surname><given-names>C</given-names></name><name><surname>Ronchi</surname><given-names>P</given-names></name><name><surname>Papadopoulos</surname><given-names>N</given-names></name><name><surname>Tarashansky</surname><given-names>AJ</given-names></name><name><surname>Hammel</surname><given-names>JU</given-names></name><name><surname>Wolf</surname><given-names>F</given-names></name><name><surname>Liang</surname><given-names>C</given-names></name><name><surname>Hernández-Plaza</surname><given-names>A</given-names></name><name><surname>Cantalapiedra</surname><given-names>CP</given-names></name><name><surname>Achim</surname><given-names>K</given-names></name><name><surname>Schieber</surname><given-names>NL</given-names></name><name><surname>Pan</surname><given-names>L</given-names></name><name><surname>Ruperti</surname><given-names>F</given-names></name><name><surname>Francis</surname><given-names>WR</given-names></name><name><surname>Vargas</surname><given-names>S</given-names></name><name><surname>Kling</surname><given-names>S</given-names></name><name><surname>Renkert</surname><given-names>M</given-names></name><name><surname>Polikarpov</surname><given-names>M</given-names></name><name><surname>Bourenkov</surname><given-names>G</given-names></name><name><surname>Feuda</surname><given-names>R</given-names></name><name><surname>Gaspar</surname><given-names>I</given-names></name><name><surname>Burkhardt</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name><name><surname>Beck</surname><given-names>M</given-names></name><name><surname>Schneider</surname><given-names>TR</given-names></name><name><surname>Kreshuk</surname><given-names>A</given-names></name><name><surname>Wörheide</surname><given-names>G</given-names></name><name><surname>Huerta-Cepas</surname><given-names>J</given-names></name><name><surname>Schwab</surname><given-names>Y</given-names></name><name><surname>Moroz</surname><given-names>LL</given-names></name><name><surname>Arendt</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Profiling cellular diversity in sponges informs animal cell type and nervous system evolution</article-title><source>Science</source><volume>374</volume><fpage>717</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1126/science.abj2949</pub-id><pub-id pub-id-type="pmid">34735222</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Musy</surname><given-names>M</given-names></name><name><surname>Jacquenot</surname><given-names>G</given-names></name><name><surname>Dalmasso</surname><given-names>G</given-names></name><name><surname>de neoglez</surname><given-names>B</given-names></name><name><surname>Zhou</surname><given-names>ZQ</given-names></name><name><surname>Sullivan</surname><given-names>B</given-names></name><name><surname>Lerner</surname><given-names>B</given-names></name><name><surname>Hrisca</surname><given-names>D</given-names></name><name><surname>Volpatto</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Marcomusy/vedo: 2022.1.0</data-title><version designator="2022.1.0">2022.1.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6247803">https://doi.org/10.5281/zenodo.6247803</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nielsen</surname><given-names>C</given-names></name><name><surname>Brunet</surname><given-names>T</given-names></name><name><surname>Arendt</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Evolution of the bilaterian mouth and anus</article-title><source>Nature Ecology &amp; Evolution</source><volume>2</volume><fpage>1358</fpage><lpage>1376</lpage><pub-id pub-id-type="doi">10.1038/s41559-018-0641-0</pub-id><pub-id pub-id-type="pmid">30135501</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pape</surname><given-names>C</given-names></name><name><surname>Beier</surname><given-names>T</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Jain</surname><given-names>V</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>Kreshuk</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Solving large multicut problems for connectomics via domain decomposition</article-title><conf-name>In Proceedings of the IEEE International Conference on Computer Vision Workshops</conf-name><pub-id pub-id-type="doi">10.1109/ICCVW.2017.7</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pape</surname><given-names>C</given-names></name><name><surname>Meechan</surname><given-names>K</given-names></name><name><surname>Moreva</surname><given-names>E</given-names></name><name><surname>Schorb</surname><given-names>M</given-names></name><name><surname>Chiaruttini</surname><given-names>N</given-names></name><name><surname>Zinchenko</surname><given-names>V</given-names></name><name><surname>Vergara</surname><given-names>H</given-names></name><name><surname>Mizzon</surname><given-names>G</given-names></name><name><surname>Moore</surname><given-names>J</given-names></name><name><surname>Arendt</surname><given-names>D</given-names></name><name><surname>Kreshuk</surname><given-names>A</given-names></name><name><surname>Schwab</surname><given-names>Y</given-names></name><name><surname>Tischer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mobie: A Fiji Plugin for Sharing and Exploration of Multi-Modal Cloud-Hosted Big Image Data</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.05.27.493763</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pytorch: An imperative style, high-performance deep learning library</article-title><conf-name>Advances in neural information processing systems</conf-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillip</surname><given-names>JM</given-names></name><name><surname>Han</surname><given-names>KS</given-names></name><name><surname>Chen</surname><given-names>WC</given-names></name><name><surname>Wirtz</surname><given-names>D</given-names></name><name><surname>Wu</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A robust unsupervised machine-learning method to quantify the morphological heterogeneity of cells and nuclei</article-title><source>Nature Protocols</source><volume>16</volume><fpage>754</fpage><lpage>774</lpage><pub-id pub-id-type="doi">10.1038/s41596-020-00432-x</pub-id><pub-id pub-id-type="pmid">33424024</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pietzsch</surname><given-names>T</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>BigDataViewer: visualization and processing for large image data sets</article-title><source>Nature Methods</source><volume>12</volume><fpage>481</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3392</pub-id><pub-id pub-id-type="pmid">26020499</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>U-net: Convolutional networks for biomedical image segmentation</article-title><conf-name>In International Conference on Medical image computing and computer-assisted intervention</conf-name></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruan</surname><given-names>X</given-names></name><name><surname>Johnson</surname><given-names>GR</given-names></name><name><surname>Bierschenk</surname><given-names>I</given-names></name><name><surname>Nitschke</surname><given-names>R</given-names></name><name><surname>Boerries</surname><given-names>M</given-names></name><name><surname>Busch</surname><given-names>H</given-names></name><name><surname>Murphy</surname><given-names>RF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Image-derived models of cell organization changes during differentiation and drug treatments</article-title><source>Molecular Biology of the Cell</source><volume>31</volume><fpage>655</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1091/mbc.E19-02-0080</pub-id><pub-id pub-id-type="pmid">31774723</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheffer</surname><given-names>LK</given-names></name><name><surname>Xu</surname><given-names>CS</given-names></name><name><surname>Januszewski</surname><given-names>M</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Takemura</surname><given-names>SY</given-names></name><name><surname>Hayworth</surname><given-names>KJ</given-names></name><name><surname>Huang</surname><given-names>GB</given-names></name><name><surname>Shinomiya</surname><given-names>K</given-names></name><name><surname>Maitlin-Shepard</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Clements</surname><given-names>J</given-names></name><name><surname>Hubbard</surname><given-names>PM</given-names></name><name><surname>Katz</surname><given-names>WT</given-names></name><name><surname>Umayam</surname><given-names>L</given-names></name><name><surname>Zhao</surname><given-names>T</given-names></name><name><surname>Ackerman</surname><given-names>D</given-names></name><name><surname>Blakely</surname><given-names>T</given-names></name><name><surname>Bogovic</surname><given-names>J</given-names></name><name><surname>Dolafi</surname><given-names>T</given-names></name><name><surname>Kainmueller</surname><given-names>D</given-names></name><name><surname>Kawase</surname><given-names>T</given-names></name><name><surname>Khairy</surname><given-names>KA</given-names></name><name><surname>Leavitt</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>PH</given-names></name><name><surname>Lindsey</surname><given-names>L</given-names></name><name><surname>Neubarth</surname><given-names>N</given-names></name><name><surname>Olbris</surname><given-names>DJ</given-names></name><name><surname>Otsuna</surname><given-names>H</given-names></name><name><surname>Trautman</surname><given-names>ET</given-names></name><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Bates</surname><given-names>AS</given-names></name><name><surname>Goldammer</surname><given-names>J</given-names></name><name><surname>Wolff</surname><given-names>T</given-names></name><name><surname>Svirskas</surname><given-names>R</given-names></name><name><surname>Schlegel</surname><given-names>P</given-names></name><name><surname>Neace</surname><given-names>E</given-names></name><name><surname>Knecht</surname><given-names>CJ</given-names></name><name><surname>Alvarado</surname><given-names>CX</given-names></name><name><surname>Bailey</surname><given-names>DA</given-names></name><name><surname>Ballinger</surname><given-names>S</given-names></name><name><surname>Borycz</surname><given-names>JA</given-names></name><name><surname>Canino</surname><given-names>BS</given-names></name><name><surname>Cheatham</surname><given-names>N</given-names></name><name><surname>Cook</surname><given-names>M</given-names></name><name><surname>Dreher</surname><given-names>M</given-names></name><name><surname>Duclos</surname><given-names>O</given-names></name><name><surname>Eubanks</surname><given-names>B</given-names></name><name><surname>Fairbanks</surname><given-names>K</given-names></name><name><surname>Finley</surname><given-names>S</given-names></name><name><surname>Forknall</surname><given-names>N</given-names></name><name><surname>Francis</surname><given-names>A</given-names></name><name><surname>Hopkins</surname><given-names>GP</given-names></name><name><surname>Joyce</surname><given-names>EM</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Kirk</surname><given-names>NA</given-names></name><name><surname>Kovalyak</surname><given-names>J</given-names></name><name><surname>Lauchie</surname><given-names>SA</given-names></name><name><surname>Lohff</surname><given-names>A</given-names></name><name><surname>Maldonado</surname><given-names>C</given-names></name><name><surname>Manley</surname><given-names>EA</given-names></name><name><surname>McLin</surname><given-names>S</given-names></name><name><surname>Mooney</surname><given-names>C</given-names></name><name><surname>Ndama</surname><given-names>M</given-names></name><name><surname>Ogundeyi</surname><given-names>O</given-names></name><name><surname>Okeoma</surname><given-names>N</given-names></name><name><surname>Ordish</surname><given-names>C</given-names></name><name><surname>Padilla</surname><given-names>N</given-names></name><name><surname>Patrick</surname><given-names>CM</given-names></name><name><surname>Paterson</surname><given-names>T</given-names></name><name><surname>Phillips</surname><given-names>EE</given-names></name><name><surname>Phillips</surname><given-names>EM</given-names></name><name><surname>Rampally</surname><given-names>N</given-names></name><name><surname>Ribeiro</surname><given-names>C</given-names></name><name><surname>Robertson</surname><given-names>MK</given-names></name><name><surname>Rymer</surname><given-names>JT</given-names></name><name><surname>Ryan</surname><given-names>SM</given-names></name><name><surname>Sammons</surname><given-names>M</given-names></name><name><surname>Scott</surname><given-names>AK</given-names></name><name><surname>Scott</surname><given-names>AL</given-names></name><name><surname>Shinomiya</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>C</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Smith</surname><given-names>NL</given-names></name><name><surname>Sobeski</surname><given-names>MA</given-names></name><name><surname>Suleiman</surname><given-names>A</given-names></name><name><surname>Swift</surname><given-names>J</given-names></name><name><surname>Takemura</surname><given-names>S</given-names></name><name><surname>Talebi</surname><given-names>I</given-names></name><name><surname>Tarnogorska</surname><given-names>D</given-names></name><name><surname>Tenshaw</surname><given-names>E</given-names></name><name><surname>Tokhi</surname><given-names>T</given-names></name><name><surname>Walsh</surname><given-names>JJ</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Horne</surname><given-names>JA</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Parekh</surname><given-names>R</given-names></name><name><surname>Rivlin</surname><given-names>PK</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Jefferis</surname><given-names>GS</given-names></name><name><surname>Ito</surname><given-names>K</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>George</surname><given-names>R</given-names></name><name><surname>Meinertzhagen</surname><given-names>IA</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Hess</surname><given-names>HF</given-names></name><name><surname>Jain</surname><given-names>V</given-names></name><name><surname>Plaza</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A connectome and analysis of the adult <italic>Drosophila</italic> central brain</article-title><source>eLife</source><volume>9</volume><elocation-id>e57443</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57443</pub-id><pub-id pub-id-type="pmid">32880371</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindelin</surname><given-names>J</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Frise</surname><given-names>E</given-names></name><name><surname>Kaynig</surname><given-names>V</given-names></name><name><surname>Longair</surname><given-names>M</given-names></name><name><surname>Pietzsch</surname><given-names>T</given-names></name><name><surname>Preibisch</surname><given-names>S</given-names></name><name><surname>Rueden</surname><given-names>C</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Schmid</surname><given-names>B</given-names></name><name><surname>Tinevez</surname><given-names>J-Y</given-names></name><name><surname>White</surname><given-names>DJ</given-names></name><name><surname>Hartenstein</surname><given-names>V</given-names></name><name><surname>Eliceiri</surname><given-names>K</given-names></name><name><surname>Tomancak</surname><given-names>P</given-names></name><name><surname>Cardona</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fiji: an open-source platform for biological-image analysis</article-title><source>Nature Methods</source><volume>9</volume><fpage>676</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id><pub-id pub-id-type="pmid">22743772</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>W</given-names></name><name><surname>Martin</surname><given-names>KM</given-names></name><name><surname>Lorensen</surname><given-names>WE</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>The Visualization Toolkit an Object-Oriented Approach to 3D Graphics</source><publisher-name>Prentice-Hall, Inc</publisher-name></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schubert</surname><given-names>PJ</given-names></name><name><surname>Dorkenwald</surname><given-names>S</given-names></name><name><surname>Januszewski</surname><given-names>M</given-names></name><name><surname>Jain</surname><given-names>V</given-names></name><name><surname>Kornfeld</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning cellular morphology with neural networks</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2736</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10836-3</pub-id><pub-id pub-id-type="pmid">31227718</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sebé-Pedrós</surname><given-names>A</given-names></name><name><surname>Chomsky</surname><given-names>E</given-names></name><name><surname>Pang</surname><given-names>K</given-names></name><name><surname>Lara-Astiaso</surname><given-names>D</given-names></name><name><surname>Gaiti</surname><given-names>F</given-names></name><name><surname>Mukamel</surname><given-names>Z</given-names></name><name><surname>Amit</surname><given-names>I</given-names></name><name><surname>Hejnol</surname><given-names>A</given-names></name><name><surname>Degnan</surname><given-names>BM</given-names></name><name><surname>Tanay</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Early metazoan cell type diversity and the evolution of multicellular gene regulation</article-title><source>Nature Ecology &amp; Evolution</source><volume>2</volume><fpage>1176</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1038/s41559-018-0575-6</pub-id><pub-id pub-id-type="pmid">29942020</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sebé-Pedrós</surname><given-names>A</given-names></name><name><surname>Saudemont</surname><given-names>B</given-names></name><name><surname>Chomsky</surname><given-names>E</given-names></name><name><surname>Plessier</surname><given-names>F</given-names></name><name><surname>Mailhé</surname><given-names>MP</given-names></name><name><surname>Renno</surname><given-names>J</given-names></name><name><surname>Loe-Mie</surname><given-names>Y</given-names></name><name><surname>Lifshitz</surname><given-names>A</given-names></name><name><surname>Mukamel</surname><given-names>Z</given-names></name><name><surname>Schmutz</surname><given-names>S</given-names></name><name><surname>Novault</surname><given-names>S</given-names></name><name><surname>Steinmetz</surname><given-names>PRH</given-names></name><name><surname>Spitz</surname><given-names>F</given-names></name><name><surname>Tanay</surname><given-names>A</given-names></name><name><surname>Marlow</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Cnidarian cell type diversity and regulation revealed by whole-organism single-cell RNA-seq</article-title><source>Cell</source><volume>173</volume><fpage>1520</fpage><lpage>1534</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.05.019</pub-id><pub-id pub-id-type="pmid">29856957</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siebert</surname><given-names>S</given-names></name><name><surname>Farrell</surname><given-names>JA</given-names></name><name><surname>Cazet</surname><given-names>JF</given-names></name><name><surname>Abeykoon</surname><given-names>Y</given-names></name><name><surname>Primack</surname><given-names>AS</given-names></name><name><surname>Schnitzler</surname><given-names>CE</given-names></name><name><surname>Juliano</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stem cell differentiation trajectories in Hydra resolved at single-cell resolution</article-title><source>Science</source><volume>365</volume><elocation-id>eaav9314</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav9314</pub-id><pub-id pub-id-type="pmid">31346039</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Starunov</surname><given-names>V</given-names></name><name><surname>Bailly</surname><given-names>X</given-names></name><name><surname>Ruta</surname><given-names>C</given-names></name><name><surname>Kerner</surname><given-names>P</given-names></name><name><surname>Cornelissen</surname><given-names>AJM</given-names></name><name><surname>Balavoine</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Globins in the marine annelid <italic>Platynereis dumerilii</italic> shed new light on hemoglobin evolution in bilaterians</article-title><source>BMC Evolutionary Biology</source><volume>20</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1186/s12862-020-01714-4</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sorkine</surname><given-names>O</given-names></name><name><surname>Alexa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>As-rigid-as-possible surface modeling</article-title><conf-name>In Symposium on Geometry processing</conf-name></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stine</surname><given-names>RR</given-names></name><name><surname>Sakers</surname><given-names>AP</given-names></name><name><surname>TeSlaa</surname><given-names>T</given-names></name><name><surname>Kissig</surname><given-names>M</given-names></name><name><surname>Stine</surname><given-names>ZE</given-names></name><name><surname>Kwon</surname><given-names>CW</given-names></name><name><surname>Cheng</surname><given-names>L</given-names></name><name><surname>Lim</surname><given-names>H-W</given-names></name><name><surname>Kaestner</surname><given-names>KH</given-names></name><name><surname>Rabinowitz</surname><given-names>JD</given-names></name><name><surname>Seale</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Prdm16 maintains homeostasis of the intestinal epithelium by controlling region-specific metabolism</article-title><source>Cell Stem Cell</source><volume>25</volume><fpage>830</fpage><lpage>845</lpage><pub-id pub-id-type="doi">10.1016/j.stem.2019.08.017</pub-id><pub-id pub-id-type="pmid">31564549</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stutz</surname><given-names>D</given-names></name><name><surname>Geiger</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning 3D shape completion under weak supervision</article-title><source>International Journal of Computer Vision</source><volume>128</volume><fpage>1162</fpage><lpage>1181</lpage><pub-id pub-id-type="doi">10.1007/s11263-018-1126-y</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanay</surname><given-names>A</given-names></name><name><surname>Sebé-Pedrós</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Evolutionary cell type mapping with single-cell genomics</article-title><source>Trends in Genetics</source><volume>37</volume><fpage>919</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1016/j.tig.2021.04.008</pub-id><pub-id pub-id-type="pmid">34020820</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Y</given-names></name><name><surname>Krishnan</surname><given-names>D</given-names></name><name><surname>Isola</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Contrastive multiview coding</article-title><conf-name>In European conference on computer vision</conf-name></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Traag</surname><given-names>VA</given-names></name><name><surname>Waltman</surname><given-names>L</given-names></name><name><surname>van Eck</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>From louvain to leiden: guaranteeing well-connected communities</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>5233</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-41695-z</pub-id><pub-id pub-id-type="pmid">30914743</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>NL</given-names></name><name><surname>Macrina</surname><given-names>T</given-names></name><name><surname>Bae</surname><given-names>JA</given-names></name><name><surname>Yang</surname><given-names>R</given-names></name><name><surname>Wilson</surname><given-names>AM</given-names></name><name><surname>Schneider-Mizell</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Lu</surname><given-names>R</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Bodor</surname><given-names>AL</given-names></name><name><surname>Bleckert</surname><given-names>AA</given-names></name><name><surname>Brittain</surname><given-names>D</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Dorkenwald</surname><given-names>S</given-names></name><name><surname>Collman</surname><given-names>F</given-names></name><name><surname>Kemnitz</surname><given-names>N</given-names></name><name><surname>Ih</surname><given-names>D</given-names></name><name><surname>Silversmith</surname><given-names>WM</given-names></name><name><surname>Zung</surname><given-names>J</given-names></name><name><surname>Zlateski</surname><given-names>A</given-names></name><name><surname>Tartavull</surname><given-names>I</given-names></name><name><surname>Yu</surname><given-names>S-C</given-names></name><name><surname>Popovych</surname><given-names>S</given-names></name><name><surname>Mu</surname><given-names>S</given-names></name><name><surname>Wong</surname><given-names>W</given-names></name><name><surname>Jordan</surname><given-names>CS</given-names></name><name><surname>Castro</surname><given-names>M</given-names></name><name><surname>Buchanan</surname><given-names>J</given-names></name><name><surname>Bumbarger</surname><given-names>DJ</given-names></name><name><surname>Takeno</surname><given-names>M</given-names></name><name><surname>Torres</surname><given-names>R</given-names></name><name><surname>Mahalingam</surname><given-names>G</given-names></name><name><surname>Elabbady</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Suckow</surname><given-names>S</given-names></name><name><surname>Becker</surname><given-names>L</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Polleux</surname><given-names>F</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>da Costa</surname><given-names>NM</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Reconstruction of neocortex: organelles, compartments, cells, circuits, and activity</article-title><source>Cell</source><volume>185</volume><fpage>1082</fpage><lpage>1100</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2022.01.023</pub-id><pub-id pub-id-type="pmid">35216674</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Van den Oord</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Representation Learning with Contrastive Predictive Coding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.03748">https://arxiv.org/abs/1807.03748</ext-link></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S</given-names></name><name><surname>Schönberger</surname><given-names>JL</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name><name><surname>Boulogne</surname><given-names>F</given-names></name><name><surname>Warner</surname><given-names>JD</given-names></name><name><surname>Yager</surname><given-names>N</given-names></name><name><surname>Gouillart</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><collab>scikit-image contributors</collab></person-group><year iso-8601-date="2014">2014</year><article-title>Scikit-image: image processing in python</article-title><source>PeerJ</source><volume>2</volume><elocation-id>e453</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Velickovic</surname><given-names>P</given-names></name><name><surname>Fedus</surname><given-names>W</given-names></name><name><surname>Hamilton</surname><given-names>WL</given-names></name><name><surname>Liò</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hjelm</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep Graph Infomax</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1809.10341">https://arxiv.org/abs/1809.10341</ext-link></element-citation></ref><ref id="bib79"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Verasztó</surname><given-names>C</given-names></name><name><surname>Jasek</surname><given-names>S</given-names></name><name><surname>Gühmann</surname><given-names>M</given-names></name><name><surname>Shahidi</surname><given-names>R</given-names></name><name><surname>Ueda</surname><given-names>N</given-names></name><name><surname>Beard</surname><given-names>JD</given-names></name><name><surname>Mendes</surname><given-names>S</given-names></name><name><surname>Heinz</surname><given-names>K</given-names></name><name><surname>Bezares-Calderón</surname><given-names>LA</given-names></name><name><surname>Williams</surname><given-names>E</given-names></name><name><surname>Jékely</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Whole-Animal Connectome and Cell-Type Complement of the Three-Segmented <italic>Platynereis dumerilii</italic> Larva</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.08.21.260984</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vergara</surname><given-names>HM</given-names></name><name><surname>Pape</surname><given-names>C</given-names></name><name><surname>Meechan</surname><given-names>KI</given-names></name><name><surname>Zinchenko</surname><given-names>V</given-names></name><name><surname>Genoud</surname><given-names>C</given-names></name><name><surname>Wanner</surname><given-names>AA</given-names></name><name><surname>Mutemi</surname><given-names>KN</given-names></name><name><surname>Titze</surname><given-names>B</given-names></name><name><surname>Templin</surname><given-names>RM</given-names></name><name><surname>Bertucci</surname><given-names>PY</given-names></name><name><surname>Simakov</surname><given-names>O</given-names></name><name><surname>Dürichen</surname><given-names>W</given-names></name><name><surname>Machado</surname><given-names>P</given-names></name><name><surname>Savage</surname><given-names>EL</given-names></name><name><surname>Schermelleh</surname><given-names>L</given-names></name><name><surname>Schwab</surname><given-names>Y</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name><name><surname>Kreshuk</surname><given-names>A</given-names></name><name><surname>Tischer</surname><given-names>C</given-names></name><name><surname>Arendt</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Whole-Body integration of gene expression and single-cell morphology</article-title><source>Cell</source><volume>184</volume><fpage>4819</fpage><lpage>4837</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2021.07.017</pub-id><pub-id pub-id-type="pmid">34380046</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waskom</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Seaborn: statistical data visualization</article-title><source>Journal of Open Source Software</source><volume>6</volume><elocation-id>3021</elocation-id><pub-id pub-id-type="doi">10.21105/joss.03021</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>AJ</given-names></name><name><surname>Northcutt</surname><given-names>MJ</given-names></name><name><surname>Rohrback</surname><given-names>SE</given-names></name><name><surname>Carpenter</surname><given-names>RO</given-names></name><name><surname>Niehaus-Sauter</surname><given-names>MM</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Wheatly</surname><given-names>MG</given-names></name><name><surname>Gillen</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Characterization of sarcoplasmic calcium binding protein (SCP) variants from freshwater crayfish Procambarus clarkii</article-title><source>Comparative Biochemistry and Physiology. Part B, Biochemistry &amp; Molecular Biology</source><volume>160</volume><fpage>8</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.cbpb.2011.04.003</pub-id><pub-id pub-id-type="pmid">21530674</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>K</given-names></name><name><surname>Rochman</surname><given-names>ND</given-names></name><name><surname>Sun</surname><given-names>SX</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cell type classification and unsupervised morphological phenotyping from low-resolution images using deep learning</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>13467</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-50010-9</pub-id><pub-id pub-id-type="pmid">31530889</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neuronal cell-type classification: challenges, opportunities and the path forward</article-title><source>Nature Reviews. Neuroscience</source><volume>18</volume><fpage>530</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.85</pub-id><pub-id pub-id-type="pmid">28775344</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Z</given-names></name><name><surname>Lauritzen</surname><given-names>JS</given-names></name><name><surname>Perlman</surname><given-names>E</given-names></name><name><surname>Robinson</surname><given-names>CG</given-names></name><name><surname>Nichols</surname><given-names>M</given-names></name><name><surname>Milkie</surname><given-names>D</given-names></name><name><surname>Torrens</surname><given-names>O</given-names></name><name><surname>Price</surname><given-names>J</given-names></name><name><surname>Fisher</surname><given-names>CB</given-names></name><name><surname>Sharifi</surname><given-names>N</given-names></name><name><surname>Calle-Schuler</surname><given-names>SA</given-names></name><name><surname>Kmecova</surname><given-names>L</given-names></name><name><surname>Ali</surname><given-names>IJ</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Trautman</surname><given-names>ET</given-names></name><name><surname>Bogovic</surname><given-names>JA</given-names></name><name><surname>Hanslovsky</surname><given-names>P</given-names></name><name><surname>Jefferis</surname><given-names>GSXE</given-names></name><name><surname>Kazhdan</surname><given-names>M</given-names></name><name><surname>Khairy</surname><given-names>K</given-names></name><name><surname>Saalfeld</surname><given-names>S</given-names></name><name><surname>Fetter</surname><given-names>RD</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A complete electron microscopy volume of the brain of adult <italic>Drosophila melanogaster</italic></article-title><source>Cell</source><volume>174</volume><fpage>730</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.06.019</pub-id><pub-id pub-id-type="pmid">30033368</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zinchenko</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>MorphoFeatures</data-title><version designator="swh:1:rev:f13d505f68e0dc08bd5fed9121ee56e45b4bd6ac">swh:1:rev:f13d505f68e0dc08bd5fed9121ee56e45b4bd6ac</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:dc0982d6a278139517e94977a48b50119419b1a5;origin=https://github.com/kreshuklab/MorphoFeatures;visit=swh:1:snp:01b134d03a405dd5ad419a9931676bd43b6e0714;anchor=swh:1:rev:f13d505f68e0dc08bd5fed9121ee56e45b4bd6ac">https://archive.softwareheritage.org/swh:1:dir:dc0982d6a278139517e94977a48b50119419b1a5;origin=https://github.com/kreshuklab/MorphoFeatures;visit=swh:1:snp:01b134d03a405dd5ad419a9931676bd43b6e0714;anchor=swh:1:rev:f13d505f68e0dc08bd5fed9121ee56e45b4bd6ac</ext-link></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80918.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Scheffer</surname><given-names>Louis K</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.05.07.490949" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.05.07.490949"/></front-stub><body><p>This paper marks a fundamental advance in reconstruction of volume EM images, by introducing the automatic assignment of cell types and tissues. This task has previously been done manually, resulting in a serious bottleneck in reconstruction, but the authors present compelling evidence that in at least some cases, automatic and semi-automatic techniques can match or better human assignment of cell and tissue types. These results will be of interest to almost all groups doing EM reconstruction, as they can speed up cell type assignment when the cell types are known, and provide an initial cell type and tissue classification when they are not.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80918.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Scheffer</surname><given-names>Louis K</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Scheffer</surname><given-names>Louis K</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Funke</surname><given-names>Jan</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013sk6x84</institution-id><institution>Janelia Research Campus</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.05.07.490949">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.05.07.490949v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;MorphoFeatures: unsupervised exploration of cell types, tissues and organs in volume electron microscopy&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, including Louis K Scheffer as Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Claude Desplan as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Jan Funke (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) What happens when there are errors in segmentation? This can easily lead to shapes which are different from any legitimate cell type, and could result in an erroneous cell type being created. On the other hand, these shapes are not likely reproduced, and so would likely result in a cluster of a single cell, which would be suspicious (at least in bilaterally symmetric creatures) as every cell type should have at least two examples.</p><p>It would be good to show the results of an existing merge error (or induce one if necessary) and report the results.</p><p>2) Along similar lines, the paper should report the used (or potential) cell-typing flow when using this method. In the paper it speaks of manually correcting the segmentation. But how do you know which cells are wrong? If you need to examine each cell manually, there will not be much time savings. So possibly you segment, then run the algorithms of this paper. Then you look for clusters of size 1, which (assuming bilateral symmetry) are likely a mistake. Then you fix those cells and iterate. It would be great to know if this approach was used, and if so how fast it converges.</p><p>3) In the section on &quot;visually interpretable&quot; features, you should provide a more quantitative idea of how many features are considered meaningful, and how those can be found. For example, are the six features shown in Figure 3 particularly meaningful, or were they chosen among many? A discussion of the feature selection protocol would be useful for replicating the method on new data. Furthermore, a supplementary figure with some of the features which are not meaningful would give the reader a better idea of the range of interpretability to expect.</p><p>4) The section on MorphoContextFeatures is missing a comparison with the MorphoFeatures. This made it unclear to me whether adding the neighborhood information is necessary for the discovery of tissues and organs. This could be remedied with a supplementary figure showing the same analysis as in figures 7 and 8 on the MorphoFeatures without the additional neighborhood information. Alternatively, since the MorphoFeatures are a subset of the MorphoContextFeatures, the authors could run a post-hoc analysis of whether the MorphoFeatures or the neighborhood features best explain the inter-class variance.</p><p>5) Finally, some extra guidance is needed to replicate this work on new data. In particular the following points could use more discussion:</p><p>5.1. How to choose the size of the MorphoFeatures vector – did the authors attempt a number other than 80 and if so, what was affected by this choice?</p><p>5.2. The protocol for when and how to define sub-clusters – were the chosen thresholds based on prior knowledge such as known tissues/organs? What do the authors suggest if this kind of information is missing?</p><p>5.3 How to link the obtained clusters back to specific, potentially meaningful, MorphoFeatures. For example, does the distinctive shape of the enteric neurons in cluster 8.3 of figure 5 correspond to an extreme of the cytoplasm shape feature described in figure 3 (lower left)?</p><p>6) In figure 1 b/c: The difference between B and lower part of C is unclear. If seen as a description of the two losses, the fact that the contrastive loss is shown twice is confusing. If seen as a description of the whole training setup, the omission of the fine-grained features is the issue.</p><p>7) In figure 2: It would be interesting to find out which subset of features correlates with which class, and whether those are meaningful. At minimum, knowing whether a shape, coarse texture, and fine texture are all involved in predictions.</p><p>8) In figure 2: The legend on the scale bars says 'mkm', which is not an abbreviation of micrometers that I am used to. Perhaps μm instead? The legend is also difficult to see (see pt. 11).</p><p>9) In figure 5: the scale bar legend is too small to see. Also, putting it above the scale bar might improve readability.</p><p>10) In Figure 7 + text: the text suggests that the clusters have been chosen manually, rather than using community detection as in the other figures. This should be justified if true.</p><p>11) In figure 8B + text (p.14): There isn't much said about the cells that are misclassified in the MorphoContextFeatures, i.e. where both manual segmentation and gene clusters agree, but MorphoContextFeatures does not. For example: green cell among the brown, or yellow cells just right of the central line of the organism, top. A justification similar to the explanations of misclassification in Figure 2 would help strengthen the argument.</p><p>For future work: Currently many EM reconstructions are nervous systems of somewhat higher animals (<italic>Drosophila</italic> and small portions of mammal brains). The shapes of these cells are very complex, and it would be interesting to see if the morphology features will continue to work on such complex cells. <italic>Drosophila</italic> could be a good example.</p><p>There is a question (line 409) of how well patch characteristics will correspond when comparing different samples. This could be tested, at least in part, by applying different image normalizations to the same sample, then treating them as two separate samples.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I have two main technical concerns. The first is what happens when there are errors in segmentation. This can easily lead to shapes which are different from any legitimate cell type, and could result in an erroneous cell type being created. On the other hand, these shapes are not likely reproduced, and so would likely result in a cluster of a single cell, which would be suspicious (at least in bilaterally symmetric creatures) as every cell type should have at least two examples. It would be good to induce a (for example) merge error between two cells and see what happens.</p><p>I would be very interested in the cell-typing flow using this method. In the paper it speaks of manually correcting the segmentation. But how do you know which cells are wrong? If you need to examine each cell manually, there will not be much time savings. So I could imagine a flow where you segment, then run the algorithms of this paper. Then you look for clusters of size 1, which (assuming bilateral symmetry) are likely a mistake. Then you fix those cells and iterate. It would be great to know if this approach was used, and if so how fast it converges.</p><p>For future work: Currently many EM reconstructions are nervous systems of somewhat higher animals (<italic>Drosophila</italic> and small portions of mammal brains). The shapes of these cells are very complex, and it would be interesting to see if the morphology features will continue to work on such complex cells. <italic>Drosophila</italic> could be a good example.</p><p>There is a question (line 409) of how well patch characteristics will correspond when comparing different samples. This could be tested, at least in part, by applying different image normalizations to the same sample, then treating them as two separate samples.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. In figure 1 b/c: The difference between B and lower part of C is unclear. If seen as a description of the two losses, the fact that the contrastive loss is shown twice is confusing. If seen as a description of the whole training setup, the omission of the fine-grained features is the issue.</p><p>2. In figure 2: It would be interesting to find out which subset of features correlates with which class, and whether those are meaningful. At minimum, knowing whether a shape, coarse texture, and fine texture are all involved in predictions.</p><p>3. In figure 2: The legend on the scale bars says 'mkm', which is not an abbreviation of micrometers that I am used to. Perhaps μm instead? The legend is also difficult to see (see pt. 4).</p><p>4. In figure 5: the scale bar legend is too small to see. Also, putting it above the scale bar might improve readability.</p><p>5. In Figure 7 + text: the text suggests that the clusters have been chosen manually, rather than using community detection as in the other figures. This should be justified if true.</p><p>6. In figure 8B + text (p.14): There isn't much said about the cells that are misclassified in the MorphoContextFeatures, i.e. where both manual segmentation and gene clusters agree, but MorphoContextFeatures does not. For example: green cell among the brown, or yellow cells just right of the central line of the organism, top. A justification similar to the explanations of misclassification in Figure 2 would help strengthen the argument.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80918.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) What happens when there are errors in segmentation? This can easily lead to shapes which are different from any legitimate cell type, and could result in an erroneous cell type being created. On the other hand, these shapes are not likely reproduced, and so would likely result in a cluster of a single cell, which would be suspicious (at least in bilaterally symmetric creatures) as every cell type should have at least two examples.</p><p>It would be good to show the results of an existing merge error (or induce one if necessary) and report the results.</p></disp-quote><p>This is an interesting question, however, there is no straightforward answer, as it depends on the type of the error and its abundance. In our data there is only one example of a segmentation error forming a separate cluster. These are 26 cells with a prominent shape phenotype, ‘cut’ through the nucleus area along one of the axes (examples now shown in Figure 3—figure supplement 1). Such cuts triggered extremely high values of some cell shape features, affecting the MorphoFeatures of these cells severely enough to result in a new cluster.</p><p>However, other types of segmentation errors do not form separate clusters for multiple reasons. Firstly, most errors are rather randomly distributed and not consistent, meaning the size of falsely attached fragments is highly variable and the location of segmentation splits is not consistent. For the cells of one class this often creates a ‘continuum’ of possible segmentation errors, which will get clustered together. Secondly, many errors are actually cell type specific. This comes from the fact that the segmentation training data contained mostly neurons, thus, other cell types further away from the training data suffered more segmentation errors. For example, muscles often experience split errors or get merged with a piece of neighbouring muscle and midgut cells often get merged with midgut lumen (Figure 3—figure supplement 2). Given random distribution of such splits and merges, they do not form separate clusters, but rather get clustered with the cells of the same cell type.</p><p>Finally, the modularity of the pipeline makes it to some extent robust to many segmentation errors, including relatively big merges (as shown in Figure 3—figure supplement 2). Such errors rarely affect nuclei segmentation, which is reported to have 99.0% accuracy (Vergara, 2021). That means that even severe cell segmentation errors will not affect at least 50% of MorphoFeature vector. In practice this number is often higher, since cell texture features will be minimally affected by split errors, and merge errors will only affect them proportionally to the size of the merged part. The only part of the feature vector heavily affected by segmentation errors is cell shape. However, since it comprises only <sub>⅙</sub> of the full feature vector, unique shapes will not necessary result in feature vectors distinct enough to create a separate cluster.</p><p>It is also important to note that the original EM dataset has been segmented using relatively standard segmentation approaches ((Beier, 2017) with modifications from (Pape, 2019)). Thus, although we can not comment on how the pipeline will behave for other types of segmentation errors, we expect similar errors to occur in similarly segmented datasets.</p><p>In the revised version of the manuscript, we now comment on the influence of segmentation errors in the Methods section (line 719):</p><p>“One of the clusters was discarded, since it contained only cells with split segmentation errors that ‘cut’ through the nucleus area along one of the axes (examples shown in Figure 3—figure supplement 1). Other types of segmentation errors in the dataset mostly did not influence the clustering quality for multiple reasons. Firstly, most errors are rather randomly distributed and not consistent, meaning the size of falsely attached fragments is highly variable and the location of segmentation splits is not consistent. Secondly, many of the errors are cell type specific. For example, midgut cells often got merged with midgut lumen (Figure 3—figure supplement 2, dark green) and muscle cells often experienced splits or merges of other muscle pieces (Figure 3—figure supplement 2, dark and light orange). Small merges that are not cell type specific (Figure 3—figure supplement 2, light green) also minimally affected the assignment. However, substantial merges could lead to a cell being grouped with a different cell type, for example, a neuron that got merged with a piece of epithelial cell bigger than the neuron size (Figure 3—figure supplement 2, yellow-green) got grouped together with epithelial cells, not neurons.”</p><disp-quote content-type="editor-comment"><p>2) Along similar lines, the paper should report the used (or potential) cell-typing flow when using this method. In the paper it speaks of manually correcting the segmentation. But how do you know which cells are wrong? If you need to examine each cell manually, there will not be much time savings. So possibly you segment, then run the algorithms of this paper. Then you look for clusters of size 1, which (assuming bilateral symmetry) are likely a mistake. Then you fix those cells and iterate. It would be great to know if this approach was used, and if so how fast it converges.</p></disp-quote><p>Based on multiple reviewers comments, we have introduced a separate guide to the potential application of our method to new data (see revision point 6). As a part of it, we also describe the cell-typing flow. However, we do not comment on manual segmentation correction, since it was done in (Vergara, 2021) and was not part of our paper. As discussed above, our pipeline is relatively robust to small segmentation errors, however, we generally expect that substantial merge errors (more than ~25% of a cell attached) are corrected before applying it.</p><disp-quote content-type="editor-comment"><p>3) In the section on &quot;visually interpretable&quot; features, you should provide a more quantitative idea of how many features are considered meaningful, and how those can be found. For example, are the six features shown in Figure 3 particularly meaningful, or were they chosen among many? A discussion of the feature selection protocol would be useful for replicating the method on new data. Furthermore, a supplementary figure with some of the features which are not meaningful would give the reader a better idea of the range of interpretability to expect.</p></disp-quote><p>The reviewers raise a very important point here, however, we find some parts of it are out of the scope of our paper. Specifically, to calculate the percentage of meaningful features, we must first define what ‘meaningful’ means. Some features might be difficult to understand as a human, yet be essential for distinguishing different types of morphologies. That is why in the paper we rather talk about visual interpretability. This characteristic, however, is not binary, with some features being easier to interpret than others. For example, for visualisation purposes we looked for clearly understandable features, but many of the features we did not include in the figure could still be visually explained after a more detailed examination. So while it is easy to define clearly interpretable features, confidently selecting some that are not interpretable at all is barely possible. That is why we now rather report the approximate percentage of ‘easily visually interpretable’ features in the Methods section (line 767):</p><p>“Visualising the extremes of various features from the dot plot we found most features (80-90%) to be relatively easy to interpret. More specifically, 82% shape features, 80% fine texture features and 90% coarse texture features could be visually mapped to explainable properties in less than 2 minutes of observing the data.</p><p>However, when selecting a random subset (10-20 random features) from each morphological component, the number of clearly interpretable features was lower, 65% for fine and 70% for coarse texture and 45% for shape features.”</p><p>Addressing the revision comment 6.3, we also changed the way we select the features to visualise, which we now describe in the Methods section (line 760):</p><p>“To illustrate some features specific to clusters (Figure 6—figure supplement 1) we first selected a set of features from the dot plot with high specificity in a given cluster and low specificity in other clusters. We then showed their extremes with cells that have the respective minimal/maximal value of this feature (Figure 6). The cells with merge errors of more than half of a cell size were ignored.”</p><disp-quote content-type="editor-comment"><p>4) The section on MorphoContextFeatures is missing a comparison with the MorphoFeatures. This made it unclear to me whether adding the neighborhood information is necessary for the discovery of tissues and organs. This could be remedied with a supplementary figure showing the same analysis as in figures 7 and 8 on the MorphoFeatures without the additional neighborhood information. Alternatively, since the MorphoFeatures are a subset of the MorphoContextFeatures, the authors could run a post-hoc analysis of whether the MorphoFeatures or the neighborhood features best explain the inter-class variance.</p></disp-quote><p>We thank the reviewers for raising this interesting point. We have further investigated the possibilities created by adding the neighbourhood information and introduced two new Supplementary Figures to show the value of MorphoContextFeatures (line 355):</p><p>“To further examine the contribution of incorporating cellular neighbourhood morphology we visualise the discovered foregut tissues and ganglia on the MorphoFeatures representation (Figure 8—figure supplement 1 and Figure 9—figure supplement 1). This shows that while some of the tissues, such as palpal ganglia or foregut epithelium, are composed of cells of one morphological type, others, e.g. circumpalpal ganglia or infracerebral gland, comprise cells with different morphology and could only be detected when taking into account cellular context as well.”</p><disp-quote content-type="editor-comment"><p>5) Finally, some extra guidance is needed to replicate this work on new data. In particular the following points could use more discussion:</p></disp-quote><p>We thank the reviewers for these suggestions that would highly improve the practical value of the paper. We wrote a guide that describes the steps necessary to apply the pipeline to a new dataset. However, given that in order to replicate the work one would first have to refer to the code in our GitHub repository, we have decided that it would be most appropriate to place the guide there:</p><p>https://github.com/kreshuklab/MorphoFeatures/blob/main/morphofeatures/howto_new_dataset.md.</p><p>We invite the reviewers to kindly advise us on whether they believe this guide should also be placed in the paper text, and if so, where exactly.</p><disp-quote content-type="editor-comment"><p>5.1. How to choose the size of the MorphoFeatures vector – did the authors attempt a number other than 80 and if so, what was affected by this choice?</p></disp-quote><p>We now briefly mention the reasoning behind this feature vector size in the Methods section (line 635):</p><p>“This vector size was found empirically using the bilateral distance as a metric. Vectors of bigger size showed similar performance, but contained more redundant features.”</p><p>We also discuss how to choose it for new data in the abovementioned guide:</p><p>“In case of no proxy task available, which could be used to evaluate different sizes, we recommend rather choosing a bigger vector size (e.g. 512). The feature vector could be then compressed to remove highly correlated features using, for example, feature clustering or dimensionality reduction techniques.”</p><disp-quote content-type="editor-comment"><p>5.2. The protocol for when and how to define sub-clusters – were the chosen thresholds based on prior knowledge such as known tissues/organs? What do the authors suggest if this kind of information is missing?</p></disp-quote><p>We now discuss the protocol for defining clustering resolution in the guide:</p><p>Our algorithm for setting this parameter and further defining subclusters was as follows:</p><p>– Choose an initial parameter that would result in a smaller number of clusters that could be easily analysed manually (10-20).</p><p>– Visually inspect the clusters and further subcluster (by increasing clustering resolution) if necessary. We suggest further subclustering in the following cases:</p><p>– cells in a cluster occupy clearly distinct areas in the volume (e.g. subclusters 8.1, 8.2 and 8.3 in Figure 3B);</p><p>– cells have distinct visual appearance (e.g. subclusters 14 and 15 in Figure 3B);</p><p>– gene expression (or other available information about the data) shows high heterogeneity in a cluster (subclustering midgut cells, Figure 5);</p><p>– prior knowledge is available, such as cell types or broader cell classes (e.g. splitting the cluster of foregut neurons and foregut epithelial cells, Figure 9).</p><disp-quote content-type="editor-comment"><p>5.3 How to link the obtained clusters back to specific, potentially meaningful, MorphoFeatures. For example, does the distinctive shape of the enteric neurons in cluster 8.3 of figure 5 correspond to an extreme of the cytoplasm shape feature described in figure 3 (lower left)?</p></disp-quote><p>We are grateful to the reviewers for suggesting this interesting analysis. We investigated the specificity of different features in morphological clusters and added a new Figure 6—figure supplement 1, which shows a dot plot of feature specificity in the clusters. We further changed the section “MorphoFeatures correspond to visually interpretable morphological properties” to discuss linking morphological clusters to specific MorphoFeatures and changed the figure in this section to show visual interpretation of some of the features specific to the clusters of outer epithelial cells, enteric neurons, midgut cells, rhabdomeric photoreceptors, ciliary band cells and foregut muscles (line 297):</p><p>“To examine whether it is possible to understand which properties of cells learned by our network distinguish the discovered morphological groups, we first identified MorphoFeatures that have high values specific to each cluster (Figure 6—figure supplement 1). Then for a set of characteristic features we visualised cells that correspond to the maximum and minimum value of the corresponding feature (Figure 6). Visual inspection showed that many of them can be matched to visually comprehensible properties.</p><p>For example, cytoplasm coarse texture feature 21 (Figure 6, upper left), which distinguishes the outer epithelial cells (cluster 11), shows its minimal value in secretory cells, which contain multiple highly distinct types of texture, and its maximal value in the cells crowded with small vesicles and cisternae on the surface of the animal. The cluster of enteric neurons (cluster 8.3) strongly differs from other neurons by nuclear coarse texture feature 4 (Figure 6, upper right), which contrasts nuclei with large amount of heterochromatin often attached to the nuclear lamina to nuclei with a high amount of euchromatin and prominent nucleoli. Cytoplasm fine texture feature 50 (Figure 6, middle left), characteristic to the midgut cells (cluster 14) is the lowest in cells with a small stretch of homogeneous cytoplasm with mitochondria and has its peak in cells with abundant Golgi cisternae and medium-sized mitochondria. Rhabdomeric photoreceptors of the adult eye (cluster 8.1) display specific nuclear fine texture feature 7 (Figure 6, middle right) that differentiates between nuclei with smooth and rather grainy texture. Cell shape feature 14 (Figure 6, lower left) has its minimal value found in beaker-shaped cells with smooth surface and its maximal value in rather round cells with extremely rugged surface, and is specific for ciliary band cells (cluster 15.2). Foregut muscles (cluster 13) can be described using nuclear shape feature 66 (Figure 6, lower right), having one extreme in nuclei with a small compartment with a rough surface (as a result of a segmentation error) and the other extreme in elongated flat nuclei.”</p><disp-quote content-type="editor-comment"><p>6) In figure 1 b/c: The difference between B and lower part of C is unclear. If seen as a description of the two losses, the fact that the contrastive loss is shown twice is confusing. If seen as a description of the whole training setup, the omission of the fine-grained features is the issue.</p></disp-quote><p>We thank the reviewers for spotting this ambiguity. We omitted fine texture features since the training procedure for coarse and fine texture is the same. We have now made it explicit in the figure and its caption:</p><p>“Training procedure for the coarse and fine texture features (here illustrated by coarse texture).”</p><disp-quote content-type="editor-comment"><p>7) In figure 2: It would be interesting to find out which subset of features correlates with which class, and whether those are meaningful. At minimum, knowing whether a shape, coarse texture, and fine texture are all involved in predictions.</p></disp-quote><p>We agree with the reviewers that such an analysis is indeed interesting. Before the paper submission we already attempted tracing the classifier's decisions to specific features doing various feature importance analyses. However, this proved to be challenging, since the features are highly correlated not just within the ‘morphology components’ (e.g. fine nuclear texture or cell shape), but also between these components. This makes sense, because most of the morphology features do correlate for biological reasons: extremely long cells (muscles) will always have contractile fibres in their fine texture, cell shape will correlate with nuclear shape, etc. However, this makes it barely possible to isolate single features, responsible for the classifier’s decisions. Nevertheless, to get at least a rough idea of how each morphological component contributes to the prediction accuracy, we trained the classifier on these components separately and reported the results in the new Figure 2—figure supplement 2 and in the Results section (line 191):</p><p>“To explore to which extent each of the morphology components (e.g. cell shape or fine nuclear texture) contributes to predicting the manually labelled cell classes, we ran a similar classification pipeline using these components separately (Figure 2—figure supplement 2). The results show that, for example, cytoplasm texture is sufficient to tell some classes apart with high precision. However, both coarse and fine cytoplasm texture perform slightly worse on distinguishing neurosecretory cells, in which nuclei occupy almost the whole cell volume. Cell shape is satisfactory to characterise neurons and muscle cells, but shows inferior performance on epithelial and secretory cells, which display a great variety of cell shapes. Surprisingly, nuclear fine texture features correctly distinguish, among other classes, muscle cells, suggesting that this class has characteristic chromatin texture.”</p><disp-quote content-type="editor-comment"><p>8) In figure 2: The legend on the scale bars says 'mkm', which is not an abbreviation of micrometers that I am used to. Perhaps μm instead? The legend is also difficult to see (see pt. 11).</p></disp-quote><p>This has been fixed.</p><disp-quote content-type="editor-comment"><p>9) In figure 5: the scale bar legend is too small to see. Also, putting it above the scale bar might improve readability.</p></disp-quote><p>This has been fixed.</p><disp-quote content-type="editor-comment"><p>10) In Figure 7 + text: the text suggests that the clusters have been chosen manually, rather than using community detection as in the other figures. This should be justified if true.</p></disp-quote><p>This was done for aesthetic reasons: to visualise cell types, not clusters on the UMAP representation. However, after the reviewers pointed it out, we do agree that it is not consistent with the figure visualising MorphoFeatures clusters. That is why we changed this figure to visualise clusters as well.</p><disp-quote content-type="editor-comment"><p>11) In figure 8B + text (p.14): There isn't much said about the cells that are misclassified in the MorphoContextFeatures, i.e. where both manual segmentation and gene clusters agree, but MorphoContextFeatures does not. For example: green cell among the brown, or yellow cells just right of the central line of the organism, top. A justification similar to the explanations of misclassification in Figure 2 would help strengthen the argument.</p></disp-quote><p>We thank reviewers for the suggestion. We now also describe the cases of misclassification by MorphoFeatures (line 350):</p><p>“Unaware of the physical tissue boundaries, MorphoContextFeatures sometimes led to ambiguous assignment of cells neighbouring other tissues, especially cells with strongly different morphology. Such noisy assignment can be noticed, for example, in the neurons bordering muscle cells on the boundary of the cirral ganglia and in the center of the brain.”</p><disp-quote content-type="editor-comment"><p>For future work: Currently many EM reconstructions are nervous systems of somewhat higher animals (<italic>Drosophila</italic> and small portions of mammal brains). The shapes of these cells are very complex, and it would be interesting to see if the morphology features will continue to work on such complex cells. <italic>Drosophila</italic> could be a good example.</p></disp-quote><p>We fully agree with the reviewers that applying the pipeline to neurons is a very interesting use case. Moreover, a recent work (Dorkenwald, 2022) has shown that applying a similar contrastive learning pipeline to the texture of segmented neurons extracts features useful to distinguish between the main glia types and general neuron classes. However, the essential component of neural identity is their branching patterns, thus, to get a full morphological description of neurons one would also need to adjust the shape learning part of our pipeline. This is, however, not trivial since our augmentations focus on the geometry of the shapes but not their topology, and are thus not appropriate for branching structures. More specifically, applying our shape deformations to neurons would deform neurite as well as somata surfaces while preserving the neurite skeletons. The resulting effect on the learned representations is difficult to estimate and would require further experiments with suitable evaluation metrics.</p><disp-quote content-type="editor-comment"><p>There is a question (line 409) of how well patch characteristics will correspond when comparing different samples. This could be tested, at least in part, by applying different image normalizations to the same sample, then treating them as two separate samples.</p></disp-quote><p>The approach suggested by the reviewers is interesting, however, we would argue that varying intensities are a lesser problem when comparing two EM datasets. Moreover, to some extent this problem could be alleviated by current intensity correction algorithms. We would expect more issues to arise, for example, from sample preparation and specific EM modality. Accurately simulating such differences, however, is not yet possible. Thus, we would rather investigate this direction once another comparable EM volume is available.</p><p>References</p><p>Beier, T., Pape, C., Rahaman, N., Prange, T., Berg, S., Bock, D. D., … and Hamprecht, F. A. (2017). Multicut brings automated neurite segmentation closer to human performance. <italic>Nature methods</italic>, <italic>14</italic>(2), 101-102.</p><p>Dorkenwald, S., Li, P., Januszewski, M., Berger, D. R., Maitin-Shepard, J., Bodor, A. L., … and Jain, V. (2022). Multi-Layered Maps of Neuropil with Segmentation-Guided Contrastive Learning. <italic>bioRxiv</italic>.</p><p>Pape, C., Matskevych, A., Wolny, A., Hennies, J., Mizzon, G., Louveaux, M., … and Kreshuk, A. (2019). Leveraging domain knowledge to improve microscopy image segmentation with lifted multicuts. <italic>Frontiers in Computer Science</italic>, 6.</p><p>Vergara, H. M., Pape, C., Meechan, K. I., Zinchenko, V., Genoud, C., Wanner, A. A., … and Arendt, D. (2021). Whole-body integration of gene expression and single-cell morphology. <italic>Cell</italic>, <italic>184</italic>(18), 4819-4837.</p></body></sub-article></article>