<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">97433</article-id><article-id pub-id-type="doi">10.7554/eLife.97433</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.97433.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>An <italic>Intranet</italic> of Things approach for adaptable control of behavioral and navigation-based experiments</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Bowler</surname><given-names>John C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0498-5743</contrib-id><email>jack.bowler@utah.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Zakka</surname><given-names>George</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0005-0035-6749</contrib-id><email>gz2333@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Yong</surname><given-names>Hyun Choong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1841-6317</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Wenke</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Rao</surname><given-names>Bovey</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Liao</surname><given-names>Zhenrui</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Priestley</surname><given-names>James B</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Losonczy</surname><given-names>Attila</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7064-0252</contrib-id><email>al2856@cumc.columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Neuroscience, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03r0ha626</institution-id><institution>Department of Neurobiology, University of Utah</institution></institution-wrap><addr-line><named-content content-type="city">Salt Lake City</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Aquabyte</institution><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Doctoral Program in Neurobiology and Behavior, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Brain Mind Institute, École polytechnique fédérale de Lausanne</institution></institution-wrap><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>26</day><month>02</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP97433</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-04-05"><day>05</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-04-05"><day>05</day><month>04</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.04.569989"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-07-11"><day>11</day><month>07</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.97433.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-20"><day>20</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.97433.2"/></event></pub-history><permissions><copyright-statement>© 2024, Bowler, Zakka et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Bowler, Zakka et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-97433-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-97433-figures-v1.pdf"/><abstract><p>Investigators conducting behavioral experiments often need precise control over the timing of the delivery of stimuli to subjects and to collect precise times of subsequent behavioral responses. Furthermore, investigators want fine-tuned control over how various multi-modal cues are presented. behaviorMate takes an ‘Intranet of Things’ approach, using a networked system of hardware and software components for achieving these goals. The system outputs a file with integrated timestamp–event pairs that investigators can then format and process using their own analysis pipelines. We present an overview of the electronic components and GUI application that make up behaviorMate as well as mechanical designs for compatible experimental rigs to provide the reader with the ability to set up their own system. A wide variety of paradigms are supported, including goal-oriented learning, random foraging, and context switching. We demonstrate behaviorMate’s utility and reliability with a range of use cases from several published studies and benchmark tests. Finally, we present experimental validation demonstrating different modalities of hippocampal place field studies. Both treadmill with burlap belt and virtual reality with running wheel paradigms were performed to confirm the efficacy and flexibility of the approach. Previous solutions rely on proprietary systems that may have large upfront costs or present frameworks that require customized software to be developed. behaviorMate uses open-source software and a flexible configuration system to mitigate both concerns. behaviorMate has a proven record for head-fixed imaging experiments and could be easily adopted for task control in a variety of experimental situations.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>place cells</kwd><kwd>navigation</kwd><kwd>hippocampus</kwd><kwd>virtual reality</kwd><kwd>behavior</kwd><kwd>in vivo</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>NIMH F31NS110316</award-id><principal-award-recipient><name><surname>Bowler</surname><given-names>John C</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH124047</award-id><principal-award-recipient><name><surname>Losonczy</surname><given-names>Attila</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH124867</award-id><principal-award-recipient><name><surname>Losonczy</surname><given-names>Attila</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01NS121106</award-id><principal-award-recipient><name><surname>Losonczy</surname><given-names>Attila</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>U01NS115530</award-id><principal-award-recipient><name><surname>Losonczy</surname><given-names>Attila</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01NS133381</award-id><principal-award-recipient><name><surname>Losonczy</surname><given-names>Attila</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01NS131728</award-id><principal-award-recipient><name><surname>Losonczy</surname><given-names>Attila</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000049</institution-id><institution>National Institute on Aging</institution></institution-wrap></funding-source><award-id>RF1AG080818</award-id><principal-award-recipient><name><surname>Losonczy</surname><given-names>Attila</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001703</institution-id><institution>EPFL ELISIR</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Priestley</surname><given-names>James B</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution>Fondation Marina Cuennet-Mauvernay</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Priestley</surname><given-names>James B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>behaviorMate provides a flexible, open-source platform for in vivo investigations that relies on a network of devices to enable observation of neural activity during navigation using physical treadmill and VR setups.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In this work, we present <italic>behaviorMate</italic>, an open-source system of modular components that communicate with each other over a local area network (LAN), orchestrated by a custom Java application. We validate our approach through benchmarking and experimental results, demonstrating the ability of behaviorMate to control head-fixed navigational experiments both in a self-powered treadmill (TM) and in a fully visual virtual reality (VR) setup with a running wheel. This solution permits researchers to select the configuration of cues they wish to present at run-time, resulting in a system that is capable of adapting quickly to incorporate changes to experimental protocols. The modular design makes it easy to adapt to shifting demands of research on the same behavioral setups while producing a consistent, easy-to-parse output file describing the experiment.</p><p>Proprietary systems such as National Instrument’s Data Acquisition Systems (DAQ) and their accompanying software controller, LabView, typically have large upfront and licensing costs for each experimental rig their system is deployed on. behaviorMate exclusively uses open-source software and is simple to construct with parts that are significantly cheaper. The customized electronics may be ordered as printed circuit boards (PCBs) and assembled by hand or purchased preassembled. The latter approach saves assembly time while adding significant expense; however, this can be mitigated by bulk ordering. The total cost of one of VR rig including five computer monitors, Android computers to render the visual scene, a frame for holding the monitors, and a lightweight running wheel is approximately $3700. The total cost for a belt-based TM system is around $1700. For two-photon (2p) imaging, additional components may be needed, such as a light-tight and laser resistant enclosure, which were not factored into the calculation. While other open-source and modular behavioral control systems have been developed which permit a variety of use cases (<xref ref-type="bibr" rid="bib3">Akam et al., 2022</xref>; <xref ref-type="bibr" rid="bib51">Saunders et al., 2022</xref>), behaviorMate does not require the user to write software code and uses compiled Arduino programs that do not incur the overhead of code interpreters to maximize performance.</p><p>Many experiments rely on using head-restrained animals to study the neuronal circuits underlying complex behaviors; such immobilization poses a fundamental challenge to behavioral research as it stymies animals’ natural movement through their environments. In order to address this issue, a plethora of ‘treadmill’ systems have been developed relying on belt-based systems (<xref ref-type="bibr" rid="bib50">Royer et al., 2012</xref>; <xref ref-type="bibr" rid="bib38">Lovett-Barron et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Jordan et al., 2021</xref>), VR (<xref ref-type="bibr" rid="bib14">Dombeck et al., 2010</xref>), or augmented reality (AR), where experimenters move ‘spatial’ cues to create the illusion of locomotion without actual movement in physical space (<xref ref-type="bibr" rid="bib31">Jayakumar et al., 2019</xref>). Belt-based systems involve placing an animal on a physical belt TM where, as the animal runs, physical cues affixed to the belt provide spatial information. VR and AR systems operate under closed-loop control, where the environment responds to the animal’s behavior (<xref ref-type="bibr" rid="bib15">Dombeck and Reiser, 2012</xref>). Normally, these cues are visually displayed on one or more computer monitors, but olfactory and auditory stimuli have also been utilized at regular virtual distance intervals to enrich the experience with more salient cues (<xref ref-type="bibr" rid="bib45">Radvansky and Dombeck, 2018</xref>; <xref ref-type="bibr" rid="bib19">Fischler-Ruiz et al., 2021</xref>). Furthermore, belt- and VR/AR-based elements have been combined to enrich experiences and enable more complex behavioral tasks. At times, traditional ‘open-loop’ stimulus may be required, such as timed cue presentations, allowing pre- and post-event neuronal activity to be examined. Increasingly, studies combine open- and closed-loop behavioral paradigms to identify stimulus–response profiles or behavioral state driven changes to stimulus responses. The proliferation of various experimental paradigms demands a high level of flexibility in experimental setups, so that methods can be integrated within single experiments or across lines of research to interrogate neuronal function.</p><p>Many of the spatial representations observed in freely moving animals are conserved in head-restrained animal setups (<xref ref-type="bibr" rid="bib4">Aronov and Tank, 2014</xref>; <xref ref-type="bibr" rid="bib14">Dombeck et al., 2010</xref>). Observations within the medial temporal lobe, focused on all subregions of the hippocampus as well as the entorhinal cortex, have shown that prominent neural correlates of navigation behaviors exist in both the TM belt style systems as well as in the visual only VR systems (<xref ref-type="bibr" rid="bib4">Aronov and Tank, 2014</xref>; <xref ref-type="bibr" rid="bib14">Dombeck et al., 2010</xref>; <xref ref-type="bibr" rid="bib50">Royer et al., 2012</xref>; <xref ref-type="bibr" rid="bib12">Danielson et al., 2016</xref>). In CA1 (<xref ref-type="bibr" rid="bib13">Danielson et al., 2017</xref>; <xref ref-type="bibr" rid="bib62">Zaremba et al., 2017</xref>) and CA3 (<xref ref-type="bibr" rid="bib56">Terada et al., 2022</xref>), place cells, goal-related tuning, and other feature-selective tuning have been observed. Additionally, in visual VR systems, grid cell-like tuning has been observed in the medial entorhinal cortex, as was distance tuning (<xref ref-type="bibr" rid="bib4">Aronov and Tank, 2014</xref>; <xref ref-type="bibr" rid="bib30">Heys et al., 2014</xref>). The increased control over reward distribution and cue manipulation that VR systems afford has been pivotal for recent findings relating to how cells in CA1 form place fields, uncovering novel mechanisms underlying synaptic plasticity (<xref ref-type="bibr" rid="bib25">Gonzalez et al., 2023</xref>; <xref ref-type="bibr" rid="bib7">Bittner et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Priestley et al., 2022</xref>), demonstrating the power of these systems to test specific theoretical predictions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>behaviorMate is an integrated system composed of several sub-components that communicate on an LAN using JSON-formatted packets transmitted via a standard Datagram packet protocol (UDP) (<xref ref-type="fig" rid="fig1">Figure 1</xref>). This approach allows for modularity and encapsulation of different concerns. Moreover, acknowledgment packets are used to ensure reliable message delivery to and from connected hardware. We dub the resulting system as an ‘<italic>Intranet</italic> of Things’ approach, mimicking the Internet of Things systems commonly found in modern ‘smart homes’, which communicate exclusively on a dedicated LAN using statically assigned internet protocol (IP) addresses. This allows for a central user interface (UI) or ‘real-time’ computer to seamlessly send and receive messages from various accessory downstream devices that have dedicated simple functions. Importantly, the techniques we describe are possible implementations of the behaviorMate system. In our experimental setups, we use behaviorMate to combine in vivo head-fixed mouse navigation experiments with 2p microscopy, focusing on one-dimensional (1D) spatial navigation tasks (Figure 3). Due to its modular design, however, behaviorMate can be easily reconfigured to provide closed- or open-loop control during a variety of behavioral paradigms (and has been used for non-navigation-based studies; such as the head-fixed trace fear conditioning task described in <xref ref-type="bibr" rid="bib2">Ahmed et al., 2020</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of the behaviorMate system with optional components.</title><p>Diagram shows the modular components behaviorMate can interact with the colored arrows show the direction of information flow. For example, the Position Controller module only receives position updates and forwards them to the computer. The Visual Display module sends data to the Display Controllers to render the scene. The Behavior Controller performs a general-purpose input/output (GPIO) function and may both send and receive data to and from the Computer. An optional external Sync Pulse is shown to demonstrate that behaviorMate will log any UDP received UDP packets, such as timestamped sync-signals which could be beneficial in certain setups to synchronize neural data with behavior. Additional Systems may also be implemented on the behaviorMate intranet, taking advantage of the flexible UDP messaging scheme.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97433-fig1-v1.tif"/></fig><p>In addition to running a PC-based UI, we combine Arduino open-source micro-controllers with custom designed PCBs to run the experiments (<xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2D, E</xref>, <italic>also see</italic> Appendix 1). The circuits include Ethernet adapters, allowing the Arduinos to communicate on the network, as well as connectors that interface them to sensors or actuators for controlling the experiments or for reporting the behavioral state back to the UI. We outline a general-purpose input/output (GPIO) circuit that can connect to a variety of sensors and reward valves generally using a transistor–transistor logic (TTL) pulse/square wave as well as I2C and serial peripheral interface (SPI) protocols (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), a position tracking circuit (<xref ref-type="fig" rid="fig2">Figure 2E</xref>) that is dedicated mainly to reading updates of the animal’s location from a rotary encoder, and a VR controller setup which can be used to provide a more immersive visual display to animals during the behaviors. Importantly, these individual components can be swapped out or added to with increased experimental demands. In most cases, the provided software and hardware are sufficient to meet users needs, however, we do also point to certain targets for easy expansion which would require editing the software provided to interface with specialized hardware (e.g. adding a novel SPI-based extension to the GPIO circuit). In general, without editing software the user has two options for controlling hardware: using one of the digital/analog port on the GPIO board or over network via an Ethernet adapter (but this could require following the same JSON API as existing behaviorMate components). For additional integration of software components or testing, the UI can be configured to send messages to the PC’s localhost address (i.e. 127.0.0.1 on most PCs). To speed up development, we created virtual implementations of the electronics in Python code to test UI changes before testing them on a physical system. Most modern programming languages, including Python and Matlab, have built-in support for JSON processing and UDP networking making component and new feature testing easy with the help of utility scripts that can be written in users’ preferred environments (<italic>see</italic> Appendix 1 for references to example Python code).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Details of user interface (UI) function.</title><p>(<bold>A</bold>) Screenshot of the UI. The interface provides a snapshot of the animal’s current status (<italic>center</italic>) as well as cumulative information about the current experiment (<italic>upper-left</italic>). The control panel along the left side provide: <italic>Top</italic>, an input for a project name and mouse id, which control where the resulting behavior file will be saved (these boxes are disabled when an experiment is currently running); <italic>Middle</italic>, controls to trigger the general-purpose input/output (GPIO) to send a transistor–transistor logic (TTL) pulse that is to issue a water reward or turn on an LED; and <italic>Bottom</italic>, controls to start/stop experiments as well as to load settings files. (<bold>B</bold>) Details of UI event loop. The UI is continuously executing an event loop on every update step which checks for messages, writes the behavior file and updates the display. (<bold>C</bold>) JSON-formatted message passing. The PC sends and receives JSON-formatted messages via UDP to active components of the system. Messages have a nested structure to allow for internal routing and subsequent actions to be taken. (<bold>D</bold>) Rendering of the GPIO circuit used. <italic>Left</italic>: JST headers connected to I/O pins and pull-down resisters to allow for easy connections with peripheral devices that can be activated by TTL pulses (both at 3.3 and 12 V, depending on the pins used). Additionally, power connections and a BNC output trigger are provided. <italic>Right</italic>: This board attaches to an Arduino Due microcontroller. (<bold>E</bold>) Updates to the position have a dedicated circuit for decoding quadrature information and sending updates to the PC. Connections for a quadrature-based rotary encoder, power, and Ethernet are provided. Additionally, an input for a ‘Lap Reset’ sensor is provided and headers to interface with an Arduino Mini.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97433-fig2-v1.tif"/></fig><p>Lastly, since our main focus is on 1D navigational tasks for 2p imaging, we provide two designs for physical rigs, which hold animals fixed in place under a microscope. However, they could also be adapted to any other methods for in vivo recording. One system is a self-powered, belt-based TM system, wherein animals are placed on a fabric belt that they pull beneath them to advance their virtual position. Locations and contexts can be distinguished in this setup by having various fabrics and physical cues. Alternatively, the other system provides navigational contexts through a series of computer displays for a purely visual experience, but allowing for additional flexibility of within-task manipulations to context and cues. We describe the details of both of these systems and, additionally, note that with the following methods, it is possible to combine aspects from both belt-based and VR systems to meet novel experimental designs. To clarify, for the remainder of the paper, ‘treadmill’ will refer to these belt-based systems while ‘VR’ will refer to the visual-based system in which animals moving a plastic running wheel.</p><sec id="s2-1"><title>The user interface</title><p>The central hub for running experiments is the behaviorMate UI (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The UI collects input from various sensors, assembling information about virtual position and behavior state data asynchronously. The UI performs three main functions: (1) accepting user input and displaying the current state of experiments to ensure proper functioning, (2) writing hardware information in a timestamped streaming text file which can be processed later for analysis of the experiment, and (3) serving as the central communications hub for sending and receiving UDP-formatted messages to the downstream components. The system is event-driven such that the hardware components send messages to the UI when the state changes rather than the UI continuously polling. There are currently two versions of the UI. The first is a long-term support release that evolved alongside the experimental systems and has been used for data collection for numerous published studies; the second is a streamlined beta version that has been rewritten to take advantage of the JavaFX visual development toolbox. Java was chosen for the UI because it is cross-platform in that it can be run on Windows, Linux, and Mac, and supports standard networking protocols. For consistency, interchangeability, and space optimization, we run the UI on an Intel(TM) NUC mini PC with a 2.4-GHz base-frequency (up to 4.2 GHz based on processor load) 4-core i5-1135G7 processor, 16-GB DDR4-3200MHz RAM, and a 512-GB NVMe M.2 SSD hard drive running Window 10 Professional Edition, although many other configurations are supported. Notably, due to the system’s modular architecture, different UIs could be implemented in any programming language and swapped in without impacting the rest of the system. The only requirement for interchangeability of the UI is to implement sending and receiving of the same JSON-formatted UDP messages packets to the downstream hardware components (<italic>see</italic> Appendix 1 for Arduino firmware which includes additional detail of the JSON messaging protocol).</p><p>The UI executes in three distinct phases during run-time. First, a setup routine is executed, then the program enters the main event loop, and finally a termination routine is executed when an experiment is completed (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The main event loop begins executing as soon as the UI is loaded, but additional steps for checking and logging the contexts’ state are added during the experiment. Devices still receive input and deliver output to the UI even when an experiment is not currently running, allowing experimenters to confirm that everything is working before starting. Experiments using behaviorMate are configured by user-generated JSON files with key–value pairs specified in the documentation. These settings files can be manually generated in a text editor or programmatically generated using a script (<italic>see also</italic> Appendix 1).</p><p>The main structure of the application logic is an event loop that iterates over enabled <italic>contexts</italic>. Within behaviorMate, a context is grouping of one or more stimuli that get activated concurrently. For many experiments, it is desirable to have multiple contexts that are triggered at various locations and times in order to construct distinct or novel environments. Contexts define presentation schemes for both simple stimuli such as an LED lighting up and water reward delivery, as well as more complex stimuli like flashing LEDs or even an entire visual VR scene. A context can be applied globally or to one or several locations; a list of locations and rules governing an individual context’s activation is referred to as a Context List. When a Context List is active, the devices assigned to it will be triggered. For example, a user could configure an LED which blinks at several designated locations along the track. Context Lists are implemented in the UI using an Object-Oriented approach that streamlines implementing logic for governing the presentation and control of novel cue types. The main rules governing how a Context List works are implemented in a check method which is passed the current state of the experiment as input arguments and returns a Boolean variable as true or false corresponding to whether a Context List should be active or not. In this way, novel Context List types may be added to the program easily through inheritance of a base class and overriding this one method. Additionally, Context Lists are instantiated using the ‘Factory Method’ pattern (<xref ref-type="bibr" rid="bib21">Gamma and Helm, 1994</xref>) and appended to a dynamic list. This pattern allows for adding complex functionality while minimizing the impact on performance and increasing code reuse and modularity. Existing Context List types can be modified by applying ‘decorators’ (<xref ref-type="bibr" rid="bib21">Gamma and Helm, 1994</xref>), resulting in <italic>composable</italic> behavioral paradigms, meaning that novel experiments can be implemented by nesting Context Lists within one or more existing decorators, without requiring any software modification. Settings files following JSON syntax specify the configuration as well as the context lists and decorators. For example, a particular decorator can be configured to cause a context to only be active on even laps while a second decorator may restrict the context from being active until after a certain amount of time has passed. Composing these two rules means it is possible to trigger the contexts on even laps, only after 5 min have passed since starting the experiment. However, if the available set of decorators is not enough to implement the required task logic, some modifications to the source code may be necessary. These modifications, in most cases, would be very simple and only a basic understanding of object-oriented programming is required. A case where this might be needed would be performing a novel customized real-time analysis on behavior data and activating a stimulus based on the result. The JavaFX version of the behaviorMate UI additionally supports a plugin architecture which will further simplify the process of adding custom decorators and context list rules in the future.</p></sec><sec id="s2-2"><title>Behavior tracking and control</title><sec id="s2-2-1"><title>GPIO/Behavior Controller</title><p>The primary GPIO circuit we implement, referred to as the Behavior Controller, is composed of an Arduino attached to a custom circuit board (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The PCB handles voltage regulation and provides the necessary connectors to operate downstream devices. The Arduino program distributed with behaviorMate controls all connected devices and communication between them and the UI. The program wraps ‘action’ messages with ‘routes’ (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). This pattern of routing messages simplifies debugging since it is clear which classes of the Arduino code received the messages and where replies were generated. The messages are JSON-formatted text, so they are human-readable and can be simulated within the behaviorMate PC for testing and debugging purposes (<italic>also see</italic> Appendix 1, for links to Arduino firmware and documentation on the JSON messaging protocol). The PCB has various connectors and other components that makes it easier to connect arbitrary hardware, including sensors and actuators, to the Arduino. Sensors and actuators can be connected to the controller using one of the 13 digital or 5 analog input/output connectors. Moreover, the controller provides an Ethernet adapter for the Arduino to permit LAN communication.</p><p>The Behavior Controller has several important functions; it receives signals from the UI to activate rewards and cues such as odor valves, LEDs, and tone generators and passes input from sensors to the PC. In our setup for 2p imaging, the Behavior Controller also sends a synchronizing signal to the microscope to initiate imaging after an experiment is started through the behaviorMate UI. Some microscopes can be configured to begin imaging by a TTL synchronizing trigger. Alternatively, microscopes might send a trigger indicating that recording has commenced. Moreover, some setups can periodically send back synchronization signals to ensure alignment during long recordings that may be affected by clock drift. The Behavior Controller can both receive and send triggers for the purpose of synchronization which will be timestamped and logged by the UI. By default, this signal is a TTL pulse (3.3 V in our implementation, <xref ref-type="fig" rid="fig2">Figure 2D</xref> <italic>also see</italic> Appendix 1) and can also be configured to activate other types of recording devices such as video cameras or electrophysiology interfaces. However, it is important to note that the sampling rate of an Arduino Due (used in Behavior Controller) is incapable of the kilohertz sampling rate of typical electrophysiology devices. In these cases, it might be necessary to send a periodic synchronization TTL from the behavior controller to the electrophysiology hardware to ensure proper timing of behavior and neural data. behaviorMate is compatible with a variety of alignment and synchronization schemes and it is left up to the users to implement the solution most appropriate for their experimental setups.</p></sec><sec id="s2-2-2"><title>Position controller</title><p>The position controller’s function is to detect animal movement and report it to behaviorMate. It is composed of an Arduino Nano attached to a custom circuit which has an onboard quadrature decoder, 16-bit counter, and connectors for attaching the rotary encoder and lap-reset circuit. For either TM or running wheel-based setups (i.e. for a VR setup), a quadrature rotary encoder device is coupled to the shaft of a wheel that turns as the animal runs. The turning of the rotary encoder generates necessary signals, or ‘ticks’, which are passed to the quadrature decoder to calculate the instantaneous velocity of the animals. These quadrature ticks are decoded and counted in the counter. The custom circuit (<xref ref-type="fig" rid="fig2">Figure 2E</xref>) uses a 10-MHz oscillator which is capable of counting high resolution encoders (&gt;4096 ticks/turn) at greater than 1K RPM. The onboard counter enables the tick counting to be decoupled from the speed of the on-board Arduino. Consequently, the Arduino can check the counter at a much slower rate (100 Hz in our setup, but can be as slow as 5 Hz) without losing count of the ticks. The Arduino is only responsible for checking the counter and generating a JSON-formatted text string to transmit to the behavior PC via Ethernet, and only transmits when there is movement. Significantly, this eliminates polling from the behavior PC and allows the communication between the Arduino and the PC to be one way instead of bidirectional. The behavior PC then translates the counts into a meaningful measurement of the linear distance traveled by the animal on the belt or running wheel.</p></sec></sec><sec id="s2-3"><title>Behavioral apparatus</title><sec id="s2-3-1"><title>TM system</title><p>Self-powered TM systems have been extensively used in the study of navigational behaviors (<xref ref-type="bibr" rid="bib50">Royer et al., 2012</xref>; <xref ref-type="bibr" rid="bib57">Tuncdemir et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Lovett-Barron et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Grienberger and Magee, 2022</xref>; <xref ref-type="bibr" rid="bib22">Geiller et al., 2017</xref>). The following section outlines a protocol for the construction of a behaviorMate compatible mouse TM setup that has been used for head-fixed imaging (<xref ref-type="bibr" rid="bib35">Kaufman et al., 2020</xref>; <xref ref-type="bibr" rid="bib56">Terada et al., 2022</xref>; <xref ref-type="bibr" rid="bib49">Rolotti et al., 2022b</xref>). In combination with the software and circuits described above, the TM system provides a flexible platform for implementing novel experimental paradigms.</p></sec><sec id="s2-3-2"><title>Frame</title><p>The TM system requires a frame to support it and maintain wheel alignment, so we provide designs for a frame made of extruded aluminum rails (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The rails support wheels on either end for a fabric ‘belt’ that can be moved by a mouse. In addition to the wheels, we added two wheel-rollers that support the belt near the animal’s fixed position (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). This design does not have a platform directly beneath the mouse, allowing the belt to absorb vibrations, which reduces motion artifacts and allows for a stable field of view during in vivo imaging experiments. TM belts are constructed from 2-inch-wide fabric ribbon and may be embellished by linking multiple fabrics or adding various physical cues such as foam balls, sequins, or hook-and-loop fasteners for texture (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). These cues help the animal to orient itself and are useful for information encoding. In this setup, the animal is head-fixed beneath a microscope, but is able to move the belt and attached physical cues which mimics navigation while still allowing investigators to perform imaging of cellular and even sub-cellular regions of interest (ROIs). To further optimize the TM for imaging, a two-goniometer device placed underneath the belt is used to adjust the angle of the animal’s head along the roll (side-to-side) and pitch (forward and backward) axes. This setup allows the investigator to level the field of view, ensuring the neuronal plane of interest is perpendicular to the optical axis of the microscope, thus increasing the number of neurons that can be imaged and improving image clarity. However, this may not be needed in setups where the objective itself can be tilted; if the objective can be tilted along two axes, no goniometers would be needed, whereas if the objective can only tilt along one axis, a single goniometer could be employed. Finally, the frame has a sliding mechanism to adjust the distance between the wheels and accommodate various sized belts to ensure proper tension. Proper belt tension ensures the animal can move easily and reduces the chance of belt slippage along the wheels.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Details of treadmill system.</title><p>(<bold>A</bold>) CAD model of voluntary treadmill that can expand or contract to accommodate different sized running belts. Implant holders sit on goniometers to allow mouse head angle to be adjusted. (<bold>B</bold>) Lap time error is defined for each lap (full turn of the treadmill belt) as the difference between the time recorded by behaviorMate and the computer vision benchmark in second (<inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>time error</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.0025</mml:mn><mml:mo>±</mml:mo><mml:mn>0.1128</mml:mn><mml:mspace width="thinmathspace"/><mml:mtext>s</mml:mtext></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, mean ± std). (<bold>C</bold>) Absolute error is defined for each lap as the difference between the position in behaviorMate and the benchmark position in millimeters (<inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>position error</mml:mtext><mml:mo>=</mml:mo><mml:mn>13</mml:mn><mml:mo>±</mml:mo><mml:mn>7.6581</mml:mn><mml:mspace width="thinmathspace"/><mml:mtext>mm</mml:mtext></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, mean ± std). (<bold>D</bold>) The time to complete a lap according to behaviorMate and the benchmark were nearly identical. (<bold>E</bold>) Schematic of head-fixed behavioral task. Two-photon objective. (<bold>F</bold>) Cue-rich belts for treadmill behavioral tasks. Belts are easily interchangeable. (<bold>G</bold>) Example test condition spanning 3 days and involving five sensory cues. Each row represents a sensory modality (visual, auditory, etc.). Cues are tied to locations on the belt or wheel as indicated by the colored rectangles. Locations and durations of cue presentations can be changed across trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97433-fig3-v1.tif"/></fig></sec><sec id="s2-3-3"><title>Electronics and peripherals</title><p>Interfacing between the TM and behaviorMate requires the previously described electronics. Any number of optional peripheral devices such as LEDs and odor release systems may be added. A rotary encoder is attached to one of the two wheel shafts and connected to the position tracking circuit to log the animal’s movement. The behaviorMate UI handles the conversion of the rotary encoder’s spinning to the linear distance traveled. It also maintains the animal’s current lap number. An additional ‘lap-reset’ feature prevents drift from accumulating between the digital tracking and physical TM belt. We provide designs for an optical lap-reset circuit for determining when a complete revolution of the belt has been made (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Setting up a lap-reset circuit additionally permits a calibration function that can be temporarily enabled prior to running an experiment to adjust the scaling between rotary encoder ‘ticks’ and distance moved along the TM belt.</p><p>Generally, position tracking drift is small (<xref ref-type="fig" rid="fig3">Figure 3C</xref>) but can occur (especially with more ‘slippery fabrics’ or if belt tension is set too low). It is important to accurately specify the track length and calibrate the position scale factor to tightly couple the PC and the electronics. If the system is not set up properly or the belt is not sufficiently tight, the belt may slip, leading to inaccurate position readings. Therefore, we provide a benchmark of the position tracking performance under normal conditions. To compute the linear distance traveled, a small LED was attached to the top of the belt and the entire TM system was moved to a completely dark room with an IR camera pointed directly at the belt. The belt was moved by hand and upon each complete revolution, the LED would pass within the field of view of the camera. OpenCV (<xref ref-type="bibr" rid="bib10">Bradski, 2000</xref>) was used to extract the lap completion times from the recorded video. These were taken to be the ‘ground truth’ lap completion times. The quantities of interest were the time and position differences between the LED and those tracked by the behaviorMate UI. The mean of the magnitude of the differences between lap times reported and the ‘ground truth’ was 0.102 s (<xref ref-type="fig" rid="fig3">Figure 3B, D</xref>), while the mean difference between position upon lap completion was 13 mm and did not exceed 23 mm (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Given that the mean and max differences were a fraction of the average size of a subject mouse and significantly smaller than the typical size of reward zones, these metrics were deemed more than acceptable. Enabling the lap-reset feature on self-powered TM prevents accumulation of these errors, ensuring that position, as tracked by the UI, is aligned with the fabric belt and cue presentations are aligned with the positions tracked by the UI.</p><p>The UI synchronizes any number of additional sensory cues such as LEDs, tones, and odor release systems, which can be present at pre-defined times or locations along the belt (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). For example, a liquid reward can be coupled to a specific location on the TM belt that is either <italic>operant</italic> (delivered only if the animal licks within a ‘reward zone’) or <italic>non-operant</italic> (delivered immediately when the animal reaches a ‘reward zone’). These ‘reward zones’ are defined by a location and radius, so there is user-specified tolerance for the areas animals can receive operant rewards. The rules governing the rewards are fully customizable and specified in the settings file for the UI.</p></sec><sec id="s2-3-4"><title>Use in past studies</title><p>The physical design of the TM was inspired by <xref ref-type="bibr" rid="bib50">Royer et al., 2012</xref> and influenced by several other hippocampus studies focusing on behavioral navigation tasks (<xref ref-type="bibr" rid="bib33">Kaifosh et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Lovett-Barron et al., 2014</xref>). Our design is flexible and has been validated by the myriad experimental paradigms that have been successfully implemented our lab (<xref ref-type="bibr" rid="bib58">Tuncdemir et al., 2023</xref>; <xref ref-type="bibr" rid="bib60">Vancura et al., 2023</xref>; <xref ref-type="bibr" rid="bib41">O’Hare et al., 2022</xref>; <xref ref-type="bibr" rid="bib48">Rolotti et al., 2022a</xref>; <xref ref-type="bibr" rid="bib48">Rolotti et al., 2022a</xref>; <xref ref-type="bibr" rid="bib56">Terada et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Geiller et al., 2022</xref>; <xref ref-type="bibr" rid="bib8">Blockus et al., 2021</xref>; <xref ref-type="bibr" rid="bib27">Grosmark et al., 2021</xref>; <xref ref-type="bibr" rid="bib23">Geiller et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Kaufman et al., 2020</xref>; <xref ref-type="bibr" rid="bib59">Turi et al., 2019</xref>; <xref ref-type="bibr" rid="bib62">Zaremba et al., 2017</xref>; <xref ref-type="bibr" rid="bib13">Danielson et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Danielson et al., 2016</xref>) as well as set up and run by collaborators (<xref ref-type="bibr" rid="bib57">Tuncdemir et al., 2022</xref>; <xref ref-type="bibr" rid="bib17">Dudok et al., 2021a</xref>; <xref ref-type="bibr" rid="bib18">Dudok et al., 2021b</xref>; <xref ref-type="bibr" rid="bib48">Rolotti et al., 2022a</xref>). Multi-modal cues can be incorporated across several days of experiments to probe different aspects of navigation and associative learning (<xref ref-type="fig" rid="fig3">Figure 3G</xref>). For example, the TM has been used to test goal-oriented learning, where goal locations are ‘hidden’ and animals must report their understanding of the correct location by licking at the reward spout in order to trigger water delivery (<xref ref-type="bibr" rid="bib62">Zaremba et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Danielson et al., 2016</xref>). During these tasks, novel cues can be presented by introducing new TM belts as well as novel non-spatial cues such as background tones, blinking LEDs, or odor stimuli. Moving the reward location forces animals to learn updates to task rules mid-experiment and has been used to investigate deficits in mouse models of psychiatric disorder (<xref ref-type="bibr" rid="bib62">Zaremba et al., 2017</xref>) as well as neural circuit analysis of the mechanisms behind reward learning (<xref ref-type="bibr" rid="bib35">Kaufman et al., 2020</xref>) and the development of signals underpinning spatial navigation (<xref ref-type="bibr" rid="bib49">Rolotti et al., 2022b</xref>; <xref ref-type="bibr" rid="bib56">Terada et al., 2022</xref>; <xref ref-type="bibr" rid="bib12">Danielson et al., 2016</xref>). Cues can be tied to time, location, or both. Switching between a time-dependent presentation and a location-dependent one provides a powerful tool for assessing how the same cue may be processed differently based on the current context, location, or task (<xref ref-type="bibr" rid="bib56">Terada et al., 2022</xref>; <xref ref-type="bibr" rid="bib57">Tuncdemir et al., 2022</xref>). In addition to cue presentation, the setup has also been used to trigger optogenetic stimulation (<xref ref-type="bibr" rid="bib35">Kaufman et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Geiller et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">O’Hare et al., 2022</xref>; <xref ref-type="bibr" rid="bib49">Rolotti et al., 2022b</xref>); this coupling of navigational cues provided by the TM belt with stimulation in hippocampal region CA1 was used for the first successful optical induction of place fields (<xref ref-type="bibr" rid="bib49">Rolotti et al., 2022b</xref>).</p></sec><sec id="s2-3-5"><title>VR system</title><p>The use of VR cues can add significant flexibility to behavioral studies and has thus become increasingly popular in neuroscience. Although different VR implementations have been described, for this manuscript, VR is defined as closed-loop control from the behaviorMate UI that integrates position updates virtually and advances cues past the animal to create the illusion of movement, unlike a belt which physically moves past the animal. Various systems for accomplishing this have been proposed and implemented previously (<xref ref-type="bibr" rid="bib14">Dombeck et al., 2010</xref>; <xref ref-type="bibr" rid="bib29">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib52">Sheffield et al., 2017</xref>; <xref ref-type="bibr" rid="bib28">Hainmueller and Bartos, 2018</xref>; <xref ref-type="bibr" rid="bib11">Campbell et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Arriaga and Han, 2017</xref>; <xref ref-type="bibr" rid="bib6">Arriaga and Han, 2019</xref>; <xref ref-type="bibr" rid="bib17">Dudok et al., 2021a</xref>). Mice can learn to use VR cues as landmarks for traversing environments to specific locations. Neuronal circuits thought to be involved in navigation in freely moving mice have been shown to be similarly engaged during the described VR tasks (<xref ref-type="bibr" rid="bib15">Dombeck and Reiser, 2012</xref>; <xref ref-type="bibr" rid="bib14">Dombeck et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Aronov and Tank, 2014</xref>). behaviorMate represents a novel contribution in that it incorporates visual VR displays into its architecture for 1D navigation and is designed to adapt seamlessly to novel arrangements of VR displays with minimal computational load for each additional display (<xref ref-type="bibr" rid="bib44">Priestley et al., 2022</xref>; <xref ref-type="bibr" rid="bib9">Bowler and Losonczy, 2023</xref>). This is achieved by rendering the virtual environments on separate Android devices (one per display) with a separate application (VRMate). Since the same behaviorMate UI is used for TM and VR experiments, all of the experimental paradigms described in the TM section can be implemented in the VR system with minimal changes to the settings file. Additionally, VR permits rapid visual context switches and the addition of novel virtual objects in a familiar scene. Furthermore, ‘hybrid’ setups are also supported in which visual displays can be added to physical TMs in order to combine both tactile and visual cues.</p></sec><sec id="s2-3-6"><title>VRMate</title><p>VRMate is the companion program to behaviorMate that runs the visual VR interface to display cues to animals as they move in virtual space. VRMate is a program developed using the Unity(TM) game engine and written in C#; it listens for incoming UDP packets carrying position updates from the behaviorMate UI. In our implementation, each visual display is connected to an ODROID C4 which is a single-board computer equipped with an Amlogic S905X3 Quad-Core Cortex-A55 (2.016 GHz) ARMv8-A Processor, 4 GiB of DDR4 RAM, Mali-G31 MP2 GPU, and 1 LAN port, all mounted on a single PCB and running the Android operating system. Each display–ODROID pair runs an instance of the VRMate program, which means scene rendering is done independently by dedicated hardware for each monitor. This allows for scalability to any number of additional displays without compromising frame rate or resolution. Prior to starting an experiment, behaviorMate supplies each instance of VRMate with all the information it needs to correctly display the virtual environment from a particular point of view (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). Notably, Unity is platform independent, so VRMate can be built for Windows, OSX, and other systems. Therefore, it is also not strictly necessary to use dedicated Android devices, and it is possible to run behaviorMate and VRMate on the same PC. Various setups can be configured to meet individual researchers’ needs.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Details of virtual reality (VR) system.</title><p>(<bold>A</bold>) The complete assembly. Five monitors are suspended in an octagonal pattern in front of the subject and display the virtual scenes. (<bold>B</bold>) A running wheel attached to a goniometer to allow for angle of subject’s head to be adjusted. Placed directly underneath the objective. (<bold>C</bold>) Box plot describing the delay between when the running wheel is moved and when the virtual scene is updated to reflect the new position (<inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>delay</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.0660</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01570</mml:mn><mml:mspace width="thinmathspace"/><mml:mtext>s</mml:mtext></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, mean ± std). (<bold>D</bold>) Plot showing the difference between the position of the animal computed by behaviorMate and the ground-truth position tracked using computer vision. (<bold>E</bold>) Box plot describing the absolute difference between the current position of the mouse according to behaviorMate and the computer vision benchmark at each point in time (<inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>position error</mml:mtext><mml:mo>=</mml:mo><mml:mn>4.1258</mml:mn><mml:mo>±</mml:mo><mml:mn>3.2558</mml:mn><mml:mspace width="thinmathspace"/><mml:mtext>mm</mml:mtext></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, mean ± std). (<bold>F</bold>) A 2D projection of the displays which will resemble what the test animal will see. When viewed on the actual monitors, the scene will appear more immersive. (<bold>G</bold>) A virtual reality (VR) scene as the subject moves down the virtual track. Modifying a few numerical parameters in the settings files allows one to change the appearance and view angle of a scene. Bottom: A scene with all red color shaders removed. (<bold>H</bold>) Left: Top view of implant. This side will make contact with microscope objective. Right: Bottom view. This side will make contact with the mouse’s brain. (<bold>I</bold>) Sketch of how the implant will appear after being surgically attached to skull.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97433-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Alternative virtual reality (VR) system.</title><p>(<bold>A</bold>) A version of the VR system that can better fit in tight spaces. Can be made to be self-contained in the event it needs to be moved or swapped with other systems. (<bold>B</bold>) Fixes the position of the mouse’s head by tightly clamping onto each side of the bar implanted into its skull. The screws used to close the two clamps are not shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97433-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Specifically for researchers conducting imaging experiments, Unity supports the implementation of custom ‘shader programs’ which can be enabled through the UI. Implementing custom shaders allows you to edit color-pixel values in the visual scenes just prior to when they are rendered. For example, this means it is possible to remove a particular color emission from the scenes or display with just a single color (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). This is particularly useful for experiments where green or red fluorescent proteins are used to indicate neuronal activity. In this case, rendering scenes with the green or red component of each pixel set to 0 minimizes the risk of artifacts in the recorded data.</p><p>Instead of having separate applications for scene rendering and task logic using VRMate and behaviorMate, respectively, some systems may involve a single application that implements task logic, scene rendering, and communication with electronic controllers such as an Arduino. This approach may be appropriate in certain use cases, however, the recommended behaviorMate architecture has several important advantages. First, by rendering each viewing angle of a scene on a dedicated device, performance is improved by splitting the computational costs across several inexpensive devices rather than requiring specialized or expensive graphics cards in order to run. Moreover, the overall system becomes more modular and easier to test and debug especially when designing multiple different behavioral paradigms. behaviorMate tells VRMate what scene to display, the viewing angle, and the current position while VRMate handles the rest – in this way separating the task controller logic from the VR display modules. Third, implementing task logic in Unity would require understanding Object-Oriented Programming and C# (or learning some other 3D development platform) which is not always accessible to researchers that are typically more familiar with scripting in Python and Matlab. behaviorMate does not necessitate programming because it uses configuration files that can be modified in a standard text editor. Finally, some experiments may not require visual cues at all, in which case a system where the VR component is optional and not tightly integrated with the application is desirable. Of course, in the current implementation VRMate could still be configured to run on a single PC with the display stretched across multiple monitors. This would mitigate any concerns about potential frame rate offsets between screens and simplify the network setup. However, our testing shows the display to be smooth and continuous as VRMate is a lightweight application and the scenes are not graphically intensive and runs at a high frame rate on the suggested hardware.</p></sec><sec id="s2-3-7"><title>Frame and display setup</title><p>Our standard setup for immersive mouse VR experiments contains five monitors surrounding the animal. We supply plans for a VR frame to construct this (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, <italic>also see</italic> Appendix 1). The VR frame is designed with extruded aluminum rails and holds the displays at the proper angle and distance from the animal. The goal is to completely encompass the animal’s visual field with the virtual scene, so the displays are placed in an octagonal setup with the animal at the center. The angle between each display is 135 degrees and the distance between the mouse and the center display is 15 inches. Since mice primarily have a visual field-oriented overhead (<xref ref-type="bibr" rid="bib16">Dräger and Olsen, 1980</xref>), the mouse is positioned 5 inches above the bottom edge of each monitor in order to align the visual stimulus to the animal’s field of view.</p><p>In cases when the standard 5 monitor setup will not fit due to space constraints, we use a wireless, tablet-based setup (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). When VRMate is built for Android OS, it can be installed on any Android tablet. Our tablet system consists of 5 ‘Fire HD 10’ (2019, 9th Gen), 10.1 inch tablets with a screen resolution of 1920 × 1200 IPS while running Android 9. Additional specs include 2 GB RAM, 4xARM Cortex-A73 (2.0 GHZ) and 4xARM Cortex-A53 (2.0 GHz) processors, and an ARM Mali-G72 MP3 GPU. Tablets may be connected to the behaviorMate intranet using a wireless router. The PC running behaviorMate connects through this router to interface with the displays. Each device on the network is assigned a unique static IP address by configuring the IP tables on the Wi-Fi-enabled router to bind each tablet’s IP address to its MAC address. A low-latency router with multiple antennae is recommended and should be placed as close as possible to the tablet displays.</p><p>Typical VR setups position animals on top of a running wheel which is less bulky and mechanically simpler than the fabric belt TM system. The running wheel design used in our setup, as described previously by <xref ref-type="bibr" rid="bib61">Warren et al., 2021</xref> is lightweight and optimized to have low rotational momentum. This provides animals with a natural feeling of running while minimizing the effort required to traverse the virtual environments. Mice are head fixed on top of the wheel using custom head restraining bars and a matching holder using a dove-tail and clamping system. The implant holders (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>) are two L-shaped components machined from solid steel. They have two M6 holes for being attached to pillar posts of aluminum rails. In addition, the implant holders each have a channel which allows the clamps to slide and accommodate head-bars of different lengths. Both the clamps and clamp holders have thumb screws not shown in the figure. The screw for the clamps tightly grips the head-bar and thus minimizes motion artifacts in imaging data. The screw for the clamp holders fixes the clamps’ position and reduces motion artifacts. The head restraining bars for mice presented with visual cues (<xref ref-type="fig" rid="fig4">Figure 4H, I</xref>) are designed to assist with blocking light that may otherwise be detected during imaging. These can be 3D printed from titanium and reused between animals if properly cleaned and sterilized.</p><p>In some cases, a modified running wheel design with goniometers is required to tilt both the implant holders and the entire wheel along the roll and pitch axes (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). For head-fixed 2p imaging, this is used to align the imaging plane to maximize the size of the field of view or, as needed, level the glass coverslip placed over the brain (which minimizes the amount of the material the beam must travel through and maximizes image clarity). For this modified design, aluminum rails are arranged in a ‘U’ shape and then placed on top of two goniometers. The implant holders sit on top of these rails, and the wheel is suspended below the rails, offset from the goniometers. The entire assembly sits on sliding mounts connected to pillar posts so the elevation of the animal’s head can be adjusted.</p></sec><sec id="s2-3-8"><title>Example setup</title><p>A simple VR system could have two displays in an arrow configuration directly in front of a mouse which is head fixed and mounted on top of a running wheel. Attached to the back of each monitor would be a small Android computer (ODROID) connected via an HDMI cable. The left screen could be given an IP address of 192.168.1.141. The right machine will be assigned 192.168.1.142. Static IP assignment is done through the Android devices’ settings menu. Arduinos running the Behavior and Position Controllers are given the IPs of 192.168.1.101 and 192.168.1.102, respectively, which is the default in our supplied code (<italic>see</italic> Appendix 1). Finally, a PC running the UI is assigned an IP of 192.168.1.100. We use standard IP address and ports for each of the displays and electronics; however, the IPs that the behaviorMate UI will use are specified in the settings file and determined at run-time (the Arduino IP assignments are made when the devices are loaded). Each device is also assigned a port which can be selected arbitrarily and can be changed as needed. The only requirement is that all devices are on the same subnet. In this example setup, all devices share the same network prefix of 192.168.1.x so they are part of the same subnet and will be able to communicate. Given that all components are connected to the same local network, usually via an unmanaged switch, packets will arrive to their intended recipient.</p><p>When the animal runs, turning the wheel and rotary encoder, the behaviorMate UI will receive and integrate movement updates from the Position Controller. Once the experiment is started from the UI, the PC will evaluate which contexts are active and send position updates to both screens simultaneously. The VRMate instances will receive these packets and render their view of the same scene independently. Latency between the UI and the computers running VRMate is fairly low (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) so this does not present an issue. As the animal runs, it enters a reward zone. behaviorMate will send a message to the Behavior Controller triggering the ‘reward context’, causing a reward valve to release a drop of water or sucrose reward. As the mouse runs, the visual scene updates accordingly. Once the mouse travels a linear distance equal to the ‘track_length’ parameter specified in the settings file, behaviorMate advances the lap count and resets the position to 0. Between-lap ‘time outs’ or dark periods may also be configured. Furthermore, the user may choose to run a ‘context-switch’ experiment: this type of experiment might involve one scene being displayed on odd laps and another on even laps. If a scene change is necessary, between laps, behaviorMate will send a message to both running instances of VRMate that specify the switch to the alternate scene.</p></sec><sec id="s2-3-9"><title>VR system performance</title><p>One key concern with closed-loop control VR experimental setups is the latency between when the animal moves the physical wheel and when the displays update position in the virtual environment. We therefore performed a benchmarking test on our standard 5 monitors VR/running wheel setup (with scenes rendered by ODROIDs connected to the UI via an unmanaged Ethernet switch). An impulse was applied to the running wheel, and the time taken for the screens to update was observed. The screen delay was measured using a high-speed 150 fps camera. Latency was sampled 20 times and found to have a median value of 0.067 s, with no sample exceeding 0.087 s (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Since mouse reaction times are approximately 0.1 s (<xref ref-type="bibr" rid="bib15">Dombeck and Reiser, 2012</xref>; <xref ref-type="bibr" rid="bib39">Mauk and Buonomano, 2004</xref>), VR position updates using this system are nearly instantaneous to the mouse. Moreover, because position updates are constantly being sent by behaviorMate to VRMate and VRMate is immediately updating the scene according to this position, the most the scene can become out of sync with the mouse’s position is proportional to the maximum latency multiplied by the running speed of the mouse. Such a degree of asynchrony is almost always negligible.</p><p>As with the fabric TM, another characteristic that should be considered for the running wheel is the accuracy of the position tracking system. Accuracy refers to how closely the mouse’s virtual position recorded by behaviorMate matches the true linear distance it has run on the wheel. To find the ‘true’ linear distance traveled, a small dim LED was attached to the side of the running wheel, and the whole setup was placed in a dark enclosure, so the only significant source of light was the LED. The wheel was advanced by hand, and using OpenCV (<xref ref-type="bibr" rid="bib10">Bradski, 2000</xref>), the total distance traversed by the LED was extracted. Concurrently, behaviorMate was detecting the current position using the attached rotary encoder. Graphs of position versus time from the data collected using computer vision and the data recorded by behaviorMate were overlaid. From <xref ref-type="fig" rid="fig4">Figure 4D</xref>, it is clear that there is little discrepancy between the two positions. Across all time points, the mean difference between the true and recorded position was 4.1 mm.</p></sec><sec id="s2-3-10"><title>Use in past experiments</title><p>While the introduction of a purely visual VR option for behaviorMate represents one of the newest updates to the setup, studies have already been published that specifically leverage this capability, primarily to examine the effect of rapid within-session context changes (<xref ref-type="bibr" rid="bib44">Priestley et al., 2022</xref>; <xref ref-type="bibr" rid="bib9">Bowler and Losonczy, 2023</xref>). VR experiments have the ability to transport animals to completely novel scenes which has been useful for investigating the mechanisms of synaptic plasticity, since it is possible to capture the animals first exposure to an environment in a controlled way (<xref ref-type="bibr" rid="bib44">Priestley et al., 2022</xref>). Since the visual VR setup is integrated into behaviorMate, it is also possible to test how animals respond to changes in the rules governing reward delivery and visual scene presentation. The system further allows for simultaneous delivery of multi-modal stimuli simultaneously such as audio and visual. Thus, our system comprises an adaptable framework for probing the function of navigational circuits in the brain and the relationship between goal and context representations (<xref ref-type="bibr" rid="bib9">Bowler and Losonczy, 2023</xref>).</p></sec></sec><sec id="s2-4"><title>Experimental validation</title><p>To investigate if spatially tuned ’place cells’ (<xref ref-type="bibr" rid="bib42">O’Keefe and Dostrovsky, 1971</xref>) with similar properties emerge in both the belt TM system and the VR running wheel system, we performed 2p imaging in CA1 of the hippocampus while animals explored in either VR or TM. Mice were injected with GCaMP8m (AAV1-syn-GCaMP8m-WPRE, Addgene) and implanted with a glass window to have optical access to CA1 (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Once mice recovered from surgeries, they were trained to do a spatial navigation task in either VR or TM. After the mice were fully trained, 2p imaging was performed during navigation. Calcium signals were processed with Suite2p for motion correction, cell segmentation, and fluorescent signal and neuropil extraction (<xref ref-type="bibr" rid="bib43">Pachitariu et al., 2017</xref>). Extracted raw fluorescent signals were corrected for neuropil contamination, and resulting Δ<italic>F</italic>/<italic>F</italic> was used to estimate spike events using OASIS (<xref ref-type="bibr" rid="bib20">Friedrich et al., 2017</xref>). A median filter was applied to binarize spike amplitudes into bins of position, and all subsequent analyses used this binarized train as an estimation of each ROIs activity. We recorded on average 1533 ± 481 and 1057 ± 169 CA1 neurons in VR and TM, respectively. Although VR had significantly fewer CA1 neurons being classified as place cells compared to TM (VR: 34.9% (3253/9195), TM: 44.9% (6101/13,454), two sample <italic>z</italic>-test, <italic>z</italic> = 14.96, p = 1.27e−50), place cells tiled the entire track in both VR and TM (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). In all subsequent analyses, we used CA1 neurons identified as a place cell unless otherwise stated.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Two-photon imaging of CA1 population for experimental validation.</title><p>(<bold>A</bold>) Field of view obtained from virtual reality (VR) and treadmill (TM). (<bold>B</bold>) Examples of place cells recorded using VR or TM. In both familiar and novel contexts, place cells encoding different locations on the track are found. Note that they are different place cells from individual animals. (<bold>C</bold>) Mean Δ<italic>f</italic>/<italic>f</italic> amplitude of place cells recorded each context–environment pair (F = familiar context, N = novel context). (<bold>D</bold>) Mean Δ<italic>f</italic>/<italic>f</italic> frequency of place cells. For (<bold>C, D</bold>), only significant Δ<italic>f</italic>/<italic>f</italic> events were used. See methods for details. (<bold>E</bold>) Place sensitivity in each context–environment pair. Significant main effect of environment (VR vs. TM, p = 0.0257). (<bold>F</bold>) Place cell specificity in each context–environment pair. Significant main effect of environment (VR vs. TM, p = 0.008). (<bold>G</bold>) Spatial information (bits/event) in each context–environment pair. Two-way repeated measures of ANOVA were used unless otherwise stated. Refer to Table <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref> for ANOVA table. For panels C–G, <italic>n</italic> = 6 mice (all mice were recorded in both VR and TM systems), 3253 cells in VR classified as significantly tuned place cells VR, and 6101 tuned cells in TM. For all panels, n.s. = non-significant, *p &lt; 0.05, ***p &lt; 0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97433-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Additional place cell properties.</title><p>(<bold>A</bold>) Proportion of place cells in virtual reality (VR) and treadmill (TM). VR had significantly less regions of interest (ROIs) that were classified as a place cell compared to TM (two sample <italic>z</italic>-test, <italic>F</italic> = 14.96, p = 1.27e−50). (<bold>B</bold>) Proportion of place cells in each context–environment pair. (<bold>C</bold>) Mean velocity per lap in each context–environment pair. Significant main effect of environment (<italic>F</italic>(1,5) = 35.95, p = 0.0019). (<bold>D</bold>) Peak location of place cells recorded in each context–environment pair. Note that place cells tile entire track in all pairs. For all panels, **p &lt; 0.01 ***p &lt; 0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97433-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>ANOVA results table of ANOVA results relating to <xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</title><p><supplementary-material id="fig5s2sdata1"><label>Figure 5—figure supplement 2—source data 1.</label><caption><title>Data file containing a digital copy of the information presented in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-97433-fig5-figsupp2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97433-fig5-figsupp2-v1.tif"/></fig></fig-group><p>To assess differences in the calcium signal of place cells found in context–environment pairs, we first compared the Δ<italic>F</italic>/<italic>F</italic> amplitude and frequency of individual ROIs identified as place cells. Only Δ<italic>F</italic>/<italic>F</italic> events that were 3 standard deviations above the mean Δ<italic>F</italic>/<italic>F</italic> were included in these analyses. To examine the effect of environment (VR vs. TM) and context (familiar (F) vs. novel (N)) on Δ<italic>F</italic>/<italic>F</italic> amplitude and frequency, we conducted two-way repeated measures of ANOVA with both environment and context as a within-subject factors. There was no significant effect of environment or context on mean Δ<italic>F</italic>/<italic>F</italic> amplitude (environment: <italic>F</italic>(1,5) = 1.9387, p = 0.222, context: <italic>F</italic>(1,5) = 0.0037, p = 0.9537, <xref ref-type="fig" rid="fig5">Figure 5C</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>) or on mean Δ<italic>F</italic>/<italic>F</italic> frequency (environment: <italic>F</italic>(1,5) = 4.37, p = 0.091, context: <italic>F</italic>(1,5) = 0.0789, p = 0.7901, <xref ref-type="fig" rid="fig5">Figure 5D</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). These results suggest that calcium transient properties of place cells observed in both environments and contexts were similar. Next, we quantified individual place cells’ specificity, sensitivity, and spatial information to identify any differences in place cell firing properties between context–environment pairs. As described above, two-way repeated measures of ANOVA were used with both context and environment as within-subject factors. The two-way ANOVA revealed a significant effect of environment on sensitivity (environment: <italic>F</italic>(1,5) = 9.85, p = 0.0257, context: <italic>F</italic>(1,5) = 3.34, p = 0.13, <xref ref-type="fig" rid="fig5">Figure 5E</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>) and specificity (environment: <italic>F</italic>(1,5) = 53.32, p = 0.0008, context: <italic>F</italic>(1,5) = 2.55, p = 0.17, <xref ref-type="fig" rid="fig5">Figure 5F</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), suggesting that place cells in TM have higher trial-by-trial in-field firing compared to those from VR. However, we did not observe any significant effect of environment or context on spatial information (<xref ref-type="bibr" rid="bib54">Skaggs and McNaughton, 1998</xref>) (environment: <italic>F</italic>(1,5) = 0.74, p = 0.43, context: <italic>F</italic>(1,5) = 3.79, p = 0.11, <xref ref-type="fig" rid="fig5">Figure 5G</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). These results are consistent with previous findings that place cells in VR have reduced trial-by-trial reliability (<xref ref-type="bibr" rid="bib46">Ravassard et al., 2013</xref>; <xref ref-type="bibr" rid="bib1">Aghajan et al., 2015</xref>), but still encode spatial information (<xref ref-type="bibr" rid="bib14">Dombeck et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Aronov and Tank, 2014</xref>; <xref ref-type="bibr" rid="bib28">Hainmueller and Bartos, 2018</xref>; <xref ref-type="bibr" rid="bib44">Priestley et al., 2022</xref>). The observed differences in the properties of VR and TM place cells may be the result of the absence of proximal visual cues and tactile cues (which are speculated to be more salient than visual cues) during VR tasks (<xref ref-type="bibr" rid="bib36">Knierim and Rao, 2003</xref>; <xref ref-type="bibr" rid="bib47">Renaudineau et al., 2007</xref>; <xref ref-type="bibr" rid="bib55">Sofroniew et al., 2014</xref>).</p><p>Next, we wanted to investigate if place cells remap in both VR and TM environments when mice are exposed to a novel context (<xref ref-type="bibr" rid="bib40">Muller and Kubie, 1987</xref>; <xref ref-type="bibr" rid="bib37">Leutgeb et al., 2005</xref>). As expected, a subset of place cells changed their place tuning properties when exposed to novel contexts (<xref ref-type="fig" rid="fig6">Figure 6A, B</xref>). Moreover, we did not observe differences in the between context rate map correlation between VR and TM, suggesting that remapping happens in both environment types. To further quantify the extent of remapping in each environment, we built a naive Bayes decoder to decode position based on population activity of CA1 neurons. When the decoder was trained with population activity from familiar trials and tested with population activity from a held-out familiar trial, the absolute decoding error was significantly below chance (p &lt; 0.05) in both VR and TM. However, when the decoder trained with familiar trials was tested to decode position in a novel trial, the absolute decoding error was at the chance level (<xref ref-type="fig" rid="fig6">Figure 6C, D</xref>) in both VR and TM. These suggest that place cells remap in both VR and TM when exposed to novel context.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Place cells remap in both environments.</title><p>(<bold>A</bold>) Place cells recorded in virtual reality (VR) or treadmill (TM) sorted by their familiar peak (top) or novel peak (bottom) locations. A subset of CA1 neurons remaps when exposed to novel context in both VR and TM. (<bold>B</bold>) Rate map correlation of individual place cells between familiar and novel contexts. No difference was observed between VR and TM (Wilcoxon rank-sum test, <italic>F</italic> = 1.38, p = 0.17). (<bold>C</bold>) Confusion matrices of the Bayesian decoder for VR and TM. F–F indicates a model trained and tested with trials from the familiar context. F–N is a model trained with trials from familiar context, and tested with a novel trial. (<bold>D</bold>) Absolute decoding error for the maximum likelihood estimator. A dashed line indicates a chance level (88 ± 1.96 cm for VR/ 50 cm for TM, see methods). Note that when the decoder is trained and tested with familiar context trials, absolute decoding accuracy is below chance in both VR and TM (VR: 27.62 ± 1.44 cm, p &lt; 0.01, TM: 17.78 ± 0.44 cm, p &lt; 0.01, permutation test). However, when the decoder is trained with familiar context trials, but tested with novel context trials, decoding accuracy was increased to the chance level (VR: 99.80 ± 3.95 cm, p = 1.00, TM: 56.82 ± 2.46 cm, p = 1.00).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-97433-fig6-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>behaviorMate is, most generally, a solution for controlling and collecting feedback from devices in behavioral experiments. The behaviorMate methodology is built around an ‘Intranet of Things’ approach, expanding to incorporate additional components to meet experimental demands but also compatible with sparser setups to ease complexity when not needed. It has typically been used concurrently with neuronal imaging, since the timestamped output of behavior events is relatively simple to align with imaging data, providing experimenters with the ability to correlate neuronal events with behavior. In addition, behaviorMate is designed to be flexible, expanding to incorporate additional components as specified in the settings while making minimal assumptions. Experimenters may incorporate other techniques such as imaging and electrophysiology recordings or just run behavior. For experimental paradigms involving any combination of displays, 1D movement (wheels, TMs, etc.), sensors (LEDs, Capacitance lick sensors, etc.), and actuators (odor release systems, solenoid valves, etc.), this is a cost-effective and reliable solution.</p><p>The main modules of behaviorMate are the Java GUI application, referred to as the UI, Behavior Controller, and Position Controller. The UI coordinates most communication between devices, although sometimes devices directly communicate with each other for performance reasons. The Behavior Controller sends messages to and receives data from nearly any kind of peripheral devices such as 2p microscopes, LEDs, speakers, computer displays, and electrophysiology systems. The Position Controller is solely responsible for receiving position updates from the rotary encoder. Any number of additional devices may be used by connecting them to the behavior controller using standard connectors. Furthermore, the companion software of VRMate allows for low-cost integration of VR experiments that are already compatible with behaviorMate. The appeal of behaviorMate is its modularity, reliability, and open-source code base.</p><p>Finally, many published studies have relied on this system and the resulting data collected (<xref ref-type="bibr" rid="bib9">Bowler and Losonczy, 2023</xref>; <xref ref-type="bibr" rid="bib58">Tuncdemir et al., 2023</xref>; <xref ref-type="bibr" rid="bib60">Vancura et al., 2023</xref>; <xref ref-type="bibr" rid="bib44">Priestley et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">O’Hare et al., 2022</xref>; <xref ref-type="bibr" rid="bib48">Rolotti et al., 2022a</xref>; <xref ref-type="bibr" rid="bib57">Tuncdemir et al., 2022</xref>; <xref ref-type="bibr" rid="bib49">Rolotti et al., 2022b</xref>; <xref ref-type="bibr" rid="bib56">Terada et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Geiller et al., 2022</xref>; <xref ref-type="bibr" rid="bib8">Blockus et al., 2021</xref>; <xref ref-type="bibr" rid="bib27">Grosmark et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Rolotti et al., 2022a</xref>; <xref ref-type="bibr" rid="bib23">Geiller et al., 2020</xref>; <xref ref-type="bibr" rid="bib35">Kaufman et al., 2020</xref>; <xref ref-type="bibr" rid="bib59">Turi et al., 2019</xref>; <xref ref-type="bibr" rid="bib62">Zaremba et al., 2017</xref>; <xref ref-type="bibr" rid="bib13">Danielson et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Danielson et al., 2016</xref>). These paradigms varied greatly and included random foraging, goal-oriented learning, multi-modal cue presentations, and context switches (both in VR environments and physical TMs). Instructions for downloading the UI software and the companion VRMate software, and assembling a Behavior Controller, Position Controller, running wheel, VR monitor frame, and TM, can be found on the Losonczy Lab website (<ext-link ext-link-type="uri" xlink:href="https://www.losonczylab.org/software">https://www.losonczylab.org/software</ext-link>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Lead contact and materials availability</title><p>Further information and requests for resources and reagents should be directed to the Lead Contact Attila Losonczy (<ext-link ext-link-type="uri" xlink:href="https://www.neurosciencephd.columbia.edu/content/attila-losonczy-md-phd">al2856@columbia.edu</ext-link>). All unique resources generated in this study are available from the Lead Contact with a completed Materials Transfer Agreement.</p></sec><sec id="s4-2"><title>Experimental model and subject details</title><sec id="s4-2-1"><title>Animals</title><p>All experiments were conducted in accordance with the NIH guidelines and with the approval of the Columbia University Institutional Animal Care and Use Committee (Protocol #AC-AABF6554). Experiments were performed with six adult mice between 8 and 16 weeks. Five mice were Unc5b crossed with C57Bl/6, and one mouse was VGAT-cre crossed with Ai9 (Jackson Laboratory). Three female and three male mice were used in this experiment to balance for sex.</p></sec><sec id="s4-2-2"><title>Surgical procedures</title><p>Surgeries were done as previously described (<xref ref-type="bibr" rid="bib38">Lovett-Barron et al., 2014</xref>). Briefly, animals were anesthetized with isoflurane and injected with jGCaMP8m (rAAV1-syn-GCaMP8m-WPRE, Addgene: 162375-AAV1) in dorsal CA1 (AP: 2.1, ML: 1.4, DV: 1.3, 1,2, 1,1, 100 µl per site) using Nanoject III (Drummond). Animals were allowed to recover from the injection surgery for 7 days, and the cortex lying above CA1 was aspirated to implant a glass cannula to enable optical access to CA1. Finally, a titanium headpost was cemented to the skull using dental cement to head-fix animals for 2p functional imaging. Analgesics were provided as per Columbia University Institutional Animal Care and Use Committee prior and after the surgeries for up to 3 days.</p></sec><sec id="s4-2-3"><title>2p imaging and behavior</title><p>After recovering from surgeries, animals were habituated to head fixation in a VR environment for 15 min. Subsequently, animals were trained to run on a lightweight wheel, and 5% sucrose was given at random locations to motivate animals’ to run in the VR or TM. When animals reliably ran at least 60 laps in VR, or 30 laps in TM, the reward was fixed to either one or two locations. A 3-m VR track was used for all animals except jy030, which had 4-m VR track instead, and 2-m TM belt was used in all animals. For VR experiments, animals were imaged for one session, and the context switch happened after animals ran 30 laps in familiar context. Animals were required to run at least another 30 laps in the novel context. In TM, imaging was done over 2 days (first day: familiar belt, second day: novel belt). 2p imaging was performed using a 8-kHz resonant scanner (Bruker) and a 16× Nikon water immersion objective (0.8 NA, 3 mm working distance). In order to image GCaMP8m signals, 920 nm excitation wavelength was used (Coherent), and the power from the tip of the objective never exceeded 100 mW. GCaMP signals were collected using a GaAsP photomultiplier tube detector (Hamamatsu, 7422P-40) following amplification via a custom dual stage preamplifier (1.4 × 105 dB, Bruker). All imaging was performed using 512 × 512 pixels with digital zoom between 1 and 1.4.</p></sec></sec><sec id="s4-3"><title>Calcium signal processing</title><p>Imaging data was processed as previously described (<xref ref-type="bibr" rid="bib44">Priestley et al., 2022</xref>). In short, the SIMA software package (<xref ref-type="bibr" rid="bib34">Kaifosh et al., 2014</xref>) was used to organize the imaging dataset, and the imaging data was processed with Suite2p (<xref ref-type="bibr" rid="bib43">Pachitariu et al., 2017</xref>) for motion correction, signal/neuropil extraction, and ROI detection. Following ROI detection, individual ROIs were manually curated using the Suite2p GUI to exclude non-somatic ROIs. Δ<italic>F</italic>/<italic>F</italic> signals were calculated after subtracting the neuropil and correcting for baseline.</p></sec><sec id="s4-4"><title>Data analysis</title><p>As previously described, all analyses were done using binarized event signals (<xref ref-type="bibr" rid="bib2">Ahmed et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Priestley et al., 2022</xref>). Briefly, Δ<italic>F</italic>/<italic>F</italic> signals were deconvolved using OASIS (<xref ref-type="bibr" rid="bib20">Friedrich et al., 2017</xref>), and the resulting estimated spike amplitude train was binarized by selecting events whose amplitudes were above 4 median absolute deviations of the raw trace. We do not claim that this reflects the true spiking activities of individual ROIs. Binarized event trains were binned into 4 cm bins and smoothed with a Gaussian filter (SD = 2) to calculate tuning curves. Individual ROIs were classified as a place cell by identifying bins that have activities greater than the 99th percentile of surrogate tuning curves (<xref ref-type="bibr" rid="bib44">Priestley et al., 2022</xref>). In short, surrogate average tuning curves were calculated by performing 1000 circular shuffles of their occupancy of individual trials. Bins that had activity greater than 99th percentile of the tuning curves were identified as potentially significant bins. In order to be classified as a place cell, ROIs needed to have at least 3 consecutive significant bins (12 cm), but less than 25 consecutive bins (100 cm). Moreover, in order to avoid spurious detection of significant bins, binarized events must have been detected in at least 50% of trials within those significant bins.</p><p>For mean Δ<italic>F</italic>/<italic>F</italic> amplitude and frequency calculations, only bins with 3 standard deviations above the mean were used. For place cell sensitivity, representing the proportion of laps that had active events within the significant field, the number of laps that had at least one binarized event within the significantly detected fields was divided by the total number of laps. For place cell specificity, total number of events within the significant field was divided by the total number of events observed within the lap. Then, it was averaged across all laps to have a single value for each ROI. If multiple fields were detected, they were computed separately and averaged across fields to have a single value for each ROI. For spatial information, it was calculated as described previously (<xref ref-type="bibr" rid="bib54">Skaggs and McNaughton, 1998</xref>). For remapping analysis, one animal was used for the analysis (jy065). Given that TM imaging happened across 2 days, FOVs recorded from TM were matched using CellReg (<xref ref-type="bibr" rid="bib53">Sheintuch et al., 2017</xref>), and spatial mask correlation was used to find the same ROIs. Results were manually curated within the Suite2p GUI. This was not necessary for VR since the context switch happened within the same session. To calculate rate map correlation, the Pearson correlation coefficient was computed between average tuning curves from the familiar and novel contexts. For TM, given that place cells may anchor on seams of the belt, the shift that generated the maximum population vector correlation between familiar and novel contexts was calculated. All ROIs recorded in the novel context were circularly shifted by the same amount, and the rate map correlation was calculated as described above. A naive Bayesian classifier was used to decode animals’ position on the track (<xref ref-type="bibr" rid="bib63">Zhang et al., 1998</xref>). For each frame and spatial bin:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <italic>N</italic> is the total number of cells, <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is the average binned activity of a cell, <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi></mml:mstyle></mml:math></inline-formula> is the bin size, and <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the activity on the frame. <italic>C</italic> is the normalization constant. The position bin with the highest probability was selected as a decoded position. Chance level for VR was calculated by randomly shuffling cell labels of the testing set. For TM, a chance level was determined as track length/4, which is a chance level of a circular environment (50 cm). The decoder was trained with <italic>n</italic> − 1 trials and tested on the left out trial. To match ROI numbers recorded between VR and TM, 500 ROIs were randomly selected, and the decoding accuracy was calculated 50 times. Average absolute decoding accuracy was compared to the chance level calculated from each iteration, and the number of times that it was below the chance level was considered to be its p value.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Formal analysis, Investigation, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Investigation, Methodology</p></fn><fn fn-type="con" id="con5"><p>Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Software, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Data curation, Software, Investigation, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con8"><p>Resources, Supervision, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. All of the animals were handled according to approved Institutional Animal Care and Use Committee (IACUC) protocols of Columbia University (#AC-AABF6554). The protocol was approved by the Committee on the Ethics of Animal Experiments of Columbia University. All surgery was performed under sodium isoflurane anesthesia, and every effort was made to minimize suffering.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-97433-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Requests for further information may be directed to the Lead Contact Attila Losonczy (<ext-link ext-link-type="uri" xlink:href="mailto:al2856@columbia.edu">al2856@columbia.edu</ext-link>). Data referenced in Figures 3–6 are openly hosted on Dryad at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.02v6wwqdf">https://doi.org/10.5061/dryad.02v6wwqdf</ext-link>. See Appendix 1 for links to all software referenced by this manuscript.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Bowler</surname><given-names>J</given-names></name><name><surname>Zakka</surname><given-names>G</given-names></name><name><surname>Yong</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Rao</surname><given-names>B</given-names></name><name><surname>Liao</surname><given-names>Z</given-names></name><name><surname>Priestley</surname><given-names>JB</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>behaviorMate: An Intranet of Things Approach for Adaptable Control of Behavioral and Navigation-Based Experiments</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.02v6wwqdf</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Drs. Ali Kaufman and Ally Lowell for troubleshooting and debugging the initial iterations of behaviorMate as well as Dr. Andres Grosmark for continued feedback on the project. Additionally, we thank Dr. Stephanie Herrlinger, Abhishek Shah, and all other members of the Losonczy lab for comments and discussion on the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aghajan</surname><given-names>ZM</given-names></name><name><surname>Acharya</surname><given-names>L</given-names></name><name><surname>Moore</surname><given-names>JJ</given-names></name><name><surname>Cushman</surname><given-names>JD</given-names></name><name><surname>Vuong</surname><given-names>C</given-names></name><name><surname>Mehta</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Impaired spatial selectivity and intact phase precession in two-dimensional virtual reality</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>121</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1038/nn.3884</pub-id><pub-id pub-id-type="pmid">25420065</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmed</surname><given-names>MS</given-names></name><name><surname>Priestley</surname><given-names>JB</given-names></name><name><surname>Castro</surname><given-names>A</given-names></name><name><surname>Stefanini</surname><given-names>F</given-names></name><name><surname>Solis Canales</surname><given-names>AS</given-names></name><name><surname>Balough</surname><given-names>EM</given-names></name><name><surname>Lavoie</surname><given-names>E</given-names></name><name><surname>Mazzucato</surname><given-names>L</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hippocampal network reorganization underlies the formation of a temporal association memory</article-title><source>Neuron</source><volume>107</volume><fpage>283</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.04.013</pub-id><pub-id pub-id-type="pmid">32392472</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Lustig</surname><given-names>A</given-names></name><name><surname>Rowland</surname><given-names>JM</given-names></name><name><surname>Kapanaiah</surname><given-names>SK</given-names></name><name><surname>Esteve-Agraz</surname><given-names>J</given-names></name><name><surname>Panniello</surname><given-names>M</given-names></name><name><surname>Márquez</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>MM</given-names></name><name><surname>Kätzel</surname><given-names>D</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Open-source, python-based, hardware and software for controlling behavioural neuroscience experiments</article-title><source>eLife</source><volume>11</volume><elocation-id>e67846</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67846</pub-id><pub-id pub-id-type="pmid">35043782</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aronov</surname><given-names>D</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Engagement of neural circuits underlying 2D spatial navigation in a rodent virtual reality system</article-title><source>Neuron</source><volume>84</volume><fpage>442</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.042</pub-id><pub-id pub-id-type="pmid">25374363</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arriaga</surname><given-names>M</given-names></name><name><surname>Han</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>DEdicated hippocampal inhibitory networks for locomotion and immobility</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>9222</fpage><lpage>9238</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1076-17.2017</pub-id><pub-id pub-id-type="pmid">28842418</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arriaga</surname><given-names>M</given-names></name><name><surname>Han</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Structured inhibitory activity dynamics in new virtual environments</article-title><source>eLife</source><volume>8</volume><elocation-id>e47611</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47611</pub-id><pub-id pub-id-type="pmid">31591960</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittner</surname><given-names>KC</given-names></name><name><surname>Milstein</surname><given-names>AD</given-names></name><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behavioral time scale synaptic plasticity underlies CA1 place fields</article-title><source>Science</source><volume>357</volume><fpage>1033</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.1126/science.aan3846</pub-id><pub-id pub-id-type="pmid">28883072</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blockus</surname><given-names>H</given-names></name><name><surname>Rolotti</surname><given-names>SV</given-names></name><name><surname>Szoboszlay</surname><given-names>M</given-names></name><name><surname>Peze-Heidsieck</surname><given-names>E</given-names></name><name><surname>Ming</surname><given-names>T</given-names></name><name><surname>Schroeder</surname><given-names>A</given-names></name><name><surname>Apostolo</surname><given-names>N</given-names></name><name><surname>Vennekens</surname><given-names>KM</given-names></name><name><surname>Katsamba</surname><given-names>PS</given-names></name><name><surname>Bahna</surname><given-names>F</given-names></name><name><surname>Mannepalli</surname><given-names>S</given-names></name><name><surname>Ahlsen</surname><given-names>G</given-names></name><name><surname>Honig</surname><given-names>B</given-names></name><name><surname>Shapiro</surname><given-names>L</given-names></name><name><surname>de Wit</surname><given-names>J</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name><name><surname>Polleux</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Synaptogenic activity of the axon guidance molecule Robo2 underlies hippocampal circuit function</article-title><source>Cell Reports</source><volume>37</volume><elocation-id>109828</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109828</pub-id><pub-id pub-id-type="pmid">34686348</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowler</surname><given-names>JC</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Direct cortical inputs to hippocampal area CA1 transmit complementary signals for goal-directed navigation</article-title><source>Neuron</source><volume>111</volume><fpage>4071</fpage><lpage>4085</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.09.013</pub-id><pub-id pub-id-type="pmid">37816349</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2000">2000</year><data-title>The opencv library</data-title><version designator="01">01</version><source>Dr Dobb’s Journal of Software Tools</source><ext-link ext-link-type="uri" xlink:href="https://www.geeksforgeeks.org/opencv-overview/">https://www.geeksforgeeks.org/opencv-overview/</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>MG</given-names></name><name><surname>Ocko</surname><given-names>SA</given-names></name><name><surname>Mallory</surname><given-names>CS</given-names></name><name><surname>Low</surname><given-names>IIC</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Principles governing the integration of landmark and self-motion cues in entorhinal cortical codes for navigation</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1096</fpage><lpage>1106</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0189-y</pub-id><pub-id pub-id-type="pmid">30038279</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danielson</surname><given-names>NB</given-names></name><name><surname>Kaifosh</surname><given-names>P</given-names></name><name><surname>Zaremba</surname><given-names>JD</given-names></name><name><surname>Lovett-Barron</surname><given-names>M</given-names></name><name><surname>Tsai</surname><given-names>J</given-names></name><name><surname>Denny</surname><given-names>CA</given-names></name><name><surname>Balough</surname><given-names>EM</given-names></name><name><surname>Goldberg</surname><given-names>AR</given-names></name><name><surname>Drew</surname><given-names>LJ</given-names></name><name><surname>Hen</surname><given-names>R</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name><name><surname>Kheirbek</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distinct contribution of adult-born hippocampal granule cells to context encoding</article-title><source>Neuron</source><volume>90</volume><fpage>101</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.019</pub-id><pub-id pub-id-type="pmid">26971949</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danielson</surname><given-names>NB</given-names></name><name><surname>Turi</surname><given-names>GF</given-names></name><name><surname>Ladow</surname><given-names>M</given-names></name><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Petrantonakis</surname><given-names>PC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>In vivo imaging of dentate gyrus mossy cells in behaving mice</article-title><source>Neuron</source><volume>93</volume><fpage>552</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.019</pub-id><pub-id pub-id-type="pmid">28132825</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dombeck</surname><given-names>DA</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Tian</surname><given-names>L</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional imaging of hippocampal place cells at cellular resolution during virtual navigation</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1433</fpage><lpage>1440</lpage><pub-id pub-id-type="doi">10.1038/nn.2648</pub-id><pub-id pub-id-type="pmid">20890294</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dombeck</surname><given-names>DA</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Real neuroscience in virtual worlds</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>3</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.10.015</pub-id><pub-id pub-id-type="pmid">22138559</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dräger</surname><given-names>UC</given-names></name><name><surname>Olsen</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Origins of crossed and uncrossed retinal projections in pigmented and albino mice</article-title><source>The Journal of Comparative Neurology</source><volume>191</volume><fpage>383</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1002/cne.901910306</pub-id><pub-id pub-id-type="pmid">7410600</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dudok</surname><given-names>B</given-names></name><name><surname>Klein</surname><given-names>PM</given-names></name><name><surname>Hwaun</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>BR</given-names></name><name><surname>Yao</surname><given-names>Z</given-names></name><name><surname>Fong</surname><given-names>O</given-names></name><name><surname>Bowler</surname><given-names>JC</given-names></name><name><surname>Terada</surname><given-names>S</given-names></name><name><surname>Sparks</surname><given-names>FT</given-names></name><name><surname>Szabo</surname><given-names>GG</given-names></name><name><surname>Farrell</surname><given-names>JS</given-names></name><name><surname>Berg</surname><given-names>J</given-names></name><name><surname>Daigle</surname><given-names>TL</given-names></name><name><surname>Tasic</surname><given-names>B</given-names></name><name><surname>Dimidschstein</surname><given-names>J</given-names></name><name><surname>Fishell</surname><given-names>G</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Soltesz</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Alternating sources of perisomatic inhibition during behavior</article-title><source>Neuron</source><volume>109</volume><fpage>997</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.01.003</pub-id><pub-id pub-id-type="pmid">33529646</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dudok</surname><given-names>B</given-names></name><name><surname>Szoboszlay</surname><given-names>M</given-names></name><name><surname>Paul</surname><given-names>A</given-names></name><name><surname>Klein</surname><given-names>PM</given-names></name><name><surname>Liao</surname><given-names>Z</given-names></name><name><surname>Hwaun</surname><given-names>E</given-names></name><name><surname>Szabo</surname><given-names>GG</given-names></name><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Vancura</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>B-S</given-names></name><name><surname>McKenzie</surname><given-names>S</given-names></name><name><surname>Homidan</surname><given-names>J</given-names></name><name><surname>Klaver</surname><given-names>LMF</given-names></name><name><surname>English</surname><given-names>DF</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name><name><surname>Soltesz</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Recruitment and inhibitory action of hippocampal axo-axonic cells during behavior</article-title><source>Neuron</source><volume>109</volume><fpage>3838</fpage><lpage>3850</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.09.033</pub-id><pub-id pub-id-type="pmid">34648750</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischler-Ruiz</surname><given-names>W</given-names></name><name><surname>Clark</surname><given-names>DG</given-names></name><name><surname>Joshi</surname><given-names>NR</given-names></name><name><surname>Devi-Chou</surname><given-names>V</given-names></name><name><surname>Kitch</surname><given-names>L</given-names></name><name><surname>Schnitzer</surname><given-names>M</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Olfactory landmarks and path integration converge to form a cognitive spatial map</article-title><source>Neuron</source><volume>109</volume><fpage>4036</fpage><lpage>4049</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.09.055</pub-id><pub-id pub-id-type="pmid">34710366</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fast online deconvolution of calcium imaging data</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005423</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005423</pub-id><pub-id pub-id-type="pmid">28291787</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gamma</surname><given-names>E</given-names></name><name><surname>Helm</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>Design Patterns: Elements of Reusable Object-Oriented Software</source><publisher-name>Addison-Wesley Professional</publisher-name></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Fattahi</surname><given-names>M</given-names></name><name><surname>Choi</surname><given-names>JS</given-names></name><name><surname>Royer</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Place cells are more strongly tied to landmarks in deep than in superficial CA1</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>14531</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14531</pub-id><pub-id pub-id-type="pmid">28218283</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Vancura</surname><given-names>B</given-names></name><name><surname>Terada</surname><given-names>S</given-names></name><name><surname>Troullinou</surname><given-names>E</given-names></name><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Tsagkatakis</surname><given-names>G</given-names></name><name><surname>Tsakalides</surname><given-names>P</given-names></name><name><surname>Ócsai</surname><given-names>K</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Rózsa</surname><given-names>BJ</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Large-scale 3D two-photon imaging of molecularly identified CA1 interneuron dynamics in behaving mice</article-title><source>Neuron</source><volume>108</volume><fpage>968</fpage><lpage>983</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.013</pub-id><pub-id pub-id-type="pmid">33022227</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Sadeh</surname><given-names>S</given-names></name><name><surname>Rolotti</surname><given-names>SV</given-names></name><name><surname>Blockus</surname><given-names>H</given-names></name><name><surname>Vancura</surname><given-names>B</given-names></name><name><surname>Negrean</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>AJ</given-names></name><name><surname>Rózsa</surname><given-names>B</given-names></name><name><surname>Polleux</surname><given-names>F</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Local circuit amplification of spatial selectivity in the hippocampus</article-title><source>Nature</source><volume>601</volume><fpage>105</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04169-9</pub-id><pub-id pub-id-type="pmid">34853473</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gonzalez</surname><given-names>KC</given-names></name><name><surname>Negrean</surname><given-names>A</given-names></name><name><surname>Liao</surname><given-names>Z</given-names></name><name><surname>Polleux</surname><given-names>F</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Synaptic Basis of Behavioral Timescale Plasticity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.10.04.560848</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Entorhinal cortex directs learning-related changes in CA1 representations</article-title><source>Nature</source><volume>611</volume><fpage>554</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05378-6</pub-id><pub-id pub-id-type="pmid">36323779</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosmark</surname><given-names>AD</given-names></name><name><surname>Sparks</surname><given-names>FT</given-names></name><name><surname>Davis</surname><given-names>MJ</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Reactivation predicts the consolidation of unbiased long-term cognitive maps</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1574</fpage><lpage>1585</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00920-7</pub-id><pub-id pub-id-type="pmid">34663956</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hainmueller</surname><given-names>T</given-names></name><name><surname>Bartos</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Parallel emergence of stable and dynamic memory engrams in the hippocampus</article-title><source>Nature</source><volume>558</volume><fpage>292</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0191-2</pub-id><pub-id pub-id-type="pmid">29875406</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Coen</surname><given-names>P</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id><pub-id pub-id-type="pmid">22419153</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heys</surname><given-names>JG</given-names></name><name><surname>Rangarajan</surname><given-names>KV</given-names></name><name><surname>Dombeck</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The functional micro-organization of grid cells revealed by cellular-resolution imaging</article-title><source>Neuron</source><volume>84</volume><fpage>1079</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.10.048</pub-id><pub-id pub-id-type="pmid">25467986</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jayakumar</surname><given-names>RP</given-names></name><name><surname>Madhav</surname><given-names>MS</given-names></name><name><surname>Savelli</surname><given-names>F</given-names></name><name><surname>Blair</surname><given-names>HT</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recalibration of path integration in hippocampal place cells</article-title><source>Nature</source><volume>566</volume><fpage>533</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-0939-3</pub-id><pub-id pub-id-type="pmid">30742074</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>JT</given-names></name><name><surname>McDermott</surname><given-names>KD</given-names></name><name><surname>Frechou</surname><given-names>MA</given-names></name><name><surname>Shtrahman</surname><given-names>M</given-names></name><name><surname>Gonçalves</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Treadmill-based task for assessing spatial memory in head-fixed mice</article-title><source>STAR Protocols</source><volume>2</volume><elocation-id>100770</elocation-id><pub-id pub-id-type="doi">10.1016/j.xpro.2021.100770</pub-id><pub-id pub-id-type="pmid">34471907</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaifosh</surname><given-names>P</given-names></name><name><surname>Lovett-Barron</surname><given-names>M</given-names></name><name><surname>Turi</surname><given-names>GF</given-names></name><name><surname>Reardon</surname><given-names>TR</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Septo-hippocampal GABAergic signaling across multiple modalities in awake mice</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1182</fpage><lpage>1184</lpage><pub-id pub-id-type="doi">10.1038/nn.3482</pub-id><pub-id pub-id-type="pmid">23912949</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaifosh</surname><given-names>P</given-names></name><name><surname>Zaremba</surname><given-names>JD</given-names></name><name><surname>Danielson</surname><given-names>NB</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>SIMA: Python software for analysis of dynamic fluorescence imaging data</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>80</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00080</pub-id><pub-id pub-id-type="pmid">25295002</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>AM</given-names></name><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A role for the locus coeruleus in hippocampal ca1 place cell reorganization during spatial reward learning</article-title><source>Neuron</source><volume>105</volume><fpage>1018</fpage><lpage>1026</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.12.029</pub-id><pub-id pub-id-type="pmid">31980319</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knierim</surname><given-names>JJ</given-names></name><name><surname>Rao</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Distal landmarks and hippocampal place cells: Effects of relative translation versus rotation</article-title><source>Hippocampus</source><volume>13</volume><fpage>604</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.1002/hipo.10092</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Leutgeb</surname><given-names>JK</given-names></name><name><surname>Barnes</surname><given-names>CA</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Independent codes for spatial and episodic memory in hippocampal neuronal ensembles</article-title><source>Science</source><volume>309</volume><fpage>619</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1126/science.1114037</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lovett-Barron</surname><given-names>M</given-names></name><name><surname>Kaifosh</surname><given-names>P</given-names></name><name><surname>Kheirbek</surname><given-names>MA</given-names></name><name><surname>Danielson</surname><given-names>N</given-names></name><name><surname>Zaremba</surname><given-names>JD</given-names></name><name><surname>Reardon</surname><given-names>TR</given-names></name><name><surname>Turi</surname><given-names>GF</given-names></name><name><surname>Hen</surname><given-names>R</given-names></name><name><surname>Zemelman</surname><given-names>BV</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dendritic inhibition in the hippocampus supports fear learning</article-title><source>Science</source><volume>343</volume><fpage>857</fpage><lpage>863</lpage><pub-id pub-id-type="doi">10.1126/science.1247485</pub-id><pub-id pub-id-type="pmid">24558155</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mauk</surname><given-names>MD</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The neural basis of temporal processing</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>307</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144247</pub-id><pub-id pub-id-type="pmid">15217335</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>RU</given-names></name><name><surname>Kubie</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells</article-title><source>The Journal of Neuroscience</source><volume>7</volume><fpage>1951</fpage><lpage>1968</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-07-01951.1987</pub-id><pub-id pub-id-type="pmid">3612226</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Hare</surname><given-names>JK</given-names></name><name><surname>Gonzalez</surname><given-names>KC</given-names></name><name><surname>Herrlinger</surname><given-names>SA</given-names></name><name><surname>Hirabayashi</surname><given-names>Y</given-names></name><name><surname>Hewitt</surname><given-names>VL</given-names></name><name><surname>Blockus</surname><given-names>H</given-names></name><name><surname>Szoboszlay</surname><given-names>M</given-names></name><name><surname>Rolotti</surname><given-names>SV</given-names></name><name><surname>Geiller</surname><given-names>TC</given-names></name><name><surname>Negrean</surname><given-names>A</given-names></name><name><surname>Chelur</surname><given-names>V</given-names></name><name><surname>Polleux</surname><given-names>F</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Compartment-specific tuning of dendritic feature selectivity by intracellular Ca<sup>2+</sup> release</article-title><source>Science</source><volume>375</volume><elocation-id>eabm1670</elocation-id><pub-id pub-id-type="doi">10.1126/science.abm1670</pub-id><pub-id pub-id-type="pmid">35298275</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Schröder</surname><given-names>S</given-names></name><name><surname>Rossi</surname><given-names>LF</given-names></name><name><surname>Dalgleish</surname><given-names>H</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Suite2p: Beyond 10,000 Neurons with Standard Two-Photon Microscopy</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061507</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priestley</surname><given-names>JB</given-names></name><name><surname>Bowler</surname><given-names>JC</given-names></name><name><surname>Rolotti</surname><given-names>SV</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Signatures of rapid plasticity in hippocampal CA1 representations during novel experiences</article-title><source>Neuron</source><volume>110</volume><fpage>1978</fpage><lpage>1992</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.03.026</pub-id><pub-id pub-id-type="pmid">35447088</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radvansky</surname><given-names>BA</given-names></name><name><surname>Dombeck</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An olfactory virtual reality system for mice</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>839</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-03262-4</pub-id><pub-id pub-id-type="pmid">29483530</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravassard</surname><given-names>P</given-names></name><name><surname>Kees</surname><given-names>A</given-names></name><name><surname>Willers</surname><given-names>B</given-names></name><name><surname>Ho</surname><given-names>D</given-names></name><name><surname>Aharoni</surname><given-names>DA</given-names></name><name><surname>Cushman</surname><given-names>J</given-names></name><name><surname>Aghajan</surname><given-names>ZM</given-names></name><name><surname>Mehta</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multisensory control of hippocampal spatiotemporal selectivity</article-title><source>Science</source><volume>340</volume><fpage>1342</fpage><lpage>1346</lpage><pub-id pub-id-type="doi">10.1126/science.1232655</pub-id><pub-id pub-id-type="pmid">23641063</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renaudineau</surname><given-names>S</given-names></name><name><surname>Poucet</surname><given-names>B</given-names></name><name><surname>Save</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Flexible use of proximal objects and distal cues by hippocampal place cells</article-title><source>Hippocampus</source><volume>17</volume><fpage>381</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1002/hipo.20277</pub-id><pub-id pub-id-type="pmid">17372978</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolotti</surname><given-names>SV</given-names></name><name><surname>Blockus</surname><given-names>H</given-names></name><name><surname>Sparks</surname><given-names>FT</given-names></name><name><surname>Priestley</surname><given-names>JB</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>Reorganization of CA1 dendritic dynamics by hippocampal sharp-wave ripples during learning</article-title><source>Neuron</source><volume>110</volume><fpage>977</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.12.017</pub-id><pub-id pub-id-type="pmid">35041805</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolotti</surname><given-names>SV</given-names></name><name><surname>Ahmed</surname><given-names>MS</given-names></name><name><surname>Szoboszlay</surname><given-names>M</given-names></name><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Negrean</surname><given-names>A</given-names></name><name><surname>Blockus</surname><given-names>H</given-names></name><name><surname>Gonzalez</surname><given-names>KC</given-names></name><name><surname>Sparks</surname><given-names>FT</given-names></name><name><surname>Solis Canales</surname><given-names>AS</given-names></name><name><surname>Tuttman</surname><given-names>AL</given-names></name><name><surname>Peterka</surname><given-names>DS</given-names></name><name><surname>Zemelman</surname><given-names>BV</given-names></name><name><surname>Polleux</surname><given-names>F</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Local feedback inhibition tightly controls rapid formation of hippocampal place fields</article-title><source>Neuron</source><volume>110</volume><fpage>783</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.12.003</pub-id><pub-id pub-id-type="pmid">34990571</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royer</surname><given-names>S</given-names></name><name><surname>Zemelman</surname><given-names>BV</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Chance</surname><given-names>F</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Control of timing, rate and bursts of hippocampal place cells by dendritic and somatic inhibition</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>769</fpage><lpage>775</lpage><pub-id pub-id-type="doi">10.1038/nn.3077</pub-id><pub-id pub-id-type="pmid">22446878</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Saunders</surname><given-names>JL</given-names></name><name><surname>Ott</surname><given-names>LA</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>AUTOPILOT: Automating experiments with lots of Raspberry Pis</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/807693</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheffield</surname><given-names>MEJ</given-names></name><name><surname>Adoff</surname><given-names>MD</given-names></name><name><surname>Dombeck</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Increased prevalence of calcium transients across the dendritic arbor during place field formation</article-title><source>Neuron</source><volume>96</volume><fpage>490</fpage><lpage>504</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.029</pub-id><pub-id pub-id-type="pmid">29024668</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheintuch</surname><given-names>L</given-names></name><name><surname>Rubin</surname><given-names>A</given-names></name><name><surname>Brande-Eilat</surname><given-names>N</given-names></name><name><surname>Geva</surname><given-names>N</given-names></name><name><surname>Sadeh</surname><given-names>N</given-names></name><name><surname>Pinchasof</surname><given-names>O</given-names></name><name><surname>Ziv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Tracking the same neurons across multiple days in Ca2+ imaging data</article-title><source>Cell Reports</source><volume>21</volume><fpage>1102</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2017.10.013</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skaggs</surname><given-names>WE</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Spatial firing properties of hippocampal CA1 populations in an environment containing two visually identical regions</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>8455</fpage><lpage>8466</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-20-08455.1998</pub-id><pub-id pub-id-type="pmid">9763488</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sofroniew</surname><given-names>NJ</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Natural whisker-guided behavior by head-fixed mice in tactile virtual reality</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>9537</fpage><lpage>9550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0712-14.2014</pub-id><pub-id pub-id-type="pmid">25031397</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Terada</surname><given-names>S</given-names></name><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Liao</surname><given-names>Z</given-names></name><name><surname>O’Hare</surname><given-names>J</given-names></name><name><surname>Vancura</surname><given-names>B</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Adaptive stimulus selection for consolidation in the hippocampus</article-title><source>Nature</source><volume>601</volume><fpage>240</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04118-6</pub-id><pub-id pub-id-type="pmid">34880499</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tuncdemir</surname><given-names>SN</given-names></name><name><surname>Grosmark</surname><given-names>AD</given-names></name><name><surname>Turi</surname><given-names>GF</given-names></name><name><surname>Shank</surname><given-names>A</given-names></name><name><surname>Bowler</surname><given-names>JC</given-names></name><name><surname>Ordek</surname><given-names>G</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name><name><surname>Hen</surname><given-names>R</given-names></name><name><surname>Lacefield</surname><given-names>CO</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Parallel processing of sensory cue and spatial information in the dentate gyrus</article-title><source>Cell Reports</source><volume>38</volume><elocation-id>110257</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.110257</pub-id><pub-id pub-id-type="pmid">35045280</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tuncdemir</surname><given-names>SN</given-names></name><name><surname>Grosmark</surname><given-names>AD</given-names></name><name><surname>Chung</surname><given-names>H</given-names></name><name><surname>Luna</surname><given-names>VM</given-names></name><name><surname>Lacefield</surname><given-names>CO</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name><name><surname>Hen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Adult-born granule cells facilitate remapping of spatial and non-spatial representations in the dentate gyrus</article-title><source>Neuron</source><volume>111</volume><fpage>4024</fpage><lpage>4039</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.09.016</pub-id><pub-id pub-id-type="pmid">37820723</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turi</surname><given-names>GF</given-names></name><name><surname>Li</surname><given-names>W-K</given-names></name><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Pandi</surname><given-names>I</given-names></name><name><surname>O’Hare</surname><given-names>J</given-names></name><name><surname>Priestley</surname><given-names>JB</given-names></name><name><surname>Grosmark</surname><given-names>AD</given-names></name><name><surname>Liao</surname><given-names>Z</given-names></name><name><surname>Ladow</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>JF</given-names></name><name><surname>Zemelman</surname><given-names>BV</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Vasoactive intestinal polypeptide-expressing interneurons in the hippocampus support goal-oriented spatial learning</article-title><source>Neuron</source><volume>101</volume><fpage>1150</fpage><lpage>1165</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.009</pub-id><pub-id pub-id-type="pmid">30713030</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vancura</surname><given-names>B</given-names></name><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Grosmark</surname><given-names>A</given-names></name><name><surname>Zhao</surname><given-names>V</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Inhibitory control of sharp-wave ripple duration during learning in hippocampal recurrent networks</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>788</fpage><lpage>797</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01306-7</pub-id><pub-id pub-id-type="pmid">37081295</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>RA</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name><name><surname>Hoffman</surname><given-names>JR</given-names></name><name><surname>Li</surname><given-names>EY</given-names></name><name><surname>Hong</surname><given-names>YK</given-names></name><name><surname>Bruno</surname><given-names>RM</given-names></name><name><surname>Sawtell</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A rapid whisker-based decision underlying skilled locomotion in mice</article-title><source>eLife</source><volume>10</volume><elocation-id>e63596</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63596</pub-id><pub-id pub-id-type="pmid">33428566</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaremba</surname><given-names>JD</given-names></name><name><surname>Diamantopoulou</surname><given-names>A</given-names></name><name><surname>Danielson</surname><given-names>NB</given-names></name><name><surname>Grosmark</surname><given-names>AD</given-names></name><name><surname>Kaifosh</surname><given-names>PW</given-names></name><name><surname>Bowler</surname><given-names>JC</given-names></name><name><surname>Liao</surname><given-names>Z</given-names></name><name><surname>Sparks</surname><given-names>FT</given-names></name><name><surname>Gogos</surname><given-names>JA</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Impaired hippocampal place cell dynamics in a mouse model of the 22q11.2 deletion</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1612</fpage><lpage>1623</lpage><pub-id pub-id-type="doi">10.1038/nn.4634</pub-id><pub-id pub-id-type="pmid">28869582</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Ginzburg</surname><given-names>I</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>1017</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.2.1017</pub-id><pub-id pub-id-type="pmid">9463459</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Key links</title><list list-type="bullet"><list-item><p>Link to website for quickstart guides and file downloads:<ext-link ext-link-type="uri" xlink:href="https://www.losonczylab.org/behaviormate">https://www.losonczylab.org/behaviormate</ext-link></p></list-item><list-item><p>Link to example settings files:<ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/behaviorMate/tree/main/example_settings_files">https://github.com/losonczylab/behaviorMate/tree/main/example_settings_files</ext-link></p></list-item><list-item><p>Link to source code for the UI:<ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/behaviorMate">https://github.com/losonczylab/behaviorMate</ext-link></p></list-item><list-item><p>Link to online java documentation for the UI:<ext-link ext-link-type="uri" xlink:href="https://www.losonczylab.org/behaviorMate-1.0.0">https://www.losonczylab.org/behaviorMate-1.0.0</ext-link></p></list-item><list-item><p>Link to electronics schematics:<ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/Hardware/tree/main/Electronics">https://github.com/losonczylab/Hardware/tree/main/Electronics</ext-link></p></list-item><list-item><p>Link to Arduino Firmware and Debugging Utilities:<ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/behaviormate_utils/">https://github.com/losonczylab/behaviormate_utils/</ext-link></p></list-item><list-item><p>Link to treadmill and VR CAD files: <ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/Hardware/">https://github.com/losonczylab/Hardware/</ext-link></p></list-item><list-item><p>Link to benchmarking and experimental validation data:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.02v6wwqdf">https://doi.org/10.5061/dryad.02v6wwqdf</ext-link></p></list-item></list></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97433.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Rice University</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>Bowler et al. present a software/hardware system for behavioral control of navigation-based virtual reality experiments, particularly suited for pairing with 2-photon imaging but applicable to a variety of techniques. This system represents a <bold>valuable</bold> contribution to the field of behavioral and systems neuroscience, as it provides a standardized, easy to implement, and flexible system that could be adopted across multiple laboratories. The authors provide <bold>compelling</bold> evidence of the functionality of their system by reporting benchmark tests and demonstrating hippocampal activity patterns consistent with standards in the field. This work will be of interest to systems neuroscientists looking to integrate flexible head-fixed behavioral control with neural data acquisition.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97433.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Bowler et al. present a thoroughly tested system for modularized behavioral control of navigation-based experiments, particularly suited for pairing with 2-photon imaging but applicable to a variety of techniques. This system, which they name behaviorMate, represents an important methodological contribution to the field of behavioral and systems neuroscience. As the authors note, behavioral control paradigms vary widely across laboratories in terms of hardware and software utilized and often require specialized technical knowledge to make changes to these systems. Having a standardized, easy to implement, and flexible system that can be used by many groups is therefore highly desirable.</p><p>Strengths:</p><p>The present manuscript provides compelling evidence of the functionality and applicability of behaviorMate. The authors report benchmark tests for high-fidelity, real-time update speed between the animal's movement and the behavioral control, on both the treadmill-based and virtual reality (VR) setups. The VR system relies on Unity, a common game development engine, but implements all scene generation and customizability in the authors' behaviorMate and VRMate software, which circumvents the need for users to program task logic in C# in Unity. Further, the authors nicely demonstrate and quantify reliable hippocampal place cell coding in both setups, using synchronized 2-photon imaging. This place cell characterization also provides a concrete comparison between the place cell properties observed in treadmill-based navigation vs. visual VR in a single study, which itself is a valuable contribution to the field.</p><p>Weaknesses: None noted.</p><p>Documentation for installing and operating behaviorMate is available via the authors' lab website and Github, linked in the manuscript.</p><p>The authors have addressed all of my requests for clarification from the previous round of review. This work will be of great interest to systems neuroscientists looking to integrate flexible head-fixed behavioral control with neural data acquisition.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97433.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>The authors present behaviorMate, an open-source behavior control system including a central GUI and compatible treadmill and display components. Notably, the system utilize the &quot;Intranet of things&quot; scheme and the components communicate through local network, making the system modular, which in turn allows user to configure the setup to suit their experimental needs. Overall, behaviorMate is a useful resource for researchers performing head-fixed VR imaging studies involving 1D navigation tasks, as the commercial alternatives are often expensive and inflexible to modify.</p><p>One major utility of behaviorMate is an open-source alternative to commercial behavior apparatus for head-fixed imaging studies involving 1D navigation tasks. The documentation, BOM, CAD files, circuit design, source and compiled software, along with the manuscript, create an invaluable resource for neuroscience researcher looking to set up a budget-friendly VR and head-fixed imaging rig. Some features of behaviorMate, including the computer vision-based calibration of treadmill, and the decentralized, Android-based display devices, are very innovative approaches and can be quite useful in practical settings.</p><p>behaviorMate can also be used as a set of generic schema and communication protocols that allows the users to incorporate recording and stimulation devices during a head-fixed imaging experiment. Due to the &quot;Intranet of things&quot; approach taken in the design, any hardware that supports UDP communication can in theory be incorporated into the system. In terms of current capability, behaviorMate supports experimental contingencies based on animal position and time and synchronization with external recording devices using a TTL start signal. Further customization involving more complicated experimental contingencies, more accurate recording synchronization (for example with ephys recording devices), incorporation of novel behavior and high-speed neural recording hardware beyond GPIO signaling would require modification of the Java source and custom hardware implementation. Modification to the Java source of behaviorMate can be performed with basic familiarity with object-oriented programming using the Java programming language, and a JavaFX-based plugin system is under development to make such customizations more approachable for users.</p><p>In summary, the manuscript presents a well-developed and useful open-source behavior control system for head-fixed VR imaging experiments with innovative features.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97433.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In this work, the authors present an open-source system called behaviourMate for acquiring data related to animal behavior. The temporal alignment of recorded parameters across various devices is highlighted as crucial to avoid delays caused by electronics dependencies. This system not only addresses this issue but also offers an adaptable solution for VR setups. Given the significance of well-designed open-source platforms, this paper holds importance.</p><p>Advantages of behaviorMate:</p><p>The cost-effectiveness of the system provided.</p><p>The reliability of PCBs compared to custom-made systems.</p><p>Open-source nature for easy setup.</p><p>Plug &amp; Play feature requiring no coding experience for optimizing experiment performance (only text based Json files, 'context List' required for editing).</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.97433.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bowler</surname><given-names>John</given-names></name><role specific-use="author">Author</role><aff><institution>University of Utah</institution><addr-line><named-content content-type="city">Salt Lake City</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Zakka</surname><given-names>George</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Yong</surname><given-names>Hyun Choong</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Wenke</given-names></name><role specific-use="author">Author</role><aff><institution>Aquabyte</institution><addr-line><named-content content-type="city">San Francisco</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Rao</surname><given-names>Bovey</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Liao</surname><given-names>Zhenrui</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Priestley</surname><given-names>James B</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>École Polytechnique Fédérale de Lausanne</institution></institution-wrap><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Losonczy</surname><given-names>Attila</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>(1) As VRMate (a component of behaviorMate) is written using Unity, what is the main advantage of using behaviorMate/VRMate compared to using Unity alone paired with Arduinos (e.g. Campbell et al. 2018), or compared to using an existing toolbox to interface with Unity (e.g. Alsbury-Nealy et al. 2022, DOI: 10.3758/s13428-021-01664-9)? For instance, one disadvantage of using Unity alone is that it requires programming in C# to code the task logic. It was not entirely clear whether VRMate circumvents this disadvantage somehow -- does it allow customization of task logic and scenery in the GUI? Does VRMate add other features and/or usability compared to Unity alone? It would be helpful if the authors could expand on this topic briefly.</p></disp-quote><p>We have updated the manuscript (lines 412-422) to clarify the benefits of separating the VR system as an isolated program and a UI that can be run independently. We argue that “…the recommended behaviorMate architecture has several important advantages. Firstly, by rendering each viewing angle of a scene on a dedicated device, performance is improved by splitting the computational costs across several inexpensive devices rather than requiring specialized or expensive graphics cards in order to run…, the overall system becomes more modular and easier to debug [and] implementing task logic in Unity would require understanding Object-Oriented Programming and C# … which is not always accessible to researchers that are typically more familiar with scripting in Python and Matlab.”</p><p>VRMate receives detailed configuration info from behaviorMate at runtime as to which VR objects to display and receives position updates during experiments. Any other necessary information about triggering rewards or presenting non-VR cues is still handled by the UI so no editing of Unity is necessary. Scene configuration information is in the same JSON format as the settings files for behaviorMate, additionally there are Unity Editor scripts which are provided in the VRmate repository which permit customizing scenes through a “drag and drop” interface and then writing the scene configuration files programmatically. Users interested in these features should see our github page to find example scene.vr files and download the VRMate repository (including the editor scripts). We provided 4 vr contexts, as well as a settings file that uses one of them which can be found on the behaviorMate github page (<ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/behaviorMate">https://github.com/losonczylab/behaviorMate</ext-link>) in the “vr_contexts” and “example_settigs_files” directories. These examples are provided to assist VRMate users in getting set up and could provide a more detailed example of how VRMate and behaviorMate interact.</p><disp-quote content-type="editor-comment"><p>(2) The section on &quot;context lists&quot;, lines 163-186, seemed to describe an important component of the system, but this section was challenging to follow and readers may find the terminology confusing. Perhaps this section could benefit from an accompanying figure or flow chart, if these terms are important to understand.</p></disp-quote><p>We maintain the use of the term context and context list in order to maintain a degree of parity with the java code. However, we have updated lines 173-175 to define the term context for the behaviorMate system: “... a context is grouping of one or more stimuli that get activated concurrently. For many experiments it is desirable to have multiple contexts that are triggered at various locations and times in order to construct distinct or novel environments.”</p><disp-quote content-type="editor-comment"><p>a. Relatedly, &quot;context&quot; is used to refer to both when the animal enters a particular state in the task like a reward zone (&quot;reward context&quot;, line 447) and also to describe a set of characteristics of an environment (Figure 3G), akin to how &quot;context&quot; is often used in the navigation literature. To avoid confusion, one possibility would be to use &quot;environment&quot; instead of &quot;context&quot; in Figure 3G, and/or consider using a word like &quot;state&quot; instead of &quot;context&quot; when referring to the activation of different stimuli.</p></disp-quote><p>Thank you for the suggestion. We have updated Figure 3G to say “Environment” in order to avoid confusion.</p><p>(3) Given the authors' goal of providing a system that is easily synchronizable with neural data acquisition, especially with 2-photon imaging, I wonder if they could expand on the following features:</p><disp-quote content-type="editor-comment"><p>a. The authors mention that behaviorMate can send a TTL to trigger scanning on the 2P scope (line 202), which is a very useful feature. Can it also easily generate a TTL for each frame of the VR display and/or each sample of the animal's movement? Such TTLs can be critical for synchronizing the imaging with behavior and accounting for variability in the VR frame rate or sampling rate.</p></disp-quote><p>Different experimental demands require varying levels of precision in this kind of synchronization signals. For this reason, we have opted against a “one-size fits all” for synchronization with physiology data in behaviorMate. Importantly this keeps the individual rig costs low which can be useful when constructing setups specifically for use when training animals. behaviorMate will log TTL pulses sent to GPIO pins setup as sensors, and can be configured to generate TTL pulses at regular intervals. Additionally all UDP packets received by the UI are time stamped and logged. We also include the output of the arduino millis() function in all UDP packets which can be used for further investigation of clock drift between system components. Importantly, since the system is event driven there cannot be accumulating drift across running experiments between the behaviorMate UI and networked components such as the VR system.</p><p>For these reasons, we have not needed to implement a VR frame synchronization TTL for any of our experiments, however, one could extend VRMate to send &quot;sync&quot; packets back to behaviorMate to log when each frame was displayed precisely or TTL pulses (if using the same ODROID hardware we recommend in the standard setup for rendering scenes). This would be useful if it is important to account for slight changes in the frame rate at which the scenes are displayed. However, splitting rendering of large scenes between several devices results in fast update times and our testing and benchmarks indicate that display updates are smooth and continuous enough to appear coupled to movement updates from the behavioral apparatus and sufficient for engaging navigational circuits in the brain.</p><disp-quote content-type="editor-comment"><p>b. Is there a limit to the number of I/O ports on the system? This might be worth explicitly mentioning.</p></disp-quote><p>We have updated lines 219-220 in the manuscript to provide this information: Sensors and actuators can be connected to the controller using one of the 13 digital or 5 analog input/output connectors.</p><disp-quote content-type="editor-comment"><p>c. In the VR version, if each display is run by a separate Android computer, is there any risk of clock drift between displays? Or is this circumvented by centralized control of the rendering onset via the &quot;real-time computer&quot;?</p></disp-quote><p>This risk is mitigated by the real-time computer/UI sending position updates to the VR displays. The maximum amount scenes can be out of sync is limited because they will all recalibrate on every position update – which occurs multiple times per second as the animal is moving. Moreover, because position updates are constantly being sent by behaviorMate to VRMate and VRMate is immediately updating the scene according to this position, the most the scene can become out of sync with the mouse's position is proportional to the maximum latency multiplied by the running speed of the mouse. For experiments focusing on eliciting an experience of navigation, such a degree of asynchrony is almost always negligible. For other experimental demands it could be possible to incorporate more precise frame timing information but this was not necessary for our use case and likely for most other use cases. Additionally, refer to the response to comment 3a.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>(1) The central controlling logic is coupled with GUI and an event loop, without a documented plugin system. It's not clear whether arbitrary code can be executed together with the GUI, hence it's not clear how much the functionality of the GUI can be easily extended without substantial change to the source code of the GUI. For example, if the user wants to perform custom real-time analysis on the behavior data (potentially for closed-loop stimulation), it's not clear how to easily incorporate the analysis into the main GUI/control program.</p></disp-quote><p>Without any edits to the existing source code behaviorMate is highly customizable through the settings files, which allow users to combine the existing contexts and decorators in arbitrary combinations. Therefore, users have been able to perform a wide variety of 1D navigation tasks, well beyond our anticipated use cases by generating novel settings files. The typical method for providing closed-loop stimulation would be to set up a context which is triggered by animal behavior using decorators (e.g. based on position, lap number and time) and then trigger the stimulation with a TTL pulse. Rarely, if users require a behavioral condition not currently implemented or composable out of existing decorators, it would require generating custom code in Java to extend the UI. Performing such edits requires only knowledge of basic object-oriented programming in Java and generating a single subclass of either the BasicContextList or ContextListDecorator classes. In addition, the JavaFX (under development) version of behaviorMate incorporates a plugin which doesn't require recompiling the code in order to make these changes. However, since the JavaFX software is currently under development, documentation does not yet exist. All software is open-sourced and available on github.com for users interested in generating plugins or altering the source code.</p><p>We have added the additional caveat to the manuscript in order to clarify this point (Line 197-202): “However, if the available set of decorators is not enough to implement the required task logic, some modifications to the source code may be necessary. These modifications, in most cases, would be very simple and only a basic understanding of object-oriented programming is required. A case where this might be needed would be performing novel customized real-time analysis on behavior data and activating a stimulus based on the result”</p><disp-quote content-type="editor-comment"><p>(2) The JSON messaging protocol lacks API documentation. It's not clear what the exact syntax is, supported key/value pairs, and expected response/behavior of the JSON messages. Hence, it's not clear how to develop new hardware that can communicate with the behaviorMate system.</p></disp-quote><p>The most common approach for adding novel hardware is to use TTL pulses (or accept an emitted TTL pulse to read sensor states). This type of hardware addition is possible through the existing GPIO without the need to interact with the software or JSON API. Users looking to take advantage of the ability to set up and configure novel behavioral paradigms without the need to write any software would be limited to adding hardware which could be triggered with and report to the UI with a TTL pulse (however fairly complex actions could be triggered this way).</p><p>For users looking to develop more customized hardware solutions that interact closely with the UI or GPIO board, additional documentation on the JSON messaging protocol has been added to the behaviormate-utils repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/losonczylab/behaviormate_utils">https://github.com/losonczylab/behaviormate_utils</ext-link>). Additionally, we have added a link to this repository in the <italic>Supplemental Materials</italic> section (line 971) and referenced this in the manuscript (line 217) to make it easier for readers to find this information.</p><p>Furthermore, developers looking to add completely novel components to the UI can implement the interface described by <ext-link ext-link-type="uri" xlink:href="http://Context.java">Context.java</ext-link> in order to exchange custom messages with hardware. (described in the JavaDoc: <ext-link ext-link-type="uri" xlink:href="https://www.losonczylab.org/behaviorMate-1.0.0/">https://www.losonczylab.org/behaviorMate-1.0.0/</ext-link>) These messages would be defined within the custom context and interact with the custom hardware (meaning the interested developer would make a novel addition to the messaging API). Additionally, it should be noted that without editing any software, any UDP packets sent to behaviorMate from an IP address specified in the settings will get time stamped and logged in the stored behavioral data file meaning that are a large variety of hardware implementation solutions using both standard UDP messaging and through TTL pulses that can work with behaviorMate with minimal effort. Finally, see response to R2.1 for a discussion of the JavaFX version of the behaviorMatee UI including plugin support.</p><disp-quote content-type="editor-comment"><p>(3) It seems the existing control hardware and the JSON messaging only support GPIO/TTL types of input/output, which limits the applicability of the system to more complicated sensor/controller hardware. The authors mentioned that hardware like Arduino natively supports serial protocols like I2C or SPI, but it's not clear how they are handled and translated to JSON messages.</p></disp-quote><p>We provide an implementation for an I2C-based capacitance lick detector which interested developers may wish to copy if support for novel I2C or SPI. Users with less development experience wishing to expand the hardware capabilities of behaviorMatecould also develop adapters which can be triggered on a TTL input/output. Additionally, more information about the JSON API and how messages are transmitted to the PC by the arduino is described in point (2) and the expanded online documentation.</p><disp-quote content-type="editor-comment"><p>a. Additionally, because it's unclear how easy to incorporate arbitrary hardware with behaviorMate, the &quot;Intranet of things&quot; approach seems to lose attraction. Since currently, the manuscript focuses mainly on a specific set of hardware designed for a specific type of experiment, it's not clear what are the advantages of implementing communication over a local network as opposed to the typical connections using USB.</p></disp-quote><p>As opposed to serial communication protocols as typical with USB, networking protocols seamlessly function based on asynchronous message passing. Messages may be routed internally (e.g. to a PCs localhost address, i.e. 0.0.0..0) or to a variety of external hardware (e.g. using IP addresses such as those in the range 192.168.1.2 - 192.168.1.254). Furthermore, network-based communication allows modules, such as VR, to be added easily. behavoirMate systems can be easily expanded using low-cost Ethernet switches and consume only a single network adapter on the PC (e.g. not limited by the number of physical USB ports). Furthermore, UDP message passing is implemented in almost all modern programming languages in a platform independent manner (meaning that the same software can run on OSX, Windows, and Linux). Lastly, as we have pointed out (Line 117) a variety of tools exist for inspecting network packets and debugging; meaning that it is possible to run behaviorMate with simulated hardware for testing and debugging.</p><p>The IOT nature of behaviorMate means there is no requirement for novel hardware to be implemented using an arduino, since any system capable of UDP communication can be configured. For example, VRMate is usually run on Odroid C4s, however one could easily create a system using Raspberry Pis or even additional PCs. behaviorMate is agnostic to the format of the UDP messages, but packaging any data in the JSON format for consistency would be encouraged. If a new hardware is a sensor that has input requiring it to be time stamped and logged then all that is needed is to add the IP address and port information to the ‘controllers’ list in a behaviorMate settings file. If more complex interactions are needed with novel hardware than a custom implementation of ContextList.java may be required (see response to R2.2). However, the provided UdpComms.java class could be used to easily send/receive messages from custom Context.java subclasses.</p><p>Solutions for highly customized hardware do require basic familiarity with object-oriented programming using the Java programming language. However, in our experience most behavioral experiments do not require these kinds of modifications. The majority of 1D navigation tasks, which behaviorMate is currently best suited to control, require touch/motion sensors, LEDs, speakers, or solenoid valves, which are easily controlled by the existing GPIO implementation. It is unlikely that custom subclasses would even be needed.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>(1) While using UDP for data transmission can enhance speed, it is thought that it lacks reliability. Are there error-checking mechanisms in place to ensure reliable communication, given its criticality alongside speed?</p></disp-quote><p>The provided GPIO/behavior controller implementation sends acknowledgement packets in response to all incoming messages as well as start and stop messages for contexts and “valves”. In this way the UI can update to reflect both requested state changes as well as when they actually happen (although there is rarely a perceptible gap between these two states unless something is unplugged or not functioning). See Line 85 in the revised manuscript “acknowledgement packets are used to ensure reliable message delivery to and from connected hardware”.</p><disp-quote content-type="editor-comment"><p>(2) Considering this year's price policy changes in Unity, could this impact the system's operations?</p></disp-quote><p>VRMate is not affected by the recent changes in pricing structure of the Unity project.</p><p>The existing compiled VRMate software does not need to be regenerated to update VR scenes, or implement new task logic (since this is handled by the behaviorMate GUI). Therefore, the VRMate program is robust to any future pricing changes or other restructuring of the Unity program and does not rely on continued support of Unity. Additionally, while the solution presented in VRMate has many benefits, a developer could easily adapt any open-source VR Maze project to receive the UDP-based position updates from behaviorMate or develop their own novel VR solutions.</p><disp-quote content-type="editor-comment"><p>(3) Also, does the Arduino offer sufficient precision for ephys recording, particularly with a 10ms check?</p></disp-quote><p>Electrophysiology recording hardware typically has additional I/O channels which can provide assistance with tracking behavior/synchronization at a high resolution. While behaviorMate could still be used to trigger reward valves, either the ephys hardware or some additional high-speed DAQ would be recommended to maintain accurately with high-speed physiology data. behaviorMate could still be set up as normal to provide closed and open-loop task control at behaviorally relevant timescales alongside a DAQ circuit recording events at a consistent temporal resolution. While this would increase the relative cost of the individual recording setup, identical rigs for training animals could still be configured without the DAQ circuit avoiding unnecessary cost and complexity.</p><disp-quote content-type="editor-comment"><p>(4) Could you clarify the purpose of the Sync Pulse? In line 291, it suggests additional cues (potentially represented by the Sync Pulse) are needed to align the treadmill screens, which appear to be directed towards the Real-Time computer. Given that event alignment occurs in the GPIO, the connection of the Sync Pulse to the Real-Time Controller in Figure 1 seems confusing.</p></disp-quote><p>A number of methods exist for synchronizing recording devices like microscopes or electrophysiology recordings with behaviorMate’s time-stamped logs of actuators and sensors. For example, the GPIO circuit can be configured to send sync triggers, or receive timing signals as input. Alternatively a dedicated circuit could record frame start signals and relay them to the PC to be logged independently of the GPIO (enabling a high-resolution post-hoc alignment of the time stamps). The optimal method to use varies based on the needs of the experiment. Our setups have a dedicated BNC output and specification in the settings file that sends a TTL pulse at the start of an experiment in order to trigger 2p imaging setups (see line 224, specifically that this is a detail of “our” 2p imaging setup). We provide this information as it might be useful suggesting how to have both behavior and physiology data start recording at the same time. We do not intend this to be the only solution for alignment. Figure 1 indicates an “optional” circuit for capturing a high speed sync pulse and providing time stamps back to the real time PC. This is another option that might be useful for certain setups (or especially for establishing benchmarks between behavior and physiology recordings). In our setup event alignment does not exclusively occur on the GPIO.</p><disp-quote content-type="editor-comment"><p>a. Additionally, why is there a separate circuit for the treadmill that connects to the UI computer instead of the GPIO? It might be beneficial to elaborate on the rationale behind this decision in line 260.</p></disp-quote><p>Event alignment does not occur on the GPIO, separating concerns between position tracking and more general input/output features which improves performance and simplifies debugging. In this sense we maintain a single event loop on the Arduino, avoiding the need to either run multithreaded operations or rely extensively on interrupts which can cause unpredictable code execution (e.g. when multiple interrupts occur at the same time). Our position tracking circuit is therefore coupled to a separate,low-cost arduino mini which has the singular responsibility of position-tracking.</p><disp-quote content-type="editor-comment"><p>b. Moreover, should scenarios involving pupil and body camera recordings connect to the Analog input in the PCB or the real-time computer for optimal data handling and processing?</p></disp-quote><p>Pupil and body camera recordings would be independent data streams which can be recorded separately from behaviorMate. Aligning these forms of full motion video could require frame triggers which could be configured on the GPIO board using single TTL like outputs or by configuring a valve to be “pulsed” which is a provided type customization.</p><p>We also note that a more advanced developer could easily leverage camera signals to provide closed loop control by writing an independent module that sends UDP packets to behavoirMate. For example a separate computer vision based position tracking module could be written in any preferred language and use UDP messaging to send body tracking updates to the UI without editing any of the behaviorMate source code (and even used for updating 1D location).</p><disp-quote content-type="editor-comment"><p>(5) Given that all references, as far as I can see, come from the same lab, are there other labs capable of implementing this system at a similar optimal level?</p></disp-quote><p>To date two additional labs have published using behaviorMate, the Soltez and Henn labs (see revised lines 341-342). Since behaviorMate has only recently been published and made available open source, only external collaborators of the Losonczy lab have had access to the software and design files needed to do this. These collaborators did, however, set up their own behavioral setups in separate locations with minimal direct support from the authors–similar to what would be available to anyone seeking to set a behaviorMate system would find online on our github page or by posting to the message board.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>(4) To provide additional context for the significance of this work, additional citations would be helpful to demonstrate a ubiquitous need for a system like behaviorMate. This was most needed in the paragraph from lines 46-65, specifically for each sentence after line 55, where the authors discuss existing variants on head-fixed behavioral paradigms. For instance, for the clause &quot;but olfactory and auditory stimuli have also been utilized at regular virtual distance intervals to enrich the experience with more salient cues&quot;, suggested citations include Radvansky &amp; Dombeck 2018 (DOI: 10.1038/s41467-018-03262-4), Fischler-Ruiz et al. 2021 (DOI: 10.1016/j.neuron.2021.09.055).</p></disp-quote><p>We thank the reviewer for the suggested missing citations and have updated the manuscript accordingly (see line 58).</p><disp-quote content-type="editor-comment"><p>(5) In addition, it would also be helpful to clarify behaviorMate's implementation in other laboratories. On line 304 the authors mention &quot;other labs&quot; but the following list of citations is almost exclusively from the Losonczy lab. Perhaps the citations just need to be split across the sentence for clarity? E.g. &quot;has been validated by our experimental paradigms&quot; (citation set 1) &quot;and successfully implemented in other labs as well&quot; (citation set 2).</p></disp-quote><p>We have split the citation set as suggested (see lines 338-342).</p><disp-quote content-type="editor-comment"><p>Minor Comments:</p><p>(6) In the paragraph starting line 153 and in Fig. 2, please clarify what is meant by &quot;trial&quot; vs. &quot;experiment&quot;. In many navigational tasks, &quot;trial&quot; refers to an individual lap in the environment, but here &quot;trial&quot; seems to refer to the whole behavioral session (i.e. synonymous with &quot;experiment&quot;?).</p></disp-quote><p>In our software implementation we had originally used “trial” to refer to an imaging session rather than experiment (and have made updates to start moving to the more conventional lexicon). To avoid confusion we have remove this use of “trial” throughout the manuscript and replaced with “experiment” whenever possible</p><disp-quote content-type="editor-comment"><p>(7) This is very minor, but in Figure 3 and 4, I don't believe the gavage needle is actually shown in the image. This is likely to avoid clutter but might be confusing to some readers, so it may be helpful to have a small inset diagram showing how the needle would be mounted.</p></disp-quote><p>We assessed the image both with and without the gavage needle and found the version in the original (without) to be easier to read and less cluttered and therefore maintained that version in the manuscript.</p><disp-quote content-type="editor-comment"><p>(8) In Figure 5 legend, please list n for mice and cells.</p></disp-quote><p>We have updated the Figure 5 legend to indicate that for panels C-G, n=6 mice (all mice were recorded in both VR and TM systems), 3253 cells in VR classified as significantly tuned place cells VR, and 6101 tuned cells in TM,</p><disp-quote content-type="editor-comment"><p>(9) Line 414: It is not necessary to tilt the entire animal and running wheel as long as the head-bar clamp and objective can rotate to align the imaging window with the objective's plane of focus. Perhaps the authors can just clarify the availability of this option if users have a microscope with a rotatable objective/scan head.</p></disp-quote><p>We have added the suggested caveat to the manuscript in order to clarify when the goniometers might be useful (see lines 281-288).</p><disp-quote content-type="editor-comment"><p>(10) Figure S1 and S2 could be referenced explicitly in the main text with their related main figures.</p></disp-quote><p>We have added explicit references to figures S1 and S2 in the relevant sections (see lines 443, 460 and 570)</p><p>(11) On line 532-533, is there a citation for &quot;proximal visual cues and tactile cues (which are speculated to be more salient than visual cues)&quot;?</p><p>We have added citations to both Knierim &amp; Rao 2003 and Renaudineau et al. 2007 which discuss the differential impact of proximal vs distal cues during navigation as well as Sofroniew et al. 2014 which describe how mice navigate more naturally in a tactile VR setup as opposed to purely visual ones.</p><disp-quote content-type="editor-comment"><p>(12) There is a typo at the end of the Figure 2 legend, where it should say &quot;Arduino Mini.&quot;</p></disp-quote><p>This typo has been fixed.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>(4) As mentioned in the public review: what is the major advantage of taking the IoT approaches as opposed to USB connections to the host computer, especially when behaviorMate relies on a central master computer regardless? The authors mentioned the readability of the JSON messages, making the system easier to debug. However, the flip side of that is the efficiency of data transmission. Although the bandwidth/latency is usually more than enough for transmitting data and commands for behavior devices, the efficiency may become a problem when neural recording devices (imaging or electrophysiology) need to be included in the system.</p></disp-quote><p>behaviorMate is not intended to do everything, and is limited to mainly controlling behavior and providing some synchronizing TTL style triggers. In this way the system can easily and inexpensively be replicated across multiple recording setups; particularly this is useful for constructing additional animal training setups. The system is very much sufficient for capturing behavioral inputs at relevant timescales (see the benchmarks in Figures 3 and 4 as well as the position correlated neural activity in Figures 5 and 6 for demonstration of this). Additional hardware might be needed to align the behaviorMate output with neural data for example a high-speed DAQ or input channels on electrophysiology recording setups could be utilized (if provided). As all recording setups are different the ideal solution would depend on details which are hard to anticipate. We do not mean to convey that the full neural data would be transmitted to the behaviorMate system (especially using the JSON/UDP communications that behaviorMate relies on).</p><disp-quote content-type="editor-comment"><p>(5) The author mentioned labView. A popular open-source alternative is bonsai (<ext-link ext-link-type="uri" xlink:href="https://github.com/bonsai-rx/bonsai">https://github.com/bonsai-rx/bonsai</ext-link>). Both include a graphical-based programming interface that allows the users to easily reconfigure the hardware system, which behaviorMate seems to lack. Additionally, autopilot (<ext-link ext-link-type="uri" xlink:href="https://github.com/auto-pi-lot/autopilot">https://github.com/auto-pi-lot/autopilot</ext-link>) is a very relevant project that utilizes a local network for multiple behavior devices but focuses more on P2P communication and rigorously defines the API/schema/communication protocols for devices to be compatible. I think it's important to include a discussion on how behaviorMate compares to previous works like these, especially what new features behaviorMate introduces.</p></disp-quote><p>We believe that behaviorMate provides a more opinionated and complete solution than the projects mentioned. A wide variety of 1D navigational paradigms can be constructed in behaviorMate without the need to write any novel software. For example, bonsai is a “visual programming language” and would require experimenters to construct a custom implementation of each of their experiments. We have opted to use Java for the UI with distributed computations across modules in various languages. Given the IOT methodology it would be possible to use any number of programming languages or APIs; a large number of design decisions were made when building the project and we have opted to not include this level of detail in the manuscript in order to maintain readability. We strongly believe in using non-proprietary and open source projects, when possible, which is why the comparison with LabView based solutions was included in the introduction. Also, we have added a reference to the autopilot reference to the section of the introduction where this is discussed.</p><disp-quote content-type="editor-comment"><p>(6) One of the reasons labView/bonsai are popular is they are inherently parallel and can simultaneously respond to events from different hardware sources. While the JSON events in behaviorMate are asynchronous in nature, the handling of those events seems to happen only in a main event loop coupled with GUI, which is sequential by nature. Is there any multi-threading/multi-processing capability of behaviorMate? If so it's an important feature to highlight. If not I think it's important to discuss the potential limitation of the current implementation.</p></disp-quote><p>IOT solutions are inherently concurrent since the computation is distributed. Additional parallelism could be added by further distributing concerns between additional independent modules running on independent hardware. The UI has an eventloop which aggregates inputs and then updates contexts based on the current state of those inputs sequentially. This sort of a “snapshot” of the current state is necessary to reason about when the start certain contexts based on their settings and applied decorators. While the behaviorMate UI uses multithreading libraries in Java to be more performant in certain cases, the degree to which this represents true vs “virtual” concurrency would depend on the individual PC architecture it is run on and how the operating system allocates resources. For this reason, we have argued in the manuscript that behaviorMate is sufficient for controlling experiments at behaviorally relevant timescales, and have presented both benchmarks and discussed different synchronization approaches and permit users to determine if this is sufficient for their needs.</p><disp-quote content-type="editor-comment"><p>(7) The context list is an interesting and innovative approach to abstract behavior contingencies into a data structure, but it's not currently discussed in depth. I think it's worth highlighting how the context list can be used to cover a wide range of common behavior experimental contingencies with detailed examples (line 185 might be a good example to give). It's also important to discuss the limitation, as currently the context lists seem to only support contingencies based purely on space and time, without support for more complicated behavior metrics (e.g. deliver reward only after X% correct).</p></disp-quote><p>To access more complex behavior metrics during runtime, custom context list decorators would need to be implemented. While this is less common in the sort of 1D navigational behaviors the project was originally designed to control, adding novel decorators is a simple process that only requires basic object oriented programming knowledge. As discussed we are also implementing a plugin-architecture in the JavaFX update to streamline these types of additions.</p><disp-quote content-type="editor-comment"><p>Minor Comments:</p><p>(8) In line 202, the author suggests that a single TTL pulse is sent to mark the start of a recording session, and this is used to synchronize behavior data with imaging data later. In other words, there are no synchronization signals for every single sample/frame. This approach either assumes the behavior recording and imaging are running on the same clock or assumes evenly distributed recording samples over the whole recording period. Is this the case? If so, please include a discussion on limitations and alternative approaches supported by behaviorMate. If not, please clarify how exactly synchronization is done with one TTL pulse.</p></disp-quote><p>While the TTL pulse triggers the start of neural data in our setups, various options exist for controlling for the described clock drift across experiments and the appropriate one depends on the type of recordings made, frame rate duration of recording etc. Therefore behaviorMate leaves open many options for synchronization at different time scales (e.g. the adding a frame-sync circuit as shown in Figure 1 or sending TTL pulses to the same DAQ recording electrophysiology data). Expanded consideration of different synchronization methods has been included in the manuscript (see lines 224-238).</p><disp-quote content-type="editor-comment"><p>(9) Is the computer vision-based calibration included as part of the GUI functionality? Please clarify. If it is part of the GUI, it's worth highlighting as a very useful feature.</p></disp-quote><p>The computer vision-based benchmarking is not included in the GUI. It is in the form of a script made specifically for this paper. However for treadmill-based experiments behaviorMate has other calibration tools built into it (see line 301-303).</p><disp-quote content-type="editor-comment"><p>(10) I went through the source code of the Arduino firmware, and it seems most &quot;open X for Y duration&quot; functions are implemented using the delay function. If this is indeed the case, it's generally a bad idea since delay completely pauses the execution and any events happening during the delay period may be missed. As an alternative, please consider approaches comparing timestamps or using interrupts.</p></disp-quote><p>We have avoided the use of interrupts on the GPIO due to the potential for unpredictable code execution. There is a delay which is only just executed if the duration is 10 ms or less as we cannot guarantee precision of the arduino eventloop cycling faster than this. Durations longer than 10 ms would be time stamped and non-blocking. We have adjusted this MAX_WAIT to be specified as a macro so it can be more easily adjusted (or set to 0).</p><disp-quote content-type="editor-comment"><p>(11) Figure 3 B, C, D, and Figure 4 D, E suffer from noticeable low resolution.</p></disp-quote><p>We have converted Figure 3B, C, D and 4C, D, E to vector graphics in order to improve the resolution.</p><disp-quote content-type="editor-comment"><p>(12) Figure 4C is missing, which is an important figure.</p></disp-quote><p>This figure appeared when we rendered and submitted the manuscript. We apologize if the figure was generated such that it did not load properly in all pdf viewers. The panel appears correctly in the online eLife version of the manuscript. Additionally, we have checked the revision in Preview on Mac OS as well as Adobe Acrobat and the built-in viewer in Chrome and all figure panels appear in each so we hope this issue has been resolved.</p><disp-quote content-type="editor-comment"><p>(13) There are thin white grid lines on all heatmaps. I don't think they are necessary.</p></disp-quote><p>The grid lines have been removed from the heatmaps as suggested.</p><disp-quote content-type="editor-comment"><p>(14) Line 562 &quot;sometimes devices directly communicate with each other for performance reasons&quot;, I didn't find any elaboration on the P2P communication in the main text. This is potentially worth highlighting as it's one of the advantages of taking the IoT approaches.</p></disp-quote><p>In our implementation it was not necessary to rely on P2P communication beyond what is indicated in Figure 1. The direct communication referred to in line 562 is meant only to refer to the examples expanded on in the rest of the paragraph i.e. the behavior controller may signal the microscope directly using a TTL signal without looping back to the UI. As necessary users could implement UDP message passing between devices, but this is outside the scope of what we present in the manuscript.</p><disp-quote content-type="editor-comment"><p>(15) Line 147 &quot;Notably, due to the systems modular architecture, different UIs could be implemented in any programming language and swapped in without impacting the rest of the system.&quot;, this claim feels unsupported without a detailed discussion of how new code can be incorporated in the GUI (plugin system).</p></disp-quote><p>This comment refers to the idea of implementing “different UIs”. This would entail users desiring to take advantage of the JSON messaging API and the proposed electronics while fully implementing their own interface. In order to facilitate this option we have improved documentation of the messaging API posted in the README file accompanying the arduino source code. We have added reference to the supplemental materials where readers can find a link to the JSON API implementation to clarify this point.</p><p>Additionally, while a plugin system is available in the JavaFX version of behaviorMate, this project is currently under development and will update the online documentation as this project matures, but is unrelated to the intended claim about completely swapping out the UI.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>(6) Figure 1 - the terminology for each item is slightly different in the text and the figure. I think making the exact match can make it easier for the reader.</p><p>- Real-time computer (figure) vs real-time controller (ln88).</p></disp-quote><p>The manuscript was adjusted to match figure terminology.</p><disp-quote content-type="editor-comment"><p>- The position controller (ln565) - position tracking (Figure).</p></disp-quote><p>We have updated Figure 1 to highlight that the position controller does the position tracking.</p><disp-quote content-type="editor-comment"><p>- Maybe add a Behavior Controller next to the GPIO box in Figure 1.</p></disp-quote><p>We updated Figure 1 to highlight that the Behavior Controller performs the GPIO responsibility such that &quot;Behavior Controller&quot; and &quot;GPIO circuit&quot; may be used interchangeably.</p><disp-quote content-type="editor-comment"><p>- Position tracking (fig) and position controller (subtitle - ln209).</p></disp-quote><p>We updated Figure 1 to highlight that the position controller does the position tracking.</p><disp-quote content-type="editor-comment"><p>- Sync Pulse is not explained in the text.</p></disp-quote><p>The caption for Figure 1 has been updated to better explain the Sync pulse and additional systems boxes</p><disp-quote content-type="editor-comment"><p>(7) For Figure 3B/C: What is the number of data points? It would be nice to see the real population, possibly using a swarm plot instead of box plots. How likely are these outliers to occur?</p></disp-quote><p>In order to better characterize the distributions presented in our benchmarking data we have added mean and standard deviation information the plots 3 and 4. For Figure 3B: 0.0025 +/- 0.1128, Figure 3C: 12.9749 +/- 7.6581, Figure 4C: 66.0500 +/- 15.6994, Figure 4E: 4.1258 +/- 3.2558.</p></body></sub-article></article>