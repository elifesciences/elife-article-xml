<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">41861</article-id><article-id pub-id-type="doi">10.7554/eLife.41861</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neural dynamics of visual ambiguity resolution by perceptual prior</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-108333"><name><surname>Flounders</surname><given-names>Matthew W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7014-4665</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-108329"><name><surname>González-García</surname><given-names>Carlos</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6627-5777</contrib-id><email>cgonzalgarcia@gmail.com</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-123673"><name><surname>Hardstone</surname><given-names>Richard</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7502-9145</contrib-id><email>Richard.Hardstone@nyumc.org</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-71932"><name><surname>He</surname><given-names>Biyu J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-1549-1351</contrib-id><email>biyu.jade.he@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Neuroscience Institute</institution><institution>New York University Langone Medical Center</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Experimental Psychology</institution><institution>Ghent University</institution><addr-line><named-content content-type="city">Ghent</named-content></addr-line><country>Belgium</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Neurology</institution><institution>New York University Langone Medical Center</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution content-type="dept">Department of Neuroscience and Physiology</institution><institution>New York University Langone Medical Center</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution content-type="dept">Department of Radiology</institution><institution>New York University Langone Medical Center</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Reviewing Editor</role><aff><institution>University Medical Center Hamburg-Eppendorf</institution><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>03</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e41861</elocation-id><history><date date-type="received" iso-8601-date="2018-09-09"><day>09</day><month>09</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-02-25"><day>25</day><month>02</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Flounders et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Flounders et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-41861-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.41861.001</object-id><p>Past experiences have enormous power in shaping our daily perception. Currently, dynamical neural mechanisms underlying this process remain mysterious. Exploiting a dramatic visual phenomenon, where a single experience of viewing a clear image allows instant recognition of a related degraded image, we investigated this question using MEG and 7 Tesla fMRI in humans. We observed that following the acquisition of perceptual priors, different degraded images are represented much more distinctly in neural dynamics starting from ~500 ms after stimulus onset. Content-specific neural activity related to stimulus-feature processing dominated within 300 ms after stimulus onset, while content-specific neural activity related to recognition processing dominated from 500 ms onward. Model-driven MEG-fMRI data fusion revealed the spatiotemporal evolution of neural activities involved in stimulus, attentional, and recognition processing. Together, these findings shed light on how experience shapes perceptual processing across space and time in the brain.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual perception</kwd><kwd>past experience</kwd><kwd>prior knowledge</kwd><kwd>MEG</kwd><kwd>7T fMRI</kwd><kwd>representational similarity analysis</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>He</surname><given-names>Biyu J</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Klingenstein-Simons Neuroscience Fellowship</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>He</surname><given-names>Biyu J</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000194</institution-id><institution>U.S. Department of State</institution></institution-wrap></funding-source><award-id>The Fulbright Program</award-id><principal-award-recipient><name><surname>González-García</surname><given-names>Carlos</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>BCS-1753218</award-id><principal-award-recipient><name><surname>He</surname><given-names>Biyu J</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural dynamics during prior experience-guided visual ambiguity resolution reveal early stimulus-feature processing and late content-specific recognition processing involving large-scale brain networks.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Perception reflects not only immediate patterns of sensory inputs but also memories acquired through prior experiences with the world (<xref ref-type="bibr" rid="bib24">Helmholtz, 1924</xref>; <xref ref-type="bibr" rid="bib2">Albright, 2012</xref>). For instance, reading handwriting greatly depends on our existing knowledge of vocabulary and grammar. Stored representations from previous experiences provide likely interpretations of sensory data about their cause and meaning, overcoming the ever-present noise, ambiguity and incompleteness of retinal image. However, to date neural mechanisms underlying prior experiences’ influence on visual perception remain largely unknown.</p><p>Here, we adopted the Mooney image paradigm to investigate visual perception of identical sensory input that results in distinct perceptual outcomes depending on whether or not prior knowledge is present. These Mooney images are degraded, two-tone images created from natural photographs of objects and animals. Even after multiple presentations, the content of these images typically remains unrecognizable. However, once exposed to the corresponding non-degraded original photograph, subjects effortlessly recognize the Mooney image in future presentations – a disambiguation effect that lasts for days, months, even a lifetime (<xref ref-type="bibr" rid="bib33">Ludmer et al., 2011</xref>; <xref ref-type="bibr" rid="bib2">Albright, 2012</xref>). This phenomenon illustrates that a prior that guides perception can be established in a remarkably fast and robust manner. Thus, Mooney images offer an experimentally controlled paradigm for dissecting how prior experience shapes perceptual processing.</p><p>Previous neuroimaging studies have observed that disambiguation of Mooney images induces widespread activation and enhanced image-specific information in both visual and frontoparietal cortices (<xref ref-type="bibr" rid="bib13">Dolan et al., 1997</xref>; <xref ref-type="bibr" rid="bib23">Hegdé and Kersten, 2010</xref>; <xref ref-type="bibr" rid="bib25">Hsieh et al., 2010</xref>; <xref ref-type="bibr" rid="bib17">Gorlin et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">van Loon et al., 2016</xref>; <xref ref-type="bibr" rid="bib16">González-García et al., 2018</xref>). However, due to the slow temporal resolution of these techniques (positron emission tomography and fMRI), the temporal dynamics underlying this effect remains unknown. This is an important open question because behavioral studies have shown that recognition of Mooney images, even after disambiguation, is slow – with reaction times at around 1.2 s (<xref ref-type="bibr" rid="bib23">Hegdé and Kersten, 2010</xref>). By contrast, neural dynamics underlying recognition of intact, unambiguous images, as well as scene-facilitation of object recognition, typically conclude within 500 ms (<xref ref-type="bibr" rid="bib8">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">van de Nieuwenhuijzen et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Kaiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib5">Brandman and Peelen, 2017</xref>). Together with a recent finding of altered content-specific neural representations in frontoparietal regions following Mooney image disambiguation (<xref ref-type="bibr" rid="bib16">González-García et al., 2018</xref>), these observations raise the intriguing possibility that slow (taking longer than 500 ms), long-distance recurrent neural dynamics involving large-scale brain networks are necessary for prior-experience-guided visual recognition.</p><p>Several previous EEG and MEG studies reported disambiguation-induced decrease in beta-band power and increase in gamma-band power (<xref ref-type="bibr" rid="bib18">Grützner et al., 2010</xref>; <xref ref-type="bibr" rid="bib35">Minami et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Moratti et al., 2014</xref>), but these effects could potentially be attributed to non-content-specific effects such as increased attention, salience or decreased task difficulty following disambiguation. To unravel neural mechanisms underlying prior experience’s influence on perception, an important unanswered question is how different information processing stages are dynamically encoded in neural activities.</p><p>Here, we probe the dynamical encoding of perceptual state (before or after disambiguation) as well as the physical features and recognition outcomes related to individual images in neural activities, using multivariate pattern decoding and representational similarity analysis (RSA) applied to whole-head MEG data. In addition, to illuminate the anatomical distribution of the evolving neural dynamics involved in different information processing stages, we applied model-driven cross-modal RSA (<xref ref-type="bibr" rid="bib22">Hebart et al., 2018</xref>) to combine the high temporal resolution of MEG data with high spatial resolution of 7T fMRI data collected using a similar paradigm. These approaches allowed us to spatiotemporally resolve neural dynamics underlying different stages of information processing during prior-guided visual perception.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Paradigm and behavioral results</title><p>Eighteen subjects were shown 33 Mooney images containing animals or manmade objects. Each Mooney image was presented six times before its corresponding grayscale image was shown to the subject, and six times after. Following each Mooney image presentation, subjects reported whether they could recognize the image using a button press (‘subjective recognition’). Each MEG ‘run’ (<xref ref-type="fig" rid="fig1">Figure 1</xref>; for details see Materials and methods, <italic>Task paradigm</italic>) included three different grayscale images, their corresponding post-disambiguation Mooney images, and three new Mooney images shown pre-disambiguation (their corresponding grayscale images would be shown in the next run). To ensure that subjects’ self-reported recognition matched the true content of the Mooney images, at the end of each run, Mooney images presented during that run were shown again and participants were asked to verbally report what they saw in the image and were allowed to answer ‘unknown’. This resulted in a verbal test for each Mooney image once before disambiguation and once after disambiguation (‘verbal identification’). Verbal responses were scored as correct or incorrect using a pre-determined list of acceptable responses for each image. In addition, six Mooney images were presented with non-matching grayscale images using identical block and run structure, which served as controls for the effect of repetition (‘catch image sets’, as opposed to the 33 ‘real image sets’ described earlier).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.41861.002</object-id><label>Figure 1.</label><caption><title>Task paradigm.</title><p>(<bold>A</bold>) Trial structure. The left/right position (and corresponding button) for ‘Yes’/‘No’ answer was randomized from trial to trial. (<bold>B</bold>) Block and run structure. Each block includes 15 trials: three grayscale images, six Mooney images in a randomized order, then a repeat of these six Mooney images in a randomized order. Three of the six Mooney images correspond to the grayscale images presented in the same run and are presented post-disambiguation. The other three Mooney images are presented pre-disambiguation, and their matching grayscale images will be shown in the following run. An experimental run consists of a block presented three times with randomized image order, followed by a verbal test (for details, see Materials and methods). Mooney images were not presented to subjects with colored frames.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41861-fig1-v2.tif"/></fig><p>Viewing the corresponding grayscale image had a dramatic effect on Mooney image recognition as shown by the following results. First, we compared recognition rates between pre- and post-disambiguation stages using a two-way repeated-measures ANOVA [factors: presentation stage and image-set type (real vs. catch)]. This analysis was carried out separately using subjective recognition rates (pooled across six presentations) and correct verbal identification rates as the dependent variable (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). There was a significant main effect of presentation stage (pre- vs. post-disambiguation) on both subjective recognition (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, left, F<sub>1,68</sub> = 42.6, p=1.0 × 10<sup>−8</sup>, η<sup>2</sup><sub>p</sub> = 0.38) and verbal identification (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right, F<sub>1,68</sub> = 60.7, p=5.2 × 10<sup>−11</sup>, η<sup>2</sup><sub>p</sub> = 0.47). Crucially, there was also a significant interaction effect of presentation stage × image set type on both subjective recognition (F<sub>1,68</sub> = 16.4, p=1.0 × 10<sup>−4</sup>, η<sup>2</sup><sub>p</sub> = 0.19) and verbal identification (F<sub>1,68</sub> = 38.5, p=3.7 × 10<sup>−8</sup>, η<sup>2</sup><sub>p</sub> = 0.36), suggesting that the effect of disambiguation by viewing the corresponding grayscale image significantly exceeds that induced by repetition.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.41861.003</object-id><label>Figure 2.</label><caption><title>Disambiguation-related behavioral effects.</title><p>(<bold>A</bold>) Subjective recognition rates (left, averaged across six presentations) and correct verbal identification rates (right) for Mooney images presented in the pre- and post- disambiguation period (gold and teal bars). Corresponding results for catch images are shown in open bars. p-Values corresponding to the interaction effect (pre vs. post × catch vs. non-catch) of two-way ANOVAs are shown in the graph. (<bold>B</bold>) Subjective recognition rates grouped by presentation number in the pre- and post- disambiguation stage (gold and teal), as well as the corresponding rates for catch images (white lines). (<bold>C</bold>) Distribution of subjective recognition rates across Mooney images in the pre- (gold bars, left) and post- (teal bars, right) disambiguation stage. Corresponding distributions for catch images are shown as open bars. All results show mean and s.e.m. across subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41861-fig2-v2.tif"/></fig><p>Second, given that each Mooney image was presented six times before and six times after disambiguation with subjective recognition probed each time, we assessed the effect of repetition on Mooney image recognition. To this end, we conducted a three-way repeated-measures ANOVA [factors: presentation number (1 – 6), presentation stage (pre- vs. post-disambiguation), and image-set type (real vs. catch)] on subjective recognition rates (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). No main or interaction effect involving presentation number was significant (all p&gt;0.2). However, consistent with the previous analysis, there was a highly significant interaction effect of presentation stage ×image set type (F<sub>1,408</sub> = 85.2, p=1.5 × 10<sup>−18</sup>, η<sup>2</sup><sub>p</sub> = 0.17). These results suggest that viewing the corresponding grayscale image, but not repeated viewing of the Mooney image, significantly facilitates Mooney image recognition.</p><p>Last, we examined the distribution of subjective recognition rates across individual Mooney images and subjects. The distribution of subjective recognition rates was separately plotted for pre- (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, left) and post- (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, right) disambiguation images, and for real (colored bars) and catch (empty bars) image sets. A bimodal distribution is observed in both stages. Accordingly, we defined those images recognized two or fewer times as ‘not-recognized’, and those recognized four or more times as ‘recognized’ (rectangles in <xref ref-type="fig" rid="fig2">Figure 2C</xref>). Based on these criteria, for real image sets, 60.9 ± 4.2% are not recognized and 34.0 ± 3.4% are recognized in the pre-disambiguation stage, whereas only 10.3 ± 2.4% are not recognized and 87.4 ± 2.5% are recognized in the post-disambiguation stage.</p><p>Together, these results show the robustness of the Mooney image disambiguation effect, whereby having seen a related unambiguous image dramatically facilitates recognition for the degraded Mooney image. Importantly, in our paradigm a grayscale image is rarely followed by its corresponding Mooney image in the immediate next trial due to the block structure and the shuffling of image sequence (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). This design and the long trial length (on average 8.5 s, see <xref ref-type="fig" rid="fig1">Figure 1A</xref>) ensured that the disambiguation effect cannot be driven by low-level priming (also see <xref ref-type="bibr" rid="bib9">Chang et al., 2016</xref>).</p></sec><sec id="s2-2"><title>Disambiguation changes neural activity patterns elicited by Mooney images</title><p>To dissect the neural dynamics underlying the resolution of visual ambiguity by prior experience, we first investigated the neural activities that distinguish between pre- and post- disambiguation Mooney images regardless of image identity, using time-resolved multivariate decoding applied to whole-brain MEG data. This analysis was carried out using two frequency bands: slow cortical potentials (SCPs, 0.05–5 Hz) that were recently shown to be involved in conscious vision (<xref ref-type="bibr" rid="bib32">Li et al., 2014</xref>; <xref ref-type="bibr" rid="bib4">Baria et al., 2017</xref>) and the classic event-related field (ERF, DC – 35 Hz) band that has been used to decode stimulus features under seen and unseen conditions (<xref ref-type="bibr" rid="bib43">Salti et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">King et al., 2016</xref>). Using the 33 real image sets, we constructed three classifiers: First, all 33 unique Mooney images are included, and presentation stage (pre- vs. post-disambiguation) was decoded (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>, green). Second, because a small number of Mooney images (34.0 ± 3.4%) were spontaneously recognized pre-disambiguation, and 10.3 ± 2.4% of Mooney images remained unrecognized in the post-disambiguation period, we performed a behaviorally constrained analysis to more precisely target the disambiguation effect: for each subject, Mooney images that were both not-recognized (/not-identified) pre-disambiguation and recognized (/identified) post-disambiguation are selected. This resulted in 16.0 ± 1.4 (mean ±s.e.m. across subjects) real image sets based on subjective recognition (or 17.5 ± 1.0 real image sets based on verbal identification). Using these image sets, the presentation stage (pre- vs. post- disambiguation) was decoded (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>, blue and magenta). In a control analysis, the presentation stage of the catch image sets was also decoded, regardless of behavioral responses (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>, black traces).</p><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.41861.004</object-id><label>Figure 3.</label><caption><title>Decoding perceptual state information.</title><p>(<bold>A</bold>) Decoding accuracy using SCP (0.05–5 Hz) activity across all sensors. Classifiers were constructed to decode i) presentation stage for all 33 Mooney images (Pre vs. Post; green); ii) presentation stage using only Mooney images that are not recognized pre-disambiguation and recognized post-disambiguation (Disambiguation, subjective recognition; blue); iii) presentation stage using only Mooney images that are not identified pre-disambiguation and identified post-disambiguation (Disambiguation, verbal identification; magenta); iv) ‘presentation stage’ for catch images, where the grayscale images did not match the Mooney images (black). Shaded areas represent s.e.m. across subjects. Horizontal bars indicate significant time points (p&lt;0.05, cluster-based permutation tests). (<bold>B</bold>) Same as A, but for ERF (DC – 35 Hz) activity. (<bold>C</bold>) Activation patterns transformed from decoder weight vectors of the ‘Disambiguation, subjective recognition’ decoder constructed using SCP (top row) and ERF (bottom row) activity, respectively. (<bold>D</bold>) Left: TGM for the ‘Disambiguation, subjective recognition’ decoder constructed using SCP. Significance is outlined by the dotted black trace. Right: Cross-time generalization accuracy for classifiers trained at five different time points (marked as red vertical lines; blue traces, corresponding to rows in the TGM). The within-time decoding accuracy (corresponding to the diagonal of the TGM and blue trace in A) is plotted in black for comparison. Solid traces show significant decoding (p&lt;0.05, cluster-based permutation test); shaded areas denote significant differences between within- and across- time decoding (p&lt;0.05, FDR corrected). The black vertical bars in A, B, and D denote onset (0 s) and offset (2 s) of image presentation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41861-fig3-v2.tif"/></fig><p>With SCP activity (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), the classifier using real image sets performed significantly above chance level at every time point from 300 ms onward (green trace and bar, p&lt;0.05, cluster-based permutation test). Classifiers based on behavioral performance performed similarly (blue and magenta). By contrast, the classifier applied to catch image sets yielded chance-level accuracy throughout the trial epoch (black). A control analysis showed that decoding of presentation stage based on six randomly selected real image sets to match the statistical power of catch image sets still yielded sustained significant decoding from 300 ms onward (figure not shown). All the results are qualitatively similar but, as expected, with more high-frequency fluctuations, when the classifiers were constructed using the ERF band (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>In order to shed light on the neural activity contributing to successful decoding, we performed activation pattern transform on the classifier weights (<xref ref-type="bibr" rid="bib21">Haufe et al., 2014</xref>). Activation patterns corresponding to the SCP and ERF classifiers based on subjective recognition responses (<xref ref-type="fig" rid="fig3">Figure 3A–B</xref>, blue traces) are plotted in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, which reveal strong frontotemporal contributions starting within 400 ms after stimulus onset (note that both large positive and large negative values contribute strongly to the classifier). Later, we will probe the spatiotemporal evolution of relevant neural activities more precisely using model-driven MEG-fMRI data fusion.</p></sec><sec id="s2-3"><title>Cross-time generalization of decoding perceptual states</title><p>The above decoding analysis assesses whether there is separable neural activity pattern information between perceptual states at each time point following stimulus onset. To understand how this pattern information evolves over time, we next investigated decoder cross-time generalization (<xref ref-type="bibr" rid="bib28">King and Dehaene, 2014</xref>). Here, a classifier trained at a given time point is tested on other time points in the trial epoch, and the extent to which it generalizes to another time point reveals how similar or different the underlying neural activity patterns are between the two time points. The temporal generalization matrix (TGM) corresponding to the subjective recognition classifier constructed with the SCP band (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, blue trace) is shown in <xref ref-type="fig" rid="fig3">Figure 3D</xref>, left panel. Results obtained using the ‘Pre vs. Post’ classifier and the verbal identification classifier, or using the ERF band, were qualitatively similar (not shown). Significant generalization (p&lt;0.05, cluster-based permutation test, outlined by the dashed trace) was obtained in a large temporal cluster that includes a wide diagonal beginning at ~300 ms, and a square shape from ~800 ms until the end of the trial epoch. The cross-time generalization accuracies of five classifiers trained at 400 ms intervals are shown as blue traces in <xref ref-type="fig" rid="fig3">Figure 3D</xref>, right panel, as compared with the within-time decoding accuracy, shown as black traces. For the classifier trained at 400 ms (bottom row), its cross-time generalization time course departs significantly from the within-time decoding time course (shown as shaded areas, p&lt;0.05, paired t-tests, FDR corrected). By contrast, for the classifiers trained at 800 ms or later, their cross-time generalization accuracies closely tracked the within-time decoding accuracy. Together, these results suggest evolving patterns of neural activity until ~800 ms after stimulus onset and thereafter sustained activity patterns that distinguish pre- and post-disambiguation perceptual states.</p></sec><sec id="s2-4"><title>Disambiguation increases across-image dissimilarity in neural dynamics</title><p>The above results show separation in neural activity patterns between Mooney images presented in the pre- vs. post-disambiguation period, when the same physical images elicit distinct perceptual outcomes (meaningless blobs vs. recognizable animals or objects). However, these results do not necessarily suggest image-content-specific processing: successful decoding of perceptual state may also reflect non-content-specific processing such as heightened attention and salience or reduced task difficulty associated with post-disambiguation images. To probe content-specific neural processing, we next turned to RSA (<xref ref-type="bibr" rid="bib29">Kriegeskorte et al., 2008a</xref>) which allows for individual-image-level comparisons of neural activity patterns. At every time point in the trial epoch, we constructed a representational dissimilarity matrix (RDM), where each element contains the neural dissimilarity (quantified as 1 – Pearson’s r, computed over all sensors, see Materials and methods, <italic>RSA</italic>) between two individual images (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Neural dissimilarity was calculated both for different images presented in the same condition (e.g., the Pre-Pre square of the RDM), and for (same or different) images presented in different conditions (e.g., the Pre-Post square of the RDM).</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.41861.005</object-id><label>Figure 4.</label><caption><title>Image-level representations are influenced by prior information.</title><p>(<bold>A</bold>) Schematic for RSA. See Materials and methods, <italic>RSA</italic>, for details. (<bold>B</bold>) Group-average MEG representation dissimilarity matrices (RDMs) at selected time points. (<bold>C</bold>) Schematics for analyses shown in D and E. (<bold>D</bold>) Mean across-image representational dissimilarity in each perceptual condition, calculated by averaging the elements in the upper triangles for each condition in the RDM (see C-i). Horizontal bars denote significant differences between conditions (p&lt;0.05, cluster-based permutation tests). (<bold>E</bold>) Results from the intra-RDM analysis showing time courses of neural activity related to ‘stimulus-based’ and ‘recognition-based’ representation, obtained by performing element-wise correlations between the Pre and Post triangles in the RDM, and between the Post and Gray triangles, respectively (see C-ii). Correlation values were Fisher-z-transformed. Horizontal bars denote significance for each time course (p&lt;0.05, cluster-based permutation tests). Shaded areas in D and E show s.e.m. across subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41861-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41861.006</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Mean across-image representational dissimilarity in each perceptual condition for catch image sets.</title><p>Format is similar to <xref ref-type="fig" rid="fig4">Figure 4D</xref>, except that representational dissimilarity was computed using catch image sets. For catch image sets, grayscale images presented did not correspond to their respective Mooney images, that is no prior information is provided. There was no significant difference between perceptual conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41861-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41861.007</object-id><label>Figure 4—figure supplement 2.</label><caption><title>Mean across-image representational dissimilarity in each perceptual condition for real image sets, calculated using Euclidean distance.</title><p>Similar to <xref ref-type="fig" rid="fig4">Figure 4D</xref>, except that representational dissimilarity was computed as squared Euclidean distance. Horizontal bars denote significant differences between conditions (p&lt;0.05, cluster-based permutation tests). Non-cross-validated (<bold>A</bold>) and cross-validated (<bold>B</bold>) results yield similar time courses differing slightly in overall amplitude. Significant differences between Pre and Post conditions begin at 1070 ms for both methods. Significant differences between Pre and Gray conditions for non-cross-validated and cross-validated results begin at 1060 ms and 1100 ms, respectively. Cross-validation was carried out using a three-fold scheme. Note that due to the preprocessing step of normalizing MEG data across sensors at each time step (see Materials and methods, <italic>RSA</italic>), both Euclidean distance and the original 1 – r measure capture the angle between two population activity vectors (for discussions on the angle measure, see <xref ref-type="bibr" rid="bib4">Baria et al., 2017</xref>). In addition, baseline correction (see Materials and methods, <italic>MEG data preprocessing</italic>) necessitated that angle difference in the pre-stimulus period is largest.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41861-fig4-figsupp2-v2.tif"/></fig></fig-group><p>Group-average RDMs at five different time points are shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. Two patterns can be seen: First, at 300–600 ms following stimulus input, dissimilarity across all image pairs is relatively low, presumably driven by visual-evoked activity that has a similar gross spatial pattern for all images. Second, after 600 ms, dissimilarity between individual Mooney images in the pre-disambiguation period (Pre-Pre square of the RDM) is visibly lower than between post-disambiguation Mooney images (Post-Post square) or between grayscale images (Gray-Gray square). We quantified these effects by averaging across elements within the upper triangles of the Pre-Pre, Post-Post and Gray-Gray squares of the RDM (<xref ref-type="fig" rid="fig4">Figure 4C–i</xref>) and plotting the mean across-image dissimilarity for each perceptual condition across time (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). In all perceptual conditions, neural dissimilarity between individual images decreases sharply following stimulus onset and reaches the trough around 400 ms. However, from ~500 ms onward, across-image dissimilarity was substantially lower for pre-disambiguation Mooney images (gold) than for post-disambiguation Mooney images (teal) or grayscale images (black), and this difference is significant from ~1 s to the end of the trial epoch (p&lt;0.05, cluster-based permutation tests). The difference between post-disambiguation Mooney images and gray-images was not significant at any time point. A similar analysis applied to catch image sets did not yield any significant difference between pre- and post-disambiguation stages, or between Mooney and grayscale images (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>To ensure that the increase of across-image dissimilarity following disambiguation was not driven by increased noise in the data, we re-conducted the analysis shown in <xref ref-type="fig" rid="fig4">Figure 4D</xref> using cross-validated Euclidean distance, a distance metric that is unaffected by changing levels of noise and only captures the signal component that is shared between different partitions of the data set (see Materials and methods, <italic>Euclidean Distance</italic>). The results, shown in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, reproduce the findings in <xref ref-type="fig" rid="fig4">Figure 4D</xref> and confirm that the increased across-image dissimilarity following disambiguation was not driven by a changing level of noise between perceptual conditions.</p><p>Together, these results suggest that, even though Mooney images presented in the two task stages are physically identical, following the acquisition of perceptual priors, different Mooney images are represented much more distinctly in neural dynamics, in fact as distinctly as the grayscale images.</p><p>This dramatic effect raises two important questions: 1) Is this effect driven by the neural representations of Mooney images shifting towards those of their respective grayscale counterparts? 2) Does this effect reflect image-content-specific processing at the single-trial level? To answer these questions, we next probe how neural representation for a particular Mooney image changes following disambiguation at the single-trial level.</p></sec><sec id="s2-5"><title>Comparing image-specific dynamic neural representations across perceptual conditions at the single-trial level</title><p>Here, we assess how disambiguation changes dynamical neural representations by analyzing single-trial separability of neural activity patterns elicited by the same (or matching Mooney-grayscale) image presented in different conditions. To this end, we used a measure akin to single-trial decoding to quantify separability (at the single-trial level) of neural activities elicited by two images (e.g. Images A and B in the same condition or different conditions, or Image A in different conditions). This measure calculates how much neural similarity across multiple presentations of the same image exceeds neural similarity between the two different images (i.e. r<sub>within</sub> − r<sub>between</sub>, <xref ref-type="fig" rid="fig5">Figure 5A–i</xref>, for details see Materials and methods, <italic>Single-trial separability</italic>). Unlike the 1 − r distance measure used in the previous analysis (<xref ref-type="fig" rid="fig4">Figure 4</xref>), this metric takes into account the within-image, across-trial variability: for instance, in the two examples given in <xref ref-type="fig" rid="fig5">Figure 5A–ii</xref> and 1 − r distance is identical, but single-trial separability (r<sub>within</sub> − r<sub>between</sub>) is higher in the top example.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.41861.008</object-id><label>Figure 5.</label><caption><title>Single-trial separability analysis.</title><p>(<bold>A</bold>) i: Schematic for separability calculation. For details, see Materials and methods, <italic>Single-trial separability</italic>. ii: Two hypothetical examples of single-trial neural activity patterns (projected to a 2-D plane) for Image A (black dots) and Image B (gray dots). The neural dissimilarity calculated based on trial-averaged activity patterns (1 – r measure used earlier) is identical between the two examples, while single-trial separability (r<sub>within</sub> - r<sub>between</sub>) is higher in the top example. (<bold>B</bold>) Group-average MEG RDMs computed with the separability (r<sub>within</sub> - r<sub>between</sub>) measure at selected time points. (<bold>C</bold>) Quantifying separability between neural activity patterns elicited by the same/matching image presented in different conditions. (i) Analysis schematic: diagonal elements in the between-condition squares of the RDM are averaged together, yielding three time-dependent outputs corresponding to the three condition-pairs. (ii) Separability time courses averaged across 33 real image sets for each between-condition comparison, following the color legend shown in C-i. The top three horizontal bars represent significant (p&lt;0.05, cluster-based permutation test) time points of each time course compared to chance level (0); and the bottom three bars represent significance of pairwise comparisons between the time courses. (<bold>D</bold>) Quantifying the difference between off-diagonal and diagonal elements in the between-condition squares of the separability RDMs. Intuitively, this analysis captures how similar an image is to itself or its matching version presented in a different condition over and above its similarity to other images presented in that same condition. Statistical significance (p&lt;0.05, cluster-based permutation test) for pairwise comparisons are shown as horizontal bars. When compared to chance (0), the three traces are significant from 40 ms (Pre-Post), 50 ms (Pre-Gray), and 60 ms (Post-Gray) onward until after image offset, respectively. Traces in C-ii- and D-ii show mean and s.e.m. across subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41861-fig5-v2.tif"/></fig><p>We reconstructed the RDMs using the separability metric calculated across all MEG sensors. As shown in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, there are darker diagonals in the between-condition squares: for example, in the Pre-Post square at 100 ms, and in the Post-Gray square at 800 ms. This suggests that single-trial separability is lower between the same/matching image presented across conditions than between different images presented across conditions. Below we evaluate how disambiguation alters neural representation of the same image using two quantitative analyses applied to these RDMs.</p><p>First, we extracted the mean of diagonal elements within each between-condition square of the RDM (<xref ref-type="fig" rid="fig5">Figure 5C–i</xref>), which quantifies how separable neural activity patterns are between the same Mooney image presented before and after disambiguation (Pre-Post), between a pre-disambiguation Mooney image and its matching grayscale image (Pre-Gray), and between a post-disambiguation Mooney image and its matching grayscale image (Post-Gray). The result, plotted in <xref ref-type="fig" rid="fig5">Figure 5C–ii</xref>, shows that pre-disambiguation Mooney images are well separable from their matching grayscale images from 50 ms until the end of trial epoch (orange trace and significance bar, p&lt;0.05, cluster-based permutation test). By contrast, post-disambiguation Mooney images are separable from their matching grayscale images only in a short window – from 50 to 530 ms (<xref ref-type="fig" rid="fig5">Figure 5C–ii</xref>, dark blue). Strikingly, neural representation of a post-disambiguation image is entirely indistinguishable from that of its matching grayscale image from ~550 ms onward (as shown by separability fluctuating around chance level; <xref ref-type="fig" rid="fig5">Figure 5C–ii</xref>, dark blue), despite the difference in the stimulus input. The separability between the same Mooney image presented before and after disambiguation starts to increase after stimulus onset, and reaches significance at 270 ms, which remains significant until the end of trial epoch (<xref ref-type="fig" rid="fig5">Figure 5C–ii</xref>, green). These results suggest that early (&lt;300 ms) neural activity patterns distinguish between different physical stimulus inputs – Mooney vs. grayscale images, while late (&gt;500 ms) neural activity patterns distinguish between recognizing the image content and failing to do so.</p><p>Second, we quantified the strength of between-condition diagonals (i.e. separability between the same/matching image presented across conditions) as compared to off-diagonal elements in the same between-condition squares (i.e. separability between different images presented across conditions), by computing the mean for each and calculating the difference between them (<xref ref-type="fig" rid="fig5">Figure 5D–i</xref>). This metric captures how similar an image is represented to itself or its matching version presented in a different condition over and above its similarity to other images presented in that same condition. The comparison between the Post-Gray traces in <xref ref-type="fig" rid="fig5">Figure 5C–ii</xref> and <xref ref-type="fig" rid="fig5">Figure 5D–ii</xref> (dark blue) reveals a striking effect after 550 ms: although neural activity patterns elicited by a post-disambiguation image are indistinguishable from those elicited by its matching grayscale image, they are well separable from other grayscale images, suggesting an image-specific shift in neural representation toward the relevant prior experience that guides perception. The difference between off-diagonal and diagonal separability is larger for the Pre-Post square than the Pre-Gray square from 20 to 520 ms (<xref ref-type="fig" rid="fig5">Figure 5D–ii</xref>, magenta), and larger for the Post-Gray square than the Pre-Gray square from 380 ms to 1.3 s (<xref ref-type="fig" rid="fig5">Figure 5D–ii</xref>, cyan).</p><p>Together, these results provide strong evidence that early (&lt;300 ms) neural dynamics carry stimulus-content-specific processing and late (&gt;500 ms) neural dynamics carry recognition-content-specific processing.</p></sec><sec id="s2-6"><title>Temporal separation of stimulus- and recognition-related neural representations</title><p>To further shed light on neural mechanisms underlying different information processing stages involved in prior-guided visual disambiguation, we investigated how the representational geometry (i.e. the set of representational distances between image-pairs) compares between perceptual conditions (for details see <italic>Materials and methods</italic>, <italic>Intra-RDM analysis</italic>). We reasoned that because the same set of Mooney images are presented in the Pre and Post stages, and they are ordered in the same sequence within the Pre-Pre and Post-Post squares of the RDM, neural activity reflecting stimulus-feature processing should exhibit a similar pattern between Pre-Pre and Post-Post squares of the RDM, and this can be quantified by performing an element-by-element correlation between these two portions of the RDM (<xref ref-type="fig" rid="fig4">Figure 4C–ii</xref>, ‘stimulus-based’ representation). Likewise, because the recognition content of a post-disambiguation Mooney image is similar to that of its corresponding grayscale image despite different stimulus input (e.g. ‘it’s a crab!’), neural activity reflecting recognition-content processing should exhibit a similar pattern between the Post-Post and Gray-Gray squares of the RDM, and this effect can be quantified by an element-by-element correlation between these two portions of the RDM (<xref ref-type="fig" rid="fig4">Figure 4C–ii</xref>, ‘recognition-based’ representation). For simplicity and ease of interpretation, for this analysis we used RDMs calculated based on trial-averaged activity patterns using the 1 – r measure (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><p>Neural activity reflecting stimulus processing (indexed by r[Pre-Pre,Post-Post]) exhibited an early sharp peak reaching significance at 30–310 ms following stimulus onset and a small second peak that reaches significance at 1340–1420 ms (p&lt;0.05, cluster-based permutation test; <xref ref-type="fig" rid="fig4">Figure 4E</xref>, green). By contrast, neural activity reflecting recognition processing (indexed by r[Post-Post,Gray-Gray]) occurs in a broad temporal period: it onsets shortly after stimulus onset and reaches significance at 690–1240 ms and again from 1550 ms to after stimulus offset (<xref ref-type="fig" rid="fig4">Figure 4E</xref>, magenta). As a control measure, the correlation between Pre-Pre and Gray-Gray squares of the RDM was not significant at any time point, suggesting that, as expected, representational geometry is different between conditions with different stimulus input and different recognition outcomes. In addition, an analysis using RDMs constructed with cross-validated Euclidean distances (for details, see Materials and methods, <italic>Euclidean distance</italic>) yielded similar results (figure not shown), suggesting that these findings are not driven by changing levels of noise between conditions. Nonetheless, likely due to insufficient statistical power, a direct contrast between r[Pre-Pre,Gray-Gray] and r[Post-Post,Gray-Gray] did not yield any significant time point following cluster-based correction. Thus, these results provide qualitative evidence – at the level of representational geometry – in accordance with our earlier conclusion that prior-guided visual disambiguation involves a dynamic two-part process including early stimulus-feature-related processing and late recognition-content-related processing. In the final analysis presented below, we will quantitatively test this possibility using a model-driven MEG-fMRI fusion analysis that simultaneously elucidates the spatial dimension of the evolving neural dynamics.</p></sec><sec id="s2-7"><title>Model-driven MEG-fMRI data fusion spatiotemporally resolves neural dynamics related to stimulus, attention, and recognition processing</title><p>In order to spatiotemporally resolve neural dynamics underlying different information processing stages in prior-guided visual perception, we applied a recently developed model-driven MEG-fMRI data fusion approach (<xref ref-type="bibr" rid="bib22">Hebart et al., 2018</xref>). Nineteen additional subjects performed a similar Mooney image task, with an identical set of Mooney and grayscale images, during whole-brain 7T fMRI scanning (for details see Materials and methods). Our earlier MVPA and RSA results obtained from this fMRI data set suggested the involvement of frontoparietal (FPN) and default-mode (DMN) network regions in Mooney image disambiguation, in addition to early and category-selective visual areas (<xref ref-type="bibr" rid="bib16">González-García et al., 2018</xref>). Based on these findings, 20 regions-of-interest (ROIs) were defined, covering early visual cortex (V1-V4), lateral occipital complex (LOC), fusiform gyrus (FG), and regions in the FPN and DMN (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p><fig-group><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.41861.009</object-id><label>Figure 6.</label><caption><title>Model-based MEG-fMRI fusion analysis.</title><p>(<bold>A</bold>) Model RDMs and analysis cartoon. <italic>Left</italic>: RDMs corresponding to ‘Stimulus’, ‘Recognition’, and ‘Attention’ models. For details, see <italic>Results. Right</italic>: MEG RDM from each time point and fMRI RDM from each ROI are compared, and shared variance between them that is accounted for by each model is computed. (<bold>B</bold>) ROIs used in the fMRI analysis. For details, see Materials and methods. These were defined based on a previous study (<xref ref-type="bibr" rid="bib16">González-García et al., 2018</xref>). (<bold>C</bold>) Schematics for the commonality analyses employed in the model-based MEG-fMRI data fusion (results shown in E-F). Because neural activities related to the Stimulus and Recognition model overlap in time (see D), to dissociate them, variance uniquely attributed to each model was calculated (left). Shared variance between MEG and fMRI RDMs accounted for by the attention model is also assessed (right). (<bold>D</bold>) Correlation between model RDMs and MEG RDMs at each time point. Horizontal bars denote significant correlation (p&lt;0.05, cluster-based permutation tests). (<bold>E</bold>) Commonality analysis results for Stimulus (<bold>i</bold>), Recognition (ii), and Attention (iii) models. Colors denote significant (p&lt;0.05, cluster-based permutation tests) presence of neural activity corresponding to the model in a given ROI and at a given time point (with 10 ms steps). (<bold>F</bold>) Commonality time courses for the Stimulus (red) and Recognition (yellow) model (analysis schematic shown in panel C, left) for five selected ROIs, showing shared MEG-fMRI variance explained by each model. Total shared variance between MEG and fMRI RDMs for each ROI is plotted as gray shaded area. PCC: posterior cingulate cortex; R Frontal: right frontal cortex in the FPN. Horizontal bars denote significant model-related commonality (p&lt;0.05, cluster-based permutation tests).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41861-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.41861.010</object-id><label>Figure 6—figure supplement 1.</label><caption><title>Stimulus and Recognition commonality results for all remaining ROIs.</title><p>Format is the same as <xref ref-type="fig" rid="fig6">Figure 6F</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-41861-fig6-figsupp1-v2.tif"/></fig></fig-group><p>We designed three model RDMs to capture different information processing stages (<xref ref-type="fig" rid="fig6">Figure 6A</xref>):</p><p><italic>First</italic>, a ‘stimulus’ model, which captures dissimilarity structure based on physical image features. This model includes three levels of dissimilarity: low (blue diagonal in the Pre-Post square, capturing dissimilarity between the same Mooney image presented pre- and post-disambiguation); medium (white off-diagonal elements in the Pre-Pre, Pre-Post, Post-Post, and Gray-Gray squares, capturing dissimilarity between different Mooney images and between different grayscale images); and high (red off-diagonal elements in the Pre-Gray and Post-Gray squares, capturing dissimilarity between Mooney and non-matching grayscale images). Thus, this model considers both gross image statistics (black-and-white Mooney vs. grayscale) and features specific to each image (the same Mooney image presented across conditions). For simplicity, the diagonals in the Pre-Gray and Post-Gray squares are excluded from this model, since we do not have an <italic>a priori</italic> judgment about these values in comparison to the others.</p><p><italic>Second</italic>, a ‘recognition’ model, which aims to capture content-specific recognition processing. This model includes three levels of dissimilarity: low (blue off-diagonal elements in the Pre-Pre square and blue diagonal in the Post-Gray square; we note that the equivalence of these two categories was arbitrary); medium (white Pre-Post and Pre-Gray squares); and high (red off-diagonal elements in the Post-Post, Post-Gray and Gray-Gray squares). This model capitalizes on the intuition that the content of recognition is image-specific, and postulates that neural representations of two recognized images that have different contents are most distinct from each other, those of two unrecognized images are most similar to each other, and the dissimilarity is intermediate between a recognized and an unrecognized image. Moreover, since a post-disambiguation Mooney image yields a similar recognition content as its matching grayscale image, the model assumes a low dissimilarity between them.</p><p><italic>Third</italic>, an ‘attention’ model, which captures dissimilarity structure based on the recognition <italic>status</italic>. This model includes two levels of dissimilarity: low (blue elements in the Pre-Pre, Post-Post, Gray-Gray, and Post-Gray squares); and high (red elements in the Pre-Post and Pre-Gray squares). This model postulates that non-recognized images (Pre) are represented similarly to each other, and recognized images (Post and Gray) are represented similarly to each other, while recognized and non-recognized images are represented differently. Thus, the ‘attention’ model captures changes induced by the status of recognition regardless of content, such as heightened attention or arousal that accompanies recognition.</p><p>We first assessed the correspondence between each model and the MEG RDM (computed using the 1 – r measure, see <xref ref-type="fig" rid="fig4">Figure 4B</xref>) at each time point (i.e. model-based RSA; for example see <xref ref-type="bibr" rid="bib20">Harel et al., 2014</xref>; <xref ref-type="bibr" rid="bib54">Wardle et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Vida et al., 2017</xref>). The results reveal distinct temporal waveforms for neural activity related to each model (<xref ref-type="fig" rid="fig6">Figure 6D</xref>): the stimulus model dominates in the early period before 500 ms and exhibits a second broad plateau between 600 ms and 2.5 s, reaching statistical significance at 50–410 and 620–2410 ms (p&lt;0.05, cluster-based permutation test). By contrast, the recognition model dominates in the late period, from ~500 ms to 2.5 s, reaching significance from 670 ms until the end of trial epoch. Interestingly, the attention model shows a peak at the transition between stimulus and recognition models, around 500 ms after stimulus onset, and reaches significance at 400–570 ms. We note that the waveforms of neural activity related to stimulus and recognition models qualitatively agree with the ‘stimulus-based’ and ‘recognition-based’ neural activity identified in the earlier intra-RDM analysis (<xref ref-type="fig" rid="fig4">Figure 4E</xref>), even though the methods employed by these two analyses are distinct.</p><p>To elucidate the brain regions contributing to each process, we conducted a model-driven MEG-fMRI data fusion analysis, using a commonality analysis approach (<xref ref-type="bibr" rid="bib44">Seibold and McPhee, 1979</xref>; <xref ref-type="bibr" rid="bib22">Hebart et al., 2018</xref>). Because neural activity related to the stimulus and recognition models overlapped in time (<xref ref-type="fig" rid="fig6">Figure 6D</xref>), we first performed an analysis to decompose the amount of shared variance between fMRI RDM from a given ROI and MEG RDM at a given time point that is uniquely explained by each model, while excluding the other model’s effect (see schematic in <xref ref-type="fig" rid="fig6">Figure 6C</xref>, left, and <xref ref-type="disp-formula" rid="equ7">eq. 5</xref> in Materials and methods). The results of this analysis are shown in <xref ref-type="fig" rid="fig6">Figure 6Ei-ii</xref>. The stimulus-related effects (<xref ref-type="fig" rid="fig6">Figure 6E–i</xref>) had the earliest onset in the right V1 and V2 at 80 ms, followed by progressive recruitment of areas along the visual hierarchy. Stimulus-related effects in most higher order frontoparietal regions reached significance much later (after 400 ms), with the exception of bilateral frontal cortices (right: 230 ms; left: 240 ms) and the PCC (240 ms), where significant effects occurred nearly simultaneously with the last visual areas. Interestingly, after 600 ms, stimulus-related effects exhibit sustained significance across category-selective visual areas (LOC and FG) and frontoparietal regions (FPN and DMN), and, at the same time, recurrent transient significance in early visual areas (V1-V4). This pattern may be driven by a continued cross-talk between higher-order and lower-order regions related to stimulus-content processing while the image is present.</p><p>By contrast, the recognition-related effects (<xref ref-type="fig" rid="fig6">Figure 6E–ii</xref>) exhibited broad spatiotemporal significance from ~650 ms until the end of trial epoch, covering all ROIs investigated but with earlier onset and more sustained significance in higher-order regions (LOC, FG, FPN and DMN) than early visual areas. Stimulus and recognition-related commonality time courses (i.e. shared MEG-fMRI variance explained by each model) are plotted in <xref ref-type="fig" rid="fig6">Figure 6F</xref> for five selected ROIs across the cortical hierarchy (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> for results from remaining ROIs).</p><p>Because neural activity related to the attention model did not overlap in time with the other two models (<xref ref-type="fig" rid="fig6">Figure 6D</xref>), we performed a second analysis to quantify the amount of shared variance between fMRI and MEG RDMs that is explained by the attention model (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, right, and <xref ref-type="disp-formula" rid="equ6">eq. 4</xref> in Materials and methods). Attention-related effects were only found in bilateral frontal and parietal cortices, which reached significance first in frontal cortices at 340 ms (<xref ref-type="fig" rid="fig6">Figure 6E–iii</xref>).</p><p>Together, these findings reveal the spatiotemporal evolution of neural activities underlying different information processing stages of prior-guided visual ambiguity resolution, including stimulus-feature, attentional-state, and recognition-content-related processing. Importantly, they show that stimulus-feature processing progresses from lower to higher order brain regions in an early time window, while recognition-related processing proceeds from higher to lower order regions in a later time period. Moreover, attention-related processing mediated by frontoparietal regions occurs at an intermediate time latency (~500 ms) and may facilitate the transition from stimulus processing to successful recognition.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Despite the pervasive need to resolve stimulus ambiguity (caused by occlusion, clutter, shading, and inherent complexities of natural objects) in natural vision (<xref ref-type="bibr" rid="bib37">Olshausen and Field, 2005</xref>) and the enormous power that prior knowledge acquired through past experiences wields in shaping perception (<xref ref-type="bibr" rid="bib24">Helmholtz, 1924</xref>; <xref ref-type="bibr" rid="bib2">Albright, 2012</xref>), the neural mechanisms underlying prior-guided visual recognition remain mysterious. Here, we exploited a dramatic visual phenomenon, where a single exposure to a clear, unambiguous image greatly facilitates recognition of a related degraded image, to shed light on dynamical neural mechanisms that allow past experiences to guide recognition of impoverished sensory input. Below we summarize our findings and discuss their implications.</p><p>Using whole-head MEG data, we first characterized the temporal evolution of neural activity patterns differentiating between perceptual states: pre- vs. post-disambiguation Mooney images, where identical stimulus input begets distinct perceptual outcomes depending on whether a perceptual prior is available. We observed that large-scale neural activity patterns (in the &lt;35 Hz range) recorded by MEG reliably distinguished perceptual states starting from 300 ms after stimulus onset, and that these activity patterns are sustained from 800 ms onward. Consistent with an earlier study (<xref ref-type="bibr" rid="bib4">Baria et al., 2017</xref>), slow cortical potentials (&lt;5 Hz) accounted for much of this effect. These multivariate findings complement previous univariate observations showing perceptual state-related changes in higher frequency power and connectivity patterns (<xref ref-type="bibr" rid="bib42">Rodriguez et al., 1999</xref>; <xref ref-type="bibr" rid="bib18">Grützner et al., 2010</xref>; <xref ref-type="bibr" rid="bib35">Minami et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Moratti et al., 2014</xref>). However, to understand how perceptual priors interact with stimulus input to resolve recognition, identifying perceptual state-related changes in neural activity is far from sufficient. Such changes may reflect disambiguation-induced changes in attention, salience or task difficulty that are unrelated to perceptual processing of individual image content. To elucidate how disambiguation influences dynamic neural representation of individual images, we calculated time-resolved RDMs, which quantify the dissimilarity (1 − r; <xref ref-type="fig" rid="fig4">Figure 4</xref>), distance (cross-validated Euclidean distance; <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>), and single-trial separability (r<sub>within</sub> − r<sub>between</sub>; <xref ref-type="fig" rid="fig5">Figure 5</xref>) between neural activity patterns elicited by every image pair (within and across perceptual conditions), and performed fine-grained analyses based on these RDMs.</p><p>We first observed that across-image dissimilarity for post-disambiguation Mooney images rises higher than for their pre-disambiguation counterparts starting from ~500 ms onward (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), and, surprisingly, closely parallels that for the grayscale images despite enormous differences in image features and the strength of bottom-up input between them.</p><p>To disentangle neural activities underlying different information processing stages involved in prior-guided visual perception, we compared the neural representational geometry (<xref ref-type="bibr" rid="bib31">Kriegeskorte and Kievit, 2013</xref>) between perceptual conditions (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). This analysis capitalizes on the intuition that neural activity encoding physical stimulus features should have identical representation for a Mooney image presented pre- and post-disambiguation. We identified the temporal evolution of neural dynamics fulfilling this criterion, which exhibits a sharp peak within ~300 ms following stimulus onset, and, interestingly, a second small peak that onsets around 800 ms and reaches significance at 1.3 s. Speculatively, this second peak may reflect top-down feedback related to filling in of perceptual details that follows the initial object recognition (<xref ref-type="bibr" rid="bib1">Ahissar and Hochstein, 2004</xref>; <xref ref-type="bibr" rid="bib7">Campana and Tallon-Baudry, 2013</xref>). Likewise, neural activity encoding the content of recognized objects (e.g. a crab, a motorcycle) should have similar representation between a post-disambiguation Mooney image and its matching grayscale image. The time course of such neural activity slowly increases following stimulus onset and exceeds stimulus-feature-related processing starting from ~500 ms.</p><p>We further investigated separability – at the single-trial level – between neural activity patterns elicited by the same image (or matching Mooney and grayscale images) presented in different perceptual conditions. The results revealed that within 500 ms after stimulus onset, neural activity patterns elicited by the same Mooney image pre- and post-disambiguation are substantially more similar to each other (i.e. lower separability) than to other images (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, green); additionally, the neural activity pattern elicited by a Mooney image is significantly separable from that elicited by its matching grayscale image (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, orange and dark blue). By contrast, after ~500 ms post-stimulus-onset, at the single-trial level, the neural activity pattern elicited by a post-disambiguation Mooney image is indistinguishable from that elicited by its matching grayscale image (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, dark blue), yet they are significantly separable from other grayscale images (<xref ref-type="fig" rid="fig5">Figure 5D</xref>, dark blue). In the same late time period, the neural activity pattern elicited by a pre-disambiguation Mooney image is well separable from that elicited by the same image presented after disambiguation or by its matching grayscale image (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, orange and green). These results provide clear evidence for a strong and specific shift of neural representation of each image toward the particular relevant prior experience, which gradually builds up after stimulus onset and is full-blown from ~500 ms onward.</p><p>Thus, multiple analyses probing neural representation format using dissimilarity, distance, and single-trial separability are in accordance with each other: they reveal content-specific neural processing of stimulus input in an early (&lt;300 ms) time period, which transitions into content-specific neural processing related to recognition outcome in a late (&gt;500 ms) time period. We note that the latency related to recognition observed in the current study is substantially later than previously reported onset times of object category and face identity-related information (<xref ref-type="bibr" rid="bib8">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">van de Nieuwenhuijzen et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Kaiser et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Vida et al., 2017</xref>). Yet, the late recognition-related neural dynamics observed herein are consistent with human subjects’ reaction times reporting recognition status for disambiguated Mooney images at around 1.2 s (<xref ref-type="bibr" rid="bib23">Hegdé and Kersten, 2010</xref>). Several aspects likely contribute to this difference: first, bottom-up stimulus information is ambiguous and much weaker for Mooney images than images showing clear and isolated objects typically used in object categorization tasks; second, our analyses probe neural activity related to recognizing individual image content rather than object category, and processing of face identity likely benefits from a specialized circuitry; third, neural activity differentiating between object categories may also reflect early processing of low-level image features that differ between categories (<xref ref-type="bibr" rid="bib12">Coggan et al., 2016</xref>). Importantly, the present finding of slow neural dynamics underlying recognition-related processing is consistent with our hypothesis that recruitment of perceptual templates encoded in higher order frontoparietal areas and long-distance recurrent activity are needed for prior-experience-guided visual ambiguity resolution. This hypothesis receives further support from the model-driven MEG-fMRI fusion analysis, which we discuss below.</p><p>We capitalized on the ability of RSA to project high-dimensional neural data from different modalities into a common representational space (<xref ref-type="bibr" rid="bib29">Kriegeskorte et al., 2008a</xref>) to combine the high temporal resolution of MEG data with high spatial resolution of a separate 7T fMRI data set. In addition, we adopted a recently developed model-driven data fusion approach building on RSA (<xref ref-type="bibr" rid="bib22">Hebart et al., 2018</xref>) to spatiotemporally resolve neural activity underlying different information processing stages. To this end, we constructed three model RDMs that capture the representation format of idealized processes related to stimulus feature, recognition content, and attentional state processing (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Although these models capture relatively coarse features in the data, the time courses of MEG activity related to each model (<xref ref-type="fig" rid="fig6">Figure 6D</xref>) are consistent with the earlier fine-grained analysis applied to MEG RDMs, including the dominance of stimulus and recognition processing in the early (&lt;500 ms) and late (&gt;500 ms) time period, respectively, and a late second peak in stimulus-related processing. Interestingly, neural activity related to the attention model, which captures dissimilarity driven by recognition status (recognized vs. non-recognized), shows a single peak around 500 ms, suggesting that a transient salience signal may facilitate recognition processing of individual image content.</p><p>The model-driven MEG-fMRI data fusion analysis dissects the shared variance between MEG RDM at a given time point and fMRI RDM from a given brain region that is uniquely accounted for by each model. Consistent with our earlier fMRI findings (<xref ref-type="bibr" rid="bib16">González-García et al., 2018</xref>), recognition-related neural activity is widely distributed across the cortical hierarchy – from early visual areas to frontoparietal and default-mode networks (<xref ref-type="fig" rid="fig6">Figure 6E–ii</xref>). Importantly, this analysis reveals that recognition-related activity reaches significance at an earlier time in higher-order regions (LOC, FG, FPN and DMN, 670–680 ms) than in lower-order regions (V1-V3, 760 ms or later). This sequence is consistent with our hypothesis that higher-order brain regions are crucial for encoding perceptual priors and initiating prior-guided recognition.</p><p>We note that bilateral frontal cortices of the FPN are the only regions showing both attention-related activity and early stimulus-related activity that reaches significance at 230–240 ms – immediately following fusiform gyri at 220–230 ms. This result resonates with our previous fMRI observation that following disambiguation, frontal areas of the FPN move up the cortical hierarchy as defined by the neural representation format (<xref ref-type="bibr" rid="bib16">González-García et al., 2018</xref>). Together, these findings support the idea that frontal areas may play a special role in utilizing internal priors to guide perceptual processing (<xref ref-type="bibr" rid="bib3">Bar et al., 2006</xref>; <xref ref-type="bibr" rid="bib46">Summerfield et al., 2006</xref>; <xref ref-type="bibr" rid="bib53">Wang et al., 2013</xref>).</p><p>In conclusion, the present results reveal, for the first time, how neural activities underlying different information processing stages during prior-guided visual perception dynamically unfold across space and time. These findings significantly further our understanding of the neural mechanisms that endow previous experiences with enormous power to shape our daily perception. In line with theories positing aberrant interactions between internal priors and sensory input in psychiatric illnesses (<xref ref-type="bibr" rid="bib14">Friston et al., 2014</xref>), behavioral and neural abnormalities associated with Mooney image disambiguation have been reported in patients with schizophrenia and autism (<xref ref-type="bibr" rid="bib47">Sun et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Rivolta et al., 2014</xref>; <xref ref-type="bibr" rid="bib48">Teufel et al., 2015</xref>). Thus, our findings may also inform studies on the pathophysiological processes involved in these debilitating illnesses.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>The experiment was approved by the Institutional Review Board of the National Institute of Neurological Disorders and Stroke (under protocol #14 N-0002). All subjects were right-handed and neurologically healthy with normal or corrected-to-normal vision. Eighteen subjects between 21 and 33 years of age (mean age 26.2; nine females) participated in the MEG experiment. Nineteen additional subjects (age range = 19–32; mean age = 24.6; 11 females) participated in a 7T fMRI experiment, using a similar task paradigm as in the MEG experiment (<xref ref-type="bibr" rid="bib16">González-García et al., 2018</xref>). The two subject groups did not have any overlap since subjects needed to be naïve to the Mooney images used in this experiment. All subjects provided written informed consent.</p></sec><sec id="s4-2"><title>Stimuli</title><p>Mooney and grayscale images were generated from grayscale photographs of real-world objects and animals selected from the Caltech (<ext-link ext-link-type="uri" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html">http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html</ext-link>) and Pascal VOC (<ext-link ext-link-type="uri" xlink:href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html</ext-link>) databases. Grayscale images were constructed by cropping photographs of objects and animals in a natural setting to 500 × 500 pixels and applying a box filter. Mooney images were constructed by thresholding the grayscale images. Threshold level and filter size were initially set at the median intensity of each image and 10 × 10 pixels, respectively. Each parameter was titrated so that the Mooney image was difficult to recognize without first seeing the corresponding grayscale image. From an original set of 252 images, thirty-nine (19 were inanimate objects, and 20 were animals – unbeknownst to the subjects) were chosen to be used in this experiment via an initial screening procedure conducted by six additional participants recruited separately from the main experiment [for details, see (<xref ref-type="bibr" rid="bib9">Chang et al., 2016</xref>). Stimuli were presented using E-Prime Software (Psychology Software Tools, Sharpsburg, PA) via a Panasonic PT-D3500U projector with a ET-DLE400 lens. All images subtended 11.9 × 11.9 degrees of visual angle.</p></sec><sec id="s4-3"><title>Task paradigm</title><p>Each trial began with a 1 s fixation period during which subjects fixated on a red cross in the center of the screen (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Thereafter, a Mooney image or a grayscale image was presented for 2 s. The red cross was present during image presentation, and subjects were instructed to keep their gaze fixated whenever it was onscreen. After image presentation, there was another fixation period, the duration of which was uniformly distributed between 1 and 2 s. This was followed by a response prompt of ‘Can you recognize the object hidden in the image?’ to assess subjective recognition of each image presentation. Below this prompt, the answer choices, ‘Yes’ and ‘No’ were presented on each side of the screen with their positions randomly varied across trials. Subjects were instructed to answer the question by pressing one of two buttons using their right thumb, with each button corresponding to one side of the screen. The response prompt terminated when a response was given, and each trial ended with a blank screen of jittered duration uniformly distributed between 1.5 and 2.5 s.</p><p>Trials were organized into blocks, using a structure similar to previous studies (<xref ref-type="bibr" rid="bib17">Gorlin et al., 2012</xref>; <xref ref-type="bibr" rid="bib9">Chang et al., 2016</xref>). Each block consisted of 15 trials: three different grayscale images followed by six Mooney images, then a repeat of the same six Mooney images (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Three of the Mooney images corresponded to the preceding grayscale images (‘post-disambiguation’) and the other three were novel (‘pre-disambiguation’). The presentation order of these six Mooney images in each repeat was randomized. The block was repeated twice with shuffled grayscale image sequences and shuffled Mooney image sequences, followed by a verbal test session. This constituted one experimental run. Grayscale images corresponding to pre-disambiguation Mooney images were presented in the subsequent run. In total, each participant completed 14 runs. For all Mooney images to be presented both pre- and post- disambiguation, the first and last runs were half runs. The first run included only three pre-disambiguation Mooney images. The final run included three post-disambiguation Mooney images and their grayscale counterparts. All other full runs consisted of three grayscale images, three post-disambiguation Mooney images, and three novel pre-disambiguation Mooney images. In all, each unique grayscale image was presented three times for each subject, and each unique Mooney image was presented six times before and six times after disambiguation. The full experiment lasted approximately 2 hr.</p><p>The verbal test was included to verify that subjects’ recognition of Mooney images was the correct interpretation. It consisted of presenting the six different Mooney images from the preceding run for 2 s each on the screen and participants were asked to verbally respond what they saw in the image. They were allowed to answer ‘I don’t know’. Verbal responses were scored as correct or incorrect using a pre-determined list of acceptable answers for each image. Subjects were verbally tested on each Mooney image once before disambiguation and once after disambiguation. No MEG signal was recorded during the verbal test.</p><p>Of the 39 unique Mooney images used for the main experiment, 33 had their corresponding grayscale images presented (‘real image sets’), while the remaining six were presented with non-matching grayscale images (‘catch image sets’) as controls. The same set of real and catch images were used for all subjects. Details on statistical analyses can be found in the following sections and in the Results.</p></sec><sec id="s4-4"><title>MEG data acquisition</title><p>While performing the task, subjects’ brain activity was recorded with a 275-channel whole-head MEG system (CTF), and their gaze position and pupil size were recorded using a SR Research Eyelink 1000+ system. Eye-tracking was used for online monitoring of fixation during the experiment. Three dysfunctional MEG sensors were removed from all analyses. MEG data were collected with a 600 Hz sampling rate and an anti-aliasing filter of 150 Hz. Before and after each run, the head position of a subject was measured using fiducial coils. Subjects responded to subjective recognition questions using a fibreoptic response button box. All MEG data samples were corrected with respect to the presentation delay of the projector (measured with a photodiode).</p></sec><sec id="s4-5"><title>MEG data preprocessing</title><p>The FieldTrip package implemented in MATLAB (<xref ref-type="bibr" rid="bib38">Oostenveld et al., 2011</xref>) was used for preprocessing in conjunction with custom-written code in MATLAB (Mathworks, Natick, MA). MEG data were demeaned, detrended, and filtered in two different frequency bands (using 3<sup>rd</sup>-order Butterworth filters) for further analyses: slow cortical potentials (SCPs, 0.05 – 5 Hz; down-sampled to 10 Hz), and classic event-related field (ERF) frequency range (DC – 35 Hz; down-sampled to 100 Hz). Independent component analysis (ICA) was applied to continuous data from each run, and components corresponding to eye blinks, eye movements, and heartbeat-related artifacts were removed. Data were then epoched into 3.5 s trials consisting of a 0.5 s pre-stimulus period and a 3 s post-stimulus period (including 2 s image presentation and the first sec of jitter-fixate period; see <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Baseline correction was applied for each sensor using the pre-stimulus time window.</p></sec><sec id="s4-6"><title>Multivariate pattern analysis (MVPA)</title><p>MVPA was carried out using both the 0.05 – 5 Hz data and the DC – 35 Hz data. First, MEG activity at each time sample was normalized across sensors (<xref ref-type="bibr" rid="bib39">Pereira et al., 2009</xref>). For each subject, classification of perceptual state (pre- vs. post-disambiguation) was performed using activity from all MEG sensors averaged across six presentations for each image in each perceptual state, using all 33 non-catch image sets. We implemented a linear support vector machine (SVM) classifier (cost = 1) using the LIBSVM package (<xref ref-type="bibr" rid="bib10">Chang and Lin, 2011</xref>) at each time point in the trial epoch. An odd-even cross-validation scheme was used, classification accuracy was averaged across the two folds and reported as balanced accuracy (<xref ref-type="bibr" rid="bib6">Brodersen et al., 2013</xref>). Activation patterns corresponding to the MEG activity contributing to the classifier were computed for each subject and time point by multiplying the vector of SVM decoder weights with the covariance matrix of the data set used to train a given classifier (<xref ref-type="bibr" rid="bib21">Haufe et al., 2014</xref>). For display purposes, activation patterns were averaged across subjects. To test the cross-time generalization of classifiers, a classifier trained at a given time point was tested at all time points in the trial epoch, yielding a temporal generalization matrix (TGM)(<xref ref-type="bibr" rid="bib45">Stokes et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">King and Dehaene, 2014</xref>). If a classifier can generalize across time points, this demonstrates that the decoded information format is similar across these time points. If it does not generalize, this indicates that the information is represented differently or not at all. A control analysis was carried out in a similar manner but using only the six catch image sets, for which pre- and post- states were defined as before and after the presentation of the artificially-assigned, non-matching grayscale images. An additional analysis (disambiguation decoding) used only non-catch image sets where Mooney image were unrecognized/non identified in the pre-disambiguation stage and recognized/identified in the post-disambiguation stage. These image sets were selected for each individual subject based on the behavioral responses.</p></sec><sec id="s4-7"><title>Cluster-based permutation tests for multivariate pattern decoding</title><p>The group-level statistical significance of classifier accuracy at each time point was assessed by a one-tailed, one-sample Wilcoxon signed rank test against chance level (50%). To correct for multiple comparisons, we used cluster-based permutation tests (<xref ref-type="bibr" rid="bib34">Maris and Oostenveld, 2007</xref>). Temporal clusters were defined as contiguous time points yielding significantly above-chance classification accuracy (p&lt;0.05). The test statistic W of the Wilcoxon signed rank test was summed across time points in a cluster to yield a cluster’s summary statistic. Cluster summary statistics were compared to a null distribution, constructed by shuffling class labels 500 times, and extracting the largest cluster summary statistic for each permutation. Clusters in original data with summary statistics exceeding the 95<sup>th</sup> percentile of null distribution were considered significant (corresponding to p&lt;0.05, cluster-corrected). For classifier temporal generalization, the permutation-based approach for cluster-level statistical inference used the same procedure as above, where clusters were defined as contiguous time points in both training and generalization dimensions with significant (p&lt;0.05) above-chance classification accuracy.</p></sec><sec id="s4-8"><title>Representational similarity analysis (RSA)</title><p>For this and following analyses, to achieve higher temporal resolution, we used data filtered in the DC – 35 Hz range (down-sampled to 100 Hz). Similar to the preprocessing for MVPA analysis, MEG data were normalized across sensors at each time point. For each subject, data were averaged across the three presentations for each grayscale image. In order to compare between Mooney images and grayscale images, for each Mooney image the first three presentations were averaged together in the pre- and post-disambiguation stage, respectively. At each time point in the trial epoch, we computed representational distance, calculated as 1 – Pearson’s r (computed across all sensors), between every image pair in all presentation stages: pre-disambiguation Mooney, post-disambiguation Mooney, and grayscale (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This generated a 99 × 99 representational dissimilarity matrix (RDM) at each time point (with 10 ms steps). Catch image sets were excluded from this analysis.</p><p>Using the time-resolved RDM, we averaged across all image-pairs within each presentation condition (i.e. off-diagonal elements within the Pre-Pre, Post-Post, and Gray-Gray squares of the RDM, see <xref ref-type="fig" rid="fig4">Figure 4C–i</xref>) to yield a mean dissimilarity time course for each condition. Dissimilarity time courses were compared between conditions using a Wilcoxon signed-rank test across subjects and corrected for multiple comparisons using a cluster-based permutation test similar to that described above, with 5000 shuffles of class labels for each subject.</p></sec><sec id="s4-9"><title>Euclidean distance</title><p>To ensure that the results were unaffected by changing levels of noise between perceptual conditions, the above analysis was repeated using RDMs constructed with Euclidean distance and cross-validated Euclidean distance. Let <italic>x</italic> and <italic>y</italic> represent neural activity vectors elicited by two different images (or the same image presented in two different conditions), (non-cross-validated) Euclidean distance is calculated as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>and cross-validated Euclidean distance is calculated as<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mo>.</mml:mo><mml:mi>v</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>A</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where A and B denote the two partitions of the data within each cross-validation fold (<xref ref-type="bibr" rid="bib19">Guggenmos et al., 2018</xref>). We used a three-fold cross-validation scheme to calculate cross-validated Euclidean distance. With cross-validation, the contribution by noise in the data cancels out, and the result is only driven by the component in the data that is consistent across partitions.</p></sec><sec id="s4-10"><title>Intra-RDM analysis</title><p>To probe fine-grained information available in the RDMs, we performed an intra-RDM analysis (for detailed rationale, see Results). This analysis contained two components: First, to assess neural activity related to processing the physical features of individual images, we calculated Pearson’s correlation between the Fisher-z transformed values of Pre-Pre and Post-Post squares of the MEG representational similarity matrices (RSMs, equal to 1-RDM) at each time point, using the upper triangle of each (<xref ref-type="fig" rid="fig4">Figure 4C–ii</xref>, ‘Stimulus-based’). Second, to assess neural activity related to the recognition outcomes of individual images, we calculated Pearson’s correlation between the Fisher-z transformed values of Post-Post and Gray-Gray squares of the MEG RSM, again using the upper triangle of each (<xref ref-type="fig" rid="fig4">Figure 4C–ii</xref>, ‘Recognition-based’). The correlation values were Fisher-z transformed, and group-level statistics were assessed by one-sample t-tests against zero followed by cluster-based permutation tests with 5000 permutations.</p></sec><sec id="s4-11"><title>Single-trial separability</title><p>To assess how well neural activities elicited by Image A and Image B (importantly, these can represent two different images presented in the same or different conditions, or the same/matching image presented in different conditions) can be separated at the single-trial level, we computed a separability measure as follows.</p><p>Let <italic>x<sub>i</sub></italic> and <italic>y<sub>i</sub></italic> be the activity vectors across all sensors on the <italic>i</italic>-th presentation of Image A and Image B, respectively. And suppose that Image A and Image B are presented for a total of <italic>m</italic> and <italic>n</italic> trials, respectively. (Each Mooney image is presented six times before and six times after disambiguation, and each grayscale image is presented three times total.) We calculated the following measures:<disp-formula id="equ3"><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ4"><mml:math id="m4"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo> <mml:mi/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>r<sub>z</sub></italic> denotes Fisher-z-transform of Pearson’s correlation r-value. Separability is calculated as<disp-formula id="equ5"><label>(3)</label><mml:math id="m5"><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Thus, separability quantifies how much neural similarity across multiple presentations of the same image exceeds neural similarity between the two different images and is akin to single-trial decoding.</p><p>We constructed the time-resolved 99 × 99 RDMs using the separability measure, which were subjected to two further quantitative analyses. In the first analysis, we extracted the mean of diagonal elements in the between-condition squares, which yielded three time-dependent outputs (<xref ref-type="fig" rid="fig5">Figure 5C–i</xref>). In the second analysis, we computed the difference between the mean off-diagonal value and the mean diagonal value within each between-condition square, which again yielded three time-dependent outputs (<xref ref-type="fig" rid="fig5">Figure 5D–i</xref>). For each analysis, we evaluated the statistical significance of each output against chance level using a one-sample t-test against 0, and the statistical significance of pairwise comparisons between the outputs using Wilcoxon sign-rank tests; all statistical tests were corrected for multiple comparisons using cluster-based permutation tests with 5000 permutations.</p></sec><sec id="s4-12"><title>7T fMRI data collection and regions of interest (ROI) definition</title><p>We carried out an fMRI study using a similar paradigm, which included the same 33 Mooney images and their grayscale counterparts that made up the non-catch image sets in the MEG experiment. Run and block structure were identical to the MEG experiment. In the fMRI experiment, each trial included a 2 s fixation period, a 4 s image presentation, a 2 s blank period, and a 2 s response period to assess subjective recognition (similar question as in the MEG experiment). Detailed methods and results related to the fMRI study are reported separately (<xref ref-type="bibr" rid="bib16">González-García et al., 2018</xref>); here we describe data collection and ROI definition procedures relevant to the current study.</p><p>fMRI data were collected on a Siemens 7T scanner equipped with a 32-channel head coil (Nova Medical). T1-weighted anatomical images were obtained using an MP-RAGE sequence (sagittal orientation, 1 × 1 × 1 mm resolution). Additionally, a proton-density (PD) sequence was used to obtain PD-weighted images also with 1 × 1 × 1 mm resolution, to help correct for field inhomogeneity in the MP-RAGE images (<xref ref-type="bibr" rid="bib49">Van de Moortele et al., 2009</xref>). Functional images were obtained using a single-shot echo planar imaging (EPI) sequence (TR = 2000 ms, TE = 25 ms, flip angle = 50°, 52 oblique slices, slice thickness = 2 mm, spacing = 0 mm, in-plane resolution = 1.8 × 1.8 mm, FOV = 192 mm, acceleration factor/GRAPPA = 3). The functional data were later resampled to 2 mm isotropic voxels. Respiration and cardiac data were collected using a breathing belt and a pulse oximeter, respectively, simultaneously with fMRI data acquisition; and physiological noise were removed during preprocessing of fMRI data using the RETROICOR method (<xref ref-type="bibr" rid="bib15">Glover et al., 2000</xref>). Anatomical and functional data preprocessing followed standard procedures and are described in detail in <xref ref-type="bibr" rid="bib16">González-García et al. (2018)</xref>.</p><p>ROIs were defined as follows. A separate retinotopic localizer and a lateral occipital complex (LOC) functional localizer were performed for each subject to define bilateral early visual ROIs (V1, V2, V3, and V4) and LOC, respectively. Fusiform gyrus (FG) ROIs were extracted using the Harvard-Oxford Cortical Structural Atlas (<ext-link ext-link-type="uri" xlink:href="https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Atlases">https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Atlases</ext-link>). Default mode network (DMN) regions, including bilateral lateral parietal cortices (LatPar), medial prefrontal cortex (mPFC), and posterior cingulate cortex (PCC), were defined using a general linear model (GLM) of the disambiguation contrast (pre-disambiguation-not-recognized vs. post-disambiguation-recognized). Lastly, statistical map from searchlight MVPA decoding of the disambiguation contrast was used to define the frontoparietal network (FPN) ROIs: bilateral frontal and parietal cortices. The localization of all ROIs are shown in <xref ref-type="fig" rid="fig6">Figure 6B</xref>; for further details see (<xref ref-type="bibr" rid="bib16">González-García et al., 2018</xref>). Critically, none of the analyses used to define the ROIs depended on the RDMs used for MEG-fMRI data fusion analysis described below; in addition, results obtained using FPN and DMN ROIs defined based on an independent resting-state study (<xref ref-type="bibr" rid="bib40">Power et al., 2011</xref>) were similar (not included due to length consideration).</p><p>For each of the 20 ROIs, a 99 × 99 RDM was constructed using the activation patterns corresponding to each image in each condition, using the same image order as for the MEG RDM. These activation patterns were derived from a GLM and averaged across image presentations (three presentations for grayscale images; only the first three presentations were used for Mooney images in each stage in order to equalize statistical power across conditions). Representational distance between every image pair was computed as 1 – Pearson’s r (computed across all voxels within an ROI), similarly as for MEG RDM.</p></sec><sec id="s4-13"><title>Model-based representational similarity analysis (RSA) of MEG data</title><p>RSA enables neural representation format to be compared with theoretical models of information representation (<xref ref-type="bibr" rid="bib29">Kriegeskorte et al., 2008a</xref>). Here, we used <italic>a priori</italic> defined models to probe how neural representation of different types of information dynamically evolved over time. Three models were defined as RDMs (same dimensions as the MEG RDMs) where high dissimilarity was expressed as 1 (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, <italic>red</italic>) and low dissimilarity was expressed as a 0 (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, <italic>blue</italic>), with intermediate dissimilarity expressed as 0.5 (<xref ref-type="fig" rid="fig6">Figure 6A</xref>, white). Black diagonals in these model RDMs (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) denote elements excluded from the model’s analysis (defined as NaN’s).</p><p>For each model and time point, Spearman correlation was computed between the upper triangles of model RDM and group-averaged MEG RDM to assess how well the model explained the MEG RDM (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Statistical significance was established using a cluster-based permutation test. The null distribution was calculated using 1000 permutations of the MEG RDM, where for each permutation the image order was shuffled, but with the same shuffled order along the x- and y- axis of the RDM and across all time points. Clusters were defined as contiguous time points where the Spearman rho value was greater than the 95<sup>th</sup> percentile of the null distribution. To identify significant clusters, we determined the 95<sup>th</sup> percentile of maximum cluster size across all permutations, and clusters in the original data that exceeded this cut-off were deemed significant (equivalent to p&lt;0.05, one-sided).</p></sec><sec id="s4-14"><title>Model-based MEG-fMRI data fusion</title><p>Following previous studies (<xref ref-type="bibr" rid="bib30">Kriegeskorte et al., 2008b</xref>; <xref ref-type="bibr" rid="bib11">Cichy et al., 2014</xref>), we employed cross-modal RSA to combine fMRI and MEG data from independent participant groups (N = 19 and 18, respectively). Furthermore, we applied a recently developed approach based on commonality analysis (<xref ref-type="bibr" rid="bib44">Seibold and McPhee, 1979</xref>; <xref ref-type="bibr" rid="bib22">Hebart et al., 2018</xref>) to use the theoretical models described in the previous section to guide the cross-modal MEG-fMRI data fusion.</p><p>Specifically, commonality analysis was employed in two ways. First, it was used to determine the shared variance between a model (M1 in <xref ref-type="disp-formula" rid="equ6">eq. 4</xref>), the fMRI RDM from a given ROI and the MEG RDM at a given time point (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, <italic>right</italic>), calculated as:<disp-formula id="equ6"><label>(4)</label><mml:math id="m6"><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula></p><p>Second, it was used to determine the shared variance between a model, the fMRI RDM and the MEG RDM which is unique to that model, and is not shared by a second model (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, <italic>left</italic>). This allows the dissociation of the respective contributions of each model to the shared variance between fMRI and MEG RDMs. The commonality (i.e. shared variance between model, fMRI and MEG RDM) for a model of interest (M1 in <xref ref-type="disp-formula" rid="equ7">eq. 5</xref>) that is not explained by a second model (M2) is calculated as follows:<disp-formula id="equ7"><label>(5)</label><mml:math id="m7"><mml:mi>C</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo> <mml:mi/><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>M</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo>.</mml:mo><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula></p><p>(RDM elements that contain NaN values in any model are excluded from the analysis.)</p><p>Significance for these commonalities was determined using cluster-based permutation tests, following the same method as explained above for model-based MEG RSA. Further, significant clusters in <xref ref-type="fig" rid="fig6">Figure 6E</xref> were masked by only including spatiotemporal locations where both MEG-Model RDM correlation and fMRI-Model RDM correlation are significant. Lastly, we assessed the correspondence between group-averaged MEG and fMRI RDMs using Spearman correlation, resulting in a cross-modal RDM similarity time course for each ROI (<xref ref-type="fig" rid="fig6">Figure 6F</xref>, gray shading). Specifically, the squared Spearman rho was calculated and compared with the model-based commonality measures derived from <xref ref-type="disp-formula" rid="equ6">eq. 4</xref> or <xref ref-type="disp-formula" rid="equ7">eq. 5</xref>. The squared Spearman rho provides the upper bound for the maximal amount of shared variance between MEG and fMRI RDMs to be explained by the theoretical models (<xref ref-type="bibr" rid="bib22">Hebart et al., 2018</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was supported by the Intramural Research Program of the National Institutes of Health/National Institute of Neurological Disorders and Stroke, and National Science Foundation (BCS-1753218, to BJH). BJH further acknowledges support by Klingenstein-Simons Neuroscience Fellowship. CGG was supported by the Department of State Fulbright program. We thank Brian Maniscalco and Tom Holroyd for helpful discussions on code implementation and data acquisition, respectively.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Funding acquisition, Investigation, Visualization</p></fn><fn fn-type="con" id="con3"><p>Software, Formal analysis, Validation, Visualization, Methodology, Writing—original draft</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Validation, Methodology, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: The experiment was approved by the Institutional Review Board of the National Institute of Neurological Disorders and Stroke (under protocol #14-N-0002). All subjects provided written informed consent.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><object-id pub-id-type="doi">10.7554/eLife.41861.011</object-id><label>Source code 1.</label><caption><title>Source data and code for <xref ref-type="fig" rid="fig4">Figures 4</xref>, <xref ref-type="fig" rid="fig5">5</xref> and <xref ref-type="fig" rid="fig6">6</xref>.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-41861-code1-v2.zip"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.41861.012</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-41861-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data generated or analysed during this study are included in the manuscript and supporting files.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahissar</surname> <given-names>M</given-names></name><name><surname>Hochstein</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The reverse hierarchy theory of visual perceptual learning</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>457</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.08.011</pub-id><pub-id pub-id-type="pmid">15450510</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albright</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>On the perception of probable things: neural substrates of associative memory, imagery, and perception</article-title><source>Neuron</source><volume>74</volume><fpage>227</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.001</pub-id><pub-id pub-id-type="pmid">22542178</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname> <given-names>M</given-names></name><name><surname>Kassam</surname> <given-names>KS</given-names></name><name><surname>Ghuman</surname> <given-names>AS</given-names></name><name><surname>Boshyan</surname> <given-names>J</given-names></name><name><surname>Schmid</surname> <given-names>AM</given-names></name><name><surname>Schmidt</surname> <given-names>AM</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Hämäläinen</surname> <given-names>MS</given-names></name><name><surname>Marinkovic</surname> <given-names>K</given-names></name><name><surname>Schacter</surname> <given-names>DL</given-names></name><name><surname>Rosen</surname> <given-names>BR</given-names></name><name><surname>Halgren</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Top-down facilitation of visual recognition</article-title><source>PNAS</source><volume>103</volume><fpage>449</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1073/pnas.0507062103</pub-id><pub-id pub-id-type="pmid">16407167</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baria</surname> <given-names>AT</given-names></name><name><surname>Maniscalco</surname> <given-names>B</given-names></name><name><surname>He</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Initial-state-dependent, robust, transient neural dynamics encode conscious visual perception</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005806</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005806</pub-id><pub-id pub-id-type="pmid">29176808</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandman</surname> <given-names>T</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Interaction between scene and object processing revealed by human fMRI and MEG decoding</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>7700</fpage><lpage>7710</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0582-17.2017</pub-id><pub-id pub-id-type="pmid">28687603</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodersen</surname> <given-names>KH</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Mathys</surname> <given-names>C</given-names></name><name><surname>Chumbley</surname> <given-names>JR</given-names></name><name><surname>Buhmann</surname> <given-names>JM</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Variational bayesian mixed-effects inference for classification studies</article-title><source>NeuroImage</source><volume>76</volume><fpage>345</fpage><lpage>361</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.008</pub-id><pub-id pub-id-type="pmid">23507390</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campana</surname> <given-names>F</given-names></name><name><surname>Tallon-Baudry</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Anchoring visual subjective experience in a neural model: the coarse vividness hypothesis</article-title><source>Neuropsychologia</source><volume>51</volume><fpage>1050</fpage><lpage>1060</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.02.021</pub-id><pub-id pub-id-type="pmid">23499720</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname> <given-names>T</given-names></name><name><surname>Tovar</surname> <given-names>DA</given-names></name><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational dynamics of object vision: the first 1000 ms</article-title><source>Journal of Vision</source><volume>13</volume><fpage>1</fpage><pub-id pub-id-type="doi">10.1167/13.10.1</pub-id><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>R</given-names></name><name><surname>Baria</surname> <given-names>AT</given-names></name><name><surname>Flounders</surname> <given-names>MW</given-names></name><name><surname>He</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Unconsciously elicited perceptual prior</article-title><source>Neuroscience of Consciousness</source><volume>2016</volume><elocation-id>niw008</elocation-id><pub-id pub-id-type="doi">10.1093/nc/niw008</pub-id><pub-id pub-id-type="pmid">27595010</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>C-C</given-names></name><name><surname>Lin</surname> <given-names>C-J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>LIBSVM: a library for support vector machines</article-title><source>ACM Transactions on Intelligent Systems and Technology</source><volume>2</volume><fpage>1</fpage><lpage>27</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname> <given-names>RM</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id><pub-id pub-id-type="pmid">24464044</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coggan</surname> <given-names>DD</given-names></name><name><surname>Baker</surname> <given-names>DH</given-names></name><name><surname>Andrews</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The role of visual and semantic properties in the emergence of Category-Specific patterns of neural response in the human brain</article-title><source>eNeuro</source><volume>3</volume><pub-id pub-id-type="doi">10.1523/ENEURO.0158-16.2016</pub-id><pub-id pub-id-type="pmid">27517086</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Fink</surname> <given-names>GR</given-names></name><name><surname>Rolls</surname> <given-names>E</given-names></name><name><surname>Booth</surname> <given-names>M</given-names></name><name><surname>Holmes</surname> <given-names>A</given-names></name><name><surname>Frackowiak</surname> <given-names>RS</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>How the brain learns to see objects and faces in an impoverished context</article-title><source>Nature</source><volume>389</volume><fpage>596</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1038/39309</pub-id><pub-id pub-id-type="pmid">9335498</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name><name><surname>Montague</surname> <given-names>R</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Computational psychiatry: the brain as a phantastic organ</article-title><source>The Lancet Psychiatry</source><volume>1</volume><fpage>148</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1016/S2215-0366(14)70275-5</pub-id><pub-id pub-id-type="pmid">26360579</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glover</surname> <given-names>GH</given-names></name><name><surname>Li</surname> <given-names>TQ</given-names></name><name><surname>Ress</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Image-based method for retrospective correction of physiological motion effects in fMRI: retroicor</article-title><source>Magnetic Resonance in Medicine</source><volume>44</volume><fpage>162</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1002/1522-2594(200007)44:1&lt;162::AID-MRM23&gt;3.0.CO;2-E</pub-id><pub-id pub-id-type="pmid">10893535</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>González-García</surname> <given-names>C</given-names></name><name><surname>Flounders</surname> <given-names>MW</given-names></name><name><surname>Chang</surname> <given-names>R</given-names></name><name><surname>Baria</surname> <given-names>AT</given-names></name><name><surname>He</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Content-specific activity in Frontoparietal and default-mode networks during prior-guided visual perception</article-title><source>eLife</source><volume>7</volume><elocation-id>e36068</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36068</pub-id><pub-id pub-id-type="pmid">30063006</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorlin</surname> <given-names>S</given-names></name><name><surname>Meng</surname> <given-names>M</given-names></name><name><surname>Sharma</surname> <given-names>J</given-names></name><name><surname>Sugihara</surname> <given-names>H</given-names></name><name><surname>Sur</surname> <given-names>M</given-names></name><name><surname>Sinha</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imaging prior information in the brain</article-title><source>PNAS</source><volume>109</volume><fpage>7935</fpage><lpage>7940</lpage><pub-id pub-id-type="doi">10.1073/pnas.1111224109</pub-id><pub-id pub-id-type="pmid">22538820</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grützner</surname> <given-names>C</given-names></name><name><surname>Uhlhaas</surname> <given-names>PJ</given-names></name><name><surname>Genc</surname> <given-names>E</given-names></name><name><surname>Kohler</surname> <given-names>A</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Wibral</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neuroelectromagnetic correlates of perceptual closure processes</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>8342</fpage><lpage>8352</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5434-09.2010</pub-id><pub-id pub-id-type="pmid">20554885</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggenmos</surname> <given-names>M</given-names></name><name><surname>Sterzer</surname> <given-names>P</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multivariate pattern analysis for MEG: a comparison of dissimilarity measures</article-title><source>NeuroImage</source><volume>173</volume><fpage>434</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.02.044</pub-id><pub-id pub-id-type="pmid">29499313</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harel</surname> <given-names>A</given-names></name><name><surname>Kravitz</surname> <given-names>DJ</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Task context impacts visual object processing differentially across the cortex</article-title><source>PNAS</source><volume>111</volume><fpage>E962</fpage><lpage>E971</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312567111</pub-id><pub-id pub-id-type="pmid">24567402</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname> <given-names>S</given-names></name><name><surname>Meinecke</surname> <given-names>F</given-names></name><name><surname>Görgen</surname> <given-names>K</given-names></name><name><surname>Dähne</surname> <given-names>S</given-names></name><name><surname>Haynes</surname> <given-names>JD</given-names></name><name><surname>Blankertz</surname> <given-names>B</given-names></name><name><surname>Bießmann</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title><source>NeuroImage</source><volume>87</volume><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.067</pub-id><pub-id pub-id-type="pmid">24239590</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname> <given-names>MN</given-names></name><name><surname>Bankson</surname> <given-names>BB</given-names></name><name><surname>Harel</surname> <given-names>A</given-names></name><name><surname>Baker</surname> <given-names>CI</given-names></name><name><surname>Cichy</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The representational dynamics of task and object processing in humans</article-title><source>eLife</source><volume>7</volume><elocation-id>e32816</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id><pub-id pub-id-type="pmid">29384473</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hegdé</surname> <given-names>J</given-names></name><name><surname>Kersten</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A link between visual disambiguation and visual memory</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>15124</fpage><lpage>15133</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4415-09.2010</pub-id><pub-id pub-id-type="pmid">21068318</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Helmholtz</surname> <given-names>HV</given-names></name></person-group><year iso-8601-date="1924">1924</year><source>Treatise on Physiological Optics</source><publisher-loc>New York</publisher-loc><publisher-name>Optical Society of America</publisher-name></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsieh</surname> <given-names>PJ</given-names></name><name><surname>Vul</surname> <given-names>E</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Recognition alters the spatial pattern of FMRI activation in early retinotopic cortex</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>1501</fpage><lpage>1507</lpage><pub-id pub-id-type="doi">10.1152/jn.00812.2009</pub-id><pub-id pub-id-type="pmid">20071627</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname> <given-names>D</given-names></name><name><surname>Azzalini</surname> <given-names>DC</given-names></name><name><surname>Peelen</surname> <given-names>MV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Shape-independent object category responses revealed by MEG and fMRI decoding</article-title><source>Journal of Neurophysiology</source><volume>115</volume><fpage>2246</fpage><lpage>2250</lpage><pub-id pub-id-type="doi">10.1152/jn.01074.2015</pub-id><pub-id pub-id-type="pmid">26740535</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Pescetelli</surname> <given-names>N</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Brain mechanisms underlying the brief maintenance of seen and unseen sensory information</article-title><source>Neuron</source><volume>92</volume><fpage>1122</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.051</pub-id><pub-id pub-id-type="pmid">27930903</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname> <given-names>JR</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id><pub-id pub-id-type="pmid">24593982</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Bandettini</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Mur</surname> <given-names>M</given-names></name><name><surname>Ruff</surname> <given-names>DA</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Bodurka</surname> <given-names>J</given-names></name><name><surname>Esteky</surname> <given-names>H</given-names></name><name><surname>Tanaka</surname> <given-names>K</given-names></name><name><surname>Bandettini</surname> <given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Kievit</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id><pub-id pub-id-type="pmid">23876494</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>Q</given-names></name><name><surname>Hill</surname> <given-names>Z</given-names></name><name><surname>He</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spatiotemporal dissociation of brain activity underlying subjective awareness, objective performance and confidence</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>4382</fpage><lpage>4395</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1820-13.2014</pub-id><pub-id pub-id-type="pmid">24647958</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludmer</surname> <given-names>R</given-names></name><name><surname>Dudai</surname> <given-names>Y</given-names></name><name><surname>Rubin</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Uncovering camouflage: amygdala activation predicts long-term memory of induced perceptual insight</article-title><source>Neuron</source><volume>69</volume><fpage>1002</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.013</pub-id><pub-id pub-id-type="pmid">21382558</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Oostenveld</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minami</surname> <given-names>T</given-names></name><name><surname>Noritake</surname> <given-names>Y</given-names></name><name><surname>Nakauchi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decreased beta-band activity is correlated with disambiguation of hidden figures</article-title><source>Neuropsychologia</source><volume>56</volume><fpage>9</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.12.026</pub-id><pub-id pub-id-type="pmid">24412688</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moratti</surname> <given-names>S</given-names></name><name><surname>Méndez-Bértolo</surname> <given-names>C</given-names></name><name><surname>Del-Pozo</surname> <given-names>F</given-names></name><name><surname>Strange</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic gamma frequency feedback coupling between higher and lower order visual cortices underlies perceptual completion in humans</article-title><source>NeuroImage</source><volume>86</volume><fpage>470</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.037</pub-id><pub-id pub-id-type="pmid">24185019</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>How close are we to understanding v1?</article-title><source>Neural Computation</source><volume>17</volume><fpage>1665</fpage><lpage>1699</lpage><pub-id pub-id-type="doi">10.1162/0899766054026639</pub-id><pub-id pub-id-type="pmid">15969914</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname> <given-names>R</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Maris</surname> <given-names>E</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>F</given-names></name><name><surname>Mitchell</surname> <given-names>T</given-names></name><name><surname>Botvinick</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Machine learning classifiers and fMRI: a tutorial overview</article-title><source>NeuroImage</source><volume>45</volume><fpage>S199</fpage><lpage>S209</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.11.007</pub-id><pub-id pub-id-type="pmid">19070668</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname> <given-names>JD</given-names></name><name><surname>Cohen</surname> <given-names>AL</given-names></name><name><surname>Nelson</surname> <given-names>SM</given-names></name><name><surname>Wig</surname> <given-names>GS</given-names></name><name><surname>Barnes</surname> <given-names>KA</given-names></name><name><surname>Church</surname> <given-names>JA</given-names></name><name><surname>Vogel</surname> <given-names>AC</given-names></name><name><surname>Laumann</surname> <given-names>TO</given-names></name><name><surname>Miezin</surname> <given-names>FM</given-names></name><name><surname>Schlaggar</surname> <given-names>BL</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional network organization of the human brain</article-title><source>Neuron</source><volume>72</volume><fpage>665</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.006</pub-id><pub-id pub-id-type="pmid">22099467</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivolta</surname> <given-names>D</given-names></name><name><surname>Castellanos</surname> <given-names>NP</given-names></name><name><surname>Stawowsky</surname> <given-names>C</given-names></name><name><surname>Helbling</surname> <given-names>S</given-names></name><name><surname>Wibral</surname> <given-names>M</given-names></name><name><surname>Grützner</surname> <given-names>C</given-names></name><name><surname>Koethe</surname> <given-names>D</given-names></name><name><surname>Birkner</surname> <given-names>K</given-names></name><name><surname>Kranaster</surname> <given-names>L</given-names></name><name><surname>Enning</surname> <given-names>F</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Leweke</surname> <given-names>FM</given-names></name><name><surname>Uhlhaas</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Source-reconstruction of event-related fields reveals hyperfunction and hypofunction of cortical circuits in antipsychotic-naive, first-episode schizophrenia patients during Mooney face processing</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>5909</fpage><lpage>5917</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3752-13.2014</pub-id><pub-id pub-id-type="pmid">24760850</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez</surname> <given-names>E</given-names></name><name><surname>George</surname> <given-names>N</given-names></name><name><surname>Lachaux</surname> <given-names>JP</given-names></name><name><surname>Martinerie</surname> <given-names>J</given-names></name><name><surname>Renault</surname> <given-names>B</given-names></name><name><surname>Varela</surname> <given-names>FJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Perception's shadow: long-distance synchronization of human brain activity</article-title><source>Nature</source><volume>397</volume><fpage>430</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/17120</pub-id><pub-id pub-id-type="pmid">9989408</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salti</surname> <given-names>M</given-names></name><name><surname>Monto</surname> <given-names>S</given-names></name><name><surname>Charles</surname> <given-names>L</given-names></name><name><surname>King</surname> <given-names>J-R</given-names></name><name><surname>Parkkonen</surname> <given-names>L</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct cortical codes and temporal dynamics for conscious and unconscious percepts</article-title><source>eLife</source><volume>4</volume><elocation-id>e05652</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05652</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seibold</surname> <given-names>DR</given-names></name><name><surname>McPhee</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Commonality analysis: a method for decomposing explained variance in multiple regression analyses</article-title><source>Human Communication Research</source><volume>5</volume><fpage>355</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1111/j.1468-2958.1979.tb00649.x</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname> <given-names>MG</given-names></name><name><surname>Kusunoki</surname> <given-names>M</given-names></name><name><surname>Sigala</surname> <given-names>N</given-names></name><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>C</given-names></name><name><surname>Egner</surname> <given-names>T</given-names></name><name><surname>Greene</surname> <given-names>M</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name><name><surname>Mangels</surname> <given-names>J</given-names></name><name><surname>Hirsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Predictive codes for forthcoming perception in the frontal cortex</article-title><source>Science</source><volume>314</volume><fpage>1311</fpage><lpage>1314</lpage><pub-id pub-id-type="doi">10.1126/science.1132028</pub-id><pub-id pub-id-type="pmid">17124325</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname> <given-names>L</given-names></name><name><surname>Grützner</surname> <given-names>C</given-names></name><name><surname>Bölte</surname> <given-names>S</given-names></name><name><surname>Wibral</surname> <given-names>M</given-names></name><name><surname>Tozman</surname> <given-names>T</given-names></name><name><surname>Schlitt</surname> <given-names>S</given-names></name><name><surname>Poustka</surname> <given-names>F</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Freitag</surname> <given-names>CM</given-names></name><name><surname>Uhlhaas</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Impaired gamma-band activity during perceptual organization in adults with autism spectrum disorders: evidence for dysfunctional network activity in frontal-posterior cortices</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>9563</fpage><lpage>9573</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1073-12.2012</pub-id><pub-id pub-id-type="pmid">22787042</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teufel</surname> <given-names>C</given-names></name><name><surname>Subramaniam</surname> <given-names>N</given-names></name><name><surname>Dobler</surname> <given-names>V</given-names></name><name><surname>Perez</surname> <given-names>J</given-names></name><name><surname>Finnemann</surname> <given-names>J</given-names></name><name><surname>Mehta</surname> <given-names>PR</given-names></name><name><surname>Goodyer</surname> <given-names>IM</given-names></name><name><surname>Fletcher</surname> <given-names>PC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Shift toward prior knowledge confers a perceptual advantage in early psychosis and psychosis-prone healthy individuals</article-title><source>PNAS</source><volume>112</volume><fpage>13401</fpage><lpage>13406</lpage><pub-id pub-id-type="doi">10.1073/pnas.1503916112</pub-id><pub-id pub-id-type="pmid">26460044</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van de Moortele</surname> <given-names>PF</given-names></name><name><surname>Auerbach</surname> <given-names>EJ</given-names></name><name><surname>Olman</surname> <given-names>C</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Uğurbil</surname> <given-names>K</given-names></name><name><surname>Moeller</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>T1 weighted brain images at 7 tesla unbiased for proton density, T2* contrast and RF coil receive B1 sensitivity with simultaneous vessel visualization</article-title><source>NeuroImage</source><volume>46</volume><fpage>432</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.02.009</pub-id><pub-id pub-id-type="pmid">19233292</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Nieuwenhuijzen</surname> <given-names>ME</given-names></name><name><surname>Backus</surname> <given-names>AR</given-names></name><name><surname>Bahramisharif</surname> <given-names>A</given-names></name><name><surname>Doeller</surname> <given-names>CF</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>van Gerven</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>MEG-based decoding of the spatiotemporal dynamics of visual category perception</article-title><source>NeuroImage</source><volume>83</volume><fpage>1063</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.07.075</pub-id><pub-id pub-id-type="pmid">23927900</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Loon</surname> <given-names>AM</given-names></name><name><surname>Fahrenfort</surname> <given-names>JJ</given-names></name><name><surname>van der Velde</surname> <given-names>B</given-names></name><name><surname>Lirk</surname> <given-names>PB</given-names></name><name><surname>Vulink</surname> <given-names>NC</given-names></name><name><surname>Hollmann</surname> <given-names>MW</given-names></name><name><surname>Scholte</surname> <given-names>HS</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>NMDA receptor antagonist ketamine distorts object recognition by reducing feedback to early visual cortex</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>1986</fpage><lpage>1996</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv018</pub-id><pub-id pub-id-type="pmid">25662715</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vida</surname> <given-names>MD</given-names></name><name><surname>Nestor</surname> <given-names>A</given-names></name><name><surname>Plaut</surname> <given-names>DC</given-names></name><name><surname>Behrmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatiotemporal dynamics of similarity-based neural representations of facial identity</article-title><source>PNAS</source><volume>114</volume><fpage>388</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1073/pnas.1614763114</pub-id><pub-id pub-id-type="pmid">28028220</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>M</given-names></name><name><surname>Arteaga</surname> <given-names>D</given-names></name><name><surname>He</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Brain mechanisms for simple perception and bistable perception</article-title><source>PNAS</source><volume>110</volume><fpage>E3350</fpage><lpage>E3359</lpage><pub-id pub-id-type="doi">10.1073/pnas.1221945110</pub-id><pub-id pub-id-type="pmid">23942129</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wardle</surname> <given-names>SG</given-names></name><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name><name><surname>Grootswagers</surname> <given-names>T</given-names></name><name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></name><name><surname>Carlson</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual similarity of visual patterns predicts dynamic neural activation patterns measured with MEG</article-title><source>NeuroImage</source><volume>132</volume><fpage>59</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.02.019</pub-id><pub-id pub-id-type="pmid">26899210</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.41861.014</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Reviewing Editor</role><aff><institution>University Medical Center Hamburg-Eppendorf</institution><country>Germany</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Büchel</surname><given-names>Christian</given-names> </name><role>Reviewer</role><aff><institution>University Medical Center Hamburg-Eppendorf</institution><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for sending your article entitled &quot;Neural dynamics of visual ambiguity resolution by perceptual prior&quot; for peer review at <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor.</p><p>The reviewers have indicated that the topic of the paper, namely identifying the temporal dynamics underlying the Mooney recognition effect and controlling for non-content-specific effects such as increased attention, salience or decreased task difficulty is very valuable and novel. However, the reviewers remain cautious about whether the data presented conclusively allows to address these claims.</p><p>1) The paper claims that the results are image-specific, but Figure 4D shows condition-specific results. In particular, Figure 4D only shows that recognition decreases between stimulus pattern dissimilarity. But this effect can be driven by many factors, e.g. decreased noise.</p><p>2) Figure 4E shows a significantly positive correlation between the representational similarity structure for recognized Mooney images and unambiguous images, but not that this effect is greater for post vs. pre Mooney images. Therefore, this effect cannot be conclusively related to Mooney image disambiguation.</p><p><italic>Reviewer #1:</italic> </p><p>This paper is on perceptual processing with respect to prior knowledge. They use Mooney images, (binary images without recognizable content), which after one has seen the underlying grey scale image, are easily identified. This is a very powerful perceptual effect and allows the investigation of how prior information affects recognition. The paper uses MEG (and fMRI) in combination with a series of clever time resolved decoding approaches to show &quot;time-courses of dissociation&quot;.</p><p>In addition they used representational similarity analysis (RSA) to show the time-course of similarities between pre and post (same physical stimulus) and post and gray (same percept that is recognition). Not too surprisingly, they show that these time resolved similarities differ, with the stimulus based similarities peaking early and the recognition based similarities peaking later.</p><p>Finally, they employ a powerful model based RSA approach where they investigate the commonalities of RSA based on MEG (as before), fMRI and a theoretically predicted RSA (i.e. a model). The model can incorporate recognition (high similarity between post and gray) etc. Importantly, by looking for commonalties across MEG and fMRI, they can, based on pattern similarity, fuse fMRI and MEG. Although clever and informative a similar approach has already been published (visual object recognition) (Cichy et al., 2014).</p><p>Although the presented data are very interesting and show what can be done with a clever multivariate methods, including model-based RSA analyses, the promise of the title &quot;Neural dynamics of visual ambiguity resolution by perceptual prior&quot; is not fulfilled by this paper. Potentially, this data could give us some insights on <italic>how</italic> the integration of prior and incoming visual information works. This is only vaguely addressed, e.g. by data shown in Figure 3C.</p><p>In addition, one could argue that the novelty of this paper is only incremental: In a previous paper by Cichy et al., 2014, and a subsequent paper by Hebart describing a similar approach using model based RSA (Hebart et al., 2018) similar results were obtained. They studied object recognition, which is also based on prior information (volunteers know the objects and have seen them before in a different manner), although there is no control condition (i.e. identical visual stimulus, but different percept) as in a Mooney faces experiment.</p><p>The current paper should either provide more information about the neural dynamics of visual ambiguity resolution or at least explain how their approach adds novel insights over and above the papers mentioned above.</p><p><italic>Reviewer #2:</italic> </p><p>This study uses fMRI and MEG and an accomplished psychophysical paradigm to investigate how experience-driven image recognition affects neural responses – across time and space. To do so, it uses several multivariate analyses as well as multivariate MEG-fMRI data fusion.</p><p>The key findings are that (i) that experience-driven recognition can be decoded from 300ms onwards based on MEG response patterns, (ii) that this information (see i) persists in a stationary manner over time, (iii) that recognition increases MEG pattern distances between Mooney images, (iv) that the representational geometry (MEG-based) of recognized Mooney images correlates significantly with that for the corresponding original gray-scale images and (v) that shared variance between MEG and fMRI RDMs uniquely accounted for by a recognition model is widespread in the brain (in all ROIs) from 500ms onwards.</p><p>The methodology is without doubt advanced and the general question that this study is supposed to address is of wide general interest. However, my first main concern is that the authors do not introduce a specific hypothesis nor outline exactly how this research is going bring us closer to understanding how experience guides recognition. As a result, the study, although being informative, comes across as &quot;fishing expedition&quot;.</p><p>Another major concern is that the authors do not report statistically solid univariate findings, which makes it impossible to relate the findings reported here to previous imaging studies employing a similar paradigm. The authors do present SVM weight-maps. However, these maps are anecdotal at best as they are not statistically evaluated in any way. Reporting univariate fMRI data would also be extremely valuable, as it would for example enable readers to assess how the MEG-fMRI modeling results relate to fMRI response amplitude (and SNR).</p><p>Furthermore, the authors make a claim that is not fully supported by their findings: they state that &quot;This analysis showed that image-specific information for post-disambiguation Mooney images rises higher than their pre-disambiguation counterparts starting from ~500 ms&quot; based on finding iii. This is misleading, because greater between-image pattern distances do not directly imply greater stimulus information. This finding could, for example, also be explained by noisier responses for recognized Mooney images.</p><p>Another issue is that a crucial test is missing related to finding iv (Figure 4E). This finding implies that recognizing Mooney images causes representational geometry (MEG based) to become more similar to that for the corresponding set of gray-scale images. However, the authors need to demonstrate that this increase in RDM-RDM similarity is significantly greater for the Post RDM as compared to the Pre RDM.</p><p>Finally, I don't see why finding ii is of interest (Figure 3D). To me it is unclear what sets this case of (MEG) pattern information persistence apart from previous reports if this phenomenon (e.g. Carlson et al., 2013), and how it functionally relates to experience-driven recognition.</p><p>Given these issues, I do not recommend publication of this manuscript in its current state.</p><p><italic>Reviewer #3:</italic> </p><p>Summary:</p><p>Flounders and colleagues investigated how the visual and cognitive processing during recognition of images unfolds over time. To pinpoint the effects of the prior, including knowledge of the image content and expectation, they presented participants with two-tone &quot;Mooney&quot; images that are initially difficult to recognize but after disambiguation allow recognition of the stimulus (take the famous picture of the Dalmatian dog as an example). Using MEG decoding, the authors show striking differences between Mooney images before recognition (&quot;pre&quot;) vs. after recognition (&quot;post&quot;) emerging after ~300 ms. Using representational similarity analysis (RSA) to compare between different experimental phases, they show that, as expected, the similarity of stimulus-related patterns of activity increases rapidly after stimulus onset (comparing pre and post-recognition Mooney images), while recognition-related patterns emerge much later ~800 ms (comparing real images with post-recognition). By relating their results to a separate 7T fMRI dataset with model-based RSA, for several regions of interest (ROIs) the authors reveal time courses of information specific to different model components, related to stimulus-based, recognition-based and attention-based processes. They find that stimulus-based processes exhibit an early and a late peak, recognition-based processes dominate throughout time and region after ~500 ms, and attention-based processes are very specifically located to frontal and parietal ROIs at specific time points. They interpret these results in light of the effects of prior experience on object recognition.</p><p>Assessment:</p><p>The authors address a timely and interesting question of how prior experience affects visual and cognitive processing in the human brain. The manuscript uses state-of-the-art methodology in MEG decoding, RSA and MEG-fMRI fusion, and all statistical analyses appear to be sound. I specifically liked the combination of MEG and fMRI data for spatiotemporally-resolved analysis, and the related results are fascinating. In addition, I very much liked the addition of a control condition to make sure the results are not merely due to stimulus repetition (post-recognition images have been seen more frequently than pre-recognition stimuli) or simple stimulus-association effects.</p><p>At the same time, I believe the authors make some claims not supported by the data. They highlight that part of the novelty of their work has to do with the fact that previous work on this topic did not reveal image-specific results and, indeed, the authors do report image-specific findings in Figure 4E. However, in contrast to the authors' claim, the other effects using RSA are likely not stimulus-specific. For example, the results in Figure 4D are averaged across stimuli, leading to <italic>condition</italic>-specific effects. To achieve stimulus-specific effects, the authors would have to either identify the similarity for the same stimulus to itself or identify the difference between same stimulus and different stimulus within different periods of the experiment. They could do this by carrying out a split-half analysis and calculating the difference (within – between). This would be equivalent to a stimulus-specific decoding analysis. I think this kind of analysis would be useful to support their results. Alternatively, the authors may want to adjust this description of their results with respect to stimulus-specificity in the Materials and methods, Results, and Discussion.</p><p>A similar argument could be made regarding the model-based MEG-fMRI fusion results. The stimulus-specific model focuses on gross differences between Mooney images and greyscale images, rather than individual images. The recognition-specific model assumes that images post-recognition all become different from each other, which would lead to high dissimilarity. However, in line with the authors' interpretation of their prior work (Gonzalez-Garcia et al., 2018), one could also argue that they should become more similar to each other (when treated as the class of objects rather than individual images). In addition, their model interpretation would assume that the image itself should at least become more similar to itself, i.e. according to their interpretation, in my understanding the model would have to contain off-diagonal elements for the same image between grayscale and post-recognition periods.</p><p>To strengthen their conclusions, I would suggest the addition of stimulus-specific or at least category-specific (e.g. animate – inanimate) decoding analyses. Further, I would suggest carrying out a category-specific analysis (e.g. animate – inanimate) to confirm the claims that the results are indeed recognition-related.</p><p>While, as mentioned above, the addition of a control analysis is great, it only makes up a fraction of the other conditions. Therefore, the absence of decoding or RSA effects may be due to reduced power. What would the equivalent analysis look like for the experimental data if it is similarly reduced in size?</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Neural dynamics of visual ambiguity resolution by perceptual prior&quot; for further consideration at <italic>eLife</italic>. Your revised article has been favorably reviewed by three reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below:</p><p>As you will see below there are remaining issues with Figure 4E as outlined in the comment by reviewer #2. The new Figure 5 needs further clarifications for the reader and there is one outstanding issue related to how exemplar i.e. image specific these findings are. Finally the model needs either adjustment or a justification in the Discussion section (see comment by reviewer #3).</p><p><italic>Reviewer #2:</italic> </p><p>I am glad to see that the authors have found an elegant way of addressing my most important issue – the lack of direct support for the claim of recognition driven content-specific effects. I think that this issue is now covered by the results shown in Figure 5.</p><p>I still would like to insist on a revision of Figure 4E. The pink line in the figure is referred to as a &quot;recognition-based&quot; time course. This is misleading as the difference between the pink and the green line can be explained by differences in representational geometry due to recognition <italic>and</italic> due to differences related to presenting a Gray vs. Mooney images. Hence, there is no (significant) evidence for this effect being more pronounced for the Post-Gray than the Pre-Gray comparison. Therefore, the authors should explicitly clarify that the difference shown in this figure cannot be conclusively attributed to recognition. They could also simply omit 4E as the content specific recognition effect is now directly demonstrated in Figure 5. The authors could also consider to conceptually link Figure 4 and 5 by clarifying that Figure 4D leaves open the question if recognition-driven enhanced (and equal-to Gray) representational dissimilarity for Mooney images is driven by activation patterns to Mooney images becoming more similar to their Grayscale counterparts. At present, a compelling motivation for the Figure 5 analysis is missing at the start of the corresponding Results section.</p><p>With respect to my introduction-related comment, I still think that the clarity of the paper would be enhanced by including a more specific hypothesis. As noted in the rebuttal, this presently boils down to this sentence: &quot;these observations raise the intriguing possibility that slow, long-distance recurrent neural dynamics involving large-scale brain networks are necessary for prior-experience-guided visual recognition.&quot; I find it hard to see how this rather broad and vague hypothesis is a natural motivation for the specific research preformed during this study nor how it is precisely addressed by the findings.</p><p><italic>Reviewer #3:</italic> </p><p>I would like to thank the authors for taking up many of my suggestions. I believe the clarifications in the text and the addition of the novel stimulus-specific analyses greatly strengthened the paper and the conclusions that can be drawn from the results. Nevertheless, I have some remaining reservations.</p><p>It is correct that the analyses in Figure 4D were carried out at the image level. Nevertheless, in my understanding it is impossible to tell whether these effects are image-specific. This is a small correction to my original assessment where I said these results were condition-specific, while in fact they simply do not allow distinguishing between condition-specific and image-specific effects. The goal of this analysis is to show differences in the image-specific effects between the three conditions. Using the dissimilarity matrices in 4B and the analyses of the authors in 4C, then indeed the expected dissimilarity matrix for image-specific effects would exhibit low dissimilarity everywhere within a condition. This dissimilarity would then be expected to be different for each condition and change across time. This is the result the authors showed in 4D. However, for condition-specific effects the expected dissimilarity matrix would show low dissimilarity within condition, as well. The only way to tell these apart is by comparing the dissimilarity within image to the dissimilarity between. Since the authors already conducted this analysis, it is just a matter of clarification that the results in 4D cannot distinguish between condition-specific and image-specific results.</p><p>Regarding the MEG-fMRI fusion modeling, I agree with the authors that the model is a choice the author has to make, but it has to be both justified and consistent. For the former, perhaps it would make sense to spell out the construction of the models in some more detail. For the latter, if the authors do not want to make adjustments, I would suggest discussing those limitations. The &quot;recognition&quot; model assumes that (1) Mooney images in the Pre-phase are not recognized and are all similarly &quot;unrecognized&quot;, (2) all recognized stimuli are different from each other, and (3) all recognized stimuli are as different from each other as they are from unrecognized images. I can follow (1). However, as I mentioned, for (2) according to the authors' interpretation the &quot;diagonals of between-condition squares&quot; should be similar to each other. Without this, the model is inconsistent. The authors argued they cannot predict the exact value expected for those cells. However, since they are using the Spearman correlation, they would just need to choose if the dissimilarity is lower than within Pre (i.e. the blue square), the same, or higher. If the authors cannot decide, they could leave out those cells and remain agnostic. Note that, however, the same issue of comparability arises regarding (3): it seems like an even stronger assumption that in terms of recognition all recognized images are as different from each other as they are from unrecognized images. The authors should either be explicit about this, adjust the model, or remove those cells from the analysis.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.41861.015</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>The reviewers have indicated that the topic of the paper, namely identifying the temporal dynamics underlying the Mooney recognition effect and controlling for non-content-specific effects such as increased attention, salience or decreased task difficulty is very valuable and novel. However, the reviewers remain cautious about whether the data presented conclusively allows to address these claims.</p><p>1) The paper claims that the results are image-specific, but Figure 4D shows condition-specific results. In particular, Figure 4D only shows that recognition decreases between stimulus pattern dissimilarity. But this effect can be driven by many factors, e.g. decreased noise.</p></disp-quote><p>Firstly, because we showed that between-image dissimilarity is higher for post- than pre- images, we believe the above comment meant to say “Figure 4D only shows that recognition <italic>increases</italic> between stimulus pattern dissimilarity. But this effect can be driven by many factors, e.g. <italic>increased</italic> noise.”</p><p>Second, we would like to point out that for the analysis shown in Figure 4D, we had already conducted a control analysis using Euclidean distance and cross-validated Euclidean distance. The results of this analysis are presented in Figure 4—figure supplement 2, which may have been overlooked. Importantly, cross-validated Euclidean distance is a metric that is unaffected by noise in the data due to cross-validation (Guggenmos et al., 2018). Specifically, it is calculated as:</p><p>dEuclidean,c.v.2x,y=x-yA(x-y)[B]T, (1) where A and B denote the two partitions of the data within each cross-validation fold. (For this analysis, we used a 3-fold cross-validation scheme.) This way, noise in the data cancels out, and the cross-validated Euclidean distance is only driven by signal, that is, the component of the data that is consistent across partitions. As explained by Guggenmos et al., this approach “can improve the reliability of distance estimates when noise levels differ between measurements” (P. 438).</p><p>As expected, this control analysis using cross-validated Euclidean distance yielded very similar result to that shown in Figure 4D. The method for this analysis was previously tucked away in the legend to Figure 4—figure supplement 2. We have now included detailed method in the manuscript (Materials and methods, subsection “Euclidean distance”) and have explained the rationale and interpretation of this control analysis better in Results (subsection “Disambiguation increases across-image dissimilarity in neural dynamics”).</p><p>Third, to further boost the claim for content-specific effects, we have now conducted a stimulus-specific decoding analysis using the “correlation (within-between)” metric as suggested by reviewer #4. This analysis allowed us to test, at the single-trial level, how well we can distinguish neural activities elicited by a Mooney image presented before vs. after disambiguation, and how well we can distinguish a pre- or post-disambiguation Mooney image from its corresponding Grayscale image. We were very pleased by the clarity and the robustness of this finding, and are deeply grateful to reviewer #4 for suggesting this analysis. The results, presented in a newly added figure (Figure 5), show that within a ~300 ms window after stimulus onset, a Mooney image – whether presented before or after disambiguation – is well separable at the single-trial level from its corresponding grayscale image. By contrast, after 500 ms post-stimulus-onset, a post-disambiguation Mooney image is entirely indistinguishable from its matching grayscale image at the single trial level despite differences in stimulus features (but well separable from other grayscale images, Figure 5D), while a pre-disambiguation Mooney image is well separable from the same image presented after disambiguation or its matching grayscale image (Figure 5C). These results reveal an image-specific shift in neural representation toward the relevant prior experience.</p><p>In a complementary analysis, we quantified the strength of the diagonals of between-condition squares (referred to as “off-diagonal elements for the same image” by reviewer #4) as compared to the off-diagonal elements in the same between-condition squares, again using the “correlation (within-between)” measure (Figure 5D). This analysis quantifies the similarity between the neural representation of the same/matching image in different perceptual conditions above and beyond its similarity to other images (e.g., is post-Image-A represented more similarly to pre-Image-A than to pre-Image-B)? The results, presented in Figure 5D, show that neural activities reflect similarities of stimulus features in an early (&lt;500 ms) time window and similarities of recognition content in a late (&gt;500 ms) time period.</p><p>We would like to refer the editors and reviewers to the newly added text in the Results (subsection “Comparing image-specific dynamic neural representations across perceptual conditions at the single-trial level”), Discussion (fifth paragraph), Materials and methods (subsection “Single-trial separability”), and the new Figure 5 for further details. We believe that these new observations provide strong evidence for content-specific neural effects – encoding of stimulus input in the early (&lt;300 ms) time period and of recognition content in the late (&gt;500 ms) time period, which further strengthens our main conclusions.</p><disp-quote content-type="editor-comment"><p>2) Figure 4E shows a significantly positive correlation between the representational similarity structure for recognized Mooney images and unambiguous images, but not that this effect is greater for post vs. pre Mooney images. Therefore, this effect cannot be conclusively related to Mooney image disambiguation.</p></disp-quote><p>In the analysis shown in Figure 4E, we had indeed probed the correlation between the representational geometry of pre-disambiguation Mooney images and grayscale images. As we mentioned in the text, “Correlations of Pre-Pre and Gray-Gray squares of the RDM were not significant at any time point.” Since no significance was found, we chose not to include this trace in Figure 4E to avoid cluttering the figure (the full figure is included as <xref ref-type="fig" rid="respfig1">Author response image 1</xref>). This result contrasts with the correlations between Post-Post and Gray-Gray squares, which yielded sustained significant (<italic>p</italic> &lt; 0.05, cluster-based permutation test) clusters after 500 ms (Figure 4E, magenta). Likely due to insufficient statistical power, a direct contrast between r(Pre-Pre, Gray-Gray) and r(Post-Post, Gray-Gray) did not yield significant clusters following correction for multiple comparisons using a cluster-based permutation test. Nonetheless, we believe that this analysis is valuable, given the sustained significance in the late (&gt;500 ms) time period for the recognition-based representation (correlating representational geometry between post-Mooney and grayscale images), and the lack of significance for the control analysis (correlating representational geometry between pre-Mooney and grayscale images).</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Same as Figure 4E, now showing element-wise correlations between the Pre-Pre and Gray-Gray triangles in the RDM as the black dashed line.</title><p>No significant cluster was found for this comparison at a level of <italic>p</italic> &lt; 0.05 (cluster-based permutation test).</p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-41861-resp-fig1-v2"/></fig><p>In sum, we believe that our results, including the new addition, present strong evidence for content-specific neural effects related to stimulus processing and subjective recognition during prior-guided visual perception, and reveal their respective time courses.</p><p>Reviewer #1:</p><disp-quote content-type="editor-comment"><p>This paper is on perceptual processing with respect to prior knowledge. They use Mooney images, (binary images without recognizable content), which after one has seen the underlying grey scale image, are easily identified. This is a very powerful perceptual effect and allows the investigation of how prior information affects recognition. The paper uses MEG (and fMRI) in combination with a series of clever time resolved decoding approaches to show &quot;time-courses of dissociation&quot;.</p><p>In addition they used representational similarity analysis (RSA) to show the time-course of similarities between pre and post (same physical stimulus) and post and gray (same percept i.e. recognition). Not too surprisingly, they show that these time resolved similarities differ, with the stimulus based similarities peaking early and the recognition based similarities peaking later.</p><p>Finally, they employ a powerful model based RSA approach where they investigate the commonalities of RSA based on MEG (as before), fMRI and a theoretically predicted RSA (i.e. a model). The model can incorporate recognition (high similarity between post and gray) etc. Importantly, by looking for commonalties across MEG and fMRI, they can, based on pattern similarity, fuse fMRI and MEG. Although clever and informative a similar approach has already been published (visual object recognition) (Cichy et al., 2014).</p></disp-quote><p>The contribution of the present study is not in methodology development, but in using recently developed methods for probing multivariate neural representations in dynamic, whole-head MEG signals and merging neural data across modalities at an informational level (e.g., Kriegeskorte et al., 2008; Cichy et al., 2014; Hebart et al., 2018; Guggenmos et al., 2018) to reveal neural mechanisms underlying prior knowledge’s influence on visual perception and recognition. Previous studies applying these methods have used images depicting clear, high-contrast, isolated objects, where the prior knowledge invoked by the images (e.g., the knowledge of cows as an animal category) was solidified typically decades ago during development. By contrast, our paradigm allows the establishment of a prior knowledge de novo in an extremely fast and robust manner; this allows us to probe and contrast visual perception/recognition without vs. with prior knowledge and reveal the time courses of the involved neural computations. The distinction of our results from previous findings is also underscored by different latencies of the identified neural effects: we find recognition-related neural effects with temporal latencies (&gt;500 ms following stimulus onset) much later than most previously reported neural effects related to object recognition using MEG (typically within 500 ms). This difference and the related considerations were addressed in the Introduction (third paragraph) and Discussion (sixth paragraph). We now better explain our topic of investigation and its broader significance in the opening paragraph of Discussion:</p><p>“Despite the pervasive need to resolve stimulus ambiguity (caused by occlusion, clutter, shading, and inherent complexities of natural objects) in natural vision (Olshausen and Field, 2005) and the enormous power that prior knowledge acquired through past experiences wields in shaping perception (Helmholtz, 1924; Albright, 2012), the neural mechanisms underlying prior-guided visual recognition remain mysterious. Here, we exploited a dramatic visual phenomenon, where a single exposure to a clear, unambiguous image greatly facilitates recognition of a related degraded image, to shed light on dynamical neural mechanisms that allow past experiences to guide recognition of impoverished sensory input.”</p><disp-quote content-type="editor-comment"><p>Although the presented data are very interesting and show what can be done with a clever multivariate methods, including model-based RSA analyses, the promise of the title &quot;Neural dynamics of visual ambiguity resolution by perceptual prior&quot; is not fulfilled by this paper. Potentially, this data could give us some insights on how the integration of prior and incoming visual information works. This is only vaguely addressed, e.g. by data shown in Figure 3C.</p><p>In addition, one could argue that the novelty of this paper is only incremental: In a previous paper by Cichy et al., 2014, and a subsequent paper by Hebart describing a similar approach using model based RSA (Hebart et al., 2018) similar results were obtained. They studied object recognition, which is also based on prior information (volunteers know the objects and have seen them before in a different manner), although there is no control condition (i.e. identical visual stimulus, but different percept) as in a Mooney faces experiment.</p><p>The current paper should either provide more information about the neural dynamics of visual ambiguity resolution or at least explain how their approach adds novel insights over and above the papers mentioned above.</p></disp-quote><p>We hope that the above responses to the reviewer’s overall assessment and the editors’ comments have sufficiently addressed these concerns.</p><p>Reviewer #2:</p><disp-quote content-type="editor-comment"><p>[…] The methodology is without doubt advanced and the general question that this study is supposed to address is of wide general interest. However, my first main concern is that the authors do not introduce a specific hypothesis nor outline exactly how this research is going bring us closer to understanding how experience guides recognition. As a result, the study, although being informative, comes across as &quot;fishing expedition&quot;.</p></disp-quote><p>We respectfully disagree with this characterization of our study. As we presented in Introduction, the current study indeed tests a specific hypothesis:</p><p>“Previous neuroimaging studies have observed that disambiguation of Mooney images induces widespread activation and enhanced image-specific information in both visual and frontoparietal cortices. […] Together with a recent finding of altered content-specific neural representations in frontoparietal regions following Mooney image disambiguation, these observations raise the intriguing possibility that slow, long-distance recurrent neural dynamics involving large-scale brain networks are necessary for prior-experience-guided visual recognition.”</p><disp-quote content-type="editor-comment"><p>Another major concern is that the authors do not report statistically solid univariate findings, which makes it impossible to relate the findings reported here to previous imaging studies employing a similar paradigm. The authors do present SVM weight-maps. However, these maps are anecdotal at best as they are not statistically evaluated in any way. Reporting univariate fMRI data would also be extremely valuable, as it would for example enable readers to assess how the MEG-fMRI modeling results relate to fMRI response amplitude (and SNR).</p></disp-quote><p>Statistically solid univariate findings using the fMRI data set have already been reported in a previous publication (Gonzalez-Garcia et al., 2018). Since the strength of MEG is in the temporal domain, not the spatial domain, and due to volume conduction, we believe that a massive univariate analysis using the MEG data set in the context of the present study would be superfluous. This is also not in line with most recent studies employing multivariate analyses applied to MEG data (e.g., Carlson et al., 2013; Cichy et al., 2014; Hebart et al. 2018, referenced by reviewers #1 and #2).</p><disp-quote content-type="editor-comment"><p>Furthermore, the authors make a claim that is not fully supported by their findings: they state that &quot;This analysis showed that image-specific information for post-disambiguation Mooney images rises higher than their pre-disambiguation counterparts starting from ~500 ms&quot; based on finding iii. This is misleading, because greater between-image pattern distances do not directly imply greater stimulus information. This finding could, for example, also be explained by noisier responses for recognized Mooney images.</p></disp-quote><p>Please see our response to the editor’s point #1 above.</p><disp-quote content-type="editor-comment"><p>Another issue is that a crucial test is missing related to finding iv (Figure 4E). This finding implies that recognizing Mooney images causes representational geometry (MEG based) to become more similar to that for the corresponding set of gray-scale images. However, the authors need to demonstrate that this increase in RDM-RDM similarity is significantly greater for the Post RDM as compared to the Pre RDM.</p></disp-quote><p>Please see our response to the editor’s point #2 above.</p><disp-quote content-type="editor-comment"><p>Finally, I don't see why finding ii is of interest (Figure 3D). To me it is unclear what sets this case of (MEG) pattern information persistence apart from previous reports if this phenomenon (e.g. Carlson et al., 2013), and how it functionally relates to experience-driven recognition.</p></disp-quote><p>Carlson et al. 2013 is no doubt a classic in the literature. However, it addresses a distinct question from the current study: dynamical neural mechanisms underlying recognition of clear, high-contrast color images of isolated objects (Carlson study) vs. dynamical neural mechanisms underlying experience-guided recognition of degraded, black-and-white images of objects embedded in scenes, where recognition without prior experience is extremely difficult (our study). In addition, the cross-decoding result in Carlson et al. (Figure 6A therein) did not show “pattern information persistence”, but rather transient effects that were fast changing over time – as shown by the diagonal pattern of significant decoding which contrasts with the rectangular pattern in our Figure 3D. (But again, these two analyses are asking very different questions in two studies that have different aims.)</p><p>What this analysis (Figure 3D) shows is that neural activity patterns distinguishing perceptual stage (pre- vs. post-disambiguation) are relatively sustained over time. Although this analysis is not content-specific (as we clearly acknowledge in the manuscript), it sets up the stage for the content-specific analyses and results presented thereafter.</p><disp-quote content-type="editor-comment"><p>Given these issues, I do not recommend publication of this manuscript in its current state.</p></disp-quote><p>We hope that we have satisfactorily addressed the reviewer’s concerns.</p><p>Reviewer #3:</p><p>[…] At the same time, I believe the authors make some claims not supported by the data. They highlight that part of the novelty of their work has to do with the fact that previous work on this topic did not reveal image-specific results and, indeed, the authors do report image-specific findings in Figure 4E. However, in contrast to the authors' claim, the other effects using RSA are likely not stimulus-specific. For example, the results in Figure 4D are averaged across stimuli, leading to condition-specific effects. To achieve stimulus-specific effects, the authors would have to either identify the similarity for the same stimulus to itself or identify the difference between same stimulus and different stimulus within different periods of the experiment. They could do this by carrying out a split-half analysis and calculating the difference (within – between). This would be equivalent to a stimulus-specific decoding analysis. I think this kind of analysis would be useful to support their results. Alternatively, the authors may want to adjust this description of their results with respect to stimulus-specificity in the Materials and methods, Results, and Discussion.</p><p>Please see our response to the editors’ point #1 above. We are deeply grateful to the reviewer for suggesting the split-half analysis using “correlation (within-between)” metric, the results of which are included in the newly added Figure 5 and described in a newly added Results subsection “Comparing image-specific dynamic neural representations across perceptual conditions at the single-trial level”). Related methods are described in Materials and methods, subsection “Single-trial separability”. Although we did not use a split-half analysis exactly, we performed such an analysis at the single-trial level to calculate the “correlation (within – between)” metric.</p><p>We also note that we think the results in Figure 4D were indeed image-specific instead of condition-specific. This is because the time courses in Figure 4D were averaged across individual image-pairs within each perceptual condition, where each value quantifies the dissimilarity between neural activity patterns related to those two individual images. Thus, this analysis shows that neural activity patterns elicited by post-disambiguation Mooney images are more distinct from each other than those elicited by the same images presented pre-disambiguation. In a control analysis (shown in Figure 4—figure supplement 2, now better explained in Results, subsection “Disambiguation increases across-image dissimilarity in neural dynamics”, and Materials and methods, subsection “Euclidean distance”), the results in Figure 4D were reproduced using cross-validated Euclidean distance, which is only contributed by signal components that are consistent across separate partitions of the data (hence, suppressing the contribution of random noise). Nonetheless, since this analysis was not conducted at the level of single trials, we have now removed the term “image-specific information” when describing this analysis (in the Abstract, title and concluding paragraph in the corresponding Results section).</p><disp-quote content-type="editor-comment"><p>A similar argument could be made regarding the model-based MEG-fMRI fusion results. The stimulus-specific model focuses on gross differences between Mooney images and greyscale images, rather than individual images. The recognition-specific model assumes that images post-recognition all become different from each other, which would lead to high dissimilarity. However, in line with the authors' interpretation of their prior work (Gonzalez-Garcia et al., 2018), one could also argue that they should become more similar to each other (when treated as the class of objects rather than individual images). In addition, their model interpretation would assume that the image itself should at least become more similar to itself, i.e. according to their interpretation, in my understanding the model would have to contain off-diagonal elements for the same image between grayscale and post-recognition periods.</p></disp-quote><p>We do not understand the reviewer’s comment “in line with the authors' interpretation of their prior work (Gonzalez-Garcia et al., 2018), one could also argue that they should become more similar to each other (when treated as the class of objects rather than individual images).” In this previous paper, we showed that dissimilarity between neural representation of individual images increases substantially after disambiguation, i.e., they became more different from each other. This was shown in several ways in that paper: i) overall redder hues of the Post-Post square than the Pre-Pre square of the RDMs (Figure 3B and 4A); ii) larger distances between dots representing individual images in the Post stage than the Pre stage in the multidimensional scaling (MDS) plots (Figure 3C and 4B); iii) statistical summary results (Figure 3D and 4C, cyan brackets).</p><p>The “off-diagonal elements for the same image between grayscale and post-recognition periods” (i.e., “the diagonal elements of between-condition squares” in our terminology, which we think is more accurate) have been systematically and comprehensively probed in the analysis described in the new Figure 5 (see our reply to the previous comment). This analysis quantifies these diagonals and compares them between each other (Figure 5C) and to the off-diagonal elements in the same between-condition squares (Figure 5D) using the empirical MEG data. We think this is a superior approach to including such diagonals in the between-condition squares of the model RDM. This is because the models were designed to probe relatively coarse effects (as we acknowledge in the manuscript), and it would be hard to know what arbitrary value to set such between-condition diagonals within each model, which contained binary values capturing coarser effects. All models are intended to capture certain aspects in the data (“all models are wrong, some are useful”). In fact, we think that it is wonderful (but not an a priori given) that this model-based fMRI-MEG fusion analysis probing relatively coarse effects yielded stimulus- and recognition-related neural activity time courses that are very much consistent with the earlier content-specific analyses applied to the MEG data alone (Figures 4 and 5). But, of course, the model-based fusion analysis provided further insight into the spatial dimension by bringing in the fMRI data.</p><disp-quote content-type="editor-comment"><p>To strengthen their conclusions, I would suggest the addition of stimulus-specific or at least category-specific (e.g. animate – inanimate) decoding analyses. Further, I would suggest carrying out a category-specific analysis (e.g. animate – inanimate) to confirm the claims that the results are indeed recognition-related.</p></disp-quote><p>We hope that we have satisfactorily addressed the concerns regarding stimulus-specific effects (summarized in our response to the editors’ point #1). We strongly believe that the current results provide very clear and very robust content-specific recognition-related effects: for example, we show that a post-disambiguation Mooney image is indistinguishable at the single-trial level from its matching grayscale image from ~500 ms onward (Figure 5C, dark blue) but well separable from other grayscale images (Figure 5D, dark blue), while in the same time period a pre-disambiguation Mooney image is well separable at the single-trial level from the same image presented post-disambiguation or from its matching grayscale image (Figure 5C, orange and green). These results reveal an image-specific shift in neural representation toward the relevant prior experience that guides perception. Furthermore, our previously included control analysis for Figure 4D, using cross-validated Euclidean distance (Figure 4—figure supplement 2), demonstrated that the increase in between-image dissimilarity following disambiguation was not driven by changing levels of noise in the data.</p><p>Given that we have shown clear content-specific effects at the level of individual images, we respectfully think that a category-level analysis is beyond the scope of this study.</p><disp-quote content-type="editor-comment"><p>While, as mentioned above, the addition of a control analysis is great, it only makes up a fraction of the other conditions. Therefore, the absence of decoding or RSA effects may be due to reduced power. What would the equivalent analysis look like for the experimental data if it is similarly reduced in size?</p></disp-quote><p>We have performed a control analysis for Figure 3A-B which matches the statistical power between real and catch image sets. Since there were only 6 catch image sets, we randomly selected 6 real image sets and re-conducted decoding of presentation stage (Pre- vs. Post-disambiguation). After matching the statistical power of catch image sets, we still obtained significant decoding of presentation stage using the SCP band, as shown in <xref ref-type="fig" rid="respfig2">Author response image 2A</xref>. Qualitatively similar results were obtained using the ERF band (<xref ref-type="fig" rid="respfig2">Author response image 2B</xref>). Due to computational intensiveness, we did not perform cluster-based permutation test for this analysis.</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Same as Figure 3A-B (black and green traces), except that 6 randomly selected real image sets were used to match the statistical power of catch image sets.</title><p>Results from 10 such randomly selected subsets were averaged together, and mean and s.e.m. of decoding accuracy across subjects are plotted for both real (green) and catch (black) image sets. Horizontal bar indicates significant difference from chance level (<italic>p</italic> &lt; 0.05, FDR corrected). Catch results are identical as in Figure 3A-B.</p></caption><graphic mime-subtype="jpeg" mimetype="image" xlink:href="elife-41861-resp-fig2-v2"/></fig><p>Since changes in statistical power would only systematically affect decoding accuracy (such as in Figure 3A-B), but not estimation of the mean (such as in Figure 4D and Figure 4—figure supplement 1), we did not perform a similar control analysis for Figure 4D. In other words, if we select 6 random real image sets and re-compute Figure 4D, we would not expect any systematic change in the result.</p><p>[Editors' note: further revisions were requested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed before acceptance, as outlined below: As you will see below there are remaining issues with Figure 4E as outlined in the comment by reviewer #2. The new Figure 5 needs further clarifications for the reader and there is one outstanding issue related to how exemplar i.e. image specific these findings are. Finally the model needs either adjustment or a justification in the Discussion section (see comment by reviewer #3).</p></disp-quote><p>We are grateful to the editors and reviewers for the favorable evaluation of our previous revision and the additional helpful suggestions. We have thoroughly further revised the manuscript in line with the editors’ and reviewers’ comments. Major changes include:</p><p>- We have significantly toned down the interpretations and conclusions derived from Figure 4E, and now specifically state that this analysis only provides qualitative evidence (albeit from a very unique angle and yielding findings that are consistent with all the other analyses), which is quantitatively assessed by the ensuing model-driven data fusion analysis.</p><p>- In response to both reviewers’ comments suggesting that Figure 4D leads naturally to Figure 5, we have now swapped the order of the Results sections related to Figure 4E and Figure 5, such that the revised text describes results in the following sequence: Figure 4D → Figure 5 → Figure 4E → Figure 6. Although unconventional in terms of figure sequence, we think that this order fits better with the logical flow of the analyses, such that the questions opened up by Figure 4D are answered by Figure 5, and the qualitative evidence provided by Figure 4E is strengthened by Figure 6.</p><p>- We have revised the models in line with reviewer #4’s suggestions. The results obtained with these updated models are consistent with our previous findings but show stronger neural effects. As a result, Figure 6 as well as Figure 6—figure supplement 1 have been updated, and the related text has been thoroughly revised.</p><p>We believe that our revision has fully addressed all of the editors’ and reviewers’ remaining concerns and the manuscript has been further strengthened as a result. Please find below a point-by-point response to the editors’ and reviewers’ comments.</p><p>Reviewer #2:</p><disp-quote content-type="editor-comment"><p>I am glad to see that the authors have found an elegant way of addressing my most important issue – the lack of direct support for the claim of recognition driven content-specific effects. I think that this issue is now covered by the results shown in Figure 5.</p></disp-quote><p>We are pleased that the reviewer found the analysis reported in Figure 5 satisfactory.</p><disp-quote content-type="editor-comment"><p>I still would like to insist on a revision of Figure 4E. The pink line in the figure is referred to as a &quot;recognition-based&quot; time course. This is misleading as the difference between the pink and the green line can be explained by differences in representational geometry due to recognition and due to differences related to presenting a Gray vs. Mooney images. Hence, there is no (significant) evidence for this effect being more pronounced for the Post-Gray than the Pre-Gray comparison. Therefore, the authors should explicitly clarify that the difference shown in this figure cannot be conclusively attributed to recognition. They could also simply omit 4E as the content specific recognition effect is now directly demonstrated in Figure 5. The authors could also consider to conceptually link Figure 4 and 5 by clarifying that Figure 4D leaves open the question if recognition-driven enhanced (and equal-to Gray) representational dissimilarity for Mooney images is driven by activation patterns to Mooney images becoming more similar to their Grayscale counterparts. At present, a compelling motivation for the Figure 5 analysis is missing at the start of the corresponding Results section.</p></disp-quote><p>We have now significantly toned down and qualified the interpretations of Figure 4E, and present it as providing <italic>qualitative</italic> evidence consistent with the other analyses. Below we reproduce the most relevant text:</p><p>“As a control measure, the correlation between Pre-Pre and Gray-Gray squares of the RDM was not significant at any time point, suggesting that, as expected, representational geometry is different between conditions with different stimulus input and different recognition outcomes. […] In the final analysis presented below, we will quantitatively test this possibility using a model-driven MEG-fMRI fusion analysis that simultaneously elucidates the spatial dimension of the evolving neural dynamics.”</p><p>As mentioned above, we have also moved the Results section related to Figure 4E later, to provide a more smooth and direct transition between Figure 4D and Figure 5:</p><p>“This dramatic effect raises two important questions: 1) Is this effect driven by the neural representations of Mooney images shifting towards those of their respective grayscale counterparts? […] To answer these questions, we next probe how neural representation for a particular Mooney image changes following disambiguation at the single-trial level.”</p><p>We believe that Figure 4E is a valuable analysis to retain in the manuscript, since it is the only analysis that probes how the fine-grained representational geometry (the set of representational distances across all image pairs) compares between conditions. It shows that the representational geometry is significantly similar between Pre and Post conditions in an early time window (&lt;300 ms), and between Post and Gray conditions in a late time window (&gt;600 ms). In addition, the control analysis of comparing between Pre and Gray conditions did not yield any significant time point, as expected. We acknowledge that the result is not as strong as one would like, since a direct contrast between Post-Gray and Pre-Gray comparisons did not yield significance after correcting for multiple comparisons using cluster-based permutation test. Given these considerations, we have opted to leave the analysis in and present it as qualitative evidence.</p><disp-quote content-type="editor-comment"><p>With respect to my introduction-related comment, I still think that the clarity of the paper would be enhanced by including a more specific hypothesis. As noted in the rebuttal, this presently boils down to this sentence: &quot;these observations raise the intriguing possibility that slow, long-distance recurrent neural dynamics involving large-scale brain networks are necessary for prior-experience-guided visual recognition.&quot; I find it hard to see how this rather broad and vague hypothesis is a natural motivation for the specific research preformed during this study nor how it is precisely addressed by the findings.</p></disp-quote><p>In the sentence quoted by the reviewer, we have now clarified that “slow” refers to “taking longer than 500 ms”. We believe that this is actually a very specific hypothesis, given that previous studies have typically reported recognition-related neural activity that concludes within 500 ms, as we previously stated in the same paragraph (reproduced below):</p><p>“By contrast, neural dynamics underlying recognition of intact, unambiguous images, as well as scene-facilitation of object recognition, typically conclude within 500 ms (Carlson et al., 2013; van de Nieuwenhuijzen et al., 2013; Kaiser et al., 2016; Brandman and Peelen, 2017). Together with a recent finding of altered content-specific neural representations in frontoparietal regions following Mooney image disambiguation (Gonzalez-Garcia et al., 2018), these observations raise the intriguing possibility that slow (taking longer than 500 ms), long-distance recurrent neural dynamics involving large-scale brain networks are necessary for prior-experience-guided visual recognition.”</p><p>We also note that not all valuable research derives from testing specific hypotheses and data-driven analyses are equally important for uncovering behaviorally relevant patterns in large, complex neural data sets without a priori biases. Some of our analyses may lie between hypothesis-driven and data-driven extremes, as we note in Introduction:</p><p>“To unravel neural mechanisms underlying prior experience’s influence on perception, an important unanswered question is how different information processing stages are dynamically encoded in neural activities.”</p><p>Reviewer #3:</p><disp-quote content-type="editor-comment"><p>I would like to thank the authors for taking up many of my suggestions. I believe the clarifications in the text and the addition of the novel stimulus-specific analyses greatly strengthened the paper and the conclusions that can be drawn from the results. Nevertheless, I have some remaining reservations.</p><p>It is correct that the analyses in Figure 4D were carried out at the image level. Nevertheless, in my understanding it is impossible to tell whether these effects are image-specific. This is a small correction to my original assessment where I said these results were condition-specific, while in fact they simply do not allow distinguishing between condition-specific and image-specific effects. The goal of this analysis is to show differences in the image-specific effects between the three conditions. Using the dissimilarity matrices in 4B and the analyses of the authors in 4C, then indeed the expected dissimilarity matrix for image-specific effects would exhibit low dissimilarity everywhere within a condition. This dissimilarity would then be expected to be different for each condition and change across time. This is the result the authors showed in 4D. However, for condition-specific effects the expected dissimilarity matrix would show low dissimilarity within condition, as well. The only way to tell these apart is by comparing the dissimilarity within image to the dissimilarity between. Since the authors already conducted this analysis, it is just a matter of clarification that the results in 4D cannot distinguish between condition-specific and image-specific results.</p></disp-quote><p>We agree. As mentioned above, we have now re-ordered the text sections related to Figure 4E and Figure 5, such that the presentation of Figure 4D is followed by Figure 5 in the Results section. We have further added a paragraph at the end of Figure 4D section (subsection “Disambiguation increases across-image dissimilarity in neural dynamics”) to discuss the limitations of this analysis and provide a better transition to the analysis presented in Figure 5.</p><disp-quote content-type="editor-comment"><p>Regarding the MEG-fMRI fusion modeling, I agree with the authors that the model is a choice the author has to make, but it has to be both justified and consistent. For the former, perhaps it would make sense to spell out the construction of the models in some more detail. For the latter, if the authors do not want to make adjustments, I would suggest discussing those limitations. The &quot;recognition&quot; model assumes that (1) Mooney images in the Pre-phase are not recognized and are all similarly &quot;unrecognized&quot;, (2) all recognized stimuli are different from each other, and (3) all recognized stimuli are as different from each other as they are from unrecognized images. I can follow (1). However, as I mentioned, for (2) according to the authors' interpretation the &quot;diagonals of between-condition squares&quot; should be similar to each other. Without this, the model is inconsistent. The authors argued they cannot predict the exact value expected for those cells. However, since they are using the Spearman correlation, they would just need to choose if the dissimilarity is lower than within Pre (i.e. the blue square), the same, or higher. If the authors cannot decide, they could leave out those cells and remain agnostic. Note that, however, the same issue of comparability arises regarding (3): it seems like an even stronger assumption that in terms of recognition all recognized images are as different from each other as they are from unrecognized images. The authors should either be explicit about this, adjust the model, or remove those cells from the analysis.</p></disp-quote><p>We would like to thank the reviewer for the very helpful suggestion. Both the consideration about Spearman correlation (only the ordering of values, not their absolute values, matters) and the strategy of excluding cells where the model is agnostic were great suggestions. We have now updated both the Stimulus and the Recognition model to address the points raised by the reviewer (specifically, 2 and 3). The new results are qualitatively consistent with what we presented in the previous submission, but show stronger neural effects related to stimulus processing. Given the extensiveness of changes to the text, we do not reproduce the revised text here, but would like to refer the reviewer to the relevant Results subsection “Model-driven MEG-fMRI data fusion spatiotemporally resolves neural dynamics related to stimulus, attention, and recognition processing”, as well as the revised Figure 6.</p></body></sub-article></article>