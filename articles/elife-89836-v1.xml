<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">89836</article-id><article-id pub-id-type="doi">10.7554/eLife.89836</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89836.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Attention modulates human visual responses to objects by tuning sharpening</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Doostani</surname><given-names>Narges</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5775-6595</contrib-id><email>narges.doostani.d@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hossein-Zadeh</surname><given-names>Gholam-Ali</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4190-6071</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Vaziri-Pashkam</surname><given-names>Maryam</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1830-2501</contrib-id><email>mvaziri@udel.edu</email><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xreqs31</institution-id><institution>School of Cognitive Sciences, Institute for Research in Fundamental Sciences (IPM)</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Iran</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046ak2485</institution-id><institution>Department of Education and Psychology, Freie Universität Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05vf56z40</institution-id><institution>School of Electrical and Computer Engineering, College of Engineering, University of Tehran</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Iran</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sbq1a82</institution-id><institution>Department of Psychological and Brain Sciences, University of Delaware</institution></institution-wrap><addr-line><named-content content-type="city">Newark</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>Laboratory of Brain and Cognition, National Institute of Mental Health</institution></institution-wrap><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University Nijmegen</institution></institution-wrap><country>Netherlands</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>16</day><month>12</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP89836</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-06-22"><day>22</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-06-05"><day>05</day><month>06</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.06.01.543205"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-09-12"><day>12</day><month>09</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89836.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-14"><day>14</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89836.2"/></event></pub-history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-89836-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-89836-figures-v1.pdf"/><abstract><p>Visual stimuli compete with each other for cortical processing and attention biases this competition in favor of the attended stimulus. How does the relationship between the stimuli affect the strength of this attentional bias? Here, we used functional MRI to explore the effect of target-distractor similarity in neural representation on attentional modulation in the human visual cortex using univariate and multivariate pattern analyses. Using stimuli from four object categories (human bodies, cats, cars, and houses), we investigated attentional effects in the primary visual area V1, the object-selective regions LO and pFs, the body-selective region EBA, and the scene-selective region PPA. We demonstrated that the strength of the attentional bias toward the target is not fixed but decreases with increasing target-distractor similarity. Simulations provided evidence that this result pattern is explained by tuning sharpening rather than an increase in gain. Our findings provide a mechanistic explanation for the behavioral effects of target-distractor similarity on attentional biases and suggest tuning sharpening as the underlying mechanism in object-based attention.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>object-based attention</kwd><kwd>tuning sharpening</kwd><kwd>target-distractor similarity</kwd><kwd>fMRI</kwd><kwd>MVPA</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>ZIA-MH002035</award-id><principal-award-recipient><name><surname>Vaziri-Pashkam</surname><given-names>Maryam</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Attentional modulation is affected by target-distractor similarity, indicating tuning sharpening as the underlying mechanism for response enhancement during object-based attention.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Everyday visual scenes typically contain a large number of stimuli. Since processing all the incoming information is impossible due to the brain’s limited neural resources, different stimuli compete for cortical representation and processing (<xref ref-type="bibr" rid="bib11">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib20">Kastner et al., 1998</xref>; <xref ref-type="bibr" rid="bib31">Reynolds et al., 1999</xref>; <xref ref-type="bibr" rid="bib3">Beck and Kastner, 2005</xref>; <xref ref-type="bibr" rid="bib30">Reddy et al., 2009</xref>; <xref ref-type="bibr" rid="bib26">McMains and Kastner, 2011</xref>). This competition can be biased by the top-down signal of attention to enhance the parts of the input that are most relevant to the task at hand (<xref ref-type="bibr" rid="bib27">Moran and Desimone, 1985</xref>; <xref ref-type="bibr" rid="bib11">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib31">Reynolds et al., 1999</xref>). Evidence from electrophysiology and fMRI studies have demonstrated the role of attention in biasing the competition by enhancing the response related to the attended stimulus by approximately 30% compared to its response when unattended, in both electrophysiology studies of the monkey brain (<xref ref-type="bibr" rid="bib38">Treue and Maunsell, 1996</xref>; <xref ref-type="bibr" rid="bib31">Reynolds et al., 1999</xref>; <xref ref-type="bibr" rid="bib40">Treue and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib32">Reynolds and Desimone, 2003</xref>; <xref ref-type="bibr" rid="bib15">Fallah et al., 2007</xref>) and fMRI studies of the human brain (<xref ref-type="bibr" rid="bib30">Reddy et al., 2009</xref>).</p><p>Competition and attentional bias likely depend on the nature of the visual scenes rather than being universally uniform. Behavioral studies indicate that the competition between stimuli is content-dependent (<xref ref-type="bibr" rid="bib5">Cohen et al., 2014</xref>), with higher competition between stimuli that are located closer to each other (<xref ref-type="bibr" rid="bib17">Franconeri et al., 2013</xref>), or between stimuli with more similar cortical representation patterns (<xref ref-type="bibr" rid="bib5">Cohen et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Cohen et al., 2017</xref>). This suggests that the attentional bias might also be affected by the relationship between the competing stimuli, such as the similarity of their cortical representation. Furthermore, behavioral studies on the effect of target-distractor similarity on performance have proposed that lower performance for more similar target-distractor pairs is due to the fact that the neural resources needed for detailed processing are shared to a greater extent (<xref ref-type="bibr" rid="bib5">Cohen et al., 2014</xref>). However, a direct neuroscientific investigation of how target-distractor similarity affects visual representations, and a mechanistic explanation of how shared resources affect attentional biases are missing.</p><p>Here, we investigated the impact of similarity in cortical representation on attentional bias and the underlying mechanism with empirical and theoretical tools. First, using functional MRI and uni- as well as multivariate analysis, we investigated how the top-down effect of attention varies as target-distractor similarity changes for multiple presented objects. Specifically, we found that the strength of the attentional bias towards the target decreases with increasing target-distractor similarity in cortical representation.</p><p>Second, using simulations of neuronal populations we determined how this effect arises from attentional enhancement of neural responses. We considered two known mechanisms through which attention affects neural firing rate: response gain and tuning sharpening. The response gain model predicts a multiplicative scaling of responses through which neural responses are increased by a gain factor (<xref ref-type="bibr" rid="bib25">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib33">Reynolds and Chelazzi, 2004</xref>). The tuning sharpening model, instead, proposes that attentional enhancement depends on the neuronal tuning for the attended stimulus, leading to an increase in response for optimal stimuli, and little increase in response or even response suppression for non-optimal stimuli (<xref ref-type="bibr" rid="bib24">Martinez-Trujillo and Treue, 2004</xref>). We find that the empirically-observed relationship between attentional enhancement and target-distractor similarity are predicted by the tuning sharpening model, but not the response gain model.</p><p>Together, our results show that attentional enhancement is dependent on the similarity between the target and the distractor in neural representation, and a more similar distractor causes the target to receive less attentional bias in the competition. Moreover, these results suggest tuning sharpening as the underlying mechanism of attentional enhancement during object-based attention.</p></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><sec id="s2-1"><title>Main experiment</title><sec id="s2-1-1"><title>Participants</title><p>17 healthy human participants (nine females, age: mean ± s.d.=29.29 ± 4.5 y) with normal or corrected-to-normal vision took part in the study. We estimated the number of participants conservatively based on the smallest amount of attentional modulation observed in our previous study (<xref ref-type="bibr" rid="bib12">Doostani et al., 2023</xref>). For a medium effect size of 0.3 and a power of 0.8, we needed a minimum number of 16 participants. Participants gave written consent and received payment for their participation in the experiment. Data collection was approved by the Ethics Committee of the Institute for Research in Fundamental Sciences, Tehran.</p><p>The behavioral data for two participants was not correctly saved during the scanning due to technical problems. While we used the fMRI data of these two participants, all behavioral reports include the performance of the 15 participants for whom the behavioral data was properly saved.</p></sec><sec id="s2-1-2"><title>Stimulus set and experimental design</title><p>To determine the effect of target-distractor similarity on attentional modulation, we used object stimuli from four categories (human bodies, cars, houses, and cats). We included body and house categories because there are regions in the brain that are highly responsive and unresponsive to each of these categories, which provided us with a range of responsiveness in the visual cortex. We chose the two remaining categories based on previous behavioral results to include categories that provided us with a range of similarities (<xref ref-type="bibr" rid="bib43">Xu and Vaziri-Pashkam, 2019</xref>). Thus, for each category there was a range of responsiveness in the brain and a range of similarity with the other categories.</p><p>We presented stimuli from each category in semi-transparent form, either in isolation (isolated conditions), or paired with stimuli from another category (paired conditions). Thus, the experiment consisted of 16 conditions: 4 isolated conditions in which isolated stimuli from one of the four categories were presented, and 12 paired conditions (six category pairs × two attentional targets for each pair) in which a target stimulus from the cued category was superimposed with a distractor stimulus from another category for all category combinations. <xref ref-type="fig" rid="fig1">Figure 1B</xref> depicts all stimulus conditions. We used isolated conditions to assess the similarity between different categories, and paired conditions to determine the effect of similarity in a category pair on attentional modulation.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Stimuli, paradigm, and regions of interest.</title><p>(<bold>A</bold>) Top images represent the four categories used in the main experiment: body, car, house, and cat. The stimulus set consisted of 10 exemplars from each category (here: cats), with exemplars differing in pose and 3D-orientation. (<bold>B</bold>) The experimental design comprised 16 task conditions (12 paired, four isolated). The 4×4 matrix on the left illustrates the 12 paired conditions, with the to-be-attended category (outlined in orange for illustration purposes, not present in the experiment) on the y-axis and the to-be-ignored category on the x-axis. The right column illustrates the four isolated conditions. (<bold>C</bold>) Experimental paradigm. A paired block is depicted with superimposed body and house stimuli. In this example block, house stimuli were cued as targets, and the participant responded on the repetition of the exact same house in two consecutive trials, as marked here by the arrow. (<bold>D</bold>) Regions of interest for an example participant; the primary visual cortex (V1), the object-selective regions lateral occipital cortex (LO) and posterior fusiform gyrus (pFs), the body-selective region extrastriate body area (EBA), and the scene-selective region parahippocampal place area (PPA).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Behavioral performance of the participants during the recording.</title><p>B, Cr, H, and Ct represent the Body, Car, House, and Cat categories, respectively. Each x-axis label represents the two conditions with stimuli from the same two categories but with different attentional targets, with the dark gray bar illustrating the results related to the condition in which the first category was attended, and the light gray bar representing the results in the condition in which the second category was attended. For instance, the B-Cr bars represent the results related to the <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> conditions, respectively. Error bars represent standard errors of the mean. N=15 human participants. (<bold>A</bold>) Average detection rate for each category pair. (<bold>B</bold>) Average reaction time for each category pair.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig1-figsupp1-v1.tif"/></fig></fig-group><p>The stimulus set consisted of gray-scaled images from the four object categories of human bodies, cats, cars, and houses, similar to stimuli used in previous studies (<xref ref-type="bibr" rid="bib41">Vaziri-Pashkam and Xu, 2017</xref>; <xref ref-type="bibr" rid="bib42">Vaziri-Pashkam and Xu, 2019</xref>; <xref ref-type="bibr" rid="bib43">Xu and Vaziri-Pashkam, 2019</xref>). Each category consisted of 10 exemplars all varying in identity, 3D-orientation (for houses and cars), and pose (for bodies and cats, see <xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><p>Images were presented at the center of the screen on a gray background, subtending 10.2<sup>o</sup> of visual angle. A red fixation point subtending 0.45<sup>o</sup> of visual angle was presented at the center of the screen throughout the run (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p></sec><sec id="s2-1-3"><title>Procedure</title><p>We used a blocked design for the main experiment. At the beginning of each block, participants were cued by a word to attend to either bodies, cars, houses, or cats. During the block, participants maintained attention on the images from the cued category, and performed a one-back repetition detection task on them by pressing the response button when the same stimulus from the attended category appeared in two consecutive trials. Repetition occurred two to three times at random times in each block. The experiment consisted of 16 block types, corresponding to the 16 task conditions (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>Each block lasted for 10 s, starting with the cue word presented for 1 s, followed by 1 s of fixation. Then, ten images from the cued category were presented in isolation or paired with ten images from another category. Each image was presented for 400 ms, followed by 400ms of fixation (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). There were 8 s of fixation in between the blocks, and a final 8 s fixation after the last block.</p><p>We organized blocks in runs, each lasting 4 min 56 s. Each run started with 8 s of fixation followed by block presentations. The presentation order of the 16 task conditions was counterbalanced across each experimental run. 10 participants completed 16 runs and 7 participants completed 12 runs of the main experiment.</p></sec></sec><sec id="s2-2"><title>Localizer experiments</title><p>Considering that we used object categories, we investigated five different regions of interest (ROIs): the object-selective areas lateral occipital cortex (LO) and posterior fusiform gyrus (pFs) as general object-selective regions, the body-selective extrastriate body area (EBA) and the scene-selective parahippocampal place area (PPA) as regions that are highly selective for specific categories, and the primary visual cortex (V1) as a control region. We chose these regions because they could all be consistently defined in both hemispheres of all participants and included a large number of voxels. To define these ROIs, each participant completed four localizer runs described in detail below.</p><sec id="s2-2-1"><title>Early visual area localizer</title><p>We used meridian mapping to localize the primary visual cortex V1. Participants viewed a black-and-white checkerboard pattern through a 60-degree polar angle wedge aperture. The wedge was presented either horizontally or vertically. Participants were asked to detect luminance changes in the wedge in a blocked-design paradigm. Each run consisted of four horizontal and four vertical blocks, each lasting 16 s, with 16 s of fixation in between. A final 16 s fixation followed the last block. Each run lasted 272 s. The order of the blocks was counterbalanced within each run. Participants completed two runs of this localizer.</p></sec><sec id="s2-2-2"><title>Category localizer</title><p>We used a category localizer to localize the cortical regions selective to scenes (PPA), bodies (EBA), and objects (LO, pFs). In a blocked-design paradigm, participants viewed stimuli from the five categories of faces, scenes, objects, bodies, and scrambled images. The stimuli differed from those used in the main experiment. Each localizer run contained two 16 s blocks of each category, with the presentation order counterbalanced within each run. An 8 s fixation period was presented at the beginning, in the middle, and at the end of the run. In each block, 20 stimuli from the same category were presented. Stimuli were presented for 750 ms followed by 50 ms of fixation on a gray background screen. Participants were asked to maintain their fixation on a red circle at the center of the screen throughout and press a key when they detected a slight jitter in the stimuli that happened two to three times per block. Each run lasted 344 s. Participants completed two runs of this localizer.</p></sec></sec><sec id="s2-3"><title>Stimulus presentation inside the scanner</title><p>We back-projected the stimuli onto a screen positioned at the rear of the magnet using an LCD projector with a refresh rate of 60 Hz and a spatial resolution of 768 × 1024. Participants observed the screen through a mirror attached to the head coil.</p></sec><sec id="s2-4"><title>MRI data acquisition</title><p>We recorded the data of 10 participants using the Siemens 3T Tim Trio MRI system with a 32-channel head coil at the Institute for Research in Fundamental Sciences (IPM). We collected the data of seven additional participants on a Siemens Prisma MRI system using a 64-channel head coil at the National Brain-mapping Laboratory (NBML). For each participant, we performed a whole-brain anatomical scan using a T1-weighted MPRAGE sequence. For the functional scans, including the main experiment and the localizer experiments, we acquired 33 slices parallel to the AC-PC line using T2<sup>*</sup>-weighted gradient-echo echo-planar imaging (EPI) sequences covering the whole brain (TR=2 s, TE=30 ms, flip angle=90<sup>o</sup>, voxel size=3 × 3 × 3 mm<sup>3</sup>, matrix size = 64 × 64).</p></sec><sec id="s2-5"><title>fMRI data preprocessing</title><p>We performed fMRI data analysis using FreeSurfer (<ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu">https://surfer.nmr.mgh.harvard.edu</ext-link>), Freesurfer Functional Analysis Stream (<xref ref-type="bibr" rid="bib9">Dale et al., 1999</xref>) and in-house MATLAB codes. fMRI data preprocessing steps included 3D motion correction, slice timing correction, and linear and quadratic trend removal. We performed no spatial smoothing on the data. We used a double gamma function to model the hemodynamic response function. We eliminated the first four volumes (8 s) of each run to avoid the initial magnetization transient. We compared the SNR values of the two groups of participants and observed no significant difference between these values (<inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.34</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.97</mml:mn></mml:mstyle></mml:math></inline-formula>).</p></sec><sec id="s2-6"><title>fMRI data analysis</title><p>For the main experiment, we performed a general linear model (GLM) analysis for each participant to estimate voxel-wise regression coefficients in each of the 16 task conditions. The onset and duration of each block were convolved with a hemodynamic response function and were then entered into the GLM as regressors. We also included movement parameters and linear and quadratic nuisance regressors in the GLM. We did not enter the cue to the GLM as a predictor. The obtained voxel-wise coefficients for each condition are thus related to the cue and the stimuli presented in that condition. We used these voxel-wise coefficients from the five ROIs as the basis for all further analyses.</p><p>For the early visual area localizer experiment, we estimated voxel regression coefficients in each of the two conditions (i.e. vertical and horizontal wedge) using a separate GLM. After convolving with a hemodynamic response function, the onset and duration of each block were entered to the GLM as regressors of interest. We also included movement parameters and linear and quadratic nuisance regressors in the GLM. We used the obtained coefficients to define the V1 ROI.</p><p>For the category localizer, we used another GLM to estimate voxel-wise regression coefficients in the five task conditions (i.e. faces, scenes, objects, bodies, and scrambled images). The GLM procedure was similar to the other two experiments. We then used these estimated coefficients to define the LO, pFs, EBA, and PPA ROIs.</p><sec id="s2-6-1"><title>Definition of ROIs</title><p>We determined the V1 ROI using a contrast of horizontal versus vertical polar angle wedges that reveals the topographic maps in the occipital cortex (<xref ref-type="bibr" rid="bib34">Sereno et al., 1995</xref>; <xref ref-type="bibr" rid="bib37">Tootell et al., 1998</xref>). To define the object-selective areas LO in the lateral occipital cortex and pFs in the posterior fusiform gyrus (<xref ref-type="bibr" rid="bib23">Malach et al., 1995</xref>; <xref ref-type="bibr" rid="bib19">Grill-Spector et al., 1998</xref>), we used a contrast of objects versus scrambled images. We selected the active voxels in the lateral occipital and ventral occipitotemporal cortex as LO and pFS, respectively, following the procedure described by <xref ref-type="bibr" rid="bib21">Kourtzi and Kanwisher, 2000</xref>. We used a contrast of scenes versus objects for defining the scene-selective area PPA in the parahippocampal gyrus (<xref ref-type="bibr" rid="bib14">Epstein et al., 1999</xref>), and a contrast of bodies versus objects for defining the body-selective area EBA in the lateral occipitotemporal cortex (<xref ref-type="bibr" rid="bib13">Downing et al., 2001</xref>). We thresholded the activation maps for both the early visual localizer and the category localizer at <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math></inline-formula>, uncorrected. We selected this threshold to allow for the selection of a reasonable number of voxels in each hemisphere across all participants.</p></sec><sec id="s2-6-2"><title>Univariate fMRI analysis</title><p>We first used a univariate analysis to determine the effect of attention for different category pairs. Using the voxel-wise coefficients of the isolated conditions associated with each category, we examined the relative response of each voxel to the two categories for each category pair. This relative response determined which of the two categories was more preferred by the voxel. Therefore, for each category pair and each voxel, the category that elicited a higher response in the isolated condition was assigned the relatively more preferred category (<inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula>) label and the other the relatively less preferred category (<inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>) label.</p><sec id="s2-6-2-1"><title>Univariate distance based on the isolated conditions</title><p>We had six pairs of categories: Body-Car, Body-House, Body-Cat, Car-House, Car-Cat, and House-Cat. As a measure of the difference between the response evoked by each of the two categories in a pair, we defined a univariate distance. We calculated the univariate distance for each pair of categories simply as the difference in voxel responses of the two isolated conditions (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi></mml:mstyle></mml:math></inline-formula> denotes the average voxel response across runs, and the subscripts <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> denote the presence of the more preferred and the less preferred stimuli, respectively. The superscript <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> denotes the attended stimulus. Note that in the isolated conditions, the presented stimulus was always attended. Thus, <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the average response related to the isolated preferred stimulus, and <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the average response to the isolated less preferred stimulus. For example, the Body-Car univariate distance was assessed by <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> for voxels more responsive to bodies than cars, and by <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> for voxels more responsive to cars than bodies. Thus, according to this measure, two categories that elicited closer responses had less univariate distance, indicating more similarity in univariate response between the two categories.</p></sec><sec id="s2-6-2-2"><title>Univariate effect of attention based on the paired conditions (Univariate shift)</title><p>For each of the six category pairs, we had two paired conditions, in which stimuli from both categories were presented, but with attention directed to either one or the other category (for example, <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> conditions for the Body-Car pair, with the superscript <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the to-be-attended stimulus). Since these paired conditions differed only in the attentional target and not in the stimuli, any difference observed in cortical response can be ascribed to the shift in attention (<xref ref-type="bibr" rid="bib29">O’Craven et al., 1999</xref>; <xref ref-type="bibr" rid="bib28">Ni et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Vaziri-Pashkam and Xu, 2017</xref>; <xref ref-type="bibr" rid="bib12">Doostani et al., 2023</xref>). We thus defined the univariate shift for each pair of categories as the change in response when attention shifted from the more preferred stimulus to the other:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> denotes the response related to the paired condition with attention directed to the more preferred category, while <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the elicited response when attending to the less preferred category in the pair. For example, considering the Body-Car pair, we assessed the univariate shift by <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> for a voxel preferring bodies to cars, and by <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> for a voxel preferring cars to bodies.</p></sec></sec></sec><sec id="s2-7"><title>Multivariate pattern analysis</title><p>To determine the effect of attention at the multivariate level, and to examine the attentional bias that the representation of the stimulus in each category pair receives, we used a multivariate pattern analysis. Here, rather than comparing the mean values of voxel-wise coefficients in each ROI, we instead considered the ROI response pattern in each condition as a response vector, with the voxel-wise coefficients as its elements. Therefore, we had 16 response vectors, one for each task condition, in each ROI. Similar to the univariate analysis, we used the responses in the isolated conditions to assess category distance, and the responses in the paired conditions to evaluate the effect of attention.</p><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> illustrates the response vectors for two stimulus categories (here termed <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>) in the isolated conditions, as well as the response vector to the paired condition with attention directed to <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>. The three vectors <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> illustrate the response patterns of these three conditions, with <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi></mml:mstyle></mml:math></inline-formula> representing the response vector in an ROI, subscripts <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the presence of the <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> stimuli, respectively, and the superscript <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the attended stimulus. Therefore, <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represents the response vector related to the isolated <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> condition (in which <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> was automatically attended), and <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represents the response vector related to the paired <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> condition with attention directed to the <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> stimulus. The projection of the paired-condition vector <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> onto the plane defined by the two isolated responses <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is illustrated as <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. Using this projection vector, we calculate the weight of <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> in the paired response.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Response vectors related to <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> stimuli in isolated and paired conditions.</title><p>(<bold>A</bold>) Example illustration of the isolated and paired responses in three-dimensional space. <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> denote the response vectors related to isolated <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and isolated <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> conditions and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> illustrates the response in the paired condition with attention directed to stimulus <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>. The paired response is projected on the plane defined by the two isolated responses <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. This projection is illustrated by <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. (<bold>B</bold>) Two-dimensional illustration of the plane defined by the two isolated response vectors <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, along with the paired response vectors <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>P</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> as the projection of the paired response, <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, on the plane. We calculated the weight of the two isolated responses in the paired response using multiple regression, with the weights of <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> shown as <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig2-v1.tif"/></fig><sec id="s2-7-1"><title>Multivariate distance based on the isolated conditions</title><p>As illustrated in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, the two isolated response vectors <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> have a certain distance because the response across the voxels varies for the two stimuli. For two stimuli that elicit more similar response patterns in an ROI, the isolated response vectors are closer to each other. Thus, we defined the multivariate distance between the two isolated response vectors <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> in each ROI using Pearson’s correlation, as shown in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> represent the response vectors related to the isolated <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> conditions and ρ denotes Pearson’s correlation coefficient between the two response vectors. For stimuli with more similar response patterns, the correlation between their response vectors will be higher, leading to lower multivariate distance.</p></sec><sec id="s2-7-2"><title>Multivariate effect of attention based on the paired conditions (Attentional weight shift)</title><p>Similar to the isolated conditions, we considered the response pattern in the paired conditions as vectors, <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. We first projected the paired vectors on the plane defined by the isolated vectors (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and then determined the weight of each isolated vector in the projected vector (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Thus, the response vectors in the paired conditions can be written as the linear combination of the response vectors in the isolated conditions, with an error term denoting the deviation of the paired-condition responses from the plane defined by the isolated-condition responses (<xref ref-type="bibr" rid="bib30">Reddy et al., 2009</xref>), as shown in <xref ref-type="disp-formula" rid="equ4">Equation 4a</xref>:<disp-formula id="equ4"><label>(4a)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><label>(4b)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, parameters <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the weights of the isolated <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> responses, respectively, when <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> is attended, and parameters <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the respective weights of isolated <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> responses when <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> is attended. <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> denote the error terms related to the deviation of the <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> from the <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> plane, respectively. While this model has been previously called <italic>weighted average</italic> (<xref ref-type="bibr" rid="bib30">Reddy et al., 2009</xref>), we chose the more general term <italic>linear combination</italic> because we did not impose any limits on the estimated weights of the two isolated responses in the paired response.</p><p>A higher <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> compared to <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> indicates that the paired response pattern is more similar to <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> than to <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and vice versa. For instance, after calculating the weights of the Body and Car stimuli in the paired response related to the simultaneous presentation of both stimuli in the LO ROI, we obtain: <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.79</mml:mn><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.31</mml:mn><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.43</mml:mn><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.68</mml:mn><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> (See <xref ref-type="table" rid="app1table4">Appendix 1—table 4</xref> for the average weights of the two stimuli for all pairs in all ROIs). Note that these weights are averaged across participants. As can be observed, in the presence of both body and car stimuli, the weight of each stimulus is higher when attended compared to the case when it is unattended. In other words, when attention shifts from body to car stimuli, the weight of the isolated body response (<inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) decreases in the paired response. We can, therefore, observe in this instance that the response in the paired condition is more similar to the isolated body response pattern when body stimuli are attended and more similar to the isolated car response pattern when car stimuli are attended.</p><p>In the presence of two stimuli, if attention could completely remove the effect of the unattended stimulus, the paired response would be the same as the response to the isolated attended stimulus. However, the information related to the unattended stimulus is not fully removed and attention has been shown to increase the weight of the response related to the attended stimulus in the paired response without completely removing the effect of the unattended stimulus (<xref ref-type="bibr" rid="bib30">Reddy et al., 2009</xref>), as observed in the above example of the Body-Car pair. As shown here, even when body stimuli were attended, the effect of the unattended car stimuli was still present in the response, shown in the weight of the isolated car response (0.31). This weight increased when attention shifted towards car stimuli to a value of less than 1 (0.68 in the attended case), showing the effect of attention on the response. To examine whether this increase in the weight of the attended stimulus is constant or if it depends on the similarity of the two stimuli in cortical representation, we defined the weight shift as the multivariate effect of attention:<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the weights of the isolated responses, estimated using <xref ref-type="disp-formula" rid="equ4">Equation 4a</xref>. We calculate the weight of the isolated <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> response once when attention is directed towards <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>), and a second time when attention is directed towards <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>). In each case, we calculate the relative weight of the isolated <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> in the paired response by dividing the weight of the isolated <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> by the sum of weights of <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> when attention is directed towards <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> when attention is directed towards <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>). We then define the weight shift, <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula>, as the change in the relative weight of the isolated <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> response in the paired response when attention shifts from <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>. A higher <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>w</mml:mi></mml:mstyle></mml:math></inline-formula> for a category pair indicates that attention is more efficient in removing the effect of the unattended stimulus in the pair. We used relative weights as a normalized measure to compensate for the difference in the sum of weights for different category pairs. Thus, using the normalized measure, we calculated the share of each stimulus in the paired response. For instance, considering the Body-Car pair, the share of the body stimulus in the paired response was equal to 0.72 and 0.38, when body stimuli were attended and unattended, respectively. We then calculated the change in the share of each stimulus caused by the shift in attention using a simple subtraction (<xref ref-type="disp-formula" rid="equ6">Equation 5</xref>: <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mn>0.34</mml:mn></mml:mstyle></mml:math></inline-formula> for the above example of the Body-Car pair in LO) and used this measure to compare between different pairs.</p></sec></sec><sec id="s2-8"><title>Simulations</title><p>We investigated the mechanisms underlying the observed effect of stimulus similarity on attentional enhancement using simulations. To examine which attentional mechanism leads to the effects observed in the empirical data, we generated the neural response to unattended object stimuli as a baseline response in the absence of attention, using the data reported by neural studies of object recognition in the visual cortex (<xref ref-type="bibr" rid="bib28">Ni et al., 2012</xref>; <xref ref-type="bibr" rid="bib1">Bao and Tsao, 2018</xref>). Then, using an attention parameter for each neuron and different attentional mechanisms, we simulated the response of each neuron to the different task conditions in our experiment. Finally, we assessed the population response by averaging neural responses. We considered two models for attentional enhancement: a response gain model (<xref ref-type="bibr" rid="bib25">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib33">Reynolds and Chelazzi, 2004</xref>) and a tuning sharpening model (<xref ref-type="bibr" rid="bib24">Martinez-Trujillo and Treue, 2004</xref>; <xref ref-type="bibr" rid="bib22">Ling et al., 2009</xref>).</p><p>According to the response gain model, attention to an object multiplicatively increases neural responses to that object (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). For instance, for a body-selective neuron, this mechanism can be implemented using <xref ref-type="disp-formula" rid="equ7">Equation 6a</xref>:<disp-formula id="equ7"><label>(6a)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(6b)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Attentional modulation by the response gain and tuning sharpening models.</title><p>We illustrate the models here for the example of a neuron with high selectivity for cat stimuli. Solid curves denote the response to unattended stimuli and dashed curves denote the response to attended stimuli. (<bold>A</bold>) According to the response gain model, the response of the neuron to attended stimuli is scaled by a constant attention factor. Therefore, the response of the cat-selective neuron to an attended stimulus is enhanced to the same degree for all stimuli. (<bold>B</bold>) According to the tuning sharpening model, the response modulation by attention depends on the neuron’s tuning for the attended stimulus. Therefore, for optimal and near-optimal stimuli such as cat and body stimuli the response is highly increased, while for non-optimal stimuli such as houses, the response is suppressed.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig3-v1.tif"/></fig><p>Here, <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the neuron’s response to an ignored body stimulus, and <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the response of the neuron to the attended body stimulus, which is enhanced by the attention factor, β and <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ8">Equation 6b</xref> denote the response of the same body-selective neuron to an ignored and an attended car stimulus, respectively. The response gain model posits that attention to either stimulus enhances the response of the neuron by the same attention factor. This multiplicative scaling preserves the shapes of the neurons’ tuning curves (<xref ref-type="bibr" rid="bib39">Treue and Martínez Trujillo, 1999</xref>; <xref ref-type="bibr" rid="bib25">McAdams and Maunsell, 1999</xref>).</p><p>In contrast, according to the tuning sharpening model, attention to an object increases neural responses relative to their responsiveness to that object (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Therefore, while the response of a neuron is substantially enhanced when an optimal stimulus is attended, its response to an attended non-optimal stimulus is increased to a lesser degree or even decreased. The tuning sharpening model thus predicts a sharpening of the neurons’ tuning curve with attention (<xref ref-type="bibr" rid="bib22">Ling et al., 2009</xref>).</p><p>We implemented this mechanism using <xref ref-type="disp-formula" rid="equ9">Equation 7a</xref>:<disp-formula id="equ9"><label>(7a)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><label>(7b)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><label>(7c)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ12"><label>(7d)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In the above equations, <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> , and <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> denote the neuron’s response to the unattended body, unattended car, attended body, and attended car stimuli, respectively. Parameters <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> denote the degree of the neuron’s selectivity to body and car stimuli, respectively. Parameter β is the attention factor. <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the response of the neuron to its optimal stimulus.</p><p>We simulated the action of the response gain model and the tuning sharpening model using numerical simulations. We composed a neural population of <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> neurons in equal proportions body-, car-, cat- or house-selective. Each neuron also responded to object categories other than its preferred category, but to a lesser degree and with variation. We chose neural responses to each stimulus from a normal distribution with the mean of 30 spikes/s and a standard deviation of 10 and each neuron was randomly assigned an attention factor in the range between 1 and 10 using a uniform distribution. These values are comparable with the values reported in neural studies of attention and object recognition in the ventral visual cortex (<xref ref-type="bibr" rid="bib28">Ni et al., 2012</xref>; <xref ref-type="bibr" rid="bib1">Bao and Tsao, 2018</xref>). We also added Poisson noise to the response of each neuron (<xref ref-type="bibr" rid="bib4">Britten et al., 1993</xref>), assigned randomly for each condition of each neuron.</p><p>Attention was implemented according to the above equations. Using <xref ref-type="disp-formula" rid="equ7 equ9">Equations 6a and 7a</xref>, we calculated the response of each neuron to the same 16 conditions as our main fMRI experiment. Then, we randomly chose 1000 neurons with similar selectivity from the population, and averaged their responses to make up a voxel.</p><p>We modeled two neural populations: a general object-selective population in which each voxel shows preference to a particular category and voxels with different preferences are mixed in with each other (similar to LO and pFS), and a category-selective population in which all voxels have a similar preference for a particular category (similar to EBA and PPA). Finally, we performed the same univariate and multivariate analyses as those used for the fMRI data to compare the predictions of each model with the observed data.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><sec id="s3-1"><title>Behavioral results</title><p>Participants performed a one-back repetition detection task to maintain attention toward the cued stimuli. Detection rate in each experimental run was checked during the scan to ensure that participants followed the instructions. Participants had an average detection rate of 90.49% across all runs, confirming effective attention towards the cued stimuli (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). As expected, the average detection rate in the isolated conditions (94.82% ± 0.046) was significantly higher than in the paired conditions (89% ± 0.07, with <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>14</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>7.2</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mstyle></mml:math></inline-formula>), since detecting a repetition in the superimposed case was more difficult.</p></sec><sec id="s3-2"><title>The effect of attention varies dependent on the target-distractor difference in response</title><p>We considered the effect of attention in five ROIs: the primary visual cortex V1, the object-selective regions LO and pFs, the body-selective region EBA, and the scene-selective region PPA. We obtained the voxel-wise responses through a GLM in those ROIs for all task conditions, consisting of four isolated conditions (blocks with isolated stimuli from one category) and 12 paired conditions (blocks with superimposed stimuli from two categories, see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). There were six combinations of category pairs: Body-Car, Body-House, Body-Cat, Car-House, Car-Cat and House-Cat. For each voxel, we determined its relative preference for the two categories of each category pair, based on its response to the two categories in isolation. Thus, for each pair, one category was labeled as the more preferred category (<inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula>), and the other as the less preferred category (<inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>). Considering the isolated and paired conditions related to each category pair, we hereafter refer to the conditions related to each category pair as <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the more preferred and the less preferred categories for each voxel, and the superscript <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the attended stimulus.</p><p>For instance, for the Body-Car pair, for a voxel that showed a higher response to body stimuli than to car stimuli, the four associated conditions related to the pair were referred to as <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (attended body stimuli), <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> (attended body stimuli paired with ignored car stimuli), <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (attended car stimuli paired with ignored body stimuli), and <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (attended car stimuli). If the same voxel was more responsive to cats than bodies, then the four conditions related to the Body-Cat pair would be referred to as: <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (attended cat stimuli), <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> (attended cat stimuli paired with ignored body stimuli), <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (attended body stimuli paired with ignored cat stimuli), and <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (attended body stimuli).</p><p>Note that the response in paired conditions can be higher or lower than the response to the isolated more preferred stimulus (condition <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>), depending on the voxel response to the two presented stimuli (see <xref ref-type="fig" rid="fig4">Figure 4C–D</xref>), as previously reported (<xref ref-type="bibr" rid="bib12">Doostani et al., 2023</xref>). This is consistent with previous studies reporting the response to multiple stimuli to be higher than the average, but lower than the sum of the response to isolated stimuli (<xref ref-type="bibr" rid="bib30">Reddy et al., 2009</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Average voxel response in the extastriate body area (EBA) for each pair of stimulus categories.</title><p>The x-axis labels represent the four conditions related to each category pair, <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the presence of the more preferred and the less preferred category and the superscript <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the attended category. For instance, <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> refers to the condition in which the more preferred stimulus was presented in isolation (and automatically attended), and <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> refers to the paired condition in which the less preferred stimulus was attended to. Red arrows in each panel illustrate the observed change in response (univariate shift) caused by the shift of attention from the more preferred to the less preferred stimulus. Green arrows in panels B and C illustrate the difference in the response to isolated stimuli. Error bars represent standard errors of the mean. N=17 human participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Average voxel response in the primary visual cortex (V1) for each pair of stimulus categories.</title><p>The x-axis labels represent the four conditions related to each category pair, <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the presence of the more preferred and the less preferred category and the superscript <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the attended category. For instance, <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> refers to the condition in which the more preferred stimulus was presented in isolation (and automatically attended), and <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> refers to the paired condition in which the less preferred stimulus was attended to.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Average voxel response in the lateral occipital cortex (LO) for each pair of stimulus categories.</title><p>The x-axis labels represent the four conditions related to each category pair, <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the presence of the more preferred and the less preferred category and the superscript <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the attended category.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Average voxel response in the posterior fusiform gyrus (pFs) for each pair of stimulus categories.</title><p>The x-axis labels represent the four conditions related to each category pair, <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the presence of the more preferred and the less preferred category and the superscript <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the attended category.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Average voxel response in the parahippocampal place area (PPA) for each pair of stimulus categories.</title><p>The x-axis labels represent the four conditions related to each category pair, <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the presence of the more preferred and the less preferred category and the superscript <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the attended category.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig4-figsupp4-v1.tif"/></fig></fig-group><p>We next determined the amount of univariate shift for each category pair using the voxel-wise coefficients related to the two paired conditions, <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. As illustrated in <xref ref-type="fig" rid="fig4">Figure 4</xref>, we defined univariate shift for each category pair as the reduction in response when attention shifted from the <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> category to the <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> category in the presence of both stimuli (<xref ref-type="bibr" rid="bib29">O’Craven et al., 1999</xref>; <xref ref-type="bibr" rid="bib28">Ni et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Vaziri-Pashkam and Xu, 2017</xref>; <xref ref-type="bibr" rid="bib12">Doostani et al., 2023</xref>).</p><p>We observed a significant univariate shift when attention shifted from the <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> stimulus to the <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> stimulus for all pairs in the higher-level ROIs (<inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.04</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mstyle></mml:math></inline-formula>) except for the Body-Car, Body-Cat, and Car-Cat pairs in PPA (<inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mstyle></mml:math></inline-formula>) and the Car-House pair in EBA (<inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mstyle></mml:math></inline-formula>). In V1, we observed no significant univariate shift for any pairs (<inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>2.5</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mstyle></mml:math></inline-formula>) except for the Body-Car pair (<inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>3.8</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mstyle></mml:math></inline-formula>). Thus, the observed effect was limited to higher-level visual areas. Since the presented stimuli were the same in both conditions, this effect is caused by the shift in attention. It is important to note that since the cue was not separately modeled in the GLM, the signals related to the cue and the stimuli were mixed. However, given that the cues were brief and presented in the form of words, they are unlikely to have an effect on the responses observed in the higher-level ROIs.</p><p>Closer comparison of the results suggests that for pairs with significant univariate shift, the shift is not uniform. Instead, it is greater for pairs in which the <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> stimuli elicited more different responses compared to pairs with <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> stimuli eliciting closer responses. For example, we observed a larger univariate shift for the Body-House pair (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) compared to the Body-Cat pair (<xref ref-type="fig" rid="fig4">Figure 4C</xref>) in all ROIs (<inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math></inline-formula>, <xref ref-type="fig" rid="fig4">Figure 4B–C</xref>, compare the size of the red arrows) except for V1 (<inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.65</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mstyle></mml:math></inline-formula>). Comparing the isolated responses for these two pairs, we observed that the difference between the response of the isolated Body and isolated House conditions was generally higher than the difference between the isolated Body and isolated Cat conditions in all ROIs (<inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mstyle></mml:math></inline-formula>, <xref ref-type="fig" rid="fig4">Figure 4B–C</xref>, compare the size of the green arrows).</p><p>To examine this relationship quantitatively for all category pairs, we used two approaches. First, in a univariate analysis using average voxel responses, we determined the relationship between the observed univariate shift and the difference in isolated responses. Next, in a multivariate pattern analysis, we considered the response patterns in each ROI and looked for the underlying basis of this effect of attention at the multivariate level. This analysis enabled us to determine whether the bias of attention on the representation of the attended stimulus differed for different category pairs.</p></sec><sec id="s3-3"><title>The univariate effect of attention decreases for target-distractor pairs that elicit closer responses</title><p>We first used a univariate analysis to determine the relationship between the univariate shift and category distance across pairings and in different ROIs. We split the fMRI data into two halves. Using the first half, we determined the voxel-wise <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> categories for each category pair. We then calculated the difference in the isolated response elicited by the two categories (univariate category distance) using the two isolated conditions <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>).</p><p>Then, using the second left-out part of the data, we assessed the univariate shift related to the pair as the amount of the reduction in response when attention shifted from the <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> stimulus to the <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> stimulus in the paired presentation of both stimuli (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). For instance, for the Body-Car pair and a voxel more responsive to bodies than cars, univariate category distance was calculated by <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and univariate shift was calculated by <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p><p>We observed a significantly positive correlation between univariate shift and category distance in all ROIs (<inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2.5</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.02</mml:mn></mml:mstyle></mml:math></inline-formula>) except V1 (<inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.56</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.58</mml:mn></mml:mstyle></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig5">Figure 5</xref>). These results demonstrate that for stimuli that elicit more different responses, attention causes a greater response modulation, while the shift of attention between stimuli with more similar responses causes little response change. This indicates that the amount of univariate shift is related to the response difference between the two presented stimuli.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Univariate shift versus category distance in each region of interest (ROI).</title><p>(<bold>A–E</bold>) The value of univariate shift versus category distance. <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> denote the two paired conditions with attention directed to the more preferred (<bold>M</bold>) or less preferred (<bold>L</bold>) stimulus, respectively. <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> represent the isolated conditions, respectively, with the more preferred or the less preferred stimulus presented in isolation. The blue, red, yellow, purple, green, and sky blue circles in each panel represent the values related to the Body-Car, Body-House, Body-Cat, Car-House, Car-Cat, and House-Cat pairs, respectively. The correlation coefficients in each ROI were calculated for single participants and the lines in the average plots are only shown for illustration purposes. (<bold>F</bold>) Correlation coefficient for the correlation between the univariate shift and category distance in each ROI. Asterisks indicate that the correlation coefficients are significantly positive (<inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Error bars represent standard errors of the mean. N=17 human participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig5-v1.tif"/></fig></sec><sec id="s3-4"><title>The multivariate effect of attention decreases for more similar target-distractor pairs</title><p>The univariate analysis above considers average response only and thus cannot capture other aspects of response variance. For example, in an object-selective region with diverse selectivity for different objects, the average response to body and house stimuli is close, but the response pattern may be very different since voxels highly responsive to bodies do not show high responses to houses, and vice versa. Thus, we had to consider voxel preferences in the univariate analysis to observe the difference in response between the two categories. Furthermore, although the paired responses can be greater than responses to both isolated conditions (<xref ref-type="fig" rid="fig4">Figure 4C–D</xref>), there is still the possibility that the univariate shift is limited by the amount of the difference between isolated-condition responses for each category pair.</p><p>We complement the univariate approach with a multivariate pattern analysis to assess the relationship between the effect of attention and category distance at the multivariate level. By considering the whole response pattern in an ROI to each stimulus, we can compare the responses to each stimulus without considering voxel preferences. Moreover, using this method we can determine the weight of the response to each isolated stimulus in the total response, and determine the attentional bias related to each category pair.</p><p>The multivariate representation of two simultaneously-presented stimuli can be modeled as the linear combination of the representations of the two stimuli presented in isolation (<xref ref-type="bibr" rid="bib30">Reddy et al., 2009</xref>): When one stimulus is attended, the weight of the response to that stimulus increases in the multivariate representation.</p><p>Taking this approach, for each category pair (e.g. Body-Car), we considered the multivariate representation of the two paired conditions (<inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, with <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi></mml:mstyle></mml:math></inline-formula> denoting the multivariate response pattern of each condition), and determined the weight of each of the isolated-stimulus responses (<inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) in the paired response (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). We then calculated the difference between the weight of each stimulus when it was the target and when it was the distractor (e.g. for the Body-Car pair, the difference between the weight of <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> in <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>).</p><p>If attention could perfectly remove the effect of the distractor, the weight of the attended stimulus would equal one and the representation of the pair would be identical to the representation of the isolated target. In this case, the difference between the weight of the stimulus representation when attended and ignored would be a maximal value of one. However, if the distractor is not completely removed, this leads to a weight shift value smaller than one. Thus, the magnitude of the weight shift is an indicator of the efficiency of attention, with greater values indicating a higher efficiency of attention in removing the distractor.</p><p>To compare the efficiency of attention across category pairs, we calculated the weight shift for each category pair (<xref ref-type="disp-formula" rid="equ6">Equation 5</xref>). Similar to the univariate analysis, we took a cross-validation approach and used one-half of the data to calculate the weight shift. Then, to determine whether this multivariate effect of attention was dependent on the similarity between the target and the distractor in their cortical representation, we calculated the multivariate category distance for each category pair using the second left-out half of the data (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>).</p><p>As illustrated in <xref ref-type="fig" rid="fig6">Figure 6A–E</xref>, we observed that the attentional weight shift was not constant for different category pairs, and that weight shift and category distance correlated positively in LO, pFs, and EBA (<inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>4.4</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>), marginally significantly in PPA (<inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1.8</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.09</mml:mn></mml:mstyle></mml:math></inline-formula>), and not in V1 (<inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.42</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.68</mml:mn></mml:mstyle></mml:math></inline-formula>). Less significant results in PPA might arise from the fact that this region shows no response to body and cat stimuli and little response to car stimuli (see <xref ref-type="table" rid="app1table2">Appendix 1—table 2</xref>). Therefore, it is not possible to observe the effect of attention for all category pairs in PPA.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Weight shift versus category distance in each region of interest (ROI).</title><p>(<bold>A–E</bold>) Attentional weight shift versus category distance. The blue, red, yellow, purple, green, and sky blue circles in each panel represent the values related to the Body-Car, Body-House, Body-Cat, Car-House, Car-Cat, and House-Cat pairs, respectively. We calculated the correlation coefficients for single participants and the lines in these average plots are only for illustration purposes. (<bold>F</bold>) Correlation coefficient for the correlation between attentional weight shift and category distance in each ROI. Asterisks indicate that the correlations are significantly positive (<inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Error bars represent standard errors of the mean. N=17 human participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Sum of weights in the multivariate analysis for each category pair in each region of interest (ROI), averaged across conditions with attention directed to each of the two categories of a pair and across participants.</title><p>For instance, the blue Body-Car bars represent the average results related to the <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> conditions, averaged across participants. Error bars represent standard errors of the mean. N=17 human participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig6-figsupp1-v1.tif"/></fig></fig-group><p>We performed the analysis including only voxels that had a significantly positive GLM coefficient across the runs and observed the same results. Moreover, to check whether the effect is robust over more selective thresholds for ROI definition, we redefined the left EBA region with <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.00001</mml:mn></mml:mstyle></mml:math></inline-formula> criteria. We observed a similar weight shift effect for both criteria. We also calculated category distance based on the euclidean distance between response patterns of category pairs and observed a similarly positive correlation between the weight shift and the euclidean category distance in all ROIs (<inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2.9</mml:mn></mml:mstyle></mml:math></inline-formula>) except V1 (<inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.66</mml:mn></mml:mstyle></mml:math></inline-formula>). These results are in agreement with our main multivariate results, indicating that the attentional bias towards a stimulus in a pair decreases as the similarity between the two stimuli in neural representation increases.</p></sec><sec id="s3-5"><title>Tuning sharpening predicts the dependence of attentional modulation on target-distractor similarity</title><p>We observed empirically that attentional enhancement is not constant and content-independent, but rather depends on the response similarity between the target and the distractor. We next asked whether a gain increase or tuning changes predict the observed effect of target-distractor similarity on the attentional bias.</p><p>Based on the response gain model, attention increases neural responses by scaling the responses by a constant attention factor (<xref ref-type="bibr" rid="bib25">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib33">Reynolds and Chelazzi, 2004</xref>). Therefore, the response gain model predicts that attention scales the neurons’ tuning function without affecting its shape (<xref ref-type="fig" rid="fig3">Figure 3A</xref>).</p><p>In contrast, the tuning sharpening model proposes that attention enhances the response of each neuron based on its preference for the attended stimulus (<xref ref-type="bibr" rid="bib24">Martinez-Trujillo and Treue, 2004</xref>; <xref ref-type="bibr" rid="bib22">Ling et al., 2009</xref>). Therefore, this model predicts that attention causes a sharpening of the neurons’ tuning function, with a sharp increase in the response to optimal stimuli, and no increase in the response to the non-optimal stimuli (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><p>To examine which of these mechanisms could account for the observed results, we simulated the responses of a neural population to isolated and paired stimuli from the four categories of bodies, cars, houses, and cats. Equivalent to the fMRI experiment, we determined neuronal responses to stimuli presented either in isolation or paired with stimuli from another category (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). We implemented attentional enhancement of the neural responses either using the response gain model (<xref ref-type="disp-formula" rid="equ7">Equation 6a</xref>), or the tuning sharpening model (<xref ref-type="disp-formula" rid="equ9">Equation 7a</xref>). We then used the univariate and multivariate analyses equivalent to those used for the fMRI data to determine which model predicts the empirical data.</p><p>We created two neural populations: (i) a population with varying selectivity across neurons, representing object-selective regions, in which neurons show different selectivities (similar to LO and pFs), and (ii) a population with similar selectivity across all neurons to represent a region with a strong preference for a specific object category, in which neurons generally show high response to stimuli from that category (similar to EBA and PPA). Then we assessed the univariate shift using the reduction in response when attention shifted from the stronger to the weaker stimulus in a pair (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), and examined its relationship with univariate category distance (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>).</p><p>We found that the response gain model predicted no relationship between univariate shift and category distance in either population (<xref ref-type="fig" rid="fig7">Figure 7A–B</xref>). In contrast, the tuning sharpening model predicted a positive correlation between univariate shift and category distance in both neural populations (<xref ref-type="fig" rid="fig7">Figure 7C–D</xref>). Thus, the tuning sharpening model provides a better prediction of the empirical data compared to the response gain model.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Univariate shift as a function of category distance, as predicted by the two attentional mechanisms.</title><p><inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> denote the two paired conditions with attention directed to the more preferred (<inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula>) or the less preferred (<inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>) stimulus, respectively. <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>M</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>L</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> represent the isolated conditions, respectively with the <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi></mml:mstyle></mml:math></inline-formula> or the <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula> stimulus presented in isolation. Top panels represent predictions in a region with a strong preference for a specific category, and bottom panels illustrate predictions in an object-selective region. Each circle represents a pair of categories. (<bold>A–B</bold>) Predicted univariate shift based on the response gain model in a region with a strong preference for a specific category (<bold>A</bold>) and in an object-selective region (<bold>B</bold>). (<bold>C–D</bold>) Predicted univariate shift based on the tuning model in a region with strong preference for a specific category (<bold>C</bold>) and in an object-selective region (<bold>D</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig7-v1.tif"/></fig><p>Next, for the multivariate analysis, we assessed the attentional weight shift for each category pair as attention shifted from one stimulus to the other (<xref ref-type="disp-formula" rid="equ6">Equation 5</xref>), and examined its relationship with the multivariate category distance (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). Here, too, we find that the response gain model predicted no relationship between attentional weight shift and category distance (<xref ref-type="fig" rid="fig8">Figure 8A–B</xref>). In contrast, the tuning sharpening model predicted a positive relationship between weight shift and category distance for both neural populations (<xref ref-type="fig" rid="fig8">Figure 8C–D</xref>), providing further evidence for tuning sharpening as the underlying mechanism for attentional enhancement.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Predicted weight shift as a function of category distance.</title><p>Weight shift for each pair is calculated using <xref ref-type="disp-formula" rid="equ6">Equation 5</xref>. Category distance represents the difference in multi-voxel representation between responses to the two isolated stimuli, calculated by <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>. Top panels are related to predictions in a region with a strong preference for a specific category and the bottom panels illustrate predictions in an object-selective region. (<bold>A–B</bold>) Weight shift predicted by the response gain model in a region with a strong preference for a specific category (<bold>A</bold>) and in an object-selective region (<bold>B</bold>). (<bold>C–D</bold>) Weight shift predicted by the tuning model in a region with strong preference for a specific category (<bold>C</bold>) and in an object-selective region (<bold>D</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-fig8-v1.tif"/></fig><p>We also tested a third model based on a labeled line mechanism for attentional enhancement (see Appendix 1). The labeled line model posits that attention to a stimulus enhances the neural response only when the attended stimulus is the neuron’s preferred stimulus. Therefore, this model is a special case of change in the neurons’ tuning curve (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). Although the labeled line model could predict the positive correlation between the univariate shift and category distance in a region with high selectivity for a certain category, it failed to predict the results in other cases (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>).</p><p>In sum, the tuning model predicts the empirically-observed effect of target-distractor similarity on attentional effects both at the univariate and at the multivariate level, while the response gain model does not.</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>Visual stimuli compete for resources in the brain. The biased competition model posits that attention to a stimulus biases this competition in favor of the attended stimulus (<xref ref-type="bibr" rid="bib27">Moran and Desimone, 1985</xref>; <xref ref-type="bibr" rid="bib11">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="bib31">Reynolds et al., 1999</xref>). Here, we examined the change in this attentional bias by systematically varying the target and distractors. Using fMRI, we showed that rather than being a constant top-down bias, attentional enhancement depends on the similarity between the target and the distractor in their cortical representation, both at the univariate level and at the multivariate level. Using simulations, we arbitrated between the response gain model and the tuning sharpening model as mechanisms of attention for the observed effect, and showed that the empirical results were explained by the latter and not the former.</p><sec id="s4-1"><title>Effect of target-distractor similarity on the attentional bias</title><p>Using stimuli from four object categories, our study reveals the neural basis of the attentional effect graded by target-distractor similarity in the human brain both at the univariate level and at the multivariate level. This finding has two important implications:</p><p>First, our results show that in the competition between multiple stimuli, the attentional bias is not constant. Previous studies have shown attentional modulation in the human brain as an average value without considering its variance for different pairings of targets and distractors (<xref ref-type="bibr" rid="bib30">Reddy et al., 2009</xref>). These previous accounts of attention cannot explain the variance in performance for the same number of stimuli from different categories. Assessing the role of stimulus content in the bias caused by attention, we confirm that attention enhances the response related to the target. We refine our understanding by showing that however, the attentional bias offers less advantage for a more similar target-distractor pair.</p><p>Second, this finding provides direct neural evidence for the adverse effects of target-distractor similarity on performance, as previously reported in behavioral studies (<xref ref-type="bibr" rid="bib5">Cohen et al., 2014</xref>; <xref ref-type="bibr" rid="bib6">Cohen et al., 2017</xref>). While behavioral data have suggested that this effect is due to limitations in processing, no investigation has been made to determine the underlying reason or find a mechanistic explanation. Our results demonstrate that this reduction in performance is because the representation of the target (relative to the distractor) is less effectively enhanced by attention when the target becomes more similar to the distractor.</p><p>We observed a significant univariate shift in higher-level regions of the occipito-temporal cortex, but not in V1. Evidence on the effect of attention on V1 responses is divergent, with some previous neuroimaging studies showing a significant effect of attention on neural responses (<xref ref-type="bibr" rid="bib35">Somers et al., 1999</xref>; <xref ref-type="bibr" rid="bib18">Gandhi et al., 1999</xref>), while others report no significant effect of attention (<xref ref-type="bibr" rid="bib7">Corbetta et al., 1990</xref>; <xref ref-type="bibr" rid="bib36">Thorat and Peelen, 2022</xref>; <xref ref-type="bibr" rid="bib12">Doostani et al., 2023</xref>). We believe that this apparent discrepancy results from the form of attention under study. Here, we study object-based attention with a superimposed design that excludes response modulation by space-based attention. Previous reports of significant attentional modulation in V1 include studies of space-based attention with stimuli presented at different locations (<xref ref-type="bibr" rid="bib35">Somers et al., 1999</xref>; <xref ref-type="bibr" rid="bib18">Gandhi et al., 1999</xref>). Considering the high reliance of V1 responses to location, the effect of attention is less pronounced when the two stimuli are presented at the same location, as is the case in the present study.</p><p>Although examples of superimposed cluttered stimuli are not very common in everyday life, they still do occur in certain situations, for example, reading text on the cellphone screen in the presence of reflection and glare on the screen or looking at the street through a patterned window. Such instances recruit object-based attention which was the aim of this study, whereas in more common cases in which attended and unattended objects occupy different locations in space, both space-based and object-based attention may work together to resolve the competition between different stimuli. Here, we chose to move away from usual everyday scenarios to study the effect of object-based attention in isolation. Future studies can reveal the effect of target-distractor similarity, i.e., proximity in space, on space-based attention and how the effects caused by object-based and space-based attention interact.</p><p>Please note that we used a blocked design in which the target and distractor categories could be predicted across each block. While it is possible that the current design has led to an enhancement of the observed effect, previous behavioral data <xref ref-type="bibr" rid="bib5">Cohen et al., 2014</xref>; <xref ref-type="bibr" rid="bib43">Xu and Vaziri-Pashkam, 2019</xref> have reported the same effect in experiments in which the distractor was not predictable. To study the effect of predictability on fMRI responses, however, an event-related design is more appropriate, an interesting venue for future fMRI studies.</p></sec><sec id="s4-2"><title>A model for object-based attentional enhancement</title><p>Using a simulation approach, we provide a mechanistic explanation for the observed graded attentional effect. Our modeling results have two implications:</p><p>First, we demonstrate that tuning sharpening, but not response gain, predicts the observed reduction in the effect of attention for more similar target-distractor pairs both at the univariate and at the multivariate level. Previous research has shown that a change in the tuning function improves attentional selection at high external noise levels (<xref ref-type="bibr" rid="bib22">Ling et al., 2009</xref>). Our results indicate that a change in tuning function could also lead to behavioral disadvantage in an environment where the target is not very different from the surrounding items. When attention is directed towards the target, the response to non-target objects that are more similar to the target is also enhanced, albeit to a lesser amount, leading to an overall weaker effect of attention for a more similar target-distractor pair.</p><p>Second, providing evidence from the human brain in favor of tuning sharpening, we suggest tuning sharpening as the underlying mechanism in the domain of object-based attention. By comparing the response gain model and the tuning sharpening model directly in a single study, we provide strong evidence that arbitrates between the theories. The effects of attention have generally been explained by attention acting through increasing the contrast or response gain, especially for space-based attention (<xref ref-type="bibr" rid="bib25">McAdams and Maunsell, 1999</xref>; <xref ref-type="bibr" rid="bib33">Reynolds and Chelazzi, 2004</xref>; <xref ref-type="bibr" rid="bib16">Fox et al., 2023</xref>). However, a simple increase in gain cannot explain all reported effects of attention, and a change in the shape of the tuning curves has been observed during visual search (<xref ref-type="bibr" rid="bib8">Çukur et al., 2013</xref>), and feature-based attention (<xref ref-type="bibr" rid="bib24">Martinez-Trujillo and Treue, 2004</xref>; <xref ref-type="bibr" rid="bib10">David et al., 2008</xref>; <xref ref-type="bibr" rid="bib22">Ling et al., 2009</xref>).</p><p>While tuning curves are commonly used for feature dimensions such as stimulus orientation or motion direction, here, we used the term to describe the variation in a neuron’s response to different object stimuli. With a finite set of object categories, as in the current study, the neural response in object space is discrete, rather than a continuous curve illustrated for features such as stimulus orientation. The neuron might have to tune for a particular feature such as curvature or spikiness (<xref ref-type="bibr" rid="bib2">Bao et al., 2020</xref>) that is present to different degrees in our object stimuli in a continuous way, but we are not measuring this directly. Nevertheless, since more preferred and less preferred features (objects in this case) can still be defined, we illustrate the neural response using a hypothetical curve in object space. As such, here, tuning sharpening refers to the fact that the response to the more preferred object categories has been enhanced while the response to the less preferred stimulus categories is suppressed.</p><p>It is important to note that our speculation on the role of tuning sharpening in object-based attention is based on simulations and not neural data. To ascertain tuning sharpening as the underlying mechanism for object-based attention, intracranial recordings from the human brain are needed.</p></sec><sec id="s4-3"><title>Conclusion</title><p>In sum, our results unravel the cortical basis by which target-distractor similarity affects attentional modulation, and indicate tuning sharpening as the underlying mechanism for response enhancement during object-based attention.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Supervision, Validation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Validation, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Validation, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Participants gave written consent and received payment for their participation in the experiment. Data collection was approved by the Ethics Committee of the Institute for Research in Fundamental Sciences, Tehran. Reference number for the ethical approval: 98/60/2184.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-89836-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>fMRI data have been deposited in OSF under <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/2QTF6">https://doi.org/10.17605/OSF.IO/2QTF6</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Doostani</surname><given-names>N</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Similarity</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/2QTF6</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Sajad Aghapour for the helpful discussions. We thank Kiarash Farahmandrad for his help with the graphical illustration of the vector plot. Maryam Vaziri-Pashkam was supported by NIH Intramural Research Program ZIA-MH002035.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Representation of multiple objects in macaque category-selective areas</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>1774</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04126-7</pub-id><pub-id pub-id-type="pmid">29720645</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id><pub-id pub-id-type="pmid">32494012</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>DM</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Stimulus context modulates competition in human extrastriate cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1110</fpage><lpage>1116</lpage><pub-id pub-id-type="doi">10.1038/nn1501</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Responses of neurons in macaque MT to stochastic motion signals</article-title><source>Visual Neuroscience</source><volume>10</volume><fpage>1157</fpage><lpage>1169</lpage><pub-id pub-id-type="doi">10.1017/S0952523800010269</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Rhee</surname><given-names>JY</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Processing multiple visual objects is limited by overlap in neural channels</article-title><source>PNAS</source><volume>111</volume><fpage>8955</fpage><lpage>8960</lpage><pub-id pub-id-type="doi">10.1073/pnas.1317860111</pub-id><pub-id pub-id-type="pmid">24889618</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual search for object categories is predicted by the representational architecture of high-level visual cortex</article-title><source>Journal of Neurophysiology</source><volume>117</volume><fpage>388</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1152/jn.00569.2016</pub-id><pub-id pub-id-type="pmid">27832600</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Miezin</surname><given-names>FM</given-names></name><name><surname>Dobmeyer</surname><given-names>S</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Attentional modulation of neural processing of shape, color, and velocity in humans</article-title><source>Science</source><volume>248</volume><fpage>1556</fpage><lpage>1559</lpage><pub-id pub-id-type="doi">10.1126/science.2360050</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Çukur</surname><given-names>T</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Attention during natural vision warps semantic representation across the human brain</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>763</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1038/nn.3381</pub-id><pub-id pub-id-type="pmid">23603707</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical Surface-Based Analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name><name><surname>Mazer</surname><given-names>JA</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Attention to stimulus features shifts spectral tuning of V4 neurons during natural vision</article-title><source>Neuron</source><volume>59</volume><fpage>509</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.07.001</pub-id><pub-id pub-id-type="pmid">18701075</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neural mechanisms of selective visual attention</article-title><source>Annual Review of Neuroscience</source><volume>18</volume><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.18.030195.001205</pub-id><pub-id pub-id-type="pmid">7605061</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doostani</surname><given-names>N</given-names></name><name><surname>Hossein-Zadeh</surname><given-names>G-A</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The normalization model predicts responses in the human visual cortex during object-based attention</article-title><source>eLife</source><volume>12</volume><elocation-id>e75726</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.75726</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Shuman</surname><given-names>M</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A cortical area selective for visual processing of the human body</article-title><source>Science</source><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="doi">10.1126/science.1063414</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R</given-names></name><name><surname>Harris</surname><given-names>A</given-names></name><name><surname>Stanley</surname><given-names>D</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The parahippocampal place area: recognition, navigation, or encoding?</article-title><source>Neuron</source><volume>23</volume><fpage>115</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80758-8</pub-id><pub-id pub-id-type="pmid">10402198</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fallah</surname><given-names>M</given-names></name><name><surname>Stoner</surname><given-names>GR</given-names></name><name><surname>Reynolds</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Stimulus-specific competitive selection in macaque extrastriate visual area V4</article-title><source>PNAS</source><volume>104</volume><fpage>4165</fpage><lpage>4169</lpage><pub-id pub-id-type="doi">10.1073/pnas.0611722104</pub-id><pub-id pub-id-type="pmid">17360494</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>KJ</given-names></name><name><surname>Birman</surname><given-names>D</given-names></name><name><surname>Gardner</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Gain, not concomitant changes in spatial receptive field properties, improves task performance in a neural network attention model</article-title><source>eLife</source><volume>12</volume><elocation-id>e78392</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.78392</pub-id><pub-id pub-id-type="pmid">37184221</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franconeri</surname><given-names>SL</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Flexible cognitive resources: competitive content maps for attention and memory</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>134</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.01.010</pub-id><pub-id pub-id-type="pmid">23428935</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gandhi</surname><given-names>SP</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Boynton</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Spatial attention affects brain activity in human primary visual cortex</article-title><source>PNAS</source><volume>96</volume><fpage>3314</fpage><lpage>3319</lpage><pub-id pub-id-type="doi">10.1073/pnas.96.6.3314</pub-id><pub-id pub-id-type="pmid">10077681</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kushnir</surname><given-names>T</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><name><surname>Edelman</surname><given-names>S</given-names></name><name><surname>Itzchak</surname><given-names>Y</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A sequence of object-processing stages revealed by fMRI in the human occipital lobe</article-title><source>Human Brain Mapping</source><volume>6</volume><fpage>316</fpage><lpage>328</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1998)6:4&lt;316::AID-HBM9&gt;3.0.CO;2-6</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Mechanisms of directed attention in the human extrastriate cortex as revealed by functional MRI</article-title><source>Science</source><volume>282</volume><fpage>108</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1126/science.282.5386.108</pub-id><pub-id pub-id-type="pmid">9756472</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Cortical regions involved in perceiving object shape</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>3310</fpage><lpage>3318</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-09-03310.2000</pub-id><pub-id pub-id-type="pmid">10777794</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ling</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How spatial and feature-based attention affect the gain and tuning of population responses</article-title><source>Vision Research</source><volume>49</volume><fpage>1194</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.05.025</pub-id><pub-id pub-id-type="pmid">18590754</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Benson</surname><given-names>RR</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Kennedy</surname><given-names>WA</given-names></name><name><surname>Ledden</surname><given-names>PJ</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title><source>PNAS</source><volume>92</volume><fpage>8135</fpage><lpage>8139</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.18.8135</pub-id><pub-id pub-id-type="pmid">7667258</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Trujillo</surname><given-names>JC</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Feature-based attention increases the selectivity of population responses in primate visual cortex</article-title><source>Current Biology</source><volume>14</volume><fpage>744</fpage><lpage>751</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.04.028</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname><given-names>CJ</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of attention on orientation-tuning functions of single neurons in macaque cortical area V4</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>431</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-01-00431.1999</pub-id><pub-id pub-id-type="pmid">9870971</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McMains</surname><given-names>S</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Interactions of top-down and bottom-up mechanisms in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>587</fpage><lpage>597</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3766-10.2011</pub-id><pub-id pub-id-type="pmid">21228167</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moran</surname><given-names>J</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Selective attention gates visual processing in the extrastriate cortex</article-title><source>Science</source><volume>229</volume><fpage>782</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1126/science.4023713</pub-id><pub-id pub-id-type="pmid">4023713</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ni</surname><given-names>AM</given-names></name><name><surname>Ray</surname><given-names>S</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Tuned normalization explains the size of attention modulations</article-title><source>Neuron</source><volume>73</volume><fpage>803</fpage><lpage>813</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.006</pub-id><pub-id pub-id-type="pmid">22365552</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Craven</surname><given-names>KM</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>fMRI evidence for objects as the units of attentional selection</article-title><source>Nature</source><volume>401</volume><fpage>584</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1038/44134</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>L</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Attention and biased competition in multi-voxel object representations</article-title><source>PNAS</source><volume>106</volume><fpage>21447</fpage><lpage>21452</lpage><pub-id pub-id-type="doi">10.1073/pnas.0907330106</pub-id><pub-id pub-id-type="pmid">19955434</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>J H</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Competitive mechanisms subserve attention in macaque areas V2 and V4</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>1736</fpage><lpage>1753</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-05-01736.1999</pub-id><pub-id pub-id-type="pmid">10024360</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Interacting roles of attention and visual salience in V4</article-title><source>Neuron</source><volume>37</volume><fpage>853</fpage><lpage>863</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00097-7</pub-id><pub-id pub-id-type="pmid">12628175</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Attentional modulation of visual processing</article-title><source>Annual Review of Neuroscience</source><volume>27</volume><fpage>611</fpage><lpage>647</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.26.041002.131039</pub-id><pub-id pub-id-type="pmid">15217345</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title><source>Science</source><volume>268</volume><fpage>889</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1126/science.7754376</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Somers</surname><given-names>DC</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Seiffert</surname><given-names>AE</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Functional MRI reveals spatially specific attentional modulation in human primary visual cortex</article-title><source>PNAS</source><volume>96</volume><fpage>1663</fpage><lpage>1668</lpage><pub-id pub-id-type="doi">10.1073/pnas.96.4.1663</pub-id><pub-id pub-id-type="pmid">9990081</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorat</surname><given-names>S</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Body shape as a visual feature: Evidence from spatially-global attentional modulation in human visual cortex</article-title><source>NeuroImage</source><volume>255</volume><elocation-id>119207</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119207</pub-id><pub-id pub-id-type="pmid">35427768</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tootell</surname><given-names>RB</given-names></name><name><surname>Hadjikhani</surname><given-names>NK</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>AK</given-names></name><name><surname>Mendola</surname><given-names>JD</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Functional analysis of primary visual cortex (V1) in humans</article-title><source>PNAS</source><volume>95</volume><fpage>811</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.3.811</pub-id><pub-id pub-id-type="pmid">9448245</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Attentional modulation of visual motion processing in cortical areas MT and MST</article-title><source>Nature</source><volume>382</volume><fpage>539</fpage><lpage>541</lpage><pub-id pub-id-type="doi">10.1038/382539a0</pub-id><pub-id pub-id-type="pmid">8700227</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Martínez Trujillo</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title><source>Nature</source><volume>399</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/21176</pub-id><pub-id pub-id-type="pmid">10376597</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Maunsell</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Effects of attention on the processing of motion in macaque middle temporal and medial superior temporal visual cortical areas</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>7591</fpage><lpage>7602</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-17-07591.1999</pub-id><pub-id pub-id-type="pmid">10460265</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Goal-directed visual processing differentially impacts human ventral and dorsal visual representations</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8767</fpage><lpage>8782</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3392-16.2017</pub-id><pub-id pub-id-type="pmid">28821655</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An Information-driven 2-pathway characterization of occipitotemporal and posterior parietal visual object representations</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>2034</fpage><lpage>2050</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy080</pub-id><pub-id pub-id-type="pmid">29659730</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Task modulation of the 2-pathway characterization of occipitotemporal and posterior parietal visual object representations</article-title><source>Neuropsychologia</source><volume>132</volume><elocation-id>107140</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2019.107140</pub-id><pub-id pub-id-type="pmid">31301350</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>The labeled line model</title><p>We also simulated a special case of change in the tuning curve called the labeled line model. Based on this model, attention to a certain stimulus enhances the neural response only if the neuron is specifically labeled for that stimulus (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). For instance, attention to body stimuli causes an enhancement in the response of body neurons, but no enhancement in the response of car neurons which might respond to body stimuli to a lesser level. We implemented this mechanism using <xref ref-type="disp-formula" rid="equ13">Equation 8a</xref>: for a body-selective neuron:<disp-formula id="equ13"><label>(8a)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><label>(8b)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For a car-selective neuron:<disp-formula id="equ15"><label>(8c)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ16"><label>(8d)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For a house-selective neuron:<disp-formula id="equ17"><label>(8e)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ18"><label>(8f)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Then, using the labeled line mechanism for attentional enhancement, we simulated the response of two neural populations in the 16 task conditions (see Materials and methods and Results). Performing the univariate analysis on the simulated responses, we assessed the univariate shift for all category pairs. The labeled line model predicted a positive correlation between the univariate shift and category distance in the population with a strong preference for a certain category, while it predicted no relationship between univariate shift and category distance in the object-selective population (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2A, B</xref>).</p><p>In the multivariate analysis, the labeled line model predicted no relationship for the neural population with a strong preference for a certain category (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2C</xref>), while it predicted a negative correlation between weight shift and category distance in the object-selective population (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2D</xref>).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Based on the labeled line model, attention enhances the response of a neuron only when the attended stimulus is the neuron’s preferred stimulus.</title><p>We illustrate the models here for the example of a neuron with high selectivity for cat stimuli. Solid curves denote the response to unattended stimuli and dashed curves denote the response to attended stimuli.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Simulation results for the labeled line model.</title><p>(<bold>A–B</bold>) Predicted univariate shift based on the labeled line model in a region with a strong preference for a specific category (<bold>A</bold>) and in an object-selective region (<bold>B</bold>). (<bold>C–D</bold>) Weight shift predicted by the labeled line model in a region with a strong preference for a specific category (<bold>C</bold>) and in an object-selective region (<bold>D</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-app1-fig2-v1.tif"/></fig><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Percentage of voxels in each regions of interest (ROI) most responsive to each of the four stimulus categories, averaged across participants.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Preferred Category</th><th align="left" valign="bottom">V1</th><th align="left" valign="bottom">LO</th><th align="left" valign="bottom">pFs</th><th align="left" valign="bottom">EBA</th><th align="left" valign="bottom">PPA</th></tr></thead><tbody><tr><td align="left" valign="bottom">Body</td><td align="char" char="." valign="bottom">21%</td><td align="char" char="." valign="bottom">31%</td><td align="char" char="." valign="bottom">21%</td><td align="left" valign="bottom">70%</td><td align="char" char="." valign="bottom">10%</td></tr><tr><td align="left" valign="bottom">Car</td><td align="char" char="." valign="bottom">19%</td><td align="char" char="." valign="bottom">18%</td><td align="char" char="." valign="bottom">17%</td><td align="left" valign="bottom">4%</td><td align="char" char="." valign="bottom">12%</td></tr><tr><td align="left" valign="bottom">House</td><td align="char" char="." valign="bottom">33%</td><td align="char" char="." valign="bottom">32%</td><td align="char" char="." valign="bottom">40%</td><td align="left" valign="bottom">3%</td><td align="char" char="." valign="bottom">68%</td></tr><tr><td align="left" valign="bottom">Cat</td><td align="char" char="." valign="bottom">27%</td><td align="char" char="." valign="bottom">19%</td><td align="char" char="." valign="bottom">22%</td><td align="left" valign="bottom">23%</td><td align="char" char="." valign="bottom">10%</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Average voxel response (general linear model, GLM coefficients) to each category in each region of interest (ROI), averaged across participants.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Stimulus Category</th><th align="left" valign="bottom">V1</th><th align="left" valign="bottom">LO</th><th align="left" valign="bottom">pFs</th><th align="left" valign="bottom">EBA</th><th align="left" valign="bottom">PPA</th></tr></thead><tbody><tr><td align="left" valign="bottom">Body</td><td align="char" char="." valign="bottom">0.28</td><td align="char" char="." valign="bottom">1.59</td><td align="char" char="." valign="bottom">0.88</td><td align="char" char="." valign="bottom">1.62</td><td align="left" valign="bottom">0.001</td></tr><tr><td align="left" valign="bottom">Car</td><td align="char" char="." valign="bottom">0.28</td><td align="char" char="." valign="bottom">1.43</td><td align="char" char="." valign="bottom">0.88</td><td align="char" char="." valign="bottom">0.60</td><td align="left" valign="bottom">0.12</td></tr><tr><td align="left" valign="bottom">House</td><td align="char" char="." valign="bottom">0.47</td><td align="char" char="." valign="bottom">1.51</td><td align="char" char="." valign="bottom">1.03</td><td align="char" char="." valign="bottom">0.48</td><td align="left" valign="bottom">0.55</td></tr><tr><td align="left" valign="bottom">Cat</td><td align="char" char="." valign="bottom">0.35</td><td align="char" char="." valign="bottom">1.49</td><td align="char" char="." valign="bottom">0.89</td><td align="char" char="." valign="bottom">1.32</td><td align="left" valign="bottom">–0.02</td></tr></tbody></table></table-wrap><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Single-subject correlation coefficients for the correlation between the univariate shift and category distance in all regions of interest (ROIs).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">V1</th><th align="left" valign="bottom">LO</th><th align="left" valign="bottom">pFs</th><th align="left" valign="bottom">EBA</th><th align="left" valign="bottom">PPA</th></tr></thead><tbody><tr><td align="left" valign="bottom">–0.60</td><td align="left" valign="bottom">0.67</td><td align="left" valign="bottom">–0.073</td><td align="char" char="." valign="bottom">0.72</td><td align="left" valign="bottom">0.18</td></tr><tr><td align="left" valign="bottom">0.16</td><td align="left" valign="bottom">0.38</td><td align="left" valign="bottom">0.015</td><td align="char" char="." valign="bottom">0.64</td><td align="left" valign="bottom">0.46</td></tr><tr><td align="left" valign="bottom">–0.53</td><td align="left" valign="bottom">0.52</td><td align="left" valign="bottom">0.89</td><td align="char" char="." valign="bottom">0.60</td><td align="left" valign="bottom">0.42</td></tr><tr><td align="left" valign="bottom">–0.062</td><td align="left" valign="bottom">0.78</td><td align="left" valign="bottom">0.93</td><td align="char" char="." valign="bottom">0.87</td><td align="left" valign="bottom">0.69</td></tr><tr><td align="left" valign="bottom">–0.098</td><td align="left" valign="bottom">0.072</td><td align="left" valign="bottom">1.0</td><td align="char" char="." valign="bottom">0.72</td><td align="left" valign="bottom">0.96</td></tr><tr><td align="left" valign="bottom">0.63</td><td align="left" valign="bottom">0.93</td><td align="left" valign="bottom">0.88</td><td align="char" char="." valign="bottom">0.94</td><td align="left" valign="bottom">0.58</td></tr><tr><td align="left" valign="bottom">–0.31</td><td align="left" valign="bottom">0.52</td><td align="left" valign="bottom">–0.11</td><td align="char" char="." valign="bottom">0.88</td><td align="left" valign="bottom">0.89</td></tr><tr><td align="left" valign="bottom">0.29</td><td align="left" valign="bottom">0.18</td><td align="left" valign="bottom">0.62</td><td align="char" char="." valign="bottom">0.67</td><td align="left" valign="bottom">0.63</td></tr><tr><td align="left" valign="bottom">–0.49</td><td align="left" valign="bottom">0.094</td><td align="left" valign="bottom">0.63</td><td align="char" char="." valign="bottom">0.81</td><td align="left" valign="bottom">0.80</td></tr><tr><td align="left" valign="bottom">0.21</td><td align="left" valign="bottom">0.57</td><td align="left" valign="bottom">0.31</td><td align="char" char="." valign="bottom">0.81</td><td align="left" valign="bottom">0.87</td></tr><tr><td align="left" valign="bottom">–0.015</td><td align="left" valign="bottom">–0.57</td><td align="left" valign="bottom">0.79</td><td align="char" char="." valign="bottom">0.35</td><td align="left" valign="bottom">0.69</td></tr><tr><td align="left" valign="bottom">0.35</td><td align="left" valign="bottom">0.79</td><td align="left" valign="bottom">0.90</td><td align="char" char="." valign="bottom">0.90</td><td align="left" valign="bottom">0.88</td></tr><tr><td align="left" valign="bottom">–0.47</td><td align="left" valign="bottom">0.058</td><td align="left" valign="bottom">–0.050</td><td align="char" char="." valign="bottom">0.93</td><td align="left" valign="bottom">–0.24</td></tr><tr><td align="left" valign="bottom">–0.047</td><td align="left" valign="bottom">–0.21</td><td align="left" valign="bottom">0.91</td><td align="char" char="." valign="bottom">0.28</td><td align="left" valign="bottom">0.86</td></tr><tr><td align="left" valign="bottom">0.66</td><td align="left" valign="bottom">0.70</td><td align="left" valign="bottom">0.91</td><td align="char" char="." valign="bottom">0.64</td><td align="left" valign="bottom">0.45</td></tr><tr><td align="left" valign="bottom">–0.74</td><td align="left" valign="bottom">0.54</td><td align="left" valign="bottom">0.24</td><td align="char" char="." valign="bottom">0.60</td><td align="left" valign="bottom">–0.31</td></tr><tr><td align="left" valign="bottom">–0.095</td><td align="left" valign="bottom">0.73</td><td align="left" valign="bottom">0.89</td><td align="char" char="." valign="bottom">0.64</td><td align="left" valign="bottom">0.95</td></tr></tbody></table></table-wrap><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Weights of each stimulus for each category pair in each region of interest (ROI), averaged across participants.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Pair</th><th align="left" valign="bottom">Attended Stimulus</th><th align="left" valign="bottom">V1</th><th align="left" valign="bottom">LO</th><th align="left" valign="bottom">pFs</th><th align="left" valign="bottom">EBA</th><th align="left" valign="bottom">PPA</th></tr></thead><tbody><tr><td align="left" valign="bottom">Body-Car</td><td align="left" valign="bottom">Body</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.54</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.53</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.79</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.31</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.68</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.42</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.93</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.15</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.41</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.42</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Body-Car</td><td align="left" valign="bottom">Car</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.40</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.70</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.43</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.68</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.39</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.70</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.53</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.46</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.35</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.54</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Body-House</td><td align="left" valign="bottom">Body</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.43</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.65</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.65</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.45</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.65</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.47</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.84</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.21</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0.42</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.61</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Body-House</td><td align="left" valign="bottom">House</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.27</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.78</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.3</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.74</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.27</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.81</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.37</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.57</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0.18</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.86</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Body-Cat</td><td align="left" valign="bottom">Body</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.63</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.55</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.86</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.35</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.81</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.45</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.90</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.30</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0.61</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.32</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Body-Cat</td><td align="left" valign="bottom">Cat</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.59</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.55</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.69</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.48</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.67</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.55</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.67</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.48</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.55</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.37</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Car-House</td><td align="left" valign="bottom">Car</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.42</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.65</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.44</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.70</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.55</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.60</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.56</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.59</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.35</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.72</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Car-House</td><td align="left" valign="bottom">House</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.31</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.73</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.25</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.85</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.36</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.78</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.46</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.65</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.27</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.93</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Car-Cat</td><td align="left" valign="bottom">Car</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.59</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.52</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.68</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.44</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.74</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.40</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.47</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.54</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.53</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.40</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">Car-Cat</td><td align="left" valign="bottom">Cat</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.45</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.70</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.41</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.75</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.55</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.60</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.23</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.92</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.46</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.46</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">House-Cat</td><td align="left" valign="bottom">House</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.76</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.32</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.87</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.24</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.91</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.23</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.56</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.44</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.99</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.11</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom">House-Cat</td><td align="left" valign="bottom">Cat</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.61</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.47</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.47</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.65</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.53</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.62</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.17</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.90</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.57</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.38</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr></tbody></table></table-wrap><table-wrap id="app1table5" position="float"><label>Appendix 1—table 5.</label><caption><title>Single-subject correlation coefficients for the correlation between weight shift and category distance in all regions of interests (ROIs).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">V1</th><th align="left" valign="bottom">LO</th><th align="left" valign="bottom">pFs</th><th align="left" valign="bottom">EBA</th><th align="left" valign="bottom">PPA</th></tr></thead><tbody><tr><td align="char" char="." valign="bottom">–0.36</td><td align="char" char="." valign="bottom">0.93</td><td align="char" char="." valign="bottom">–0.25</td><td align="char" char="." valign="bottom">0.46</td><td align="char" char="." valign="bottom">0.06</td></tr><tr><td align="char" char="." valign="bottom">0.31</td><td align="char" char="." valign="bottom">–0.06</td><td align="char" char="." valign="bottom">0.16</td><td align="char" char="." valign="bottom">0.82</td><td align="char" char="." valign="bottom">–0.25</td></tr><tr><td align="char" char="." valign="bottom">–0.29</td><td align="char" char="." valign="bottom">0.35</td><td align="char" char="." valign="bottom">0.80</td><td align="char" char="." valign="bottom">0.48</td><td align="char" char="." valign="bottom">–0.42</td></tr><tr><td align="char" char="." valign="bottom">–0.038</td><td align="char" char="." valign="bottom">0.70</td><td align="char" char="." valign="bottom">0.75</td><td align="char" char="." valign="bottom">0.57</td><td align="char" char="." valign="bottom">0.38</td></tr><tr><td align="char" char="." valign="bottom">0.72</td><td align="char" char="." valign="bottom">0.60</td><td align="char" char="." valign="bottom">0.90</td><td align="char" char="." valign="bottom">0.58</td><td align="char" char="." valign="bottom">–0.77</td></tr><tr><td align="char" char="." valign="bottom">0.53</td><td align="char" char="." valign="bottom">0.69</td><td align="char" char="." valign="bottom">0.76</td><td align="char" char="." valign="bottom">0.51</td><td align="char" char="." valign="bottom">0.59</td></tr><tr><td align="char" char="." valign="bottom">–0.32</td><td align="char" char="." valign="bottom">0.90</td><td align="char" char="." valign="bottom">–0.25</td><td align="char" char="." valign="bottom">0.29</td><td align="char" char="." valign="bottom">0.065</td></tr><tr><td align="char" char="." valign="bottom">0.33</td><td align="char" char="." valign="bottom">0.22</td><td align="char" char="." valign="bottom">–0.36</td><td align="char" char="." valign="bottom">0.24</td><td align="char" char="." valign="bottom">–0.48</td></tr><tr><td align="char" char="." valign="bottom">0.52</td><td align="char" char="." valign="bottom">0.31</td><td align="char" char="." valign="bottom">0.63</td><td align="char" char="." valign="bottom">0.54</td><td align="char" char="." valign="bottom">0.91</td></tr><tr><td align="char" char="." valign="bottom">0.019</td><td align="char" char="." valign="bottom">0.96</td><td align="char" char="." valign="bottom">0.80</td><td align="char" char="." valign="bottom">0.61</td><td align="char" char="." valign="bottom">0.71</td></tr><tr><td align="char" char="." valign="bottom">–0.11</td><td align="char" char="." valign="bottom">0.87</td><td align="char" char="." valign="bottom">0.89</td><td align="char" char="." valign="bottom">0.69</td><td align="char" char="." valign="bottom">0.51</td></tr><tr><td align="char" char="." valign="bottom">–0.27</td><td align="char" char="." valign="bottom">0.94</td><td align="char" char="." valign="bottom">0.81</td><td align="char" char="." valign="bottom">0.63</td><td align="char" char="." valign="bottom">0.50</td></tr><tr><td align="char" char="." valign="bottom">–0.28</td><td align="char" char="." valign="bottom">0.28</td><td align="char" char="." valign="bottom">–0.11</td><td align="char" char="." valign="bottom">0.49</td><td align="char" char="." valign="bottom">0.59</td></tr><tr><td align="char" char="." valign="bottom">0.42</td><td align="char" char="." valign="bottom">0.11</td><td align="char" char="." valign="bottom">0.61</td><td align="char" char="." valign="bottom">0.42</td><td align="char" char="." valign="bottom">0.59</td></tr><tr><td align="char" char="." valign="bottom">0.67</td><td align="char" char="." valign="bottom">0.77</td><td align="char" char="." valign="bottom">0.78</td><td align="char" char="." valign="bottom">0.36</td><td align="char" char="." valign="bottom">0.36</td></tr><tr><td align="char" char="." valign="bottom">–0.96</td><td align="char" char="." valign="bottom">0.78</td><td align="char" char="." valign="bottom">0.48</td><td align="char" char="." valign="bottom">0.43</td><td align="char" char="." valign="bottom">–0.24</td></tr><tr><td align="char" char="." valign="bottom">–0.12</td><td align="char" char="." valign="bottom">0.69</td><td align="char" char="." valign="bottom">0.82</td><td align="char" char="." valign="bottom">0.63</td><td align="char" char="." valign="bottom">0.56</td></tr></tbody></table></table-wrap></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89836.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Peelen</surname><given-names>Marius V</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Radboud University Nijmegen</institution><country>Netherlands</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study has the potential to shed mechanistic light on how attention mechanisms that influence competition between multiple visual stimuli are modulated by the relative neural similarity of these stimuli. The study provides <bold>convincing</bold> data that will also be used for future modeling efforts. The study will be of interest to researchers working on the neural basis of visual attention.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89836.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors report an fMRI investigation of the neural mechanisms by which selective attention allows capacity-limited perceptual systems to preferentially represent task-relevant visual stimuli. Specifically, they examine competitive interactions between two simultaneously-presented items from different categories, to reveal how task-directed attention to one of them modulates the activity of brain regions that respond to both. The specific hypothesis is that attention will bias responses to be more like those elicited by the relevant object presented on its own, and further that this modulation will be stronger for more dissimilar stimulus pairs. This pattern was confirmed in univariate analyses that measured the mass response of a priori regions of interest, as well as multivariate analyses that considered the patterns of evoked activity within the same regions. The authors follow these neuroimaging results with a simulation study that favours a &quot;tuning&quot; mechanism of attention (enhanced responses to highly effective stimuli, and suppression for ineffective stimuli) to explain this pattern.</p><p>Strengths:</p><p>The manuscript clearly articulates a core issue in the cognitive neuroscience of attention, namely the need to understand how limited perceptual systems cope with complex environments in the service of the observer's goals. The use of a priori regions of interest (and a control region), and the inclusion of both univariate and multivariate analyses as well as a simple model, are further strengths. The authors carefully derive clear indices of attentional effects (for both univariate and multivariate analyses) which makes explication of their findings easy to follow.</p><p>Weaknesses:</p><p>Direct estimation of baseline responses may have improved the validity of the modelling. The presentation of transparently overlapping items has some methodological advantages, but somewhat limits the ecological validity of connections to real-world visual &quot;clutter&quot;.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89836.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Doostani</surname><given-names>Narges</given-names></name><role specific-use="author">Author</role><aff><institution>Institute for Research in Fundamental Sciences</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Iran</country></aff></contrib><contrib contrib-type="author"><name><surname>Hossein-Zadeh</surname><given-names>Gholam-Ali</given-names></name><role specific-use="author">Author</role><aff><institution>University of Tehran</institution><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Iran</country></aff></contrib><contrib contrib-type="author"><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><role specific-use="author">Author</role><aff><institution>Freie Universität Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Vaziri-Pashkam</surname><given-names>Maryam</given-names></name><role specific-use="author">Author</role><aff><institution>National Institute of Mental Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>The authors report an fMRI investigation of the neural mechanisms by which selective attention allows capacity-limited perceptual systems to preferentially represent task-relevant visual stimuli. Specifically, they examine competitive interactions between two simultaneously-presented items from different categories, to reveal how task-directed attention to one of them modulates the activity of brain regions that respond to both. The specific hypothesis is that attention will bias responses to be more like those elicited by the relevant object presented on its own, and further that this modulation will be stronger for more dissimilar stimulus pairs. This pattern was confirmed in univariate analyses that measured the mass response of a priori regions of interest, as well as multivariate analyses that considered the patterns of evoked activity within the same regions. The authors follow these neuroimaging results with a simulation study that favours a &quot;tuning&quot; mechanism of attention (enhanced responses to highly effective stimuli, and suppression for ineffective stimuli) to explain this pattern.</p><p>Strengths:</p><p>The manuscript clearly articulates a core issue in the cognitive neuroscience of attention, namely the need to understand how limited perceptual systems cope with complex environments in the service of the observer's goals. The use of a priori regions of interest, and the inclusion of both univariate and multivariate analyses as well as a simple model, are further strengths. The authors carefully derive clear indices of attentional effects (for both univariate and multivariate analyses) which makes explication of their findings easy to follow.</p><p>Weaknesses:</p><p>There are some relatively minor weaknesses in presentation, where the motivation behind some of the procedural decisions could be clearer. There are some apparently paradoxical findings reported -- namely, cases in which the univariate response to pairs of stimuli is greater than to the preferred stimulus alone -- that are not addressed. It is possible that some of the main findings may be attributable to range effects: notwithstanding the paradox just noted, it seems that a floor effect should minimise the range of possible attentional modulation of the responses to two highly similar stimuli. One possible limitation of the modelled results is that they do not reveal any attentional modulation at all under the assumptions of the gain model, for any pair of conditions, implying that as implemented the model may not be correctly capturing the assumptions of that hypothesis.</p></disp-quote><p>We thank the reviewer for the constructive comments. In response, in the current version of the manuscript we have improved the presentation. We further discuss how the response in paired conditions is in some cases higher than the response to the preferred stimulus in this letter. For this, we provide a vector illustration, and a supplementary figure of the sum of weights to show that the weights of isolated-stimulus responses for each category pair are not bound to the similarity of the two isolated responses.</p><p>Regarding the simulation results, we have clarified that the univariate effect of attention is not the attentional modulation itself, but the change in the amount of attentional modulation in the two paired conditions. We provide an explanation for this in this letter below, and have changed the term “attentional modulation” to “univariate shift” in the manuscript to avoid the confusion.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>In an fMRI study requiring participants to attend to one or another object category, either when the object was presented in isolation or with another object superimposed, the authors compared measured univariate and multivariate activation from object-selective and early visual cortex to predictions derived from response gain and tuning sharpening models. They observed a consistent result across higher-level visual cortex that more-divergent responses to isolated stimuli from category pairs predicted a greater modulation by attention when attending to a single stimulus from the category pair presented simultaneously, and argue via simulations that this must be explained by tuning sharpening for object categories.</p><p>Strengths:</p><p>- Interesting experiment design &amp; approach - testing how category similarity impacts neural modulations induced by attention is an important question, and the experimental approach is principled and clever.</p><p>- Examination of both univariate and multivariate signals is an important analysis strategy.</p><p>- The acquired dataset will be useful for future modeling studies.</p><p>Weaknesses:</p><p>- The experimental design does not allow for a neutral 'baseline' estimate of neural responses to stimulus categories absent attention (e.g., attend fixation), nor of the combination of the stimulus categories. This seems critical for interpreting results (e.g., how should readers understand univariate results like that plotted in Fig. 4C-D, where the univariate response is greater for 2 stimuli than one, but the analyses are based on a shift between each extreme activation level?).</p></disp-quote><p>We are happy to clarify our research rationale. We aimed to compare responses in paired conditions when the stimuli were kept constant while varying the attentional target. After we showed that the change in the attentional target resulted in a response change , we compared the amount of this response change to different stimulus category pairs to investigate the effect of representation similarity between the target and the distractor on the response modulation caused by attentional shift. While an estimate of the neural responses in the absence of attention might be useful for other modeling studies, it would not provide us with more information than the current data to answer the question of this study.</p><p>Regarding the univariate results in Fig. 4C-D (and other equivalent ROI results in the revised version) and our analyses, we did not impose any limit on the estimated weights of the two isolated responses in the paired response and thus the sum of the two weights could be any number. We however see that the naming of “weighted average”, which implies a sum of weights being capped at one, has been misleading . We have now changed the name of this model to “linear combination” to avoid confusion</p><p>Previous studies (Reddy et al., 2009, Doostani et al., 2023) using a similar approach have shown a related results pattern: the response to multiple stimuli is higher than the average, but lower than the sum of the isolated responses, which is exactly what our results suggest. We have added discussion on this topic in the Results section in lines 409-413 for clarification:</p><p>“Note that the response in paired conditions can be higher or lower than the response to the isolated more preferred stimulus (condition Mat), depending on the voxel response to the two presented stimuli, as previously reported (Doostani et al. 2023). This is consistent with previous studies reporting the response to multiple stimuli to be higher than the average, but lower than the sum of the response to isolated stimuli (Reddy et al. 2009).”</p><p>We are not sure what the reviewer means by “each extreme activation level”. Our analyses are based on all four conditions. The two isolated conditions are used to calculate the distance measures and the two paired conditions are used for calculating the shift index. Please note that either the isolated or the paired conditions could show the highest response and we seeboth cases in our data. For example, as shown in Figure 4A in EBA, the isolated Body condition and the paired BodyatCar condition show the highest activation levels for the Body-Car pair, whereas in Figure 4C, the two paired conditions (BodyatCat and BodyCatat) elicit the highest response.</p><disp-quote content-type="editor-comment"><p>- Related, simulations assume there exists some non-attended baseline state of each individual object representation, yet this isn't measured, and the way it's inferred to drive the simulations isn't clearly described.</p></disp-quote><p>We agree that the simulations assume a non-attended baseline state, and that we did not measure that state empirically. We needed this non-attended response in the simulations to test which attention mechanism led to the observed results. Thus, we generated the non-attended response using the data reported in previous neural studies of object recognition and attention in the visual cortex (Ni et al., 2012, Bao and Tsao, 2018). Note that the simulations are checking for the profile of the modulations based on category distance. Thus, they do not need to exactly match the real isolated responses in order to show the effect of gain and tuning shift on the results. We include the clarification and the range of neural responses and attention parameters used in the simulations in the revised manuscript in lines 327-333:</p><p>“To examine which attentional mechanism leads to the effects observed in the empirical data, we generated the neural response to unattended object stimuli as a baseline response in the absence of attention, using the data reported by neural studies of object recognition in the visual cortex (Ni et al., 2012, Bao and Tsao, 2018). Then, using an attention parameter for each neuron and different attentional mechanisms, we simulated the response of each neuron to the different task conditions in our experiment. Finally, we assessed the population response by averaging neural responses.”</p><disp-quote content-type="editor-comment"><p>- Some of the simulation results seem to be algebraic (univariate; Fig. 7; multivariate, gain model; Fig. 8)</p></disp-quote><p>This is correct. We have used algebraic equations for the effect of attention on neural responses in the simulations. In fact, thinking about the two models of gain and tuning shift leads to the algebraic equations, which in turn logically leads to the observed results, if no noise is added to the data. The simulations are helpful for visualizing these logical conclusions. Also, after assigning different noise levels to each condition for each neuron, the results are not algebraic anymore which is shown in updated Figure 7 and Figure 8.</p><disp-quote content-type="editor-comment"><p>- Cross-validation does not seem to be employed - strong/weak categories seem to be assigned based on the same data used for computing DVs of interest - to minimize the potential for circularity in analyses, it would be better to define preferred categories using separate data from that used to quantify - perhaps using a cross-validation scheme? This appears to be implemented in Reddy et al. (2009), a paper implementing a similar multivariate method and cited by the authors (their ref 6).</p></disp-quote><p>Thank you for pointing out the missing details about how we used cross-validation. In the univariate analysis, we did use cross validation, defining preferred categories and calculating category distance on one half of the data and calculating the univariate shift on the other half of the data. Similarly, we employed cross-validation for the multivariate analysis by using one half of the data to calculate the multivariate distance between category pairs, and the other half of the data to calculate the weight shift for each category pair. We have now added this methodological information in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>- Multivariate distance metric - why is correlation/cosine similarity used instead of something like Euclidean or Mahalanobis distance? Correlation/cosine similarity is scale-invariant, so changes in the magnitude of the vector would not change distance, despite this likely being an important data attribute to consider.</p></disp-quote><p>Since we are considering response patterns as vectors in each ROI, there is no major difference between the two measures for similarity. Using euclidean distance as a measure of distance (i.e. inverse of similarity) we observed the same relationship between weight shift and category euclidean distance. There was a positive correlation between weight shift and the euclidean category distance in all ROIs (ps &lt; 0.01, ts &gt; 2.9) except for V1 (p = 0.5, t = 0.66). We include this information in the revised manuscript in the Results section lines 513-515:</p><p>“We also calculated category distance based on the euclidean distance between response patterns of category pairs and observed a similarly positive correlation between the weight shift and the euclidean category distance in all ROIs (ps &lt; 0.01, ts &gt;2.9) except V1 (p = 0.5, t = 0.66).”</p><disp-quote content-type="editor-comment"><p>- Details about simulations implemented (and their algebraic results in some cases) make it challenging to interpret or understand these results. E.g., the noise properties of the simulated data aren't disclosed, nor are precise (or approximate) values used for simulating attentional modulations.</p></disp-quote><p>We clarify that the average response to each category was based on previous neurophysiology studies (Ni et al., 2012, Bao and Tsao, 2018). The attentional parameter was also chosen based on previous neurophysiology (Ni et al., 2012) and human fMRI (Doostani et al., 2023) studies of visual attention by randomly assigning a value in the range from 1 to 10. We have included the details in the Methods section in lines 357-366:</p><p>“We simulated the action of the response gain model and the tuning sharpening model using numerical simulations. We composed a neural population of 4⨯105 neurons in equal proportions body-, car-, cat- or house-selective. Each neuron also responded to object categories other than its preferred category, but to a lesser degree and with variation. We chose neural responses to each stimulus from a normal distribution with the mean of 30 spikes/s and standard deviation of 10 and each neuron was randomly assigned an attention factor in the range between 1 and 10 using a uniform distribution. These values are comparable with the values reported in neural studies of attention and object recognition in the ventral visual cortex (Ni et al. 2012, Bao and Tsao 2018). We also added poisson noise to the response of each neuron (Britten et al. 1993), assigned randomly for each condition of each neuron.”</p><disp-quote content-type="editor-comment"><p>- Eye movements do not seem to be controlled nor measured. Could it be possible that some stimulus pairs result in more discriminable patterns of eye movements? Could this be ruled out by some aspect of the results?</p></disp-quote><p>Subjects were instructed to direct their gaze towards the fixation point. Given the variation in the pose and orientation of the stimuli, it is unlikely that eye movements would help with the task. Eye movements have been controlled in previous experiments with individual stimulus presentation (Xu and Vaziri-Pashkam, 2019) and across attentional tasks in which colored dots were superimposed on the stimuli (Vaziri-Pashkam and Xu, 2017) and no significant difference for eye movement across categories or conditions was observed. As such, we do not think that eye movements would play a role in the results we are observing here.</p><disp-quote content-type="editor-comment"><p>- A central, and untested/verified, assumption is that the multivariate activation pattern associated with 2 overlapping stimuli (with one attended) can be modeled as a weighted combination of the activation pattern associated with the individual stimuli. There are hints in the univariate data (e.g., Fig. 4C; 4D) that this might not be justified, which somewhat calls into question the interpretability of the multivariate results.</p></disp-quote><p>If the reviewer is referring to the higher response in the paired compared to the isolated conditions, as explained above, we have not forced any limit on the sum of the estimated weights to equal 1 or 2. Therefore, our model is an estimation of a linear combination of the two multivariate patterns in the isolated conditions. In fact, Leila Reddy et al. (reference 6) reported that while the combination is closer to a weighted average than to a weighted sum, the sum of the weights are on average larger than 1. In Figure 4C and 4D the responses in the paired conditions are higher than either of the isolated-condition responses. This suggests that the weights for the linear combination of isolated responses in the multivariate analysis should add up to larger than one. This is what we find in our results. We have added a supplementary figure to Figure 6, depicting the sum of weights for different category pairs in all ROIs. The figure illustrates that in each ROI, the sum of weights are greater than 1 for some category pairs. It is however noteworthy that we normalized the weights in each condition by the sum of weights to calculate the weight shift in our analysis. The amount of the weight shift was therefore not affected by the absolute value of the weights.</p><disp-quote content-type="editor-comment"><p>- Throughout the manuscript, the authors consistently refer to &quot;tuning sharpening&quot;, an idea that's almost always used to reference changes in the width of tuning curves for specific feature dimensions (e.g., motion direction; hue; orientation; spatial position). Here, the authors are assaying tuning to the category (across exemplars of the category). The link between these concepts could be strengthened to improve the clarity of the manuscript.</p></disp-quote><p>The reviewer brings up an excellent point. Whereas tuning curves have been extensively used for feature dimensions such as stimulus orientation or motion direction, here, we used the term to describe the variation in a neuron’s response to different object stimuli.</p><p>With a finite set of object categories, as is the case in the current study, the neural response in object space is discrete, rather than a continuous curve illustrated for features such as stimulus orientation. However, since more preferred and less preferred features (objects in this case) can still be defined, we illustrated the neural response using a hypothetical curve in object space in Figure 3 to show how it relates with other stimulus features. Therefore, here, tuning sharpening refers to the fact that the response to the more preferred object categories has been enhanced while the response to the less preferred stimulus categories is suppressed.</p><p>We clarify this point in the revised manuscript in the Discussion section lines 649-659:</p><p>“While tuning curves are commonly used for feature dimensions such as stimulus orientation or motion direction, here, we used the term to describe the variation in a neuron’s response to different object stimuli. With a finite set of object categories, as is the case in the current study, the neural response in object space is discrete, rather than a continuous curve illustrated for features such as stimulus orientation. The neuron might have tuning for a particular feature such as curvature or spikiness (Bao et al., 2020) that is present to different degrees in our object stimuli in a continuous way, but we are not measuring this directly. Nevertheless, since more preferred and less preferred features (objects in this case) can still be defined, we illustrate the neural response using a hypothetical curve in object space. As such, here, tuning sharpening refers to the fact that the response to the more preferred object categories has been enhanced while the response to the less preferred stimulus categories is suppressed.”</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>a. The authors should address the apparent paradox noted above (and report whether it is seen in other regions of interest as well). On what model would the response to any pair of stimuli exceed that of the response to the preferred stimulus alone? This implies some kind of Gestalt interaction whereby the combined pair generates a percept that is even more effective for the voxels in question than the &quot;most preferred&quot; one?</p></disp-quote><p>The response to a pair of stimuli can exceed the response to each of the stimuli presented in isolation if the voxel is responsive to both stimuli and as long as the voxel has not reached its saturation level. This phenomenon has been reported in many previous studies (Zoccolan et al., 2005, Reddy et al., 2009, Ni et al., 2012, Doostani et al., 2023) and can be modeled using a linear combination model which does not limit the weights of the isolated responses to equal 1 (Doostani et al., 2023). Note that the “most preferred” stimulus does not necessarily saturate the voxel response, thus the response to two stimuli could be more effective based on voxel responsiveness to the second stimulus.</p><p>As for the current study, the labels “more preferred” and “less preferred” are only relatively defined (as explained in the Methods section), meaning that the more preferred stimulus is not necessarily the most preferred stimulus for the voxels. Furthermore, the presented stimuli are semi-transparent and presented with low-contrast, which moves the responses further away from the saturation level. Based on reported evidence for multiple-stimulus responses, responses to single stimuli are in many cases sublinearly added to yield the multiple-stimulus response (Zoccolan et al., 2005, Reddy et al., 2009, Doostani et al., 2023). This means that the multiple-stimulus response is lower than the sum of the isolated responses and not lower than each of the isolated responses. Therefore, it is not paradoxical to observe higher responses in paired conditions compared to the isolated conditions. We observe similar results in other ROIs, which we provide as supplementary figures to Figure 4 in the revised manuscript.</p><p>We address this observation and similar reports in previous studies in the Results section of the revised manuscript in lines 409-413:</p><p>“Note that the response in paired conditions can be higher or lower than the response to the isolated more preferred stimulus (condition Mat), depending on the voxel preference for the two presented stimuli, as previously reported (Doostani et al., 2023). This is consistent with previous studies reporting the response to multiple stimuli to be higher than the average, but lower than the sum of the response to isolated stimuli (Reddy et al., 2009).”</p><disp-quote content-type="editor-comment"><p>b. Paradox aside, I wondered to what extent the results are in part explained by range limits. Take two categories that evoke a highly similar response (either mean over a full ROI, or in the multivariate sense). That imposes a range limit such that attentional modulation, if it works the way we think it does, could only move responses within that narrow range. In contrast, the starting point for two highly dissimilar categories leaves room in principle for more modulation.</p></disp-quote><p>We do not believe that the results can be explained by range limits because responses in paired conditions are not limited by the isolated responses, as can be observed in Figure 4. However, to rule out the possibility of the similarity between responses in isolated conditions affecting the range within which responses in paired conditions can change, we turned to the multivariate analysis. We used the weight shift measure as the change in the weight of each stimulus with the change in the attentional target. In this method, no matter how close the two isolated vectors are, the response to the pair could still have a whole range of different weights of the isolated responses. We have plotted an example illustration of two-dimensional vectors for better clarification. Here, the vectors Vxat and Vyat denote the responses to the isolated x and y stimuli, respectively, and the vector Pxaty denotes the response to the paired condition in which stimulus x is attended. The weights a1 and a2 are illustrated in the figure, which are equal to regression coefficients if we solve the equation Pxaty = [a1 a2] [x y]’. While the weight values depend on the amplitude of and the angle between the three vectors, they are not limited by a lower angle between Vxat and Vyat.</p><p>We have updated Figure 2 in the manuscript to avoid the confusion. We have also added a figure including the sum of weights for different category pairs in different regions, showing that the sum of weights are not dependent on the similarity between the two stimuli. The conclusions based on the weight shift are therefore not confounded by the similarity between the two stimuli.</p><disp-quote content-type="editor-comment"><p>c. Finally, related to the previous point, while including V1 is a good control, I wonder if it is getting a &quot;fair&quot; test here, because the range of responses to the four categories in this region, in terms of (dis)similarity, seems compressed relative to the other categories.</p></disp-quote><p>We believe that V1 is getting a fair test because the single-subject range of category distance in V1 is similar to LO, as can be observed Author response image 1_:_</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Range of category distance in each ROI averaged across participants.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-sa2-fig1-v1.tif"/></fig><p>The reason that V1 is showing a more compressed distance range on the average plot is that the category distance in V1 is not consistent among participants. Although the average plots are shown in Figure 5 and Figure 6, we tested statistical significance in each ROI based on single-subject correlation coefficients.</p><p>Please also note that a more compressed range of dissimilarity does not necessarily lead to a less strong effect of category distance on the effect of attention. For instance, while LO shows a more compressed dissimilarity range for the presented categories compared to the other object selective regions, it shows the highest correlation between weight shift and category distance. Furthermore, as illustrated in Figure 5, no significant correlation is observed between univariate shift and category distance in V1, even though the range of the univariate distance in V1 is similar to LO and pFs, where we observed a significant correlation between category distance and univariate shift.</p><disp-quote content-type="editor-comment"><p>d. In general, the manuscript does a very good job explaining the methods of the study in a way that would allow replication. In some places, the authors could be clearer about the reasoning behind those methodological choices. For example: - How was the sample size determined?</p></disp-quote><p>Estimating conservatively based on the smallest amount of attentional modulation we observed in a previous study (Doostani et al., 2023), we chose a medium effect size (0.3). For a power of 0.8, the minimum number of participants should be 16. We have added the explanation to the Methods section in lines 78-81:</p><p>“We estimated the number of participants conservatively based on the smallest amount of attentional modulation observed in our previous study (Doostani et al., 2023). For a medium effect size of 0.3 and a power of 0.8, we needed a minimum number of 16 participants.”</p><disp-quote content-type="editor-comment"><p>- Why did the authors choose those four categories? What was the evidence that would suggest these would span the range of similarities needed here?</p></disp-quote><p>We chose these four categories based on a previous behavioral study reporting the average reaction time of participants when detecting a target from one category among distractors from another category (Xu and Vaziri-Pashkam, 2019). Ideally the experiment should include as many object categories as possible. However, since we were limited by the duration of the experiment, the number of conditions had to be controlled, leading to a maximum of 4 object categories. We chose two animate and two inanimate object categories to include categories that are more similar and more different based on previous behavioral results (Xu and Vaziri-Pashkam, 2019). We included body and house categories because they are both among the categories to which highly responsive regions exist in the cortex. We chose the two remaining categories based on their similarity to body and house stimuli. In this way, for each category there was another category that elicited similar cortical responses, and two categories that elicited different responses. While we acknowledge that the chosen categories do not fully span the range of similarities, they provide an observable variety of similarities in different ROIs which we find acceptable for the purposes of our study.</p><p>We include this information in the Methods section of the revised manuscript in lines 89-94:</p><p>“We included body and house categories because there are regions in the brain that are highly responsive and unresponsive to each of these categories, which provided us with a range of responsiveness in the visual cortex. We chose the two remaining categories based on previous behavioral results to include categories that provided us with a range of similarities (Xu and Vaziri-Pashkam, 2019). Thus, for each category there was a range of responsiveness in the brain and a range of similarity with the other categories.”</p><disp-quote content-type="editor-comment"><p>- Why did the authors present the stimuli at the same location? This procedure has been adopted in previous studies, but of course, it does also move the stimulus situation away from the real-world examples of cluttered scenes that motivate the Introduction.</p></disp-quote><p>We presented the stimuli at the same location because we aimed to study the mechanism of object-based attention and this experimental design helped us isolate it from spatial attention. We do not think that our design moves the stimulus situation away from real-world examples in such a way that our results are not generalizable. We include real-world instances, as well as a discussion on this point, in the Discussion section of the revised manuscript, in lines 611-620:</p><p>“Although examples of superimposed cluttered stimuli are not very common in everyday life, they still do occur in certain situations, for example reading text on the cellphone screen in the presence of reflection and glare on the screen or looking at the street through a patterned window. Such instances recruit object-based attention which was the aim of this study, whereas in more common cases in which attended and unattended objects occupy different locations in space, both space-based and object-based attention may work together to resolve the competition between different stimuli. Here we chose to move away from usual everyday scenarios to study the effect of object-based attention in isolation. Future studies can reveal the effect of target-distractor similarity, i.e. proximity in space, on space-based attention and how the effects caused by object-based and space-based attention interact.”</p><disp-quote content-type="editor-comment"><p>- While I'm not concerned about this (all relevant comparisons were within-participants) was there an initial attempt to compare data quality from the two different scanners?</p></disp-quote><p>We compared the SNR values of the two groups of participants and observed no significant difference between these values (ps &gt; 0.34, ts &lt; 0.97). We have added this information to the Methods section.</p><p>Regarding the observed effect, we performed a t-test between the results of the participants from the two scanners. For the univariate results, the observed correlation between univariate attentional modulation and category distance was not significantly different for participants of the two scanners in any ROIs (ps &gt; 0.07 , ts &lt; 1.9). For the multivariate results, the observed correlation between the weight shift and multivariate category distance was not significantly different in any ROIs (ps &gt; 0.48 , ts &lt; 0.71) except for V1 (p-value = 0.015 , t-value = 2.75).</p><p>We include a sentence about the comparison of the SNR values in the preprocessing section in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>e. There are a couple of analysis steps that could be applied to the existing data that might strengthen the findings. For one, the authors have adopted a liberal criterion of p &lt; 0.001 uncorrected to include voxels within each ROI. Why, and to what extent is the general pattern of findings robust over more selective thresholds? Also, there are additional regions that are selective for bodies (fusiform body area) and scenes (occipital place area and retrosplenial cortex). Including these areas might provide more diversity of selectivity patterns (e.g. different responses to non-preferred categories) that would provide further tests of the hypothesis.</p></disp-quote><p>We selected this threshold to allow for selection of a reasonable number of voxels in each hemisphere across all participants. To check whether the effect is robust over more selective thresholds, we exemplarily redefined the left EBA region using p &lt; 0.0001 and p &lt; 0.00001 and observed that the weight shift effect remained equivalent. We have made a note of this analysis in the Results section. As for the additional regions suggested by the reviewer, we chose not to include them because they could not be consistently defined in both hemispheres of all participants. Please note that the current ROIs also show different responses to non-preferred categories (e.g. in LO and pFs). We include this information in the Methods section in lines 206-207:</p><p>“We selected this threshold to allow for selection of a reasonable number of voxels in each hemisphere across all participants.”</p><p>And in the Results section in lines 509-512:</p><p>“We performed the analysis including only voxels that had a significantly positive GLM coefficient across the runs and observed the same results. Moreover, to check whether the effect is robust over more selective thresholds for ROI definition, we redefined the left EBA region with p &lt; 0.0001 and p &lt; 0.00001 criteria. We observed a similar weight shift effect for both criteria.”</p><disp-quote content-type="editor-comment"><p>f. One point the authors might address is the potential effect of blocking the paired conditions. If I understood right, the irrelevant item in each paired display was from the same category throughout a block. To what extent might this knowledge shape the way participants attend to the task-relevant item (e.g. by highlighting to them certain spatial frequencies or contours that might be useful in making that particular pairwise distinction)? In other words, are there theoretical reasons to expect different effects if the irrelevant category is not predictable?</p></disp-quote><p>We believe that the participants’ knowledge about the distractor does not significantly affect our results because our results are in agreement with previous behavioral data (Cohen et al., 2014, Xu and Vaziri-Pashkam, 2019), in which the distractor could not be predicted. These reports suggest there is a theoretical reason to expect similar effects if the participants could not predict the distractor. To directly test this, one would need to perform an fMRI experiment using an event-related design, an interesting venue for future research.</p><p>We have made a note of this point in the Discussion section of the revised manuscript in lines 621-626:</p><p>“Please note that we used a blocked design in which the target and distractor categories could be predicted across each block. While it is possible that the current design has led to an enhancement of the observed effect, previous behavioral data (Cohen et al., 2014, Xu and Vaziri-Pashkam, 2019) have reported the same effect in experiments in which the distractor was not predictable. To study the effect of predictability on fMRI responses, however, an event-related design is more appropriate, an interesting venue for future fMRI studies.”</p><disp-quote content-type="editor-comment"><p>g. The authors could provide behavioural data as a function of the specific category pairs. There is a clear prediction here about which pairs should be more or less difficult.</p></disp-quote><p>We provide the behavioral data as a supplementary figure to Figure 1 in the revised manuscript. We however do not see differences in behavior for the different category paris. This is so because our fMRI task was designed in a way to make sure the participants could properly attend to the target for all conditions. The task was rather easy across all conditions and due to the ceiling effect, there was no significant difference between behavioral performance for different category pairs. However, the effect of category pair on behavior has been previously tested and reported in a visual search paradigm with the same categories (Xu and Vaziri-Pashkam, 2019), which was in fact the basis for our choice of categories in this study (as explained in response to point “d” above).</p><disp-quote content-type="editor-comment"><p>h. Figure 4 shows data for EBA in detail; it would be helpful to have a similar presentation of the data for the other ROIs as well.</p></disp-quote><p>We provide data for all ROIs as figure supplements 1-4 to Figure 4 in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>i. For the pFs and LOC ROIs, it would be helpful to have an indication of what proportion of voxels was most/least responsive to each of the four categories. Was this a relatively even balance, or generally favouring one of the categories?</p></disp-quote><p>In LO, the proportion of voxels most responsive to each of the four categories was relatively even for Body (31%) and House (32%) stimuli, which was higher than the proportion of Car- and Cat-preferring voxels (18% and 19%, respectively). In pFs, 40% of the voxels were house-selective, while the proportion was relatively even for voxels most responsive to bodies, cars, and houses with 21%, 17%, and 22% of the voxels, respectively. We include the percentage of voxels most responsive to each of the four categories in each ROI as Appendix 1-table 1.</p><disp-quote content-type="editor-comment"><p>j. Were the stimuli in the localisers the same as in the main experiment?</p></disp-quote><p>No, we used different sets of stimuli for the localizers and the main experiment. We have added the information in line 146 of the Methods section.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>(1) Why are specific ROIs chosen? Perhaps some discussion motivating these choices, and addressing the possible overlap between these and retinotopic regions (based on other studies, or atlases - Wang et al, 2015) would be useful.</p></disp-quote><p>Considering that we used object categories, we decided to look at general object-selective regions (LO, pFS) as well as regions that are highly selective for specific categories (EBA, PPA). We also looked at the primary visual cortex as a control region. We have added this clarification in the Methods section lines 128-133:</p><p>“Considering that we used object categories, we investigated five different regions of interest (ROIs): the object-selective areas lateral occipital cortex (LO) and posterior fusiform (pFs) as general object-selective regions, the body-selective extrastriate body area (EBA) and the scene-selective parahippocampal place area (PPA) as regions that are highly selective for specific categories, and the primary visual cortex (V1) as a control region. We chose these regions because they could all be consistently defined in both hemispheres of all participants and included a large number of voxels.”</p><disp-quote content-type="editor-comment"><p>(2) The authors should consider including data on the relative prevalence of voxels preferring each category for each ROI (and/or the mean activation level across voxels for each category for each ROI). If some ROIs have very few voxels preferring some categories, there's a chance the observed results are a bit noisy when sorting based on those categories (e.g., if a ROI has essentially no response to a given pair of categories, then there's not likely to be much attentional modulation detectable, because the ROI isn't driven by those categories to begin with).</p></disp-quote><p>We thank the reviewer for the insightful comment.</p><p>We include the percentage of voxels most responsive to each of the four categories in each ROI in the Appendix (Appendix 1-table 1, please see the answer to point “i” of the first reviewer).</p><p>We also provide a table of average activity across voxels for each category in all ROIs as Appendix 1-table 2.</p><p>As shown in the table, voxels show positive activity for all categories in all ROIs except for PPA, where voxels show no response to body and cat stimuli. This might explain why we observed a marginally significant correlation between weight shift and category distance in PPA only. As the reviewer mentions, since this region does not respond to body and cat stimuli, we do not observe a significant change in response due to the shift in attention for some pairs. We include the table in the Appendix and add the explanation to the Results section of the revised manuscript in lines 506-508:</p><p>_“_Less significant results in PPA might arise from the fact that PPA shows no response to body and cat stimuli and little response to car stimuli (Appendix 1-table 2). Therefore, it is not possible to observe the effect of attention for all category pairs.”</p><disp-quote content-type="editor-comment"><p>a. Related - would it make sense to screen voxels for inclusion in analysis based on above-basely activation for one or both of the categories? [could, for example, imagine you're accidentally measuring from the motor cortex - you'd be able to perform this analysis, but it would be largely nonsensical because there's no established response to the stimuli in either isolated or combined states].</p></disp-quote><p>We performed all the analyses including only voxels that had a significantly positive GLM coefficient across the runs and the results remained the same. We have added the explanation in the Results section in line 509-510.</p><disp-quote content-type="editor-comment"><p>(3) Behavioral performance is compared against chance level, but it doesn't seem that 50% is chance for the detection task. The authors write on page 4 that the 1-back repetition occurred between 2-3 times per block, so it doesn't seem to be the case that each stimulus had a 50% chance of being a repetition of the previous one.</p></disp-quote><p>We apologize for the mistake in our report. We have reported the detection rate for the target-present trials (2-3 per block), not the behavioral performance across all trials. We have modified the sentence in the Results section.</p><disp-quote content-type="editor-comment"><p>(4) Authors mention that the stimuli are identical for 2-stimulus trials where each category is attended (for a given pair) - but the cue is different, and the cue appears as a centrally-fixated word for 1 s. Is this incorporated into the GLM? I can't imagine this would have much impact, but the strict statement that the goals of the participant are the only thing differentiating trials with otherwise-identical stimuli isn't quite true.</p></disp-quote><p>The word cue was not incorporated as a separate predictor into the GLM. As the reviewer notes, the signals related to the cue and stimuli are mixed. But given that the cues are brief and in the form of words rather than images, they are unlikely to have an effect on the response in the regions of interest.</p><p>To be more accurate, we have included the clarification in the Methods section in lines 181-182:</p><p>“We did not enter the cue to the GLM as a predictor. The obtained voxel-wise coefficients for each condition are thus related to the cue and the stimuli presented in that condition.”</p><p>And in the Results section in lines 425-428 :</p><p>“It is important to note that since the cue was not separately modeled in the GLM, the signals related to the cue and the stimuli were mixed. However, given that the cues were brief and presented in the form of words, they are unlikely to have an effect on the responses observed in the higher-level ROIs.”</p><disp-quote content-type="editor-comment"><p>(5) Eq 5: I expected there to be some comparison of a and b directly as ratios (e.g., a_1 &gt; b_1, as shown in Fig. 2). The equations used here should be walked through more carefully - it's very hard to understand what this analysis is actually accomplishing. I'm not sure I follow the explanation of relative weights given by the authors, nor how that maps onto the delta_W quantity in Equation 5.</p></disp-quote><p>We provide a direct comparison of a and b, as well as a more thorough clarification of the analysis, in the Methods section in lines 274-276:</p><p>“We first projected the paired vector on the plane defined by the isolated vectors (Figure 2A) and then determined the weight of each isolated vector in the projected vector (Figure 2B).”</p><p>And in lines 286-297:</p><p>“A higher a1 compared to a2 indicates that the paired response pattern is more similar to Vxat compared to Vyat, and vice versa. For instance, if we calculate the weights of the Body and Car stimuli in the paired response related to the simultaneous presentation of both stimuli, we can write in the LO region: VBodyatCar = 0.81 VBody + 0.31 VCar, VBodyCarat = 0.43 VBody + 0.68 VCar. Note that these weights are averaged across participants. As can be observed, in the presence of both body and car stimuli, the weight of each stimulus is higher when attended compared to the case when it is unattended. In other words, when attention shifts from body to car stimuli, the weight of the isolated body response (VBody) decreases in the paired response. We can therefore observe that the response in the paired condition is more similar to the isolated body response pattern when body stimuli are attended and more similar to the isolated car response pattern when car stimuli are attended.”</p><p>And lines 303-306:</p><p>“As shown here, even when body stimuli are attended, the effect of the unattended car stimuli is still present in the response, shown in the weight of the isolated car response (0.31). However, this weight increases when attention shifts towards car stimuli (0.68 in the attended case).”</p><p>We also provide more detailed clarification for the 𝛥w and the relative weights in lines 309-324:</p><p>“To examine whether this increase in the weight of the attended stimulus was constant or depended on the similarity of the two stimuli in cortical representation, we defined the weight shift as the multivariate effect of attention:</p><p>𝛥w = a1/(a1+a2) – b1/(b1+b2) (5)</p><p>Here, a1, a2, b1,and b2 are the weights of the isolated responses, estimated using Equation 4. We calculate the weight of the isolated x response once when attention is directed towards x (a1), and a second time when attention is directed towards y (b1). In each case, we calculate the relative weight of the isolated x in the paired response by dividing the weight of the isolated x by the sum of weights of x and y (a1+a2 when attention is directed towards x, and b1+b2 when attention is directed towards y). We then define the weight shift, Δw, as the change in the relative weight of the isolated x response in the paired response when attention shifts from x to y. A higher Δw for a category pair indicates that attention is more efficient in removing the effect of the unattended stimulus in the pair. We used relative weights as a normalized measure to compensate for the difference in the sum of weights for different category pairs. Thus, using the normalized measure, we calculated the share of each stimulus in the paired response. For instance, considering the Body-Car pair, the share of the body stimulus in the paired response was equal to 0.72 and 0.38, when body stimuli were attended and unattended, respectively. We then calculated the change in the share of each stimulus caused by the shift in attention using a simple subtraction (Equation 5: Δw=0.34 for the above example of the Body-Car pair in LO) and used this measure to compare between different pairs.”</p><p>We hope that this clarification makes it easier to understand the multivariate analysis and the weight shift calculation in Equation 5.</p><p>We additionally provide the values of the weights (a1, b1, a2, and b2) for each category pair averaged across participants as Appendix 1 -table 4.</p><disp-quote content-type="editor-comment"><p>(6) For multivariate analyses (Fig. 6A-E), x axis is normalized (pattern distance based on Pearson correlation), while the delta_W does not seem to be similarly normalized.</p></disp-quote><p>We calculated ΔW by dividing the weights in each condition by the sum of weights in that condition. Thus, we use relative weights which are always in the range of 0 to 1, and ΔW is thus always in the range of -1 to 1. This means that both axes are normalized. Note that even if one axis were not normalized, the relationship between the independent and the dependent variables would remain the same despite the change in the range of the axis.</p><disp-quote content-type="editor-comment"><p>(7) Simulating additional scenarios like attention to both categories just increasing the mean response would be helpful - is this how one would capture results like those shown in some panels of Fig. 4?</p></disp-quote><p>We did not have a condition in which participants were asked to attend to both categories. Therefore it was not useful for our simulations to include such a scenario. Please also note that the goal of our simulations is not to capture the exact amount of attentional modulation, but to investigate the effect of target-distractor similarity on the change in attentional modulation (univariate shift and weight shift).</p><p>As for the results in some panels of Figure 4, we have explained the reason underlying higher responses in paired conditions compared to isolated conditions in response to the “weaknesses” section of the second reviewer. We hope that these points satisfy the reviewer’s concern regarding the results in Figure 4 and our simulations.</p><disp-quote content-type="editor-comment"><p>(8) Lines 271-276 - the &quot;latter&quot; and &quot;former&quot; are backwards here I think.</p></disp-quote><p>We believe that the sentence was correct, but confusing.. We have rephrased the sentence to avoid the confusion in lines 371-376 of the revised manuscript:</p><p>“We modeled two neural populations: a general object-selective population in which each voxel shows preference to a particular category and voxels with different preferences are mixed in with each other (similar to LO and pFS), and a category-selective population in which all voxels have a similar preference for a particular category (similar to EBA and PPA).”</p><disp-quote content-type="editor-comment"><p>(9) Line 314 - &quot;body-car&quot; pair is mentioned twice in describing the non-significant result in PPA ROI.</p></disp-quote><p>Thank you for catching the typo. We have changed the second Body-Car to Body-Cat.</p><disp-quote content-type="editor-comment"><p>(10) Fig. 5 and Fig. 6 - I was expecting to see a plot that demonstrated variability across subjects rather than across category pairs. Would it be possible to show the distribution of each pair's datapoints across subjects, perhaps by coloring all (e.g.) body-car datapoints one color, all body-cat datapoints another, etc? This would also help readers better understand how category preferences (which differ across ROIs) impact the results.</p></disp-quote><p>We demonstrated variability across category pairs rather than subjects because we aimed to investigate how the variation in the similarity between categories (i.e. category distance) affected the univariate and multivariate effects of attention. The variability across subjects is reflected in the error bars in the bar plots of Figure 5 and Figure 6.</p><p>Here we show the distribution of each category pair’s data points across subjects by using a different color for each pair:</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Univariate shift versus category distance including single-subject data points in all ROIs.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-sa2-fig2-v1.tif"/></fig><fig id="sa2fig3" position="float"><label>Author response image 3.</label><caption><title>Weight shift versus category distance including single-subject data points in all ROIs.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89836-sa2-fig3-v1.tif"/></fig><p>As can be observed in the figures, category preference has little impact on the results. Rather, the similarity in the preference (in the univariate case) or the response pattern (in the multivariate case) to the two presented categories is what impacts the amount of the univariate shift and the weight shift, respectively. For instance, in EBA we observe a low amount of attentional shift both for the Body-Cat pair, with two stimuli for which the ROI is highly selective, and the Car-House pair, including stimuli to which the region shows little response. A similar pattern is observed in the object-selective regions LO and pFs which show high responses to all stimulus categories.</p><p>We believe that the figures including the data points related to all subjects are not strongly informative. However, we agree that using different colors for each category pair helps the readers better understand that category preference has little impact on the results in different ROIs. We therefore present the colored version of Figure 5 and Figure 6 in the revised manuscript, with a different color for each category pair.</p><disp-quote content-type="editor-comment"><p>(11) Fig. 5 and Fig. 6 use R^2 as a dependent variable across participants to conclude a positive relationship. While the positive relationship is clear in the scatterplots, which depict averages across participants for each category pair, it could still be the case that there are a substantial number of participants with negative (but predictive, thus high positive R^2) slopes. For completeness and transparency, the authors should illustrate the average slope or regression coefficient for each of these analyses.</p></disp-quote><p>We concluded the positive relationship and calculated the significance in Figure 5 and Figure 6 using the correlation r rather than r.^2 This is why the result was not significantly positive in V1. We acknowledge that the use of r-squared in the bar plot leads to confusion. We have therefore changed the bar plots to show the correlation coefficient instead of the r-squared. Furthermore, we have added a table of the correlation coefficient for all participants in all ROIs for the univariate and weight shift analyses supplemental to Figure 5 and Figure 6, respectively.</p><disp-quote content-type="editor-comment"><p>(12) No statement about data or analysis code availability is provided</p></disp-quote><p>Thanks for pointing this out. The fMRI data is available on OSF. We have added a statement about it in the Data Availability section of the revised manuscript in line 669.</p></body></sub-article></article>