<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">98521</article-id><article-id pub-id-type="doi">10.7554/eLife.98521</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.98521.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Decoding the physics of observed actions in the human brain</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Wurm</surname><given-names>Moritz F</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4358-9815</contrib-id><email>moritz.wurm@unitn.it</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Erigüç</surname><given-names>Doruk Yiğit</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0003-8073-8135</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05trd4x28</institution-id><institution>CIMeC – Center for Mind/Brain Sciences, University of Trento</institution></institution-wrap><addr-line><named-content content-type="city">Rovereto</named-content></addr-line><country>Italy</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Press</surname><given-names>Clare</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>10</day><month>02</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP98521</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-04-29"><day>29</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-04-05"><day>05</day><month>04</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.04.560860"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-08-12"><day>12</day><month>08</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98521.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-24"><day>24</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.98521.2"/></event></pub-history><permissions><copyright-statement>© 2024, Wurm and Erigüç</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Wurm and Erigüç</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-98521-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-98521-figures-v1.pdf"/><abstract><p>Recognizing goal-directed actions is a computationally challenging task, requiring not only the visual analysis of body movements, but also analysis of how these movements causally impact, and thereby induce a change in, those objects targeted by an action. We tested the hypothesis that the analysis of body movements and the effects they induce relies on distinct neural representations in superior and anterior inferior parietal lobe (SPL and aIPL). In four fMRI sessions, participants observed videos of actions (e.g. breaking stick, squashing plastic bottle) along with corresponding point-light-display (PLD) stick figures, pantomimes, and abstract animations of agent–object interactions (e.g. dividing or compressing a circle). Cross-decoding between actions and animations revealed that aIPL encodes abstract representations of action effect structures independent of motion and object identity. By contrast, cross-decoding between actions and PLDs revealed that SPL is disproportionally tuned to body movements independent of visible interactions with objects. Lateral occipitotemporal cortex (LOTC) was sensitive to both action effects and body movements. These results demonstrate that parietal cortex and LOTC are tuned to physical action features, such as how body parts move in space relative to each other and how body parts interact with objects to induce a change (e.g. in position or shape/configuration). The high level of abstraction revealed by cross-decoding suggests a general neural code supporting mechanical reasoning about how entities interact with, and have effects on, each other.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>action recognition</kwd><kwd>biological motion</kwd><kwd>intuitive physics</kwd><kwd>fMRI</kwd><kwd>MVPA</kwd><kwd>RSA</kwd><kwd>multivariate pattern analysis</kwd><kwd>representational similarity analysis</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009629</institution-id><institution>Caritro Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Erigüç</surname><given-names>Doruk Yiğit</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Inferior parietal and lateral occipitotemporal cortex encode the effects of actions at an abstract level of representation, independently of the body movements that induce them.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Action recognition is central for navigating social environments, as it provides the basis for understanding others’ intentions, predicting future events, and social interaction. Many actions aim to induce a change in the world, often targeting inanimate objects (e.g. opening or closing a door) or persons (e.g. kissing or hitting someone). Recognizing such goal-directed actions is a computationally challenging task, as it requires not only the temporospatial processing of body movements, but also processing of how the body interacts with, and thereby induces an effect on, the object targeted by the action, for example a change in location, shape, or state. While a large body of work has investigated the neural processing of observed body movements as such (<xref ref-type="bibr" rid="bib16">Grossman et al., 2000</xref>; <xref ref-type="bibr" rid="bib12">Giese and Poggio, 2003</xref>; <xref ref-type="bibr" rid="bib34">Puce and Perrett, 2003</xref>; <xref ref-type="bibr" rid="bib32">Peuskens et al., 2005</xref>; <xref ref-type="bibr" rid="bib31">Peelen et al., 2006</xref>), the neural mechanisms underlying the analysis of action effects, and how the representations of body movements and action effects differ from each other, remain unexplored.</p><p>The recognition of action effects builds on a complex analysis of spatial and temporal relations between entities. For example, recognizing a given action as ‘opening a door’ requires the analysis of how different objects or object parts (e.g. door and doorframe) spatially relate to each other and how these spatial relations change over time. The specific interplay of temporospatial relations is usually characteristic for an action type (e.g. opening, as opposed to closing), independent of the concrete target object (e.g. door or trash bin), and is referred to here as <italic>action effect structure</italic> (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In addition, action effects are often independent of specific body movements – for example, we can open a door by pushing or by pulling the handle, depending on which side of the door we are standing on. This suggests that body movements and the effects they induce might be at least partially processed independently from each other. In this study, we define action effects as induced by intentional agents, but the notion of action effect structures might be generalizable to physical changes as such (e.g. an object’s change of location or configuration, independently of whether the change is induced by an agent or not). Moreover, we argue that representations of action effect structures are distinct of conceptual action representations: The former capture the temporospatial structure of an object change (e.g. the separation of a closing object element), the latter capture the meaning of an action (e.g. bringing an object into an opened state to make something accessible) and can also be activated via language (e.g. by reading ‘she opens the box’). Previous research suggests that conceptual action knowledge is represented in left anterior lateral occipitotemporal cortex (LOTC) (<xref ref-type="bibr" rid="bib45">Watson et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Lingnau and Downing, 2015</xref>; <xref ref-type="bibr" rid="bib50">Wurm and Caramazza, 2022</xref>) whereas structural representations of action effects have not been investigated yet.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic illustration of action effect structures and cross-decoding approach.</title><p>(<bold>A</bold>) Simplified schematic illustration of the action effect structure of ‘opening’. Action effect structures encode the specific interplay of temporospatial object relations that are characteristic for an action type independently of the concrete object (e.g. a state change from closed to open). (<bold>B</bold>) Cross-decoding approach to isolate representations of action effect structures and body movements. Action effect structure representations were isolated by training a classifier to discriminate neural activation patterns associated with actions (e.g. ‘breaking a stick’) and testing the classifier on its ability to discriminate activation patterns associated with corresponding abstract action animations (e.g. ‘dividing’). Body movement representations were isolated by testing the classifier trained with actions on activation patterns of corresponding point-light-display (PLD) stick figures.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig1-v1.tif"/></fig><p>We argue that object- and movement-general representations of action effect structures are necessary for the recognition of goal-directed actions as they allow for inferring the induced effect (e.g. that something is opened) independently of specific, including novel, objects. Strikingly, humans recognize actions even in the absence of any object- and body-related information: In the animations of <xref ref-type="bibr" rid="bib18">Heider and Simmel, 1944</xref>, the only available cues are abstract geometrical shapes and how these shapes move relative to each other and to scene elements. Yet, humans automatically and effortlessly attribute actions to these animations (e.g. opening, chasing, hiding, kissing), which argues against an inferential process and rather points toward an evolutionary optimized mechanism in the service of action recognition. Here, we test for the existence of a processing stage in the action recognition hierarchy that encode action effect representations independently from representations of body movements. We argue that both the recognition of body movements and the effects they induce rely critically on distinct but complementary subregions in parietal cortex, which is associated with visuospatial processing (<xref ref-type="bibr" rid="bib14">Goodale and Milner, 1992</xref>; <xref ref-type="bibr" rid="bib23">Kravitz et al., 2011</xref>), action recognition (<xref ref-type="bibr" rid="bib5">Caspers et al., 2010</xref>), and mechanical reasoning about manipulable objects (<xref ref-type="bibr" rid="bib2">Binkofski and Buxbaum, 2013</xref>; <xref ref-type="bibr" rid="bib24">Leshinskaya et al., 2020</xref>) and physical events (<xref ref-type="bibr" rid="bib8">Fischer et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Fischer and Mahon, 2021</xref>). Specifically, we hypothesize that the neural analysis of action effects relies on anterior inferior parietal lobe (aIPL), whereas the analysis of body movements relies on superior parietal lobe (SPL). aIPL shows a representational profile that seems ideal for the processing of action effect structures at a high level of generality: Action representations in bilateral aIPL generalize across perceptually variable action exemplars, such as opening a bottle or a box (<xref ref-type="bibr" rid="bib47">Wurm and Lingnau, 2015</xref>; <xref ref-type="bibr" rid="bib17">Hafri et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Vannuscorps et al., 2019</xref>), as well as structurally similar actions and object events, for example, a girl kicking a chair and a ball bouncing against a chair (<xref ref-type="bibr" rid="bib20">Karakose-Akbiyik et al., 2023</xref>). Moreover, aIPL is critical for understanding how tools can be used to manipulate objects (<xref ref-type="bibr" rid="bib13">Goldenberg and Spatt, 2009</xref>; <xref ref-type="bibr" rid="bib36">Reynaud et al., 2016</xref>). More generally, aIPL belongs to a network important for physical inferences of how objects move and impact each other (<xref ref-type="bibr" rid="bib8">Fischer et al., 2016</xref>).</p><p>Also the recognition of body movements builds on visuospatial and temporal processing, but their representation should be more specific for certain movement trajectories (e.g. pulling the arm toward the body, regardless of the movement’s intent to open or close a door). The visual processing of body movements has been shown to rely on posterior superior temporal sulcus (<xref ref-type="bibr" rid="bib16">Grossman et al., 2000</xref>; <xref ref-type="bibr" rid="bib12">Giese and Poggio, 2003</xref>; <xref ref-type="bibr" rid="bib34">Puce and Perrett, 2003</xref>; <xref ref-type="bibr" rid="bib32">Peuskens et al., 2005</xref>; <xref ref-type="bibr" rid="bib31">Peelen et al., 2006</xref>). However, recent research found that also SPL, but less so aIPL, encodes observed body movements: SPL is more sensitive in discriminating actions (e.g. a girl kicking a chair) than structurally similar object events (e.g. a ball bouncing against a chair) (<xref ref-type="bibr" rid="bib20">Karakose-Akbiyik et al., 2023</xref>). Moreover, point-light-displays (PLDs) of actions, which convey only motion-related action information but not the interactions between the body and other entities, can be decoded with higher accuracy in SPL compared to aIPL (<xref ref-type="bibr" rid="bib51">Yargholi et al., 2023</xref>). Together, these findings support the hypothesis of distinct neural systems for the processing of observed body movements in SPL and the effect they induce in aIPL.</p><p>Using an fMRI-based cross-decoding approach (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), we isolated the neural substrates for the recognition of action effects and body movements in parietal cortex. Specifically, we demonstrate that aIPL encodes abstract representations of action effect structures independently of motion and object identity, whereas SPL is more tuned to body movements irrespective of visible effects on objects. Moreover, cross-decoding between pantomimes and animations revealed that right aIPL represents action effects even in response to implied object interactions. These findings elucidate the neural basis of understanding the physics of actions, which is a key stage in the processing hierarchy of action recognition.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To isolate neural representations of action effect structures and body movements from observed actions, we used a cross-decoding approach: In four separate fMRI sessions, right-handed participants observed videos of actions (e.g. breaking a stick, squashing a plastic bottle) along with corresponding PLD stick figures, pantomimes, and abstract animations of agent–object interactions (<xref ref-type="fig" rid="fig2">Figure 2</xref>) while performing a simple catch-trial-detection task (see Methods for details).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Experimental design.</title><p>In four fMRI sessions, participants observed 2-s-long videos of five actions and corresponding animations, point-light-display (PLD) stick figures, and pantomimes. For each stimulus type, eight perceptually variable exemplars were used (e.g. different geometric shapes, persons, viewing angles, and left–right flipped versions of the videos). A fixed order of sessions from abstract animations to naturalistic actions was used to minimize memory and imagery effects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig2-v1.tif"/></fig><p>To identify neural representations of action effect structures, we trained a classifier to discriminate the neural activation patterns associated with the action videos, and then tested the classifier on its ability to discriminate the neural activation patterns associated with the animations (and vice versa). We thereby isolated the component that is shared between the naturalistic actions and the animations – the perceptually invariant action effect structure – irrespective of other action features, such as motion, object identity, and action-specific semantic information (e.g. the specific meaning of ‘breaking a stick’).</p><p>Likewise, to isolate representations of body movements independently of the effect they have on target objects, we trained a classifier on action videos and tested it on the PLD stick figures (and vice versa). We thereby isolated the component that is shared between the naturalistic actions and the PLD stick figures – the coarse body movement patterns – irrespective of action features related to the target object, such as the way they are grasped and manipulated, and the effect induced by the action.</p><p>Additionally, we used pantomimes of the actions, which are perceptually richer than the PLD stick figures and provide more fine-grained information about hand posture and movements. Thus, pantomimes allow inferring how an object is grasped and manipulated. Using cross-decoding between pantomimes and animations, we tested whether action effect representations are sensitive to implied hand–object interactions or require a visible object change.</p><sec id="s2-1"><title>Cross-decoding of action effect structures and body movements</title><p>We first tested whether aIPL is more sensitive in discriminating abstract representations of action effect structures, whereas SPL is more sensitive to body movements. Action-animation cross-decoding revealed significant decoding accuracies above chance in left aIPL but not left SPL, as well as in right aIPL and, to a lesser extent, in right SPL (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Action-PLD cross-decoding revealed the opposite pattern of results, that is, significant accuracies in SPL and, to a lesser extent, in aIPL. A repeated measures ANOVA with the factors region of interest (ROI; aIPL, SPL), TEST (action-animation, action-PLD), and HEMISPHERE (left, right) revealed a significant interaction between ROI and TEST (<italic>F</italic>(1,23) = 35.03, p = 4.9E−06), confirming the hypothesis that aIPL is more sensitive to effect structures of actions, whereas SPL is more sensitive to body movements. Post hoc <italic>t</italic>-tests revealed that, for action-animation cross-decoding in both left and right hemispheres, decoding accuracies were higher in aIPL than in SPL (left: <italic>t</italic>(23) = 1.81, p = 0.042, right: 4.01, p = 0.0003, one-tailed), whereas the opposite effects were found for action-PLD cross-decoding (left: <italic>t</italic>(23) = −4.17, p = 0.0002, right: −2.93, p = 0.0038, one-tailed). Moreover, we found ANOVA main effects of TEST (<italic>F</italic>(1,23) = 33.08, p = 7.4E−06), indicating stronger decoding for action-PLD versus action-animation cross-decoding, and of HEMISPHERE (<italic>F</italic>(1,23) = 12.75, p = 0.0016), indicating stronger decoding for right versus left ROIs. An interaction between TEST and HEMISPHERE indicated that action-animation cross-decoding was disproportionally stronger in the right versus left hemisphere (<italic>F</italic>(1,23) = 9.94, p = 0.0044).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Cross-decoding of action effect structures (action-animation) and body movements (action-PLD).</title><p>(<bold>A</bold>) Region of interest (ROI) analysis in left and right anterior inferior parietal lobe (aIPL) and superior parietal lobe (SPL) (Brodmann areas 40 and 7, respectively; see Methods for details). Decoding of action effect structures (action-animation cross-decoding) is stronger in aIPL than in SPL, whereas decoding of body movements (action-PLD cross-decoding) is stronger in SPL than in aIPL. Asterisks indicate FDR-corrected significant decoding accuracies above chance (*p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001, ****p &lt; 0.0001). Error bars indicate SEM (N=24). (<bold>B</bold>) Mean accuracy whole-brain maps thresholded using Monte-Carlo correction for multiple comparisons (voxel threshold p = 0.001, corrected cluster threshold p = 0.05). Action-animation cross-decoding is stronger in the right hemisphere and reveals additional representations of action effect structures in lateral occipitotemporal cortex (LOTC).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Cross-decoding of action effect structures (action-animation) and body movements (action-PLD) in left and right LOTC, pSTS, and V1 (respectively; see Methods for details on region of interest [ROI] definition).</title><p>Dark tones show effects in left ROIs, light tones show effects in right ROIs. Asterisks indicate FDR-corrected significant decoding accuracies above chance (***p &lt; 0.001, ****p &lt; 0.0001). Error bars indicate SEM (N=24).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig3-figsupp1-v1.tif"/></fig></fig-group><p>These findings were corroborated by the results of a searchlight analysis (<xref ref-type="fig" rid="fig3">Figure 3B</xref>): Within the parietal cortex, action-animation cross-decoding revealed a cluster peaking in right aIPL, whereas the parietal clusters for the action-PLD cross-decoding peaked in bilateral SPL. The whole-brain results further demonstrated overall stronger decoding for action-PLD throughout the action observation network, and in particular in LOTC extending into pSTS (see also <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for ROI results in LOTC and pSTS), which was expected because of the similarity of movement kinematics between the naturalistic actions and the PLDs. Note that we were not interested in the representation of movement kinematics in the action observation network as such, but in testing the specific hypothesis that SPL is disproportionally sensitive to movement kinematics as opposed to aIPL. Interestingly, the action-animation cross-decoding searchlight analysis revealed an additional prominent cluster in right LOTC (and to a lesser extent in left LOTC), suggesting that not only aIPL is critical for the representation of effect structures, but also right LOTC. We therefore include LOTC in the following analyses and discussion. In addition, we observed subtle but significant above chance decoding for action-animation in bilateral early visual cortex (EVC; see also <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This was surprising because the different stimulus types (action videos and animations) should not share low-level visual features. However, it is possible that there were coincidental similarities between action videos and animations that were picked up by the classifier. To assess this possibility, we tested whether the five actions can be cross-decoded using motion energy features extracted from the action videos and animations (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). This analysis revealed significant above chance decoding accuracy (30%), suggesting that actions and animations indeed contain coincidental visual similarities. To test whether these similarities can explain the effects observed in V1, we used the motion energy decoding matrix as a model for a representational similarity analysis (RSA; see Results section ‘Representational geometry of action-structure- and body-motion-related representations’).</p></sec><sec id="s2-2"><title>Representation of implied versus visible action effects</title><p>Humans can recognize many goal-directed actions from mere body movements, as in pantomime. This demonstrates that the brain is capable of inferring the effect that an action has on objects based on the analysis of movement kinematics without the analysis of a visible interaction with an object. Inferring action effects from body movements is easier via pantomimes than with PLD stick figures, because the former provide richer and more fine-grained body information, and in the case of object manipulations, object information (e.g. shape) implied by the pantomimed grasp. Hence, neither pantomimes nor PLDs contain visible information about objects and action effects, but this information is more easily accessible in pantomimes than in PLDs. This difference between pantomimes and PLDs allows testing whether there are brain regions that represent effect structures in the absence of visual information about objects and action effects. We focused on brain regions that revealed the most robust decoding of action effect structures, that is, aIPL and LOTC (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for results in SPL). We first tested for sensitivity to implied effect structures by comparing the cross-decoding of actions and pantomimes (strongly implied hand-object interaction) with the cross-decoding of actions and PLDs (less implied hand-object interaction). This was the case in both aIPL and LOTC: action-pantomime cross-decoding revealed higher decoding accuracies than action-PLD cross-decoding (<xref ref-type="fig" rid="fig4">Figure 4A</xref>; all <italic>t</italic>(23) &gt; 3.54, all p &lt; 0.0009; one-tailed). The same pattern should be observed in the comparison of action-pantomime and pantomime-PLD cross-decoding, which was indeed the case (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; all <italic>t</italic>(23) &gt; 2.96, all p &lt; 0.0035; one-tailed). These findings suggest that the representation of action effect structures in aIPL does not require a visible interaction with an object. However, the higher decoding across actions and pantomimes might also be explained by the higher visual and kinematic similarity between actions and pantomimes, in particular the shared information about hand posture and hand movements, which are not present in the PLDs. A more selective test is therefore the comparison of animation-pantomime and animation-PLD cross-decoding: as the animations do not provide any body-related information, a difference can only be explained by the stronger matching of effect structures between animations and pantomimes. We found higher cross-decoding for animation-pantomime versus animation-PLD in right aIPL and bilateral LOTC (all <italic>t</italic>(23) &gt; 3.09, all p &lt; 0.0025; one-tailed), but not in left aIPL (<italic>t</italic>(23) = 0.73, p = 0.23, one-tailed). However, a repeated measures ANOVA revealed no significant interaction between TEST (animation-pantomime, animation-PLD) and ROI (left aIPL, right aIPL; <italic>F</italic>(1,23) = 3.66, p = 0.068), precluding strong conclusions about differential sensitivity of right vs. left aIPL in representing implied action effects.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Cross-decoding of implied action effect structures.</title><p>(<bold>A</bold>) Region of interest (ROI) analysis. Cross-decoding schemes involving pantomimes but not point-light-displays (PLDs) (<italic>action-pantomime</italic>, <italic>animation-pantomime</italic>) reveal stronger effects in right anterior inferior parietal lobe (aIPL) than cross-decoding schemes involving PLDs (<italic>action-PLD</italic>, <italic>pantomime-PLD</italic>, <italic>animation-PLD</italic>), suggesting that action effect structure representations in right aIPL respond to implied object manipulations in pantomime irrespective of visuospatial processing of observable object state changes. Same conventions as in <xref ref-type="fig" rid="fig3">Figure 3</xref>. (<bold>B</bold>) Conjunction of the contrasts <italic>action-pantomime versus action-PLD</italic>, <italic>action-pantomime versus pantomime-PLD</italic>, and <italic>animation-pantomime versus animation-PLD</italic>. Uncorrected t-map thresholded at p = 0.01; yellow outlines indicate clusters surviving Monte-Carlo correction for multiple comparisons (voxel threshold p = 0.001, corrected cluster threshold p = 0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Cross-decoding of implied action effect structures in superior parietal lobe (SPL).</title><p>Implied action effect structures should reveal higher decoding accuracies in cross-decoding schemes involving pantomimes but not point-light-displays (PLDs) (<italic>action-pantomime</italic>, <italic>animation-pantomime</italic>) as compared to cross-decoding schemes involving PLDs (<italic>action-PLD</italic>, <italic>pantomime-PLD</italic>, <italic>animation-PLD</italic>). In both left and right SPL, there were no differences for the comparisons of Action-Pant with Action-PLD and Pant-PLD, whereas there was stronger decoding for Anim-Pant versus Anim-PLD. This pattern of results is not straightforward to explain: First, the equally strong decoding for Action-Pant, Action-PLD, and Pant-PLD suggests that SPL is not substantially sensitive to body part details. Rather, the decoding relied on the coarse body part movements, independently of the specific stimulus type (action, pantomime, PLD). However, the stronger difference between Anim-Pant and Anim-PLD suggests that SPL is also sensitive to implied AES. This appears unlikely, because no effects (in left anterior inferior parietal lobe [aIPL]) or only weak effects (in right SPL) were found for the more canonical Action-Anim cross-decoding. The Anim-Pant cross-decoding was even stronger than the Action-Anim cross-decoding, which is counterintuitive because naturalistic actions contain more information than pantomimes, specifically with regard to action effect structures. How can this pattern of results be interpreted? Perhaps, for pantomimes and animations, not only aIPL and lateral occipitotemporal cortex (LOTC) but also SPL is involved in inferring (implied) action effect structures. However, for this conclusion, also differences for the comparison of Action-Pant with Action-PLD and for Action-Pant with Pant-PLD should be found. Another non-mutually exclusive interpretation is related to the fact that both animations and pantomimes are more ambiguous in terms of the specific action, as opposed to naturalistic actions. For example, the ‘squashing’ animation and pantomime are both ambiguous in terms of what is squashed/compressed, which might require additional load to infer both the action and the induced effect. The increased activation of action-related information might in turn increase the chance for a match between neural activation patterns of animations and pantomimes. In any case, these additional results in SPL do not question the effects reported in the main text, that is, disproportionate sensitivity for action effect structures in right aIPL and LOTC and for body movements in SPL and other AON regions. The evidence for implied action effect structures representation in SPL is mixed and should be interpreted with caution. Asterisks indicate FDR-corrected significant decoding accuracies above chance (*p &lt; 0.05, ***p &lt; 0.001, ****p &lt; 0.0001). Error bars indicate SEM (N=24).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Together, these results suggest that right aIPL and bilateral LOTC are sensitive to implied action effects. This finding was also obtained in a whole-brain conjunction analysis, which revealed effects in right aIPL and bilateral LOTC but not in other brain regions (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p></sec><sec id="s2-3"><title>Representational content of brain regions sensitive to action effect structures and body motion</title><p>To explore in more detail what types of information were isolated by the action-animation and action-PLD cross-decoding, we performed an RSA.</p><p>We first focus on the representations identified by the action-animation decoding. To characterize the representational content in the ROIs, we extracted the classification matrices of the action-animation decoding from the ROIs (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) and compared them with different similarity models (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) using multiple regression. Specifically, we aimed at testing at which level of granularity action effect structures are represented in aIPL and LOTC: Do these regions encode the broad type of action effects (change of shape, change of location, ingestion) or do they encode specific action effects (compression, division, etc.)? In addition, we aimed at testing whether the effects observed in EVC can be explained by a motion energy model that captures the similarities between actions and animations that we observed in the stimulus-based action-animation decoding using motion energy features. We therefore included V1 in the ROI analysis. We found clear evidence that the representational content in right aIPL and bilateral LOTC can be explained by the effect type model but not by the action-specific model (<xref ref-type="fig" rid="fig5">Figure 5C</xref>; all two-sided paired <italic>t</italic>-tests between models p &lt; 0.005). In left V1, we found that the motion energy model could indeed explain some representational variance; however, in both left and right V1 we also found effects for the effect type model. We assume that there were additional visual similarities between the broad types of actions and animations that were not captured by the motion energy model (or other visual models; see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). A searchlight RSA revealed converging results, and additionally found effects for the effect type model in the ventral part of left aIPL and for the action-specific model in the left anterior temporal lobe, left dorsal central gyrus, and right EVC (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). The latter findings were unexpected and should be interpreted with caution, as these regions (except right EVC) were not found in the action-animation cross-decoding and therefore should not be considered reliable (<xref ref-type="bibr" rid="bib37">Ritchie et al., 2017</xref>). The motion energy model did not reveal effects that survived the correction for multiple comparison, but a more lenient uncorrected threshold of p = 0.005 revealed clusters in left EVC and bilateral posterior SPL.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Representational similarity analysis (RSA) for the action-animation representations.</title><p>(<bold>A</bold>) Classification matrices of regions of interest (ROIs). (<bold>B</bold>) Similarity models used in the RSA. (<bold>C</bold>) Multiple regression RSA ROI analysis. Asterisks indicate FDR-corrected significant decoding accuracies above chance (**p &lt; 0.01, ****p &lt; 0.0001). Error bars indicate SEM (N=24). (<bold>D</bold>) Multiple regression RSA searchlight analysis. T-maps are thresholded using Monte-Carlo correction for multiple comparisons (voxel threshold p = 0.001, corrected cluster threshold p = 0.05) except for the motion energy model (p = 0.005, uncorrected).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Cross-decoding results for Action-Pantomime, Pantomime-PLD, Animation-Pantomime, and Animation-PLD.</title><p>(<bold>A</bold>) Cross-decoding maps for Action-Pantomime, Pantomime-PLD, Animation-Pantomime, and Animation-PLD (Monte-Carlo corrected for multiple comparisons; voxel threshold p = 0.001, corrected cluster threshold p = 0.05). (<bold>B</bold>) Classification matrices extracted from the cross-decoding maps.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig5-figsupp1-v1.tif"/></fig></fig-group><p>To characterize the representations identified by the action-PLD cross-decoding, we used a model of manuality that captures whether the actions are unimanual or bimanual, an action-specific model as used in the action-animation RSA above, and a kinematics model that was based on the three-dimensional (3D) kinematic marker positions of the PLDs (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Since pSTS is a key region for biological motion perception, we included this region in the ROI analysis. The manuality model explained the representational variance in the parietal ROIs, pSTS, and LOTC, but not in V1 (<xref ref-type="fig" rid="fig6">Figure 6C</xref>; all two-sided paired <italic>t</italic>-tests between V1 and other ROIs p &lt; 0.002). By contrast, the action-specific model revealed significant effects in V1 and LOTC, but not in pSTS and parietal ROIs (but note that effects in V1 and pSTS did not differ significantly from each other; all other two-sided paired <italic>t</italic>-tests between mentioned ROIs were significant at p &lt; 0.0005). The kinematics model explained the representational variance in all ROIs. A searchlight RSA revealed converging results, and additionally found effects for the manuality model in bilateral dorsal/medial prefrontal cortex and in right ventral prefrontal cortex and insula (<xref ref-type="fig" rid="fig6">Figure 6D</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Representational similarity analysis (RSA) for the action-PLD representations.</title><p>(<bold>A</bold>) Classification matrices of regions of interest (ROIs). (<bold>B</bold>) Similarity models used in the RSA. (<bold>C</bold>) Multiple regression RSA ROI analysis. (<bold>D</bold>) Multiple regression RSA searchlight analysis. Same figure conventions as in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig6-v1.tif"/></fig></sec><sec id="s2-4"><title>Representational similarity between brain regions</title><p>Finally, we investigated how similar the ROIs were with regard to the representational structure obtained by the action-animation and action-PLD cross-decoding. To this end, we correlated the classification matrices for both decoding schemes and all ROIs with each other (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) and displayed the similarities between them using multidimensional scaling (<xref ref-type="fig" rid="fig7">Figure 7B</xref>) and dendrograms (<xref ref-type="fig" rid="fig7">Figure 7C</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Region of interest (ROI) similarity for action-animation and action-PLD representations.</title><p>(<bold>A</bold>) Correlation matrix for the action-animation and action-PLD decoding and all ROIs. (<bold>B</bold>) Multidimensional scaling. (<bold>C</bold>) Dendrogram plot.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-fig7-v1.tif"/></fig><p>The aim for this analysis was twofold: First, we wanted to assess for the action-animation decoding how the representational structure in V1 relates to LOTC and aIPL. If V1 is representationally similar to LOTC and aIPL, this might point toward potential visual factors that drove the cross-decoding in visual cortex but potentially also in higher-level LOTC and aIPL. However, this was not the case: The V1 ROIs formed a separate cluster that was distinct from a cluster formed by aIPL and LOTC. This suggests that the V1 represents different information than aIPL and LOTC.</p><p>Second, we aimed at testing whether the effects in aIPL for the action-PLD decoding reflect the representation of action effect structures or rather representations related to body motion. In the former case, the representational organization in aIPL should be similar for the action-animation and action-PLD cross-decoding. In the latter case, the representational organization for action-PLD should be similar between aIPL and the other ROIs. We found that for the action-PLD decoding, all ROIs were clustered relatively closely together, and aIPL did not show similarity to the action-animation ROIs, specifically to aIPL. This finding argues against the interpretation that the effects in aIPL for the action-PLD cross-decoding were driven by action effect structures. Rather, it suggests that aIPL also encodes body motion, which is in line with the RSA results reported in the previous section.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We provide evidence for neural representations of action effect structures in aIPL and LOTC that generalize between perceptually highly distinct stimulus types – naturalistic actions and abstract animations. The representation of effect structures in aIPL is distinct from parietal representations of body movements, which were predominantly located in SPL. While body movement representations are generally bilateral, action effect structure representations are lateralized to the right aIPL and LOTC. In right aIPL and bilateral LOTC, action effect structure representations do not require a visible interaction with objects but also respond to action effects implied by pantomime.</p><p>Recognizing goal-directed actions requires a processing stage that captures the effect an action has on a target entity. Using cross-decoding between actions and animations, we found that aIPL and LOTC encode representations that are sensitive to the core action effect structure, that is, the type of change induced by the action. As the animations did not contain biological motion or specific object information matching the information in the action videos, these representations are independent of specific motion characteristics and object identity. This suggests an abstract level of representation of visuospatial and temporal relations between entities and their parts that may support the identification of object change independently of specific objects (e.g. dividing, compressing, ingesting, or moving something). Object-generality is an important feature as it enables the recognition of action effects on novel, unfamiliar objects. This type of representation fits the idea of a more general neural mechanism supporting mechanical reasoning about how entities interact with, and have effects on, each other (<xref ref-type="bibr" rid="bib8">Fischer et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">Karakose-Akbiyik et al., 2023</xref>). Using multiple regression RSA, we showed that action effect structure representations in these regions capture the broad action effect type, that is, a change of shape/configuration, a change of location, and ingestion. However, since this analysis was based on only five actions, a more comprehensive investigation is needed to understand the organization of a broader range of action effect types (see also <xref ref-type="bibr" rid="bib46">Worgotter et al., 2013</xref>).</p><p>In right aIPL and bilateral LOTC, the representation of action effect structures did not depend on a visible interaction with objects but could also be activated by pantomime, that is, an implied interaction with objects. This suggests that right aIPL and LOTC do not merely represent temporospatial relations of entities in a perceived scene. Rather, the effects in these regions might reflect a more inferential mechanism critical for understanding hypothetical effects of an interaction on a target object.</p><p>Interestingly, action effect structures appear lateralized to the right hemisphere. This is in line with the finding that perception of cause–effect relations, for example, estimating the effects of colliding balls, activates right aIPL (<xref ref-type="bibr" rid="bib11">Fugelsang et al., 2005</xref>; <xref ref-type="bibr" rid="bib40">Straube and Chatterjee, 2010</xref>). However, in the context of action recognition, the involvement of aIPL is usually bilateral or sometimes left-lateralized, in particular for actions involving an interaction with objects (<xref ref-type="bibr" rid="bib5">Caspers et al., 2010</xref>). Also, mechanical reasoning about tools – the ability to infer the effects of tools on target objects based on the physical properties of tools and objects, such as shape and weight – is usually associated with left rather than right aIPL (<xref ref-type="bibr" rid="bib13">Goldenberg and Spatt, 2009</xref>; <xref ref-type="bibr" rid="bib36">Reynaud et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Leshinskaya et al., 2020</xref>). Thus, left and right aIPL appear to be disproportionally sensitive to different structural aspects of actions and events: Left aIPL appears to be more sensitive to the type of interaction between entities, that is, how a body part or an object exerts a force onto a target object (e.g. how a hand makes contact with an object to push it), whereas right aIPL appears to be more sensitive to the effect induced by that interaction (the displacement of the object following the push). In our study, the animations contained interactions, but they did not show precisely how a force was exerted onto the target object that led to the specific effects: In all animations, the causer made contact with the target object in the same manner. Thus, the interaction could not drive the cross-decoding between actions and animations. Only the effects – the object changes – differed and could therefore be discriminated by the classification. Two questions arise from this interpretation: Would similar effects be observed in right aIPL (and LOTC) if the causer were removed, so that only the object change were shown in the animation? And would effects be observed in the left aIPL for distinguishable interactions (e.g. a triangle hitting a target object with the sharp or the flat side), perhaps even in the absence of the induced effect (dividing or compressing object, respectively)?</p><p>Action effect representations were found not only in aIPL but also LOTC. Interestingly, the RSA did not reveal substantially different representational content – both regions are equally sensitive to the effect type and their representational organization in response to the five action effects used in this experiment is highly similar. As it appears unlikely that aIPL and LOTC represent identical information, this raises the question of what different functions these regions provide in the context of action effect representation. Right LOTC is associated with the representation of socially relevant information, such as faces, body parts, and their movements (<xref ref-type="bibr" rid="bib6">Chao et al., 1999</xref>; <xref ref-type="bibr" rid="bib33">Pitcher and Ungerleider, 2021</xref>). Our findings suggest that right LOTC is not only sensitive to the perception of body-related information but also to body-independent information important for action recognition, such as object change. It remains to be investigated whether there is a dissociation between the action-independent representation of mere object change (e.g. in shape or location) and a higher-level representation of object change as an effect of an action. Left LOTC is sensitive to tools and effectors (<xref ref-type="bibr" rid="bib3">Bracci and Peelen, 2013</xref>), which might point toward a role in representing putative causes of the observed object changes. Moreover, action representations in left LOTC are perceptually more invariant, as they can be activated by action verbs (<xref ref-type="bibr" rid="bib45">Watson et al., 2013</xref>), generalize across vision and language (<xref ref-type="bibr" rid="bib49">Wurm and Caramazza, 2019b</xref>), and more generally show signatures of conceptual representation (<xref ref-type="bibr" rid="bib25">Lingnau and Downing, 2015</xref>; <xref ref-type="bibr" rid="bib50">Wurm and Caramazza, 2022</xref>). Thus, left LOTC might have generalized across actions and animations at a conceptual, possibly propositional level (e.g. the meaning of dividing, compressing, etc.), rather than at a structural level. Notably, conceptual action representations are typically associated with left anterior LOTC, but not right LOTC and aIPL, which argues against the interpretation that the action-animation cross-decoding in right LOTC and aIPL captured conceptual action representations rather than structural representations of the temporospatial object change type. From a more general perspective, cross-decoding between different stimulus types and formats might be a promising approach to address the fundamental question of whether the format of certain representations is propositional (<xref ref-type="bibr" rid="bib35">Pylyshyn, 2003</xref>) or depictive (<xref ref-type="bibr" rid="bib22">Kosslyn et al., 2006</xref>; <xref ref-type="bibr" rid="bib27">Martin, 2016</xref>).</p><p>In contrast to the perceptually general representation of action effect structures in aIPL, the representation of body movements is more specific in terms of visuospatial relations between involved elements, that is, body parts. These representations were predominantly found in bilateral SPL, rather than aIPL, as well as in LOTC. This is in line with previous studies demonstrating stronger decoding of PLD actions in SPL than in aIPL (<xref ref-type="bibr" rid="bib51">Yargholi et al., 2023</xref>) and stronger decoding of human actions as opposed to object events in SPL (<xref ref-type="bibr" rid="bib20">Karakose-Akbiyik et al., 2023</xref>). The RSA revealed that SPL, as well as adjacent regions in parietal cortex including aIPL and LOTC, are particularly sensitive to manuality of the action (uni- vs. bimanual) and the movement kinematics specific to an action. An interesting question for future research is whether movement representation in SPL is particularly tuned to biological motion or equally to similarly complex nonbiological movements. LOTC was not only sensitive to manuality and kinematics, but particularly in discriminating the five actions from each other, which suggests that LOTC is most sensitive to capture even subtle movement differences.</p><p>The action-PLD cross-decoding revealed widespread effects in LOTC and parietal cortex, including aIPL. What type of representation drove the decoding in aIPL? One possible interpretation is that aIPL encodes both body movements (isolated by the action-PLD cross-decoding) and action effect structures (isolated by the action-animation cross-decoding). Alternatively, aIPL selectively encodes action effect structures, which have been activated by the PLDs. A behavioral test showed that PLDs at least weakly allow for recognition of the specific actions (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>), which might have activated corresponding action effect structure representations. In addition, the finding that aIPL revealed effects for the cross-decoding between animations and PLDs further supports the interpretation that PLDs have activated, at least to some extent, action effect structure representations. On the other hand, if aIPL encodes <italic>only</italic> action effect structures, we would expect that the representational similarity patterns in aIPL are similar for the action-PLD and action-animation cross-decoding. However, this was not the case; rather, the representational similarity pattern in aIPL was more similar to SPL for the action-PLD decoding, which argues against substantially distinct representational content in aIPL versus SPL for the action-PLD decoding. In addition, the RSA revealed sensitivity to manuality and kinematics also in aIPL, which suggests that the action-PLD decoding in aIPL was at least partially driven by representations related to body movements. Taken together, these findings suggest that aIPL encodes not only action effect structures, but also representations related to body movements. Likewise, also SPL shows some sensitivity to action effect structures, as demonstrated by effects in SPL for the action-animation and pantomime-animation cross-decoding. Thus, our results suggest that aIPL and SPL are not selectively but disproportionally sensitive to action effects and body movements, respectively.</p><p>The action effect structure and body movement representations in aIPL and SPL identified here may not only play a role in the recognition of others' actions but also in the execution of goal-directed actions, which requires visuospatial processing of own body movements and of the changes in the world induced by them (<xref ref-type="bibr" rid="bib9">Fischer and Mahon, 2021</xref>). This view is compatible with the proposal that the dorsal ‘where/how’ stream is subdivided into sub-streams for the visuomotor coordination of body movements in SPL and the manipulation of objects in aIPL (<xref ref-type="bibr" rid="bib38">Rizzolatti and Matelli, 2003</xref>; <xref ref-type="bibr" rid="bib2">Binkofski and Buxbaum, 2013</xref>).</p><p>In conclusion, our study dissociated important stages in the visual processing of actions: the representation of body movements and the effects they induce in the world. These stages draw on subregions in parietal cortex – SPL and aIPL – as well as LOTC. These results help to clarify the roles of these regions in action understanding and more generally in understanding the physics of dynamic events. The identification of action effect structure representations in aIPL and LOTC has implications for theories of action understanding: Current theories (see for review e.g. <xref ref-type="bibr" rid="bib52">Zentgraf et al., 2011</xref>; <xref ref-type="bibr" rid="bib21">Kemmerer, 2021</xref>; <xref ref-type="bibr" rid="bib26">Lingnau and Downing, 2024</xref>) largely ignore the fact that the recognition of many goal-directed actions requires a physical analysis of the action-induced effect, that is, a state change of the action target. Moreover, premotor and inferior parietal cortex are usually associated with motor- or body-related processing during action observation. Our results, together with the finding that premotor and inferior parietal cortex are similarly sensitive to actions and inanimate object events (<xref ref-type="bibr" rid="bib20">Karakose-Akbiyik et al., 2023</xref>), suggest that large parts of the ‘action observation network’ are less specific for body-related processing in action perception than usually thought. Rather, this network might provide a substrate for the physical analysis and predictive simulation of dynamic events in general (<xref ref-type="bibr" rid="bib39">Schubotz, 2007</xref>; <xref ref-type="bibr" rid="bib10">Fischer, 2024</xref>). In addition, our finding that the (body-independent) representation of action effects substantially draws on right LOTC contradicts strong formulations of a ‘social perception’ pathway in LOTC that is <italic>selectively</italic> tuned to the processing of moving faces and bodies (<xref ref-type="bibr" rid="bib33">Pitcher and Ungerleider, 2021</xref>). The finding of action effect representation in right LOTC/pSTS might also offer a novel interpretation of a right pSTS subregion thought to specialized for social interaction recognition: Right pSTS shows increased activation for the observation of contingent action-reaction pairs (e.g. agent A points toward object; agent B picks up object) as compared to two independent actions (i.e. the action of agent A has no effect on the action of agent B) (<xref ref-type="bibr" rid="bib19">Isik et al., 2017</xref>). Perhaps the activation reflects the representation of a <italic>social</italic> action effect – the change of an agent’s state induced by someone else’s action. Thus, the representation of action effects might not be limited to physical object changes but might also comprise social effects not induced by a physical interaction between entities. Finally, not all actions induce an observable change in the world. It remains to be tested whether the recognition of, for example, communication (e.g. speaking, gesturing) and perception actions (e.g. observing, smelling) similarly relies on structural action representations in aIPL and LOTC.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Twenty-four right-handed adults (15 females; mean age, 23.7 years; age range, 20–38 years) participated in this experiment. All participants had normal or corrected-to-normal vision and no history of neurological or psychiatric disease. All procedures were approved by the Ethics Committee for research involving human participants at the University of Trento, Italy (Protocol Nr. 2019-022).</p></sec><sec id="s4-2"><title>Stimuli</title><p>The stimulus set consisted of videos of five object-directed actions (squashing a plastic bottle, breaking a stick, drinking water, hitting a paper ball, and placing a cup on a saucer) that were shown in four different formats: naturalistic actions, pantomimes, PLD stick figures, and abstract animations (<xref ref-type="fig" rid="fig2">Figure 2</xref>; informed consent, and consent to publish, was obtained from the actor shown in the figure). The actions were selected among a set of possible actions based on two criteria: (1) The actions should be structurally different from each other as much as possible. (2) The action structures (e.g. of dividing) should be depictable as animations, but at the same time the animations should be associated with the corresponding concrete actions as little as possible to minimize activation of conceptual action representations (e.g. of ‘breaking a stick’). The resulting set of five actions belonged to three broad categories of changes: shape/configuration changes (break, squash), location changes (hit, place), and ingestion (drink). This categorization was not planned before designing the study but resulted from the stimulus selection.</p><p>For each action and stimulus format, eight exemplars were generated to increase the perceptual variance of the stimuli. All videos were in RGB color, had a length of 2 s (30 frames per second), and a resolution of 400 × 225 pixels.</p><p>Naturalistic actions and corresponding pantomimes were performed by two different persons (female, male) sitting on a chair at a table in a neutral setting. The actions were filmed from two different camera viewpoints (approx. 25° and 40°). Finally, each video was mirrored to create left- and right-sided variants of the actions.</p><p>For the generation of PLD stick figures, the actions were performed in the same manner as the action videos in a motion-capture lab equipped with a Qualisys motion-capture system (Qualisys AB) comprising 5 ProReflex 1000 infrared cameras (100 frames per second). Thirteen passive kinematic markers (14 mm diameter) were attached to the right and left shoulders, elbows, hands, hips, knees, feet, and forehead of a single actor (the same male actor as in the action and pantomime videos), who performed each action two times. Great care was taken that the actions were performed with the same movements as in the action and pantomime videos. 3D kinematic marker positions were processed using the Qualisys track manager and Biomotion Toolbox V2 (<xref ref-type="bibr" rid="bib41">van Boxtel and Lu, 2013</xref>). Missing marker positions were calculated using the interpolation algorithm of the Qualisys track manager. To increase the recognizability of the body, we connected the points in the PLDs with white lines to create arms, legs, trunk, and neck. PLD stick figures were shown from two angles (25° and 40°), and the resulting videos were left–right mirrored.</p><p>Abstract animations were designed to structurally match the five actions in terms of the induced object change. At the same time, they were produced to be as abstract as possible so as to minimize the match at both basic perceptual levels (e.g. shape, motion) and conceptual levels. To clarify the latter, the abstract animation matching the ‘breaking’ action was designed to be structurally similar (causing an object to divide in half) without activating a specific action meaning such as ‘breaking a stick’. In all animations, the agent object (a circle with a smiley) moved toward a target object (a rectangle or a circle). The contact with the target object at 1 s after video onset induced different kinds of effects, that is, the target object broke in half or was compressed, ingested (decreased in size until it disappeared), propelled, or pushed to the side. The animations were created in MATLAB (Mathworks; RRID:<ext-link ext-link-type="uri" xlink:href="http://identifiers.org/RRID:SCR_001622">SCR_001622</ext-link>) with Psychtoolbox-3 (<xref ref-type="bibr" rid="bib4">Brainard, 1997</xref>). The speeds of all agent and target-object movements were constant across the video. To increase stimulus variance, 8 exemplars per action were generated using two target-object shapes (rectangle and circle), two color schemes for the agent–target pairs (green-blue and pink-yellow), and two action directions (left-to-right and right-to-left).</p></sec><sec id="s4-3"><title>Behavioral experiment</title><p>To assess how much the animations, PLD stick figures, and pantomimes were associated with the specific action meanings of the naturalistic actions, we performed a behavioral experiment. Fourteen participants observed videos of the animations, PLDs (without stick figures), and pantomimes in three separate sessions (in that order) and were asked to describe what kind of actions the videos depict and to rate their confidence on a Likert scale from 1 (not confident at all) to 10 (very confident). Because the results for PLDs were unsatisfying (several participants did not recognize human motion in the PLDs), we added stick figures to the PLDs as described above and repeated the rating for PLD stick figures with seven new participants, as reported below.</p><p>A general observation was that almost no participant used verb–noun phrases (e.g. ‘breaking a stick’) in their descriptions for all stimulus types. For the animations, the participants used more abstract verbs or nouns to describe the actions (e.g. dividing, splitting, division; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). These abstract descriptions matched the intended action structures quite well, and participants were relatively confident about their responses (mean confidences between 6 and 7.8). These results suggest that the animations were not substantially associated with specific action meanings (e.g. ‘breaking a stick’) but captured the coarse action structures. For the PLD stick figures (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>), responses were more variable and actions were often confused with kinematically similar but conceptually different actions (e.g. breaking --&gt; shaking, hitting --&gt; turning page, squashing --&gt; knitting). Confidence ratings were relatively low (mean confidences between 3 and 5.1). These results suggest that PLD stick figures, too, were not substantially associated with specific action meanings and additionally did not clearly reveal the underlying action effect structures. Finally, pantomimes were recognized much better, which was also reflected in high confidence ratings (mean confidences between 8 and 9.2; <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). This suggests that, unlike PLD stick figures, pantomimes allowed much better to access the underlying action effect structures.</p></sec><sec id="s4-4"><title>Experimental design</title><p>For all four sessions, stimuli were presented in a mixed event-related design. In each trial, videos were followed by a 1-s fixation period. Each of the five conditions was presented four times in a block, intermixed with three catch trials (23 trials per block). Four blocks were presented per run, separated by 8-s fixation periods. Each run started with a 2-s fixation period and ended with a 16-s fixation period. In each run, the order of conditions was pseudorandomized to ensure that each condition followed and preceded each other condition a similar number of times in each run. Each participant was scanned in four sessions (animations, PLDs, pantomimes, actions), each consisting of three functional scans. The order of sessions was chosen to minimize the possibility that participants would associate specific actions/objects with the conditions in the animation and PLD sessions. In other words, during the first session (animations), participants were unaware that they would see human actions in the following sessions; during the second session (PLDs), they were ignorant of the specific objects and hand postures/movements. Each of the five conditions was shown 48 times (4 trials per block × 4 blocks × 3 runs) in each session. Each exemplar of every video was presented six times in the experiment.</p></sec><sec id="s4-5"><title>Task</title><p>We used a catch-trial-detection task to ensure that participants paid constant attention during the experiment and were not biased to different types of information in the various sessions. Participants were instructed to attentively watch the videos and to press a button with the right index finger on a response-button box whenever a video contained a glitch, that is, when the video did not play smoothly but jerked for a short moment (300 ms). Glitches were created by selecting a random time window of eight video frames of the video (excluding the first 10 and last 4 frames) and shuffling the order of the frames within that window. The task was the same for all sessions. Before fMRI, participants were instructed and trained for the first session only (animations). In all four sessions, the catch trials were identified with robust accuracy (animations: 0.73 ± 0.02 SEM, PLDs: 0.65 ± 0.02, pantomimes: 0.69 ± 0.02, actions: 0.68 ± 0.02). Participants were not informed about the purpose and design of the study before the experiment.</p></sec><sec id="s4-6"><title>Data acquisition</title><p>Functional and structural data were collected using a 3T Siemens Prisma MRI scanner and a 64-channel head coil. Functional images were acquired with a T2*-weighted gradient echo-planar imaging (EPI) sequence. Acquisition parameters were a repetition time (TR) of 1.5 s, an echo time of 28 ms, a flip angle of 70°, field of view of 200 mm matrix size of 66 × 66, voxel resolution 3 × 3 × 3 mm. We acquired 45 slices in ascending interleaved odd–even order. Each slice was 3 mm thick. There were 211 volumes acquired in each functional run.</p><p>Structural T1-weighted images were acquired using an MPRAGE sequence (Slice number = 176, TR = 2.53 s, inversion time = 1.1 s, flip angle = 7°, 256 × 256 mm field of view, 1 × 1 × 1 mm resolution).</p></sec><sec id="s4-7"><title>Preprocessing</title><p>Data were analyzed using BrainVoyager QX 2.84 (BrainInnovation) in combination with the SPM12 and NeuroElf (BVQXTools) toolboxes and custom software written in Matlab (MathWorks; RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_001622">SCR_001622</ext-link>). Anatomical scans of individual subjects were normalized to the standard SPM12 EPI template (Montreal Neurological Institute [MNI] stereotactic space). Slice time correction was performed on the functional data followed by a 3D motion correction (trilinear interpolation, with the first volume of the first run of each participant as reference). Functional data were co-registered with the normalized anatomical scans followed by spatial smoothing with a Gaussian kernel of 8 mm full width at half maximum (FWHM) for univariate analysis and 3 mm FWHM for multivariate pattern analysis (MVPA).</p></sec><sec id="s4-8"><title>Multivariate pattern classification</title><p>For each participant, session, and run, a general linear model was computed using design matrices containing 10 action predictors (2 for each action; based on 8 trials from the first two blocks of a run and the second half from the last two blocks of a run, to increase the number of beta samples for classification), a catch-trial predictor, 6 predictors for each parameter of motion correction (3D translation and rotation), and 6 temporal-drift predictors. Each trial was modeled as an epoch lasting from video onset to offset (2 s). The resulting reference time courses were used to fit the signal time courses of each voxel. Predictors were convolved with a dual-gamma hemodynamic impulse response function. Since there were 3 runs per session, there were thus 6 beta maps per action condition.</p><p>Searchlight classification was done on each subject in volume space using a searchlight sphere of 12 mm and an LDA (linear discriminant analysis) classifier, as implemented in the CosmoMVPA toolbox (<xref ref-type="bibr" rid="bib30">Oosterhof et al., 2016</xref>). We also tested for robustness of effects across MVPA parameter choices by running the analysis with different ROI sizes (9 and 15 mm) and a support vector machine (SVM) classifier, which revealed similar findings, that is, all critical findings were also found with the alternative MVPA parameters.</p><p>For the within-session analyses, all five action conditions of a given session were entered into a five-way multiclass classification using leave-one-out cross validation, that is, the classifier was trained with five out of six beta patterns per action and was tested with the held-out beta pattern. This was done until each beta pattern was tested. The resulting accuracies were averaged across the six iterations and assigned to the center voxel of the sphere. Within-session decoding analyses were performed as sanity checks to ensure that for all stimulus types, the five actions could be reliably decoded (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). For cross-decoding analyses, a classifier was trained to discriminate the voxel activation patterns associated with the five action conditions from one session (e.g. actions) and tested on its ability to discriminate the voxel activation patterns associated with the five action conditions of another session (e.g. animations). The same was done vice versa (training with animations and testing with actions), and the accuracies were averaged across the two directions (<xref ref-type="bibr" rid="bib42">van den Hurk and Op de Beeck, 2019</xref>) (see <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref> for contrasts between cross-decoding directions). In total, there were six across-session pairs: action-animation, action-PLD, action-pantomime, animation-pantomime, pantomime-PLD, and animation-PLD. For searchlight maps of the latter four decoding schemes see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p><p>For all decoding schemes, a one-tailed, one-sample <italic>t</italic>-test was performed on the resulting accuracy maps to determine which voxels had a decoding accuracy that was significantly above the chance level (20%). The resulting t-maps were corrected for multiple comparisons with Monte-Carlo correction as implemented in CosmoMVPA (<xref ref-type="bibr" rid="bib30">Oosterhof et al., 2016</xref>), using an initial threshold of p = 0.001 at the voxel level, 10,000 Monte-Carlo simulations, and a one-tailed corrected cluster threshold of p = 0.05 (<italic>z</italic> = 1.65).</p><p>Conjunction maps were computed by selecting the minimal <italic>z</italic>-value (for corrected maps) or <italic>t</italic>-value (for uncorrected maps) for each voxel of the input maps.</p></sec><sec id="s4-9"><title>ROI analysis</title><p>ROIs were based on MNI coordinates of the center locations of Brodmann areas (BA) associated with the aIPL/supramarginal gyrus (BA 40; left: −53, –32, 33; right: 51, –33, 34), SPL (BA 7; left: −18, –61, 55; right: 23, –60, 61), LOTC (BA 19; left: −45, –75, 11; right: 44, –75, 5), and EVC (BA 17; left: −11, –81, 7; right: 11, –78, 9), using the MNI2TAL application of the BioImage Suite WebApp (<ext-link ext-link-type="uri" xlink:href="https://bioimagesuiteweb.github.io/webapp/">https://bioimagesuiteweb.github.io/webapp/</ext-link>). Coordinates for left and right pSTS were taken from a meta analysis for human versus non-human body movements (<xref ref-type="bibr" rid="bib15">Grosbras et al., 2012</xref>). To keep the ROI size constant across the different ROIs, we used spherical ROIs as in our previous studies (<xref ref-type="bibr" rid="bib49">Wurm and Caramazza, 2019b</xref>; <xref ref-type="bibr" rid="bib48">Wurm and Caramazza, 2019a</xref>; <xref ref-type="bibr" rid="bib20">Karakose-Akbiyik et al., 2023</xref>). A visualization of the ROIs projected on the cortical surface can be found in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>. For each participant, ROI, and decoding scheme, decoding accuracies from the searchlight analysis were extracted from all voxels within a sphere of 12 mm around the ROI center, averaged across voxels, and entered into one-tailed, one-sample <italic>t</italic>-tests against chance (20%). In addition, paired <italic>t</italic>-tests and repeated measures ANOVAs were conducted to test for the differences between ROIs and different decoding schemes. The statistical results of <italic>t</italic>-tests were FDR-corrected for the number of tests and ROIs (<xref ref-type="bibr" rid="bib1">Benjamini and Yekutieli, 2001</xref>).</p></sec><sec id="s4-10"><title>Representational similarity analysis</title><p>To analyze the representational content isolated by the action-animation and action-PLD cross-decoding, we performed a multiple regression RSA. First, we extracted classification matrices from the the action-animation and action-PLD cross-decoding maps: For each subject, voxel, and cross-decoding scheme, we extracted the classification matrices, symmetrized them, and rescaled them into values between 0 and 1. For each ROI, we averaged the matrices across voxels. The classification matrices were converted into neural representational dissimilarity matrices (RDMs) by subtracting 1 from them.</p><p>The neural RDMs were then compared with model RDMs using a multiple regression RSA. The following models were used for the action-animation RSA: (1) An ‘effect type’ model that captures the similarity in terms of action effect type (shape/configuration changes: break, squash; location changes: hit, place; and ingestion: drink). (2) A ‘specific’ model that discriminates each specific action from each other with equal distance. (3) A ‘motion energy’ model that was based on the stimulus-based action-animation decoding (see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>) and that captures the similarity of animations and actions in terms of motion energy. No critical collinearity was observed (variance inflation factors &lt;2.6, condition indices &lt;4, variance decomposition proportions &lt;0.9). The following models were used for the action-PLD RSA: (1) A manuality model that captures whether the actions were carried out with one versus both hands (bimanual: break, squash, drink; unimanual: hit, place). (2) A ‘specific’ model that discriminates each specific action from each other with equal distance. (3) A kinematic model that was based on marker positions of the PLDs. The kinematics model was constructed by averaging the kinematic data across the 2 exemplars per PLD, vectorizing the 3D marker positions of all time points of the PLDs (3 dimensions × 13 markers × 200 time points), computing the pairwise correlations between the five vectors, and converting the correlations into dissimilarity values by subtracting 1 − <italic>r</italic>. No critical collinearity was observed (variance inflation factors &lt;3.2, condition indices &lt;4, variance decomposition proportions &lt;0.96). The multiple regression RSA was first done at the whole-brain level using a searchlight approach: For each subject, voxel, and cross-decoding scheme, we extracted the classification matrices and converted them into neural RDMs as described above. Each neural RDM was entered as dependent variable into a multiple regression, together with the model RDMs as independent variables. Note that we included the on-diagonal values of the neural and model RDMs as they contain interpretable zero points (<xref ref-type="bibr" rid="bib44">Walther et al., 2016</xref>), which is necessary for testing the action-specific model. Resulting beta values were Fisher-transformed and entered into one-tailed one-sample <italic>t</italic>-tests. The resulting t-maps were corrected for multiple comparisons with Monte-Carlo correction as described above. For ROI analyses, beta values of the RSA were extracted from the ROIs, averaged across voxels, and entered into statistical tests as described above (see ROI analysis’).</p></sec><sec id="s4-11"><title>Representational similarity between brain regions</title><p>To test how similar the representational content of the ROIs for the action-animation and action-PLD decoding are to each other, we used informational connectivity analysis (<xref ref-type="bibr" rid="bib7">Coutanche and Thompson-Schill, 2013</xref>): First, we correlated the neural RDMs of all ROIs and decoding schemes with each other. Specifically, we included the lower triangle of the RDMs (off-diagonal pairwise distances between actions) and also the on-diagonal values of the RDMs, which contain the correct classifications (<xref ref-type="bibr" rid="bib44">Walther et al., 2016</xref>). This was done to increase the number of informative data in the correlations. We then converted the resulting correlations between ROIs/decoding schemes into distances by subtracting 1 − <italic>r</italic>, and visualized the pairwise distances between ROIs and decoding schemes using multidimensional scaling (metric stress) and a dendrogram plot following a cluster analysis (nearest distance).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Informed consent, and consent to publish, was obtained from all participants. All procedures were approved by the Ethics Committee for research involving human participants at the University of Trento, Italy (Protocol Number 2019-022).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Results of behavioral pilot experiment for abstract animations.</title><p>Verbal descriptions of each participant and mean confidence ratings (from 1 = not at all to 10 = very much ± standard deviations).</p></caption><media xlink:href="elife-98521-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Results of behavioral pilot experiment for PLD stick figures.</title><p>Verbal descriptions of each participant and mean confidence ratings (from 1 = not at all to 10 = very much ± standard deviations).</p></caption><media xlink:href="elife-98521-supp2-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Results of behavioral pilot experiment for pantomimes.</title><p>Verbal descriptions of each participant and mean confidence ratings (from 1 = not at all to 10 = very much ± standard deviations).</p></caption><media xlink:href="elife-98521-supp3-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-98521-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Stimuli, MRI data, and code are deposited at the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/am346/">https://osf.io/am346/</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Erigüç</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Decoding the physics of observed actions in the human brain</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/am346/">am346</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Seoyoung Lee for assistance in preparing the video stimuli, Ingmar de Vries for assistance in preparing the PLD stimuli, Ben Timberlake for proof-reading, and the Caramazza Lab for helpful feedback on the study design and interpretation of results. This research was supported by the Caritro Foundation, Italy.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Yekutieli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The control of the false discovery rate in multiple testing under dependency</article-title><source>The Annals of Statistics</source><volume>29</volume><fpage>1165</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1214/aos/1013699998</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binkofski</surname><given-names>F</given-names></name><name><surname>Buxbaum</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Two action systems in the human brain</article-title><source>Brain and Language</source><volume>127</volume><fpage>222</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2012.07.007</pub-id><pub-id pub-id-type="pmid">22889467</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Body and object effectors: the organization of object representations in high-level visual cortex reflects body-object interactions</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>18247</fpage><lpage>18258</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1322-13.2013</pub-id><pub-id pub-id-type="pmid">24227734</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caspers</surname><given-names>S</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>ALE meta-analysis of action observation and imitation in the human brain</article-title><source>NeuroImage</source><volume>50</volume><fpage>1148</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.112</pub-id><pub-id pub-id-type="pmid">20056149</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>LL</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Attribute-based neural substrates in temporal cortex for perceiving and knowing about objects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>913</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1038/13217</pub-id><pub-id pub-id-type="pmid">10491613</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coutanche</surname><given-names>MN</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Informational connectivity: identifying synchronized discriminability of multi-voxel patterns across the brain</article-title><source>Frontiers in Human Neuroscience</source><volume>7</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00015</pub-id><pub-id pub-id-type="pmid">23403700</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Mikhael</surname><given-names>JG</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional neuroanatomy of intuitive physical inference</article-title><source>PNAS</source><volume>113</volume><fpage>E5072</fpage><lpage>E5081</lpage><pub-id pub-id-type="doi">10.1073/pnas.1610344113</pub-id><pub-id pub-id-type="pmid">27503892</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Mahon</surname><given-names>BZ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>What tool representation, intuitive physics, and action have in common: the brain’s first-person physics engine</article-title><source>Cognitive Neuropsychology</source><volume>38</volume><fpage>455</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1080/02643294.2022.2106126</pub-id><pub-id pub-id-type="pmid">35994054</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Physical reasoning is the missing link between action goals and kinematics: a comment on “An active inference model of hierarchical action understanding, learning, and imitation” by Proietti et al</article-title><source>Physics of Life Reviews</source><volume>48</volume><fpage>198</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.plrev.2023.08.017</pub-id><pub-id pub-id-type="pmid">38350304</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fugelsang</surname><given-names>JA</given-names></name><name><surname>Roser</surname><given-names>ME</given-names></name><name><surname>Corballis</surname><given-names>PM</given-names></name><name><surname>Gazzaniga</surname><given-names>MS</given-names></name><name><surname>Dunbar</surname><given-names>KN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Brain mechanisms underlying perceptual causality</article-title><source>Brain Research. Cognitive Brain Research</source><volume>24</volume><fpage>41</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2004.12.001</pub-id><pub-id pub-id-type="pmid">15922156</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giese</surname><given-names>MA</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neural mechanisms for the recognition of biological movements</article-title><source>Nature Reviews. Neuroscience</source><volume>4</volume><fpage>179</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1038/nrn1057</pub-id><pub-id pub-id-type="pmid">12612631</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldenberg</surname><given-names>G</given-names></name><name><surname>Spatt</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The neural basis of tool use</article-title><source>Brain</source><volume>132</volume><fpage>1645</fpage><lpage>1655</lpage><pub-id pub-id-type="doi">10.1093/brain/awp080</pub-id><pub-id pub-id-type="pmid">19351777</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Separate visual pathways for perception and action</article-title><source>Trends in Neurosciences</source><volume>15</volume><fpage>20</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(92)90344-8</pub-id><pub-id pub-id-type="pmid">1374953</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosbras</surname><given-names>MH</given-names></name><name><surname>Beaton</surname><given-names>S</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Brain regions involved in human movement perception: a quantitative voxel-based meta-analysis</article-title><source>Human Brain Mapping</source><volume>33</volume><fpage>431</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1002/hbm.21222</pub-id><pub-id pub-id-type="pmid">21391275</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossman</surname><given-names>E</given-names></name><name><surname>Donnelly</surname><given-names>M</given-names></name><name><surname>Price</surname><given-names>R</given-names></name><name><surname>Pickens</surname><given-names>D</given-names></name><name><surname>Morgan</surname><given-names>V</given-names></name><name><surname>Neighbor</surname><given-names>G</given-names></name><name><surname>Blake</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Brain areas involved in perception of biological motion</article-title><source>Journal of Cognitive Neuroscience</source><volume>12</volume><fpage>711</fpage><lpage>720</lpage><pub-id pub-id-type="doi">10.1162/089892900562417</pub-id><pub-id pub-id-type="pmid">11054914</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafri</surname><given-names>A</given-names></name><name><surname>Trueswell</surname><given-names>JC</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural representations of observed actions generalize across static and dynamic visual input</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>3056</fpage><lpage>3071</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2496-16.2017</pub-id><pub-id pub-id-type="pmid">28209734</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heider</surname><given-names>F</given-names></name><name><surname>Simmel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1944">1944</year><article-title>An experimental study of apparent behavior</article-title><source>The American Journal of Psychology</source><volume>57</volume><elocation-id>243</elocation-id><pub-id pub-id-type="doi">10.2307/1416950</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name><name><surname>Beeler</surname><given-names>D</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Perceiving social interactions in the posterior superior temporal sulcus</article-title><source>PNAS</source><volume>114</volume><fpage>E9145</fpage><lpage>E9152</lpage><pub-id pub-id-type="doi">10.1073/pnas.1714471114</pub-id><pub-id pub-id-type="pmid">29073111</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karakose-Akbiyik</surname><given-names>S</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name><name><surname>Wurm</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A shared neural code for the physics of actions and object events</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>3316</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-39062-8</pub-id><pub-id pub-id-type="pmid">37286553</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemmerer</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>What modulates the mirror neuron system during action observation?: multiple factors involving the action, the actor, the observer, the relationship between actor and observer, and the context</article-title><source>Progress in Neurobiology</source><volume>205</volume><elocation-id>102128</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2021.102128</pub-id><pub-id pub-id-type="pmid">34343630</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kosslyn</surname><given-names>SM</given-names></name><name><surname>Thompson</surname><given-names>WL</given-names></name><name><surname>Ganis</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>The Case for Mental Imagery</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195179088.001.0001</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A new neural framework for visuospatial processing</article-title><source>Nature Reviews. Neuroscience</source><volume>12</volume><fpage>217</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1038/nrn3008</pub-id><pub-id pub-id-type="pmid">21415848</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Leshinskaya</surname><given-names>A</given-names></name><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Concepts of actions and their objects</chapter-title><person-group person-group-type="editor"><name><surname>Gazzaniga</surname><given-names>M</given-names></name><name><surname>Mangun</surname><given-names>GR</given-names></name><name><surname>Poepped</surname><given-names>D</given-names></name></person-group><source>The Cognitive Neurosciences</source><publisher-name>MIT Press</publisher-name><fpage>757</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.7551/mitpress/11442.001.0001</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lingnau</surname><given-names>A</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The lateral occipitotemporal cortex in action</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>268</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.03.006</pub-id><pub-id pub-id-type="pmid">25843544</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lingnau</surname><given-names>A</given-names></name><name><surname>Downing</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2024">2024</year><source>Action Understanding</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/9781009386630</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>GRAPES—Grounding representations in action, perception, and emotion systems: How object properties and categories are represented in the human brain</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>979</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0842-3</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reconstructing visual experiences from brain activity evoked by natural movies</article-title><source>Current Biology</source><volume>21</volume><fpage>1641</fpage><lpage>1646</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.08.031</pub-id><pub-id pub-id-type="pmid">21945275</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Lescroart</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Motion_energy_matlab</data-title><version designator="7848623">7848623</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/gallantlab/motion_energy_matlab">https://github.com/gallantlab/motion_energy_matlab</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>CoSMoMVPA: multi-modal multivariate pattern analysis of neuroimaging data in matlab/gnu octave</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>27</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00027</pub-id><pub-id pub-id-type="pmid">27499741</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Wiggett</surname><given-names>AJ</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Patterns of fMRI activity dissociate overlapping functional brain areas that respond to biological motion</article-title><source>Neuron</source><volume>49</volume><fpage>815</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.02.004</pub-id><pub-id pub-id-type="pmid">16543130</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peuskens</surname><given-names>H</given-names></name><name><surname>Vanrie</surname><given-names>J</given-names></name><name><surname>Verfaillie</surname><given-names>K</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Specificity of regions processing biological motion</article-title><source>The European Journal of Neuroscience</source><volume>21</volume><fpage>2864</fpage><lpage>2875</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2005.04106.x</pub-id><pub-id pub-id-type="pmid">15926934</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname><given-names>D</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Evidence for a third visual pathway specialized for social perception</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>100</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.11.006</pub-id><pub-id pub-id-type="pmid">33334693</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Perrett</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Electrophysiology and brain imaging of biological motion</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>358</volume><fpage>435</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1221</pub-id><pub-id pub-id-type="pmid">12689371</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pylyshyn</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Return of the mental image: are there really pictures in the brain?</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>113</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(03)00003-2</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynaud</surname><given-names>E</given-names></name><name><surname>Lesourd</surname><given-names>M</given-names></name><name><surname>Navarro</surname><given-names>J</given-names></name><name><surname>Osiurak</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>On the neurocognitive origins of human tool use : a critical review of neuroimaging data</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>64</volume><fpage>421</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.03.009</pub-id><pub-id pub-id-type="pmid">26976352</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Avoiding illusory effects in representational similarity analysis: what (not) to do with the diagonal</article-title><source>NeuroImage</source><volume>148</volume><fpage>197</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.12.079</pub-id><pub-id pub-id-type="pmid">28069538</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G</given-names></name><name><surname>Matelli</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Two different streams form the dorsal visual system: anatomy and functions</article-title><source>Experimental Brain Research</source><volume>153</volume><fpage>146</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1007/s00221-003-1588-0</pub-id><pub-id pub-id-type="pmid">14610633</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schubotz</surname><given-names>RI</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Prediction of external events with our motor system: towards a new framework</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>211</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.02.006</pub-id><pub-id pub-id-type="pmid">17383218</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Straube</surname><given-names>B</given-names></name><name><surname>Chatterjee</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Space and time in perceptual causality</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><elocation-id>28</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2010.00028</pub-id><pub-id pub-id-type="pmid">20463866</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Boxtel</surname><given-names>JJA</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A biological motion toolbox for reading, displaying, and manipulating motion capture data in research settings</article-title><source>Journal of Vision</source><volume>13</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.1167/13.12.7</pub-id><pub-id pub-id-type="pmid">24130256</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>van den Hurk</surname><given-names>J</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Generalization asymmetry in multivariate cross86 classification: when representation A generalizes better to representation B than B to A</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/592410</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname><given-names>G</given-names></name><name><surname>F Wurm</surname><given-names>M</given-names></name><name><surname>Striem-Amit</surname><given-names>E</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Large-scale organization of the hand action observation network in individuals born without hands</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>3434</fpage><lpage>3444</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy212</pub-id><pub-id pub-id-type="pmid">30169751</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ejaz</surname><given-names>N</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>137</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id><pub-id pub-id-type="pmid">26707889</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>CE</given-names></name><name><surname>Cardillo</surname><given-names>ER</given-names></name><name><surname>Ianni</surname><given-names>GR</given-names></name><name><surname>Chatterjee</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Action concepts in the brain: an activation likelihood estimation meta-analysis</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1191</fpage><lpage>1205</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00401</pub-id><pub-id pub-id-type="pmid">23574587</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Worgotter</surname><given-names>F</given-names></name><name><surname>Aksoy</surname><given-names>EE</given-names></name><name><surname>Kruger</surname><given-names>N</given-names></name><name><surname>Piater</surname><given-names>J</given-names></name><name><surname>Ude</surname><given-names>A</given-names></name><name><surname>Tamosiunaite</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A simple ontology of manipulation actions based on hand-object relations</article-title><source>IEEE Transactions on Autonomous Mental Development</source><volume>5</volume><fpage>117</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1109/TAMD.2012.2232291</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Lingnau</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Decoding actions at different levels of abstraction</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>7727</fpage><lpage>7735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0188-15.2015</pub-id><pub-id pub-id-type="pmid">25995462</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Distinct roles of temporal and frontoparietal cortex in representing actions across vision and language</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>289</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-08084-y</pub-id><pub-id pub-id-type="pmid">30655531</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Lateral occipitotemporal cortex encodes perceptual components of social actions rather than abstract representations of sociality</article-title><source>NeuroImage</source><volume>202</volume><elocation-id>116153</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116153</pub-id><pub-id pub-id-type="pmid">31491524</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Two “what” pathways for action and object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>26</volume><fpage>103</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2021.10.003</pub-id><pub-id pub-id-type="pmid">34702661</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yargholi</surname><given-names>E</given-names></name><name><surname>Hossein-Zadeh</surname><given-names>GA</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Two distinct networks containing position-tolerant representations of actions in the human brain</article-title><source>Cerebral Cortex</source><volume>33</volume><fpage>1462</fpage><lpage>1475</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhac149</pub-id><pub-id pub-id-type="pmid">35511702</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zentgraf</surname><given-names>K</given-names></name><name><surname>Munzert</surname><given-names>J</given-names></name><name><surname>Bischoff</surname><given-names>M</given-names></name><name><surname>Newman-Norlund</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Simulation during observation of human actions--theories, empirical studies, applications</article-title><source>Vision Research</source><volume>51</volume><fpage>827</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.01.007</pub-id><pub-id pub-id-type="pmid">21277318</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Stimulus-based cross-decoding.</title><p>For each video, motion energy features were extracted as described in <xref ref-type="bibr" rid="bib28">Nishimoto et al., 2011</xref> using Matlab code from <ext-link ext-link-type="uri" xlink:href="https://github.com/gallantlab/motion_energy_matlab">GitHub</ext-link> (<xref ref-type="bibr" rid="bib29">Nishimoto and Lescroart, 2018</xref>). To reduce the number of features for the subsequent MVPA, we used the motion energy model with 2139 channels. The resulting motion energy features were averaged across time and entered into within- and across-decoding schemes as described for the fMRI-based decoding. For the decoding within stimulus type, we used leave-one-exemplar-out cross-validation, that is, the classifier was train on seven of the eight exemplars for each action and tested on the remaining exemplar, etc. For the cross-decoding, the classifier was trained on all eight exemplars of stimulus type A and tested on all eight exemplars of stimulus type B (and vice versa). Significance was determined using a permutation test: For each decoding scheme, 10,000 decoding tests with shuffled class labels were performed to create a null distribution. p-values were computed by counting the number of values in the null distribution that were greater or as great as the observed decoding accuracy. The within-stimulus-type decoding served as a control analysis and revealed highly significant decoding accuracies for each stimulus type (animations: 100%, PLDs: 100%, pantomimes: 65%, actions: 55%), which suggests that the motion energy data generally contains information that can be detected by a classifier. The cross-decoding between stimulus types was significantly above chance for action-animation and action-pantomime, but not significantly different from chance for the remaining decoding schemes. Interestingly, all cross-decoding schemes with PLDs did not perform well and revealed similar classification matrices (systematically confusing <italic>squash</italic>, <italic>hit</italic>, and <italic>place</italic> with <italic>break</italic> and <italic>drink</italic>). This might be due to different feature complexity and motion information at different spatial frequencies for PLDs, which do not generalize to the other stimulus types. We also tested whether the different stimulus types can be cross-decoded using other visual features. To test for pixelwise similarities, we averaged the video frames of each video, vectorized and <italic>z</italic>-scored them, and entered them into the decoding schemes as described above. We found above chance decoding for all within-stimulus type schemes, but not for the cross-decoding schemes (animations: 55%, PLDs: 80%, pantomimes: 40%, actions: 30%, action-anim: 15%, action-PLD: 20%, action-pant: 20%, pant-PLD: 12.5%, anim-PLD: 20%, anim-pant: 22.5%). To test for local motion similarities, we computed frame-to-frame optical flow vectors (using Matlab’s opticalFlowHS function) for each video, averaged the resulting optical flow values across frames, vectorized and <italic>z</italic>-scored them, and entered them into the decoding schemes as described above. We found above chance decoding for all within-stimulus type schemes and the action-pantomime cross-decoding, but not for the other cross-decoding schemes (animations: 75%, PLDs: 100%, pantomimes: 65%, actions: 40%, action-anim: 15%, action-PLD: 18.7%, action-pant: 38.7%, pant-PLD: 26.2%, anim-PLD: 22.5%, anim-pant: 21.2%).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Univariate baseline and within-session decoding maps.</title><p>(<bold>A</bold>) Univariate activation maps for each session (all five actions vs. Baseline; FDR-corrected at p = 0.05) and (<bold>B</bold>) within-session decoding maps (Monte-Carlo corrected for multiple comparisons; voxel threshold p = 0.001, corrected cluster threshold p = 0.05).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-app1-fig2-v1.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Direction-specific cross-decoding effects.</title><p>To test whether there were differences between the two directions in the cross-decoding analyses, we ran, for each of the six across-session decoding schemes, two-tailed paired samples <italic>t</italic>-tests between the decoding maps of one direction (e.g. action → animation) versus the other direction (animation → action). Direction effects were observed in left early visual cortex for the directions action → animation, PLD → animation, and pantomime → PLD, as well in right middle temporal gyrus and dorsal premotor cortex for action → PLD. These effects might be due to noise differences between stimulus types (<xref ref-type="bibr" rid="bib42">van den Hurk and Op de Beeck, 2019</xref>) and do not affect the interpretation of direction-averaged cross-decoding effects in the main text. Monte-Carlo corrected for multiple comparisons; voxel threshold p = 0.001, corrected cluster threshold p = 0.05.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-app1-fig3-v1.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>ROIs used in the study.</title><p>Spherical ROIs were in volume space (12 mm radius); here, we projected them on the cortical surface for a better comparison with the whole-brain maps in the main article.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-app1-fig4-v1.tif"/></fig></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98521.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Press</surname><given-names>Clare</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>In an <bold>important</bold> fMRI study with an elegant experimental design and rigorous cross-decoding analyses, this work shows a <bold>convincing</bold> dissociation between two parietal regions in visually processing actions. Specifically, aIPL is found to be sensitive to the causal effects of observed actions, while SPL is sensitive to the patterns of body motion involved in those actions. The work will be of broad interest to cognitive neuroscientists, particularly vision and action researchers.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98521.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors report a study aimed at understanding the brain's representations of viewed actions, with a particular aim to distinguish regions that encode observed body movements, from those that encode the effects of actions on objects. They adopt a cross-decoding multivariate fMRI approach, scanning adult observers who viewed full-cue actions, pantomimes of those actions, minimal skeletal depictions of those actions, and abstract animations that captured analogous effects to those actions. Decoding across different pairs of these action conditions allowed the authors to pull out the contributions of different action features in a given region's representation. The main hypothesis, which was largely confirmed, was that the superior parietal lobe (SPL) more strongly encodes movements of the body, whereas the anterior inferior parietal lobe (aIPL) codes for action effects of outcomes. Specifically, region of interest analyses showed dissociations in the successful cross-decoding of action category across full-cue and skeletal or abstract depictions. Their analyses also highlight the importance of the lateral occipito-temporal cortex (LOTC) in coding action effects. They also find some preliminary evidence about the organisation of action kinds in the regions examined, and take some steps to distinguishing the differences and similarities of action-evoked patterns in primary visual cortex and the other examined regions.</p><p>Strengths:</p><p>The paper is well-written, and it addresses a topic of emerging interest where social vision and intuitive physics intersect. The use of cross-decoding to examine actions and their effects across four different stimulus formats is a strength of the study. Likewise the a priori identification of regions of interest (supplemented by additional full-brain analyses) is a strength. Finally, the authors successfully deployed a representational-similarity approach that provides more detailed evidence about the different kinds of action features that seem to be captured in each of the regions that were examined.</p><p>Weaknesses:</p><p>Globally, the findings provide support for the predicted anatomical distinctions, and for the distinction between body-focused representations of actions and more abstract &quot;action effect structures&quot;. Viewed more narrowly, the picture is rather complex, and the patterns of (dis)similarity in the activity evoked by different action kinds do not always divide neatly. Probably, examining many more kinds of actions with the multi-format decoding approach developed here will be needed to more effectively disentangle the various contributions of movement, posture, low-level visual properties, and action outcomes/effects.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.98521.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wurm</surname><given-names>Moritz F</given-names></name><role specific-use="author">Author</role><aff><institution>University of Trento</institution><addr-line><named-content content-type="city">Rovereto</named-content></addr-line><country>Italy</country></aff></contrib><contrib contrib-type="author"><name><surname>Erigüç</surname><given-names>Doruk Yiğit</given-names></name><role specific-use="author">Author</role><aff><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife Assessment</bold></p><p>In an important fMRI study with an elegant experimental design and rigorous cross-decoding analyses, this work shows a solid dissociation between two parietal regions in visually processing actions. Specifically, aIPL is found to be sensitive to the causal effects of observed actions, while SPL is sensitive to the patterns of body motion involved in those actions. Additional analysis and explanation would help to determine the strength of evidence and the mechanistic underpinnings would benefit from closer consideration. Nevertheless, the work will be of broad interest to cognitive neuroscientists, particularly vision and action researchers.</p></disp-quote><p>We thank the editor and the reviewers for their assessment and their excellent comments and suggestions. We really believe they helped us to provide a stronger and more nuanced paper. In our revision, we addressed all points raised by the reviewers. Most importantly, we added a new section on a series of analyses to characterize in more detail the representations isolated by the action-animation and action-PLD cross-decoding. Together, these analyses strengthen the conclusion that aIPL and LOTC represent action effect structures at a categorical rather than specific level, that is, the type of change (e.g., of location or configuration) rather than the specific effect type (e.g. division, compression). SPL is sensitive to body-specific representations, specifically manuality (unimanual vs. bimanual) and movement kinematics. We also added several other analyses and addressed each point of the reviewers. Please find our responses below.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>The authors report a study aimed at understanding the brain's representations of viewed actions, with a particular aim to distinguish regions that encode observed body movements, from those that encode the effects of actions on objects. They adopt a cross-decoding multivariate fMRI approach, scanning adult observers who viewed full-cue actions, pantomimes of those actions, minimal skeletal depictions of those actions, and abstract animations that captured analogous effects to those actions. Decoding across different pairs of these actions allowed the authors to pull out the contributions of different action features in a given region's representation. The main hypothesis, which was largely confirmed, was that the superior parietal lobe (SPL) more strongly encodes movements of the body, whereas the anterior inferior parietal lobe (aIPL) codes for action effects of outcomes. Specifically, region of interest analyses showed dissociations in the successful cross-decoding of action category across full-cue and skeletal or abstract depictions. Their analyses also highlight the importance of the lateral occipito-temporal cortex (LOTC) in coding action effects. They also find some preliminary evidence about the organisation of action kinds in the regions examined.</p><p>Strengths:</p><p>The paper is well-written, and it addresses a topic of emerging interest where social vision and intuitive physics intersect. The use of cross-decoding to examine actions and their effects across four different stimulus formats is a strength of the study. Likewise, the a priori identification of regions of interest (supplemented by additional full-brain analyses) is a strength.</p><p>Weaknesses:</p><p>I found that the main limitation of the article was in the underpinning theoretical reasoning. The authors appeal to the idea of &quot;action effect structures (AES)&quot;, as an abstract representation of the consequences of an action that does not specify (as I understand it) the exact means by which that effect is caused, nor the specific objects involved. This concept has some face validity, but it is not developed very fully in the paper, rather simply asserted. The authors make the claim that &quot;The identification of action effect structure representations in aIPL has implications for theories of action understanding&quot; but it would have been nice to hear more about what those theoretical implications are. More generally, I was not very clear on the direction of the claim here. Is there independent evidence for AES (if so, what is it?) and this study tests the following prediction, that AES should be associated with a specific brain region that does not also code other action properties such as body movements? Or, is the idea that this finding -- that there is a brain region that is sensitive to outcomes more than movements -- is the key new evidence for AES?</p></disp-quote><p>Thank you for raising this important issue. We reasoned that AES should exist to support the recognition of perceptually variable actions, including those that we have never experienced before. To the best of our knowledge, there is only indirect evidence for the existence of AES, namely that humans effortlessly and automatically recognize actions (and underlying intentions and feelings) in movements of abstract shapes, as in the famous Heider and Simmel (1949) animations. As these animations do not contain any body posture or movement information at all, the only available cues are the spatiotemporal relations between entities and entity parts in the perceived scene. We think that the effortless and automatic attribution of actions to these stimuli points toward an evolutionary optimized mechanism to capture action effect structures from highly variable action instantiations (so general that it even works for abstract animations). Our study thus aimed to test for the existence of such a level of representation in the brain. We clarified this point in the introduction.</p><p>In our revised manuscript, we also revised our discussion of the implications of the finding of AES representations in the brain:</p><p>&quot;The identification of action effect structure representations in aIPL and LOTC has implications for theories of action understanding: Current theories (see for review e.g. Zentgraf et al., 2011; Kemmerer, 2021; Lingnau and Downing, 2024) largely ignore the fact that the recognition of many goal-directed actions requires a physical analysis of the action-induced effect, that is, a state change of the action target. Moreover, premotor and inferior parietal cortex are usually associated with motor- or body-related processing during action observation. Our results, together with the finding that premotor and inferior parietal cortex are similarly sensitive to actions and inanimate object events (Karakose-Akbiyik et al., 2023), suggest that large parts of the 'action observation network' are less specific for body-related processing in action perception than usually thought. Rather, this network might provide a substrate for the physical analysis and predictive simulation of dynamic events in general (Schubotz, 2007; Fischer, 2024). In addition, our finding that the (body-independent) representation of action effects substantially draws on right LOTC contradicts strong formulations of a 'social perception' pathway in LOTC that is selectively tuned to the processing of moving faces and bodies (Pitcher and Ungerleider, 2021). The finding of action effect representation in right LOTC/pSTS might also offer a novel interpretation of a right pSTS subregion thought to specialized for social interaction recognition: Right pSTS shows increased activation for the observation of contingent action-reaction pairs (e.g. agent A points toward object; agent B picks up object) as compared to two independent actions (i.e., the action of agent A has no effect on the action of agent B) (Isik et al., 2017). Perhaps the activation reflects the representation of a social action effect - the change of an agent's state induced by someone else's action. Thus, the representation of action effects might not be limited to physical object changes but might also comprise social effects not induced by a physical interaction between entities. Finally, not all actions induce an observable change in the world. It remains to be tested whether the recognition of, e.g., communication (e.g. speaking, gesturing) and perception actions (e.g. observing, smelling) similarly relies on structural action representations in aIPL and LOTC&quot;</p><disp-quote content-type="editor-comment"><p>On a more specific but still important point, I was not always clear that the significant, but numerically rather small, decoding effects are sufficient to support strong claims about what is encoded or represented in a region. This concern of course applies to many multivariate decoding neuroimaging studies. In this instance, I wondered specifically whether the decoding effects necessarily reflected fully five-way distinction amongst the action kinds, or instead (for example) a significantly different pattern evoked by one action compared to all of the other four (which in turn might be similar). This concern is partly increased by the confusion matrices that are presented in the supplementary materials, which don't necessarily convey a strong classification amongst action kinds. The cluster analyses are interesting and appear to be somewhat regular over the different regions, which helps. However: it is hard to assess these findings statistically, and it may be that similar clusters would be found in early visual areas too.</p></disp-quote><p>We agree that in our original manuscript, we did not statistically test what precisely drives the decoding, e.g., specific actions or rather broader categories. In our revised manuscript, we included a representational similarity analysis (RSA) that addressed this point. In short, we found that the action-animation decoding was driven by categorical distinctions between groups of actions (e.g. hit/place vs. the remaining actions) rather than a fully five-way distinction amongst all action kinds. The action-PLD decoding was mostly driven by , specifically manuality (unimanual vs. bimanual) and movement kinematics; in left and right LOTC we found additional evidence for action-specific representations.</p><p>Please find below the new paragraph on the RSA:</p><p>&quot;To explore in more detail what types of information were isolated by the action-animation and action-PLD cross-decoding, we performed a representational similarity analysis.</p><p>We first focus on the representations identified by the action-animation decoding. To inspect and compare the representational organization in the ROIs, we extracted the confusion matrices of the action-animation decoding from the ROIs (Fig. 5A) and compared them with different similarity models (Fig. 5B) using multiple regression. Specifically, we aimed at testing at which level of granularity action effect structures are represented in aIPL and LOTC: Do these regions encode the broad type of action effects (change of shape, change of location, ingestion) or do they encode specific action effects (compression, division, etc.)? In addition, we aimed at testing whether the effects observed in EVC can be explained by a motion energy model that captures the similarities between actions and animations that we observed in the stimulus-based action-animation decoding using motion energy features. We therefore included V1 in the ROI analysis. We found clear evidence that the representational content in right aIPL and bilateral LOTC can be explained by the effect type model but not by the action-specific model (all p &lt; 0.005; two-sided paired t-tests between models; Fig. 5C). In left V1, we found that the motion energy model could indeed explain some representational variance; however, in both left and right V1 we also found effects for the effect type model. We assume that there were additional visual similarities between the broad types of actions and animations that were not captured by the motion energy model (or other visual models; see Supplementary Information). A searchlight RSA revealed converging results, and additionally found effects for the effect type model in the ventral part of left aIPL and for the action-specific model in the left anterior temporal lobe, left dorsal central gyrus, and right EVC (Fig. 5D). The latter findings were unexpected and should be interpreted with caution, as these regions (except right EVC) were not found in the action-animation cross-decoding and therefore should not be considered reliable (Ritchie et al., 2017). The motion energy model did not reveal effects that survived the correction for multiple comparison, but a more lenient uncorrected threshold of p = 0.005 revealed clusters in left EVC and bilateral posterior SPL.</p><p>To characterize the representations identified by the action-PLD cross-decoding, we used a manuality model that captures whether the actions were performed with both hands vs. one hand, an action-specific model as used in the action-animation RSA above, and a kinematics model that was based on the 3D kinematic marker positions of the PLDs (Fig. 6B). Since pSTS is a key region for biological motion perception, we included this region in the ROI analysis. The manuality model explained the representational variance in the parietal ROIs, pSTS, and LOTC, but not in V1 (all p &lt; 0.002; two-sided paired t-tests between V1 and other ROIs; Fig. 6C). By contrast, the action-specific model revealed significant effects in V1 and LOTC, but not in pSTS and parietal ROIs (but note that effects in V1 and pSTS did not differ significantly from each other; all other two-sided paired t-tests between mentioned ROIs were significant at p &lt; 0.0005). The kinematics model explained the representational variance in all ROIs. A searchlight RSA revealed converging results, and additionally found effects for the manuality model in bilateral dorsal/medial prefrontal cortex and in right ventral prefrontal cortex and insula (Fig. 6D).”</p><p>We also included an ROI covering early visual cortex (V1) in our analysis. While there was significant decoding for action-animation in V1, the representational organization did not substantially match the organization found in aIPL and LOTC: A cluster analysis revealed much higher similarity between LOTC and aIPL than between these regions and V1:</p><p>(please note that in this analysis we included the action-PLD RDMs as reference, and to test whether aIPL shows a similar representational organization in action-anim and action-PLD; see below)</p><p>Given these results, we think that V1 captured different aspects in the action-animation cross-decoding than aIPL and LOTC. We address this point in more detail in our response to the &quot;Recommendations for The Authors&quot;.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>This study uses an elegant design, using cross-decoding of multivariate fMRI patterns across different types of stimuli, to convincingly show a functional dissociation between two sub-regions of the parietal cortex, the anterior inferior parietal lobe (aIPL) and superior parietal lobe (SPL) in visually processing actions. Specifically, aIPL is found to be sensitive to the causal effects of observed actions (e.g. whether an action causes an object to compress or to break into two parts), and SPL to the motion patterns of the body in executing those actions.</p><p>To show this, the authors assess how well linear classifiers trained to distinguish fMRI patterns of response to actions in one stimulus type can generalize to another stimulus type. They choose stimulus types that abstract away specific dimensions of interest. To reveal sensitivity to the causal effects of actions, regardless of low-level details or motion patterns, they use abstract animations that depict a particular kind of object manipulation: e.g. breaking, hitting, or squashing an object. To reveal sensitivity to motion patterns, independently of causal effects on objects, they use point-light displays (PLDs) of figures performing the same actions. Finally, full videos of actors performing actions are used as the stimuli providing the most complete, and naturalistic information. Pantomime videos, with actors mimicking the execution of an action without visible objects, are used as an intermediate condition providing more cues than PLDs but less than real action videos (e.g. the hands are visible, unlike in PLDs, but the object is absent and has to be inferred). By training classifiers on animations, and testing their generalization to full-action videos, the classifiers' sensitivity to the causal effect of actions, independently of visual appearance, can be assessed. By training them on PLDs and testing them on videos, their sensitivity to motion patterns, independent of the causal effect of actions, can be assessed, as PLDs contain no information about an action's effect on objects.</p><p>These analyses reveal that aIPL can generalize between animations and videos, indicating that it is sensitive to action effects. Conversely, SPL is found to generalize between PLDs and videos, showing that it is more sensitive to motion patterns. A searchlight analysis confirms this pattern of results, particularly showing that action-animation decoding is specific to right aIPL, and revealing an additional cluster in LOTC, which is included in subsequent analyses. Action-PLD decoding is more widespread across the whole action observation network.</p><p>This study provides a valuable contribution to the understanding of functional specialization in the action observation network. It uses an original and robust experimental design to provide convincing evidence that understanding the causal effects of actions is a meaningful component of visual action processing and that it is specifically localized in aIPL and LOTC.</p><p>Strengths:</p><p>The authors cleverly managed to isolate specific aspects of real-world actions (causal effects, motion patterns) in an elegant experimental design, and by testing generalization across different stimulus types rather than within-category decoding performance, they show results that are convincing and readily interpretable. Moreover, they clearly took great care to eliminate potential confounds in their experimental design (for example, by carefully ordering scanning sessions by increasing realism, such that the participants could not associate animation with the corresponding real-world action), and to increase stimulus diversity for different stimulus types. They also carefully examine their own analysis pipeline, and transparently expose it to the reader (for example, by showing asymmetries across decoding directions in Figure S3). Overall, this is an extremely careful and robust paper.</p><p>Weaknesses:</p><p>I list several ways in which the paper could be improved below. More than 'weaknesses', these are either ambiguities in the exact claims made, or points that could be strengthened by additional analyses. I don't believe any of the claims or analyses presented in the paper show any strong weaknesses, problematic confounds, or anything that requires revising the claims substantially.</p><p>(1) Functional specialization claims: throughout the paper, it is not clear what the exact claims of functional specialization are. While, as can be seen in Figure 3A, the difference between action-animation cross-decoding is significantly higher in aIPL, decoding performance is also above chance in right SPL, although this is not a strong effect. More importantly, action-PLD cross-decoding is robustly above chance in both right and left aIPL, implying that this region is sensitive to motion patterns as well as causal effects. I am not questioning that the difference between the two ROIs exists - that is very convincingly shown. But sentences such as &quot;distinct neural systems for the processing of observed body movements in SPL and the effect they induce in aIPL&quot; (lines 111-112, Introduction) and &quot;aIPL encodes abstract representations of action effect structures independently of motion and object identity&quot; (lines 127-128, Introduction) do not seem fully justified when action-PLD cross-decoding is overall stronger than action-animation cross-decoding in aIPL. Is the claim, then, that in addition to being sensitive to motion patterns, aIPL contains a neural code for abstracted causal effects, e.g. involving a separate neural subpopulation or a different coding scheme. Moreover, if sensitivity to motion patterns is not specific to SPL, but can be found in a broad network of areas (including aIPL itself), can it really be claimed that this area plays a specific role, similar to the specific role of aIPL in encoding causal effects? There is indeed, as can be seen in Figure 3A, a difference between action-PLD decoding in SPL and aIPL, but based on the searchlight map shown in Figure 3B I would guess that a similar difference would be found by comparing aIPL to several other regions. The authors should clarify these ambiguities.</p></disp-quote><p>We thank the reviewer for this careful assessment. The observation of action-PLD cross-decoding in aIPL is indeed not straightforward to interpret: It could mean that aIPL encodes both body movements and action effect structures by different neural subpopulations. Or it could mean that representations of action effect structures were also activated by the PLDs, which lead to successful decoding in the action-PLD cross-decoding. Our revision allows a more nuanced view on this issue:</p><p>First, we included the results of a behavioral test show that PLDs at least weakly allow for recognition of the specific actions (see our response to the second comment), which in turn might activate action effect structure representations. Second, the finding that also the cross-decoding between animations and PLDs revealed effects in left and right aIPL (as pointed out by the reviewer in the second comment) supports the interpretation that PLDs have activated, to some extent, action effect structure representations.</p><p>On the other hand, if aIPL encodes only action-effect-structures, that were also captured in the action-PLD cross-decoding, we would expect that the RDMs in aIPL are similar for the action-PLD and action-animation cross-decoding. However, the cluster analysis (see our response to Reviewer 1 above) does not show this; rather, all action-PLD RDMs are representationally more similar with each other than with action-animation RDMs, specifically with regard to aIPL. In addition, the RSA revealed sensitivity to manuality and kinematics also in aIPL. This suggests that the action-PLD decoding in aIPL was at least partially driven by representations related to body movements.</p><p>Taken together, these findings suggest that aIPL encodes also body movements. In fact, we didn't want to make the strong claim that aIPL is selectively representing action effect structures. Rather, we think that our results show that aIPL and SPL are disproportionally sensitive to action effects and body movements, respectively. We added this in our revised discussion:</p><p>&quot;The action-PLD cross-decoding revealed widespread effects in LOTC and parietal cortex, including aIPL. What type of representation drove the decoding in aIPL? One possible interpretation is that aIPL encodes both body movements (isolated by the action-PLD cross-decoding) and action effect structures (isolated by the action-animation cross-decoding). Alternatively, aIPL selectively encodes action effect structures, which have been activated by the PLDs. A behavioral test showed that PLDs at least weakly allow for recognition of the specific actions (Tab. S2), which might have activated corresponding action effect structure representations. In addition, the finding that aIPL revealed effects for the cross-decoding between animations and PLDs further supports the interpretation that PLDs have activated, at least to some extent, action effect structure representations. On the other hand, if aIPL encodes only action effect structures, we would expect that the representational similarity patterns in aIPL are similar for the action-PLD and action-animation cross-decoding. However, this was not the case; rather, the representational similarity pattern in aIPL was more similar to SPL for the action-PLD decoding, which argues against distinct representational content in aIPL vs. SPL isolated by the action-PLD decoding. In addition, the RSA revealed sensitivity to manuality and kinematics also in aIPL, which suggests that the action-PLD decoding in aIPL was at least partially driven by representations related to body movements. Taken together, these findings suggest that aIPL encodes not only action effect structures, but also representations related to body movements. Likewise, also SPL shows some sensitivity to action effect structures, as demonstrated by effects in SPL for the action-animation and pantomime-animation cross-decoding. Thus, our results suggest that aIPL and SPL are not selectively but disproportionally sensitive to action effects and body movements, respectively.&quot;</p><p>A clarification to the sentence &quot;aIPL encodes abstract representations of action effect structures independently of motion and object identity&quot;: Here we are referring to the action-animation cross decoding only; specifically, the fact that because the animations did not show body motion and concrete objects, the representations isolated in the action-animation cross decoding must be independent of body motion and concrete objects. This does not rule out that the same region encodes other kinds of representations in addition.</p><p>And another side note to the RSA: It might be tempting to test the &quot;effects&quot; model (distinguishing change of shape, change of location and ingest) also in the action-PLD multiple regression RSA in order to test whether this model explains additional variance in aIPL, which would point towards action effect structure representations. However, the &quot;effect type&quot; model is relatively strongly correlated with the &quot;manuality&quot; model (VIF=4.2), indicating that multicollinearity might exist. We therefore decided to not include this model in the RSA. However, we nonetheless tested the inclusion of this model and did not find clear effects for the &quot;effects&quot; model in aIPL (but in LOTC). The other models revealed largely similar effects as the RSA without the &quot;effects&quot; model, but the effects appeared overall noisier. In general, we would like to emphasize that an RSA with just 5 actions is not ideal because of the small number of pairwise comparisons, which increases the chance for coincidental similarities between model and neural RDMs. We therefore marked this analysis as &quot;exploratory&quot; in the article.</p><disp-quote content-type="editor-comment"><p>(2) Causal effect information in PLDs: the reasoning behind the use of PLD stimuli is to have a condition that isolates motion patterns from the causal effects of actions. However, it is not clear whether PLDs really contain as little information about action effects as claimed. Cross-decoding between animations and PLDs is significant in both aIPL and LOTC, as shown in Figure 4. This indicates that PLDs do contain some information about action effects. This could also be tested behaviorally by asking participants to assign PLDs to the correct action category. In general, disentangling the roles of motion patterns and implied causal effects in driving action-PLD cross-decoding (which is the main dependent variable in the paper) would strengthen the paper's message. For example, it is possible that the strong action-PLD cross-decoding observed in aIPL relies on a substantially different encoding from, say, SPL, an encoding that perhaps reflects causal effects more than motion patterns. One way to exploratively assess this would be to integrate the clustering analysis shown in Figure S1 with a more complete picture, including animation-PLD and action-PLD decoding in aIPL.</p></disp-quote><p>With regard to the suggestion to behaviorally test how well participants can grasp the underlying action effect structures: We indeed did a behavioral experiment to assess the recognizability of actions in the PLD stick figures (as well as in the pantomimes). In short, this experiment revealed that participants could not well recognize the actions in the PLD stick figures and often confused them with kinematically similar but conceptually different actions (e.g. breaking --&gt; shaking, hitting --&gt; swiping, squashing --&gt; knitting). However, the results also show that it was not possible to completely eliminate that PLDs contain some information about action effects.</p><p>Because we considered this behavioral experiment as a standard assessment of the quality of the stimuli, we did not report them in the original manuscript. We now added an additional section to the methods that describes the behavioral experiments in detail:</p><p>&quot;To assess how much the animations, PLD stick figures, and pantomimes were associated with the specific action meanings of the naturalistic actions, we performed a behavioral experiment. 14 participants observed videos of the animations, PLDs (without stick figures), and pantomimes in three separate sessions (in that order) and were asked to describe what kind of actions the animations depict and give confidence ratings on a Likert scale from 1 (not confident at all) to 10 (very confident). Because the results for PLDs were unsatisfying (several participants did not recognize human motion in the PLDs), we added stick figures to the PLDs as described above and repeated the rating for PLD stick figures with 7 new participants, as reported below.</p><p>A general observation was that almost no participant used verb-noun phrases (e.g. &quot;breaking a stick&quot;) in their descriptions for all stimulus types. For the animations, the participants used more abstract verbs or nouns to describe the actions (e.g. dividing, splitting, division; Tab. S1). These abstract descriptions matched the intended action structures quite well, and participants were relatively confident about their responses (mean confidences between 6 and 7.8). These results suggest that the animations were not substantially associated with specific action meanings (e.g. &quot;breaking a stick&quot;) but captured the coarse action structures. For the PLD stick figures (Tab. S2), responses were more variable and actions were often confused with kinematically similar but conceptually different actions (e.g. breaking --&gt; shaking, hitting --&gt; turning page, squashing --&gt; knitting). Confidence ratings were relatively low (mean confidences between 3 and 5.1). These results suggest that PLD stick figures, too, were not substantially associated with specific action meanings and additionally did not clearly reveal the underlying action effect structures. Finally, pantomimes were recognized much better, which was also reflected in high confidence ratings (mean confidences between 8 and 9.2; Tab. S3). This suggests that, unlike PLD stick figures, pantomimes allowed much better to access the underlying action effect structures.&quot;</p><p>We also agree with the second suggestion to investigate in more detail the representational profiles in aIPL and SPL. We think that the best way to do so is the RSA that we reported above. However, to provide a complete picture of the results, we also added the whole brain maps and RDMs for the animation-pantomime, animation-PLD, pantomime-PLD, and action-pantomime to the supplementary information.</p><disp-quote content-type="editor-comment"><p>(3) Nature of the motion representations: it is not clear what the nature of the putatively motion-driven representation driving action-PLD cross-decoding is. While, as you note in the Introduction, other regions such as the superior temporal sulcus have been extensively studied, with the understanding that they are part of a feedforward network of areas analyzing increasingly complex motion patterns (e.g. Riese &amp; Poggio, Nature Reviews Neuroscience 2003), it doesn't seem like the way in which SPL represents these stimuli are similarly well-understood. While the action-PLD cross-decoding shown here is a convincing additional piece of evidence for a motion-based representation in SPL, an interesting additional analysis would be to compare, for example, RDMs of different actions in this region with explicit computational models. These could be, for example, classic motion energy models inspired by the response characteristics of regions such as V5/MT, which have been shown to predict cortical responses and psychophysical performance both for natural videos (e.g. Nishimoto et al., Current Biology 2011) and PLDs (Casile &amp; Giese Journal of Vision 2005). A similar cross-decoding analysis between videos and PLDs as that conducted on the fMRI patterns could be done on these models' features, obtaining RDMs that could directly be compared with those from SPL. This would be a very informative analysis that could enrich our knowledge of a relatively unexplored region in action recognition. Please note, however, that action recognition is not my field of expertise, so it is possible that there are practical difficulties in conducting such an analysis that I am not aware of. In this case, I kindly ask the authors to explain what these difficulties could be.</p></disp-quote><p>Thank you for this very interesting suggestion. We conducted a cross-decoding analysis that was based on the features of motion energy models as described in Nishimoto et al. (2011). Control analyses within each stimulus type revealed high decoding accuracies (animations: 100%, PLDs: 100%, pantomimes: 65%, actions: 55%), which suggests that the motion energy data generally contains information that can be detected by a classifier. However, the cross-decoding between actions and PLDs was at chance (20%), and the classification matrix did not resemble the neural RDMs. We also tested optical flow vectors as input to the decoding, which revealed similarly high decoding for the within-stimulus-type decoding (animations: 75%, PLDs: 100%, pantomimes: 65%, actions: 40%), but again at-chance decoding for action-PLD (20%), notably with a very different classification pattern:</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-98521-sa2-fig1-v1.tif"/></fig><p>Given these mixed results, we decided not to use these models for a statistical comparison with the neural action-PLD RDMs.</p><p>It is notable that the cross-decoding worked generally less well for decoding schemes that involve PLDs, which is likely due to highly different feature complexity of actions and PLDs: Naturalistic actions have much richer visual details, texture, and more complex motion cues. Therefore, motion energy features extracted from these videos likely capture a mixture of both fine-grained and broad motion information across different spatial frequencies. By contrast, motion energy features of PLDs are sparse and might not match the features of naturalistic actions. In a way, this was intended, as we were interested in higher-level body kinematics rather than lower-level motion features. We therefore decided to use a different approach to investigate the representational structure found in the action-PLD cross-decoding: As the PLDs were based on kinematic recordings of actions that were carried out in exactly the same manner as the naturalistic actions, we computed the dissimilarity of the 5 actions based on the kinematic marker positions. Specifically, we averaged the kinematic data across the 2 exemplars per PLD, vectorized the 3D marker positions of all time points of the PLDs (3 dimensions x 13 markers x 200 time points), computed the pairwise correlations between the 5 vectors, and converted the correlations into dissimilarity values by subtracting 1 - r. This RDM was then compared with the neural RDMs extracted from the action-PLD cross-decoding. This was done using a multiple regression RSA (see also our response to Reviewer 1's public comment 2), which allowed us to statistically test the kinematic model against other dissimilarity models: a categorical model of manuality (uni- vs. bimanual) and an action-specific model that discriminates each specific action from each other with equal distance.</p><p>This analysis revealed interesting results: the kinematic model explained the representational variance in bilateral SPL and (particularly right) pSTS as well as in right fusiform cortex and early visual cortex. The action-specific model revealed effects restricted to bilateral LOTC. The manuality model revealed widespread effects throughout the action observation network but not in EVC.</p><disp-quote content-type="editor-comment"><p>(4) Clustering analysis: I found the clustering analysis shown in Figure S1 very clever and informative. However, there are two things that I think the authors should clarify. First, it's not clear whether the three categories of object change were inferred post-hoc from the data or determined beforehand. It is completely fine if these were just inferred post-hoc, I just believe this ambiguity should be clarified explicitly. Second, while action-anim decoding in aIPL and LOTC looks like it is consistently clustered, the clustering of action-PLD decoding in SPL and LOTC looks less reliable. The authors interpret this clustering as corresponding to the manual vs. bimanual distinction, but for example &quot;drink&quot; (a unimanual action) is grouped with &quot;break&quot; and &quot;squash&quot; (bimanual actions) in left SPL and grouped entirely separately from the unimanual and bimanual clusters in left LOTC. Statistically testing the robustness of these clusters would help clarify whether it is the case that action-PLD in SPL and LOTC has no semantically interpretable organizing principle, as might be the case for a representation based entirely on motion pattern, or rather that it is a different organizing principle from action-anim, such as the manual vs. bimanual distinction proposed by the authors. I don't have much experience with statistical testing of clustering analyses, but I think a permutation-based approach, wherein a measure of cluster robustness, such as the Silhouette score, is computed for the clusters found in the data and compared to a null distribution of such measures obtained by permuting the data labels, should be feasible. In a quick literature search, I have found several papers describing similar approaches: e.g. Hennig (2007), &quot;Cluster-wise assessment of cluster stability&quot;; Tibshirani et al. (2001) &quot;Estimating the Number of Clusters in a Data Set Via the Gap Statistic&quot;. These are just pointers to potentially useful approaches, the authors are much better qualified to pick the most appropriate and convenient method. However, I do think such a statistical test would strengthen the clustering analysis shown here. With this statistical test, and the more exhaustive exposition of results I suggested in point 2 above (e.g. including animation-PLD and action-PLD decoding in aIPL), I believe the clustering analysis could even be moved to the main text and occupy a more prominent position in the paper.</p></disp-quote><p>With regard to the first point, we clarified in the methods that we inferred the 3 broad action effect categories after the stimulus selection: &quot;This categorization was not planned before designing the study but resulted from the stimulus selection.&quot;</p><p>Thank you for your suggestion to test more specifically the representational organization in the action-PLD and action-animation RDMs. However, after a careful assessment, we decided to replace the cluster analysis with an RSA. We did this for two reasons:</p><p>First, we think that RSA is a better (and more conventional) approach to statistically investigate the representational structure in the ROIs (and in the whole brain). The RSA allowed us, for example, to specifically test the mentioned distinction between unimanual and bimanual actions, and to test it against other models, i.e., a kinematic model and an action-specific model. This indeed revealed interesting distinct representational profiles of SPL and LOTC.</p><p>Second, we learned that the small number of items (5) is generally not ideal for cluster analyses (absolute minimum for meaningful interpretability is 4, but to form at least 2-3 clusters a minimum of 10-15 items is usually recommended). A similar rule of thumb applies to methods to statistically assess the reliability of cluster solutions (e.g., Silhouette Scores, Cophenetic Correlation Coefficient, Jaccard Coefficient). Finally, the small number of items is not ideal to run a permutation test because the number of unique permutations (for shuffling the data labels: 5! = 30) is insufficient to generate a meaningful null distribution. We therefore think it is best to discard the cluster analysis altogether. We hope you agree with this decision.</p><disp-quote content-type="editor-comment"><p>(5) ROI selection: this is a minor point, related to the method used for assigning voxels to a specific ROI. In the description in the Methods (page 16, lines 514-24), the authors mention using the MNI coordinates of the center locations of Brodmann areas. Does this mean that then they extracted a sphere around this location, or did they use a mask based on the entire Brodmann area? The latter approach is what I'm most familiar with, so if the authors chose to use a sphere instead, could they clarify why? Or, if they did use the entire Brodmann area as a mask, and not just its center coordinates, this should be made clearer in the text.</p></disp-quote><p>We indeed used a sphere around the center coordinate of the Brodmann areas. This was done to keep the ROI sizes / number of voxels constant across ROIs. Since we aimed at comparing the decoding accuracies between aIPL and SPL, we thereby minimized the possibility that differences in decoding accuracy between ROIs are due to ROI size differences. The approach of using spherical ROIs is a quite well established practice that we are using in our lab by default (e.g. Wurm &amp; Caramazza, NatComm, 2019; Wurm &amp; Caramazza, NeuroImage, 2019; Karakose, Caramazza, &amp; Wurm, NatComm, 2023). We clarified that we used spherical ROIs to keep the ROI sizes constant in the revised manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>This study tests for dissociable neural representations of an observed action's kinematics vs. its physical effect in the world. Overall, it is a thoughtfully conducted study that convincingly shows that representations of action effects are more prominent in the anterior inferior parietal lobe (aIPL) than the superior parietal lobe (SPL), and vice versa for the representation of the observed body movement itself. The findings make a fundamental contribution to our understanding of the neural mechanisms of goal-directed action recognition, but there are a couple of caveats to the interpretation of the results that are worth noting:</p><p>(1) Both a strength of this study and ultimately a challenge for its interpretation is the fact that the animations are so different in their visual content than the other three categories of stimuli. On one hand, as highlighted in the paper, it allows for a test of action effects that is independent of specific motion patterns and object identities. On the other hand, the consequence is also that Action-PLD cross-decoding is generally better than Action-Anim cross-decoding across the board (Figure 3A) - not surprising because the spatiotemporal structure is quite different between the actions and the animations. This pattern of results makes it difficult to interpret a direct comparison of the two conditions within a given ROI. For example, it would have strengthened the argument of the paper to show that Action-Anim decoding was better than Action-PLD decoding in aIPL; this result was not obtained, but that could simply be because the Action and PLD conditions are more visually similar to each other in a number of ways that influence decoding. Still, looking WITHIN each of the Action-Anim and Action-PLD conditions yields clear evidence for the main conclusion of the study.</p></disp-quote><p>The reviewer is absolutely right: Because the PLDs are more similar to the actions than the animations, a comparison of the effects of the two decoding schemes is not informative. As we also clarified in our response to Reviewer 2, we cannot rule out that the action-PLD decoding picked up information related to action effect structures. Thus, the only firm conclusion that we can draw from our study is that aIPL and SPL are disproportionally sensitive to action effects and body movements, respectively. We clarified this point in our revised discussion.</p><disp-quote content-type="editor-comment"><p>(2) The second set of analyses in the paper, shown in Figure 4, follows from the notion that inferring action effects from body movements alone (i.e., when the object is unseen) is easier via pantomimes than with PLD stick figures. That makes sense, but it doesn't necessarily imply that the richness of the inferred action effect is the only or main difference between these conditions. There is more visual information overall in the pantomime case. So, although it's likely true that observers can more vividly infer action effects from pantomimes vs stick figures, it's not a given that contrasting these two conditions is an effective way to isolate inferred action effects. The results in Figure 4 are therefore intriguing but do not unequivocally establish that aIPL is representing inferred rather than observed action effects.</p></disp-quote><p>We agree that higher decoding accuracies for Action-Pant vs. Action-PLD and Pant-PLD could also be due to visual details (in particular of hands and body) that are more similar in actions and pantomimes relative to PLDs. However, please note that for this reason we included also the comparison of Anim-Pant vs. Anim-PLD. For this comparison, visual details should not influence the decoding. We clarified this point in our revision.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>It struck me that there are structural distinctions amongst the 5 action kinds that were not highlighted and may have been unintentional. Specifically, three of the actions are &quot;unary&quot; in a sense: break(object), squash(object), hit(object). One is &quot;binary&quot;: place(object, surface), and the fifth (drink) is perhaps ternary - transfer(liquid, cup, mouth)? Might these distinctions be important for the organization of action effects (or actions generally)?</p></disp-quote><p>This is an interesting aspect that we did not think of yet. We agree that for the organization of actions (and perhaps action effects) this distinction might be relevant. One issue we noticed, however, is that for the animations the suggested organization might be less clear, in particular for &quot;drink&quot; as ternary, and perhaps also for &quot;place&quot; as binary. Thus, in the action-animation cross-decoding, this distinction - if it exists in the brain - might be harder to capture. We nonetheless tested this distinction. Specifically, we constructed a dissimilarity model (using the proposed organization, valency model hereafter) and tested it in a multiple regression RSA against an effect type model and two other models for specific actions (discriminating each action from each other with the same distance) and motion energy (as a visual control model). This analysis revealed no effects for the &quot;valency&quot; model in the ROI-based RSA. Also a searchlight analysis revealed no effects for this model. Since we think that the valency model is not ideally suited to test representations of action effects (using data from the action-animation cross-decoding) and to make the description of the RSA not unnecessarily complicated, we decided to not include this model in the final RSA reported in the manuscript.</p><p>In general, I found it surprising that the authors treated their LOTC findings as surprising or unexpected. Given the long literature associating this region with several high-level visual functions related to body perception, action perception, and action execution, I thought there were plenty of a priori reasons to investigate the LOTC's behaviour in this study. Looking at the supplementary materials, indeed some of the strongest effects seem to be in that region.</p><p>(Likewise, classically, the posterior superior temporal sulcus is strongly associated with the perception of others' body movements; why not also examine this region of interest?)</p><disp-quote content-type="editor-comment"><p>One control analysis that would considerably add to the strength of the authors' conclusions would be to examine how actions could be cross-decoded (or not) in the early visual cortex. Especially in comparisons of, for example, pantomime to full-cue video, we might expect a high degree of decoding accuracy, which might influence the way we interpret similar decoding in other &quot;higher level&quot; regions.</p></disp-quote><p>We agree that it makes sense to also look into LOTC and pSTS, and also EVC. We therefore added ROIs for these regions: For EVC and LOTC we used the same approach based on Brodmann areas as for aIPL and SPL, i.e., we used BA 17 for V1 and BA 19 for LOTC. For pSTS, we defined the ROI based on a meta analysis contrast for human vs. non-human body movements (Grobras et al., HBM 2012). Indeed we find that the strongest effects (for both action effect structures and body movements) can be found in LOTC. We also found effects in EVC that, at least for the action-animation cross-decoding, are more difficult to interpret. To test for a coincidental visual confound between actions and animations, we included a control model for motion energy in the multiple regression RSA, which could indeed explain some of the representational content in V1. However, also the effect type model revealed effects in V1, suggesting that there were additional visual features that caused the action-animation cross-decoding in V1. Notably, as pointed out in our response to the Public comments, the representational organization in V1 was relatively distinct from the representational organization in aIPL and LOTC, which argues against the interpretation that effects in aIPL and LOTC were driven by the same (visual) features as in V1.</p><disp-quote content-type="editor-comment"><p>Regarding the analyses reported in Figure 4: wouldn't it be important to also report similar tests for SPL?</p></disp-quote><p>In the analysis of implied action effect structures, we focused on the brain regions that revealed robust effects for action-animation decoding in the ROI and the searchlight analysis, that is, aIPL and SPL. However, we performed a whole brain conjunction analysis to search for other brain regions that show a profile for implied action effect representation. This analysis (that we forgot to mention in our original manuscript; now corrected) did not find evidence for implied action effect representations in SPL.</p><p>However, for completeness, we also added a ROI analysis for SPL. This analysis revealed a surprisingly complex pattern of results: We observed stronger decoding for Anim-Pant vs. Anim-PLD, whereas there were no differences for the comparisons of Action-Pant with Action-PLD and Pant-PLD:</p><p>This pattern of results is not straightforward to explain: First, the equally strong decoding for Action-Pant, Action-PLD, and Pant-PLD suggests that SPL is not substantially sensitive to body part details. Rather, the decoding relied on the coarse body part movements, independently of the specific stimulus type (action, pantomime, PLD). However, the stronger difference between Anim-Pant and Anim-PLD suggests that SPL is also sensitive to implied AES. This appears unlikely, because no effects (in left aIPL) or only weak effects (in right SPL) were found for the more canonical Action-Anim cross-decoding. The Anim-Pant cross-decoding was even stronger than the Action-Anim cross-decoding, which is counterintuitive because naturalistic actions contain more information than pantomimes, specifically with regard to action effect structures. How can this pattern of results be interpreted? Perhaps, for pantomimes and animations, not only aIPL and LOTC but also SPL is involved in inferring (implied) action effect structures. However, for this conclusion, also differences for the comparison of Action-Pant with Action-PLD and for Action-Pant with Pant-PLD should be found. Another non-mutually exclusive interpretation is that both animations and pantomimes are more ambiguous in terms of the specific action, as opposed to naturalistic actions. For example, the squashing animation and pantomime are both ambiguous in terms of what is squashed/compressed, which might require additional load to infer both the action and the induced effect. The increased activation of action-related information might in turn increase the chance for a match between neural activation patterns of animations and pantomimes.</p><p>In any case, these additional results in SPL do not question the effects reported in the main text, that is, disproportionate sensitivity for action effect structures in right aIPL and LOTC and for body movements in SPL and other AON regions. The evidence for implied action effect structures representation in SPL is mixed and should be interpreted with caution.</p><p>We added this analysis and discussion as supplementary information.</p><disp-quote content-type="editor-comment"><p>Statistical arguments that rely on &quot;but not&quot; are not very strong, e.g. &quot;We found higher cross-decoding for animation-pantomime vs. animation-PLD in right aIPL and bilateral LOTC (all t(23) &gt; 3.09, all p &lt; 0.0025; one-tailed), but not in left aIPL (t(23) = 0.73, p = 0.23, one-tailed).&quot; Without a direct statistical test between regions, it's not really possible to support a claim that they have different response profiles.</p></disp-quote><p>Absolutely correct. Notably, we did not make claims about different profiles of the tested ROIs with regard to implied action effect representations. But of course it make sense to test for differential profiles of left vs. right aIPL, so we have added a repeated measures ANOVA to test for an interaction between TEST (animation-pantomime, animation-PLD) and ROI (left aIPL, right aIPL), which, however, was not significant (F(1,23)=3.66, p = 0.068). We included this analysis in the revised manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for The Authors):</bold></p><p>(1) I haven't found any information about data and code availability in the paper: is the plan to release them upon publication? This should be made clear.</p></disp-quote><p>Stimuli, MRI data, and code are deposited at the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/am346/">https://osf.io/am346/</ext-link>). We included this information in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>(2) Samples of videos of the stimuli (or even the full set) would be very informative for the reader to know exactly what participants were looking at.</p></disp-quote><p>We have uploaded the full set of stimuli on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/am346/">https://osf.io/am346/</ext-link>).</p><disp-quote content-type="editor-comment"><p>(3) Throughout the paper, decoding accuracies are averaged across decoding directions (A-&gt;B and B-&gt;A). To my knowledge, this approach was proposed in van den Hurk &amp; Op de Beeck (2019), &quot;Generalization asymmetry in multivariate cross-classification: When representation A generalizes better to representation B than B to A&quot;. I believe it would be fair to cite this paper.</p></disp-quote><p>Absolutely, thank you very much for the hint. We included this reference in our revised manuscript.</p><disp-quote content-type="editor-comment"><p>(4) Page 3, line 70: this is a very nitpicky point, but &quot;This suggests that body movements and the effects they induce are at least partially processed independently from each other.&quot; is a bit of an inferential leap from &quot;these are distinct aspects of real-world actions&quot; to &quot;then they should be processed independently in the brain&quot;. The fact that a distinction exists in the world is a prerequisite for this distinction existing in the brain in terms of functional specialization, but it's not in itself a reason to believe that functional specialization exists. It is a reason to hypothesize that the specialization might exist and to test that hypothesis. So I think this sentence should be rephrased as &quot;This suggests that body movements and the effects they induce might be at least partially processed independently from each other.&quot;, or something to that effect.</p></disp-quote><p>Your reasoning is absolutely correct. We revised the sentence following your suggestion.</p><disp-quote content-type="editor-comment"><p>(5) Page 7, line 182: the text says &quot;stronger decoding for action-animation vs. action-PLD&quot; (main effect of TEST), which is the opposite of what can be seen in the figure. I assume this is a typo?</p></disp-quote><p>Thanks for spotting this, it was indeed a typo. We corrected it: “…stronger decoding for action-PLD vs. action-animation cross-decoding..”</p><disp-quote content-type="editor-comment"><p>(6) Page 7, Figure 3B: since the searchlight analysis is used to corroborate the distinction between aIPL and SPL, it would be useful to overlay the contours of these ROIs (and perhaps LOTC as well) on the brain maps.</p></disp-quote><p>We found that overlaying the contours of the ROIs onto the decoding searchlight maps would make the figure too busy, and the contours would partially hide effects. However, we added a brain map with all ROIs in the supplementary information.</p><disp-quote content-type="editor-comment"><p>(7) Page 9, Figure 4A: since the distinction between the significant difference between anim-pant and anim-PLD is quite relevant in the text, I believe highlighting the lack of difference between the two decoding schemes in left aIPL (for example, by writing &quot;ns&quot;) in the figure would help guide the reader to see the relevant information. It is generally quite hard to notice the absence of something.</p></disp-quote><p>We added “n.s.” to the left aIPL in Fig. 4A.</p><disp-quote content-type="editor-comment"><p>(8) Page 11, line 300: &quot;Left aIPL appears to be more sensitive to the type of interaction between entities, e.g. how a body part or an object exerts a force onto a target object&quot; since the distinction between this and the effect induced by that interaction&quot; is quite nuanced, I believe a concrete example would clarify this for the reader: e.g. I guess the former would involve a representation of the contact between hand and object when an object is pushed, while the latter would represent only the object's displacement following the push?</p></disp-quote><p>Thank you for the suggestion. We added a concrete example: “Left aIPL appears to be more sensitive to the type of interaction between entities, that is, how a body part or an object exerts a force onto a target object (e.g. how a hand makes contact with an object to push it), whereas right aIPL appears to be more sensitive to the effect induced by that interaction (the displacement of the object following the push).”</p><disp-quote content-type="editor-comment"><p>(9) Page 12, line 376: &quot;Informed consent, and consent to publish, was obtained from the participant in Figure 2.&quot; What does this refer to? Was the person shown in the figure both a participant in the study and an actor in the stimulus videos? Since this is in the section about participants in the experiment, it sounds like all participants also appeared in the videos, which I guess is not the case. This ambiguity should be clarified.</p></disp-quote><p>Right, the statement sounds misleading in the “Participants” section. We rephrased it and moved it to the “Stimuli” section: “actions…were shown in 4 different formats: naturalistic actions, pantomimes, point light display (PLD) stick figures, and abstract animations (Fig. 2; informed consent, and consent to publish, was obtained from the actor shown in the figure).”</p><disp-quote content-type="editor-comment"><p>(10) Page 15, line 492: Here, &quot;within-session analyses&quot; are mentioned. However, these analyses are not mentioned in the text (only shown in Figure S2) and their purpose is not clarified. I imagine they were a sanity check to ensure that the stimuli within each stimulus type could be reliably distinguished. This should be explained somewhere.</p></disp-quote><p>We clarified the purpose of the within session decoding analyses in the methods section: &quot;Within-session decoding analyses were performed as sanity checks to ensure that for all stimulus types, the 5 actions could be reliably decoded (Fig. S2).&quot;</p><disp-quote content-type="editor-comment"><p>(11) Page 20, Figure S1: I recommend using the same color ranges for the two decoding schemes (action-anim and action-PLD) in A and C, to make them more directly comparable.</p></disp-quote><p>Ok, done.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>(1) When first looking at Figure 1B, I had a hard time discerning what action effect was being shown (I thought maybe it was &quot;passing through&quot;) Figure 2 later clarified it for me, but it would be helpful to note in the caption that it depicts breaking.</p></disp-quote><p>Thank you for the suggestion. Done.</p><disp-quote content-type="editor-comment"><p>(2) It would be helpful to show an image of the aIPL and SPL ROIs on a brain to help orient readers - both to help them examine the whole brain cross-decoding accuracy and to aid in comparisons with other studies.</p></disp-quote><p>We added a brain map with all ROIs in the supplementary information.</p><disp-quote content-type="editor-comment"><p>(3) Line 181: I'm wondering if there's an error, or if I'm reading it incorrectly. The line states &quot;Moreover, we found ANOVA main effects of TEST (F(1,24)=33.08, p=7.4E-06), indicating stronger decoding for action-animation vs. action-PLD cross-decoding...&quot; But generally, in Figure 3A, it looks like accuracy is lower for Action-Anim than Action-PLD in both hemispheres.</p></disp-quote><p>You are absolutely right, thank you very much for spotting this error. We corrected the sentence: “…stronger decoding for action-PLD vs. action-animation cross-decoding..”</p><disp-quote content-type="editor-comment"><p>(4) It might be useful to devote some more space in the Introduction to clarifying the idea of action-effect structures. E.g., as I read the manuscript I found myself wondering whether there is a difference between action effect structures and physical outcomes in general... would the same result be obtained if the physical outcomes occurred without a human actor involved? This question is raised in the discussion, but it may be helpful to set the stage up front.</p></disp-quote><p>We clarified this point in the introduction:</p><p>In our study, we define action effects as induced by intentional agents. However, the notion of action effect structures might be generalizable to physical outcomes or object changes as such (e.g. an object's change of location or configuration, independently of whether the change is induced by an agent or not).</p><disp-quote content-type="editor-comment"><p>(5) Regarding my public comment #2, it would perhaps strengthen the argument to run the same analysis in the SPL ROIs. At least for the comparison of Anim-Pant with Anim-PLD, the prediction would be no difference, correct?</p></disp-quote><p>The prediction would indeed be that there is no difference for the comparison of Anim-Pant with Anim-PLD, but also for the comparison of Action-Pant with Action-PLD and for Action-Pant with Pant-PLD, there should be no difference. As explained in our response to the public comment #2, we ran a whole brain conjunction (Fig. 4B) to test for the combination of these effects and did not find SPL in this analysis. However, we did found differences for Anim-Pant vs. Anim-PLD, which is not straightforward to interpret (see our response to your public comment #2 for a discussion of this finding).</p></body></sub-article></article>