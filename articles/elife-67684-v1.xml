<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.1d3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">67684</article-id><article-id pub-id-type="doi">10.7554/eLife.67684</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Interacting rhythms enhance sensitivity of target detection in a fronto-parietal computational model of visual attention</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-227236"><name><surname>Aussel</surname><given-names>Amélie</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0498-2905</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-229846"><name><surname>Fiebelkorn</surname><given-names>Ian C</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-2436"><name><surname>Kastner</surname><given-names>Sabine</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9742-965X</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-130497"><name><surname>Kopell</surname><given-names>Nancy J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-89726"><name><surname>Pittman-Polletta</surname><given-names>Benjamin Rafael</given-names><suffix>PhD</suffix></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6798-7191</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Cognitive Rhythms Collaborative</institution>, <institution>Boston University</institution>, <addr-line><named-content content-type="city">Boston</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><institution content-type="dept">Department of Neuroscience</institution>, <institution>University of Rochester</institution>, <addr-line><named-content content-type="city">Rochester</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><institution content-type="dept">Princeton Neuroscience Institute</institution>, <institution>Princeton University</institution>, <addr-line><named-content content-type="city">Princeton</named-content></addr-line>, <country>United States</country></aff><aff id="aff4"><institution content-type="dept">Department of Mathematics and Statistics</institution>, <institution>Boston University</institution>, <addr-line><named-content content-type="city">Boston</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-21837"><name><surname>Haegens</surname><given-names>Saskia</given-names></name><role>Reviewing editor</role><aff><institution>Columbia University College of Physicians and Surgeons</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>aaussel@bu.edu</email> (AA);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>31</day><month>01</month><year>2023</year></pub-date><volume>12</volume><elocation-id>e67684</elocation-id><history><date date-type="received"><day>19</day><month>02</month><year>2021</year></date><date date-type="accepted"><day>12</day><month>01</month><year>2023</year></date></history><permissions><copyright-statement>© 2023, Aussel et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Aussel et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-67684-v1.pdf"/><abstract><p>Even during sustained attention, enhanced processing of attended stimuli waxes and wanes rhythmically, with periods of enhanced and relatively diminished visual processing (and subsequent target detection) alternating at 4 or 8 Hz in a sustained visual attention task. These alternating attentional states occur alongside alternating dynamical states, in which lateral intraparietal cortex (LIP), the frontal eye field (FEF), and the mediodorsal pulvinar (mdPul) exhibit different activity and functional connectivity at α, β and γ frequencies-rhythms associated with visual processing, working memory, and motor suppression. To assess whether and how these multiple interacting rhythms contribute to periodicity in attention, we propose a detailed computational model of FEF and LIP. When driven by θ-rhythmic inputs simulating experimentally-observed mdPul activity, this model reproduced the rhythmic dynamics and behavioral consequences of observed attentional states, revealing that the frequencies and mechanisms of the observed rhythms allow for peak sensitivity in visual target detection while maintaining functional flexibility.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P50 MH109429</award-id><principal-award-recipient><name><surname>Fiebelkorn</surname><given-names>Ian C</given-names></name><name><surname>Kastner</surname><given-names>Sabine</given-names></name><name><surname>Kopell</surname><given-names>Nancy J</given-names></name><name><surname>Pittman-Polletta</surname><given-names>Benjamin Rafael</given-names><suffix>PhD</suffix></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>RO1-MH64043</award-id><principal-award-recipient><name><surname>Fiebelkorn</surname><given-names>Ian C</given-names></name><name><surname>Kastner</surname><given-names>Sabine</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>RO1-EY017699</award-id><principal-award-recipient><name><surname>Fiebelkorn</surname><given-names>Ian C</given-names></name><name><surname>Kastner</surname><given-names>Sabine</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Modelling code is available on the ModelDB open repositories.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Amélie Aussel</collab></person-group><year iso-8601-date="2023">2023</year><source>LIP and FEF rhythmic attention model (Aussel et al. 2023)</source><ext-link ext-link-type="uri" xlink:href="https://senselab.med.yale.edu/modeldb/enterCode?model=267619">https://senselab.med.yale.edu/modeldb/enterCode?model=267619</ext-link><comment>ModelDB, 267619</comment></element-citation></p></sec></sec></back></article>