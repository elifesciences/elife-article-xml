<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">60433</article-id><article-id pub-id-type="doi">10.7554/eLife.60433</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Cortical encoding of acoustic and linguistic rhythms in spoken narratives</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-199469"><name><surname>Luo</surname><given-names>Cheng</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-51085"><name><surname>Ding</surname><given-names>Nai</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3428-2723</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">College of Biomedical Engineering and Instrument Sciences, Zhejiang University, Hangzhou, China 310027</institution>, <institution>Zhejiang University</institution>, <addr-line><named-content content-type="city">Hangzhou</named-content></addr-line>, <country>China</country></aff><aff id="aff2"><institution content-type="dept">Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Sciences, Zhejiang University, Hangzhou, China 310027</institution>, <institution>Zhejiang University</institution>, <addr-line><named-content content-type="city">Hangzhou</named-content></addr-line>, <country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-76551"><name><surname>van Wassenhove</surname><given-names>Virginie</given-names></name><role>Reviewing editor</role><aff><institution>CEA, DRF/I2BM, NeuroSpin; INSERM, U992, Cognitive Neuroimaging Unit</institution>, <country>France</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>ding_nai@zju.edu.cn</email> (ND);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>21</day><month>12</month><year>2020</year></pub-date><volume>9</volume><elocation-id>e60433</elocation-id><history><date date-type="received"><day>26</day><month>06</month><year>2020</year></date><date date-type="accepted"><day>20</day><month>12</month><year>2020</year></date></history><permissions><copyright-statement>Â© 2020, Luo &amp; Ding</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Luo &amp; Ding</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-60433-v1.pdf"/><abstract><p>Speech contains rich acoustic and linguistic information. Using highly controlled speech materials, previous studies have demonstrated that cortical activity is synchronous to the rhythms of perceived linguistic units, e.g., words and phrases, on top of basic acoustic features, e.g., the speech envelope. When listening to natural speech, it remains unclear, however, how cortical activity jointly encodes acoustic and linguistic information. Here, we investigate the neural encoding of words using electroencephalography, and observe neural activity synchronous to multi-syllabic words when participants naturally listen to narratives. An amplitude modulation (AM) cue for word rhythm enhances the word-level response, but the effect is only observed during passive listening. Furthermore, words and the AM cue are encoded by spatially separable neural responses that are differentially modulated by attention. These results suggest that bottom-up acoustic cues and top-down linguistic knowledge separately contribute to cortical encoding of linguistic units in spoken narratives.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31771248</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>Major Scientific Research Project of Zhejiang Lab</institution></institution-wrap></funding-source><award-id>2019KB0AC02</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution>National Key R &amp; D Program of China</institution></institution-wrap></funding-source><award-id>2019YFC0118200</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>Zhejiang Provincial Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>LGF19H090020</award-id><principal-award-recipient><name><surname>Luo</surname><given-names>Cheng</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap></funding-source><award-id>2020FZZX001-05</award-id><principal-award-recipient><name><surname>Ding</surname><given-names>Nai</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The experimental procedures were approved by the Research Ethics Committee of the College of Medicine, Zhejiang University (2019-047). All participants provided written informed consent prior to the experiment and were paid.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>The EEG data and analysis code (in MatLab) were uploaded as Source data files.</p></sec><supplementary-material><ext-link xlink:href="elife-60433-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>