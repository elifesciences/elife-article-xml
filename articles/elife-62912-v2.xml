<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">62912</article-id><article-id pub-id-type="doi">10.7554/eLife.62912</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Optimal plasticity for memory maintenance during ongoing synaptic change</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-126946"><name><surname>Raman</surname><given-names>Dhruva V</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8992-1353</contrib-id><email>dvr23@cam.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-49705"><name><surname>O'Leary</surname><given-names>Timothy</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1029-0158</contrib-id><email>tso24@cam.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>Department of Engineering, University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role>Reviewing Editor</role><aff><institution>Ecole Normale Superieure Paris</institution><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>14</day><month>09</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e62912</elocation-id><history><date date-type="received" iso-8601-date="2020-09-08"><day>08</day><month>09</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-09-13"><day>13</day><month>09</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-08-19"><day>19</day><month>08</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.08.19.257220"/></event></pub-history><permissions><copyright-statement>© 2021, Raman and O'Leary</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Raman and O'Leary</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-62912-v2.pdf"/><abstract><p>Synaptic connections in many brain circuits fluctuate, exhibiting substantial turnover and remodelling over hours to days. Surprisingly, experiments show that most of this flux in connectivity persists in the absence of learning or known plasticity signals. How can neural circuits retain learned information despite a large proportion of ongoing and potentially disruptive synaptic changes? We address this question from first principles by analysing how much compensatory plasticity would be required to optimally counteract ongoing fluctuations, regardless of whether fluctuations are random or systematic. Remarkably, we find that the answer is largely independent of plasticity mechanisms and circuit architectures: compensatory plasticity should be at most equal in magnitude to fluctuations, and often less, in direct agreement with previously unexplained experimental observations. Moreover, our analysis shows that a high proportion of learning-independent synaptic change is consistent with plasticity mechanisms that accurately compute error gradients.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>synaptic plasticity</kwd><kwd>learning</kwd><kwd>memory</kwd><kwd>mathematical modelling</kwd><kwd>neural circuits</kwd><kwd>lifelong learning</kwd><kwd>optimization</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>StG 2016 716643 FLEXNEURO</award-id><principal-award-recipient><name><surname>Raman</surname><given-names>Dhruva V</given-names></name><name><surname>O'Leary</surname><given-names>Timothy</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Simple mathematical reasoning shows that the large amount of synaptic turnover seen in many parts of the brain is in fact optimal for memory retention.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Learning depends upon systematic changes to the connectivity and strengths of synapses in neural circuits. This has been shown across experimental systems (<xref ref-type="bibr" rid="bib35">Moczulska et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Lai et al., 2012</xref>; <xref ref-type="bibr" rid="bib20">Hayashi-Takagi et al., 2015</xref>) and is assumed by most theories of learning (<xref ref-type="bibr" rid="bib21">Hebb, 1949</xref>; <xref ref-type="bibr" rid="bib6">Bienenstock et al., 1982</xref>; <xref ref-type="bibr" rid="bib17">Gerstner et al., 1996</xref>).</p><p>Neural circuits are required not only to learn, but also to retain previously learned information. One might therefore expect synaptic stability in the absence of an explicit learning signal. However, many recent experiments in multiple brain areas have documented substantial ongoing synaptic modification in the absence of any obvious learning or change in behaviour (<xref ref-type="bibr" rid="bib2">Attardo et al., 2015</xref>; <xref ref-type="bibr" rid="bib44">Pfeiffer et al., 2018</xref>; <xref ref-type="bibr" rid="bib22">Holtmaat et al., 2005</xref>; <xref ref-type="bibr" rid="bib32">Loewenstein et al., 2015</xref>; <xref ref-type="bibr" rid="bib58">Yasumatsu et al., 2008</xref>; <xref ref-type="bibr" rid="bib31">Loewenstein et al., 2011</xref>).</p><p>This ongoing synaptic flux is heterogeneous in its magnitude and form. For instance, the expected lifetime of dendritic spines in mouse CA1 hippocampus has been estimated as 1–2 weeks (<xref ref-type="bibr" rid="bib2">Attardo et al., 2015</xref>). Elsewhere in the brain, over 70% of spines in mouse barrel cortex are found to persist for 18 months (<xref ref-type="bibr" rid="bib60">Zuo et al., 2005</xref>), although these persistent spines exhibited large deviations in size over the imaging period (on average, a &gt;25% deviation in spine head diameter).</p><p>The sources of these ongoing changes remain unaccounted for, but are hypothesised to fall into systematic changes associated with learning, development and homeostatic maintenance, and unsystematic changes due to random turnover (<xref ref-type="bibr" rid="bib50">Rule et al., 2019</xref>; <xref ref-type="bibr" rid="bib36">Mongillo et al., 2017</xref>; <xref ref-type="bibr" rid="bib59">Ziv and Brenner, 2018</xref>). A number of experimental studies have attempted to disambiguate and quantify the contributions of different biological processes to overall synaptic changes, either by directly interfering with synaptic plasticity, or by correlating changes to circuit-wide measurements of ongoing physiological activity (<xref ref-type="bibr" rid="bib40">Nagaoka et al., 2016</xref>; <xref ref-type="bibr" rid="bib46">Quinn et al., 2019</xref>; <xref ref-type="bibr" rid="bib58">Yasumatsu et al., 2008</xref>; <xref ref-type="bibr" rid="bib34">Minerbi et al., 2009</xref>; <xref ref-type="bibr" rid="bib12">Dvorkin and Ziv, 2016</xref>). Consistently, these studies find that the total rate of ongoing synaptic change is reduced by only 50% or less in the absence of neural activity or when plasticity pathways are blocked.</p><p>Thus, the bulk of steady-state synaptic changes seem to arise from fluctuations that are independent of activity patterns at pre/post synaptic neurons or known plasticity induction pathways. As such, it seems unlikely that their source is some external learning signal or internal reconsolidation mechanism. This is surprising, because maintenance of neural circuit properties and learned behaviour would intuitively require changes across synapses to be highly co-ordinated. To our knowledge, there is no theoretical account or model prediction that explains these observations.</p><p>One way of reconciling stable circuit function with unstable synapses is to assume that ongoing synaptic changes are localised to ‘unimportant’ synapses, which do not affect circuit function. While this may hold in particular circuits and contexts (<xref ref-type="bibr" rid="bib36">Mongillo et al., 2017</xref>), at least some of the ongoing synaptic changes are likely associated with ongoing learning, which must somehow affect overall circuit function to be effective (<xref ref-type="bibr" rid="bib51">Rule et al., 2020</xref>). Furthermore, this model does not account for the dominant contribution of fluctuations among those synapses that do not remain stable over time.</p><p>In this work we explore another, non-mutually exclusive hypothesis that active plasticity mechanisms continually maintain the overall function of a neural circuit by compensating changes that degrade memories and learned task performance. This fits within the broad framework of memory maintenance via internal replay and reconsolidation, a widely hypothesised class of mechanisms for which there is widespread evidence (<xref ref-type="bibr" rid="bib9">Carr et al., 2011</xref>; <xref ref-type="bibr" rid="bib15">Foster, 2017</xref>; <xref ref-type="bibr" rid="bib39">Nader and Einarsson, 2010</xref>; <xref ref-type="bibr" rid="bib55">Tronson and Taylor, 2007</xref>).</p><p>Compensatory plasticity can be induced by external reinforcement signals (<xref ref-type="bibr" rid="bib23">Kappel et al., 2018</xref>), interactions between different brain areas and circuits (<xref ref-type="bibr" rid="bib1">Acker et al., 2018</xref>), or spontaneous, network-level reactivation events (<xref ref-type="bibr" rid="bib13">Fauth and van Rossum, 2019</xref>). Either way, we can conceptually divide plasticity processes into two types: those that degrade previously learned information, and those that protect against such degradation. We will typically refer to memory-degrading processes as ‘fluctuations’. While these may be stochastic in origin, for example due to intrinsic molecular noise in synapses, we do not demand that this is the case. Fluctuations will therefore account for any synaptic change, random or systematic, that disrupts stored information.</p><p>The central question we address in this work is how compensatory plasticity should act in order to optimally maintain stored information at the circuit level, in the presence of ongoing synaptic fluctuations. To do this, we develop a general modelling framework and conduct a first-principles mathematical analysis that is independent of specific plasticity mechanism and circuit architectures. We find that the rate of compensatory plasticity should not exceed that of the synaptic fluctuations, in direct agreement with experimental measurements. Moreover, fluctuations should dominate as the precision of compensatory plasticity mechanisms increases, where ‘precision’ is defined as the quality of approximation of an error gradient. This provides a potential means of accounting for differences in relative magnitudes of fluctuations in different neural circuits. We validate our theoretical predictions through simulation. Together, our results explain a number of consistent but puzzling experimental findings by developing the hypothesis that synaptic plasticity is optimised for dynamic maintenance of learned information.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Review of key experimental findings</title><p>To motivate the main analysis in this paper we begin with a brief survey of quantitative, experimental measurements of ongoing synaptic dynamics. These studies, summarised in <xref ref-type="table" rid="table1">Table 1</xref>, provide quantifications of the rates of systematic/activity-dependent plasticity relative to ongoing synaptic fluctuations.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Synaptic plasticity rates across experimental models, and the effect of activity suppression.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Reference</th><th>Experimental system</th><th>Total baseline synaptic change</th><th>% synaptic change that is activity / learning-independent</th></tr></thead><tbody><tr><td><xref ref-type="bibr" rid="bib44">Pfeiffer et al., 2018</xref></td><td>Adult mouse hippocampus</td><td>40% turnover over 4 days</td><td>NA</td></tr><tr><td><xref ref-type="bibr" rid="bib31">Loewenstein et al., 2011</xref></td><td>Adult mouse auditory cortex</td><td>&gt;70% of spines changed size by &gt;50% over 20 days</td><td>NA</td></tr><tr><td><xref ref-type="bibr" rid="bib60">Zuo et al., 2005</xref></td><td>Adult mouse (barrel, primary motor, frontal) cortex</td><td>3–5% turnover over 2 weeks for all regions. 73.9 ± 2.8% of spines stable over 18 months (barrel cortex)</td><td>NA</td></tr><tr><td><xref ref-type="bibr" rid="bib40">Nagaoka et al., 2016</xref></td><td>Adult mouse visual cortex</td><td>8% turnover per 2 days in visually deprived environment. 15% in visually enriched environment. 7–8% in both environments under pharmacological suppression of spiking.</td><td><inline-formula><mml:math id="inf1"><mml:mrow><mml:mi/><mml:mo mathsize="80%" stretchy="false">≈</mml:mo><mml:mrow><mml:mn mathsize="80%">50</mml:mn><mml:mo mathsize="80%" stretchy="false">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (turnover)</td></tr><tr><td><xref ref-type="bibr" rid="bib46">Quinn et al., 2019</xref></td><td>Glutamatergic synapses, dissociated rat hippocampal culture</td><td>28 ± 3.7% of synapses formed over 24 hr period. 28.6 ± 2.3% eliminated. Activity suppression through tetanus neurotoxin -light chain. Plasticity rate unmeasured.</td><td><inline-formula><mml:math id="inf2"><mml:mrow><mml:mi/><mml:mo mathsize="80%" stretchy="false">≈</mml:mo><mml:mrow><mml:mn mathsize="80%">75</mml:mn><mml:mo mathsize="80%" stretchy="false">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (turnover)</td></tr><tr><td><xref ref-type="bibr" rid="bib58">Yasumatsu et al., 2008</xref></td><td>CA1 pyramidal neurons, primary culture, rat hippocampus</td><td>Measured rates of synaptic turnover and spine-head volume change. Baseline conditions vs activity suppression (NMDAR inhibitors). Turnover rates: 32.8 ± 3.7% generation/elimination per day (control) vs 22.0 ± 3.6% (NMDAR inhibitor). Rate of spine-head volume change:</td><td><inline-formula><mml:math id="inf3"><mml:mrow><mml:mi/><mml:mo mathsize="80%" stretchy="false">≈</mml:mo><mml:mrow><mml:mn mathsize="80%">67</mml:mn><mml:mo mathsize="80%" stretchy="false">±</mml:mo><mml:mrow><mml:mn mathsize="80%">17</mml:mn><mml:mo mathsize="80%" stretchy="false">%</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (turnover). Size-dependent, but consistently &gt;50% (spine-head volume)</td></tr><tr><td><xref ref-type="bibr" rid="bib12">Dvorkin and Ziv, 2016</xref></td><td>Glutamatergic synapses in cultured networks of mouse cortical neurons</td><td>Partitioned commonly innervated (CI) synapses sharing same axon and dendrite, and non-CI synapses. Quantified covariance in fluorescence change for CI vs non-CI synapses to estimate relative contribution of activity histories to synaptic remodelling</td><td>62–64% (plasticity)</td></tr><tr><td><xref ref-type="bibr" rid="bib34">Minerbi et al., 2009</xref></td><td>Rat cortical neurons in primary culture</td><td>Created ‘<italic>relative synaptic remodeling measure</italic>’ (RRM) based on frequency of changes in the rank ordering of synapses by fluorescence. Compared baseline RRM to when neural activity was suppressed by tetrodotoxin (TTX). RRM: 0.4 (control) vs 0.3 (TTX) after 30 hr.</td><td><inline-formula><mml:math id="inf4"><mml:mrow><mml:mi/><mml:mo mathsize="80%" stretchy="false">≈</mml:mo><mml:mrow><mml:mn mathsize="80%">75</mml:mn><mml:mo mathsize="80%" stretchy="false">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (plasticity)</td></tr><tr><td><xref ref-type="bibr" rid="bib25">Kasthuri et al., 2015</xref></td><td>Adult mouse neocortex (Three-dimensional <italic>post mortem</italic> reconstruction using electron microscopy).</td><td>Data on 124 pairs of ‘redundant’ synapses sharing a pre/post-synaptic neuron was analysed in <xref ref-type="bibr" rid="bib12">Dvorkin and Ziv, 2016</xref>. They calculated the correlation coefficient of spine volumes and post-synaptic density sizes between redundant pairs. This should be one if pre/post-synaptic activity history perfectly explains these variables.</td><td>77% (post-synaptic density, <inline-formula><mml:math id="inf5"><mml:mrow><mml:msup><mml:mi mathsize="80%">r</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">0.23</mml:mn></mml:mrow></mml:math></inline-formula>). <break/>66% (spine volume, <inline-formula><mml:math id="inf6"><mml:mrow><mml:msup><mml:mi mathsize="80%">r</mml:mi><mml:mn mathsize="80%">2</mml:mn></mml:msup><mml:mo mathsize="80%" stretchy="false">=</mml:mo><mml:mn mathsize="80%">0.34</mml:mn></mml:mrow></mml:math></inline-formula>)</td></tr><tr><td><xref ref-type="bibr" rid="bib59">Ziv and Brenner, 2018</xref></td><td>Literature review across multiple systems</td><td>‘<italic>Collectively these findings suggest that the contributions of spontaneous processes and specific activity histories to synaptic remodeling are of similar magnitudes</italic>’</td><td><inline-formula><mml:math id="inf7"><mml:mrow><mml:mi/><mml:mo mathsize="80%" stretchy="false">≈</mml:mo><mml:mrow><mml:mn mathsize="80%">50</mml:mn><mml:mo mathsize="80%" stretchy="false">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>We focused on studies that measured ‘baseline’ synaptic changes that occur outside of any behavioural learning paradigm, and which controlled for stimuli that may induce widespread changes in synaptic strength. The approaches fall into two categories:</p><list list-type="order"><list-item><p>Those that chemically suppress neural activity, and/or block known synaptic plasticity pathways, quantifying consequent changes in the rate of synaptic dynamics, in vitro (<xref ref-type="bibr" rid="bib58">Yasumatsu et al., 2008</xref>; <xref ref-type="bibr" rid="bib34">Minerbi et al., 2009</xref>; <xref ref-type="bibr" rid="bib46">Quinn et al., 2019</xref>) and in vivo (<xref ref-type="bibr" rid="bib40">Nagaoka et al., 2016</xref>). The latter study included a challenging experiment in which neural activity was pharmacologically suppressed in the visual cortex of mice raised in visually enriched conditions.</p></list-item><list-item><p>Those that compare ‘redundant’ synapses sharing pre and post-synaptic neurons, and quantify the proportion of synaptic strength changes attributable to spontaneous processes independent of their shared activity history. These included in vitro studies that involved precise longitudinal imaging of dendritic spines in cultured cortical neurons (<xref ref-type="bibr" rid="bib12">Dvorkin and Ziv, 2016</xref>). They also included in vivo studies, that used electron microscopy to reconstruct and compare the sizes of redundant synapses (<xref ref-type="bibr" rid="bib25">Kasthuri et al., 2015</xref>) <italic>post mortem</italic>.</p></list-item></list><p>The studies in <xref ref-type="table" rid="table1">Table 1</xref> consistently report that the the main component (more than 50%) of baseline synaptic dynamics is due to synaptic fluctuations that are independent of neural activity and/or easily identifiable plasticity signals. This is surprising because such a large contribution of fluctuations might be expected to disrupt circuit function. A key question that we address in this study is whether such a large relative magnitude of fluctuations can be accounted for from first principles, assuming that neural circuits need to protect overall function against perturbations.</p><p>The hypothesis we assumed is that some active plasticity mechanism compensates for the degradation of a learned memory trace or circuit function caused by ongoing synaptic fluctuations. We will thus express overall plasticity as a combination of <italic>synaptic fluctuations</italic> (task-independent processes that degrade memory quality) and <italic>compensatory plasticity</italic>, which counteracts this effect. There are various ways such a compensatory mechanism might access information on the integrity of overall circuit function, memory quality or ’task performance’. It could use external reinforcement signals (<xref ref-type="bibr" rid="bib23">Kappel et al., 2018</xref>; <xref ref-type="bibr" rid="bib51">Rule et al., 2020</xref>). Alternatively, such information could come from another brain region, as hypothesised in for example <xref ref-type="bibr" rid="bib1">Acker et al., 2018</xref>, where cortical memories are stabilised by hippocampal replay events. Spontaneous, network-level reactivation events internal to the neural circuit itself could also plausibly induce performance-increasing plasticity (<xref ref-type="bibr" rid="bib13">Fauth and van Rossum, 2019</xref>). Regardless, the decomposition of total ongoing plasticity into fluctuations and systematic plasticity allows us to derive relationships between both that are independent of the underlying mechanisms, which are not the focus of this study.</p><p>We must acknowledge that it is difficult, experimentally, to pin down and control for all physiological factors that regulate synaptic changes, or indeed to measure such changes accurately. However, even if one does not take the observations in <xref ref-type="table" rid="table1">Table 1</xref> – or their interpretation – at face value, the conceptual question we ask remains relevant for any neural circuit that needs to retain information in the face of ongoing synaptic change.</p></sec><sec id="s2-2"><title>Modelling setup</title><p>Suppose a neural circuit is maintaining previously learned information on a task. The circuit is subject to task-independent synaptic fluctuations which can degrade the quality of learned information. Meanwhile, some compensatory plasticity mechanism counteracts this degradation. Throughout this paper, we treat ‘memory’ and ‘task performance’ as interchangeable because our framework analyses the effect of synaptic weight change on overall circuit function. In this context, we ask:</p><p><italic>if a network optimally maintains learned task performance, what rate of compensatory plasticity is required relative to the rate of synaptic fluctuations?</italic></p><p>By ‘rate’ we mean magnitude of change in a given time interval. Our setup is depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We make the following assumptions, which are also stated mathematically in <xref ref-type="box" rid="box1">Box 1</xref>:</p><list list-type="order"><list-item><p>The neural network has <inline-formula><mml:math id="inf8"><mml:mi>N</mml:mi></mml:math></inline-formula> adaptive elements that we call ‘synaptic weights’ for convenience, although they could include parameters controlling intrinsic neural excitability. We represent these elements through a vector <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which we call the neural network state. Changes to <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> correspond to plasticity.</p></list-item><list-item><p>Any state <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is associated with a quantifiable (scalar) level of task error, denoted <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and called the loss function. A higher value of <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> implies greater corruption of previously learned information.</p></list-item><list-item><p>The network state can be varied continuously. Task error varies smoothly with respect to changes in <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>At any point of time, we can represent the rate of change (i.e. time-derivative) of the synaptic weights as<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>𝐰</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>𝐜</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>ϵ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>as discussed previously, which correspond to compensatory plasticity and synaptic fluctuations, respectively.</p></list-item></list><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Motivating simulation results.</title><p>(<bold>a</bold>) We consider a network attempting to retain previously learned information that is subject to ongoing synaptic changes due to synaptic fluctuations and compensatory plasticity. (<bold>b</bold>) Simulations performed in this study use an abstract, rate based neural network (described in section <italic>Motivating example</italic>). The rate of synaptic fluctuations is constant over time. By iteratively increasing the compensatory plasticity rate in steps we observe a ‘sweet-spot’ compensatory plasticity rate, which is lower than that of the synaptic fluctuations, and which best controls task error. (<bold>c</bold>) A snapshot of the simulation described in b, at the point where the rates of synaptic fluctuations and compensatory plasticity are matched. Even as task error fluctuates around a mean value, individual weights experience systematic changes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62912-fig1-v2.tif"/></fig><p>The magnitude and direction of plasticity may or may not change continually over time. Correspondingly, we may pick an appropriately small time interval, <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, (which is not necessarily infinitesimally small) over which the directions of plasticity can be assumed constant, and write<disp-formula id="equ2"><label>(1)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐜</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where for any time-dependent variable <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we use the notation <inline-formula><mml:math id="inf17"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. We regard <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as coming from unknown probability distributions, which obey the following constraints:</p><list list-type="bullet"><list-item><p>Synaptic fluctuations <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>ϵ</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>: We want to capture ‘task independent’ plasticity mechanisms. As such, we demand that the probability of the mechanism increasing or decreasing any particular synaptic weight over <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is independent of whether such a change increases or decreases task error. A trivial example would be white noise, but systematic mechanisms, such as homeostatic plasticity, could also contribute (<xref ref-type="bibr" rid="bib43">O’Leary, 2018</xref>; <xref ref-type="bibr" rid="bib41">O'Leary and Wyllie, 2011</xref>).</p></list-item><list-item><p>Compensatory plasticity <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>: We demand that compensatory plasticity mechanisms change the network state in a direction of decreasing task error, on average. As such, they cause the network to preserve previously stored information, though not in general by restoring synaptic weights to their previous values following a perturbation.</p></list-item></list><boxed-text id="box1"><label>Box 1.</label><caption><p>Mathematical assumptions on plasticity.</p></caption><p>To quantify memory quality/task performance we consider a loss function <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is twice differentiable in <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This loss function is simply an implicit measure of memory quality; we do not assume that the network explicitly represents <inline-formula><mml:math id="inf25"><mml:mi>F</mml:mi></mml:math></inline-formula>, or has direct access to it. Consider an infinitesimal weight-change <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi></mml:mrow></mml:math></inline-formula> over the infinitesimal time-interval <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. We apply a second order Taylor expansion to express the consequent change in task error: <inline-formula><mml:math id="inf28"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ3"><label>(2)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf29"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the first two derivatives (gradient and hessian) of <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, with respect to a change in the weights <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We assume that <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> are sufficiently small (due to the short time interval) that the third-order term <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒪</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be ignored.</p><p>Next, we assume that <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> are generated from unknown probability distributions. We place some constraints on these distributions. Firstly, synaptic fluctuations should be uncorrelated, in expectation, with the derivatives of <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which govern learning. Accordingly,<disp-formula id="equ4"><label>(3a)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><label>(3b)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>𝐜</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Secondly, we require that <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐜</mml:mi></mml:mrow></mml:math></inline-formula> points in a direction of plasticity that decreases task error, for sufficiently small<disp-formula id="equ6"><label>(3c)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p></boxed-text></sec><sec id="s2-3"><title>Motivating example</title><p>Having described a generic modelling framework, we next uncover a key observation using a simple simulation.</p><p><xref ref-type="fig" rid="fig1">Figure 1</xref> depicts an abstract, artificial neural network trying to maintain a given input-output mapping over time, which is analogous to preservation of a memory trace or learned task. At every timestep, synaptic fluctuations corrupt the weights, and a compensatory plasticity mechanism acts to reduce any error in the input-output mapping (see <xref ref-type="disp-formula" rid="equ36">Equation (1)</xref>). We fix the rate (i.e. magnitude per timestep) of synaptic fluctuations throughout. We increase the compensatory plasticity rate in stages, ranging from a level far below the synaptic fluctuation rate, to a level far above it. Each stage is maintained so that task error can settle to a steady state.</p><p>Two interesting phenomena emerge. The task error of the network is smallest when the compensatory plasticity rate is smaller than the synaptic fluctuation rate (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Meanwhile, individual weights in the network continually change even as overall task error remains stable due to redundancy in the weight configuration (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), (see e.g. <xref ref-type="bibr" rid="bib50">Rule et al., 2019</xref> for a review).</p><p>In this simple simulation, we made a number of arbitrary and non-biologically motivated choices. In particular, we used an abstract, rate-based network, and synthesised compensatory plasticity directions using the biologically questionable backpropagation rule (see Materials and methods for full simulation details). Nevertheless, <xref ref-type="fig" rid="fig1">Figure 1</xref> highlights a phenomenon that we claim is more general:</p><p><italic>The ‘sweet-spot’ compensatory plasticity rate that leads to optimal, steady-state retention of previously learned information is at most equal to the rate of synaptic fluctuations, and often less.</italic></p><p>In the remainder of the results section, we will build intuition as to when and why this claim holds. We will also explore factors influence the precise ‘sweet-spot’ compensatory plasticity rate.</p></sec><sec id="s2-4"><title>The loss landscape</title><p>In order to analyse a general learning scenario that can accommodate biologically relevant assumptions about synaptic plasticity, we will develop a few general mathematical constructs that will allow us to draw conclusions about how synaptic weights affect the overall function of a network.</p><p>We first describe the ‘loss landscape’: a conceptually useful, geometrical visualisation of task error <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see also <xref ref-type="fig" rid="fig2">Figure 2</xref>). Every point on the landscape corresponds to a different network state <inline-formula><mml:math id="inf41"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula>. Whereas any point on a standard three-dimensional landscape has two lateral (xy) co-ordinates, any point on the loss landscape has <inline-formula><mml:math id="inf42"><mml:mi>N</mml:mi></mml:math></inline-formula> co-ordinates representing each synaptic strength. Plasticity changes <inline-formula><mml:math id="inf43"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula>, and thus corresponds to movement on the landscape. Any movement <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi></mml:mrow></mml:math></inline-formula> has both a direction <inline-formula><mml:math id="inf45"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> (where hats denote normalised vectors), and a magnitude <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. Meanwhile, the elevation of a point <inline-formula><mml:math id="inf47"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> on the landscape represents the degree of task error, <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Compensatory plasticity improves task error, and thus moves downhill, regardless of the underlying plasticity mechanism.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Task error landscape and synaptic weight trajectories.</title><p>(<bold>a</bold>) Task error is visualised as the height of a ‘landscape’. Lateral co-ordinates represent the values of different synaptic strengths (only two are visualisable in 3D). Any point on the landscape defines a network state, and the height of the point is the associated task error. Both compensatory plasticity and synaptic fluctuations alter network state, and thus task error, by changing synaptic strengths. Compensatory plasticity reduces task error by moving ‘downwards’ on the landscape. (<bold>b</bold>) Eventually, an approximate steady state is reached where the effect of the two competing plasticity sources on task error cancel out. The synaptic weights wander over a rough level set of the landscape. (<bold>c</bold>) The effect of synaptic fluctuations on task error depends on local curvature in the landscape. Top: a flat landscape without curvature. Even though the landscape is sloped, synaptic fluctuations have no effect on task error in expectation: up/downhill directions are equally likely. Bottom: Although up/downhill synaptic fluctuations are still equally likely, most directions are upwardly curved. Thus, uphill directions increase task error more, and downhill directions decrease task error less. So in expectation, synaptic fluctuations wander uphill.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62912-fig2-v2.tif"/></fig></sec><sec id="s2-5"><title>Understanding curvature in the loss landscape</title><p>Intuitively, one would expect task-independent synaptic fluctuations to increase task error. This is true even if fluctuations are unbiased in moving in an uphill or downhill direction on the loss landscape (see <xref ref-type="disp-formula" rid="equ4">Equation (3a)</xref>) due to the curvature of the landscape (see <xref ref-type="fig" rid="fig2">Figure 2C</xref>). For instance, the slope (mathematically represented by the gradient <inline-formula><mml:math id="inf49"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) at the bottom of a valley is zero. However, every direction is positively curved, and thus moves uphill. More generally, consider a fluctuation that is unbiased in selecting uphill or downhill directions, at a network state <inline-formula><mml:math id="inf50"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula>. The fluctuation will increase task error in expectation if the total curvature of the upwardly curved directions at <inline-formula><mml:math id="inf51"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> exceeds that of the downwardly curved directions, as illustrated in <xref ref-type="fig" rid="fig2">Figure 2c</xref>. We refer to such a state as partially trained. If all directions are upwardly curved, such as at/near the bottom of a valley, we refer to the state as highly trained. Mathematical definitions for these terms are provided in <xref ref-type="box" rid="box2">Box 2</xref>.</p><boxed-text id="box2"><label>Box 2.</label><caption><p>Curvature and the loss landscape.</p></caption><p>Consider a fluctuation <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi></mml:mrow></mml:math></inline-formula> at a state <inline-formula><mml:math id="inf53"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula>. The change in task error, to second order, can be written as<disp-formula id="equ7"><label>(4)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>≈</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>via a Taylor expansion. Suppose the fluctuation is task-independent. So it is unbiased with respect to selecting uphill/downhill, and more/less curved directions on the loss landscape. In this case<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐰</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>In expectation, <xref ref-type="disp-formula" rid="equ60">Equation (4)</xref> thus becomes<disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">[</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>If <inline-formula><mml:math id="inf54"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, then the expected change in task error is positive, and we refer to the network state as ‘partially trained’. If additionally, <inline-formula><mml:math id="inf55"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>⪰</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, that is, <inline-formula><mml:math id="inf56"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for any choice of <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi></mml:mrow></mml:math></inline-formula>, then we refer to the network as highly trained. The ‘highly trained’ condition always holds in a neighbourhood of a local minimum of task error.</p></boxed-text><p>Comparison of the upward curvature of different plasticity directions plays an important role in the remainder of the section. Therefore, we introduce the following operator:<disp-formula id="equ10"><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is mathematical shorthand for the degree of curvature in the direction <inline-formula><mml:math id="inf59"><mml:mi mathvariant="bold">𝐯</mml:mi></mml:math></inline-formula>, at point <inline-formula><mml:math id="inf60"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> on the loss landscape, and is depicted in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. Note that <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> depends solely upon the direction, and not the magnitude, of <inline-formula><mml:math id="inf62"><mml:mi mathvariant="bold">𝐯</mml:mi></mml:math></inline-formula>.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Quantifying effect of task error lanscape curvature on compensatory plasticity.</title><p>(<bold>a</bold>) Geometrical intuition behind the operator <inline-formula><mml:math id="inf63"><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub></mml:math></inline-formula>. The operator charts the degree to which a (normalised) direction is upwardly curved (i.e. lifts off the tangent plane depicted in grey). The red, shaded areas filling the region between the tangent plane and the upwardly curved directions are proportional to <inline-formula><mml:math id="inf64"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf65"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. (<bold>b</bold>) Compensatory plasticity points in a direction of locally decreasing task-error. Excessive plasticity in this direction can be detrimental, due to upward curvature (‘overshoot’). The optimal magnitude for a given direction is smaller if upward curvature (i.e. the <inline-formula><mml:math id="inf66"><mml:mi>Q</mml:mi></mml:math></inline-formula>-value) is large, as for cases (i) and (ii), and if the initial slope is shallow, as for case (ii). It is greater if the initial slope is steep, as for case (iii). This intuition underlies <xref ref-type="disp-formula" rid="equ12">Equation (6)</xref> for the optimal magnitude of a given compensatory plasticity direction, which includes as a coefficient the ratio of slope to curvature. (<bold>c</bold>) <xref ref-type="disp-formula" rid="equ21">Equation (11)</xref> depends upon the ratio of the upward curvatures in the two plasticity directions, <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>. As illustrated, steep downhill directions often exhibit more upward curvature than arbitrary directions. In such cases, the optimal magnitude of compensatory plasticity should be outcompeted by synaptic fluctuations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62912-fig3-v2.tif"/></fig></sec><sec id="s2-6"><title>An expression for the optimal degree of compensatory plasticity during learning</title><p>The rates of compensatory plasticity and synaptic fluctuations, at time <inline-formula><mml:math id="inf69"><mml:mi>t</mml:mi></mml:math></inline-formula>, are <inline-formula><mml:math id="inf70"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf71"><mml:mrow><mml:mover accent="true"><mml:mi>ϵ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. These rates may change continually over time. Let’s temporarily assume they are fixed over a small time interval <inline-formula><mml:math id="inf72"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. Thus,<disp-formula id="equ11"><label>(5)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>What magnitude of compensatory plasticity, <inline-formula><mml:math id="inf73"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, most decreases task error over <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>? The answer is<disp-formula id="equ12"><label>(6)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mover><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>A mathematical derivation is contained in <xref ref-type="box" rid="box3">Box 3</xref>, with geometric intuition in <xref ref-type="fig" rid="fig3">Figure 3b</xref>. Note that our answer turns out to be independent of the synaptic fluctuation rate <inline-formula><mml:math id="inf75"><mml:mrow><mml:mover accent="true"><mml:mi>ϵ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Here,</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> represents the sensitivity of the task error to changes (i.e. the steepness of the loss landscape).</p></list-item><list-item><p><inline-formula><mml:math id="inf77"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the accuracy of the compensatory plasticity direction in conforming to the steepest downhill direction on the loss landscape (in particular, their normalised correlation).</p></list-item><list-item><p><inline-formula><mml:math id="inf78"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the upward curvature of the compensatory plasticity direction. As shown in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, excessive plasticity in an upwardly curved, but downhill, direction, can eventually increase task error. Thus, upward curvature limits the ideal magnitude of compensatory plasticity in the direction <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>.</p></list-item></list><boxed-text id="box3"><label>Box 3.</label><caption><p>Optimal magnitude of compensatory plasticity.</p></caption><p>Let us rewrite <xref ref-type="disp-formula" rid="equ56">Equation (2)</xref>, using the operator <inline-formula><mml:math id="inf80"><mml:mi>Q</mml:mi></mml:math></inline-formula> and omitting higher order terms, as justified in <xref ref-type="box" rid="box1">Box 1</xref>:<disp-formula id="equ13"><label>(7)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We can substitute our assumptions on synaptic fluctuations (<xref ref-type="disp-formula" rid="equ59">Equations (3)</xref>) into <xref ref-type="disp-formula" rid="equ70">Equation (7)</xref> to get<disp-formula id="equ14"><label>(8)</label><mml:math id="m14"><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Note that the requirement for assumption (3b) can be removed, but the alternative resulting derivation is more involved (see SI section two for this alternative).</p><p>We can differentiate <xref ref-type="disp-formula" rid="equ72">Equation (8)</xref> in <inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, to get:<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The root of this derivative gives a global minimum of the <xref ref-type="disp-formula" rid="equ72">Equation (8)</xref> in <inline-formula><mml:math id="inf82"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, as long as <inline-formula><mml:math id="inf83"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> holds (justified in SI section 2.1). We get <xref ref-type="disp-formula" rid="equ12">Equation (6)</xref>, which defines the compensatory plasticity magnitude that minimises <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula>, and thus overall task error, at time <inline-formula><mml:math id="inf85"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></boxed-text><p>For now, <xref ref-type="disp-formula" rid="equ12">Equation (6)</xref> is valid only if the compensatory plasticity direction is fixed during <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. If we want <xref ref-type="disp-formula" rid="equ12">Equation (6)</xref> to also be compatible with continually changing compensatory plasticity directions, it needs to be valid for an arbitrarily small <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. However, enacting a non-negligible magnitude <inline-formula><mml:math id="inf88"><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> of plasticity over an arbitrarily small time interval <inline-formula><mml:math id="inf89"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> would require an unattainable, ‘infinitely-fast’ plasticity rate.</p><p>In fact, we show in the next section that our expression for <inline-formula><mml:math id="inf90"><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> does become compatible with continuously changing plasticity at the end of learning, when task-error is stable.</p></sec><sec id="s2-7"><title>Characterising the optimal rate of compensatory plasticity at steady state</title><p>Consider a scenario where task error is approximately stable. In this case, <inline-formula><mml:math id="inf91"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>≈</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> over <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. In this scenario, <xref ref-type="disp-formula" rid="equ12">Equation (6)</xref> simplifies to<disp-formula id="equ16"><label>(9a)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>as derived in <xref ref-type="box" rid="box4">Box 4</xref> and illustrated geometrically in <xref ref-type="fig" rid="fig3">Figure 3c</xref>. We see that the magnitude <inline-formula><mml:math id="inf93"><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> is proportional to <inline-formula><mml:math id="inf94"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, which is itself proportional to <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ61">Equation (5)</xref>, given some fixed rate of synaptic fluctuations. Thus, <inline-formula><mml:math id="inf96"><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> is attainable even as <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> shrinks to zero, and is thus compatible with continually changing compensatory plasticity directions. In this case, <xref ref-type="disp-formula" rid="equ16">Equation (9)</xref> can be rewritten as<disp-formula id="equ17"><label>(9b)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ16">Equation (9)</xref> is a key result of the paper. It applies regardless of the underlying plasticity mechanisms that induced <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>. It is compatible with continually or occasionally changing directions of compensatory plasticity (i.e. infinitesimal or non-infinitesimal <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>). It says that the optimal compensatory plasticity rate, relative to the rate of synaptic fluctuations, depends on the relative upward curvature of these two plasticity directions on the loss landscape.</p><p>A corollary is that the optimal rate of compensatory plasticity is greater during learning than at steady state. If we substitute the steady-state requirement: <inline-formula><mml:math id="inf101"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, with the condition for learning: <inline-formula><mml:math id="inf102"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, in the derivation of <xref ref-type="box" rid="box4">Box 4</xref>, then we get<disp-formula id="equ18"><label>(10)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Indeed, the faster the optimal potential learning rate <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the greater the optimal compensatory plasticity rate. Thus <inline-formula><mml:math id="inf104"><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup></mml:math></inline-formula> decreases as learning slows to a halt, eventually reaching the level of <xref ref-type="disp-formula" rid="equ17">Equation (9b)</xref>.</p><boxed-text id="box4"><label>Box 4.</label><caption><p>Optimal compensatory plasticity magnitude at steady state error.</p></caption><p>Let us substitute the special condition <inline-formula><mml:math id="inf105"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (steady-state task error) into <xref ref-type="disp-formula" rid="equ72">Equation (8)</xref>. This gives<disp-formula id="equ19"><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Next, we substitute in our optimal reconsolidation magnitude (<xref ref-type="disp-formula" rid="equ12">Equation (6)</xref>). This gives<disp-formula id="equ20"><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>which in turn implies the result (<xref ref-type="disp-formula" rid="equ16">Equation (9)</xref>).</p><p>Note that <xref ref-type="disp-formula" rid="equ16">Equation (9)</xref> is only valid when both the numerator and denominator of the right hand side are both positive. The converse is unlikely in a partially trained network, and impossible in a highly trained network (see SI section 2.1).</p></boxed-text><sec id="s2-7-1"><title>Main claim</title><p>We now claim that generically, the optimal compensatory plasticity rate should not outcompete the rate of synaptic fluctuations at steady state error. We will first provide geometric intuition for our claim, before bolstering with analytical arguments and making precise our notion of ‘generically’.</p><p>From <xref ref-type="disp-formula" rid="equ16">Equation (9)</xref>, our main claim holds if<disp-formula id="equ21"><label>(11)</label><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>that is, <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> points in a more upwardly curved direction than <inline-formula><mml:math id="inf107"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>. When would this be true?</p><p>First consider <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>. Statistical independence from the task error means it should point in an ‘averagely’ curved direction. Mathematically (see SI secton 2.1), this means<disp-formula id="equ22"><label>(12)</label><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Our assumption of ‘average’ curvature fails if synaptic fluctuations are specialised to ‘unimportant’ synapses whose changes have little effect on task error. In this case <inline-formula><mml:math id="inf109"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> would be even smaller, since <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> would be constrained to consistently shallow, less-curved directions. Thus, this possibility does not interfere with our main claim.</p><p>For <xref ref-type="disp-formula" rid="equ21">Equation (11)</xref> to hold, <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> should point in directions of ‘more-than-average’ upward curvature. This follows intuitively because a steep downhill direction, which effectively reduces task error, will usually have higher upward curvature than an arbitrary direction (see <xref ref-type="fig" rid="fig3">Figure 3c</xref> for intuition). It remains to formalise this argument mathematically, and consider edge cases where it doesn’t hold.</p></sec></sec><sec id="s2-8"><title>Dependence of the optimal magnitude of steady-state, compensatory plasticity on the mechanism</title><p>Compensatory plasticity is analogous to learning, since it acts to reduce task error. We do not yet know the algorithms that neural circuits use to learn, although constructing biologically plausible learning algorithms is an active research area. Nevertheless, all the potential learning algorithms we are aware of fit into three broad categories. For each category, we shall show why and when our main claim holds. We will furthermore investigate quantitative differences in the optimal compensatory plasticity rate, across and within categories. A full mathematical justification of all the assertions we make is found in SI section 1.3.</p><p>We first highlight a few general points:</p><list list-type="bullet"><list-item><p>For any compensatory plasticity mechanism, <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> depends not only on the algorithm, but the point <inline-formula><mml:math id="inf113"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> on the landscape. We cannot ever claim that <xref ref-type="disp-formula" rid="equ21">Equation (11)</xref> holds for all network states.</p></list-item><list-item><p>We calculate the expected value of <inline-formula><mml:math id="inf114"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for an ‘average’, trained, state <inline-formula><mml:math id="inf115"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula>, across classes of algorithm. This corresponds to a plausible best-case tuning of compensatory plasticity that a neural circuit might be able to achieve. Any improvement would rely on online calculation of <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which we do not believe would be plausible biologically.</p></list-item></list><p>Learning algorithms attempt to move to the bottom of the loss landscape. But they are blind. Spying a distant valley equates to ‘magically’ predicting that a very different network state will have very low task error. How do they find their way downhill? There are three broad strategies (<xref ref-type="bibr" rid="bib49">Raman and O'Leary, 2021</xref>):</p><list list-type="bullet"><list-item><p>0<sup>th</sup> order algorithms take small, exploratory steps in random directions. Information from the change in task error over these steps informs retained changes. For instance, steps that improve task error are retained. A notable 0-order algorithm is REINFORCE (<xref ref-type="bibr" rid="bib57">Williams, 1992</xref>). Many computational models of biological learning in different circuits derive from this algorithm (<xref ref-type="bibr" rid="bib53">Seung, 2003</xref>; <xref ref-type="bibr" rid="bib14">Fee and Goldberg, 2011</xref>; <xref ref-type="bibr" rid="bib8">Bouvier et al., 2018</xref>; <xref ref-type="bibr" rid="bib26">Kornfeld et al., 2020</xref>).</p></list-item><list-item><p>1<sup>st</sup> order algorithms explicitly approximate/calculate, and then step down the locally steepest direction (i.e. the gradient <inline-formula><mml:math id="inf117"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). The backpropagation algorithm implements perfect gradient descent. Many approximate gradient descent methods with more biologically plausible assumptions have been developed in the recent literature (see e.g. <xref ref-type="bibr" rid="bib38">Murray, 2019</xref>; <xref ref-type="bibr" rid="bib56">Whittington and Bogacz, 2019</xref>; <xref ref-type="bibr" rid="bib4">Bellec et al., 2020</xref>; <xref ref-type="bibr" rid="bib29">Lillicrap et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Guerguiev et al., 2017</xref>, and <xref ref-type="bibr" rid="bib30">Lillicrap et al., 2020</xref> for a review).</p></list-item><list-item><p>2<sup>nd</sup> order algorithms additionally approximate/calculate the hessian <inline-formula><mml:math id="inf118"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which provides information on local curvature. They look for descent directions that are both steep, and less upwardly curved. We doubt it is possible for biologically plausible learning rules to accurately approximate the hessian, which has <inline-formula><mml:math id="inf119"><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> entries representing the interaction between every possible pair of synaptic weights.</p></list-item></list><p><xref ref-type="table" rid="table2">Table 2</xref> shows the categories for which our main claim holds.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Table elements highlighted in teal correspond to scenarios in which our main claim holds, as <xref ref-type="disp-formula" rid="equ21">Equation (11)</xref> is satisfied.</title></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Quadratic <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></th><th>Nonlinear <inline-formula><mml:math id="inf121"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, low steady-state error</th><th>Nonlinear <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, high steady-state error</th></tr></thead><tbody><tr><td>0<sup>th</sup> order algorithm</td><td><inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf124"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf125"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td>0<sup>st</sup> order algorithm</td><td><inline-formula><mml:math id="inf126"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf127"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf128"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td>0<sup>nd</sup> order algorithm</td><td><inline-formula><mml:math id="inf129"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf130"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf131"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>We first consider the simplest case of a quadratic loss function <inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, directions of curvature in any direction are constant (mathematically, the hessian <inline-formula><mml:math id="inf133"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> does not vary with network state). Moreover, the gradient obeys a consistent relationship with the hessian:<disp-formula id="equ23"><label>(13)</label><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Components of <inline-formula><mml:math id="inf134"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with high upward curvature are magnified under the transformation <inline-formula><mml:math id="inf135"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, since they correspond to eigenvectors of <inline-formula><mml:math id="inf136"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with high eigenvalue. Conversely, components with low upward curvature are shrunk. As the gradient <inline-formula><mml:math id="inf137"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the output of such a transformation from <xref ref-type="disp-formula" rid="equ23">Equation (13)</xref>, this suggests it is biased towards directions of high upward curvature. Indeed, we can quantify this bias. Let <inline-formula><mml:math id="inf138"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> be the eigenvalues of <inline-formula><mml:math id="inf139"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf140"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> the projections of the corresponding eigenvectors onto <inline-formula><mml:math id="inf141"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. Then<disp-formula id="equ24"><label>(14)</label><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The value of <xref ref-type="disp-formula" rid="equ24">Equation (14)</xref> depends on the values <inline-formula><mml:math id="inf142"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. In the ‘average’ case, where they are equal, and <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is thus a direction of ‘average’ curvature, <inline-formula><mml:math id="inf144"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> holds. This inequality gap widens with increasing anisotropy in the curvature of different directions (i.e. with a wider spread of eigenvalues <inline-formula><mml:math id="inf145"><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, corresponding to more elliptical/less circular level sets in the illustration of <xref ref-type="fig" rid="fig4">Figure 4b</xref>). Indeed, simulation results in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> (top row) show how the ratio <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> that optimises steady-state task error is significantly less than one, in a quadratic error function where compensatory plasticity accurately follows the gradient, and for different synaptic fluctuation rates.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Geometric intuition for the optimal magnitude of different compensatory plasticity directions.</title><p>Colours depict level sets of the loss landscape. Elliptical level sets correspond to a quadratic loss function (which approximates any loss function in the neighbourhood of a local minimum). In c and d, we depict compensatory plasticity and synaptic fluctuations as sequential, alternating processes for illustrative purposes, although they are modelled as concurrent throughout the paper. (<bold>a</bold>) Compensatory plasticity directions locally decrease task error, so point from darker to lighter colours. Optimal magnitude is reached when the vectors ‘kiss’ a smaller level set, that is, intersect that level set while running parallel to its border. Increasing magnitude past this past this point increases task error, by moving network state to a higher-error level set. (<bold>b</bold>) If compensatory plasticity is parallel to the gradient (i.e. it enacts gradient descent), then it runs perpendicular to the border of the level set on which it lies (i.e. the tangent plane). This is shown explicitly for the ‘exact gradient’ direction of plasticity. The optimal magnitude of plasticity in this direction is smaller than that of a corrupted gradient descent direction, even though the former is more effective in reducing task error, because the exact gradient points in a more highly curved direction. (<bold>c</bold>) Synaptic fluctuations of a certain magnitude perturb the network state. The optimal magnitude of compensatory plasticity (in the exact gradient descent direction, for this example) is significantly smaller than that of the synaptic fluctuations, using the geometric heuristic explained in (<bold>a</bold>). If the magnitude of compensation increased to match the synaptic fluctuation magnitude there would be overshoot, and task error would converge to a higher steady state. (<bold>d</bold>) If compensatory plasticity mechanisms can perfectly calculate both the local gradient and hessian (curvature) of the loss landscape, then network state will move in the direction of the ‘Newton step’. In the quadratic case (elliptical level sets), this will directly ‘backtrack’ the synaptic fluctuations. Thus, the optimal magnitude of compensatory plasticity will be equal to that of the synaptic fluctuations. However, time delays in the sensing of synaptic fluctuations and limited precision of the compensatory plasticity mechanism will preclude this.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62912-fig4-v2.tif"/></fig><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The dependence of steady state task performance in a nonlinear network on the magnitudes of compensatory plasticity and synaptic fluctuations, and on the learning rule quality.</title><p>Each <inline-formula><mml:math id="inf147"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> value on a given graph corresponds to an 8000 timepoint nonlinear neural network simulation (see ‘Methods’ for details). The <inline-formula><mml:math id="inf148"><mml:mi>y</mml:mi></mml:math></inline-formula> value gives the steady-state task error (average task error of the last 500 timepoints) of the simulation, while the <inline-formula><mml:math id="inf149"><mml:mi>x</mml:mi></mml:math></inline-formula> value gives the ratio of the magnitudes of the compensatory plasticity and synaptic fluctuations terms. Steady state error is averaged across 8 simulation repeats; shading depicts one standard deviation. Between graphs, we change simulation parameters. Down rows, we increase the proportionate noise corruption of the compensatory plasticity term (see Materials and methods section for details). Across columns, we increase the magnitude of synaptic fluctuations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62912-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>The dependence of steady state task performance in a linear network on the magnitudes of compensatory plasticity and synaptic fluctuations, and on the learning rule quality.</title><p>The description of this figure is identical to that of <xref ref-type="fig" rid="fig5">Figure 5</xref>. The only difference is the choice of network. Here, we use the linear networks as described in Methods.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62912-fig5-figsupp1-v2.tif"/></fig></fig-group><p>What about the case of a nonlinear loss function? Close to a minimum <inline-formula><mml:math id="inf150"><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, the relationship of <xref ref-type="disp-formula" rid="equ23">Equation (13)</xref> approximately holds (the loss function is locally quadratic). So if steady-state error is very low, we can directly transport the intuition of the quadratic case. However when steady state error increases, <xref ref-type="disp-formula" rid="equ23">Equation (13)</xref> becomes increasingly approximate. In the limiting case, we could consider <inline-formula><mml:math id="inf151"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as being completely uncorrelated from <inline-formula><mml:math id="inf152"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, in which case <inline-formula><mml:math id="inf153"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> would hold. Numerical results in <xref ref-type="fig" rid="fig5">Figure 5</xref> supports this assertion in nonlinear networks: the optimal ratio satisfies <inline-formula><mml:math id="inf154"><mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in conditions where steady-state task error is high, and <inline-formula><mml:math id="inf155"><mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in conditions where it is low.</p><p>Overall, we see that if <inline-formula><mml:math id="inf156"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (i.e. compensatory plasticity enacts gradient descent), then we would expect compensatory plasticity to be outcompeted by synaptic fluctuations to maintain optimal steady-state error.</p><p>Even if compensatory plasticity does not move in the steepest direction of error decrease (i.e. the error gradient), it must move in an approximate downhill direction to improve task error (see e.g. <xref ref-type="bibr" rid="bib47">Raman et al., 2019</xref>). Furthermore, the worse the quality of the gradient approximation, the <italic>larger</italic> the optimal level of compensatory plasticity (illustrated conceptually in <xref ref-type="fig" rid="fig4">Figure 4b–c</xref>, and numerically in <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Why? We can rewrite such a learning rule as<disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐜</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf157"><mml:mi>ν</mml:mi></mml:math></inline-formula> represents systematic error in the gradient approximation. The upward curvature in the direction <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> becomes a (nonlinear) interpolation of the upward curvatures in the directions <inline-formula><mml:math id="inf159"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf160"><mml:mi>ν</mml:mi></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ65">Equation (A6)</xref> of the SI). As long as <inline-formula><mml:math id="inf161"><mml:mi>ν</mml:mi></mml:math></inline-formula> is less biased towards high curvature directions than <inline-formula><mml:math id="inf162"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> itself, then this decreases the upward curvature in the direction <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, and thus increases the optimal compensatory plasticity rate. Indeed <xref ref-type="fig" rid="fig5">Figure 5</xref> shows in simulation that this rate increases for more inaccurate compensatory plasticity mechanisms.</p><p>We now turn to zero-order learning algorithms, such as REINFORCE. These do not explicitly approximate a gradient, but generate random plasticity directions, which are retained/opposed based upon their observed effect on task error. We would expect randomly generated plasticity directions to have ‘average’ upward curvature, similarly to synaptic fluctuations. In this case, we would therefore get <inline-formula><mml:math id="inf164"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and compensatory plasticity should thus equal synaptic fluctuations in magnitude.</p><p>Finally, we consider second-order learning algorithms, and in particular the Newton update:<disp-formula id="equ26"><mml:math id="m26"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐜</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As previously discussed, we assume that learning algorithms that require detailed information about the Hessian are biologically implausible. As such, our treatment is brief, and mainly contained in SI section 2.2.2.</p><p>In a linear network, the Newton update corresponds to compensatory plasticity making a direct ‘beeline’ for <inline-formula><mml:math id="inf165"><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig4">Figure 4d</xref>). As such <inline-formula><mml:math id="inf166"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and the optimal magnitude of compensatory plasticity should match synaptic fluctuations. The same is true for a nonlinear network in a near-optimal state. However if steady-state task error is high in a nonlinear network, then compensatory plasticity should outcompete synaptic fluctuations. This case does not contradict our central claim however, since high task error at steady state implies that the task is not truly learned.</p><p>Together our results and analyses show that the magnitude of compensatory plasticity, at steady state task error, should be less or equal to that of synaptic fluctuations. This conclusion does not depend upon circuit architecture, or choice of biologically plausible learning algorithm.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>A long-standing question in neuroscience is how neural circuits maintain learned memories while being buffeted by synaptic fluctuations from noise and other task-independent processes (<xref ref-type="bibr" rid="bib16">Fusi et al., 2005</xref>). There are several hypotheses that offer potential answers, none of which are mutually exclusive. One possibility is that fluctuations only occur in a subset of volatile connections that are relatively unimportant for learned behaviours (<xref ref-type="bibr" rid="bib35">Moczulska et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Chambers and Rumpel, 2017</xref>; <xref ref-type="bibr" rid="bib24">Kasai et al., 2003</xref>). Following this line of thought, circuit models have been proposed that only require stability in a subset of synapses for stable function (<xref ref-type="bibr" rid="bib11">Clopath et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Mongillo et al., 2018</xref>; <xref ref-type="bibr" rid="bib54">Susman et al., 2018</xref>).</p><p>Another hypothesis is that any memory degradation due to fluctuations is counteracted by restorative plasticity processes that allow circuits to continually ‘relearn’ stored associations. The information source directing this restorative plasticity could come from an external reinforcement signal (<xref ref-type="bibr" rid="bib23">Kappel et al., 2018</xref>), from interactions with other circuits (<xref ref-type="bibr" rid="bib1">Acker et al., 2018</xref>), or spontaneous, network-level reactivation events (<xref ref-type="bibr" rid="bib13">Fauth and van Rossum, 2019</xref>). A final possibility is that ongoing synaptic fluctuations are accounted for by behavioural changes unrelated to learned task performance .</p><p>All these hypotheses share two core assumptions that we make, and several include a third that our results depend on:</p><list list-type="order"><list-item><p>Not all synaptic changes are related to learning.</p></list-item><list-item><p>Unchecked, these learning-independent plasticity sources generically hasten the degradation of previously stored information within a neural circuit.</p></list-item><list-item><p>Some internal compensatory plasticity mechanism counteracts the degradation of previously stored information.</p></list-item></list><p>We extracted mathematical consequences of these three assumptions by building a general framework. We first modelled the the degree of degradation of previously learned information in terms of an abstract, scalar-valued, task error function or ‘loss landscape’. The brain may not have, and in any case does not require, explicit representation of such a function for a specific task. All that is required is error feedback from the environment and/or some internal prediction.</p><p>We then noted that compensatory plasticity should act to decrease task error, and thus point in a downhill direction on the ‘loss landscape’. We stress that we do not assume a gradient-based learning rule such as the backpropagation algorithm, the plausibility of which is an ongoing debate (<xref ref-type="bibr" rid="bib56">Whittington and Bogacz, 2019</xref>).</p><p>Our results do not depend on whether synaptic changes during learning are gradual, or occur in large, abrupt steps. Although most theory work assumes plasticity to be gradual, there is evidence that plasticity can proceed in discrete jumps. For instance, abrupt potentiation of synaptic inputs that lead to the formation of place fields in mouse CA1 hippocampal neurons can occur within seconds as an animal explores a new environment (<xref ref-type="bibr" rid="bib7">Bittner et al., 2017</xref>). Even classical plasticity paradigms that depend upon millisecond level precision in the relative timing of pre/post synaptic spikes follow a paradigm where there is a short ‘induction phase’ of a minute or so, following which there is a large and sustained change in synaptic efficacy (e.g. <xref ref-type="bibr" rid="bib33">Markram et al., 1997</xref>; <xref ref-type="bibr" rid="bib5">Bi and Poo, 1998</xref>). It is therefore an open question as to whether various forms of synaptic plasticity are best accounted for as an accumulation of small changes or a threshold phenomenon that results in a stepwise change. Our analysis is valid in either case. We quantify plasticity rate by picking a (large or small) time interval over which the net plasticity direction is approximately constant, and evaluate the optimal, steady-state magnitude of compensatory plasticity over this interval, relative to the magnitude of synaptic fluctuations.</p><p>A combination of learning-induced and learning-independent plasticity should lead to an eventual steady state level of task error, at which point the quality of stored information does not decay appreciably over time. The absolute quality of this steady state depends upon both the magnitude of the synaptic fluctuations, and the effectiveness of the compensatory plasticity.</p><p>Our main finding was that the quality of this steady state is optimal when the rate of compensatory plasticity does not outcompete that of the synaptic fluctuations. This result, which is purely mathematical in nature, is far from obvious. While it is intuitively clear that retention of circuit function will suffer when compensatory plasticity is absent or too weak, it is far less intuitive that the same is true generally when compensatory plasticity is too strong.</p><p>We also found that the precision of compensatory plasticity influenced its optimal rate. When ‘precision’ corresponds to the closeness of an approximation to a gradient-based compensatory plasticity rule, an increase in precision resulted in the optimal rate of compensatory plasticity being strictly less than that of fluctuations. In other words, sophisticated learning rules need to do less work to optimally overcome the damage done by learning-independent synaptic fluctuations. Indeed experimental estimates (see <xref ref-type="table" rid="table1">Table 1</xref>) suggest that activity-independent synaptic fluctuations can significantly outcompete systematic, activity-dependent changes in certain experimental contexts. Tentatively, this means that the high degree of synaptic turnover in these systems is in fact evidence for the operation of precise synaptic plasticity mechanisms as opposed to crude and imprecise mechanisms.</p><p>Our results are generic, in that they follow from fundamental mathematical relationships in optimisation theory, and hence are not dependent on particular circuit architectures or plasticity mechanisms. We considered cases in which synaptic fluctuations were distributed across an entire neural circuit. However, the basic framework easily extends, allowing for predictions in more specialised cases. For instance, recent theoretical work (<xref ref-type="bibr" rid="bib11">Clopath et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Mongillo et al., 2018</xref>; <xref ref-type="bibr" rid="bib54">Susman et al., 2018</xref>) have hypothesised that synaptic fluctuations could be restricted to ‘unimportant’ synapses. These correspond to low curvature (globally insensitive) directions in the ‘loss landscape’. Our framework (<xref ref-type="disp-formula" rid="equ16">Equation (9)</xref> in particular) immediately predicts that the optimal rate of compensatory plasticity will decrease proportionately with this curvature.</p><p>Precise experimental isolation/elimination of the plasticity sources attributable to learning and retention of memories remains challenging. Nevertheless, in conventional theories of learning (e.g. Hebbian learning), neural networks learn through plasticity induced by patterns of pre- and postsynaptic neural activity. A reasonable approximation, therefore, is to equate the ‘compensatory/learning-induced’ plasticity of our paper with ‘activity-dependent’ plasticity in experimental setups. With this assumption, our results provide several testable predictions.</p><p>Firstly, our results show that that the rate of compensatory (i.e. learning-dependent) plasticity is greater when a neural circuit is in a phase of active learning, as opposed to maintaining previously learned information (see <xref ref-type="disp-formula" rid="equ18">Equation (10)</xref> and the surrounding discussion). Consequently, the relative contribution of synaptic fluctuations to the overall plasticity rate should be lower in this case. It would be interesting to test whether this were indeed the case, by comparing brain circuits in immature vs mature organisms, and in neural circuits thought to be actively learning vs those thought to be retaining previously learned information. One way to do this would be to measure the covariance of functional synaptic strengths at coinnervated synapses using EM reconstructions of neural tissue. A higher covariance implies a lower proportion of activity-dependent (i.e. compensatory) plasticity, since co-innervated synapses share presynaptic activity histories. Interestingly, two very similar experiments (<xref ref-type="bibr" rid="bib3">Bartol et al., 2015</xref>) and (<xref ref-type="bibr" rid="bib12">Dvorkin and Ziv, 2016</xref>) did indeed examine covariance in EM reconstructions of hippocampus and neocortex, respectively. This covariance appears to be much lower in hippocampus (compare Figure 1 of <xref ref-type="bibr" rid="bib3">Bartol et al., 2015</xref> to Figure 8 of <xref ref-type="bibr" rid="bib12">Dvorkin and Ziv, 2016</xref>). Many cognitive theories characterise hippocampus as a continual learner and neocortex as a consolidator of previously learned information (e.g. <xref ref-type="bibr" rid="bib42">O'Reilly and Rudy, 2001</xref>). Our analysis provides support for this hypothesis at a mechanistic level by linking low covariance in coinnervated hippocampal synapses to continual learning.</p><p>Secondly, a number of experimental studies (<xref ref-type="bibr" rid="bib40">Nagaoka et al., 2016</xref>; <xref ref-type="bibr" rid="bib46">Quinn et al., 2019</xref>; <xref ref-type="bibr" rid="bib58">Yasumatsu et al., 2008</xref>; <xref ref-type="bibr" rid="bib34">Minerbi et al., 2009</xref>; <xref ref-type="bibr" rid="bib12">Dvorkin and Ziv, 2016</xref>) note a persistence of the bulk of synaptic plasticity in the absence of activity-dependent plasticity or other correlates of an explicit learning signal, as explained in our review of key experimental findings. However, there are two important caveats for relating our work to these experimental observations:</p><list list-type="bullet"><list-item><p>Experimentally isolating different plasticity mechanisms, measuring synaptic changes, and accounting for confounding behavioural/physiological changes is extremely challenging. The most compelling in vivo support comes from <xref ref-type="bibr" rid="bib40">Nagaoka et al., 2016</xref>, where an analogue of compensatory plasticity in the mouse visual cortex was suppressed both chemically (by suppression of spiking activity) and behaviourally (by raising the mouse in visually impoverished conditions). Synaptic turnover was reduced by about half for both suppression protocols, and also when they were applied simultaneously. Further studies that quantified changes in synaptic strength in addition to spine turnover in an analogous setup would lend further credence to our results.</p></list-item><list-item><p>We do not know if observed synaptic plasticity in the experiments we cite truly reflect a neural circuit attempting to minimise steady-state error on a particular learning goal (as captured through an abstract, implicit, ‘loss function’). Our analysis simply shows that somewhat surprising levels of ongoing plasticity can be explained parsimoniously in such a framework. In particular, the concepts of ‘learning’ and behaviour have no clear relationship with neural circuit dynamics in vitro. Nevertheless, we might speculate that synapses could tune the extent to which they respond to ‘endogenous’ (task independent) signals versus external signals that could convey task information in the intact animal. Even if the information conveyed by activity-dependent signals were disrupted in vitro, the fact that activity-dependent signals determined such a small proportion of plasticity is notable, and seems to carry over to the in vivo case.</p></list-item></list><p>Thus, while our results offer a surprising agreement with a number of experimental observations, we believe it is important to further replicate measurements of synaptic modification in a variety of settings, both in vivo and in vitro. We hope our analysis provides an impetus for this difficult experimental work by offering a first-principles theory for the volatility of connections in neural circuits.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Simulations</title><p>We simulated two types of network, which we refer to as linear (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) and nonlinear (<xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig5">5</xref>) respectively. We ran our simulations in the Julia programming language (version 1.3), and in particular used the Flux.jl software package (version 0.9) to construct and update networks. Source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Dhruva2/OptimalPlasticityRatios">https://github.com/Dhruva2/OptimalPlasticityRatios</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:99a9eca93055b26e2e47805788fc5c2b62888113;origin=https://github.com/Dhruva2/OptimalPlasticityRatios;visit=swh:1:snp:1ac3958410687f0808a1839d57a8daedb0a4e058;anchor=swh:1:rev:fcb1717a822f90b733c49d62bfc2f970155b7364">swh:1:rev:fcb1717a822f90b733c49d62bfc2f970155b7364</ext-link>, <xref ref-type="bibr" rid="bib48">Raman, 2021</xref>).</p></sec><sec id="s4-2"><title>Nonlinear networks</title><p>Networks were rate-based, with the firing rate <inline-formula><mml:math id="inf167"><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of a given neuron defined as<disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf168"><mml:mi>w</mml:mi></mml:math></inline-formula> is the vector of presynaptic strengths, <inline-formula><mml:math id="inf169"><mml:mi>u</mml:mi></mml:math></inline-formula> represents the firing rate of the associated presynaptic neurons, and <inline-formula><mml:math id="inf170"><mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> is the sigmoid function. Initial weight values were generated randomly, according to the standard Xavier distribution (<xref ref-type="bibr" rid="bib18">Glorot and Bengio, 2010</xref>). Networks were organised into three layers, containing 12, 20, and 10 neurons, respectively. Any given neuron was connected to all neurons in the previous layer. For the first layer, the firing rates of the ‘previous layer’ corresponded to the network inputs.</p></sec><sec id="s4-3"><title>Linear networks</title><p>Networks were organised into an input layer of 12 neurons, and an output layer of 10 neurons. Each output neuron was connected to all input layer neurons. Networks were rate-based, with the firing rate <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of a given neuron defined as<disp-formula id="equ28"><mml:math id="m28"><mml:mrow><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf172"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to the <inline-formula><mml:math id="inf173"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> input (input-layer neuron) or the firing rate of the <inline-formula><mml:math id="inf174"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> input-layer neuron (output-layer neuron). Initial weight values were generated randomly, according to the Xavier distribution (<xref ref-type="bibr" rid="bib18">Glorot and Bengio, 2010</xref>).</p></sec><sec id="s4-4"><title>Task error</title><p>For each network, we generated 1000 different, random, input vectors. Each component of the vector was generated from a unit Gaussian distribution. Task error, at the <inline-formula><mml:math id="inf175"><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> timestep, was taken as the mean squared error of the network in recreating the outputs of the initial (<inline-formula><mml:math id="inf176"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) network, in response to the suite of inputs. Mathematically, this equates to<disp-formula id="equ29"><mml:math id="m29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the output of the network given the synaptic strengths at time <inline-formula><mml:math id="inf178"><mml:mi>t</mml:mi></mml:math></inline-formula>, in response to input <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒰</mml:mi></mml:mrow></mml:math></inline-formula>. Note that this task error recreates the ‘student-teacher’ framework of e.g. (<xref ref-type="bibr" rid="bib28">Levin et al., 1990</xref>; <xref ref-type="bibr" rid="bib52">Seung et al., 1992</xref>), where a fixed copy of the initial network is the teacher.</p></sec><sec id="s4-5"><title>Weight dynamics</title><p>At each simulation timestep, synaptic weights were updated as<disp-formula id="equ30"><mml:math id="m30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We took the synaptic fluctuations term, <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, as scaled white noise, that is,<disp-formula id="equ31"><mml:math id="m31"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>𝕀</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The constant of proportionality was calculated so that the magnitude <inline-formula><mml:math id="inf181"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> conformed to a pre-specified value. This magnitude was 2 in the simulation of <xref ref-type="fig" rid="fig1">Figure 1</xref>, and was a graphed variable in the simulations of <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p><p>The compensatory plasticity term, <inline-formula><mml:math id="inf182"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, was calculated in two stages. First we applied the backpropagation algorithm, using <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the ideal network outputs to train against. This generated an ‘ideal’ direction of compensatory plasticity , proportional to the negative gradient <inline-formula><mml:math id="inf184"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> we then corrupted this gradient with a tunable proportion of white noise. Overall, this gives,<disp-formula id="equ32"><mml:math id="m32"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>𝐜</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf185"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>𝕀</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the noise corruption term, and <inline-formula><mml:math id="inf186"><mml:mrow><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> are tunable hyperparameters. The higher the ratio <inline-formula><mml:math id="inf187"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, the greater the noise corruption. Meanwhile, <inline-formula><mml:math id="inf188"><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:math></inline-formula> sets the overall magnitude of compensatory plasticity . By tuning <inline-formula><mml:math id="inf189"><mml:msub><mml:mi>γ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf190"><mml:msub><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, we can therefore independently modify the magnitude and precision of the compensatory plasticity term. In <xref ref-type="fig" rid="fig1">Figure 1</xref>, we set <inline-formula><mml:math id="inf191"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work was supported by ERC grant StG 2016 716643 FLEXNEURO.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Funding acquisition, Visualization, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-62912-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All code is publicly available on github at this URL: <ext-link ext-link-type="uri" xlink:href="https://github.com/Dhruva2/OptimalPlasticityRatios">https://github.com/Dhruva2/OptimalPlasticityRatios</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:fcb1717a822f90b733c49d62bfc2f970155b7364">https://archive.softwareheritage.org/swh:1:rev:fcb1717a822f90b733c49d62bfc2f970155b7364</ext-link>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Acker</surname> <given-names>D</given-names></name><name><surname>Paradis</surname> <given-names>S</given-names></name><name><surname>Miller</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Stable memory and computation in randomly rewiring neural networks</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/367011</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attardo</surname> <given-names>A</given-names></name><name><surname>Fitzgerald</surname> <given-names>JE</given-names></name><name><surname>Schnitzer</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Impermanence of dendritic spines in live adult CA1 Hippocampus</article-title><source>Nature</source><volume>523</volume><fpage>592</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1038/nature14467</pub-id><pub-id pub-id-type="pmid">26098371</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartol</surname> <given-names>TM</given-names></name><name><surname>Bromer</surname> <given-names>C</given-names></name><name><surname>Kinney</surname> <given-names>J</given-names></name><name><surname>Chirillo</surname> <given-names>MA</given-names></name><name><surname>Bourne</surname> <given-names>JN</given-names></name><name><surname>Harris</surname> <given-names>KM</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Nanoconnectomic upper bound on the variability of synaptic plasticity</article-title><source>eLife</source><volume>4</volume><elocation-id>e10778</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10778</pub-id><pub-id pub-id-type="pmid">26618907</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellec</surname> <given-names>G</given-names></name><name><surname>Scherr</surname> <given-names>F</given-names></name><name><surname>Subramoney</surname> <given-names>A</given-names></name><name><surname>Hajek</surname> <given-names>E</given-names></name><name><surname>Salaj</surname> <given-names>D</given-names></name><name><surname>Legenstein</surname> <given-names>R</given-names></name><name><surname>Maass</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A solution to the learning dilemma for recurrent networks of spiking neurons</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3625</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17236-y</pub-id><pub-id pub-id-type="pmid">32681001</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname> <given-names>GQ</given-names></name><name><surname>Poo</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>10464</fpage><lpage>10472</lpage><pub-id pub-id-type="pmid">9852584</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bienenstock</surname> <given-names>EL</given-names></name><name><surname>Cooper</surname> <given-names>LN</given-names></name><name><surname>Munro</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>2</volume><fpage>32</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.02-01-00032.1982</pub-id><pub-id pub-id-type="pmid">7054394</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittner</surname> <given-names>KC</given-names></name><name><surname>Milstein</surname> <given-names>AD</given-names></name><name><surname>Grienberger</surname> <given-names>C</given-names></name><name><surname>Romani</surname> <given-names>S</given-names></name><name><surname>Magee</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behavioral time scale synaptic plasticity underlies CA1 place fields</article-title><source>Science</source><volume>357</volume><fpage>1033</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.1126/science.aan3846</pub-id><pub-id pub-id-type="pmid">28883072</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouvier</surname> <given-names>G</given-names></name><name><surname>Aljadeff</surname> <given-names>J</given-names></name><name><surname>Clopath</surname> <given-names>C</given-names></name><name><surname>Bimbard</surname> <given-names>C</given-names></name><name><surname>Ranft</surname> <given-names>J</given-names></name><name><surname>Blot</surname> <given-names>A</given-names></name><name><surname>Nadal</surname> <given-names>JP</given-names></name><name><surname>Brunel</surname> <given-names>N</given-names></name><name><surname>Hakim</surname> <given-names>V</given-names></name><name><surname>Barbour</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cerebellar learning using perturbations</article-title><source>eLife</source><volume>7</volume><elocation-id>e31599</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31599</pub-id><pub-id pub-id-type="pmid">30418871</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname> <given-names>MF</given-names></name><name><surname>Jadhav</surname> <given-names>SP</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>147</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1038/nn.2732</pub-id><pub-id pub-id-type="pmid">21270783</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname> <given-names>AR</given-names></name><name><surname>Rumpel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A stable brain from unstable components: emerging concepts and implications for neural computation</article-title><source>Neuroscience</source><volume>357</volume><fpage>172</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2017.06.005</pub-id><pub-id pub-id-type="pmid">28602920</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clopath</surname> <given-names>C</given-names></name><name><surname>Bonhoeffer</surname> <given-names>T</given-names></name><name><surname>Hübener</surname> <given-names>M</given-names></name><name><surname>Rose</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Variance and invariance of neuronal long-term representations</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160161</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0161</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dvorkin</surname> <given-names>R</given-names></name><name><surname>Ziv</surname> <given-names>NE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Relative contributions of specific activity histories and spontaneous processes to size remodeling of glutamatergic synapses</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002572</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002572</pub-id><pub-id pub-id-type="pmid">27776122</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fauth</surname> <given-names>MJ</given-names></name><name><surname>van Rossum</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Self-organized reactivation maintains and reinforces memories despite synaptic turnover</article-title><source>eLife</source><volume>8</volume><elocation-id>e43717</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43717</pub-id><pub-id pub-id-type="pmid">31074745</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fee</surname> <given-names>MS</given-names></name><name><surname>Goldberg</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A hypothesis for basal ganglia-dependent reinforcement learning in the songbird</article-title><source>Neuroscience</source><volume>198</volume><fpage>152</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2011.09.069</pub-id><pub-id pub-id-type="pmid">22015923</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Replay comes of age</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>581</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031538</pub-id><pub-id pub-id-type="pmid">28772098</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fusi</surname> <given-names>S</given-names></name><name><surname>Drew</surname> <given-names>PJ</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Cascade models of synaptically stored memories</article-title><source>Neuron</source><volume>45</volume><fpage>599</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.02.001</pub-id><pub-id pub-id-type="pmid">15721245</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstner</surname> <given-names>W</given-names></name><name><surname>Kempter</surname> <given-names>R</given-names></name><name><surname>van Hemmen</surname> <given-names>JL</given-names></name><name><surname>Wagner</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A neuronal learning rule for sub-millisecond temporal coding</article-title><source>Nature</source><volume>383</volume><fpage>76</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1038/383076a0</pub-id><pub-id pub-id-type="pmid">8779718</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Glorot</surname> <given-names>X</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Understanding the difficulty of training deep feedforward neural networks</article-title><conf-name>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</conf-name><fpage>249</fpage><lpage>256</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerguiev</surname> <given-names>J</given-names></name><name><surname>Lillicrap</surname> <given-names>TP</given-names></name><name><surname>Richards</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Towards deep learning with segregated dendrites</article-title><source>eLife</source><volume>6</volume><elocation-id>e22901</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22901</pub-id><pub-id pub-id-type="pmid">29205151</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name><name><surname>Yagishita</surname> <given-names>S</given-names></name><name><surname>Nakamura</surname> <given-names>M</given-names></name><name><surname>Shirai</surname> <given-names>F</given-names></name><name><surname>Wu</surname> <given-names>YI</given-names></name><name><surname>Loshbaugh</surname> <given-names>AL</given-names></name><name><surname>Kuhlman</surname> <given-names>B</given-names></name><name><surname>Hahn</surname> <given-names>KM</given-names></name><name><surname>Kasai</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Labelling and optical erasure of synaptic memory traces in the motor cortex</article-title><source>Nature</source><volume>525</volume><fpage>333</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1038/nature15257</pub-id><pub-id pub-id-type="pmid">26352471</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hebb</surname> <given-names>DO</given-names></name></person-group><year iso-8601-date="1949">1949</year><source>The Organization of Behavior: A Neuropsychological Theory</source><publisher-loc>Wiley</publisher-loc><publisher-name>Chapman &amp; Hall</publisher-name></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holtmaat</surname> <given-names>AJ</given-names></name><name><surname>Trachtenberg</surname> <given-names>JT</given-names></name><name><surname>Wilbrecht</surname> <given-names>L</given-names></name><name><surname>Shepherd</surname> <given-names>GM</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Knott</surname> <given-names>GW</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Transient and persistent dendritic spines in the neocortex in vivo</article-title><source>Neuron</source><volume>45</volume><fpage>279</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.01.003</pub-id><pub-id pub-id-type="pmid">15664179</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kappel</surname> <given-names>D</given-names></name><name><surname>Legenstein</surname> <given-names>R</given-names></name><name><surname>Habenschuss</surname> <given-names>S</given-names></name><name><surname>Hsieh</surname> <given-names>M</given-names></name><name><surname>Maass</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A dynamic connectome supports the emergence of stable computational function of neural circuits through Reward-Based learning</article-title><source>Eneuro</source><volume>5</volume><elocation-id>ENEURO.0301-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0301-17.2018</pub-id><pub-id pub-id-type="pmid">29696150</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kasai</surname> <given-names>H</given-names></name><name><surname>Matsuzaki</surname> <given-names>M</given-names></name><name><surname>Noguchi</surname> <given-names>J</given-names></name><name><surname>Yasumatsu</surname> <given-names>N</given-names></name><name><surname>Nakahara</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Structure-stability-function relationships of dendritic spines</article-title><source>Trends in Neurosciences</source><volume>26</volume><fpage>360</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(03)00162-0</pub-id><pub-id pub-id-type="pmid">12850432</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kasthuri</surname> <given-names>N</given-names></name><name><surname>Hayworth</surname> <given-names>KJ</given-names></name><name><surname>Berger</surname> <given-names>DR</given-names></name><name><surname>Schalek</surname> <given-names>RL</given-names></name><name><surname>Conchello</surname> <given-names>JA</given-names></name><name><surname>Knowles-Barley</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Vázquez-Reina</surname> <given-names>A</given-names></name><name><surname>Kaynig</surname> <given-names>V</given-names></name><name><surname>Jones</surname> <given-names>TR</given-names></name><name><surname>Roberts</surname> <given-names>M</given-names></name><name><surname>Morgan</surname> <given-names>JL</given-names></name><name><surname>Tapia</surname> <given-names>JC</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name><name><surname>Roncal</surname> <given-names>WG</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Burns</surname> <given-names>R</given-names></name><name><surname>Sussman</surname> <given-names>DL</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name><name><surname>Pfister</surname> <given-names>H</given-names></name><name><surname>Lichtman</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Saturated reconstruction of a volume of neocortex</article-title><source>Cell</source><volume>162</volume><fpage>648</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.06.054</pub-id><pub-id pub-id-type="pmid">26232230</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kornfeld</surname> <given-names>J</given-names></name><name><surname>Januszewski</surname> <given-names>M</given-names></name><name><surname>Schubert</surname> <given-names>P</given-names></name><name><surname>Jain</surname> <given-names>V</given-names></name><name><surname>Denk</surname> <given-names>W</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An anatomical substrate of credit assignment in reinforcement learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.02.18.954354</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname> <given-names>CS</given-names></name><name><surname>Franke</surname> <given-names>TF</given-names></name><name><surname>Gan</surname> <given-names>WB</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Opposite effects of fear conditioning and extinction on dendritic spine remodelling</article-title><source>Nature</source><volume>483</volume><fpage>87</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1038/nature10792</pub-id><pub-id pub-id-type="pmid">22343895</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levin</surname> <given-names>E</given-names></name><name><surname>Tishby</surname> <given-names>N</given-names></name><name><surname>Solla</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>A statistical approach to learning and generalization in layered neural networks</article-title><source>Proceedings of the IEEE</source><volume>78</volume><fpage>1568</fpage><lpage>1574</lpage><pub-id pub-id-type="doi">10.1109/5.58339</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname> <given-names>TP</given-names></name><name><surname>Cownden</surname> <given-names>D</given-names></name><name><surname>Tweed</surname> <given-names>DB</given-names></name><name><surname>Akerman</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13276</pub-id><pub-id pub-id-type="pmid">27824044</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname> <given-names>TP</given-names></name><name><surname>Santoro</surname> <given-names>A</given-names></name><name><surname>Marris</surname> <given-names>L</given-names></name><name><surname>Akerman</surname> <given-names>CJ</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Backpropagation and the brain</article-title><source>Nature Reviews Neuroscience</source><volume>21</volume><fpage>335</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0277-3</pub-id><pub-id pub-id-type="pmid">32303713</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname> <given-names>Y</given-names></name><name><surname>Kuras</surname> <given-names>A</given-names></name><name><surname>Rumpel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>9481</fpage><lpage>9488</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6130-10.2011</pub-id><pub-id pub-id-type="pmid">21715613</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loewenstein</surname> <given-names>Y</given-names></name><name><surname>Yanover</surname> <given-names>U</given-names></name><name><surname>Rumpel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predicting the dynamics of network connectivity in the neocortex</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>12535</fpage><lpage>12544</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2917-14.2015</pub-id><pub-id pub-id-type="pmid">26354919</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname> <given-names>H</given-names></name><name><surname>Lübke</surname> <given-names>J</given-names></name><name><surname>Frotscher</surname> <given-names>M</given-names></name><name><surname>Sakmann</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title><source>Science</source><volume>275</volume><fpage>213</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1126/science.275.5297.213</pub-id><pub-id pub-id-type="pmid">8985014</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minerbi</surname> <given-names>A</given-names></name><name><surname>Kahana</surname> <given-names>R</given-names></name><name><surname>Goldfeld</surname> <given-names>L</given-names></name><name><surname>Kaufman</surname> <given-names>M</given-names></name><name><surname>Marom</surname> <given-names>S</given-names></name><name><surname>Ziv</surname> <given-names>NE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Long-term relationships between synaptic tenacity, synaptic remodeling, and network activity</article-title><source>PLOS Biology</source><volume>7</volume><elocation-id>e1000136</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000136</pub-id><pub-id pub-id-type="pmid">19554080</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moczulska</surname> <given-names>KE</given-names></name><name><surname>Tinter-Thiede</surname> <given-names>J</given-names></name><name><surname>Peter</surname> <given-names>M</given-names></name><name><surname>Ushakova</surname> <given-names>L</given-names></name><name><surname>Wernle</surname> <given-names>T</given-names></name><name><surname>Bathellier</surname> <given-names>B</given-names></name><name><surname>Rumpel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamics of dendritic spines in the mouse auditory cortex during memory formation and memory recall</article-title><source>PNAS</source><volume>110</volume><fpage>18315</fpage><lpage>18320</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312508110</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname> <given-names>G</given-names></name><name><surname>Rumpel</surname> <given-names>S</given-names></name><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Intrinsic volatility of synaptic connections - a challenge to the synaptic trace theory of memory</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>7</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.06.006</pub-id><pub-id pub-id-type="pmid">28710971</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mongillo</surname> <given-names>G</given-names></name><name><surname>Rumpel</surname> <given-names>S</given-names></name><name><surname>Loewenstein</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inhibitory connectivity defines the realm of excitatory plasticity</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1463</fpage><lpage>1470</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0226-x</pub-id><pub-id pub-id-type="pmid">30224809</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Local online learning in recurrent networks with random feedback</article-title><source>eLife</source><volume>8</volume><elocation-id>e43299</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43299</pub-id><pub-id pub-id-type="pmid">31124785</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nader</surname> <given-names>K</given-names></name><name><surname>Einarsson</surname> <given-names>EO</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Memory reconsolidation: an update</article-title><source>Annals of the New York Academy of Sciences</source><volume>1191</volume><fpage>27</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2010.05443.x</pub-id><pub-id pub-id-type="pmid">20392274</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagaoka</surname> <given-names>A</given-names></name><name><surname>Takehara</surname> <given-names>H</given-names></name><name><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name><name><surname>Noguchi</surname> <given-names>J</given-names></name><name><surname>Ishii</surname> <given-names>K</given-names></name><name><surname>Shirai</surname> <given-names>F</given-names></name><name><surname>Yagishita</surname> <given-names>S</given-names></name><name><surname>Akagi</surname> <given-names>T</given-names></name><name><surname>Ichiki</surname> <given-names>T</given-names></name><name><surname>Kasai</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Abnormal intrinsic dynamics of dendritic spines in a fragile X syndrome mouse model <italic>in vivo</italic></article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>26651</elocation-id><pub-id pub-id-type="doi">10.1038/srep26651</pub-id><pub-id pub-id-type="pmid">27221801</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Wyllie</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neuronal homeostasis: time for a change?</article-title><source>The Journal of Physiology</source><volume>589</volume><fpage>4811</fpage><lpage>4826</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2011.210179</pub-id><pub-id pub-id-type="pmid">21825033</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Reilly</surname> <given-names>RC</given-names></name><name><surname>Rudy</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Conjunctive representations in learning and memory: principles of cortical and hippocampal function</article-title><source>Psychological Review</source><volume>108</volume><fpage>311</fpage><lpage>345</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.108.2.311</pub-id><pub-id pub-id-type="pmid">11381832</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Leary</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Homeostasis, failure of homeostasis and degenerate ion channel regulation</article-title><source>Current Opinion in Physiology</source><volume>2</volume><fpage>129</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/j.cophys.2018.01.006</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname> <given-names>T</given-names></name><name><surname>Poll</surname> <given-names>S</given-names></name><name><surname>Bancelin</surname> <given-names>S</given-names></name><name><surname>Angibaud</surname> <given-names>J</given-names></name><name><surname>Inavalli</surname> <given-names>VK</given-names></name><name><surname>Keppler</surname> <given-names>K</given-names></name><name><surname>Mittag</surname> <given-names>M</given-names></name><name><surname>Fuhrmann</surname> <given-names>M</given-names></name><name><surname>Nägerl</surname> <given-names>UV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Chronic 2P-STED imaging reveals high turnover of dendritic spines in the Hippocampus in vivo</article-title><source>eLife</source><volume>7</volume><elocation-id>e34700</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.34700</pub-id><pub-id pub-id-type="pmid">29932052</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Polyak</surname> <given-names>BT</given-names></name></person-group><year iso-8601-date="1987">1987</year><source>Introduction to Optimization</source><publisher-loc>New York</publisher-loc><publisher-name>Optimization Software, Publications Division</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname> <given-names>DP</given-names></name><name><surname>Kolar</surname> <given-names>A</given-names></name><name><surname>Harris</surname> <given-names>SA</given-names></name><name><surname>Wigerius</surname> <given-names>M</given-names></name><name><surname>Fawcett</surname> <given-names>JP</given-names></name><name><surname>Krueger</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The stability of glutamatergic synapses is independent of activity level, but predicted by synapse size</article-title><source>Frontiers in Cellular Neuroscience</source><volume>13</volume><elocation-id>291</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2019.00291</pub-id><pub-id pub-id-type="pmid">31316356</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raman</surname> <given-names>DV</given-names></name><name><surname>Rotondo</surname> <given-names>AP</given-names></name><name><surname>O'Leary</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fundamental bounds on learning performance in neural circuits</article-title><source>PNAS</source><volume>116</volume><fpage>10537</fpage><lpage>10546</lpage><pub-id pub-id-type="doi">10.1073/pnas.1813416116</pub-id><pub-id pub-id-type="pmid">31061133</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Raman</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>OptimalPlasticityRatios</data-title><source>Software Heritage</source><version designator="swh:1:rev:fcb1717a822f90b733c49d62bfc2f970155b7364">swh:1:rev:fcb1717a822f90b733c49d62bfc2f970155b7364</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:99a9eca93055b26e2e47805788fc5c2b62888113;origin=https://github.com/Dhruva2/OptimalPlasticityRatios;visit=swh:1:snp:1ac3958410687f0808a1839d57a8daedb0a4e058;anchor=swh:1:rev:fcb1717a822f90b733c49d62bfc2f970155b7364">https://archive.softwareheritage.org/swh:1:dir:99a9eca93055b26e2e47805788fc5c2b62888113;origin=https://github.com/Dhruva2/OptimalPlasticityRatios;visit=swh:1:snp:1ac3958410687f0808a1839d57a8daedb0a4e058;anchor=swh:1:rev:fcb1717a822f90b733c49d62bfc2f970155b7364</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raman</surname> <given-names>DV</given-names></name><name><surname>O'Leary</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Frozen algorithms: how the brain's wiring facilitates learning</article-title><source>Current Opinion in Neurobiology</source><volume>67</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2020.12.017</pub-id><pub-id pub-id-type="pmid">33508698</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rule</surname> <given-names>ME</given-names></name><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causes and consequences of representational drift</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>141</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.08.005</pub-id><pub-id pub-id-type="pmid">31569062</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rule</surname> <given-names>ME</given-names></name><name><surname>Loback</surname> <given-names>AR</given-names></name><name><surname>Raman</surname> <given-names>DV</given-names></name><name><surname>Driscoll</surname> <given-names>LN</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>O'Leary</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Stable task information from an unstable neural population</article-title><source>eLife</source><volume>9</volume><elocation-id>e51121</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.51121</pub-id><pub-id pub-id-type="pmid">32660692</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname> <given-names>HS</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Tishby</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Statistical mechanics of learning from examples</article-title><source>Physical Review A</source><volume>45</volume><fpage>6056</fpage><lpage>6091</lpage><pub-id pub-id-type="doi">10.1103/PhysRevA.45.6056</pub-id><pub-id pub-id-type="pmid">9907706</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission</article-title><source>Neuron</source><volume>40</volume><fpage>1063</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(03)00761-X</pub-id><pub-id pub-id-type="pmid">14687542</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Susman</surname> <given-names>L</given-names></name><name><surname>Brenner</surname> <given-names>N</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Stable memory with unstable synapses</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1808.00756">https://arxiv.org/abs/1808.00756</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tronson</surname> <given-names>NC</given-names></name><name><surname>Taylor</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Molecular mechanisms of memory reconsolidation</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>262</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1038/nrn2090</pub-id><pub-id pub-id-type="pmid">17342174</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname> <given-names>JCR</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Theories of error Back-Propagation in the brain</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>235</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.12.005</pub-id><pub-id pub-id-type="pmid">30704969</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title><source>Machine Learning</source><volume>8</volume><fpage>229</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1007/BF00992696</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yasumatsu</surname> <given-names>N</given-names></name><name><surname>Matsuzaki</surname> <given-names>M</given-names></name><name><surname>Miyazaki</surname> <given-names>T</given-names></name><name><surname>Noguchi</surname> <given-names>J</given-names></name><name><surname>Kasai</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Principles of long-term dynamics of dendritic spines</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>13592</fpage><lpage>13608</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0603-08.2008</pub-id><pub-id pub-id-type="pmid">19074033</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziv</surname> <given-names>NE</given-names></name><name><surname>Brenner</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synaptic tenacity or lack thereof: spontaneous remodeling of synapses</article-title><source>Trends in Neurosciences</source><volume>41</volume><fpage>89</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2017.12.003</pub-id><pub-id pub-id-type="pmid">29275902</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuo</surname> <given-names>Y</given-names></name><name><surname>Lin</surname> <given-names>A</given-names></name><name><surname>Chang</surname> <given-names>P</given-names></name><name><surname>Gan</surname> <given-names>WB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Development of long-term dendritic spine stability in diverse regions of cerebral cortex</article-title><source>Neuron</source><volume>46</volume><fpage>181</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.04.001</pub-id><pub-id pub-id-type="pmid">15848798</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Alternative derivation of <xref ref-type="disp-formula" rid="equ16">Equation (9a)</xref></title><p>We provide an alternative derivation of <xref ref-type="disp-formula" rid="equ16">Equation (9a)</xref> that removes the need for assumption (3b). We did not put this main derivation in the main text as we perceive it to have less clarity.</p><p>The derivation proceeds identically to that given in the main text until <xref ref-type="disp-formula" rid="equ70">Equation (7)</xref>. We can then use (3a) to simplify <xref ref-type="disp-formula" rid="equ70">Equation (7)</xref>. We get<disp-formula id="equ33"><mml:math id="m33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ34"><mml:math id="m34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Recall that expectation is taken over an unknown probability distribution from which <inline-formula><mml:math id="inf192"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> is drawn, which satisfies <xref ref-type="disp-formula" rid="equ4">Equation (3a)</xref>.</p><p>We then assume that we are in a phase of stable memory retention, so that <inline-formula><mml:math id="inf193"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Now if the magnitude of compensatory plasticity <inline-formula><mml:math id="inf194"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is tuned to minimise steady state error <inline-formula><mml:math id="inf195"><mml:mi>F</mml:mi></mml:math></inline-formula>, then any change to <inline-formula><mml:math id="inf196"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> will result in an increase in <inline-formula><mml:math id="inf197"><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. So <inline-formula><mml:math id="inf198"><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is locally minimal in <inline-formula><mml:math id="inf199"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. This implies<disp-formula id="equ35"><mml:math id="m35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We also claim that local minimality implies<disp-formula id="equ36"><label>(1)</label><mml:math id="m36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Why? <inline-formula><mml:math id="inf200"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> implies that <inline-formula><mml:math id="inf201"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mfrac><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. If a small change to <inline-formula><mml:math id="inf202"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> results in <inline-formula><mml:math id="inf203"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, then it also results in <inline-formula><mml:math id="inf204"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mfrac><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, since <inline-formula><mml:math id="inf205"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is non-negative.</p><p>Expanding the LHS of <xref ref-type="disp-formula" rid="equ36">Equation (1)</xref>, we get<disp-formula id="equ37"><mml:math id="m37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">{</mml:mo></mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="2em"/><mml:mspace width="2em"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Differentiating, we get<disp-formula id="equ38"><mml:math id="m38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ39"><mml:math id="m39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">⇒</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>from which (9a) follows.</p></sec><sec id="s9" sec-type="appendix"><title>Positivity of the numerator and denominator in <xref ref-type="disp-formula" rid="equ16">Equation (9a)</xref></title><p><xref ref-type="disp-formula" rid="equ16">Equation (9a)</xref> of the main text asserts that<disp-formula id="equ40"><mml:math id="m40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>holds as long as both the numerator and denominator of the RHS are positive. Here we describe sufficient conditions for positivity.</p><p>The inequality <inline-formula><mml:math id="inf206"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>⪰</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> must hold in some neighbourhood of any minimum <inline-formula><mml:math id="inf207"><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>. Recall that we referred to such a neighbourhood as a highly trained state of the network in the main text. In such a state, our assertion follows immediately, as <inline-formula><mml:math id="inf208"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐯</mml:mi></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, for any vector <inline-formula><mml:math id="inf209"><mml:mi mathvariant="bold">𝐯</mml:mi></mml:math></inline-formula>. Therefore, <inline-formula><mml:math id="inf210"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf211"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>We now consider a partially trained network state, which we defined in the main text as any <inline-formula><mml:math id="inf212"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> satisfying <inline-formula><mml:math id="inf213"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Note that<disp-formula id="equ41"><mml:math id="m41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We assumed in the main text (<xref ref-type="disp-formula" rid="equ4">Equation (3a)</xref>), that <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> is uncorrelated with the gradient <inline-formula><mml:math id="inf215"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in expectation, since <inline-formula><mml:math id="inf216"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> is realised by memory-independent processes. Similarly we can assume that <inline-formula><mml:math id="inf217"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> is unbiased in how it projects onto the eigenvectors of <inline-formula><mml:math id="inf218"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In other words,<disp-formula id="equ42"><mml:math id="m42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>for any normalised eigenvectors <inline-formula><mml:math id="inf219"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf220"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> of <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. In expectation, we can therefore simplify to<disp-formula id="equ43"><mml:math id="m43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">[</mml:mo></mml:mrow><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ44"><mml:math id="m44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the dimensionality of the vector <inline-formula><mml:math id="inf223"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula>. So a partially trained network is one for which small, memory-independent weight fluctuations (such as <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>, or white noise) are expected to decrease task performance.</p><p>Now recall that <inline-formula><mml:math id="inf225"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. So we have<disp-formula id="equ45"><mml:math id="m45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where the positivity constraint comes from being in a partially trained network.</p><p>We now consider why <inline-formula><mml:math id="inf226"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> should be generically positive in a partially trained network. Suppose <inline-formula><mml:math id="inf227"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> holds. We can rewrite this as <inline-formula><mml:math id="inf228"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐜</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. In this case, maintaining the same compensatory plasticity <inline-formula><mml:math id="inf229"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> over the time interval <inline-formula><mml:math id="inf230"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> would result in increased improvement in loss, as<disp-formula id="equ46"><mml:math id="m46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒪</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Effectively, memory improvement due to compensatory plasticity <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> would be in an ‘accelerating’ direction, and maintaining the same direction <inline-formula><mml:math id="inf232"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> of compensatory plasticity would lead to ever faster learning. However, by assumption, we are in a regime of steady state task performance, where<disp-formula id="equ47"><mml:math id="m47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="s10" sec-type="appendix"><title>Optimal plasticity ratios in specific learning rules</title><sec id="s10-1"><title>Noise-free learning rules (first-order)</title><p>Let us first consider the case where <inline-formula><mml:math id="inf233"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> can be computed with perfect access to the gradient <inline-formula><mml:math id="inf234"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but without access to <inline-formula><mml:math id="inf235"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Such a <inline-formula><mml:math id="inf236"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> is known as a first-order learning rule, as it has access only to the first derivative of <inline-formula><mml:math id="inf237"><mml:mi>F</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib45">Polyak, 1987</xref>). Imperfect access is considered subsequently. In this case, the optimal direction of compensatory plasticity is<disp-formula id="equ48"><mml:math id="m48"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>𝐜</mml:mi></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In other words, <inline-formula><mml:math id="inf238"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> would implement perfect gradient descent on <inline-formula><mml:math id="inf239"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The condition of <xref ref-type="disp-formula" rid="equ21">Equation (11)</xref> for synaptic fluctuations to outcompete reconsolidation plasticity evaluates to<disp-formula id="equ49"><mml:math id="m49"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To what extent can we quantify <inline-formula><mml:math id="inf240"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>? First let us relate the gradient and Hessian of <inline-formula><mml:math id="inf241"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf242"><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> be an optimal state of the network (i.e. one where <inline-formula><mml:math id="inf243"><mml:mi>F</mml:mi></mml:math></inline-formula> is minimised). Let us parameterise the straight line connecting <inline-formula><mml:math id="inf244"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf245"><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>:<disp-formula id="equ50"><mml:math id="m50"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>𝐰</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="12.5pt">,</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Then<disp-formula id="equ51"><mml:math id="m51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mtext> where </mml:mtext></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ52"><mml:math id="m52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This gives<disp-formula id="equ53"><mml:math id="m53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>First let us rewrite<disp-formula id="equ54"><mml:math id="m54"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ55"><mml:math id="m55"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi>𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf246"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="inf247"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> eigenvalue/eigenvector pair of <inline-formula><mml:math id="inf248"><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> (sorted in ascending order of <inline-formula><mml:math id="inf249"><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>), and <italic>c</italic><sub><italic>i</italic></sub>, <italic>d</italic><sub><italic>i</italic></sub> are some scalar weights. Now<disp-formula id="equ56"><label>(2)</label><mml:math id="m56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The value of <inline-formula><mml:math id="inf250"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> now depends upon the distribution of mass of the sequence <inline-formula><mml:math id="inf251"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. If later elements of the sequence are larger (i.e. <inline-formula><mml:math id="inf252"><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> projects more highly onto eigenvectors of <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with large eigenvalue), then <inline-formula><mml:math id="inf254"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> becomes larger, and the optimal magnitude of reconsolidation plasticity decreases, relative to the magnitude of synaptic fluctuations. The opposite is true if earlier elements of the sequence are larger.</p><p>Guaranteed bounds on the value of <xref ref-type="disp-formula" rid="equ56">Equation (2)</xref> are vacuous. If we do not restrict <inline-formula><mml:math id="inf255"><mml:mi>M</mml:mi></mml:math></inline-formula>, then we can tailor the sequence <inline-formula><mml:math id="inf256"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> as we like, and we end up with <inline-formula><mml:math id="inf257"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. However, pragmatic bounds are much tighter. Let us now consider two plausibly extremal cases.</p><p>First consider the simplest case of a network that linearly transforms its outputs, and which has a quadratic loss function <inline-formula><mml:math id="inf258"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In this case <inline-formula><mml:math id="inf259"><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> is a constant, (independent of <inline-formula><mml:math id="inf260"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula>), positive-semidefinite matrix, and <inline-formula><mml:math id="inf261"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This means that<disp-formula id="equ57"><mml:math id="m57"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ58"><mml:math id="m58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Condition (11) then becomes<disp-formula id="equ59"><label>(3)</label><mml:math id="m59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mspace width="1em"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">⇔</mml:mo><mml:mspace width="1em"/><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>A conservative sufficient condition for (<xref ref-type="disp-formula" rid="equ59">Equation 3</xref>), using Chebyshev’s summation inequality, is that<disp-formula id="equ60"><label>(4)</label><mml:math id="m60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>≥</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mtext> for all </mml:mtext><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Under what conditions would a plausible reconsolidation mechanism choose to ‘outcompete’ synaptic fluctuations, in this linear example? For <inline-formula><mml:math id="inf262"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to even hold, (<bold>26</bold>) would have to be broken, and significantly so due to conservatism in the inequality. In other words, <inline-formula><mml:math id="inf263"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> must project quite biasedly onto the eigenvectors of <inline-formula><mml:math id="inf264"><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> with smaller-than-average eigenvalue. If the discrepancy between <inline-formula><mml:math id="inf265"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf266"><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> were caused by fluctuations (which are independent of <inline-formula><mml:math id="inf267"><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula>), then this would not be the case, in expectation. Even if this were the case, the reconsolidation mechanism would have to know about the described bias. This requires knowledge of both <inline-formula><mml:math id="inf268"><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf269"><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula>, and is thus implausible.</p><p>Now let us consider the case of a generic nonlinear network. At one extreme, if <inline-formula><mml:math id="inf270"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is small, then <inline-formula><mml:math id="inf271"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and the discussion of the linear case is valid. This corresponds to the case where steady state error is close to the minimum achievable by the network. As <inline-formula><mml:math id="inf272"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> increases (i.e. steady state error gets worse), the correspondence between <inline-formula><mml:math id="inf273"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf274"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> will likely decrease. Thus the optimal magnitude of reconsolidation plasticity, relative to the level of synaptic fluctuations, will rise.</p><p>We could consider another ‘extreme’ case in which <inline-formula><mml:math id="inf275"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf276"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were completely independent of each other. In this case,<disp-formula id="equ61"><label>(5)</label><mml:math id="m61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In other words, the projection of <inline-formula><mml:math id="inf277"><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> onto the different eigenvectors of <inline-formula><mml:math id="inf278"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is approximately even. Using (24), this gives<disp-formula id="equ62"><mml:math id="m62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In summary, we have two plausible extremes. One occurs where <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and another occurs where <inline-formula><mml:math id="inf280"><mml:mi>M</mml:mi></mml:math></inline-formula> is completely independent of <inline-formula><mml:math id="inf281"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In either case, <inline-formula><mml:math id="inf282"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and so the magnitude of synaptic fluctuations should optimally outcompete/equal the magnitude of reconsolidation plasticity. Of course, there might be particular values of <inline-formula><mml:math id="inf283"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> where the correspondence between <inline-formula><mml:math id="inf284"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf285"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is ‘worse’ than chance. In other words, eigenvectors of <inline-formula><mml:math id="inf286"><mml:mi>M</mml:mi></mml:math></inline-formula> with large eigenvalue preferentially project onto eigenvectors of <inline-formula><mml:math id="inf287"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with small eigenvalue. In such cases, we would have <inline-formula><mml:math id="inf288"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≤</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. However, we find it implausible that a reconsolidation mechanism would be able to gain sufficient information on <inline-formula><mml:math id="inf289"><mml:mi>M</mml:mi></mml:math></inline-formula> to determine this at particular points in time, and thereby increase its plasticity magnitude.</p></sec><sec id="s10-2"><title>Noise-free learning rules (second-order)</title><p>Let us now suppose that <inline-formula><mml:math id="inf290"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> can be computed with perfect access to both <inline-formula><mml:math id="inf291"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf292"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In this case, the reconsolidation mechanism would optimally apply plasticity in the direction of the Newton step: we would have<disp-formula id="equ63"><mml:math id="m63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that the Newton step is often conceptualised as a weighted form of gradient descent, where movement on the loss landscape is biased towards direction of lower curvature. Thus we would expect <inline-formula><mml:math id="inf293"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be smaller, and the optimal proportion of reconsolidation plasticity to be larger. This is indeed the case. For mathematical tractability, we will restrict our discussion to the case in which <inline-formula><mml:math id="inf294"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≻</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf295"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≻</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. This would hold if <inline-formula><mml:math id="inf296"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were convex, or if <inline-formula><mml:math id="inf297"><mml:mi mathvariant="bold">𝐰</mml:mi></mml:math></inline-formula> were sufficiently close to a unique local minimum <inline-formula><mml:math id="inf298"><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>. In this case we can rewrite<disp-formula id="equ64"><mml:math id="m64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>which gives<disp-formula id="equ65"><mml:math id="m65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true" rowspacing="3pt"><mml:mlabeledtr><mml:mtd><mml:mtext>(6a)</mml:mtext></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(6b)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr><mml:mlabeledtr><mml:mtd><mml:mtext>(6c)</mml:mtext></mml:mtd><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mtd><mml:mtd/></mml:mlabeledtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Once again, we first consider the case of a linear network with quadratic loss function, and hence with constant Hessian <inline-formula><mml:math id="inf299"><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula>. This gives <inline-formula><mml:math id="inf300"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and<disp-formula id="equ66"><mml:math id="m66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ67"><mml:math id="m67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We again assume that the reconsolidation mechanism does not have knowledge of the relative projections of <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> onto the different eigenvectors of <inline-formula><mml:math id="inf302"><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula>, which requires knowledge of <inline-formula><mml:math id="inf303"><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>. Without such information, we can use an analogous argument to that preceding (<xref ref-type="disp-formula" rid="equ61">Equation 5</xref>) to argue that the approximation <inline-formula><mml:math id="inf304"><mml:mrow><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>≈</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is reasonable. This gives <inline-formula><mml:math id="inf305"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Note that the Newton step, in the linear-quadratic case just considered, corresponds to a direction <inline-formula><mml:math id="inf306"><mml:mrow><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>-</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi></mml:mrow></mml:math></inline-formula>, that is, a direct path to a local minimum. So we could consider a compensatory plasticity mechanism implementing the Newton step as one directly undoing synaptic changes caused by <inline-formula><mml:math id="inf307"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>We now consider the case of a nonlinear network. As before, if <inline-formula><mml:math id="inf308"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is small, then we have <inline-formula><mml:math id="inf309"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and the arguments of the linear network hold. As <inline-formula><mml:math id="inf310"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> increases, the correspondence between <inline-formula><mml:math id="inf311"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf312"><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula> will decrease. We again consider the plausible extreme where <inline-formula><mml:math id="inf313"><mml:mi>M</mml:mi></mml:math></inline-formula> is completely uncorrelated with <inline-formula><mml:math id="inf314"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and so the approximation (<xref ref-type="disp-formula" rid="equ61">Equation 5</xref>) holds. In this case, <xref ref-type="disp-formula" rid="equ12">Equation (6)</xref> can be simplified to give<disp-formula id="equ68"><mml:math id="m68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We assumed that <inline-formula><mml:math id="inf315"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≻</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Therefore, all eigenvalues are positive. This allows us to use Chebyshev’s summation inequality to arrive at<disp-formula id="equ69"><mml:math id="m69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>So as <inline-formula><mml:math id="inf316"><mml:msub><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>-</mml:mo><mml:msup><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> increases, the magnitude of reconsolidation plasticity will optimally outcompete that of synaptic fluctuations. This is the one case that contradicts our main claim.</p></sec><sec id="s10-3"><title>Imperfect learning rules</title><p>The previous section applied in the implausible case where a reconsolidation mechanism had perfect access to <inline-formula><mml:math id="inf317"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and/or <inline-formula><mml:math id="inf318"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Recall from the main text that at least some information on <inline-formula><mml:math id="inf319"><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is required, in order for compensatory plasticity to move in a direction of decreasing task error. What if <inline-formula><mml:math id="inf320"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow></mml:math></inline-formula> contains a mean-zero noise term, corresponding to unbiased noise corruption of these quantities? We will now show how such noise pushes <inline-formula><mml:math id="inf321"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> towards equality with <inline-formula><mml:math id="inf322"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and thus pushes the optimal magnitude of reconsolidation plasticity towards the magnitude of synaptic fluctuations. Let us use the model<disp-formula id="equ70"><label>(7)</label><mml:math id="m70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>ν</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf323"><mml:mi>ν</mml:mi></mml:math></inline-formula> is some mean-zero random variable, and <inline-formula><mml:math id="inf324"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> is the ideal output of the reconsolidation mechanism, assuming perfect access to the derivatives of <inline-formula><mml:math id="inf325"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Here <inline-formula><mml:math id="inf326"><mml:mi>ν</mml:mi></mml:math></inline-formula> represents the portion of compensatory plasticity attributable to systematic error in the algorithm, due to imperfect information on <inline-formula><mml:math id="inf327"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This could arise due to imperfect sensory information or limited communication between synapses. We can therefore assume, as for <inline-formula><mml:math id="inf328"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>, that it does not contain information on <inline-formula><mml:math id="inf329"><mml:mrow><mml:mrow><mml:msup><mml:mo>∇</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi>F</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">𝐰</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We therefore get<disp-formula id="equ71"><mml:math id="m71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>analogously to <xref ref-type="disp-formula" rid="equ22">Equation (12)</xref>. Now the operator <inline-formula><mml:math id="inf330"><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub></mml:math></inline-formula> satisfies<disp-formula id="equ72"><label>(8)</label><mml:math id="m72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>ν</mml:mi><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>ν</mml:mi><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>So depending upon the relative magnitudes of <inline-formula><mml:math id="inf331"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="inf332"><mml:mi>ν</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf333"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> interpolates between <inline-formula><mml:math id="inf334"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf335"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In particular, as the crudeness of the learning rule (i.e. the ratio <inline-formula><mml:math id="inf336"><mml:mfrac><mml:mrow><mml:mo>∥</mml:mo><mml:mi>ν</mml:mi><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>∥</mml:mo></mml:mrow></mml:mfrac></mml:math></inline-formula> ) grows, <inline-formula><mml:math id="inf337"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐜</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> approaches equality (from below) with <inline-formula><mml:math id="inf338"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and thus <inline-formula><mml:math id="inf339"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="bold">𝐰</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, completing our argument.</p></sec></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62912.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role>Reviewing Editor</role><aff><institution>Ecole Normale Superieure Paris</institution><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Loewenstein</surname><given-names>Yonatan</given-names> </name><role>Reviewer</role><aff><institution>Hebrew University of Jerusalem</institution><country>Israel</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Hennig</surname><given-names>Matthias H</given-names></name><role>Reviewer</role><aff><institution>University of Edinburgh</institution><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The half-century research on synaptic plasticity has primarily focused on how neural activity gives rise to changes in synaptic connections, and how these changes underlie learning and memory. However, recent studies have shown that in fact, most of the synaptic changes are activity-independent. This result is surprising given the generally held belief that activity-dependent changes in connectivity underlie network functionality. This manuscript proposes a theoretical explanation of why this should be the case. Specifically, this work presents a mathematical analysis of the amount of synaptic plasticity required to maintain learned circuit function in presence of random synaptic changes. The central finding, supported by simulations, is that for an &quot;optimal&quot; learning algorithm that rectifies random changes in connectivity, task-related plasticity should generally be smaller than the magnitude of the fluctuations. All reviewers agreed that this is a very interesting theoretical perspective on an important biological problem.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Optimal synaptic dynamics for memory maintenance in the presence of noise&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Timothy Behrens as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Yonatan Loewenstein (Reviewer #2); Matthias H Hennig (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>The half-century research on synaptic plasticity has primarily focused on how neural activity gives rise to changes in synaptic connections, and how these changes underlie learning and memory. However, recent studies have shown that in fact, most of the synaptic changes are activity-independent. This result is surprising given the generally held belief that activity-dependent changes in connectivity underlie network functionality. This manuscript proposes an explanation of why this should be the case.</p><p>Specifically, this work presents a mathematical analysis of the amount of synaptic plasticity required to maintain learned circuit function in presence of random synaptic changes. The central finding, supported by simulations, is that for an &quot;optimal&quot; learning algorithm that rectifies random changes in connectivity, task-related plasticity should generally be smaller than the magnitude of the fluctuations.</p><p>All reviewers agreed that this is a very interesting perspective on an important biological problem. However the reviewers have had difficulties fully understanding the specific claim, its derivation and potential implications. These issues are detailed in the Main Comments below, and need to be clarified. Additional suggestions are summarised in Other Comments.</p><p>Main comments:</p><p>1. How much does the main claim depend on the assumed learning algorithm? What is the family of learning algorithms that the authors refer to as &quot;optimal&quot; that have the property that directed changes are smaller in their magnitude than the noise-driven changes?</p><p>The reviewers' current understanding is the following, please clarify whether it is correct:</p><p>A) If the learning algorithm simply backtracks the noise, then the magnitude of learning-induced changes will be trivially equal that of the noise-induced changes. An optimal algorithm will not be worse than this trivial one.</p><p>B) If the learning algorithm (slowly) goes in the direction of the gradient of the objective function then (1) if the network is in a maximum of the objective function and changes are small then the magnitude of learning-induced changes will be equal to that of the noise-induced changes; (2) if the network is NOT in a maximum of the objective function then unless the noise is in the opposite direction to the gradient, the learning-induced path to the same level of performance would be shorter. Thus, the magnitude of learning-induced changes would be smaller that of that of the noise-induced changes</p><p>C) There exist (inefficient) learning algorithms such that the magnitude of the learning-induced changes are larger than the magnitude of the noise-induced ones. A learning algorithm that overshoots because of a too-large learning rate could be one of them, and it is trivial to construct other inefficient learning algorithms.</p><p>2. The reviewers have found the mathematical derivation in the main text difficult to follow, in part because it seems to use an unnecessarily complex set of mathematical notations and arguments. The reviewers suggest to focus on intuitive arguments in the main text, to make the arguments more transparent, and the paper more accessible to the broad neuroscience audience. Ultimately, this is up to authors to decide. In any case, the main arguments need to be clarified as laid out above.</p><p>3. The reviewers were not convinced by the role played by the numerical simulations. On one hand, it was not clear what the simulations added with respect to the analytical derivations. Is the purpose to check any specific approximation? On the other hand, the model does not seem to help link the derivations to the original biological question, as it is quite far removed from a biological setup. A minimal suggestion would be to use the model to illustrate more directly the geometric intuition behind the derivation, for instance by showing movements of weights along with some view of the gradient, contrasting optimal and sub-optimal.</p><p>Other comments:</p><p>4. The whole argument rests on an assumption that biological networks optimise a cost function, in particular during reconsolidation. How that assumption applies to the experimental setups detailed in the first part is unclear. At the very least, a clear statement and motivation of this assumption is needed.</p><p>5. One of the reviewers was not convinced (but happy to debate) cellular noise is such a major contributor to synaptic changes as stated in the introduction and in Table 1, as silencing neural activity pharmacologically will almost certainly affect synaptic function strongly. Strong synapses (big spines) tend to be more stable, and in any case it would be very difficult to know which of the observed modifications reported in the referenced papers have functional consequences. It would be very interesting to see what turnover (or weight dynamics) this model would predict under optimal and non-optimal conditions. In Figure 1 it is implied that weight changes continues unchanged in presence of noise (and optimal learning to maintain the objective), is this actually the case? What concrete experimental predictions the authors would (dare to) make?</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Optimal plasticity for memory maintenance in the presence of synaptic fluctuations&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Timothy Behrens (Senior Editor) and a Reviewing Editor.</p><p>All reviewers have found that the manuscript has been very much improved and should be eventually published. The reviewers now fully understand the mathematical framework and the main mathematical result. During the consultation, it has however appeared that all reviewers feel that the main message of the manuscript, and in particular its implications for biology, need to be further clarified. Two main issues remain, which we suggest should be either addressed in the Results section or discussed in detail:</p><p>1. How much do the results rely on the assumption that synaptic changes are &quot;strong&quot;? To what extent is this assumption consistent with experiments? Is this theoretical framework really needed when infinitesimally small noise is immediately corrected by a learning signal, as often assumed (see below for more details)? Is the main result trivial when changes are small? Is the main contribution is to show that the result also holds far from this trivial regime, when noise and corrections are large?</p><p>2. what are the implications of the main mathematical result for interpreting measurable experimental quantities? The relation with experiments listed in the Discussion seems rather indirect (eg on lines 330-335 the interpretations of EM reconstructions in the authors' modelling framework seems unclear; it would be worth unpacking how the papers listed on lines 347-348 are consistent with the main result). Moreover, in many of the panels shown in Figures5-6, the dependence of the loss on the ratio between compensatory plasticity and synaptic fluctuations is rather flat; what does this imply for experimental data?</p><p>More details on the first point from one of the reviewers:</p><p>When studying learning in neuronal networks, the underlying assumption is always (&quot;always&quot; to the best of my knowledge) that learning-induced changes are gradual. For example, some form of activity-dependent plasticity has, on average, a negative projection on the gradient of the loss function of the current state. Small changes to synaptic efficacies are made and now the network is in a slightly different (improved) state. Activity-dependent plasticity in that new state has, again, a negative projection on the (new) gradient of the loss function at the new state, etc. If learning is sufficiently slow, we can average over the stochasticities in the learning process, organism's actions, rewards etc. and learning will improve performance.</p><p>In contrast to this approach, this paper suggests a very different learning process: activity-independent &quot;noise&quot; induces a LARGE change in the synaptic efficacies. This change is followed by a SINGLE LARGE compensatory learning-induced change. The question addressed in this manuscript is how large should this single optimal compensatory learning-induced change be relative to the single noise-induced change. The fact that the compensatory changes are not assumed to be small and that learning is done in a single step, rather than learning being gradual, allowing the local sampling of the loss function, complicates the mathematical analysis. While for analyzing infinitesimally-small changes we only need to consider the local gradient of the loss function, higher-order terms are required when considering single large changes.</p><p>What is the justification to this approach given that gradual learning is what we seem to observe in the experiments, specifically those cited in this manuscript? There is a lot of evidence of gradual changes in numbers of spines or synaptic efficacies, etc. If everything is gradual, why not &quot;recompute&quot; the gradient on the fly as is done in all previous models of learning?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62912.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Main comments:</p><p>1. How much does the main claim depend on the assumed learning algorithm? What is the family of learning algorithms that the authors refer to as &quot;optimal&quot; that have the property that directed changes are smaller in their magnitude than the noise-driven changes?</p></disp-quote><p>We have made this much more clear in the main manuscript, with an additional table that summarises the families of learning algorithm satisfying the main claim.</p><p>Before proceeding, let us emphasise that we do not consider a learning algorithm itself as `optimal'. Learning algorithms induce both a direction and a magnitude of plasticity. We study the optimal magnitude of plasticity, for a given (probably imperfect) direction set by the learning algorithm. This leads to the equation:<disp-formula id="equ74"><mml:math id="m74"><mml:mrow><mml:mfrac><mml:mrow><mml:mo form="postfix" stretchy="false">∥</mml:mo><mml:mi>Δ</mml:mi><mml:mi>c</mml:mi><mml:msubsup><mml:mo form="postfix" stretchy="false">∥</mml:mo><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msubsup><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo form="postfix" stretchy="false">∥</mml:mo><mml:mi>Δ</mml:mi><mml:mi>ϵ</mml:mi><mml:msubsup><mml:mo form="postfix" stretchy="false">∥</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mspace width="0.222em"/><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mi>w</mml:mi><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mi>Δ</mml:mi><mml:mi>ϵ</mml:mi><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mi>w</mml:mi><mml:mo form="prefix" stretchy="false">[</mml:mo><mml:mi>Δ</mml:mi><mml:mi>c</mml:mi><mml:mo form="postfix" stretchy="false">]</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>in the main text, which is valid for any direction Δc of learning-induced plasticity.</p><p>Our main claim (learning-independent plasticity should outcompete learning-induced plasticity) follows as long as equation (1) is less than one. The value of equation (1), at steady-state error, does depend on the direction Δc of learning-induced plasticity and current network state (and hence on the learning algorithm).</p><p>For a network to exactly calculate equation (1), and thus work out the optimal magnitude of learning-induced plasticity, it would need to exactly calculate the Hessian ∇<sup>2</sup>F[w], which we consider implausible. Instead, the network could set the optimal magnitude using an expected value of (1) for an 'average' weight w. We calculate this value for different families of learning algorithm (see the new Table 2 in the main text). We now summarise these results (but they are also discussed in the section: Dependence of the optimal, steady-state magnitude of compensatory, learning-induced plasticity on learning algorithm).</p><p>The entire space of learning algorithms we consider is the space of incremental error based learning rules; that is, learning rules that induce small synaptic changes on small time intervals, for which the increment depends on some recent measure of error in a given task or deviation from some pre specified goal. This covers all standard rules assumed in theoretical studies: supervised, unsupervised and reinforcement learning that express weight change as a differential quantity.</p><p>We divide this space of learning algorithms into three cases: 0<sup>th</sup>-order algorithms (i.e. perturbation based), 1<sup>st</sup>-order algorithms (which approximate/calculate the gradient), and 2<sup>nd</sup>-order algorithms (which approximate/calculate both the gradient and the hessian).</p><p>In the case of a quadratic error function, we show that all three cases should should obey our main claim. We then note that nonlinear error functions, near a local minimum, should look quadratic, and thus have analogous conclusions. For nonlinear error functions far from a local minimum, we find that second order algorithms do not obey our main claim: learning-induced plasticity should exceed learning-independent plasticity at steady-state error. However, in the main text we question the biological plausibility of an accurate second order algorithm operating at a steady-state error far from a local minimum.</p><p>We have made the assumptions underlying the previously described results much more clear. In particular, we describe how we calculate our results for `perfect' (0<sup>th</sup>/1<sup>st</sup>/2<sup>nd</sup>)-order algorithms, and push these insights to `approximate' (0<sup>th</sup>/1<sup>st</sup>/2<sup>nd</sup>)-order algorithms by assuming that the approximation error term will not project onto the hessian ∇<sup>2</sup>F[w] more biasedly than the `perfect' component of the relevant term.</p><disp-quote content-type="editor-comment"><p>The reviewers' current understanding is the following, please clarify whether it is correct:</p><p>A) If the learning algorithm simply backtracks the noise, then the magnitude of learning-induced changes will be trivially equal that of the noise-induced changes. An optimal algorithm will not be worse than this trivial one.</p></disp-quote><p>The first sentence is correct. The second sentence is not. The word `optimal' has been misconstrued: it applies to the magnitude of plasticity induced by a given learning algorithm, not the algorithm itself (which our results are essentially agnostic to, as explained above). Consider an arbitrary learning algorithm falling into the previously described space of algorithms we consider. The algorithm, especially if it obeys biological constraints, may be inefficient in selecting a `good' direction of plasticity (i.e. one that effectively decreases task error). Thus it may perform worse (in maintaining good steady-state error) than the ‘trivial' algorithm that backtracks the noise. It may also perform better, especially if the `backtracking' is onto some highly suboptimal network state. Regardless of how good the directions of plasticity induced by the learning algorithm is, there will be an optimal, associated magnitude of plasticity. We claim that this magnitude of plasticity should optimally be smaller or equal to the magnitude of ongoing synaptic fluctuations. We do not make claims about the direction of plasticity induced by different algorithms.</p><disp-quote content-type="editor-comment"><p>B) If the learning algorithm (slowly) goes in the direction of the gradient of the objective function then (1) if the network is in a maximum of the objective function and changes are small then the magnitude of learning-induced changes will be equal to that of the noise-induced changes; (2) if the network is NOT in a maximum of the objective function then unless the noise is in the opposite direction to the gradient, the learning-induced path to the same level of performance would be shorter. Thus, the magnitude of learning-induced changes would be smaller that of that of the noise-induced changes</p></disp-quote><p>This paragraph talks about gradient descent, which is one of the cases explored in the manuscript. Other cases are described in the main reply above. For the remainder of this reply, we assume that learning-induced plasticity is in the direction of the gradient of the objective function.</p><p>If the network is at steady-state task error, and close to a minimum w* of the task error (i.e. maximum of the objective function), we would expect the optimal magnitude of learning-induced plasticity to be less than that of the learning-independent plasticity. The more anisotropic the curvature (i.e. eigenvalues of ∇<sup>2</sup>F[w*]), the smaller this learning-induced magnitude should be, relative to the learning-independent plasticity. Extremally, it reaches equality with the magnitude of learning-induced plasticity when the eigenvalues of the latter matrix are completely isotropic.</p><p>The further the steady-state task error is from a minimum w*, the closer to parity we would expect the optimal magnitude of learning-induced plasticity to be, relative to the level of learning-independent plasticity.</p><p>We have revamped the figures, and in particular the new Figure 4 directly explains the geometric intuition behind this claim. Meanwhile, the section `Dependence of the optimal magnitude of steady-state, compensatory plasticity on the mechanism' justifies this claim.</p><disp-quote content-type="editor-comment"><p>C) There exist (inefficient) learning algorithms such that the magnitude of the learning-induced changes are larger than the magnitude of the noise-induced ones. A learning algorithm that overshoots because of a too-large learning rate could be one of them, and it is trivial to construct other inefficient learning algorithms.</p></disp-quote><p>Our understanding of the reviewer's comment is as follows: a learning algorithm could be `inefficient' for two reasons:</p><p>1. It selects noisy/imperfect directions of plasticity due to biological constraints. The degree to which this occurs in different learning systems is an open scientific question.</p><p>2. Given a (possibly imperfect) direction of plasticity, the accompanying magnitude of plasticity is too high/low. We agree that any learning algorithm could set the magnitude of plasticity associated with a particular direction too high. In this case the magnitude of learning-induced changes could indeed be larger than the learning-independent ones. By lowering the magnitude of learning-induced changes in this case, better steady-state task error would be achieved.</p><disp-quote content-type="editor-comment"><p>2. The reviewers have found the mathematical derivation in the main text difficult to follow, in part because it seems to use an unnecessarily complex set of mathematical notations and arguments. The reviewers suggest to focus on intuitive arguments in the main text, to make the arguments more transparent, and the paper more accessible to the broad neuroscience audience. Ultimately, this is up to authors to decide. In any case, the main arguments need to be clarified as laid out above.</p></disp-quote><p>We've tried out best to incorporate this suggestion. In particular, almost all of the maths is now contained in yellow boxes, which are separated from the main text. A reader who does not want to engage with the mathematics can read the entirety of the Results section without referring to the yellow boxes. We've expanded the description of the geometric intuition behind our results, and added new figures to help with this.</p><disp-quote content-type="editor-comment"><p>3. The reviewers were not convinced by the role played by the numerical simulations. On one hand, it was not clear what the simulations added with respect to the analytical derivations. Is the purpose to check any specific approximation? On the other hand, the model does not seem to help link the derivations to the original biological question, as it is quite far removed from a biological setup. A minimal suggestion would be to use the model to illustrate more directly the geometric intuition behind the derivation, for instance by showing movements of weights along with some view of the gradient, contrasting optimal and sub-optimal.</p></disp-quote><p>The numerical simulations themselves were there just to check the validity of the analytic derivations under different conditions (i.e. different magnitudes of learning-independent plasticity, and different accuracies of learning-induced plasticity). We agree that they did not provide much geometric intuition into the results, and that this geometric intuition was lacking in the original submission. We have rectified this by providing more detailed figures highlighting the geometric intuition (Figure 4 in particular). These depict your `minimal suggestion'. These were drawn, rather than derived by simulation, since they were depicting precise geometrical features of weight changes at a particular timepoint that we found difficult to cleanly show through simulation. They contrast `optimal' and `suboptimal', as requested, and provide intuition into why the optimal magnitude of learning-induced plasticity is usually lower than the fixed, learning-independent term. We added an extra `motivating' simulation in Figure 1.</p><p>The main claim of the paper is quite generic, and not specific to a particular circuit architecture and/or `biologically plausible' learning rule. We therefore decided to test the claim on an abstract, simple-to-explain setup, where we could easily and intuitively manipulate the `accuracy' of learning-induced plasticity. We decided not to run simulations on a more biologically-motivated circuit architecture/learning rule. If we had done so, we would have had to choose a particular, but arbitrary biologically-motivated setup. This would have required a detailed explanation that was unrelated to the point of the paper. It may have additionally confused casual readers as to the generic nature of the results. We have more clearly described the motivation behind the examples in the new section: Motivating example.</p><disp-quote content-type="editor-comment"><p>Other comments:</p><p>4. The whole argument rests on an assumption that biological networks optimise a cost function, in particular during reconsolidation. How that assumption applies to the experimental setups detailed in the first part is unclear. At the very least, a clear statement and motivation of this assumption is needed.</p></disp-quote><p>We've rewritten this part of the paper to make this assumption much more explicit. We now list our exact assumptions at the beginning of the `Modelling setup' section. We note that any descriptive quantity such as `memory quality', or `learning performance', makes the implicit assumption of a loss function. Of course, the loss function may not be explicitly optimised by a neural circuit as the reviewer notes, but this does not actually matter from a mathematical point of view. As long as there is some organised state that a network evolves toward, one can posit an implicit loss function (or set of loss functions) that are being optimised in order to carry out the kind of analysis we performed here.</p><p>This is analogous to very widely known `cost functions' in physics: closed thermodynamic systems tend to maximise entropy over time; conservative mechanical systems minimise an abstract quantity called action. The components in these systems don't represent or interact with such abstract cost functions, yet the theories that assume them capture the relevant phenomena very successfully.</p><p>We would say that any view of reconsolidation that relies on retaining (to the greatest possible extent) some previously learned information, is implicitly trying to optimise a steady-state for some implicit loss function. Clearly, the notion of retaining previously learned information does not make sense in e.g. an in vitro setup. Nevertheless, the low-level plasticity mechanisms are likely to be have similarities to an intact setup, even if the signals they are receiving (e.g. patterns of incoming neural activity) are pathological. That said, we might speculate that synapses could tune the extent to which they respond to `endogenous' (task independent) signals versus external signals that could convey task information in the intact animal. If true, this is a potential way to interpret the in vitro data. We have included this point in the discussion.</p><p>We have reworded the introduction, making the link between experimental results and our analysis more clear. Note that we do not directly analyse the data provided by the relevant experimental papers. We explored the conclusions of a particular hypothesis: that neural circuits attempt to retain previously learned information in the face of synaptic fluctuations, through compensatory plasticity mechanisms. We then found that many experiments across the literature seemed consistent with our view, even if they don't conclusively imply it. Our work serves as a motivation to conduct further, specific experiments that attempt to isolate plasticity attributable to learning-independent mechanisms.</p><disp-quote content-type="editor-comment"><p>5. One of the reviewers was not convinced (but happy to debate) cellular noise is such a major contributor to synaptic changes as stated in the introduction and in Table 1, as silencing neural activity pharmacologically will almost certainly affect synaptic function strongly. Strong synapses (big spines) tend to be more stable, and in any case it would be very difficult to know which of the observed modifications reported in the referenced papers have functional consequences. It would be very interesting to see what turnover (or weight dynamics) this model would predict under optimal and non-optimal conditions. In Figure 1 it is implied that weight changes continues unchanged in presence of noise (and optimal learning to maintain the objective), is this actually the case? What concrete experimental predictions the authors would (dare to) make?</p></disp-quote><p>Firstly, we have reworded the manuscript to make more clear the fact that synaptic fluctuations may not only represent noise. They represent any plasticity process independent of the learned task: the probability of such a process increasing a weight is independent of whether such an increase is locally beneficial to task performance. Intrinsic processes such as homeostatic plasticity may also be important.</p><p>We agree that pharmacological silencing will strongly affect synaptic activity. Note that we also found experiments in the literature where the proportion of learning/activity-independent plasticity was estimated without pharmacological intervention. [2] looked at commonly innervated spines sharing a pre/post neuron, and by observing their in vitro weight dynamics found that the majority of such dynamics were accounted for by activity- independent processes. [2] also found the same conclusion in vivo by analysing commonly innervated synapses from an EM reconstruction of brain tissue from [3]. Meanwhile, [4] did a control experiment of raising mice in a visually impoverished environment, to compare against pharmacological silencing.</p><p>The literature does strongly suggest that large spines are more stable (e.g. [6]). We don't believe this contradicts any of the conclusions of the paper. We have added some more detail in the results and the discussion about how our results should be interpreted in the case where synaptic fluctuations are not distributed evenly within the neural circuit. In particular, the more that synaptic fluctuations are biased to more heavily alter less functionally important synapses, the lower the optimal magnitude of compensatory plasticity.</p><p>Note that even for simplified probabilistic models of evenly-distributed synaptic fluctuations, such as e.g. a white noise process, larger spines would still be more stable, as the probability of constant magnitude fluctuations eliminating a spine within a time period would decrease with spine size. In general, our results apply for any probabilistic form of synaptic fluctuation, as long as the probability of synaptic fluctuations increasing/decreasing a particular synaptic strength is independent of whether such a change would be beneficial for task performance.</p><p>Figure 1 one does imply that weight changes continue at steady state error. We have changed Figure 1 to show an explicit numerical simulation showing precisely that. As long as compensatory plasticity is not directly backtracking synaptic fluctuations, there will be an overall change in synaptic weights over time.</p><p>During learning, the magnitude of compensatory (i.e. learning) plasticity will be larger than at the point that steady state error is achieved. We have an extra section on the optimal magnitude of compensatory plasticity during learning describing this. Thus, the overall magnitude of plasticity (compensatory plus synaptic fluctuations) is predicted to be greater during learning.</p><p>A concrete experimental prediction follows from this observation that we now outline in the discussion: the proportion of learning-independent plasticity should be lower in a system that is actively learning, as opposed to retaining previously learned information. Interestingly, the literature seems to tentatively support this. Both [2] and [1] considered the covariance of functional synaptic strengths for co-innervated synapses, but in neocortex and hippocampus respectively. The hippocampal experiment showed much less activity- independent plasticity (compare Figure 1 of [1] to Figure 8 of [2]). This would make sense in light of our results if the hippocampus was in a phase of active learning, while the neocortex was in a phase of retaining previously learned information. In fact, many conventional cognitive theories of hippocampus and neocortex characterise the hippocampus as a continual, active learner, with the neocortex as a consolidator of previously learned information (see e.g. [5]).</p><p>References</p><p>[1] Thomas M Bartol Jr, Cailey Bromer, Justin Kinney, Michael A Chirillo, Jennifer N Bourne, Kristen M Harris, and Terrence J Sejnowski. Nanoconnectomic upper bound on the variability of synaptic plasticity. eLife, 4:e10778, 2015.</p><p>[2] Roman Dvorkin and Noam E. Ziv. Relative Contributions of Specific Activity Histories and Spontaneous Processes to Size Remodeling of Glutamatergic Synapses. PLOS Biology, 14(10):e1002572, 2016.</p><p>[3] Narayanan Kasthuri, Kenneth Jeffrey Hayworth, Daniel Raimund Berger, Richard Lee Schalek, Jose Angel Conchello, Seymour Knowles-Barley, Dongil Lee, Amelio Vazquez- Reina, Verena Kaynig, Thouis Raymond Jones, Mike Roberts, Josh Lyskowski Morgan, Juan Carlos Tapia, H. Sebastian Seung, William Gray Roncal, Joshua Tzvi Vogel- stein, Randal Burns, Daniel Lewis Sussman, Carey Eldin Priebe, Hanspeter Pfister, and Jeff William Lichtman. Saturated reconstruction of a volume of neocortex. Cell, 162(3):648{661, 2015.</p><p>[4] Akira Nagaoka, Hiroaki Takehara, Akiko Hayashi-Takagi, Jun Noguchi, Kazuhiko Ishii, Fukutoshi Shirai, Sho Yagishita, Takanori Akagi, Takanori Ichiki, and Haruo Kasai. Abnormal intrinsic dynamics of dendritic spines in a fragile X syndrome mouse model in vivo. Scientific Reports, 6:26651, 2016.</p><p>[5] Randall C. O'Reilly and Jerry W. Rudy. Conjunctive representations in learning and memory: principles of cortical and hippocampal function. Psychological review, 108(2):311, 2001.</p><p>[6] Nobuaki Yasumatsu, Masanori Matsuzaki, Takashi Miyazaki, Jun Noguchi, and Haruo Kasai. Principles of Long-Term Dynamics of Dendritic Spines. Journal of Neuroscience, 28(50):13592{13608, 2008.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>1. How much do the results rely on the assumption that synaptic changes are &quot;strong&quot;? To what extent is this assumption consistent with experiments? Is this theoretical framework really needed when infinitesimally small noise is immediately corrected by a learning signal, as often assumed (see below for more details)? Is the main result trivial when changes are small? Is the main contribution is to show that the result also holds far from this trivial regime, when noise and corrections are large?</p></disp-quote><p>There are a number of questions to unpack here. Before doing so we'd like to point out that the main results go beyond the calculations that corroborate magnitudes of fluctuations and systematic plasticity in experiments. The contributions include an analysis framework that lets us query and understand general relationships between learning rule quality and ongoing synaptic change without making detailed assumptions about circuit architecture and plasticity rules. For example, the relationships we derived reveal that fluctuations should dominate at steady state (significantly exceed 50% of total ongoing change) when a learning rule closely approximate the gradient of a loss function. Given the intense interest in whether approximations of gradient descent occur biologically in synaptic learning rules, this observation alone says that the high degree of turnover observed in some parts of the brain is in fact consistent with, and maybe regarded as circumstantial evidence for, gradient-like learning rules. We'd argue that this insight and the framework that provides it are far from trivial.</p><p>We now turn to the specifics of the reviewers' comment.</p><disp-quote content-type="editor-comment"><p>More details on the first point from one of the reviewers:</p><p>When studying learning in neuronal networks, the underlying assumption is always (&quot;always&quot; to the best of my knowledge) that learning-induced changes are gradual. For example, some form of activity-dependent plasticity has, on average, a negative projection on the gradient of the loss function of the current state. Small changes to synaptic efficacies are made and now the network is in a slightly different (improved) state. Activity-dependent plasticity in that new state has, again, a negative projection on the (new) gradient of the loss function at the new state, etc.</p></disp-quote><p>It is an open question whether `gradual' change is the only way by which synaptic plasticity manifests experimentally. For instance, recent results from Jeff Magee's lab show that a single burst of synaptic plasticity over a single behavioural trial can activate place fields in mouse hippocampal CA1 neurons [1]. Moreover, this plasticity burst can be triggered even where there is a difference of seconds between the necessary factors of synaptic transmission and post-synaptic activation: the synapse integrates information over a long window before potentiating (or not). Even in more classical LTP papers from the last few decades that we now cite are somewhat equivocal on whether synaptic changes occur in one lump, or can be reduced to incremental changes. What is undeniable, empirically, is that a large change (e.g. 200% potentiation) can occur on a timescale of a few minutes, typical of most `induction windows'. Relative to behavioural timescales this is rather fast, so should it be modelled as gradual? In the end it might simply be mathematical convention or convenience that has led most theory papers to assume continuous changes. Fortunately, the setup of our paper accounts for both cases: continual, gradual change or temporally sparse bursts of synaptic plasticity. We have added detailed discussion points in the paper to make this clear.</p><disp-quote content-type="editor-comment"><p>If learning is sufficiently slow, we can average over the stochasticities in the learning process, organism's actions, rewards etc. and learning will improve performance.</p></disp-quote><p>We disagree with this statement in the context where learning-independent synaptic fluctuations are present. Learning must be fast enough to compensate for the synaptic fluctuations. If learning is arbitrarily slow, synaptic fluctuations will grow unchecked. How fast should learning (ie compensatory plasticity) be to have optimal learning performance in the presence of a given degree of synaptic fluctuations. That is the subject of the paper. We do agree with this statement in the context where the only source of stochasticity is the learning rule. In this case, the magnitude of stochasticity decreases with the speed of learning.</p><disp-quote content-type="editor-comment"><p>In contrast to this approach, this paper suggests a very different learning process: activity-independent &quot;noise&quot; induces a LARGE change in the synaptic efficacies. This change is followed by a SINGLE LARGE compensatory learning-induced change. The question addressed in this manuscript is how large should this single optimal compensatory learning-induced change be relative to the single noise-induced change. The fact that the compensatory changes are not assumed to be small and that learning is done in a single step, rather than learning being gradual, allowing the local sampling of the loss function, complicates the mathematical analysis. While for analyzing infinitesimally-small changes we only need to consider the local gradient of the loss function, higher-order terms are required when considering single large changes.</p></disp-quote><p>There is no sequential ordering of the compensatory plasticity and synaptic fluctuation terms, and we are not considering an alternating scenario, where a compensatory plasticity change reacts to a noisy change. Instead, we consider the relative proportions of the two, ongoing, plasticity terms over a (potentially infinitesimally small) time window. Our mathematics is in fact a first order, and not a second order analysis, and the results (in the regime of steady state task error) thus hold in the infinitesimal limit of small changes over the considered time window. We appreciate that this wasn't clear in the previous iteration, and have rewritten the Results section to make this more clear.</p><p>How can our analysis, which critically depends upon the Hessian (i.e. second order term) of the loss function, be a first order (i.e. linear) analysis, which is the relevant analysis in the limit of infinitesimally small changes? To find the optimal rate of compensatory plasticity, we have to differentiate the effect of compensatory plasticity with respect to its magnitude, and set the derivative equal to zero. This derivative-taking (unlabelled equation in Box 3, between equations 8 and 9) turns coefficients that were previously quadratic (second order) in the rate of compensatory plasticity (i.e. the Hessian), into first order (linear) coefficients. Indeed, our formula is locally linear: if we double the rate of fluctuations, it says that the optimal rate of compensatory plasticity should correspondingly double. The aforementioned derivative also turns the first order term in the loss function (i.e. the local gradient mentioned by the reviewer) into a zeroth order (constant) term, that is independent of plasticity rates. As a demonstration, suppose an ongoing compensatory plasticity mechanism (gradient descent, for simplicity) corrected the effects of white-noise, synaptic fluctuations. Meanwhile, task error was at a steady state F[w] = k. Over an infinitesimal time period δt, the white noise fluctuations changed the synaptic weights by a magnitude ϵ(δt). What rate of compensatory plasticity is required to cancel out the effect of white noise on the task error?</p><p>White noise is uncorrelated in expectation with the gradient of the task error. A first order Taylor expansion that only considers the local gradient would therefore give</p><p>E[F[w(<italic>t</italic>) + ϵ(δt)] - F[w(<italic>t</italic>)]] = E[∇<italic>F</italic>[w(<italic>t</italic>)]<sup>T</sup> ϵ(<italic>δt</italic>)] = 0.</p><p>This analysis would suggest that the optimal rate of compensatory plasticity over δt is zero, since the synaptic fluctuations have no effect, to first order, on the task error. This is a zeroth order approximation of the optimal magnitude of compensatory plasticity. In other words, it is independent of the rate of synaptic fluctuations. They could double, and this analysis would still suggest that the optimal rate of compensatory plasticity should still be zero. This is why an analysis only considering the local gradient of the loss function is insufficient, even in the infinitesimal limit of small weight changes.</p><p>We have rewritten the Results section to make clear the correspondence between magnitudes of plasticity over small time intervals Δt, and instantaneous rates of plasticity, and have rephrased our terminology, where appropriate, to refer to plasticity `rates'.</p><disp-quote content-type="editor-comment"><p>What is the justification to this approach given that gradual learning is what we seem to observe in the experiments, specifically those cited in this manuscript? There is a lot of evidence of gradual changes in numbers of spines or synaptic efficacies, etc. If everything is gradual, why not &quot;recompute&quot; the gradient on the fly as is done in all previous models of learning?</p></disp-quote><p>In light of the previous comments, we can say that the manuscript is consistent with, and indeed describes, the gradual learning observed in the mentioned experiments. In the numerical simulations, each timestep corresponds to an “on the fly recomputation&quot; of the (noisy) gradient, consistent with previous models of learning.</p><disp-quote content-type="editor-comment"><p>2. what are the implications of the main mathematical result for interpreting measurable experimental quantities? The relation with experiments listed in the Discussion seems rather indirect (eg on lines 330-335 the interpretations of EM reconstructions in the authors' modelling framework seems unclear; it would be worth unpacking how the papers listed on lines 347-348 are consistent with the main result). Moreover, in many of the panels shown in Figures5-6, the dependence of the loss on the ratio between compensatory plasticity and synaptic fluctuations is rather flat; what does this imply for experimental data?</p></disp-quote><p>We have expanded the discussion on how our modelling framework relates to the mentioned papers. In particular, we have spelled out how our notion of `compensatory plasticity' may be approximated using</p><p>– the covariance in synaptic strengths for co-innervated synapses in EM reconstructions;</p><p>– the ‘activity-independent' plasticity in experiments that suppress neural activity,</p><p>We also go into greater detail about the biological assumptions inherent in our modelling. Even more detail is provided in the first subsection of the Results (review of key experimental findings), as we are aware of the need to keep the discussion reasonably concise.</p><p>As to the relative flatness of the curves in Figure 6, this is for a linear network and is included for mathematical completeness. We have now made this a supplement to Figure 5 (nonlinear network) which is likely more relevant biologically. In Figure 5 itself, the reviewers will note that the relationship is only at for an interval of relatively poor quality learning rules (middle row, where steady state error is almost 2 orders of magnitude worse than the case in the top row, which itself has a learning rule with a correlation of only 0.1 with a gradient). We included this along with the third row (where the dependence is once again steep) to show the general trend, in which a strong U-shape is more typical. In any case, a flat dependence for high compensatory plasticity, while consistent with the theory in certain regimes, is less consistent with the experimental data we reviewed and sought to account for.</p><disp-quote content-type="editor-comment"><p>Additional comment</p><p>Lines 180, 198: Figure 3d is referred to from the text but seems to be missing!</p></disp-quote><p>We corrected this typo, which occurred when we merged panels c and d in a draft copy of Figure 3.</p><p>References</p><p>[1] Katie C Bittner, Aaron D Milstein, Christine Grienberger, Sandro Romani, and Jeffrey C Magee. Behavioral time scale synaptic plasticity underlies ca1 place fields. <italic>Science</italic>, 357(6355):1033{1036, 2017.</p></body></sub-article></article>