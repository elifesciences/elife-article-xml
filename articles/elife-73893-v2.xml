<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">73893</article-id><article-id pub-id-type="doi">10.7554/eLife.73893</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>CompoundRay, an open-source tool for high-speed and high-fidelity rendering of compound eyes</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-253429"><name><surname>Millward</surname><given-names>Blayze</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9025-1484</contrib-id><email>b.f.millward@sheffield.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-255165"><name><surname>Maddock</surname><given-names>Steve</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3179-0263</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-166496"><name><surname>Mangan</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Department of Computer Science, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>13</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e73893</elocation-id><history><date date-type="received" iso-8601-date="2021-09-14"><day>14</day><month>09</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-10-12"><day>12</day><month>10</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-09-23"><day>23</day><month>09</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.09.20.461066"/></event></pub-history><permissions><copyright-statement>© 2022, Millward et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Millward et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-73893-v2.pdf"/><abstract><p>Revealing the functioning of compound eyes is of interest to biologists and engineers alike who wish to understand how visually complex behaviours (e.g. detection, tracking, and navigation) arise in nature, and to abstract concepts to develop novel artificial sensory systems. A key investigative method is to replicate the sensory apparatus using artificial systems, allowing for investigation of the visual information that drives animal behaviour when exposed to environmental cues. To date, ‘compound eye models’ (CEMs) have largely explored features such as field of view and angular resolution, but the role of shape and overall structure have been largely overlooked due to modelling complexity. Modern real-time ray-tracing technologies are enabling the construction of a new generation of computationally fast, high-fidelity CEMs. This work introduces a new open-source CEM software (<italic>CompoundRay</italic>) that is capable of accurately rendering the visual perspective of bees (6000 individual ommatidia arranged on 2 realistic eye surfaces) at over 3000 frames per second. We show how the speed and accuracy facilitated by this software can be used to investigate pressing research questions (e.g. how low resolution compound eyes can localise small objects) using modern methods (e.g. machine learning-based information exploration).</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>compound eyes</kwd><kwd>arthropod</kwd><kwd>compound vision</kwd><kwd>software</kwd><kwd>ray tracing</kwd><kwd>visual perspective</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/P006094/1</award-id><principal-award-recipient><name><surname>Millward</surname><given-names>Blayze</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/S030964/1</award-id><principal-award-recipient><name><surname>Mangan</surname><given-names>Michael</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Rendering compound vision at high speed with high precision, enabling rapid, data-driven exploration of the compound eye design space.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Insects visually solve an array of complex problems including the detection and tracking of fast moving prey (<xref ref-type="bibr" rid="bib62">Wiederman et al., 2017</xref>), long-distance navigation (<xref ref-type="bibr" rid="bib60">Wehner, 2020</xref>), and even three-dimensional (3D) depth estimation (<xref ref-type="bibr" rid="bib38">Nityananda et al., 2018</xref>). These capabilities are realised using a sensory apparatus that is fundamentally different from those of mammals. Therefore revealing the functional properties of the insect visual system offers insights for biologists as well as inspiration for engineers looking to develop novel artificial imaging systems (<xref ref-type="bibr" rid="bib27">Land and Fernald, 1992</xref>; <xref ref-type="bibr" rid="bib28">Land, 1997</xref>; <xref ref-type="bibr" rid="bib2">Arendt, 2003</xref>; <xref ref-type="bibr" rid="bib50">Song et al., 2013</xref>).</p><p>Arthropods possess two primary visual sensors known as compound eyes. Each eye is constructed from a patchwork of self-contained light-sensing structures known as ommatidia, each featuring a lens, a light guide, and a cluster of photosensitive cells (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Ommatidia are physically interlocked with their neighbours, together forming a bulbous outer structure (the compound eye itself) that can vary in size, shape, and curvature, offering a range of adaptations for particular tasks and environments (<xref ref-type="bibr" rid="bib29">Land and Nilsson, 2002</xref>; <xref ref-type="fig" rid="fig1">Figure 1b</xref>). The properties of the lens and photo-receptors are fixed for individual ommatidia but can vary across regions of an individual’s eye (<xref ref-type="bibr" rid="bib33">Meyer and Labhart, 1993</xref>) as well as between individuals of different castes (<xref ref-type="bibr" rid="bib9">Collett and Land, 1975</xref>) and species (<xref ref-type="bibr" rid="bib26">Land, 1989</xref>). This arrangement of independent, interlocked light sensing elements and lenses differs greatly from the mammalian system that utilises a single lens to project a high-resolution image onto a retina.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The structure of the compound eye.</title><p>(<bold>a</bold>) A diagram of a single ommatidium, shown with the lensing apparatus at the top that guides light into the photo-sensitive cells below. (<bold>b</bold>) An image of a real compound eye consisting of hundreds of ommatidia, available in WikiMedia Commons copyright holder <italic>Moussa Direct Ltd</italic>., copyright year 2001, distributed under the terms of a CC BY-SA 3.0 license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-sa/3.0">https://creativecommons.org/licenses/by-sa/3.0</ext-link>), and cropped with an added zoom indicator (red) from the original.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig1-v2.tif"/><permissions><copyright-statement>© 2001, Moussa Direct Ltd</copyright-statement><copyright-year>2001</copyright-year><copyright-holder>Moussa Direct Ltd</copyright-holder><license><ali:license_ref>https://creativecommons.org/licenses/by-sa/3.0/</ali:license_ref><license-p>Panel b is modified from an image of a real compound eye consisting of hundreds of ommatidia, available in WikiMedia Commons, <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-sa/3.0/">distributed under the terms of a CC BY-SA 3.0 license</ext-link></license-p></license></permissions></fig><p>Creation of compound eye models (CEMs), in both hardware or software, is a well-established mechanism to explore the information provided by compound eyes and assess their impact on behaviour. Insights derived from this methodology include demonstration of the benefits of a wide field of view (FOV) and low resolution for visual navigation (<xref ref-type="bibr" rid="bib65">Zeil et al., 2003</xref>; <xref ref-type="bibr" rid="bib56">Vardy and Moller, 2005</xref>; <xref ref-type="bibr" rid="bib32">Mangan and Webb, 2009</xref>; <xref ref-type="bibr" rid="bib63">Wystrach et al., 2016</xref>), and the role played by non-visible light sensing for place recognition (<xref ref-type="bibr" rid="bib35">Möller, 2002</xref>; <xref ref-type="bibr" rid="bib51">Stone et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Differt and Möller, 2016</xref>) and direction sensing (<xref ref-type="bibr" rid="bib25">Lambrinos et al., 1997</xref>; <xref ref-type="bibr" rid="bib16">Gkanias et al., 2019</xref>). Yet, simulated CEMs tend to suffer from a common design shortcoming that limits their ability to accurately replicate insect vision. Specifically, despite differences in the sampling techniques used (e.g. see <xref ref-type="bibr" rid="bib32">Mangan and Webb, 2009</xref>; <xref ref-type="bibr" rid="bib3">Baddeley et al., 2012</xref> for custom sampling approaches, <xref ref-type="bibr" rid="bib36">Neumann, 2002</xref>; <xref ref-type="bibr" rid="bib6">Basten and Mallot, 2010</xref> for rendering based cubemapping CEMs, and <xref ref-type="bibr" rid="bib42">Polster et al., 2018</xref> for ray-casting methods), all contemporary CEMs sample from a single viewpoint. In contrast, the distributed arrangement of ommatidia on distinct 3D eye surfaces provides insects with a multi-viewpoint system that generates different information for different eye shapes.</p><p>To facilitate exploration of such features, e.g., the placement of ommatidia on arbitrary surfaces, an ideal rendering system would allow light to be sampled from different 3D locations through individually configured ommatidia replicating the complex structure of real compound eyes. High-fidelity compound-vision rendering engines with some of these features (though notably slower than real time) were developed previously (<xref ref-type="bibr" rid="bib15">Giger, 1996</xref>; <xref ref-type="bibr" rid="bib10">Collins, 1998</xref>; <xref ref-type="bibr" rid="bib42">Polster et al., 2018</xref>) but were not widely adopted. Difficulties arising as a result of the computational complexity (and so execution time) of CEMs are diminishing as dedicated ray-casting hardware emerges that allows for the capture of visual data from multiple locations in parallel at high speed (e.g. Nvidia [Santa Clara, California, United States] <italic>RTX</italic> series GPUs [<xref ref-type="bibr" rid="bib43">Purcell et al., 2005</xref>; <xref ref-type="bibr" rid="bib7">Burgess, 2020</xref>]). Such systems present an ideal tool tferreo replicate insect vision in unprecedented accuracy at the speeds needed for effective exploration of the compound eye design space itself. As outlined in <xref ref-type="bibr" rid="bib34">Millward et al., 2020</xref>, a next-generation insect eye renderer should:</p><list list-type="order"><list-item><p>Allow for the arrangement of an arbitrary number of ommatidia at arbitrary 3D points.</p></list-item><list-item><p>Allow for the configuration of individual ommatidial properties (e.g. lens acceptance angle).</p></list-item><list-item><p>Perform beyond real time to allow exploration of the design space.</p></list-item></list><p>This paper presents a ray-casting-based renderer, <italic>CompoundRay</italic>, that leverages modern hardware-accelerated ray-tracing (RT) graphics pipelines and fulfils all three of these criteria, allowing researchers in the fields of compound vision and biorobotics to quickly explore the impact varying eye designs have on an autonomous agent’s ability to perceive the world.</p></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><sec id="s2-1"><title>Ray-casting-based insect eye renderer</title><p>A common approach for general real-time rendering such as that found in video games and interactive displays follows polygon-projection-based methods inspired by the simple pinhole camera. As <xref ref-type="fig" rid="fig2">Figure 2a</xref> shows, these systems function by directly projecting the faces and vertices of the scene’s 3D geometry through a singular imaging point and on to a projection plane. In contrast, compound visual systems essentially form multiple pinhole-like viewing systems (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), each with their own image point. This is similar to the effects that mirrors, lenses, and other reflective, diffracting, and refractive surfaces have on light transport, in which multiple imaging points are formed as the result of light being diverted from its original course (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). A polygon projection approach will struggle with these optical phenomena as the process directly transforms the faces of the 3D object from the scene onto the camera’s view plane, in effect applying a projection transform onto a plane or other regular surface forming a singular image point (<xref ref-type="fig" rid="fig2">Figure 2a</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>How different rendering approaches form differing image points (not to scale).</title><p>(<bold>a</bold>) In three-dimensional (3D) projection-based rendering, the polygons that comprise the 3D geometry are projected down to a viewing plane, through one image point. (<bold>b</bold>) In 3D ray-casting-based rendering (used in this paper), rays are emitted into a scene and <italic>sample</italic> the 3D geometry they contact, potentially forming many image points. (<bold>c</bold>) A compound eye requires many image points in order to image its surroundings.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig2-v2.tif"/></fig><p>Drawing on previous work from <xref ref-type="bibr" rid="bib42">Polster et al., 2018</xref>, <italic>CompoundRay</italic> uses a ray-casting approach to rendering the insect visual perspective. Ray-based methods offer an alternative to projective transform rendering: rays are sent out from the virtual viewpoint, simulating – in reverse – the paths of photons from the scene into the cone of vision, accumulating colour from the surfaces they interact with (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). This allows for surfaces such as mirrors and lenses to accurately reflect light being rendered. The term <italic>ray-based methods</italic> here is used as an umbrella term for all rendering approaches that primarily use the intersection of rays and a scene in order to generate an image. In particular, we refer to <italic>ray casting</italic> as the act of sampling a scene using a single ray (as per <xref ref-type="bibr" rid="bib46">Roth, 1982</xref>) – a process that CompoundRay performs multiple times from each ommatidium – and <italic>ray tracing</italic> as the act of using multiple recursive ray casts to simulate light bouncing around a scene and off of objects within, similar to early work in the field of shading (<xref ref-type="bibr" rid="bib1">Appel, 1968</xref>; <xref ref-type="bibr" rid="bib61">Whitted, 1979</xref>). In this work we do <italic>not</italic> recursively cast rays, instead only using a single ray cast to sample directly the environment from an arbitrary point in 3D space (in this case, over the sampling cone of an ommatidium).</p><p>Ray-based approaches can be incredibly computationally complex, as each ray (of which there can be thousands per pixel) needs to be tested against every object in the scene (which can be composed of millions of objects) to detect and simulate interactions. Thus, they have historically only been used in offline cases where individual frames of an animation can take many hours to render, such as in cinema (<xref ref-type="bibr" rid="bib8">Christensen et al., 2018</xref>), and prior to that in architectural and design drafting (<xref ref-type="bibr" rid="bib1">Appel, 1968</xref>; <xref ref-type="bibr" rid="bib46">Roth, 1982</xref>). In recent years, however, graphics processing units (GPUs) have been increasing in capability. In order to better capture the photo-realistic offerings of ray-based rendering methods, GPU manufacturers have introduced dedicated programmable RT hardware into their graphics pipelines (<xref ref-type="bibr" rid="bib43">Purcell et al., 2005</xref>). These RT cores are optimised for efficient parallel triangle-line intersection, allowing billions of rays to be cast in real time into complex 3D scenes.</p><p>As compound eyes consist of a complex lensing structure formed over a non-uniform surface, they naturally form a multitude of imaging points across a surface (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). These projection surfaces are often unique and varied, meaning it is practically infeasible to find a single projective transform to produce the appropriate composite view in a single projection-based operation. As a result, ray-based methods become the natural choice for simulating compound vision, as opposed to the more commonly seen projection-based methods. By utilising modern hardware, it is possible to effectively capture the optical features of the compound eye from the ommatidia up, in a high-fidelity yet performant way.</p></sec><sec id="s2-2"><title>Modelling individual ommatidia</title><p>As the compound eyes of insects contain many hundreds or even thousands of lenses (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) and each acts to focus light to its own point (forming a unique perspective), ray-based methods become the natural technology to use to implement a simulator that can capture their unique sensory abilities. Here, a single ommatidium is first examined, simulated, and then integrated into a complete compound eye simulation.</p><p>Each individual ommatidium in a compound eye captures light from a cone of vision (defined by the <italic>ommatidial acceptance angle</italic>), much in the way that our own eyes observe only a forward cone of the world around us. However, in the case of the ommatidium, all light captured within the cone of vision is focused to a singular point on a small photo-receptor cluster, rather than the many millions of photo-receptors in the human eye that allow for the high-resolution image that we experience – in this way, the vision of a singular ommatidium is more akin to the singular averaged colour of all that is visible within its given cone of vision – its <italic>sampling domain</italic>, much as a pixel from a photo is the average colour of all that lies ‘behind’ it.</p><p><italic>CompoundRay</italic> uses Monte Carlo integration (<xref ref-type="bibr" rid="bib22">Kajiya, 1986</xref>) to estimate the light intensity from the scene within the ommatidium’s sampling domain, effectively forming a small pinhole-type camera at the position of the ommatidium. These discrete samples must be accrued in alignment with the ommatidium’s <italic>sampling function</italic> – as the lens gathers more light from forward angles than those diverging away from the ommatidial axis. Increasing the sample count will result in a more accurate estimate of the light converging on the point.</p><p>Previous works <xref ref-type="bibr" rid="bib10">Collins, 1998</xref>; <xref ref-type="bibr" rid="bib42">Polster et al., 2018</xref> have implemented this approximation of the light sampling function by assuming the influence of individual light rays, distributed statically across the sampling cone, is modulated dependent on the angle to the ommatidial axis via a Gaussian function with a full width at half maximum equal to the acceptance angle of the ommatidium, as seen in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. <italic>CompoundRay</italic> also uses the Gaussian function as a basis for sampling modulation; however, unlike other works, it is not approximated via a static sampling pattern with weighting biasing the forward direction (as is seen in <xref ref-type="fig" rid="fig3">Figure 3a:i</xref>). Instead, sampling <italic>direction</italic> is modulated and chosen at random in relation to the Gaussian probability curve of the sampling function on a temporal, frame-by-frame basis. Each sample is assumed to have an equal weight, with the visual input to a single ommatidium then being formed as the average of all samples (<xref ref-type="fig" rid="fig3">Figure 3a:ii</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Ommatidial sampling distributions, statically and stochastically distributed.</title><p>(<bold>a</bold>) The Gaussian sampling curve (red) with a full width at half maximum (FWHM) equal to the acceptance angle of each ommatidium. (<bold>i</bold>) Example statically distributed sampling rays, length indicates weight. (<bold>ii</bold>) Example stochastically distributed sampling rays, equal weight, distributed according to Gaussian distribution. (<bold>b</bold>) A concentric circular static sampling pattern (red dots) aliases away the three concentric dark grey rings present on a surface. (<bold>c</bold>) The per-sample colours as seen from a single ommatidium mapping to the samples in <italic>b</italic>, note the lack of any influence from the dark grey concentric rings, which have been aliased out.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig3-v2.tif"/></fig><p>Static sampling can result in elements of the scene (in the case demonstrated in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, dark grey circles) being aliased out of the final measurement of sensed light. Increasing per-ommatidium sampling rate <italic>can</italic> ease these aliasing problems (even in the case of a static sampling pattern) at close range but only serves to delay triggering aliasing until further distances where structural frequency again matches sampling pattern. Rather, by choosing to stochastically vary the sampled direction on a frame-by-frame basis removes the structured nature of any single frame’s aliasing, as that same view point will generate different aliasing artefacts on the next frame, having the net effect of reducing structured aliasing when considering sequences of frames (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). One benefit of this approach is that averaging renderings from the same perspective becomes synonymous with an increased sampling rate, indeed, this is how the renderer increases sampling rate internally: by increasing the rendering volume for the compound perspective, effectively batch-rendering and averaging frames from the same point in time. However, as each ommatidium will sample the simulated environment differently for every frame rendered, small variations in the colour received at any given ommatidium occur over time. This point is explored further in the ‘Results’ section.</p><p>In the natural compound eye, light focused from each ommatidium’s sampling domain is transported via a crystalline cone and one or more rhabdomeres toward the receiving photo-receptor cluster – a process that has been modelled by <xref ref-type="bibr" rid="bib49">Song et al., 2009</xref>. However, modelling the biophysical intricacies of the internal ommatidium structure is considered beyond the scope of this software and is considered a post-processing step that could be run on the generated image, considering each pixel intensity as a likelihood measure of photon arrival at an ommatidium.</p></sec><sec id="s2-3"><title>From single ommatidia to full compound eye</title><p>By arranging the simulated ommatidia within 3D space so as to mimic the way real compound eyes are arranged, an image can be generated that captures the full compound eye view. For this task, per-ommatidium facet diameter and acceptance angle can be used to generate the cone of influence of each ommatidium, and simulated ommatidia can be placed at any position and orientation within the environment. As the system is using RT techniques to effectively simulate pinhole cameras at each individual ommatidium, orientation and position relative to the lens of each ommatidium can be set with a single per-ommatidium data point (consisting of the position, orientation, and lens properties), allowing each ommatidium to spawn rays independently of each other. This has the benefit of allowing the system to simulate any type of eye surface shape, and even multiple eyes simultaneously, fulfilling the first of the three defined criteria: <italic>allowing for the arrangement of an arbitrary number of ommatidia at arbitrary 3D points</italic>. The generated images can then be used to allow human viewers to visualise the information captured by the eye by projecting this visual surface onto a two-dimensional (2D) plane using orientation-wise or position-wise spherical equirectangular Voronoi diagrams of each ommatidium’s visual input. Alternatively, a simpler vectorised projection method that plots the visual inputs of each ommatidium in a vector can be used directly as a standardised input to computational models of insect neurobiology.</p></sec><sec id="s2-4"><title>The CompoundRay software pipeline</title><p>The renderer is written in C++ and the Nvidia Compute Unified Device Architecture (CUDA) GPU programming language and allows for a 3D environment representing a given experimental setup to be rendered from the perspective of an insect. The core of the renderer runs in parallel on the GPU (the <italic>device</italic>) as a series of CUDA shader programs, which are driven by a C++ program running on the host computer (the <italic>host</italic>). Environments are stored in <italic>GL Transmission Format</italic> (glTF; <xref ref-type="bibr" rid="bib45">Robinet et al., 2014</xref>) files consisting of 3D objects and cameras, the latter of which can be of two types: compound (structured sets of ommatidia) or traditional (perspective, orthographic, and panoramic). Traditional cameras are implemented to aid the user in the design and analysis of their assembled 3D environment. Compound cameras contain all relevant information for rendering a view of the environment from the perspective of a compound eye. Each camera stores the information required for rendering its view (such as its orientation, position, FOV, or ommatidial structure) in an on-device data record data structure.</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> shows the operational design of the renderer from the device side. The renderer is built on top of Nvidia’s <italic>Optix</italic> (<xref ref-type="bibr" rid="bib40">Parker et al., 2010</xref>) RT framework, which is a pipeline-based system: a pipeline is assembled, which then allows for parallel per-pixel rendering of the environment. A pipeline consists of a ray generator shader, a geometry acceleration structure (GAS), and numerous material shaders. A shader is a small program that is designed to be run in a massively parallel fashion. In a typical application, an individual shader program will be loaded onto a GPU and many thousands of instances of the program will be executed, with varying input parameters. The returned values of each instance are then returned to the ‘host-side’ of the application, allowing for the optimisation of tasks that are well suited to parallelisation.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Graphics processing unit (GPU)-side renderer operational diagram.</title><p>Featured are the joined <italic>ommatidial</italic> and <italic>display</italic> rendering pipelines (including display buffers) as well as the on-device (running on the GPU) per-camera configuration records and related memory allocations. Data structure memory allocations are shown in grey (indicating in-place data) or purple (indicating pointers to other sections of data). Vertical stacks of memory indicate contiguously allocated structures. Data structures marked with an orange square are automatically updated as their host-side (running on the CPU) copies change; those with red circles are manually updated via application programming interface (API) calls. Circled numbers (<bold>1–4</bold>) indicate the path of processing through both pipelines required to display a compound eye view. FOV: field of views.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig4-v2.tif"/></fig><p>A ray generator shader spawns rays to sample the environment with – these are dependent on the type, position, and orientation of the chosen camera. In the case of a panoramic camera (in the display pipeline), for instance, rays are spawned in a spherical manner around a central point (the <italic>position</italic> component of the currently selected camera’s ray generation configuration record). In the case of a compound eye (in the ommatidial pipeline), each ray is generated as a function of the relative position and direction of a single ommatidium to the eye’s position and orientation (note that the ‘eye’-type data records in <xref ref-type="fig" rid="fig4">Figure 4</xref> contain links to per-eye ommatidial arrays that define the ommatidial configuration of the eye).</p><p>A GAS stores the 3D geometry of the scene on the device in a format optimised for ray-geometry intersection and also provides functions to perform these intersection tests. Upon the intersection of a generated ray and the environment the material shader associated with the intersected geometry is run. These material shaders are responsible for the calculation and retrieval of the correct colour for the given geometry. For example, if a ray were to fall onto a blade of grass, the material shader would be expected to compute and return the colour green. We note that CompoundRay currently implements a simple texture lookup, interpolating the nearest pixels of the texture associated with the geometry intersected. Future extensions are presented in more detail in the ‘Discussion section.</p><p>Two pipelines are used in the renderer: an ommatidial pipeline and a display pipeline. The ommatidial pipeline handles sampling of the environment through the currently active compound eye, saving all samples from all ommatidia to a buffer stored within the eye’s ‘raw ommatidial view’ portion of its data record. First, it generates per-ommatidium rays (<xref ref-type="fig" rid="fig4">Figure 4</xref> -1) via instances of the ommatidial ray generation shader. It then uses these to sample the environment through the GAS and appropriate material shaders, finally storing each ommatidium’s view as a vector of single-pixel tri-colour samples in the raw ommatidial view buffer (<xref ref-type="fig" rid="fig4">Figure 4</xref> -2).</p><p>Conversely, the display pipeline handles the generation of the user-facing display buffer (<xref ref-type="fig" rid="fig4">Figure 4</xref> –4). In the case of generating a compound eye view display, <italic>ommatidial projector</italic> shaders are spawned (one per output pixel) and used to lookup (orange arrow connecting <xref ref-type="fig" rid="fig4">Figure 4</xref> parts 2 and 3) and average the values associated with each given ommatidium from the per-camera raw ommatidial view buffer. These values are then re-projected into a human-interpretable view – in the case of the image seen at <xref ref-type="fig" rid="fig4">Figure 4</xref> –4, using an equirectangular orientation-wise projection of the Voronoi regions of each ommatidium. However, for practical applications, this re-projection can be to a simple vector of all ommatidial values, allowing an algorithm to examine the data on a per-ommatidium basis, much in the same way that a real neural circuit would have direct access to the light sensors within the eye. In the case where the current camera is non-compound the display pipeline becomes the only active pipeline, handling the initiation of ray generation, GAS intersection, and materials shading for any given camera view in a single pass using a standard 2D camera projection model (simple pinhole, equirectangular panoramic, or orthographic). By referencing both with GPU video and RAM pointers, both pipelines are able to share a common GAS and material shaders so as to save device memory.</p><p>A full compound eye rendering (as indicated via numbered red circles in <xref ref-type="fig" rid="fig4">Figure 4</xref>) consists of first rendering the ommatidial view via the ommatidial pipeline to the current camera’s raw ommatidial view buffer using the associated eye data record. After this, the raw ommatidial view buffer is re-projected onto the display buffer as a human-interpretable view via the ommatidial projector shaders within the display pipeline. Alternatively, for debugging purposes, rendering can be performed using a traditional camera rather than via an eye model, skipping steps 1 and 2 and instead simply rendering to the display view by passing fully through the display pipeline using the data record associated with the selected camera.</p></sec><sec id="s2-5"><title>Scene composure and use</title><p>Using the glTF file format for scene storage allows the experimental configuration of 3D models, surface textures, and camera poses to be packed into a single file (although per-eye ommatidial configurations are stored separately as comma-separated value (CSV) files and linked to within the glTF file to avoid file bloat, retaining the readability of the glTF file). The glTF format is a JavaScript object notation (JSON)-based human-readable file format for 3D scene storage that is readily supported by a number of popular 3D editors and has allowances for extensions within its specification. As such, all configurable parts of the scene that relate to CompouundRay are placed within each camera’s reserved ‘extras’ property. The renderer is programmed to ingest these extra properties, expanding the scene definition beyond standard glTF without compromising the readability of scene files by other third-party 3D modelling suites.</p><p>While the software comes packaged with a stand-alone executable that can load and render compound eye scenes, this is not the most powerful way of using the tool. When compiled, a shared object library is also generated that can then be called from more accessible languages, such as Python (<xref ref-type="bibr" rid="bib55">Van Rossum and Drake, 2009</xref>). Through the use of this, Python’s <italic>ctypes</italic> system and a helper library that is bundled with the code, can be used directly with Python and the Numpy (<xref ref-type="bibr" rid="bib19">Harris et al., 2020</xref>) mathematical framework. This allows for a significantly wider experimental scope through automated configuration – for instance, one can programmatically move an insect eye around a simulated world extracting views directly from the renderer to be used alongside neural modelling frameworks accessible under Python. This is the recommended way of using the rendering system due to the increased utility that it affords.</p></sec><sec id="s2-6"><title>Example inter-eye comparison method</title><p>To demonstrate the utility of CompoundRay for modern, data-intensive scientific analysis, we designed an example experiment. We note that any data outcomes are secondary to the validation of the tool and experimental procedure. Three virtual eye models were created, based on increasing simplifications of real-world eye data. The virtual eye models were assembled based on one (<italic>Apis mellifera</italic>) of eight scans of real-world samples (two <italic>Apis mellifera</italic>, six <italic>Bombus terrestris</italic>) taken by <xref ref-type="bibr" rid="bib5">Baird and Taylor, 2017</xref>, with the 3D surface shape used to derive sample points and axial directions for each of the 6374 ommatidia present in both eyes. This model was used as a baseline ‘real’ model, from which two simplified derivatives, ‘split’ and ‘single’ were created. These were created by editing the eye’s ommatidial positions to either converge on two or one single point (<xref ref-type="fig" rid="fig5">Figure 5 i-iii</xref>, i-iii) to mimic assumptions made in past simulated compound vision studies (<xref ref-type="bibr" rid="bib13">Franz et al., 1998</xref>; <xref ref-type="bibr" rid="bib3">Baddeley et al., 2012</xref>; <xref ref-type="bibr" rid="bib37">Nityananda et al., 2016</xref>; <xref ref-type="bibr" rid="bib42">Polster et al., 2018</xref>). The three variations on the real eye design were then assessed against each other for their ability to resolve the 3D position of a target point within a simple black and white environment by training a multi-layer perceptron (MLP) network with two fully connected hidden layers to estimate the point’s position, encoding the relationship between visual scene and relative location - a task that was taken as a proxy for the eye’s ability to perform basic small-point spatial awareness tasks. The accuracy of the network’s ability to represent this encoding at any given point within a sampling volume (forming an error volume as seen in <xref ref-type="fig" rid="fig5">Figure 5c</xref>) could then be compared to another, producing maps of difference in utility of a given eye design over the sampling volume when compared to another (Figure 11b, c). The results of this experiment are presented in the later section ‘Example experiment: <italic>Apis mellifera</italic> visual field comparison<italic>’</italic>.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>An example pipeline for exploring eye designs leveraging CompoundRay’s ability to produce large quantities of data.</title><p>(<bold>a</bold>) A camera is placed at various locations within an environment consisting of a single small point in an entirely white world. (<bold>b</bold>) Images (and the associated relative position of the camera in the <inline-formula><mml:math id="inf1"><mml:mi>x</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf2"><mml:mi>y</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf3"><mml:mi>z</mml:mi></mml:math></inline-formula> axes) are collected from each location through the differing eye designs that are being compared (here ‘realistic’, ‘split’, and ‘single’ designs). (<bold>c</bold>) A simple multi-layer perceptron neural network is trained to predict the the relative position (<inline-formula><mml:math id="inf4"><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf6"><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula>) given a compound eye view. The quality of this encoding can then be interrogated by sampling uniformly across the test volume forming an error volume, here presented directly as the L1 loss (the sum of all absolute errors between the output of the network and each axis of the ground truth relative position vector) for any given point.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig5-v2.tif"/></fig><p>Comparing the views from varying ommatidial eye designs presents a number of challenges. In particular, the differing surface shapes and ommatidial counts between any two eyes make direct comparison very challenging as a result of the lack of a one-to-one correspondence between the ommatidia of each eye. Due to these challenges, the method of indirect comparison via a proxy variable was chosen. In this example comparison method, we propose using the task of small-point localisation as a basis for measuring eye utility, as similar tasks have formed key parts of many behavioural studies (<xref ref-type="bibr" rid="bib54">van Praagh et al., 1980</xref>; <xref ref-type="bibr" rid="bib20">Juusola and French, 1997</xref>; <xref ref-type="bibr" rid="bib37">Nityananda et al., 2016</xref>). As shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>, a camera simulating the desired eye design is placed within a matte white environment with a small matte black sphere (in this case, 2 mm in diameter) placed at its centre. The camera can then be moved around a sampling volume (here 50 mm<sup>3</sup>), and a pair consisting of the view from the eye and the relative <inline-formula><mml:math id="inf7"><mml:mi>x</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf8"><mml:mi>y</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf9"><mml:mi>z</mml:mi></mml:math></inline-formula> offset, forming its <italic>relative position</italic>. A neural network can then be trained on a sufficient number of pairs encode the mapping from eye view to relative position (localisation).</p><p>The non-Euclidian surface geometry of the compound eye makes convolutional neural network architectures (which typically operate over euclidian 2D or 3D space [<xref ref-type="bibr" rid="bib30">Li et al., 2021</xref>]) inappropriate, as doing so would require a conversion from eye surface to a 2D surface, potentially introducing biases (as further elaborated in the ‘Discussion’ section). Graph neural networks (<xref ref-type="bibr" rid="bib47">Scarselli et al., 2009</xref>) – neural networks that operate independent of assumptions of implicit spatially uniform sampling, instead operating over data structured in a graph – provide a method of operating over the 3D surface of the eye. However, the simpler approach of flattening the visual field into a single vector (with each element representing a single ommatidium’s view sample) was taken here. This approach chosen as the task of self-localisation from a single dark point in a light environment becomes a simple pattern matching task due to every position in the field producing a unique view, not requiring translation or rotation invariant feature recognition, all of which can be encompassed in a simple fully connected MLP network with two or more hidden layers (<xref ref-type="bibr" rid="bib31">Lippmann, 1987</xref>). Furthermore, the views were passed to the network as one-dimensional black and white images to reduce on computational cost when running the experiment, as the environment was purely black and white, resulting in the red, green, and blue components to each ommatidial sample being identical.</p><p>For the purposes of our example experiment any encoding sufficiently capable of representing the mapping of visual input to relative location could have been used. The example MLP network we provide is specific to the example scenario given and as such should only be referred to as a starting point when considering a neural-network-based abstracted comparison method. We designed an MLP with 2 hidden layers (1000 and 256 neurons, with rectified linear activation functions to act as non-linearities) and a final linear output layer of 3 neurons to represent the x, y, and z components of the relative position vector. The hidden layers act to encode the non-linear relation between the views and the relative position of the eye, with the count at each layer chosen to provide a gradual decline in complexity from input visual field size to the 3D output vector.</p><p>The data was standardised using z-score normalisation across all training points for each eye on both input and output dimensions before training to ensure learning was weighted equally over all dimensions of the input vector and to reduce bias on output vectors. As each eye sample had a differing number of ommatidia, the size of the input vector (and as such the first layer weight vector) for the neural network differed from eye to eye. Batched stochastic gradient descent was then used to optimise the sum of all absolute errors between the output of the network and each axis of the ground truth relative position vector (the <italic>L1 loss function</italic>). This error could then be observed with respect to the experimental sampling volume to assess the eye design’s task-specific performance across the sampling space (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). A batch size of 64 was selected in order to train at an appropriate speed while balancing efficient use of available hardware (<xref ref-type="bibr" rid="bib17">Golmant et al., 2018</xref>); however, in practice, any commonly used batch size would be suitable.</p><p>For each eye design, 100,000 image position pairs were generated for training purposes. Of these, 80,000 were used for training, and 20,000 were used as a validation dataset in order to plot training progress. For each derivative eye design, the neural network was trained across 100 epochs (Figure 11a shows the error over the validation set during training). A total of 100 epochs were chosen as by this point in training no significant further training gains are made, as can be seen inFigure 11a.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><p>The analysis that follows first assesses the performance of CompoundRay with respect to the three criteria defined in the ‘Introduction’, before showing its utility in an example experiment. Performance is benchmarked in two 3D environments: a lab environment inspired by those used in insect cognitive experiments (<xref ref-type="bibr" rid="bib39">Ofstad et al., 2011</xref>); and a 3D scan of a real-world natural environment covering an area of a number of square kilometers. The natural environment was constructed using structure-from-motion photogrammetry using photos captured from a drone over rural Harpenden, UK (3D model subject to upcoming publication, available via contact of Dr. Joe Woodgate, Queen Mary University of London).</p><sec id="s3-1"><title>Criterion 1: arbitrary arrangements of ommatidia</title><p>Criterion 1 states that the renderer must support arbitrary arrangements of ommatidia within 3D space. <xref ref-type="fig" rid="fig6">Figure 6</xref> shows two different environments rendered using three differing eye designs, from a simple spherical model with spherical uniformly distributed ommatidia that more closely aligns with current compound eye rendering methods, to a hypothetical arbitrary arrangement similar to that of the now long-extinct <italic>Erbenochile erbeni</italic>. While <xref ref-type="fig" rid="fig6">Figure 6a&amp;b</xref> is still possible to simulate using traditional projection-based methods due to their ommatidial axes converging onto one central spot, 6 c demonstrates an eye with multiple unique focal points across an irregular surface which must be rendered from the viewpoint of each ommatidium independently.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Rendering from eyes with ommatidia placed on arbitrary surfaces in one lab environment and one three-dimensional scan of a natural outdoor environment.</title><p>Renderings are projected to two-dimensional using a spherical orientation-wise Voronoi latitude/longitude mapping (<italic>orientation-wise equirectangular spherical mapping</italic>). (<bold>a</bold>) Spherically uniformly distributed ommatidia. (<bold>b</bold>) Ommatidia clumped to form a horizontal acute zone (<bold>c</bold>) A design inspired by the <italic>Erbenochile erbeni</italic>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig6-v2.tif"/></fig></sec><sec id="s3-2"><title>Criterion 2: inhomogeneous ommatidial properties</title><p>Criterion 2 states that it should be possible to specify heterogeneous ommatidial optical properties across an eye, such as the enlarged facets as found in robberflies (<xref ref-type="bibr" rid="bib59">Wardill et al., 2017</xref>). As demonstrated in <xref ref-type="fig" rid="fig7">Figure 7</xref>, the renderer achieves this by allowing for shaping of the acceptance cone of each ommatidium. In doing so, <xref ref-type="fig" rid="fig7">Figure 7</xref> also demonstrates the importance of heterogeneous ommatidial configurations.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>A single environment as viewed through: (<bold>a</bold> and <bold>b</bold>) eye designs with homogeneous ommatidial acceptance angles, note the blurred edges in (<bold>a</bold>) and sharp aliased edges in (<bold>b</bold>).</title><p>(<bold>c</bold>) An eye with heterogeneous ommatidial acceptance angles. Below each output image is a three-dimensional depiction of the field of view of the specific array of ommitidia making up each eye: (<bold>a</bold>) oversampled, homogeneous eye; (<bold>b</bold>) undersampled, homogeneous eye; (<bold>c</bold>) evenly sampled, heterogeneous eye. Model (<bold>c</bold>) has the benefit of being able to leverage the most useful aspects of (b) and (a). Renderings are projected to wo-dimensional using an orientation-wise equirectangular spherical mapping.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig7-v2.tif"/></fig><p><xref ref-type="fig" rid="fig7">Figure 7a&amp;b</xref> shows a non-uniform eye design with homogeneous, globally identical, ommatidial acceptance angles. It can be seen that in <xref ref-type="fig" rid="fig7">Figure 7b</xref>, where the acceptance angle is lower, aliasing occurs as objects are only partially observed through any given ommatidium, causing blind spots to form, introducing noise into the image. Conversely, in <xref ref-type="fig" rid="fig7">Figure 7a</xref>, where the acceptance angle is large, the image becomes blurry, with each ommatidium oversampling portions of its neighbour’s cone of vision. <xref ref-type="fig" rid="fig7">Figure 7c</xref>, however, demonstrates a heterogeneous ommatidial distribution, in which the acceptance angles toward the dorsal and ventral regions of the eye are larger compared with the angles of those around the eye’s horizontal acute zone. This mirrors what is seen in nature, where ommatidial FOV is seen to vary in order to encompass the entirety of an insect’s spherical visual field (<xref ref-type="bibr" rid="bib29">Land and Nilsson, 2002</xref>). As can be seen in the generated image, this is important as it appropriately avoids creating blind-spots in the dorsal and ventral regions while also avoiding oversampling (blurring) in the horizontally acute region, resulting in a much clearer picture of the observed environment (before the visual data is passed into any insect visual processing neural circuits) while also minimising the required number of ommatidia. The data presented in <xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig7">7</xref> clearly demonstrates differences in visual information provided by different ommatidial layouts, reinforcing the need for realistic eye modelling methods.</p></sec><sec id="s3-3"><title>Criterion 3: speed</title><sec id="s3-3-1"><title>Minimum sample count</title><p>Criterion 3 states that to rapidly explore the information content of various eye designs any such rendering system should be able to perform at real time or faster. However, speed of the system is dependent on the number of rays required to render a given scene from a given eye; higher total ray counts will require more compute power. As discussed in the section ‘Modelling individual ommatidia<italic>’</italic>, per-ommatidial sampling choice can have a dramatic impact on the image generated, meaning that lower ray counts – while increasing speed – will decrease the accuracy of the image produced. As a result, it is important to be able to define some minimum number of samples required for any given eye in any given environment in a reliable and repeatable manner. Previous works <xref ref-type="bibr" rid="bib42">Polster et al., 2018</xref> have attempted to establish a baseline number of samples required per ommatidium before additional samples become redundant only via qualitative visual analysis of the results produced. Here we perform quantitative analysis of images produced by the renderer in order to more definitively define this baseline (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>A single viewpoint viewed using variable samples per ommatidium.</title><p>Top: How changing sample count changes the generated image. Note that the left side (with only 1 sample per ommatidium) is more ‘jagged’ than the right side image (70 samples per ommatidium) due to aliasing. Middle: Relative standard deviations of each ommatidium across 1000 frames, where difference is measured as the Euclidean distance between the two colours, this highlights the significantly larger standard deviation at points of high visual contrast in the image. Bottom: Plot of the average standard deviation of an eye per sample rays per ommatidium, normalised to one steradian. The standard deviation decreases as per-ommatidium sampling rate increases.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig8-v2.tif"/></fig><p>Unlike previous works, the temporally stochastic nature of <italic>CompoundRay</italic>s’ sampling approach can be used to aid the measurement of impact that sampling rate has on the final image. By measuring the per-ommatidium spread (here, standard deviation) of the Euclidean distance of the received colour as plotted in RGB colour space over a range of frames captured from a static compound camera, the precision of the light sampling function approximation can be measured. To control for the varying FOV of each ommatidium (which impacts the number of samples required, as larger FOVs require more sampling rays to accurately capture the ommatidium’s sampling domain), we normalize the measure of spread recorded at each ommatidium against its degree of coverage, in steradians, expressing the solid angle subtended by the ommatidium’s FOV at the eye’s centre. That is, a steradian measures the solid angle within a sphere, in the same same way that a radian measures 2D angles within a circle.</p><p><xref ref-type="fig" rid="fig8">Figure 8</xref> shows this measure of variance within the compound eye across a range of sample-rays-per-ommatidium, from 1 to 700. As the number of samples increases, the variance decreases logarithmically. We propose iteratively increasing samples until the maximum variance falls below a defined threshold value. Here, we use a threshold of 1%, meaning that the maximum expected standard deviation of the most deviant ommatidium on an eye (normalised to one steradian) should be no greater than 1% of the maximum difference between two received colours, here defined as the length of vector [255, 255, 255]. This method allows for standardised, eye-independent configuration of per-ommatidial sampling rate.</p><p>However, the required number of samples per eye is highly positively correlated with the visual frequency and intensity of the simulated environment at any given point, with locations exhibiting high contrast seen from a distance requiring higher sampling rates in order to properly resolve. This can be seen in the skyline in <xref ref-type="fig" rid="fig8">Figure 8</xref>, which remains dominant throughout the majority of the sampling rates, consistently forming the source of the highest spread. <xref ref-type="fig" rid="fig9">Figure 9</xref> shows how the perceived variance changes over two example environments. The varying environment presents an obstacle in our attempts to standardise sampling rates, as deriving a suitable minimum sampling rate at one point in an environment – e.g., where the visual frequency was, on average, low – could lead to aliasing occurring in more visually complex areas of the same environment, potentially introducing bias for or against certain areas if analysis on the generated data were sensitive to the variance introduced.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Standard deviation, averaged over all ommatidia in an eye, mapped over the natural and lab environments, showing a range of ‘hotspots’ of high variance in renderings.</title><p>These hotspots must be accounted for when calculating minimum sample ray count. Inserts bottom left and right: the maximum (over all ommatidia) standard deviation recorded in the eye at each location.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig9-v2.tif"/></fig><p>Ultimately, the varying visual complexity poses a challenge. In an ideal scenario maximum per-ommatidium frame-to-frame variation caused by visual complexity should be a controllable factor. In the examples presented in this paper, we worked to minimise this variation to a point at which the most variable ommatidial response had a standard deviation of 1% or less of the total maximal difference in colour. One way of achieving this would be to keep a running measure of the standard deviation of each ommatidium and dynamically adapt sampling rates to ensure that no ommatidium had such a high sampling spread (adaptive antialiasing). In this work, however, we define a minimum sampling count for a given environment by performing a search for that environment’s point of highest visual complexity (indicated by highest per-ommatidium deviation) – with this location found, sampling rate can be increased until deviation decreases to acceptable limits (in this case, within 1%). <xref ref-type="fig" rid="fig9">Figure 9</xref> shows plots of the relative variation in standard deviation over the lab and (partial) natural environments.</p></sec><sec id="s3-3-2"><title>Performance</title><p><xref ref-type="fig" rid="fig10">Figure 10</xref> shows the rendering speed, in frames per second, against the number of sample rays being emitted in each frame when running using a selection of high-end consumer-grade graphics cards and our two sample environments. Marked on the graph are the total per-frame sample counts required by common insect eyes, chosen to encourage a maximum per-ommatidium standard deviation of 1%.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>The average frames per second (FPS; 500 samples) per total number of rays dispatched into the scene for 3 different graphics cards in the 2 environments depicted in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</title><p>Marked are expected FPS values for a desert ant (<xref ref-type="bibr" rid="bib48">Schwarz et al., 2011</xref>), honey bee (<xref ref-type="bibr" rid="bib18">Greiner et al., 2004</xref>), and dragonfly (<xref ref-type="bibr" rid="bib24">Labhart and Nilsson, 1995</xref>; <xref ref-type="bibr" rid="bib29">Land and Nilsson, 2002</xref>). Averaged data is smoothed using a 1-dimensional box filter acting over 10 samples, clipped at the beginning and end.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig10-v2.tif"/></fig><p>As can be seen, performance improves significantly when the renderer is able to utilise the next-generation RT hardware available in the newer RTX series of NVidia graphics cards, consistently outperforming older cards in the larger natural environment and outperforming in the smaller lab environment when using more than 30,000 rays. Counterintuitively, the previous generation of NVidia graphics cards was found to outperform the RTX series in the (relatively small) lab environment at lower ray counts (below 30,000). This appears to be due to static overhead involved with the use of the RTX series’ RT cores, which may also be the cause of the small positive trend seen between 1000 and 8000 samples when using the 2080Ti. Another potentially counterintuitive result that can be seen here is the higher performance of the dragonfly eye in the natural (high polygon) environment versus in the low-polygon laboratory environment. This is because the laboratory environment produces significantly higher visual contrast (emphasised in <xref ref-type="fig" rid="fig9">Figure 9</xref>) due to its stark black/white colour scheme, resulting in a requirement of almost double the number of rays per steradian to target 1% standard deviation, resulting in higher ray counts overall, particularly in models with a high number of ommatidia.</p></sec><sec id="s3-3-3"><title>Example experiment: <italic>Apis mellifera</italic> visual field comparison</title><p>Leveraging the speed of CompoundRay, an example data-driven comparative analysis was performed between a realistic eye model and two simplified derivatives. We note that the data outcomes should be considered as pilot data, with the focus being on proving the tool and its usability. As laid out in the section ‘Example inter-eye comparison method’, this experiment used an MLP network to estimate the relative position of a 2 mm sphere within a 50 mm<sup>3</sup> test area using only the visual information provided from each of the eye models, with neural network performance acting as a measure of eye configuration utility. The models were split into the ‘real’ eye model (<xref ref-type="fig" rid="fig5">Figure 5i</xref>), an eye pair based directly on the measurements of <xref ref-type="bibr" rid="bib5">Baird and Taylor, 2017</xref>; the ‘split’ eye model (<xref ref-type="fig" rid="fig5">Figure 5ii</xref>), an eye pair retaining per-ommatidial headings (ommatidial axial directions), but sampled from the centre points of each eye (as per approaches common in insect binocular vision study); and finally the ‘single’ eye model, an eye pair sampled from a single point between the two eyes (<xref ref-type="fig" rid="fig5">Figure 5iii</xref>). We hypothesise that the ‘real’ eye design will outperform the ‘split’ and ‘single’ eye models by increasing margins, as for each of the latter two designs the data relating to the surface shape and relative positions of the eyes is increasingly reduced.</p><p>In terms of total error over the validation set, we observed that – as expected – the ‘real’ eye design appears to offer a higher utility (lower total error when trying to locate the relative position of the sphere), ultimately decreasing its network’s validation error quicker and to a lower minimum after 100 epochs when compared to both the ‘split’ and ‘single’ eye designs (<xref ref-type="fig" rid="fig11">Figure 11a</xref>). Interestingly, despite an initial slow in learning rate, the ‘single’ eye design eventually achieved a similar utility score on the validation set than that of the ‘split’ eye design.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>The results of a sample experiment comparing the relative effectiveness of three progressively simplified dual-eye designs – a ‘real’ design built from data collected from real-world <italic>Apis mellifera</italic> (<xref ref-type="bibr" rid="bib5">Baird and Taylor, 2017</xref>) and two progressively simplified designs: the ‘split’ design, collapsing ommatidial viewing points onto two eye centres, and the ‘single’ design, projecting all ommatidial viewing points onto one central position (figurative examples shown in <xref ref-type="fig" rid="fig5">Figure 5i–iii</xref>).</title><p>(<bold>a</bold>) The validation error graphs over 100 epochs of training for each eye design. The majority of learning occurs between epoch 0 and 60, and by epoch 100 all networks have converged in optimal-approximating states. (<bold>b</bold>) The absolute relative differences across the error volumes (see <xref ref-type="fig" rid="fig5">Figure 5c</xref>) between the ‘real’ and ‘single’ eye designs, as seen from the top. (<bold>c</bold>) As in (b), but between the ‘real’ and ‘split’ designs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-73893-fig11-v2.tif"/></fig><p>To qualitatively assess the performance of each eye across the entire 50 mm<sup>3</sup> sampling cube, a further 1,000,000 samples were taken at regular intervals forming an error volume for each eye design by measuring the error between the trained neural network’s relative position and the real relative position (the error volume for the ‘real’ eye is pictured in <xref ref-type="fig" rid="fig5">Figure 5c</xref>). In this case, to demonstrate versatility, CompoundRay was used in real time to generate these views, rather than simply being used in an offline manner to generate these benchmarking datasets.</p><p>The most obvious region of high error is that outside of the insect’s FOV. As these eyes were designed with uniform average acceptance angles, this is larger than might be expected of a real bee eye. Comparing the ‘real’ eye to its ‘single’ and ‘split’ counterparts by means of comparing the absolute difference between their error volumes (creating the difference volumes shown from top-down in <xref ref-type="fig" rid="fig11">Figure 11b&amp;c</xref>), patterns emerge. Seemingly, the most significant region of difference between the ‘real’ eye model and the ‘single’ eye model can be seen around the edges of the eye’s field of vision. As this difference follows the entirety of the edge of the eye’s field of vision, this appears to be likely due to parallax – the ‘real’ eye samples from two points, allowing it slightly extended range of vision compared to that of the ‘single’ eye model, resulting in slightly different ranges of FOV between the two, with this difference increasing the further from the eye the point is. In the ‘real’ vs ‘split’ comparison image, however, there are two regions of higher error (highlighted in red circles in <xref ref-type="fig" rid="fig11">Figure 11c</xref>) toward the forward-facing regions of the eye’s vision. These indicate a region of improved visual information when using a full eye surface as compared to simple two-point sampling of the environment.</p></sec></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>This paper has introduced a new compound eye perspective rendering software, <italic>CompoundRay</italic>, that addresses the need for a fast, compound eye perspective rendering pipeline (<xref ref-type="bibr" rid="bib52">Stürzl et al., 2015</xref>; <xref ref-type="bibr" rid="bib53">Taylor and Baird, 2017</xref>; <xref ref-type="bibr" rid="bib34">Millward et al., 2020</xref>). In particular, the tool supports arbitrary instantiation of heterogeneous ommatidia at any place in space and can perform rendering tasks at speeds in the order of thousands of frames per second using consumer-grade graphics hardware. We have demonstrated the utility afforded by the software and highlighted the importance of using higher-fidelity compound eye perspective rendering systems by demonstrating the visual differences introduced by altering these traits. It has also set a grounding for reasonable use of the software to ensure reproducibility and reduce biasing introduced as a result of the variety present in simulated environments and compound eye designs while warning of potential dangers that might arise during data analysis. With the introduction of the CompoundRay rendering library, it is possible to simulate in a timely manner the visual experience of structurally diverse compound eyes in complex environments, digitally captured or otherwise manually constructed. CompoundRay is provided as an open-sourced library (<ext-link ext-link-type="uri" xlink:href="https://github.com/BrainsOnBoard/compound-ray">https://github.com/BrainsOnBoard/compound-ray</ext-link>).</p><p>This work lays the technical foundations for future research into elements of compound eye design that have previously been given little consideration. In particular, CompoundRay can act as the previously missing step to integrate recently catalogued real eye designs (<xref ref-type="bibr" rid="bib5">Baird and Taylor, 2017</xref>; <xref ref-type="bibr" rid="bib4">Bagheri et al., 2020</xref>) into mapped insect environments (<xref ref-type="bibr" rid="bib52">Stürzl et al., 2015</xref>; <xref ref-type="bibr" rid="bib44">Risse et al., 2018</xref>) to explore the information provided to insects in their natural surroundings. Similarly, the ability to configure ommatidial properties individually and with respect to eye surface shape enables new investigations into the benefits of asymmetric compound eyes such as those found in robber flies (<xref ref-type="bibr" rid="bib59">Wardill et al., 2017</xref>) and crabs (<xref ref-type="bibr" rid="bib64">Zeil et al., 1986</xref>). Furthermore, we see opportunities to model the recently reported microsaccadic sampling in fruit flies (<xref ref-type="bibr" rid="bib21">Juusola et al., 2017</xref>), as well as aid with similar vision-centric neurological studies (<xref ref-type="bibr" rid="bib58">Viollet, 2014</xref>; <xref ref-type="bibr" rid="bib63">Wystrach et al., 2016</xref>; <xref ref-type="bibr" rid="bib23">Kemppainen et al., 2021</xref>).</p><p>Our example study demonstrated the utility of CompoundRay for this strand of research. Beyond its ability to replicate insect vision in unprecedented detail, its image generation speeds facilitate use of contemporary data-driven methods to explore the eye design space. For example, we used over 3.3 million training images, which CompoundRay rendered in only 2 hr. The insights gained from more thorough analysis of the insect visual perspective—and its design relative to visual feature extraction—will help guide the development of artificial visual systems by considering not only visual post-processing steps but also the intrinsic structure and design of the image retrieval system itself. CompoundRay is explicitly designed to be highly configurable to allow members of the research community to pursue their specific research questions.</p><p>In our example study, we compared eye designs using an MLP network as a utility function, bypassing the need to directly compare the images generated by two differing eye designs. This was done as the task of directly comparing two dissimilar eye designs – either analytically using a comparison program or qualitatively via human observation – is non-trivial. Compound eyes are inherently different from our own, and the CompoundRay rendering system allows for large-scale data collection on arbitrary compound surfaces. We note that with this comes the risk of anthropomorphising the data received when directly observing it, or placing bias on one area over another when comparing programmatically. At its base level, the eye data gathered is most ‘pure’ when considered as a single vector of ommatidial samples, much in the same way that the neural systems of any insect will interpret the data. In contrast, all the images that we have presented here have been projected onto a 2D plane in order to be displayed. Great care must be taken when considering these projections, as they can introduce biases that may be easy to overlook. For instance, in <xref ref-type="fig" rid="fig6">Figure 6a&amp;b</xref>, orientation-wise spherical projection mapping was used to artificially place the ommatidia across the 2D spaces, forming panoramic Voronoi plots of observed light at each ommatidium. In these projections, much as is seen in 2D projections of Earth, the points at the poles of the image are artificially inflated, giving a bias in terms of raw surface area affected toward ommatidia present in the ventral and dorsal regions of the eye. Not only do these visual distortions warp the image produced, but if that image is used as the basis for algorithmic comparison (e.g., as the input to an image difference function [<xref ref-type="bibr" rid="bib41">Philippides et al., 2011</xref>]) then that too, will be effected by these projection biases. In the case where direct comparison between two eyes with entirely dissimilar surfaces is attempted, the problems can be magnified further, as the projections of each eye will introduce biases that may interact in unexpected ways.</p><p>These projection biases highlight how fundamentally difficult it is to directly compare eye designs that do not share an underlying surface structure, unless those surfaces happen to be affine transformations of one another (they are <italic>affinely equivalent</italic>). For any surfaces that are affinely equivalent, the surface itself can be used as a projection surface. In this way, for instance, the outputs of two cubic eyes could be compared to each other by performing all comparison calculations on the cube’s surface, interpolating between points where an ommatidia exists on one eye but not another. It is trivial to directly compare compound eye images from eyes with the same structure in terms of ommatidial placement, and eyes with surfaces that are affinely equivalent can also be compared; however, further precautions must be taken when comparing the images from two or more eyes that do not have compatible projection surfaces so as not to unduly bias a portion of one surface over any other. If direct comparison between two eye designs is required, and their surfaces are not affine transformations of each other, then intermediate surfaces to perform calculations over must be found within the interpolation from one surface to the other, with a significant amount of care taken to reduce as much as possible projection artefacts forming between the two. However, we recommend instead taking a proxy metric as demonstrated here (using a localisation task) to measure differences between eye-accessible information content.</p><p>By basing CompoundRay around the conceptually simple approach of ray-casting and adhering to open standards, the rendering system has been designed with open development in mind. In the future, the library could be extended to further enhance biological realism by adding non-visible spectral (e.g. ultraviolet) lighting (<xref ref-type="bibr" rid="bib35">Möller, 2002</xref>; <xref ref-type="bibr" rid="bib51">Stone et al., 2006</xref>; <xref ref-type="bibr" rid="bib11">Differt and Möller, 2015</xref>) and polarisation (e.g.<xref ref-type="bibr" rid="bib16">Gkanias et al., 2019</xref>) sensitivities, as well as more realistic photo-receptor response characteristics (e.g. <xref ref-type="bibr" rid="bib49">Song et al., 2009</xref>). Similarly, new technologies could be rapidly evaluated by simulating the input to novel imaging systems (e.g. dynamic; <xref ref-type="bibr" rid="bib57">Viollet and Franceschini, 2010</xref> or heterogeneous pixel arrays) and processing pipelines (e.g. event-based retinas; <xref ref-type="bibr" rid="bib14">Gallego et al., 2022</xref>). In addition, environmental features such as shadow casting, reflection, and refractions present obvious extensions as well the introduction of features such as adaptive anti-aliasing and increasing the projections available to users. Currently, <italic>CompoundRay</italic> returns all output images in an eight-bit colourspace, which aligns with most contemporary display equipment. However, we note that the underlying GPU implementation operates on 32-bit floating-point variables which if exposed to the high-level Python API could improve the system’s ability to represent visual sensors that experience a wider bandwidth of light (hyperspectral visual systems). It is hoped that as further efforts are conducted to map the eye structures of insects, CompoundRay will serve as a key tool in uncovering the intricacies of these eye designs in a modern, data-driven way.</p></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Supervision, Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-73893-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The manuscript is a computational study, with all modelling code and data accessible on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/ManganLab/eye-renderer">https://github.com/ManganLab/eye-renderer</ext-link>. Use of the natural environment was kindly provided by Dr. JoeWoodgate, Queen Mary University of London and is subject to upcoming publication. As such, instead included in the CompoundRay repository is a stand-in natural 3D terrain model. As all models are used for demonstrative purpose, this stand-in model offers little difference to the natural model used, bar it's subjectively lower-quality aesthetics.</p></sec><ack id="ack"><title>Acknowledgements</title><p>Thanks to Dr James Knight (University of Sussex) for his input and feedback on the manuscript, and Dr Joe Woodgate (Queen Mary University of London) for provision of the natural environment 3D model. Graphics cards were supplied by the Brains on Board research project (EP/P006094/1) and project partner NVidia. Grant funding was provided by EPSRC awards EP/P006094/1 and EP/S030964/1. For the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Appel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Some techniques for shading machine renderings of solids the April 30--May 2, 1968</article-title><conf-name>spring joint computer conference</conf-name><pub-id pub-id-type="doi">10.1145/1468075.1468082</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arendt</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Evolution of eyes and photoreceptor cell types</article-title><source>The International Journal of Developmental Biology</source><volume>47</volume><fpage>563</fpage><lpage>571</lpage><pub-id pub-id-type="pmid">14756332</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname><given-names>B</given-names></name><name><surname>Graham</surname><given-names>P</given-names></name><name><surname>Husbands</surname><given-names>P</given-names></name><name><surname>Philippides</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A model of ant route navigation driven by scene familiarity</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002336</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002336</pub-id><pub-id pub-id-type="pmid">22241975</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bagheri</surname><given-names>ZM</given-names></name><name><surname>Jessop</surname><given-names>AL</given-names></name><name><surname>Kato</surname><given-names>S</given-names></name><name><surname>Partridge</surname><given-names>JC</given-names></name><name><surname>Shaw</surname><given-names>J</given-names></name><name><surname>Ogawa</surname><given-names>Y</given-names></name><name><surname>Hemmi</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A new method for mapping spatial resolution in compound eyes suggests two visual streaks in fiddler crabs</article-title><source>The Journal of Experimental Biology</source><volume>223</volume><elocation-id>jeb210195</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.210195</pub-id><pub-id pub-id-type="pmid">31822556</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baird</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>X-Ray micro computed-tomography</article-title><source>Current Biology</source><volume>27</volume><fpage>R289</fpage><lpage>R291</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.01.066</pub-id><pub-id pub-id-type="pmid">28441557</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basten</surname><given-names>K</given-names></name><name><surname>Mallot</surname><given-names>HA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Simulated visual homing in desert ant natural environments: efficiency of skyline cues</article-title><source>Biol Cybern</source><volume>102</volume><fpage>413</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1007/s00422-010-0375-9</pub-id><pub-id pub-id-type="pmid">20300942</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rtx on—the NVIDIA Turing GpU</article-title><source>IEEE Micro</source><volume>40</volume><fpage>36</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1109/MM.2020.2971677</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christensen</surname><given-names>P</given-names></name><name><surname>Fong</surname><given-names>J</given-names></name><name><surname>Shade</surname><given-names>J</given-names></name><name><surname>Wooten</surname><given-names>W</given-names></name><name><surname>Schubert</surname><given-names>B</given-names></name><name><surname>Kensler</surname><given-names>A</given-names></name><name><surname>Friedman</surname><given-names>S</given-names></name><name><surname>Kilpatrick</surname><given-names>C</given-names></name><name><surname>Ramshaw</surname><given-names>C</given-names></name><name><surname>Bannister</surname><given-names>M</given-names></name><name><surname>Rayner</surname><given-names>B</given-names></name><name><surname>Brouillat</surname><given-names>J</given-names></name><name><surname>Liani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>RenderMan: an advanced path-tracing architecture for movie rendering</article-title><source>ACM Transactions on Graphics</source><volume>37</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1145/3182162</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname><given-names>TS</given-names></name><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Visual control of flight behaviour in the hoverflysyritta pipiens L</article-title><source>Journal of Comparative Physiology ? A</source><volume>99</volume><fpage>1</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1007/BF01464710</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1998">1998</year><chapter-title>Reconstructing the visual field of compound eyes</chapter-title><person-group person-group-type="editor"><name><surname>Rocamora</surname><given-names>SP</given-names></name></person-group><source>Eurographics Rendering Workshop</source><publisher-name>Springer Wien</publisher-name><fpage>81</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1007/978-3-7091-6858-5</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Differt</surname><given-names>D</given-names></name><name><surname>Möller</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Insect models of illumination-invariant skyline extraction from UV and green channels</article-title><source>Journal of Theoretical Biology</source><volume>380</volume><fpage>444</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1016/j.jtbi.2015.06.020</pub-id><pub-id pub-id-type="pmid">26113191</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Differt</surname><given-names>D</given-names></name><name><surname>Möller</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Spectral skyline separation: extended landmark databases and panoramic imaging</article-title><source>Sensors</source><volume>16</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3390/s16101614</pub-id><pub-id pub-id-type="pmid">27690053</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franz</surname><given-names>MO</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Mallot</surname><given-names>HA</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Where did I take that snapshot? scene-based homing by image matching</article-title><source>Biol Cybern</source><volume>79</volume><fpage>191</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1007/s004220050470</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>G</given-names></name><name><surname>Delbruck</surname><given-names>T</given-names></name><name><surname>Orchard</surname><given-names>G</given-names></name><name><surname>Bartolozzi</surname><given-names>C</given-names></name><name><surname>Taba</surname><given-names>B</given-names></name><name><surname>Censi</surname><given-names>A</given-names></name><name><surname>Leutenegger</surname><given-names>S</given-names></name><name><surname>Davison</surname><given-names>AJ</given-names></name><name><surname>Conradt</surname><given-names>J</given-names></name><name><surname>Daniilidis</surname><given-names>K</given-names></name><name><surname>Scaramuzza</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Event-based vision: A survey</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>44</volume><fpage>154</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2020.3008413</pub-id><pub-id pub-id-type="pmid">32750812</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Giger</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>PhD thesis: Honeybee vision: analysis of pattern orientation</article-title><publisher-name>The Australian National University</publisher-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gkanias</surname><given-names>E</given-names></name><name><surname>Risse</surname><given-names>B</given-names></name><name><surname>Mangan</surname><given-names>M</given-names></name><name><surname>Webb</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>From skylight input to behavioural output: a computational model of the insect polarised light COMPASS</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007123</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007123</pub-id><pub-id pub-id-type="pmid">31318859</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Golmant</surname><given-names>N</given-names></name><name><surname>Vemuri</surname><given-names>N</given-names></name><name><surname>Yao</surname><given-names>Z</given-names></name><name><surname>Feinberg</surname><given-names>V</given-names></name><name><surname>Gholami</surname><given-names>A</given-names></name><name><surname>Rothauge</surname><given-names>K</given-names></name><name><surname>Mahoney</surname><given-names>MW</given-names></name><name><surname>Gonzalez</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>On The Computational Inefficiency Of Large Batch Sizes For Stochastic Gradient Descent Technical report</source><publisher-name>ICLR</publisher-name></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greiner</surname><given-names>B</given-names></name><name><surname>Ribi</surname><given-names>WA</given-names></name><name><surname>Warrant</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Retinal and optical adaptations for nocturnal vision in the halictid bee megalopta genalis</article-title><source>Cell and Tissue Research</source><volume>316</volume><fpage>377</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1007/s00441-004-0883-9</pub-id><pub-id pub-id-type="pmid">15064946</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Picus</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>S</given-names></name><name><surname>van Kerkwijk</surname><given-names>MH</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Haldane</surname><given-names>A</given-names></name><name><surname>Del Río</surname><given-names>JF</given-names></name><name><surname>Wiebe</surname><given-names>M</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Gérard-Marchant</surname><given-names>P</given-names></name><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Abbasi</surname><given-names>H</given-names></name><name><surname>Gohlke</surname><given-names>C</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Array programming with numpy</article-title><source>Nature</source><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><pub-id pub-id-type="pmid">32939066</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>French</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Visual acuity for moving objects in first- and second-order neurons of the fly compound eye</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>1487</fpage><lpage>1495</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.3.1487</pub-id><pub-id pub-id-type="pmid">9084613</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Dau</surname><given-names>A</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Solanki</surname><given-names>N</given-names></name><name><surname>Rien</surname><given-names>D</given-names></name><name><surname>Jaciuch</surname><given-names>D</given-names></name><name><surname>Dongre</surname><given-names>SA</given-names></name><name><surname>Blanchard</surname><given-names>F</given-names></name><name><surname>de Polavieja</surname><given-names>GG</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name><name><surname>Takalo</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Microsaccadic sampling of moving image information provides <italic>Drosophila</italic> hyperacute vision</article-title><source>eLife</source><volume>6</volume><elocation-id>e26117</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26117</pub-id><pub-id pub-id-type="pmid">28870284</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kajiya</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>The rendering equation the 13th annual conference</article-title><conf-name>ACM SIGGRAPH Computer Graphics</conf-name><pub-id pub-id-type="doi">10.1145/15922.15902</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kemppainen</surname><given-names>J</given-names></name><name><surname>Scales</surname><given-names>B</given-names></name><name><surname>Haghighi</surname><given-names>KR</given-names></name><name><surname>Takalo</surname><given-names>J</given-names></name><name><surname>Mansour</surname><given-names>N</given-names></name><name><surname>McManus</surname><given-names>J</given-names></name><name><surname>Leko</surname><given-names>G</given-names></name><name><surname>Saari</surname><given-names>P</given-names></name><name><surname>Hurcomb</surname><given-names>J</given-names></name><name><surname>Antohi</surname><given-names>A</given-names></name><name><surname>Suuronen</surname><given-names>JP</given-names></name><name><surname>Blanchard</surname><given-names>F</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Hampton</surname><given-names>M</given-names></name><name><surname>Eckermann</surname><given-names>M</given-names></name><name><surname>Westermeier</surname><given-names>F</given-names></name><name><surname>Frohn</surname><given-names>J</given-names></name><name><surname>Hoekstra</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>CH</given-names></name><name><surname>Huttula</surname><given-names>M</given-names></name><name><surname>Mokso</surname><given-names>R</given-names></name><name><surname>Juusola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Binocular Mirror-Symmetric Microsaccadic Sampling Enables <italic>Drosophila</italic> Hyperacute 3D-Vision</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.05.03.442473</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Labhart</surname><given-names>T</given-names></name><name><surname>Nilsson</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The dorsal eye of the dragonfly sympetrum: specializations for prey detection against the blue sky</article-title><source>Journal of Comparative Physiology A</source><volume>176</volume><fpage>437</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1007/BF00196410</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambrinos</surname><given-names>D</given-names></name><name><surname>Kobayashi</surname><given-names>H</given-names></name><name><surname>Pfeifer</surname><given-names>R</given-names></name><name><surname>Maris</surname><given-names>M</given-names></name><name><surname>Labhart</surname><given-names>T</given-names></name><name><surname>Wehner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>An autonomous agent navigating with a polarized light COMPASS</article-title><source>Adaptive Behavior</source><volume>6</volume><fpage>131</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1177/105971239700600104</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1989">1989</year><chapter-title>Variations in the structure and design of compound eyes</chapter-title><person-group person-group-type="editor"><name><surname>Stavenga</surname><given-names>DG</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name></person-group><source>In Facets of Vision</source><publisher-loc>Berlin Heidelberg, Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>90</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-74082-4_5</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name><name><surname>Fernald</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The evolution of eyes</article-title><source>Annual Review of Neuroscience</source><volume>15</volume><fpage>1</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.15.030192.000245</pub-id><pub-id pub-id-type="pmid">1575438</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Visual acuity in insects</article-title><source>Annual Review of Entomology</source><volume>42</volume><fpage>147</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1146/annurev.ento.42.1.147</pub-id><pub-id pub-id-type="pmid">15012311</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name><name><surname>Nilsson</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Animal Eyes</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Peng</surname><given-names>S</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A survey of convolutional neural networks: analysis, applications, and prospects</article-title><conf-name>IEEE Transactions on Neural Networks and Learning Systems</conf-name><pub-id pub-id-type="doi">10.1109/TNNLS.2021.3084827</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lippmann</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>An introduction to computing with neural nets</article-title><source>IEEE ASSP Magazine</source><volume>4</volume><fpage>4</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1109/MASSP.1987.1165576</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mangan</surname><given-names>M</given-names></name><name><surname>Webb</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Modelling place memory in crickets</article-title><source>Biol Cybern</source><volume>101</volume><fpage>307</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1007/s00422-009-0338-1</pub-id><pub-id pub-id-type="pmid">19862550</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>EP</given-names></name><name><surname>Labhart</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Morphological specializations of dorsal rim ommatidia in the compound eye of dragonflies and damselfies (Odonata)</article-title><source>Cell &amp; Tissue Research</source><volume>272</volume><fpage>17</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1007/BF00323566</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Millward</surname><given-names>B</given-names></name><name><surname>Maddock</surname><given-names>S</given-names></name><name><surname>Mangan</surname><given-names>M</given-names></name><collab>Sheffield Robotics, The University Of Sheffield</collab><collab>Department of Computer Science, The University Of Sheffield</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Towards Insect Inspired Visual Sensors for Robots</article-title><conf-name>UKRAS20 Conference</conf-name><pub-id pub-id-type="doi">10.31256/Do2Ik3H</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Möller</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Insects could exploit UV-green contrast for landmark navigation</article-title><source>Journal of Theoretical Biology</source><volume>214</volume><fpage>619</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1006/jtbi.2001.2484</pub-id><pub-id pub-id-type="pmid">11851371</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neumann</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>Modeling insect compound eyes: space-variant spherical vision</chapter-title><person-group person-group-type="editor"><name><surname>Poggio</surname><given-names>TA</given-names></name></person-group><source>Biologically Motivated Computer Vision</source><publisher-name>Springer</publisher-name><fpage>360</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1007/3-540-36181-2</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nityananda</surname><given-names>V</given-names></name><name><surname>Bissianna</surname><given-names>G</given-names></name><name><surname>Tarawneh</surname><given-names>G</given-names></name><name><surname>Read</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Small or far away? size and distance perception in the praying mantis</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>371</volume><elocation-id>20150262</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0262</pub-id><pub-id pub-id-type="pmid">27269605</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nityananda</surname><given-names>V</given-names></name><name><surname>Tarawneh</surname><given-names>G</given-names></name><name><surname>Henriksen</surname><given-names>S</given-names></name><name><surname>Umeton</surname><given-names>D</given-names></name><name><surname>Simmons</surname><given-names>A</given-names></name><name><surname>Read</surname><given-names>JCA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A novel form of stereo vision in the praying mantis</article-title><source>Current Biology</source><volume>28</volume><fpage>588</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.012</pub-id><pub-id pub-id-type="pmid">29429616</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ofstad</surname><given-names>TA</given-names></name><name><surname>Zuker</surname><given-names>CS</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual place learning in <italic>Drosophila melanogaster</italic></article-title><source>Nature</source><volume>474</volume><fpage>204</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1038/nature10131</pub-id><pub-id pub-id-type="pmid">21654803</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>SG</given-names></name><name><surname>Bigler</surname><given-names>J</given-names></name><name><surname>Dietrich</surname><given-names>A</given-names></name><name><surname>Friedrich</surname><given-names>H</given-names></name><name><surname>Hoberock</surname><given-names>J</given-names></name><name><surname>Luebke</surname><given-names>D</given-names></name><name><surname>McAllister</surname><given-names>D</given-names></name><name><surname>McGuire</surname><given-names>M</given-names></name><name><surname>Morley</surname><given-names>K</given-names></name><name><surname>Robison</surname><given-names>A</given-names></name><name><surname>Stich</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>OptiX: a general purpose ray tracing engine</article-title><source>ACM Transactions on Graphics</source><volume>29</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1145/1778765.1778803</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Philippides</surname><given-names>A</given-names></name><name><surname>Baddeley</surname><given-names>B</given-names></name><name><surname>Cheng</surname><given-names>K</given-names></name><name><surname>Graham</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How might ants use panoramic views for route navigation?</article-title><source>The Journal of Experimental Biology</source><volume>214</volume><fpage>445</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1242/jeb.046755</pub-id><pub-id pub-id-type="pmid">21228203</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Polster</surname><given-names>J</given-names></name><name><surname>Petrasch</surname><given-names>J</given-names></name><name><surname>Menzel</surname><given-names>R</given-names></name><name><surname>Landgraf</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reconstructing the Visual Perception of Honey Bees in Complex 3-D Worlds</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1811.07560">https://arxiv.org/abs/1811.07560</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Purcell</surname><given-names>TJ</given-names></name><name><surname>Buck</surname><given-names>I</given-names></name><name><surname>Mark</surname><given-names>WR</given-names></name><name><surname>Hanrahan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Ray tracing on programmable graphics hardware</article-title><conf-name>ACM SIGGRAPH 2005 Courses</conf-name><pub-id pub-id-type="doi">10.1145/1198555.1198798</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Risse</surname><given-names>B</given-names></name><name><surname>Mangan</surname><given-names>M</given-names></name><name><surname>Stürzl</surname><given-names>W</given-names></name><name><surname>Webb</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Software to convert terrestrial lidar scans of natural environments into photorealistic meshes</article-title><source>Environmental Modelling &amp; Software</source><volume>99</volume><fpage>88</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.envsoft.2017.09.018</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Robinet</surname><given-names>F</given-names></name><name><surname>Arnaud</surname><given-names>R</given-names></name><name><surname>Parisi</surname><given-names>T</given-names></name><name><surname>Cozzi</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>GlTF: designing an open-standard runtime asset format</chapter-title><person-group person-group-type="editor"><name><surname>Engel</surname><given-names>W</given-names></name></person-group><source>GPU Pro 5: Advanced Rendering Techniques</source><publisher-name>CRC Press</publisher-name><fpage>18</fpage><lpage>522</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Ray casting for modeling solids</article-title><source>Computer Graphics and Image Processing</source><volume>18</volume><fpage>109</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/0146-664X(82)90169-1</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scarselli</surname><given-names>F</given-names></name><name><surname>Gori</surname><given-names>M</given-names></name><name><surname>Tsoi</surname><given-names>AC</given-names></name><name><surname>Hagenbuchner</surname><given-names>M</given-names></name><name><surname>Monfardini</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The graph neural network model</article-title><source>IEEE Transactions on Neural Networks</source><volume>20</volume><fpage>61</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1109/TNN.2008.2005605</pub-id><pub-id pub-id-type="pmid">19068426</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname><given-names>S</given-names></name><name><surname>Narendra</surname><given-names>A</given-names></name><name><surname>Zeil</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The properties of the visual system in the Australian desert ant melophorus bagoti</article-title><source>Arthropod Structure &amp; Development</source><volume>40</volume><fpage>128</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/j.asd.2010.10.003</pub-id><pub-id pub-id-type="pmid">21044895</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Coca</surname><given-names>D</given-names></name><name><surname>Billings</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Biophysical Modeling of a Drosophila Photoreceptor. Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-74082-4_5</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>YM</given-names></name><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Malyarchuk</surname><given-names>V</given-names></name><name><surname>Xiao</surname><given-names>J</given-names></name><name><surname>Jung</surname><given-names>I</given-names></name><name><surname>Choi</surname><given-names>KJ</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Lu</surname><given-names>C</given-names></name><name><surname>Kim</surname><given-names>RH</given-names></name><name><surname>Li</surname><given-names>R</given-names></name><name><surname>Crozier</surname><given-names>KB</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Rogers</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Digital cameras with designs inspired by the arthropod eye</article-title><source>Nature</source><volume>497</volume><fpage>95</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1038/nature12083</pub-id><pub-id pub-id-type="pmid">23636401</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Stone</surname><given-names>T</given-names></name><name><surname>Mangan</surname><given-names>M</given-names></name><name><surname>Ardin</surname><given-names>P</given-names></name><name><surname>Webb</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Sky segmentation with ultraviolet images can be used for navigation</article-title><conf-name>Robotics</conf-name><pub-id pub-id-type="doi">10.15607/RSS.2014.X.047</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stürzl</surname><given-names>W</given-names></name><name><surname>Grixa</surname><given-names>I</given-names></name><name><surname>Mair</surname><given-names>E</given-names></name><name><surname>Narendra</surname><given-names>A</given-names></name><name><surname>Zeil</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Three-Dimensional models of natural environments and the mapping of navigational information</article-title><source>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</source><volume>201</volume><fpage>563</fpage><lpage>584</lpage><pub-id pub-id-type="doi">10.1007/s00359-015-1002-y</pub-id><pub-id pub-id-type="pmid">25863682</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>GJ</given-names></name><name><surname>Baird</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>INSECT VISION: SEGMENTATION TO SIMULATIONS</article-title><conf-name>In 3rd International Conference on Tomography of Materials and Structures</conf-name></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Praagh</surname><given-names>JP</given-names></name><name><surname>Ribi</surname><given-names>W</given-names></name><name><surname>Wehrhahn</surname><given-names>C</given-names></name><name><surname>Wittmann</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Drone bees fixate the Queen with the dorsal frontal part of their compound eyes</article-title><source>Journal of Comparative Physiology</source><volume>136</volume><fpage>263</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1007/BF00657542</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van Rossum</surname><given-names>G</given-names></name><name><surname>Drake</surname><given-names>FL</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Python 3 Reference Manual</source><publisher-name>CreateSpace, Scotts Valley, CA</publisher-name></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vardy</surname><given-names>A</given-names></name><name><surname>Moller</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Biologically plausible visual homing methods based on optical flow techniques</article-title><source>Connection Science</source><volume>17</volume><fpage>47</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1080/09540090500140958</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viollet</surname><given-names>S</given-names></name><name><surname>Franceschini</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A hyperacute optical position sensor based on biomimetic retinal micro-scanning</article-title><source>Sensors and Actuators A</source><volume>160</volume><fpage>60</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.sna.2010.03.036</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viollet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Vibrating makes for better seeing: from the fly’s micro-eye movements to hyperacute visual sensors</article-title><source>Frontiers in Bioengineering and Biotechnology</source><volume>2</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.3389/fbioe.2014.00009</pub-id><pub-id pub-id-type="pmid">25152883</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wardill</surname><given-names>TJ</given-names></name><name><surname>Fabian</surname><given-names>ST</given-names></name><name><surname>Pettigrew</surname><given-names>AC</given-names></name><name><surname>Stavenga</surname><given-names>DG</given-names></name><name><surname>Nordström</surname><given-names>K</given-names></name><name><surname>Gonzalez-Bellido</surname><given-names>PT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A novel interception strategy in a miniature robber fly with extreme visual acuity</article-title><source>Current Biology</source><volume>27</volume><fpage>854</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.01.050</pub-id><pub-id pub-id-type="pmid">28286000</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wehner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Desert Navigator</source><publisher-name>Harvard University Press</publisher-name><pub-id pub-id-type="doi">10.4159/9780674247918</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Whitted</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>An improved illumination model for shaded display</article-title><conf-name>the 6th annual conference</conf-name><pub-id pub-id-type="doi">10.1145/800249.807419</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiederman</surname><given-names>SD</given-names></name><name><surname>Fabian</surname><given-names>JM</given-names></name><name><surname>Dunbier</surname><given-names>JR</given-names></name><name><surname>O’Carroll</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A predictive focus of gain modulation encodes target trajectories in insect vision</article-title><source>eLife</source><volume>6</volume><elocation-id>e26478</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26478</pub-id><pub-id pub-id-type="pmid">28738970</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wystrach</surname><given-names>A</given-names></name><name><surname>Dewar</surname><given-names>A</given-names></name><name><surname>Philippides</surname><given-names>A</given-names></name><name><surname>Graham</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How do field of view and resolution affect the information content of panoramic scenes for visual navigation? A computational investigation</article-title><source>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</source><volume>202</volume><fpage>87</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1007/s00359-015-1052-1</pub-id><pub-id pub-id-type="pmid">26582183</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeil</surname><given-names>J</given-names></name><name><surname>Nalbach</surname><given-names>G</given-names></name><name><surname>Nalbach</surname><given-names>HO</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Eyes, eye stalks and the visual world of semi-terrestrial crabs</article-title><source>Journal of Comparative Physiology A</source><volume>159</volume><fpage>801</fpage><lpage>811</lpage><pub-id pub-id-type="doi">10.1007/BF00603733</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeil</surname><given-names>J</given-names></name><name><surname>Hofmann</surname><given-names>MI</given-names></name><name><surname>Chahl</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Catchment areas of panoramic snapshots in outdoor scenes</article-title><source>Journal of the Optical Society of America. A, Optics, Image Science, and Vision</source><volume>20</volume><fpage>450</fpage><lpage>469</lpage><pub-id pub-id-type="doi">10.1364/josaa.20.000450</pub-id><pub-id pub-id-type="pmid">12630831</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73893.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.09.20.461066" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.20.461066"/></front-stub><body><p>In this important work, the authors develop compelling new open source methods to study compound eye vision, with particular emphasis and examples in insects and appropriately supporting arbitrarily diverse spatial distributions, types and mixtures of types of ommatidia. The manuscript introduces example experiments to illustrate the use of the new methodology. This work supports future studies of invertebrate brains, a timely addition to the newly mapped connectomes of insect brains.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73893.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Cardona</surname><given-names>Albert</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Saalfeld</surname><given-names>Stephan</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013sk6x84</institution-id><institution>Janelia Research Campus</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Straw</surname><given-names>Andrew D</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0245cg223</institution-id><institution>University of Freiburg</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.20.461066">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.09.20.461066v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;CompoundRay: An open-source tool for high-speed and high-fidelity rendering of compound eyes&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by myself Albert Cardona as a Reviewing Editor and Tirin Moore as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Stephan Saalfeld (Reviewer #1); Andrew D Straw (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) All reviewers agree that the raw ommatidial view buffer aspect of the work is the strongest piece and with the most potential to support neuroscience experiments. Expanding on this and discussing the possible applications in simulating insect vision and e.g., interpreting the neural connectomes of insect visual systems is most appropriate. Discussion of the modeling of e.g., insect compound eyes with asymmetric ommatidia is a must; one such example could be robber flies.</p><p>2) The motivation for the visualization for human observers is weak, and it is unclear of what use it can be for insect vision or vision research more generally. On surface, it seems a contradiction that the software goes to great lengths to model ommaditia with independent focal points, as necessary for the accurate modeling of each ommatidium, only to then project to a single nodal point to create a camera eye image, contrary to the main point of the work. A stronger motivation with explicit applications that aren't possible otherwise with this camera image are necessary. Otherwise, this aspect of the work ought to be downsized relative to the clearly innovative and useful raw ommaditial view buffer approach for compound eye vision research.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>More details as they emerged, comments overlap:</p><p>98 Do you really mean that each ray has a maximum recursion depth of 0? Did you implement illumination by a light source? If rays do not recurse or trace to light sources, all surfaces are 100% diffuse and ambient and ray casting is equivalent to polygon projection with local illumination. If no illumination by an independent light source is implemented, all surfaces are 100% ambient and ray casting is equivalent to polygon projection. Please clarify.</p><p>64 – 67 The first two criteria both cover &quot;arbitrary ommatidia geometry&quot; which I think is one criterion.</p><p>65 Spectral sensitivity, which could be criterion 2 is not addressed as far as I understand. The renderer is purely RGB in 8-bit depth, correct? This is not discussed in section &quot;Criterion 2&quot; and could be a big obstacle for real applications. Is it hard to fix?</p><p>78 shadow maps are not more resource hungry than ray tracing with direct illumination, can you please explain better or remove?</p><p>84 interreflection of light can be implemented by e.g. calculating radiosity maps or doing sampling of rays cast from light sources. I do not see that CompoundRay is doing either, so it is not clear what the relationship to this work is? Can you please clarify?</p><p>Overall, I find the paragraph about ray-casting ray-tracing and the concrete relationship with CompoundRay unnecessarily confusing. As said before, I believe it would be better to explain the relevant differences of rendering methods only in how they relate to this project. I.e. how they produce an image (projection rendering by projecting polygons using painters algorithm, texture mapping and/ or colorize with local illumination model; ray-casting by identifying first intersection, texture mapping and/ or colorize with local illumination model, averaging over rays). Mention illumination, shadows (maps for each light in projection, testing for object intersection by tracing to light in ray-tracing) and global models (radiosity or photon sampling) only as possible extensions, not used in this work.</p><p>124 – 128 state the above but the language and reasoning are unclear.</p><p>227 – 229 eventually clarify that CompoundRay does not currently implement any illumination model or first order ray recursion to cast shadows, so the output would be equivalent to per ommatidium camera rendering and integration, this is way too late in the text as illumination and shading capabilities were used earlier to root for ray-casting for no practical reason.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Abstract:</p><p>L12-13: I think some expectation of why this might be relevant or interesting would be useful.</p><p>L19: Was there any insight that arose from doing this? Especially in regards to the idea raised above – that the shape and overall structure have been ignored?</p><p>Intro:</p><p>Figure 2: It would be helpful to explicitly indicate which approach is used in the present paper.</p><p>Figure 7, top panel and caption for top panel: If I understand it correctly, this top panel is a composite of 100s of individual images but I am confused that I do not see many, if any, vertical edges where one set of samples gives way to another. Also, the jagged effect is not particularly obvious.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Below I list a number of small details in need of attention.</p><p>Figure 2c could improve, at least by making it larger, or by magnifying a few adjacent ommatidia to better appreciate the effect of not sharing a focal point.</p><p>Line 124: Figure 1b is not adequate to illustrate that insect eyes &quot;contain many hundreds or even thousands of lenses&quot;.</p><p>Line 183: &quot;an image can be generated that captures the full compound eye view.&quot; But isn't the point that an image cannot be generated because of the lack of a shared focal point, assuming then that the brain's internal representation of the field of view will be the product of a neural network parsing the output of the many ommatidia? The &quot;cone of influence&quot; of each ommatidium on the final image seems … premature. Neural circuits will combine inputs from ommatidia in ways that have not been worked out in the insect literature, as far as I know. The motivation for wanting to generate an image is unclear: is this for ease of the researchers, which are human, or what is the goal?</p><p>The &quot;raw ommatidial view buffer&quot; seems most useful to neuroscientists. The design of a neural network that composes your &quot;Display buffer&quot; from the &quot;raw ommatidial view buffer&quot; would be a most interesting research project, particularly for the opportunity to compare it with the existing, mapped circuit wiring diagram, or connectome, of the <italic>Drosophila</italic> optic lobes. You could discuss this. In other words, Line 240, &quot;These values are then re-projected into a human-interpretable view&quot; is the potential start of a fascinating research project in its own right, if one redefines this as &quot;fruit-fly interpretable view&quot;, as in, the brain of the fruit fly needs to make effective use of this information to successfully navigate the world.</p><p>Line 227: &quot;Currently CompoundRay implements only a simple texture lookup, interpolating the nearest pixels of the texture associated with the geometry intersected.&quot; What would be gained by, or what is lost now relative to, implementing the full ray tracing approach?</p><p>Line 247: &quot;Both pipelines share a common geometry acceleration structure and material shaders so as to save device memory.&quot; How is this possible? Can you hand-hold the reader a bit more to clarify how can both pipelines share the same GPU shaders? I may have missed a critical point.</p><p>Line 253: &quot;rendering using a traditional camera skips steps 1&quot;, you mean, your software can alternatively use traditional cameras in place of modeled ommatidia? I am not sure I get it.</p><p>Line 293: &quot;inhomogeneous ommatidial properties&quot;. An example you could cite is the eyes of the robber flies, Wardill et al., 2017 Curr Biol., where a frontal fovea with enlarged facets is reported.</p><p>Figure 6: seems to imply that overlap is not desirable? Overlap could allow for all kinds of spatial computations, from supraresolution to contrast enhancement to motion detection.</p><p>Line 309: the statements would apply only in the absence of a nervous system that would make full use of the ommatidial signals.</p><p>And, in general: aren't insect compound eyes known to present significant overlap in the field of view of each ommatidium relative to its neighbors? Figure 6c would then be the most realistic, if ignoring post-processing by the optic lobes?</p><p>Figure 7: qualitatively, the top rendered scene looks quite similar throughout. I am not able to notice much difference between the left and right ends. Figure 7 needs work to better convey your message, and to better establish or highlight the vertical relationship between the images in the top and middle and the plot at the bottom.</p><p>Line 357: &quot;using a genetic algorithm to ﬁnd the location&quot;, where are the technical details of this approach? Parameters? Framework used, or code? Is it your own code?</p><p>Line 440: &quot;On top of the potential biological insights CompoundRay may be able to offer&quot;, indeed, your paper would be so much stronger if you showed one such biological insight. It would then become a reference software for compound eye vision.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;CompoundRay, an open-source tool for high-speed and high-fidelity rendering of compound eyes&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Tirin Moore (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Line 234: the sentence starting with &quot;This will aid…&quot; makes claims that, first, aren't entirely clear, and second, seem speculative and belong to the discussion.</p><p>Figure 10: the legend is far too short. Needs more detail on what each panel is, particularly for the lower panels. Also, where in the methods is the neural network described? Could be referenced here. Also, &quot;network&quot; is not quite the right shorthand to describe a neural network. Was it a CNN? Or generically an ANN? Or a perceptron?</p><p>Line 504: where is the Materials and methods description for the &quot;4-layer fully-connected neural network&quot;?</p><p>Line 532: &quot;neural network was trained across 100 epochs three times&quot;, why? Why not 200 epochs, or 50, and 2 times, or 10? Why a batch size of 64? Also, these details are hardly worthy of opening the &quot;Experimental results&quot; section. Rather, they belong to the methods, alongside explanations and figures demonstrating why the set of parameters chosen are sensible and appropriate for the problem at hand.</p><p>Line 538, isn't the fact that a simple &quot;single eye&quot; design achieved a similar utility score concerning? Perhaps this reveals a disconnect between the test, which runs for 100 epochs (as plotted in Figure 10), and the real world, where an insect needs to take split-second decisions from the let go.</p><p>Line 565 reference to &quot;Figure 10 bottom left&quot; could really use the lettering of panels. In addition to a far lengthier and detailed explanation on what, exactly, is in that panel and how it was generated. And if these panels are as critical as they seem to be to evaluate the performance of the 3 eye models, then they ought to be bigger and far more richly annotated, to assist the reader in understanding what is being plotted.</p><p>Line 603: if you see opportunities for modeling the &quot;recently reported&quot; (2017) microsaccadic sampling in fruit flies, then why didn't you? Or is this the subject of a paper in preparation or under review elsewhere?</p><p>A question for the discussion: with its intrinsically distributed light sampling, compound eyes surely work not with frame-processing downstream circuits but rather event-based circuits, such as those of the dynamic vision sensor or silicon retina (Tobi Delbruck et al., various papers). Could you discuss the implications of asynchronous compound eye-based sensors, particularly when considering uneven ommatidial size which is not available in camera-based eyes such as in mammals or cephalopods? In a fast-moving animal such as a flying honeybee, asynchronous processing surely has advantages for motion detection and obstacle detection and avoidance, and scene recognition (i.e., flowers to land on), or better, to decode the waggle dance language of conspecifics.</p><p>Considering event-based computing could make for a much nicer discussion than the caveats of anthropomorphised, frame-based visual computing, or at least complement it well, outlining the intrinsic advantages of compound eyes.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.73893.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) All reviewers agree that the raw ommatidial view buffer aspect of the work is the strongest piece and with the most potential to support neuroscience experiments. Expanding on this and discussing the possible applications in simulating insect vision and e.g., interpreting the neural connectomes of insect visual systems is most appropriate. Discussion of the modeling of e.g., insect compound eyes with asymmetric ommatidia is a must; one such example could be robber flies.</p></disp-quote><p>Thank you for this comment. We have addressed each of the suggestions as follows:</p><p>1. To more adequately demonstrate the utility of the raw ommatidial view buffer a new section, “Example Experiment: <italic>Apis mellifera</italic> Visual Field Comparison” has been added. This offers a more detailed insight into the process of using CompoundRay for a simple experiment analysing the information content available to specific eye designs, posed as a comparative analysis between the arbitrarily-surfaced eyes capable of being supported in CompoundRay and two common approximations of insect visual systems.</p><p>2. We have also added an example experiment that demonstrates applications,</p><p>3. Introduced a paragraph to the Discussion that outlines immediate areas in which the tool could be used (e.g. asymmetric patterns in robber flies)</p><disp-quote content-type="editor-comment"><p>2) The motivation for the visualization for human observers is weak, and it is unclear of what use it can be for insect vision or vision research more generally. On surface, it seems a contradiction that the software goes to great lengths to model ommaditia with independent focal points, as necessary for the accurate modeling of each ommatidium, only to then project to a single nodal point to create a camera eye image, contrary to the main point of the work. A stronger motivation with explicit applications that aren't possible otherwise with this camera image are necessary. Otherwise, this aspect of the work ought to be downsized relative to the clearly innovative and useful raw ommaditial view buffer approach for compound eye vision research.</p></disp-quote><p>Thank you for the comment. We completely agree with your statement that raw ommatidial view buffer (or, more accurately, a vectorised representation of it) is the more important data generated by CompoundRay. Our intention with the discussion of human visualisation and biassing issues was to highlight, visually, the pitfalls that can occur when attempting to compare two eyes with dissimilar projection surfaces. In addition, we hoped to demonstrate the utility of visualising the insect view in a human-readable way for debugging and sanity-checking purposes, itself with the added caveat that this is susceptible to biases arising from anthropomorphisation as well as projection, and should thus be avoided. To this end, we have made extensive edits to the Methods, Results (including an example experiment), and Discussion which we hope makes this position clearer. Specifically our discussion of visualisation, and the associated cross-comparison issues have been re-framed to focus on the impacts these may have on comparing dissimilar eye designs, making more limited use of the human visualisation scenario as an explanatory example and referencing the work performed in the newly added Example Experiment section.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>More details as they emerged, comments overlap:</p><p>98 Do you really mean that each ray has a maximum recursion depth of 0? Did you implement illumination by a light source? If rays do not recurse or trace to light sources, all surfaces are 100% diffuse and ambient and ray casting is equivalent to polygon projection with local illumination. If no illumination by an independent light source is implemented, all surfaces are 100% ambient and ray casting is equivalent to polygon projection. Please clarify.</p></disp-quote><p>Yes, the reviewer is correct that CompoundRay currently uses a recursive depth of 0 (lines 176-179). As a first implementation our aim was to prove that arbitrary surface shapes could be rendered at the speed needed for modelling. More realistic light modelling could be introduced in later advances as outlined in the updated Discussion (line 545-547). Thus, there is an equivalency to polygon projection from each individual ommatidia, which is now discussed and explicitly stated in the paper (lines 136-137), along with reasoning in favour of ray-casting (lines 99-101), namely that it serves as a better platform for implementing new features (As an example, light-source simulation) should they need to be added in the future.</p><p>Also see “Changes Relating to Colour and Light Sampling” below.</p><disp-quote content-type="editor-comment"><p>64 – 67 The first two criteria both cover &quot;arbitrary ommatidia geometry&quot; which I think is one criterion.</p></disp-quote><p>These criteria are derived from previously published work, which has been re-worded to be more clearly the case.</p><disp-quote content-type="editor-comment"><p>65 Spectral sensitivity, which could be criterion 2 is not addressed as far as I understand. The renderer is purely RGB in 8-bit depth, correct? This is not discussed in section &quot;Criterion 2&quot; and could be a big obstacle for real applications. Is it hard to fix?</p></disp-quote><p>Yes, the renderer is currently RGB only (Please refer to “Changes Relating to Colour and Light Sampling”) but could be extended to include non-visible light, polarisation etc in the future (See new Discussion). 8-bit has been discussed already in response to point 3.</p><disp-quote content-type="editor-comment"><p>78 shadow maps are not more resource hungry than ray tracing with direct illumination, can you please explain better or remove?</p></disp-quote><p>This section has been removed as suggested.</p><disp-quote content-type="editor-comment"><p>84 interreflection of light can be implemented by e.g. calculating radiosity maps or doing sampling of rays cast from light sources. I do not see that CompoundRay is doing either, so it is not clear what the relationship to this work is? Can you please clarify?</p><p>Overall, I find the paragraph about ray-casting ray-tracing and the concrete relationship with CompoundRay unnecessarily confusing. As said before, I believe it would be better to explain the relevant differences of rendering methods only in how they relate to this project. i.e. how they produce an image (projection rendering by projecting polygons using painters algorithm, texture mapping and/ or colorize with local illumination model; ray-casting by identifying first intersection, texture mapping and/ or colorize with local illumination model, averaging over rays). Mention illumination, shadows (maps for each light in projection, testing for object intersection by tracing to light in ray-tracing) and global models (radiosity or photon sampling) only as possible extensions, not used in this work.</p></disp-quote><p>We agree that the previous description of Raycasting was confusing and thus have streamlined the Methods section to reduce confusion on the concrete relationship between ray-casting methods and CompoundRay. We have also made a number of small changes throughout the section entitled “The CompoundRay Software Pipeline” that further clarifies the rendering process. All discussion of advanced light modelling etc has now been downsized as suggested and moved to the Discussion and introduced as future extensions.</p><p>Again, more clarification are provided in “Changes Relating to Colour and Light Sampling”</p><disp-quote content-type="editor-comment"><p>124 – 128 state the above but the language and reasoning are unclear</p></disp-quote><p>We have now added reasoning for the of raycasting (lines 99-101) which combined with the newly streamlined Methods, and movement of description of advanced lighting models to the Discussion address the above comment.</p><disp-quote content-type="editor-comment"><p>227 – 229 eventually clarify that CompoundRay does not currently implement any illumination model or first order ray recursion to cast shadows, so the output would be equivalent to per ommatidium camera rendering and integration, this is way too late in the text as illumination and shading capabilities were used earlier to root for ray-casting for no practical reason.</p></disp-quote><p>Explicit description of the equivalency to polygon projection from each individual ommatidia has been added (lines 136-137) as well as explicit statement of recursive depth (lines 176-179). Again, as the focus here is on the geometry modelling of eye models advanced lighting models are considered future work and thus introduced in the updated Discussion.</p><p>Please refer to “Changes Relating to Colour and Light Sampling” for additional information.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p></disp-quote><disp-quote content-type="editor-comment"><p>L12-13: I think some expectation of why this might be relevant or interesting would be useful.</p></disp-quote><p>Additional explanatory text added (lines 8-9)</p><disp-quote content-type="editor-comment"><p>L19: Was there any insight that arose from doing this? Especially in regards to the idea raised above – that the shape and overall structure have been ignored?</p></disp-quote><p>We have added an example experiment in which we compared realistic vs simple eye designs in the task of small object localisation, which covers this in depth (lines 394-467).</p><p>We have also added a sentence to the abstract noting this addition.</p><disp-quote content-type="editor-comment"><p>Figure 2: It would be helpful to explicitly indicate which approach is used in the present paper.</p></disp-quote><p>The approach used in this paper is now included within the figure caption.</p><disp-quote content-type="editor-comment"><p>Figure 7, top panel and caption for top panel: If I understand it correctly, this top panel is a composite of 100s of individual images but I am confused that I do not see many, if any, vertical edges where one set of samples gives way to another. Also, the jagged effect is not particularly obvious.</p></disp-quote><p>This image has now been updated to better communicate the differences between the two views by performing a direct comparison between two points on the sampling curve rather than showing a composite of many points – this makes the difference significantly more visible.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Below I list a number of small details in need of attention.</p><p>Figure 2c could improve, at least by making it larger, or by magnifying a few adjacent ommatidia to better appreciate the effect of not sharing a focal point.</p></disp-quote><p>A magnification inset has been added as suggested.</p><disp-quote content-type="editor-comment"><p>Line 124: Figure 1b is not adequate to illustrate that insect eyes &quot;contain many hundreds or even thousands of lenses&quot;.</p></disp-quote><p>The subfigure has been changed to display only one insect eye (that of the Erbenochile), making the density of the lenses on the eye more clear, and including a zoom-section to the diagram of the ommatidium making the linkage between the diagram and the eye image explicit.</p><disp-quote content-type="editor-comment"><p>Line 183: &quot;an image can be generated that captures the full compound eye view.&quot; But isn't the point that an image cannot be generated because of the lack of a shared focal point, assuming then that the brain's internal representation of the field of view will be the product of a neural network parsing the output of the many ommatidia? The &quot;cone of influence&quot; of each ommatidium on the final image seems … premature. Neural circuits will combine inputs from ommatidia in ways that have not been worked out in the insect literature, as far as I know. The motivation for wanting to generate an image is unclear: is this for ease of the researchers, which are human, or what is the goal?</p></disp-quote><p>Yes, you are completely correct. Our motivation is to more accurately model the light that arrives to the insect eye ready for analysis and integration into computational models, as such when discussing the “cone of influence” of each ommatidium, we are explicitly describing the light that arrives at the lens. Lines 182-188 have been updated to explicitly state the motivation for creating an image (debugging), and our newly added example experiment (lines 394-467) shows how the realistic eye outputs can be used with artificial neural networks to compare eye designs for specific tasks. This addition exemplifies our intended usage of CompoundRay.</p><disp-quote content-type="editor-comment"><p>The &quot;raw ommatidial view buffer&quot; seems most useful to neuroscientists. The design of a neural network that composes your &quot;Display buffer&quot; from the &quot;raw ommatidial view buffer&quot; would be a most interesting research project, particularly for the opportunity to compare it with the existing, mapped circuit wiring diagram, or connectome, of the <italic>Drosophila</italic> optic lobes. You could discuss this. In other words, Line 240, &quot;These values are then re-projected into a human-interpretable view&quot; is the potential start of a fascinating research project in its own right, if one redefines this as &quot;fruit-fly interpretable view&quot;, as in, the brain of the fruit fly needs to make effective use of this information to successfully navigate the world.</p></disp-quote><p>A new section (outlined below in the “Changes Demonstrating Renderer Use”) has been introduced to give an example of the render’s usage.</p><disp-quote content-type="editor-comment"><p>Line 227: &quot;Currently CompoundRay implements only a simple texture lookup, interpolating the nearest pixels of the texture associated with the geometry intersected.&quot; What would be gained by, or what is lost now relative to, implementing the full ray tracing approach?</p></disp-quote><p>The current approach allows arbitrary shaped eyes to be modelled and rendered in a fast way while maintaining the simplicity of the ray-casting approach, which is easier to expand in the future. In essence, compoundRay simulates pinhole-cameras as a regular polygon-projection approach would but doesn’t require abuse of the whole vertex-rendering pipeline to render these multiple cameras. Full ray-tracing could more accurately model light transport through the environments which we introduce as future work in the Discussion. Also see the section below “Changes Relating to Colour and Lighting” below.</p><disp-quote content-type="editor-comment"><p>Line 247: &quot;Both pipelines share a common geometry acceleration structure and material shaders so as to save device memory.&quot; How is this possible? Can you hand-hold the reader a bit more to clarify how can both pipelines share the same GPU shaders? I may have missed a critical point.</p></disp-quote><p>This has now been explained (line 264).</p><disp-quote content-type="editor-comment"><p>Line 253: &quot;rendering using a traditional camera skips steps 1&quot;, you mean, your software can alternatively use traditional cameras in place of modeled ommatidia? I am not sure I get it.</p></disp-quote><p>Yes, for users to interact with the 3D environment for debugging and analysis purposes CompoundRay allows use of normal cameras as well as ommnitidial-based renderings. We have added a clarification of this point to the text (line 254-255).</p><disp-quote content-type="editor-comment"><p>Line 293: &quot;inhomogeneous ommatidial properties&quot;. An example you could cite is the eyes of the robber flies, Wardill et al. 2017 Curr Biol., where a frontal fovea with enlarged facets is reported.</p></disp-quote><p>Thanks for this suggestion. We have added references as requested (lines 298 and 490)</p><disp-quote content-type="editor-comment"><p>Figure 6: seems to imply that overlap is not desirable? Overlap could allow for all kinds of spatial computations, from supraresolution to contrast enhancement to motion detection.</p></disp-quote><p>The caption has been updated to specify that in particular, being able to leverage the most useful parts of homogeneous configurations (i.e. some overlap) is the benefit of a heterogeneous eye design.</p><disp-quote content-type="editor-comment"><p>Line 309: the statements would apply only in the absence of a nervous system that would make full use of the ommatidial signals.</p><p>And, in general: aren't insect compound eyes known to present significant overlap in the field of view of each ommatidium relative to its neighbors? Figure 6c would then be the most realistic, if ignoring post-processing by the optic lobes?</p></disp-quote><p>Thank you for this feedback, and you are correct in pointing out the differences between the data shown and real insect eyes. However, Figures5 and 6 are intended not to show accurate compound eye models per se, but rather to show the different information that is provided by different geometries (Figure 5) and varying omnitidial properties (Figure 6). This analysis is intended to convince users of the necessity for this level of detail rather than speak directly to actual insect eyes. However, these are precisely the type of research questions that we have designed CompoundRay to be used to investigate. In particular, figure 6c is modelled as being able to leverage the potentially useful aspects of both the under- and over-sampled views in subfigures a and b, however the exact aspects that are leveraged are left as a simplified selection (just packing cones of vision) to better demonstrate the possibilities present within heterogeneous eye designs.</p><disp-quote content-type="editor-comment"><p>Figure 7: qualitatively, the top rendered scene looks quite similar throughout. I am not able to notice much difference between the left and right ends. Figure 7 needs work to better convey your message, and to better establish or highlight the vertical relationship between the images in the top and middle and the plot at the bottom.</p></disp-quote><p>The image is now a simple split between the 1 and 70-sampled images, with their exact locations on the sampling graph highlighted. This exemplifies the difference between the two and puts the graph into better context.</p><disp-quote content-type="editor-comment"><p>Line 357: &quot;using a genetic algorithm to ﬁnd the location&quot;, where are the technical details of this approach? Parameters? Framework used, or code? Is it your own code?</p></disp-quote><p>We have removed the reference to a genetic algorithm from the text as any optimisation or brute-force search would allow this parameter to be optimised. We feel that our new description better conveys the task to the reader without drastically with specific tools. New text in lines (368-373)</p><disp-quote content-type="editor-comment"><p>Line 440: &quot;On top of the potential biological insights CompoundRay may be able to offer&quot;, indeed, your paper would be so much stronger if you showed one such biological insight. It would then become a reference software for compound eye vision.</p></disp-quote><p>Indeed, we have added an entirely new example experiment which shows CompoundRay being used with modern deep-learning tools to investigate small object detection of different eye designs. We feel that this addresses the issue raised by the reviewer and enhances the paper. Thanks for the suggestion. (See the introduced new section outlined below in the “Changes Demonstrating Renderer Use”).</p><p>Changes Relating to Colour and Light Sampling</p><p>Previous discussion of colour and light sampling was over-complicated and the reasons for differing choices were not justified. We have changed this by removing broad discussion of light transport simulation and instead directly describing what CompoundRay does, particularly within the context of it essentially simulating multiple individual pinhole-cameras in parallel. The reasoning for the selection of ray-casting methods as a basis for the system are mentioned within the methods and discussions sections, and examples of further light and colour sampling methods that could be built on top of CompoundRay are placed in the “Future Extensions” section.</p><p>Changes Demonstrating Renderer Use</p><p>Following the requests for a specific demonstration of the renderer’s usage we have added an example study that comprises of a comparative analysis of the difference between a more complete model of an insect eye (that includes the surface shape), and two simplified derivatives based on the ideas of pairs of or singular panoramic image sampling methods. This study uses <italic>Apis mellifera</italic> visual systems as a base, as there already exists a collection of high-fidelity 3D scans of these eyes, one of which was chosen here.</p><p>The example study focuses on explaining to the reader the methodology behind using CompoundRay in to perform these comparisons and offers an example path to follow to perform analysis. Basic qualitative analysis is performed on the results, highlighting some surprising behaviours and points of interest.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Figure 10: the legend is far too short. Needs more detail on what each panel is, particularly for the lower panels. Also, where in the methods is the neural network described? Could be referenced here. Also, &quot;network&quot; is not quite the right shorthand to describe a neural network. Was it a CNN? Or generically an ANN? Or a perceptron?</p><p>Line 504: where is the Materials and methods description for the &quot;4-layer fully-connected neural network&quot;?</p><p>Line 532: &quot;neural network was trained across 100 epochs three times&quot;, why? Why not 200 epochs, or 50, and 2 times, or 10? Why a batch size of 64? Also, these details are hardly worthy of opening the &quot;Experimental results&quot; section. Rather, they belong to the methods, alongside explanations and figures demonstrating why the set of parameters chosen are sensible and appropriate for the problem at hand.</p><p>Line 565 reference to &quot;Figure 10 bottom left&quot; could really use the lettering of panels. In addition to a far lengthier and detailed explanation on what, exactly, is in that panel and how it was generated. And if these panels are as critical as they seem to be to evaluate the performance of the 3 eye models, then they ought to be bigger and far more richly annotated, to assist the reader in understanding what is being plotted.</p></disp-quote><p>The above comments are addressed together as they relate to the same section and figure. We have taken a number of steps. Firstly, we have separated the previous Figure 10 into 2 figures (Figures5 and 11). Figure 5 describes the experimental procedure only and has a much fuller caption as requested. This supports a new dedicated section in the Methods (Example Inter-Eye Comparison Method, diff pdf lines 280-354) that details the methodology, parameters used etc. Finally, Figure 11 now presents just the example results which links directly to the Results section – we think everything flows better now. Overall, we believe that the new text addresses all of the concerns raised. We appreciate the opportunity to make this clearer for readers.</p><p>We have made the main results panels larger and added some annotations to highlight the primary regions of space in which there is a difference in information content for the differing eye designs. However, as outlined above the main contribution of this example experiment is the demonstration of the utility of CompoundRay for modern data-intensive analysis, the pilot data plots are thus intended to inspire users and not define new findings per se.</p><disp-quote content-type="editor-comment"><p>Line 538, isn't the fact that a simple &quot;single eye&quot; design achieved a similar utility score concerning? Perhaps this reveals a disconnect between the test, which runs for 100 epochs (as plotted in Figure 10), and the real world, where an insect needs to take split-second decisions from the let go.</p></disp-quote><p>We did similarly note that there seems little variation in the eye designs and we are sure that this is not related to epoch size. We have added an additional explanation on epoch size selection (diff pdf lines 352-354), and have reframed the task to reflect the use of the neural network (diff pdf lines 280-354) which should clarify this for readers. Specifically, we are learning a close-to-optimal mapping between visual field and position over the course of a number of epochs (here 100), but at test, this mapping is simply used in a single-shot approximation of this mapping function, as is the case with all feedforward neural networks. Thus, the epoch size has no influence at the test time outside of reducing the noise present in the mapping of visual input to relative position.</p><disp-quote content-type="editor-comment"><p>Line 603: if you see opportunities for modeling the &quot;recently reported&quot; (2017) microsaccadic sampling in fruit flies, then why didn't you? Or is this the subject of a paper in preparation or under review elsewhere?</p></disp-quote><p>We have not yet modelled this work, nor is it submitted elsewhere. We see the primary contribution of the paper as a new method that will facilitate multiple new studies of various aspects of compound eyes. We intentionally focussed on the contributions of the methods to keep the focus of the paper clear. This reflects the guidance for Tools and Resources article types: “Tools and Resources articles do not have to report major new biological insights or mechanisms, but it must be clear that they will enable such advances to take place, for example, through exploratory or proof-of-concept experiments. Specifically, submissions will be assessed in terms of their potential to facilitate experiments that address problems that to date have been challenging or even intractable.” We feel that adding more analysis from the pilot study will adversely shift the focus away from Methods and muddy the clarity for the readers.</p><p>We note that we are now initiating follow-up work that will use CompoundRay in a far more detailed study of compound eye designs which will be the subject of a separate publication in the future focused on the new insights gained. Inclusion of this work in this manuscript would make it overly long and again deviate from the communication of the Method. Rather we included this text to highlight one of the interesting research topics that this tool could be used to study.</p><disp-quote content-type="editor-comment"><p>A question for the discussion: with its intrinsically distributed light sampling, compound eyes surely work not with frame-processing downstream circuits but rather event-based circuits, such as those of the dynamic vision sensor or silicon retina (Tobi Delbruck et al., various papers). Could you discuss the implications of asynchronous compound eye-based sensors, particularly when considering uneven ommatidial size which is not available in camera-based eyes such as in mammals or cephalopods? In a fast-moving animal such as a flying honeybee, asynchronous processing surely has advantages for motion detection and obstacle detection and avoidance, and scene recognition (i.e., flowers to land on), or better, to decode the waggle dance language of conspecifics.</p><p>Considering event-based computing could make for a much nicer discussion than the caveats of anthropomorphised, frame-based visual computing, or at least complement it well, outlining the intrinsic advantages of compound eyes.</p></disp-quote><p>Thank you for pointing out this potential usage of CompoundRay. Indeed, we agree that CompoundRay could be an excellent input for a simulated DVS pipeline due to its speed and reconfigurability. We have now added an extra sentence which makes this potential usage explicit. This potential technological usage compliments well some of the biological research directions mentioned in the previous sentence. (diff pdf lines 626-630)</p></body></sub-article></article>