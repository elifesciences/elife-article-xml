<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">89929</article-id><article-id pub-id-type="doi">10.7554/eLife.89929</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>A neuromorphic model of active vision shows how spatiotemporal encoding in lobula neurons can aid pattern recognition in bees</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7612-6465</contrib-id><email>maboudi@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Roper</surname><given-names>Mark</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1135-6187</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author"><name><surname>Guiraud</surname><given-names>Marie-Geneviève</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5843-9188</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund9"/><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Juusola</surname><given-names>Mikko</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4428-5330</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Chittka</surname><given-names>Lars</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8153-1732</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Marshall</surname><given-names>James AR</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1506-167X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf3"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Department of Computer Science, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>School of Biosciences, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Neuroscience Institute, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/026zzn846</institution-id><institution>School of Biological and Behavioural Sciences, Queen Mary University of London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution>Drone Development Lab, Ben Thorns Ltd</institution><addr-line><named-content content-type="city">Colchester</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sf06y89</institution-id><institution>School of Natural Sciences, Macquarie University</institution></institution-wrap><addr-line><named-content content-type="city">North Ryde</named-content></addr-line><country>Australia</country></aff><aff id="aff7"><label>7</label><institution>Opteran Technologies Ltd</institution><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Louis</surname><given-names>Matthieu</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California, Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Desplan</surname><given-names>Claude</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>01</day><month>07</month><year>2025</year></pub-date><volume>14</volume><elocation-id>e89929</elocation-id><history><date date-type="received" iso-8601-date="2023-06-05"><day>05</day><month>06</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2025-05-13"><day>13</day><month>05</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2023-06-06"><day>06</day><month>06</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="http://doi.org/10.1101/2023.06.04.543620"/></event></pub-history><permissions><copyright-statement>© 2025, MaBouDi et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>MaBouDi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-89929-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-89929-figures-v1.pdf"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/elife.106332" id="ra1"/><abstract><p>Bees’ remarkable visual learning abilities make them ideal for studying active information acquisition and representation. Here, we develop a biologically inspired model to examine how flight behaviours during visual scanning shape neural representation in the insect brain, exploring the interplay between scanning behaviour, neural connectivity, and visual encoding efficiency. Incorporating non-associative learning—adaptive changes without reinforcement—and exposing the model to sequential natural images during scanning, we obtain results that closely match neurobiological observations. Active scanning and non-associative learning dynamically shape neural activity, optimising information flow and representation. Lobula neurons, crucial for visual integration, self-organise into orientation-selective cells with sparse, decorrelated responses to orthogonal bar movements. They encode a range of orientations, biased by input speed and contrast, suggesting co-evolution with scanning behaviour to enhance visual representation and support efficient coding. To assess the significance of this spatiotemporal coding, we extend the model with circuitry analogous to the mushroom body, a region linked to associative learning. The model demonstrates robust performance in pattern recognition, implying a similar encoding mechanism in insects. Integrating behavioural, neurobiological, and computational insights, this study highlights how spatiotemporal coding in the lobula efficiently compresses visual features, offering broader insights into active vision strategies and bio-inspired automation.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>image statistics</kwd><kwd>lobula</kwd><kwd>mushroom bodies</kwd><kwd>non-associative learning</kwd><kwd>scanning behaviour</kwd><kwd>visual recognition</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGP0022/2014</award-id><principal-award-recipient><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><name><surname>Roper</surname><given-names>Mark</given-names></name><name><surname>Guiraud</surname><given-names>Marie-Geneviève</given-names></name><name><surname>Chittka</surname><given-names>Lars</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/Poo6094/1</award-id><principal-award-recipient><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><name><surname>Juusola</surname><given-names>Mikko</given-names></name><name><surname>Chittka</surname><given-names>Lars</given-names></name><name><surname>Marshall</surname><given-names>James AR</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100018693</institution-id><institution>HORIZON EUROPE Framework Programme</institution></institution-wrap></funding-source><award-id>NimbleAI</award-id><principal-award-recipient><name><surname>Chittka</surname><given-names>Lars</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/F012071/1</award-id><principal-award-recipient><name><surname>Juusola</surname><given-names>Mikko</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/X006247/1</award-id><principal-award-recipient><name><surname>Juusola</surname><given-names>Mikko</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/X019705/1</award-id><principal-award-recipient><name><surname>Juusola</surname><given-names>Mikko</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>RPG-2024-016</award-id><principal-award-recipient><name><surname>Juusola</surname><given-names>Mikko</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>DP230100006</award-id><principal-award-recipient><name><surname>Guiraud</surname><given-names>Marie-Geneviève</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>DP210100740</award-id><principal-award-recipient><name><surname>Guiraud</surname><given-names>Marie-Geneviève</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100011730</institution-id><institution>Templeton World Charity Foundation</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.54224/20539</award-id><principal-award-recipient><name><surname>Guiraud</surname><given-names>Marie-Geneviève</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Active vision dynamically refines spatiotemporal neural representations, optimising visual processing through scanning behaviour and non-associative learning, providing insights into efficient sensory encoding in dynamic environments.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Bees are capable of remarkable cognitive feats, particularly in visual learning (<xref ref-type="bibr" rid="bib22">Chittka, 2022</xref>; <xref ref-type="bibr" rid="bib135">Srinivasan, 2010</xref>; <xref ref-type="bibr" rid="bib141">Turner, 1911</xref>; <xref ref-type="bibr" rid="bib150">von Frisch, 1914</xref>; <xref ref-type="bibr" rid="bib154">Wehner, 1967</xref>). They can not only learn to associate a colour or orientation of a bar with reward (<xref ref-type="bibr" rid="bib35">Dyer et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Guiraud et al., 2025b</xref>; <xref ref-type="bibr" rid="bib85">MaBouDi et al., 2020b</xref>; <xref ref-type="bibr" rid="bib134">Srinivasan, 1994</xref>; <xref ref-type="bibr" rid="bib136">Stach et al., 2004</xref>) but are also able to identify specific features to categorise visual patterns, by finding the relevant stimuli properties (<xref ref-type="bibr" rid="bib11">Benard et al., 2006</xref>; <xref ref-type="bibr" rid="bib51">Guiraud et al., 2025a</xref>; <xref ref-type="bibr" rid="bib136">Stach et al., 2004</xref>). Furthermore, bees have demonstrated the capacity to grasp abstract concepts (<xref ref-type="bibr" rid="bib8">Avarguès-Weber et al., 2011</xref>; <xref ref-type="bibr" rid="bib46">Giurfa et al., 2001</xref>; <xref ref-type="bibr" rid="bib49">Guiraud et al., 2018</xref>; <xref ref-type="bibr" rid="bib86">MaBouDi et al., 2020c</xref>; <xref ref-type="bibr" rid="bib100">Menzel, 2012</xref>) and solve numerosity tasks by sequentially scanning the elements within a stimulus (<xref ref-type="bibr" rid="bib84">MaBouDi et al., 2020a</xref>). These exceptional capabilities position bees as a valuable animal model for investigating the principles of visual learning through the analysis of their behavioural responses (<xref ref-type="bibr" rid="bib99">Menzel and Giurfa, 2006</xref>; <xref ref-type="bibr" rid="bib135">Srinivasan, 2010</xref>). Nevertheless, it remains unclear how bees, despite their supposedly low visual acuity (<xref ref-type="bibr" rid="bib48">Gribakin, 1975</xref>; <xref ref-type="bibr" rid="bib131">Spaethe and Chittka, 2003</xref>; <xref ref-type="bibr" rid="bib133">Srinivasan and Lehrer, 1988</xref>) and limited neural resources, recognise complex patterns and perceive the intricacies of the natural world encountered during foraging (<xref ref-type="bibr" rid="bib20">Chittka and Niven, 2009</xref>; <xref ref-type="bibr" rid="bib47">Giurfa, 2013</xref>).</p><p>The natural scene that animals encounter is structured differently from random or artificial ones (<xref ref-type="bibr" rid="bib23">Clark et al., 2014</xref>; <xref ref-type="bibr" rid="bib116">Ruderman and Bialek, 1994</xref>; <xref ref-type="bibr" rid="bib126">Simoncelli and Olshausen, 2001</xref>; <xref ref-type="bibr" rid="bib161">Zimmermann et al., 2018</xref>). It has been hypothesised that visual sensory neurons evolve to exploit statistical regularities in natural scenes, efficiently encoding information through their spatiotemporal structures (<xref ref-type="bibr" rid="bib10">Barlow, 1961</xref>). Over evolutionary time, insect visual neurons have developed mechanisms that provide robust and efficient responses to naturalistic inputs (<xref ref-type="bibr" rid="bib33">Dyakova et al., 2019</xref>; <xref ref-type="bibr" rid="bib31">Dyakova et al., 2015</xref>; <xref ref-type="bibr" rid="bib32">Dyakova and Nordström, 2017</xref>; <xref ref-type="bibr" rid="bib69">Juusola et al., 2025</xref>; <xref ref-type="bibr" rid="bib129">Song and Juusola, 2014</xref>; <xref ref-type="bibr" rid="bib160">Zheng et al., 2006</xref>). For instance, <xref ref-type="bibr" rid="bib129">Song and Juusola, 2014</xref> showed that fly photoreceptors extract more information from naturalistic time series than from artificial stimuli or white noise, yielding stronger responses with a higher signal-to-noise ratio (<xref ref-type="bibr" rid="bib129">Song and Juusola, 2014</xref>). Additionally, numerous studies have demonstrated that insect sensory pathways and their associated behaviours dynamically adapt to varying environmental conditions, adjusting their responses based on input parameters such as contrast, spatial frequency, and spatiotemporal correlations (<xref ref-type="bibr" rid="bib6">Arenz et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Brinkworth and O’Carroll, 2009</xref>; <xref ref-type="bibr" rid="bib23">Clark et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Dyakova et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Dyakova and Nordström, 2017</xref>; <xref ref-type="bibr" rid="bib69">Juusola et al., 2025</xref>; <xref ref-type="bibr" rid="bib68">Juusola and Song, 2017</xref>; <xref ref-type="bibr" rid="bib120">Schwegmann et al., 2014</xref>; <xref ref-type="bibr" rid="bib123">Serbe et al., 2016</xref>; <xref ref-type="bibr" rid="bib129">Song and Juusola, 2014</xref>; <xref ref-type="bibr" rid="bib129">Song and Juusola, 2014</xref>; <xref ref-type="bibr" rid="bib144">van Hateren, 1997</xref>; <xref ref-type="bibr" rid="bib143">van Hateren, 1992</xref>). Experience-dependent adaptation has been observed in fly photoreceptors and motion-sensitive neurons in the lobula plate, enabling efficient visual processing under varying conditions. For instance, photoreceptors adapt their response dynamics to different light intensities, optimising sensitivity to natural stimuli (<xref ref-type="bibr" rid="bib64">Juusola and Hardie, 2001a</xref>; <xref ref-type="bibr" rid="bib65">Juusola and Hardie, 2001b</xref>; <xref ref-type="bibr" rid="bib66">Juusola and de Polavieja, 2003</xref>). Similarly, motion-sensitive neurons such as the H1 neuron adjust their response properties based on prior motion exposure, enhancing motion detection in dynamic environments (<xref ref-type="bibr" rid="bib92">Maddess et al., 1985</xref>). This dynamic plasticity allows insects to process ecologically relevant information in real time. However, the precise neural mechanisms underlying natural scene processing remain elusive and require further investigation. Here, we examine how insect visual circuitry has adapted to regularities in natural scenes, focusing on the efficient coding strategies and robust response mechanisms that enhance visual pattern recognition.</p><p>In animal vision, active sampling strategies—wherein animals actively scan their environment to extract visual information over time—are widely observed across species (<xref ref-type="bibr" rid="bib74">Land, 1999</xref>; <xref ref-type="bibr" rid="bib75">Land and Nilsson, 2012</xref>; <xref ref-type="bibr" rid="bib125">Severance and Washburn, 1907</xref>; <xref ref-type="bibr" rid="bib145">Varella et al., 2024</xref>; <xref ref-type="bibr" rid="bib151">Washburn, 1908</xref>; <xref ref-type="bibr" rid="bib152">Washburn, 1916</xref>; <xref ref-type="bibr" rid="bib157">Yarbus, 1967</xref>). Primates employ eye movements, including saccades and microsaccades, to enhance fine spatial resolution and improve the encoding of natural stimuli (<xref ref-type="bibr" rid="bib3">Anderson et al., 2020</xref>; <xref ref-type="bibr" rid="bib74">Land, 1999</xref>; <xref ref-type="bibr" rid="bib103">Näher et al., 2023</xref>; <xref ref-type="bibr" rid="bib114">Rucci et al., 2007</xref>; <xref ref-type="bibr" rid="bib115">Rucci and Victor, 2015</xref>). Similarly, insects utilise active vision strategies, incorporating characteristic head and body movements or specific approach trajectories to optimise visual processing during behavioural tasks (<xref ref-type="bibr" rid="bib12">Bertrand et al., 2021</xref>; <xref ref-type="bibr" rid="bib21">Chittka and Skorupski, 2017</xref>; <xref ref-type="bibr" rid="bib28">Dawkins and Woodington, 2000</xref>; <xref ref-type="bibr" rid="bib36">Egelhaaf et al., 2009</xref>; <xref ref-type="bibr" rid="bib73">Land, 1973</xref>; <xref ref-type="bibr" rid="bib75">Land and Nilsson, 2012</xref>; <xref ref-type="bibr" rid="bib76">Langridge et al., 2021</xref>; <xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>). Recent studies have shown that <italic>Drosophila</italic> generate photomechanical photoreceptor microsaccades and can move their retinas to stabilise their retinal images, achieving hyperacute vision and enhancing depth perception (<xref ref-type="bibr" rid="bib38">Fenk et al., 2022</xref>; <xref ref-type="bibr" rid="bib56">Hardie and Franze, 2012</xref>; <xref ref-type="bibr" rid="bib67">Juusola et al., 2017</xref>; <xref ref-type="bibr" rid="bib71">Kemppainen et al., 2022b</xref>). Likewise, honeybee vision may require sequential sampling and integration of colour information due to their limited ability to discriminate between similar hues in brief flashes (&lt;50 ms) (<xref ref-type="bibr" rid="bib104">Nityananda et al., 2014</xref>). To overcome this constraint, bees engage in systematic scanning movements, continuously sampling their surroundings to construct an internal neural representation of their environment (<xref ref-type="bibr" rid="bib13">Boeddeker et al., 2015</xref>; <xref ref-type="bibr" rid="bib25">Collett et al., 1993</xref>; <xref ref-type="bibr" rid="bib30">Doussot et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Guiraud et al., 2018</xref>; <xref ref-type="bibr" rid="bib70">Kemppainen et al., 2022a</xref>; <xref ref-type="bibr" rid="bib76">Langridge et al., 2021</xref>; <xref ref-type="bibr" rid="bib77">Lehrer and Collett, 1994</xref>; <xref ref-type="bibr" rid="bib84">MaBouDi et al., 2020a</xref>; <xref ref-type="bibr" rid="bib155">Werner et al., 2016</xref>). For instance, bumblebees enumerate visual elements sequentially rather than processing them in parallel, suggesting a reliance on scanning behaviour for feature extraction parallel (<xref ref-type="bibr" rid="bib84">MaBouDi et al., 2020a</xref>), and their flight trajectories further indicate that they prioritise specific pattern regions before making a decision, rather than processing the entire pattern globally (<xref ref-type="bibr" rid="bib76">Langridge et al., 2021</xref>; <xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>). Given the low-resolution nature of compound eyes and the potentially reduced parallel processing capacity in insects compared to vertebrates, it is likely that bees rely on active vision and sequential sampling to construct a more robust neural representation of their environment (<xref ref-type="bibr" rid="bib21">Chittka and Skorupski, 2017</xref>; <xref ref-type="bibr" rid="bib104">Nityananda et al., 2014</xref>). These active strategies, akin to primate eye movements, play a crucial role in early visual processing, redundancy reduction, and efficient encoding of visual stimuli (<xref ref-type="bibr" rid="bib30">Doussot et al., 2020</xref>; <xref ref-type="bibr" rid="bib72">Kuang et al., 2012</xref>; <xref ref-type="bibr" rid="bib105">Odenthal et al., 2020</xref>). However, it remains poorly understood how such mechanisms allow bees to overcome representational constraints, detect visual regularities, and solve complex discrimination tasks. Understanding these strategies is key to uncovering the fundamental principles of insect vision and their broader implications for visual processing across biological and artificial systems.</p><p>Building on our previous work analysing bee flight paths during a simple visual task (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>), we further investigated the main circuit elements that contribute to active vision in achromatic pattern recognition, focusing on a simplified yet biologically plausible model. Our primary objective was to determine how bees’ scanning behaviour contributes to the functional organisation and connectivity of neurons in the visual lobes. We hypothesised that the bees’ scanning behaviours have adapted to sample complex visual features in a way that efficiently encodes them into spatiotemporal patterns of activity in the lobula neurons, facilitating distinct and specific representations that support learning in the bees’ compact brain. To test this, we developed a neuromorphic model of the bee optic lobes incorporating efficient coding principles via a novel model of non-associative plasticity. This model demonstrates how spatial scanning behaviour in response to naturalistic visual inputs has shaped the connectivity within the medulla (the second optic ganglion) to facilitate an efficient representation of these inputs in the lobula (the third optic ganglion). This efficiency is achieved through the self-organisation of a specific set of orientation-selective neurons in the lobula, highlighting the combined impact of scanning behaviour and non-associative learning on shaping the neural circuitry within the bees’ optic lobes.</p><p>To evaluate the proposed visual network, we enhance our visual processing framework by incorporating a secondary decision-making module inspired by insect associative learning mechanisms, grounded in previous neurobiological evidence (<xref ref-type="bibr" rid="bib19">Cassenaer and Laurent, 2012</xref>; <xref ref-type="bibr" rid="bib39">Fiala and Kaun, 2024</xref>; <xref ref-type="bibr" rid="bib40">Fisher et al., 2015</xref>; <xref ref-type="bibr" rid="bib78">Li et al., 2017</xref>; <xref ref-type="bibr" rid="bib107">Okada et al., 2007</xref>; <xref ref-type="bibr" rid="bib111">Paulk et al., 2009</xref>; <xref ref-type="bibr" rid="bib109">Paulk and Gronenberg, 2008a</xref>). Visual input and flight dynamics for the model were derived from our observations of bee behaviour during a visual discrimination task (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>). This allowed us to evaluate and test the hypothesis of active sampling from our model against real-world behaviour results (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>), as well as other published visual discrimination tasks performed by bees (<xref ref-type="bibr" rid="bib11">Benard et al., 2006</xref>; <xref ref-type="bibr" rid="bib34">Dyer et al., 2005</xref>; <xref ref-type="bibr" rid="bib50">Guiraud et al., 2022</xref>; <xref ref-type="bibr" rid="bib135">Srinivasan, 2010</xref>; <xref ref-type="bibr" rid="bib134">Srinivasan, 1994</xref>; <xref ref-type="bibr" rid="bib158">Zhang and Horridge, 1992</xref>). Furthermore, we conducted a detailed analysis comparing the neural response features emerging from our model with existing neurobiological findings (<xref ref-type="bibr" rid="bib62">James and Osorio, 1996</xref>; <xref ref-type="bibr" rid="bib110">Paulk et al., 2008b</xref>; <xref ref-type="bibr" rid="bib121">Seelig and Jayaraman, 2013</xref>; <xref ref-type="bibr" rid="bib93">Maddess and Yang, 1997</xref>). This alignment enhances the credibility of our model in capturing essential aspects of neural processing underlying active vision.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A bio-inspired neural network for active vision</title><p>To investigate how bee scanning behaviour optimises neural activity in the visual lobes and enhances visual information processing for efficient pattern recognition, we developed a neural network model inspired by the key morphological and functional characteristics of the bee brain (<xref ref-type="fig" rid="fig1">Figure 1A, C</xref>). The network abstracts the circuitry responsible for the initial processing of visual input in the bee’s lamina and medulla (the first and second optic ganglia). To mimic temporal encoding during scans (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), we introduced a progressive time delay of 1–5 ‘temporal instances’ between the outputs of medulla neurons and wide-field neurons in the lobula (the third optic ganglion; <xref ref-type="fig" rid="fig1">Figure 1D</xref>, see Methods). This temporal structuring facilitates sequential sampling of specific locations along the scan trajectory, gradually integrating visual information into a coherent internal representation that emerges as the final output of the lobula neurons.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Neural network of active vision inspired by neurobiology and flight dynamics of bees.</title><p>(<bold>A</bold>) The right side displays the front view of the bumblebee head showing the component eye and antenna. Left-hand side presents a schematic view of the bee’s brain regions. Part of neural pathways from the retina to the mushroom bodies is also represented. Labels: AL, antennal lobe; LH, lateral horn; CC, central complex; La, lamina; Me, medulla; Lo, lobula; MB, mushroom body. Figure was designed by Alice Bridges. (<bold>B</bold>) A representation of the modelled bee’s scanning behaviour of a flower demonstrating how a sequence of patches project to the simulated bee’s eye with lateral movement from left to right. Below are five image patches sampled by the simulated bee. (<bold>C</bold>) Representation of the neural network model of active vision inspired by micromorphology of the bee brain that underlie learning, memory, and experience-dependent control of behaviour. The photoreceptors located in the eye are excited by the input pattern. The activities of photoreceptors change the membrane potential of a neuron in the next layer, lamina. The lamina neurons send signals (through <italic>W</italic> connectivity matrix) to the medulla neurons to generate spikes in this layer. Each wide-field lobula neuron integrates the synaptic output of five small-field medulla neurons. The lobula neurons are laterally inhibited by local lobula interconnections (via <italic>Q</italic> connectivity matrix). Lobula neurons project their axons into the mushroom body, forming connections with Kenyon cells (KCs) through a randomly weighted connectivity matrix, <italic>S</italic>. The KCs all connect to a single mushroom body output neuron (MBON) through random synaptic connections <italic>D</italic>. A single reinforcement neuron (yellow neuron) modulates the synaptic weights between KCs and MBON by simulating the release of octopamine or dopamine when presented with specific visual stimuli (see Methods). (<bold>D</bold>) A temporal coding model that is proposed as the connectivity between medulla and lobula neurons. Each matrix shows the inhibitory (blue) and excitatory (red) connectivity between lamina neurons to a medulla neuron at a given time delay. In this model, the five small-field medulla neurons that are activated by the locally visual input, at different times of scanning, send their activities to a wide-field lobula neuron with a synaptic delay such that the lobula neuron receives all medulla input signals at the same instance (i.e. in the presented simulation the lobula neuron is maximally activated by the black vertical bar passing across the visual field from the left to right. Each underlying medulla neuron encodes the vertical bar in a different location of the visual field).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig1-v1.tif"/></fig><p>Building on findings from bee scanning behaviour (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>), the model extracts image input in five sequential patches of 75 × 75 pixels, sampled at a speed of 0.1 m/s, corresponding to a lateral displacement of 15 pixels between consecutive patches (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; see Methods for details). The green pixel intensities of each patch modulate the membrane potentials of 5625 (75 × 75) grid photoreceptors within the simulated bee’s single eye. These photoreceptor responses converge onto 625 lamina neurons via recurrent neural connectivity, providing a feedforward mechanism for transferring visual information. The lamina neurons then project to 250 small-field medulla neurons through a simple feedforward pathway (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, see Methods).</p><p>For each of the five sequential patches that compose a full scan, medulla neuron responses are computed using a spiking neural model. These responses are progressively integrated into the synapses of their corresponding lobula neuron with a structured time delay. As depicted in <xref ref-type="fig" rid="fig1">Figure 1D</xref>, the synaptic weights dynamically encode the visual information at different temporal instances (T, 2T, 3T, 4T, and 5T), effectively aligning sequentially sampled spatial information into a temporally coherent representation. This ensures that the lobula neuron accumulates and processes the underlying medulla input signals at a synchronised time point, mirroring mechanisms that may occur in biological systems (see Discussion). Additionally, lateral inhibitory connections (red connections in <xref ref-type="fig" rid="fig1">Figure 1C</xref>) are proposed between lobula neurons to reduce correlation between them, enhancing redundancy reduction in the process.</p><p>It is important to note that this proposed spatiotemporal coding is a simplification. In the bee brain, similar processes are likely mediated through dendritic and synaptic latencies, as well as intermediate neuron transmission within the medulla, influenced by non-associative learning in the visual lobe (<xref ref-type="fig" rid="fig1">Figure 1C, D</xref>). We hypothesise that connectivity in the medulla and lobula can be refined through exposure to sequences of time-varying images, incorporating non-associative learning rules and efficient coding principles. These mechanisms are optimised and shaped through a generative learning process to align with the statistical properties of natural scenes, enhancing the system’s capacity for processing complex visual inputs (see next section) (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Neural responses of the simulated bee model to visual patterns.</title><p>(<bold>A</bold>) Top: each square in the matrix corresponds to a single time slice of the obtained spatiotemporal receptive field of a lobula neuron (5 × 10 lobula neurons) that emerged from non-associative learning in the visual lobes after exposing the model to images of flowers and nature scenes (see <xref ref-type="video" rid="video3">Video 3</xref>). Bottom: spatiotemporal receptive field of two example lobula neurons are visualised in the five-time delay slices of the matrices of synaptic connectivity between lamina and five medulla neurons (see <xref ref-type="fig" rid="fig1">Figure 1D</xref>). The lobula neuron integrates signals from these medulla neurons at each of five time periods as the simulated bees scan a pattern (time goes from left to right). Blue and red cells show inhibitory and excitatory synaptic connectivity, respectively. The first example lobula neuron (#1) encodes the 150° angled bar moving from lower left to the upper right of the visual field. The second example lobula neuron (#48) encodes the movement of the horizontal bar moving up in the visual field. (<bold>B</bold>) An example of an image sequence projected to the simulated bee’ eye with lateral movement from left to right. Below shows the five images patched sampled by the simulated bee. The right side presents the firing rate of all lobula neurons responding to the image sequence. The spatiotemporal receptive field of two highest active neurons to the image sequence are highlighted in purple. (<bold>C</bold>) The polar plot shows the average orientation selectivity of one example lobula neuron (#1) to differently angled bars moving across the visual field in a direction orthogonal to their axis (average of 50 simulations). This neuron is most sensitive to movement when the bar orientation is at 150°. (<bold>D</bold>) The spiking response of the lobula neuron to the preferred orientation raised as the contrast was increased, whereas the response of the lobula neuron to a non-preferred orientation is maintained irrespective of contrast. (<bold>E</bold>) The average velocity–sensitivity curve (± SEM) of the orientation-sensitive lobula neuron (#1) is obtained from the responses of the lobula neuron to optimal (angle of maximum sensitivity) moving stimuli presented to the model at different velocities. The red line shows the Gamma function fitted to the data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Emergence of receptive fields in Lobula neurons requires structured natural inputs.</title><p>(<bold>A</bold>) Spatiotemporal receptive fields of lobula neurons emerging from non-associative learning when the number of lobula neurons is set to 100 (see Video 2).The details follow those described in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. (<bold>B</bold>) Spatiotemporal receptive fields of 50 lobula neurons trained with shuffled natural images. The neurons fail to develop meaningful connections, resulting in random synaptic weight distributions, indicating that spatial coherence in training images is essential for efficient feature extraction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig2-figsupp1-v1.tif"/></fig></fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Effect of non-associative learning on lobula neuron activity and response sparseness.</title><p>(<bold>A</bold>) Correlation matrix of lobula neuron responses after training with natural images. The near-diagonal structure indicates that neurons develop distinct and strongly uncorrelated responses, suggesting an efficient, decorrelated representation of visual input. (<bold>B</bold>) Sparseness index of lobula neurons before and after training with different image sets. Before training, neural responses are broadly distributed. Training with shuffled natural images does not change the sparseness of lobula population, whereas training with natural images significantly increases response sparseness, indicating that exposure to structured visual inputs enhances efficient coding. Error bars represent SEM. Asterisks (*) indicate p-values &lt;0.05, while ‘n.s.’ denotes non-significant results.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig3-v1.tif"/></fig><p>The neural representation of the visual inputs was subsequently transmitted and processed in the mushroom body—the learning centre of the bee brain (<xref ref-type="bibr" rid="bib37">Ehmer and Gronenberg, 2002</xref>; <xref ref-type="bibr" rid="bib78">Li et al., 2017</xref>; <xref ref-type="bibr" rid="bib110">Paulk et al., 2008b</xref>; <xref ref-type="bibr" rid="bib109">Paulk and Gronenberg, 2008a</xref>; <xref ref-type="bibr" rid="bib118">Schmalz et al., 2022</xref>; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). To simplify the model, we incorporated a single mushroom body output neuron (MBON), whose firing rate reflects the simulated bee’s preference for a given visual input. By adjusting synaptic weights within the mushroom body, the network was trained to classify visual patterns as either positive (low MBON firing rates) or negative (high MBON firing rates; see Discussion). Following non-associative learning and extensive exposure to natural images, the entire network was trained and tested on various pattern recognition tasks commonly used in experimental studies (<xref ref-type="bibr" rid="bib11">Benard et al., 2006</xref>; <xref ref-type="bibr" rid="bib34">Dyer et al., 2005</xref>; <xref ref-type="bibr" rid="bib135">Srinivasan, 2010</xref>; <xref ref-type="bibr" rid="bib134">Srinivasan, 1994</xref>; <xref ref-type="bibr" rid="bib158">Zhang and Horridge, 1992</xref>), including the discrimination of ‘plus’ and ‘multiplication sign’ patterns, as previously examined in real bumblebees (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>; <xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Simulated bees’ performance in a pattern recognition task using different scanning strategies.</title><p>Twenty simulated bees, with random initial neuronal connectivity in mushroom bodies (see Methods) and a fixed connectivity in the visual lobe that were shaped from the non-associative learning, were trained to discriminate a <italic>plus</italic> from a <italic>multiplication</italic> symbol (100 random training exposures per pattern). The simulated bees scanned different regions of the patterns at different speeds. (<bold>A</bold>) Top and below panels show the five image patches sampled from the plus and multiplication symbols by simulated bees, respectively. It is assumed that the simulated bees scanned the lower half of the patterns with lateral movement from left to right with normal speed (0.1 m/s). (<bold>B</bold>) The plot shows the average responses of the mushroom body output neuron (MBON) to rewarding multiplication and punishing plus patterns during training procedure (multiplication symbol rewarding, producing an Octopamine release by the reinforcement neuron, and the plus symbol inducing a Dopamine release). This shows how the response of the MBON to the rewarding plus was decreased while its response to the punishing multiplication pattern was increased during the training. The MBON equally responded to both multiplication and plus before the training (at number of visits = 0). (<bold>C</bold>) The performance of the simulated bees in discriminating the right-angled plus and a 45° rotated version of the same cross (i.e. multiplication symbol) (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>; <xref ref-type="bibr" rid="bib134">Srinivasan, 1994</xref>), when the stimulated bees scanned different regions of the pattern (left corner, lower half, whole pattern) at different speeds: no speed 0.0 m/s (i.e. all medulla to lobula temporal slices observed the same visual input), normal speed at 0.1 m/s and fast speed at 0.3 m/s, and from a simulated distance of 2 cm from stimuli (default) and 10 cm (distal view). The optimal model parameters were for the stimulated bees at the default distance when only a local region of the pattern (bottom half or lower left quadrant) was scanned at a normal speed. (<bold>D</bold>) Mean performance (± SEM) of two groups of simulated bees in discriminating the plus from multiplication patterns when their inhibitory connectivity between lobula neurons were not modified by non-associative learning rules. Asterisks (*) indicate p-values &lt; 0.05, while ‘n.s.’ denotes non-significant results.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig4-v1.tif"/></fig><p>To evaluate the performance of the active vision model, we analysed MBON activity, which functions as a decision-making unit. A lower MBON response from its baseline activity to a particular pattern indicates preference, whereas a higher response suggests rejection. After multiple training trials through a novel associative learning (see Methods and Discussion), the MBON exhibited a distinct response pattern, with reduced activity towards the chosen visual stimulus and increased activity towards the rejected one. This suggests that the MBON plays a role akin to a decision neuron in pattern recognition tasks. Importantly, no reinforcement learning, or synaptic updates were applied during the testing phase, ensuring that the observed responses reflected the network’s learned capacity for visual discrimination rather than online adaptation.</p></sec><sec id="s2-2"><title>Non-associative learning shapes spatiotemporal coding in the lobula to align with the statistical features of natural scenes</title><p>The synaptic weights in the optic lobe were updated through exposure to natural images during the model’s lateral scanning process (see Methods, <xref ref-type="fig" rid="fig1">Figure 1B</xref>). While lamina-to-medulla connections are structured based on temporal coding (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), lobula neurons are configured to laterally inhibit each other, facilitating competitive interactions. Synaptic connections were updated using Oja’s implementation of Hebb’s rule (<xref ref-type="bibr" rid="bib106">Oja, 1982</xref>). Simultaneously, a symmetric inhibitory spike-timing-dependent plasticity (iSTDP) rule was applied to lateral inhibitory connections among lobula neurons (<xref ref-type="bibr" rid="bib149">Vogels et al., 2011</xref>). These local synaptic plasticity rules, which govern interactions between lamina, medulla, and lobula neurons, support non-associative learning—that is synaptic modifications occurring in the absence of reward (see Methods). Together, these plasticity mechanisms drive the network towards an efficient representation of visual input, reducing redundancy while preserving essential visual information.</p><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> illustrates the receptive fields of lobula neurons, which exhibit spatiotemporal orientation selectivity after training on 100 flower and natural images (comprising 50,000 time-varying image patches). Each square in the figure represents one of the 50 lobula neurons, with the heat map indicating the synaptic weights of the corresponding lamina neurons (connected via the medulla neurons). To aid interpretation, the lower portion of (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) provides examples of two individual lobula neurons, detailing their lamina synaptic weights for each of the five medulla neurons. For a more dynamic representation of these receptive fields over time, see <xref ref-type="video" rid="video1">Video 1</xref>. The receptive fields of lobula neurons are characterised by an elongated ‘on’ area (regions representing positive synaptic weights) adjacent to an antagonistic ‘off’ area (regions with negative synaptic weights). These regions are generally aligned along a specific orientation, and their balance changes dynamically across time-delayed instances of medulla responses. For instance, one lobula neuron responds most strongly to a 135° bar moving orthogonally to its ‘on’ or ‘off’ areas while exhibiting little or no response to other orientations (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The population of 50 lobula neurons demonstrates specificity to both orientation and direction, closely resembling neuronal responses observed in bees and other insects (<xref ref-type="bibr" rid="bib62">James and Osorio, 1996</xref>; <xref ref-type="bibr" rid="bib110">Paulk et al., 2008b</xref>; <xref ref-type="bibr" rid="bib121">Seelig and Jayaraman, 2013</xref>; <xref ref-type="bibr" rid="bib93">Maddess and Yang, 1997</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-89929-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Spatiotemporal dynamics of receptive fields in 50 lobula neurons emerging from non-associative learning and active scanning.</title><p>Each square in the matrix represents a single time slice of the spatiotemporal receptive field for a lobula neuron (5 × 10 array of neurons). These receptive fields illustrate the connectivity matrix between five medulla neurons and their corresponding lobula neuron, operating under a temporal coding structure. In this framework, each of the five medulla neurons sequentially transfers a portion of the visual input to the lobula neuron through excitatory (red) and inhibitory (blue) synaptic connections. These receptive fields develop within the visual lobes after the model is exposed to natural images, including flowers and scenery. As the simulated bee scans a visual pattern, lobula neurons dynamically integrate inputs from medulla neurons over time, forming a temporally structured neural representation of the visual scene.</p></caption></media><p>To illustrate how lobula neurons process natural visual inputs, <xref ref-type="fig" rid="fig2">Figure 2B</xref> depicts the sequence of image patches scanned during a simulated horizontal movement. The results show that only a small subset of lobula neurons respond at any given moment, indicating that their activity is decorrelated and relatively selective—an outcome of non-associative learning mechanisms in the visual lobe (see Discussion). Notably, the two most active lobula neurons captured distinct structural features of the flower petal: one neuron’s receptive field aligned with the left 45° edge of the petal, while another matched the right-angled edge. This suggests that the model effectively extracts distinct visual features with a minimal number of filters (lobula neurons).</p><p>To examine the selectivity of lobula neurons further, we analysed the spiking activity of a representative neuron tuned to a 150° orientation. As expected, the neuron showed maximal firing (26 spikes/sec) when presented with a 150° moving bar. It also exhibited moderate responses to a horizontal bar and a 120° moving bar (18 spikes/sec) but remained largely unresponsive to other orientations (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Consistent with experimental findings (<xref ref-type="bibr" rid="bib93">Maddess and Yang, 1997</xref>), the firing rate of lobula neurons increased with contrast at their preferred orientations, whereas responses to non-preferred orientations remained unchanged across contrast levels (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>Finally, <xref ref-type="fig" rid="fig2">Figure 2E</xref> highlights the velocity sensitivity of lobula neurons. Each neuron responds maximally at a specific velocity, demonstrating a tuning curve that aligns with known insect neural responses (<xref ref-type="bibr" rid="bib110">Paulk et al., 2008b</xref>; <xref ref-type="bibr" rid="bib93">Maddess and Yang, 1997</xref>). This reinforces that our model successfully captures key quantitative properties of lobula edge detector neurons, including their joint selectivity for orientation, contrast, and motion velocity.</p><p>The model demonstrates robustness in generating spatiotemporal receptive fields, even as the number of lobula neurons increases. Training the non-associative learning model with a larger lobula neuronal population while maintaining the same underlying structure from photoreceptors to the medulla, enhances the diversity of orientation-selective responses (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref> and <xref ref-type="video" rid="video2">Video 2</xref>). As the number of lobula neurons increases, their tuning properties become more distributed, enabling a finer and more precise encoding of different orientations and motion patterns. This scalability highlights the model’s ability to generalise its representation of natural scene statistics while achieving varying levels of resolution in visual encoding.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-89929-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Spatiotemporal dynamics of receptive fields in 100 lobula neurons emerging from non-associative learning and active scanning.</title><p>This follows the same structure as <xref ref-type="video" rid="video1">Video 1</xref>, depicting the receptive field evolution in a larger population of 100 lobula neurons under the temporal coding framework and non-associative learning.</p></caption></media></sec><sec id="s2-3"><title>Lobula neuron responses become sparse and decorrelated through non-associative learning with natural images</title><p>To assess the impact of training on the population activity of lobula neurons, we quantified their response sparsity and decorrelation before and after learning. <xref ref-type="fig" rid="fig3">Figure 3A</xref> presents the correlation matrix of lobula neurons in response to 10,000 sequential scans of natural images after training. The results reveal a highly decorrelated response pattern, with a strong diagonal structure indicating that each lobula neuron maintains a distinct response profile. This demonstrates that non-associative learning enhances both the selectivity and independence of neural representations, allowing the network to develop more efficient and diverse feature encoding.</p><p>Further supporting this, <xref ref-type="fig" rid="fig3">Figure 3B</xref> displays the sparseness index of lobula neurons under different training conditions. Before training, lobula neuron activity was broadly distributed, as reflected in the high sparseness index (see Methods). Training on natural images significantly reduced the sparseness index, indicating that lobula neurons developed a more selective and efficient coding scheme, where only a small subset of neurons responded to any given input. In contrast, training on shuffled natural images led to only a moderate reduction in sparseness, suggesting that the implemented local synaptic plasticity rules alone are insufficient for optimal feature encoding without exposure to structured natural inputs. Notably, while shuffling natural images preserves pixel intensity and overall distribution, it disrupts spatial correlations and higher-order structures present in natural scenes. Consequently, training on these shuffled datasets results in non-structured receptive fields, leading to broadly distributed and less selective coding in the lobula neurons (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>).</p><p>These findings underscore the role of non-associative learning in shaping neural representations, fostering both sparsity and decorrelation in lobula neurons. Such sparse coding is crucial for efficient sensory processing, as it minimises redundancy while preserving essential visual information. Moreover, it optimises metabolic efficiency in neural networks, aligning with coding strategies observed in biological visual systems (see Discussion).</p></sec><sec id="s2-4"><title>Active vision enhances visual discrimination through sequential scanning</title><p>To replicate bee behavioural findings reported in the literature (<xref ref-type="bibr" rid="bib11">Benard et al., 2006</xref>; <xref ref-type="bibr" rid="bib34">Dyer et al., 2005</xref>; <xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>; <xref ref-type="bibr" rid="bib134">Srinivasan, 1994</xref>; <xref ref-type="bibr" rid="bib135">Srinivasan, 2010</xref>; <xref ref-type="bibr" rid="bib158">Zhang and Horridge, 1992</xref>), we implemented computational plasticity within the mushroom body circuitry, leveraging sparse lobula neurons that emerge through non-associative learning—critical for encoding both appetitive and aversive values. Specifically, we incorporated classical spike-timing-dependent plasticity (STDP), modulated by dopamine, to regulate synaptic modifications between mushroom body Kenyon cells (KCs) and extrinsic MBONs in response to negative (unrewarded) patterns. Additionally, we introduced a novel STDP-based plasticity rule, modulated by octopamine (see Methods, Figure 9), which we hypothesise induces synaptic depression among KC–MBON connections in response to positive (rewarded) patterns. These plasticity mechanisms allowed us to explore synaptic dynamics underlying the discrimination of rewarded and non-rewarded stimuli (see Discussion) (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The effect of scanning behaviour on the spatiotemporal encoding of visual patterns in lobula neurons responses.</title><p>(<bold>A, B</bold>) Neural responses of simulated lobula neurons to different scanning conditions. Left panels: heatmaps showing the spiking activity of 50 lobula neurons in response to visual patterns when scanning either the lower half of the pattern at normal speed (<bold>A</bold>) or the whole pattern at normal speed (<bold>B</bold>). Right panels: the mean and standard deviation of the spike rate responses of individual lobula neurons to two distinct visual patterns (plus and multiplication), with colour-coded responses (purple for ⊕, green for ⊗). Scanning behaviour significantly alters the neural responses, with distinct sets of neurons preferentially responding to each stimulus. (<bold>C</bold>) Mean angular distance between lobula neuron responses for different scanning conditions. Lower half-normal speed scanning results in greater separation between neural representations, suggesting that scanning of local region enhances feature selectivity. Error bars represent SEM. Asterisks (*) indicate p-values &lt;0.05, while ‘n.s.’ denotes non-significant results.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig5-v1.tif"/></fig><p>The model was trained using a differential conditioning paradigm, where the correct stimulus (S<sup>+</sup>) was paired with a reward and the incorrect stimulus (S⁻) was associated with punishment. For simplicity, we denote positive patterns as S<sup>+</sup> and negative patterns as S⁻. During associative learning simulations, only the synaptic weights between KCs and MBONs corresponding to the presented patterns were updated, while randomly weighted connections between the lobula and KCs were incorporated to ensure sparse activity in KCs (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; see Methods). To capture individual variability observed in bees, we repeated the simulations with different initial conditions, including random neural connectivity between lobula neurons and KCs, as well as between KCs and MBONs. The model’s performance was then assessed across multiple visual discrimination paradigms (<xref ref-type="fig" rid="fig4">Figures 4</xref>, <xref ref-type="fig" rid="fig6">6</xref>, and <xref ref-type="fig" rid="fig7">7</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Proposed neural network performance to published bee pattern experiments.</title><p>Twenty simulated bees, with random initial neuronal connectivity in mushroom bodies (see Methods), were trained to discriminate a positive target pattern from a negative distractor pattern (50 training exposures per pattern). The simulated bees’ performances were examined via unrewarded tests, where synaptic weights were not updated (average of 20 simulated pattern pair tests per bee). All simulations were conducted under the assumption that model bees viewed the targets from a distance of 2 cm while flying at a normal speed of 0.1 m/s. During this process, the bees scanned the lower half of the pattern. (<bold>A</bold>) Mean percentage of correct choices (± SEM) in discriminating bars oriented at 90° to each other, 25.5° angled cross with a 45° rotated version of the same cross, and a pair of mirrored spiral patterns (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>; <xref ref-type="bibr" rid="bib134">Srinivasan, 1994</xref>). The simulated bees achieved greater than chance performances. (<bold>B</bold>) Performance of simulated bees trained with a generalisation protocol (<xref ref-type="bibr" rid="bib11">Benard et al., 2006</xref>). Trained to 6 pairs of perpendicular oriented gratings (10 exposures per grating). Simulated bees then tested with a novel gating pair, and a single oriented bar pair. The simulated bees performed well in distinguishing between the novel pair of gratings; less well, but still significantly above chance, to the single bars. This indicates that the model can generalise the orientation of the training patterns to distinguish the novel patterns. (<bold>C</bold>) Mean performance (± SEM) of the simulated bees in discriminating the positive orientation from negative orientation. Additionally, the performance in recognising the positive orientation from the novel pattern, and preference for the negative pattern from a novel pattern. Simulated bees learnt to prefer positive patterns, but also reject negative patterns, in this case preferring novel stimuli. (<bold>D</bold>) Performance of simulated bees trained to a horizontal and –45° bar in the lower pattern half versus a vertical and +45° bar (<xref ref-type="bibr" rid="bib136">Stach et al., 2004</xref>). The simulated bees could easily discriminate between the trained bars, and a colour inverted version of the patterns. They performed less well when the bars were replaced with similarly oriented gratings, but still significantly above chance. When tested on the positive pattern vs. a novel pattern with one correctly and one incorrectly oriented bar, the simulated bees chose the positive patterns (fourth and fifth bars), whereas with the negative pattern versus this same novel pattern the simulated bees rejected the negative pattern in preference for the novel pattern with single positive oriented bar (two last bars). (<bold>E</bold>) The graph shows the mean percentage of correct choices for the 20 simulated bees during a facial recognition task (<xref ref-type="bibr" rid="bib34">Dyer et al., 2005</xref>). Simulated bees were trained to the positive (rewarded) face image versus a negative (non-rewarded) distractor face. The model bee is able to recognise the target face from distractors after training, and also to recognise the positive face from novel faces even if the novel face is similar to the target face (fourth bar). However, it failed to discriminate between the positive and negative faces rotated by 180°. (<bold>F</bold>) The model was trained on spatially structured patterns from <xref ref-type="bibr" rid="bib136">Stach et al., 2004</xref>, requiring recognition of orientation arrangements across four quadrants. Unlike bees, the model failed to discriminate these patterns, highlighting its limitations in integrating local features into a coherent global representation. Asterisks (*) indicate p-values &lt; 0.05, while ‘n.s.’ denotes non-significant results.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig6-v1.tif"/></fig><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Minimum number of lobula neurons that are necessary for pattern recognition.</title><p>(<bold>A</bold>) Obtained spatiotemporal receptive field of lobula neurons when the number of lobula neurons were set at 36, 16, or 4 during the non-associative learning in the visual lobe (see <xref ref-type="fig" rid="fig5">Figure 5A</xref>). This shows the models with lower number of lobula neurons encode less variability of orientations and temporal coding of the visual inputs (see <xref ref-type="video" rid="video5">Videos 5</xref> and <xref ref-type="video" rid="video6">6</xref>) The average correct choices of the three models with 36, 16, or 4 lobula neurons after training to a pair of plus and multiplication patterns (<bold>B</bold>), mirrored spiral patterns (<bold>C</bold>), and human face discrimination (<bold>D</bold>). The model with 36 lobula neurons still can solve pattern recognition tasks at a level above chance. It indicates that only 36 lobula neurons that provide all inputs to mushroom bodies are sufficient for the simulated bees to be able to discriminate between patterns. Asterisks (*) indicate p-values &lt;0.05, while ‘n.s.’ denotes non-significant results.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig7-v1.tif"/></fig><p>In the initial implementation, simulated bees were trained to distinguish a plus sign from a multiplication sign, with training focused on the lower half of the plus pattern (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Following training, MBON activity decreased in response to the plus (S<sup>+</sup>) while increasing in response to the multiplication sign (S⁻), whereas prior to training, MBON responses to both patterns were similar (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). This demonstrates that the model successfully discriminated between S<sup>+</sup> and S⁻ through temporal coding and sequential scanning of the visual pattern (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). In contrast, a model with fixed, random connectivity in the visual lobe failed to differentiate between the plus and multiplication patterns (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). This underscores the importance of structured connectivity that emerges in the bee visual lobes through non-associative learning—specifically, the development of spatiotemporal receptive fields in lobula neurons—for successful visual learning.</p><p>Our model further revealed that rewarding patterns elicited a reduction in extrinsic neuron responses, while punished patterns led to increased responses (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), a phenomenon consistent with neural activity recorded from alpha lobe PE1 neurons in the mushroom body (<xref ref-type="bibr" rid="bib107">Okada et al., 2007</xref>). Initially, the simulated bees performed worse than real bees in the plus versus multiplication discrimination task (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, last pair of bars). Using experimentally derived parameters from bumblebee studies—including an average scanning speed of 0.1 m/s (referred to as normal speed) for whole-pattern scanning and a viewing distance of 20 mm—simulated bees achieved a correct choice rate of only 63% for the plus stimulus and 60% for the reciprocal cross-protocol (averaged over 20 simulations; see <xref ref-type="fig" rid="fig4">Figure 4C</xref>, fifth pair of bars).</p><p>However, when the experimental conditions were adjusted so that simulated bees scanned only the lower half of the patterns or focused on the lower left corner—consistent with real bee behaviour (<xref ref-type="bibr" rid="bib88">MaBouDi et al., 2021b</xref>)—correct choice performance improved significantly, reaching ≥96% and ≥98%, respectively (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, first two pairs of bars). Conversely, increasing scanning speed (resulting in larger separations between sampled image patches) reduced accuracy to 70%, while stationary simulated bees—those that did not actively scan—achieved only 60% correct choices.</p><p>In additional experiments where the model bees were trained from a greater distance (&gt;100 mm), they failed to discriminate between patterns. This aligns with behavioural findings in bees, where they initially select a pattern at random and move closer to scan it before making a decision (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>). Studies show that bumblebees approach stimuli to scan patterns at close range, and their initial approach choices are random (<xref ref-type="bibr" rid="bib49">Guiraud et al., 2018</xref>; <xref ref-type="bibr" rid="bib88">MaBouDi et al., 2021b</xref>). These findings underscore the critical role of active vision in visual learning and discrimination, demonstrating that the model effectively captures key aspects of biological visual processing, including sequential sampling, spatial integration, and plasticity-driven adaptation.</p></sec><sec id="s2-5"><title>Scanning strategy modulates lobula neural representations: enhanced selectivity with localised sampling</title><p>To further investigate how scanning behaviour influences the distinctiveness of neural representations in the lobula and impacts performance in visual learning tasks, we analysed activity patterns and response magnitudes under two different scanning conditions from the first experiment (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Heatmaps of lobula neuron activity (<xref ref-type="fig" rid="fig5">Figure 5A, B</xref>) illustrate neural responses to two stimulus conditions: scanning only the lower half of the pattern at normal speed (Panel A) and scanning the entire pattern at normal speed (Panel B).</p><p>The activity maps indicate that different subsets of lobula neurons responded preferentially to the lower half of the stimulus, exhibiting stronger activation in a smaller neuronal subset. In contrast, scanning the entire pattern resulted in a more widespread and overlapping activation across the lobula neuron population. Corresponding spike rate plots (<xref ref-type="fig" rid="fig5">Figure 5</xref>, right panels) further highlight these differences, revealing that neurons displayed stronger selectivity when scanning was restricted to the lower half at normal speed compared to whole-pattern scanning. These findings suggest that lobula neurons exhibit stimulus-specific selectivity that is modulated by the spatial dynamics of scanning behaviour.</p><p>To quantify the separability of these population responses, we computed the angular distance (<italic>θ</italic>) between their activity vectors over 20 stimulations using the cosine similarity metric (see Methods). The results (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) indicate that neural responses exhibited a significantly larger angular distance when the model scanned only the lower half of the stimuli at normal speed compared to scanning the entirety of the same pattern. This suggests that neural activity population were more distinct when scanning was confined to a localised region of these visual patterns, reinforcing the idea that restricted sampling enhances neural discriminability.</p><p>These findings demonstrate that lobula neurons encode visual stimuli in a structured manner, with response contrast influenced by both spatial and temporal properties of scanning behaviour (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>). By selectively sampling specific regions of a stimulus, the system enhances the differentiation of these visual patterns, supporting the hypothesis that active vision plays a crucial role in effective neural coding and discrimination.</p></sec><sec id="s2-6"><title>Neural network model of active vision bee behaviours across various visual experiments</title><p>In this study, we evaluated our model—using scans from the lower half of the visual field—by comparing its performance with results from bee experiments reported in the literature. (Note: Bees may exhibit variations in scanning behaviour under different patterns and training conditions; see Discussion). Our simulated bees successfully discriminated between angled bars (<xref ref-type="bibr" rid="bib142">van Hateren et al., 1990</xref>), a 22.5° angled cross from a 90° rotated version (<xref ref-type="bibr" rid="bib134">Srinivasan, 1994</xref>), and spiral patterns (<xref ref-type="bibr" rid="bib158">Zhang and Horridge, 1992</xref>; <xref ref-type="fig" rid="fig6">Figure 6A</xref>). When trained on grating patterns with –45° versus +45° orientations, the simulated bees successfully identified the correct pattern. Moreover, it demonstrated the ability to transfer the learned rule to novel patterns the model had never encountered during training, including single-bar patterns (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). This suggests that the model captures key aspects of visual generalisation observed in real bees. <xref ref-type="fig" rid="fig6">Figure 6C</xref> shows that the proposed model not only learned to identify the correctly oriented bar pattern but also to distinguish the rewarding pattern from a novel one (two circles). Notably, the model exhibited a 22% lower preference for the negatively trained pattern compared to the novel pattern, validating the implementation of the rejection behaviour and demonstrating that the model can simultaneously learn rewarding and aversive stimuli.</p><p>This was further explored by training the network with patterns containing two oriented bars in each lower quadrant (<xref ref-type="fig" rid="fig6">Figure 6D</xref>; <xref ref-type="bibr" rid="bib11">Benard et al., 2006</xref>; <xref ref-type="bibr" rid="bib136">Stach et al., 2004</xref>; <xref ref-type="bibr" rid="bib158">Zhang and Horridge, 1992</xref>). The simulated bees discriminated these training patterns with over 99% accuracy, but performance dropped to an average of 61% when presented with a simplified variant. When tested with the original positive pattern and novel patterns containing only one correct orientation, the bees showed a high preference for the correct stimulus. Similarly, the simulated bees exhibited a clear preference for a pattern with a single correct feature over the trained negative pattern, indicating that the model can extract multiple features during scanning.</p><p>To present a more complex pattern recognition task, we replicated a facial recognition experiment performed on honeybees (<xref ref-type="bibr" rid="bib34">Dyer et al., 2005</xref>) by training the neural network with images of two human faces (<xref ref-type="fig" rid="fig6">Figure 6E</xref>). As with honeybees, our simulated bees were able to identify the positive trained face from the negative one, as well as distinguish two novel faces and a caricature. Both the real and simulated bees failed to discriminate the faces when rotated through 180°. These results demonstrate that complex visual features can be condensed through spatiotemporal encoding in the lobula neurons into specific and distinct neuronal representations that are critical for learning in the miniature bee brain.</p><p>To evaluate the model’s ability to discriminate more complex visual stimuli, we trained it on a set of patterns from <xref ref-type="bibr" rid="bib136">Stach et al., 2004</xref>, which require recognising the spatial arrangement of orientations across four quadrants (<xref ref-type="fig" rid="fig6">Figure 6F</xref>). Unlike previous experiments where scanning behaviour was confined to specific regions, the complexity of these stimuli necessitated training the model using whole-pattern exposure. The results indicate that the model failed to replicate the bees’ ability to discriminate these patterns (<xref ref-type="fig" rid="fig6">Figure 6F</xref>), despite successfully distinguishing between the plus and multiplication signs when scanning the entire pattern (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), even if its performance remained lower than that of real bees. Moreover, the model was unable to generalise this capability to more configurations, suggesting that while it can process simple spatial features through sequential scanning, it lacks the longer and more dynamic scanning required for assembling and integrating local features into a coherent global representation, as observed in bees (see Discussion).</p></sec><sec id="s2-7"><title>What is the minimally sufficient number of lobula neurons and the necessary connectivity for active vision in bees?</title><p>As reported above, our model successfully accomplishes various pattern recognition tasks (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig6">6</xref>). We then asked whether our neural networks could perform with a very limited number of lobula neurons that transfer visual information to the mushroom body. To investigate this, we ran the non-associative learning process with different numbers of lobula neurons—specifically, 4, 16, or 36 neurons (the original model had 50). The visual network was subsequently trained using the same set of natural images and protocol as the original model (<xref ref-type="fig" rid="fig7">Figure 7A</xref>).</p><p>Interestingly, the non-associative learning process led to the emergence of distinct spatiotemporal structures in the lobula neurons. We found that reducing the number of lobula neurons decreased the variability in their spatiotemporal receptive (<xref ref-type="fig" rid="fig7">Figure 7A</xref> and <xref ref-type="video" rid="video3">Videos 3</xref>–<xref ref-type="video" rid="video5">5</xref>). In particular, when the network was limited to four neurons, it could not encode the full spatiotemporal structure of the training patterns obtained with the model with 50 lobula neurons—only vertical and horizontal receptive fields were produced (<xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video5">5</xref>). As expected, the overall performance of the model decreased as the number of lobula neurons was reduced. Although the model with 16 lobula neurons demonstrated the ability to discriminate more complex patterns beyond the plus and multiplication signs (<xref ref-type="fig" rid="fig7">Figure 7B, C</xref>), it remained insufficient for recognising highly complex stimuli such as human faces (<xref ref-type="fig" rid="fig7">Figure 7D, E</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-89929-video3.mp4" id="video3"><label>Video 3.</label><caption><title>Spatiotemporal dynamics of receptive fields in 36 lobula neurons emerging from non-associative learning and active scanning.</title><p>This follows the same structure as <xref ref-type="video" rid="video1">Video 1</xref>, depicting the receptive field evolution in a larger population of 36 lobula neurons under the temporal coding framework and non-associative learning.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-89929-video4.mp4" id="video4"><label>Video 4.</label><caption><title>Spatiotemporal dynamics of receptive fields in 16 lobula neurons emerging from non-associative learning and active scanning.</title><p>This follows the same structure as <xref ref-type="video" rid="video1">Video 1</xref>, depicting the receptive field evolution in a larger population of 16 lobula neurons under the temporal coding framework and non-associative learning (compare to <xref ref-type="video" rid="video1">Videos 1 and 3</xref>).</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-89929-video5.mp4" id="video5"><label>Video 5.</label><caption><title>Spatiotemporal dynamics of receptive fields in only four lobula neurons emerging from non-associative learning and active scanning.</title><p>This follows the same structure as <xref ref-type="video" rid="video1">Video 1</xref>, depicting the receptive field evolution in a larger population of four lobula neurons under the temporal coding framework and non-associative learning (compare to <xref ref-type="video" rid="video1">Videos 1, 3, and 4</xref>).</p></caption></media><p>Moreover, to investigate the effect of inhibitory neurons within the visual lobe on lobula neuron output, we trained the model using the same protocol but fixed the synaptic weights of the inhibitory connections (i.e. these weights were not updated during exposure to the training images). These fixed inhibitory connections limited the ability of the lobula neuron population to encode moving orientations (<xref ref-type="fig" rid="fig8">Figure 8</xref> and <xref ref-type="video" rid="video6">Video 6</xref>), indicating that the plasticity of inhibitory interneurons in the visual lobe plays a crucial role in facilitating an efficient representation of the visual environment. While this suggests that increasing the network size can enhance the model’s capacity for discriminating complex visual patterns, the results (<xref ref-type="fig" rid="fig4">Figure 4</xref>) indicate that scanning behaviour plays a crucial role in overcoming this limitation. Specifically, adopting a more targeted scanning strategy can improve discriminability by directing visual sampling to the most informative regions of the stimulus (see Discussion).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>The role of lateral inhibitory connections between lobula neurons.</title><p>Obtained spatiotemporal receptive field of lobula neurons (<bold>B</bold>) when the lateral inhibitory connectivity between lobula neurons is fixed (<bold>A</bold>) during the non-associative learning (see <xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video6">6</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig8-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-89929-video6.mp4" id="video6"><label>Video 6.</label><caption><title>Spatiotemporal dynamics of receptive fields in 50 lobula neurons emerging from non-associative learning and active scanning.</title><p>This follows the same structure as <xref ref-type="video" rid="video1">Video 1</xref>, but with fixed lateral inhibitory connectivity between lobula neurons during non-associative learning. The video illustrates how receptive fields evolve under the temporal coding framework, providing a comparison to <xref ref-type="video" rid="video1">Video 1</xref>, where lateral inhibition was plastic.</p></caption></media><p>Taken together, these findings demonstrate that our assumption regarding non-associative plasticity in the visual lobe successfully replicates the neural responses of lobula neurons across various patterns and conditions. This plasticity yields a sparse, uncorrelated representation of the visual input that benefits subsequent learning processes in the mushroom body. Importantly, these results closely align with theoretical studies (see Discussion), further supporting the effectiveness of the active vision in capturing the underlying principles of information encoding in the insect visual system.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we investigated the core computational requirements for visual pattern recognition by examined a minimal neural network inspired by the active scanning flight behaviours of bees (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>). We developed a novel model based on the insect visual system, simulating how a small population of lobula neurons encodes the visual environment through spatiotemporal responses. By incorporating non-associative learning, the model self-organises its connectivity within the visual lobe, generating efficient environmental representations (<xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref>, and <xref ref-type="fig" rid="fig5">5</xref>). The process leads to the emergence of orientation-selective cells in the lobula, which are essential for encoding complex visual scenes (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>Our simulations reveal that a small subset of lobula neurons, sensitive to specific orientations and velocities, can condense complex visual environments into spatiotemporal representations expressed as firing rates (<xref ref-type="fig" rid="fig2">Figure 2</xref>). These sparse representations effectively discriminate between the plus and multiplication patterns used in behavioural experiments (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>) while also generalising to novel stimuli—including successful recognition of human faces—highlighting the model’s broader applicability (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig6">6</xref>).</p><p>Furthermore, our findings highlight the crucial role of bee movement, or active vision, in optimising the analysis and encoding of environmental information (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Spatiotemporal encoding in the visual lobe emerges as a key mechanism driving the efficiency of minimal intelligent systems. Our study underscores the fundamental computational principles of visual pattern recognition, particularly the interplay between active vision and spatiotemporal encoding in insect information processing. These insights not only advance our understanding of biological vision but also inspire the development of novel computational models for visual recognition tasks (<xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref>).</p><p>The question of how animals cope with a noisy and complex natural world has long been a central topic in neuroscience and behavioural ecology (<xref ref-type="bibr" rid="bib10">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib43">Gibson, 1978</xref>; <xref ref-type="bibr" rid="bib99">Menzel and Giurfa, 2006</xref>; <xref ref-type="bibr" rid="bib135">Srinivasan, 2010</xref>). One key theoretical framework addressing this challenge is the efficient coding hypothesis, which posits that early sensory systems compress incoming information into a more efficient format, optimising the transmission of relevant signals to higher brain regions (<xref ref-type="bibr" rid="bib10">Barlow, 1961</xref>). According to this hypothesis, individual visual neurons should maximise their output capacity (e.g. reaching their maximum firing rate) when responding to natural stimuli while population responses should exhibit statistical independence (<xref ref-type="bibr" rid="bib126">Simoncelli and Olshausen, 2001</xref>). Despite the relative simplicity of our model compared to recently available full <italic>Drosophila</italic> connectome data (<xref ref-type="bibr" rid="bib79">Lin et al., 2024</xref>; <xref ref-type="bibr" rid="bib117">Schlegel et al., 2024</xref>), our findings suggest that insects optimise visual coding through non-associative learning while actively exploring their environment (see below). Specifically, we demonstrate that neural features of the insect brain, combined with active vision, facilitate this optimisation by developing uncorrelated and sparse coding in the lobula (<xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref>, and <xref ref-type="fig" rid="fig5">5</xref>). This supports the idea that efficient coding is not merely a passive process but one actively shaped by an animal’s interactions with its surroundings. However, bees exhibit a remarkably diverse behavioural repertoire despite their small brains—ranging from fine-scale object inspections to long-distance navigation (<xref ref-type="bibr" rid="bib22">Chittka, 2022</xref>; <xref ref-type="bibr" rid="bib69">Juusola et al., 2025</xref>; <xref ref-type="bibr" rid="bib100">Menzel, 2012</xref>; <xref ref-type="bibr" rid="bib135">Srinivasan, 2010</xref>). This behavioural diversity makes them an excellent model for investigating how ecological constraints shape neural computation and, ultimately, efficient coding. Understanding how insects dynamically refine sensory representations in response to environmental demands offers broader insights into the fundamental principles of neural information processing in biological systems.</p><p>The non-associative model presented in this study operates as a linear generative model that effectively captures the receptive fields of lobula neurons, linking the spatiotemporal statistics of natural environments to principles of efficient neural coding (<xref ref-type="bibr" rid="bib10">Barlow, 1961</xref>; <xref ref-type="bibr" rid="bib108">Olshausen, 2003</xref>). Following training, lobula neuron activity in response to naturalistic spatiotemporal signals becomes highly decorrelated, with only a limited subset of neurons selectively responding to specific visual stimuli (<xref ref-type="fig" rid="fig5">Figure 5</xref>). This sparse coding strategy enhances energy efficiency by minimising overall neural activity while maintaining distinct stimulus encoding. By ensuring that only a small fraction of neurons is active at any given moment, this mechanism optimises information processing, reduces redundancy, and enhances metabolic efficiency, reinforcing the adaptive advantages of efficient coding in visual systems. Our model introduces a novel generative framework that can be extended to other species, including primates, to investigate how movement contributes to visual–spatial encoding in larger brains. Given the ubiquity of active vision across the animal kingdom (<xref ref-type="bibr" rid="bib73">Land, 1973</xref>; <xref ref-type="bibr" rid="bib75">Land and Nilsson, 2012</xref>; <xref ref-type="bibr" rid="bib151">Washburn, 1908</xref>; <xref ref-type="bibr" rid="bib152">Washburn, 1916</xref>; <xref ref-type="bibr" rid="bib157">Yarbus, 1967</xref>), the principles identified in this study may be broadly conserved across different taxa, underscoring the fundamental role of sensorimotor interactions in shaping neural representations. These insights provide a valuable foundation for future comparative studies on the interplay between movement, efficient coding, and sensory processing across diverse neural architectures.</p><p>Our findings align with previous studies on bumblebees’ discrimination of plus and multiplication sign patterns (<xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>), demonstrating improved model performance when scanning the lower half of patterns at specific velocities (<xref ref-type="fig" rid="fig3">Figure 3</xref>). However, bees exhibit variations in scanning behaviour depending on pattern complexity and training (<xref ref-type="bibr" rid="bib45">Giurfa et al., 1999</xref>; <xref ref-type="bibr" rid="bib49">Guiraud et al., 2018</xref>). Research has shown that both honeybees and bumblebees solve visual tasks by extracting localised or elemental features within patterns, adapting their discrimination strategies accordingly (<xref ref-type="bibr" rid="bib45">Giurfa et al., 1999</xref>; <xref ref-type="bibr" rid="bib91">MaBouDi et al., 2025</xref>; <xref ref-type="bibr" rid="bib136">Stach et al., 2004</xref>; <xref ref-type="bibr" rid="bib137">Stach and Giurfa, 2005</xref>). This suggests that bees develop tailored flight manoeuvres during training, optimising their scanning behaviour to maximise visual information extraction. Although our model simplifies visual flight dynamics by employing a five-step constant-speed horizontal scan (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), this abstraction was useful for isolating key computational principles. However, its failure to solve the complex pattern recognition task in <xref ref-type="fig" rid="fig6">Figure 6F</xref> underscores its limitations, suggesting that incorporating longer and more dynamic scanning strategies could enhance visual processing capacity. Extending the scanning duration while integrating multiple visual features across quadrants—alongside mechanisms such as working memory and sequential learning—could improve performance by enabling the model to retain and integrate previously acquired visual information. Real-world insect vision, however, relies on more flexible and adaptive scanning behaviours shaped by flight speed, head movements, and environmental feedback. Future work should leverage recent advances in insect connectomics, which reveal a diverse range of neuron types—including small object-detecting neurons, motion-sensitive neurons, and colour-processing cells—alongside machine learning techniques for analysing animal movement to develop a more comprehensive flight dynamics model. Incorporating variable scan trajectories and real-time sensorimotor feedback will offer deeper insights into how active vision optimises information acquisition and enhances learning in dynamic environments.</p><p>A key advantage of our model lies in its ability to leverage sparsity and selectivity to efficiently process sequential visual data (<xref ref-type="fig" rid="fig2">Figures 2B</xref>, <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig6">6</xref>), in contrast to models that rely on pixel-wise image representation for training. Traditional models that directly process raw pixel values often require substantial computational resources and struggle with scalability, particularly when handling large-scale visual inputs (<xref ref-type="bibr" rid="bib2">Amin et al., 2025</xref>; <xref ref-type="bibr" rid="bib4">Ardin et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Baddeley et al., 2012</xref>). In contrast, our model extracts and encodes sequential visual information within a small population of lobula neurons (<xref ref-type="fig" rid="fig2">Figures 2B</xref> and <xref ref-type="fig" rid="fig6">6</xref>), significantly compressing the visual input while preserving essential features. This sparse representation reduces redundancy, enhances computational efficiency, and ensures that only the most informative aspects of the scene are processed for pattern recognition. Furthermore, our model demonstrates adaptive selectivity, dynamically adjusting to different visual inputs by learning lobula responses optimised to the statistical features of the scene. Unlike pixel-based models, which require processing every individual pixel in an image, our approach extracts compact, high-contrast signals that are more robust to noise and enhance generalisability. This is particularly relevant for bio-inspired visual processing, as it aligns with known sparse and decorrelated representations in biological vision systems. By integrating biologically inspired sparse coding, adaptive selectivity, and motion-driven encoding, our model provides a robust alternative to conventional pixel-based architectures. This not only improves computational efficiency but also enhances discriminability and generalisation, making it well suited for real-world applications, including robotic vision and autonomous navigation, where rapid adaptation to dynamic environments is crucial.</p><p>The results of our model suggest that passive visual exposure to natural images alters the connectivity in the visual lobes, leading to enhanced pattern recognition abilities (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig6">6</xref>). Notably, these synaptic connections develop independently of the initial connectivity profiles of the simulated bees. We propose that, beyond the general gross neuroanatomy of the insect optic lobes—which has been preserved since the Cambrian period for efficient neural representation (<xref ref-type="bibr" rid="bib81">Ma et al., 2012</xref>)—the specific visual experiences encountered by real bees during early life play a crucial role in shaping their individual visual representations. This, in turn, may influence their subsequent performance in behavioural tasks (<xref ref-type="bibr" rid="bib58">Hertel, 1982</xref>; <xref ref-type="bibr" rid="bib59">Hertel, 1983</xref>; <xref ref-type="bibr" rid="bib83">MaBouDi et al., 2017</xref>; <xref ref-type="bibr" rid="bib147">Vetter and Visscher, 1997</xref>). There is direct empirical evidence for such neural developmental processes in olfactory systems of bees, where early passive exposure improves subsequent odour discrimination (<xref ref-type="bibr" rid="bib5">Arenas and Farina, 2008</xref>; <xref ref-type="bibr" rid="bib80">Locatelli et al., 2013</xref>). Our previous research in olfactory coding demonstrated that the iSTDP learning rule can establish specific connectivity in the sensory system and enhance the separability of odour representations in antennal lobe outputs (<xref ref-type="bibr" rid="bib83">MaBouDi et al., 2017</xref>). A similar mechanism was observed here among lobula neurons, where only a limited subset is activated by specific visual inputs, resulting in sparse and distinct outputs to the mushroom body learning centres (<xref ref-type="fig" rid="fig2">Figures 2B</xref> and <xref ref-type="fig" rid="fig5">5</xref>). The receptive fields of lobula cells, maintained with the fixed lateral connectivity, shows that inhibition is required for orientation selectivity and temporal coding in the visual lobe (<xref ref-type="bibr" rid="bib40">Fisher et al., 2015</xref>). These findings highlight the critical role of inhibitory connections within the visual lobes. Accordingly, our model predicts that bees with limited early-life visual experiences will perform worse in visual learning and memory compared to bees with rich visual experiences. Further behavioural and neurobiological studies are needed to test this prediction.</p><p>Mushroom bodies are critical centres for associative learning and memory in insects (<xref ref-type="bibr" rid="bib39">Fiala and Kaun, 2024</xref>; <xref ref-type="bibr" rid="bib57">Heisenberg, 2003</xref>; <xref ref-type="bibr" rid="bib100">Menzel, 2012</xref>; <xref ref-type="bibr" rid="bib101">Menzel, 2022</xref>). Synapses between KCs and extrinsic mushroom body neurons follow a Hebbian STDP rule (<xref ref-type="bibr" rid="bib7">Aso et al., 2014</xref>; <xref ref-type="bibr" rid="bib18">Cassenaer and Laurent, 2007</xref>; <xref ref-type="bibr" rid="bib95">Markram et al., 1997</xref>); however, the STDP rule alone cannot maintain associative learning (<xref ref-type="bibr" rid="bib1">Abbott and Nelson, 2000</xref>; <xref ref-type="bibr" rid="bib97">Meeks and Holy, 2008</xref>). In insects, associative learning appears to rely on the neurotransmitters octopamine and dopamine to signal unconditioned appetitive and aversive values (<xref ref-type="bibr" rid="bib24">Cognigni et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Davidson et al., 2023</xref>; <xref ref-type="bibr" rid="bib41">Fisher, 2024</xref>; <xref ref-type="bibr" rid="bib53">Hammer, 1993</xref>; <xref ref-type="bibr" rid="bib54">Hammer and Menzel, 1995</xref>; <xref ref-type="bibr" rid="bib96">Matsumoto et al., 2015</xref>; <xref ref-type="bibr" rid="bib102">Mohammad et al., 2024</xref>; <xref ref-type="bibr" rid="bib112">Perry and Barron, 2013</xref>; <xref ref-type="bibr" rid="bib119">Schwaerzel et al., 2003</xref>; <xref ref-type="bibr" rid="bib122">Selcho, 2024</xref>). These neurotransmitters are released into the mushroom body lobes, where KCs connect to MBON (<xref ref-type="bibr" rid="bib16">Burke et al., 2012</xref>; <xref ref-type="bibr" rid="bib101">Menzel, 2022</xref>; <xref ref-type="bibr" rid="bib107">Okada et al., 2007</xref>; <xref ref-type="bibr" rid="bib139">Strube-Bloss et al., 2011</xref>). Using in vivo electrophysiology in locusts, <xref ref-type="bibr" rid="bib19">Cassenaer and Laurent, 2012</xref> reported that octopamine depresses synapses underlying STDP rule, leading to a lower response in MBONs when octopamine is present.</p><p>Following this observation, we modelled associative learning by pairing the positive pattern with the reward via octopamine-modulated STDP (<xref ref-type="disp-formula" rid="equ5">Equation 4</xref>; <xref ref-type="fig" rid="fig1">Figure 1</xref>). In this formulation, the temporal ordering of pre- or postsynaptic spikes depresses the synaptic connection between KCs and the MBONs. Conversely, synapses are updated according to classical STDP when negative patterns are paired with the punishment (<xref ref-type="disp-formula" rid="equ4">Equation 3</xref>; <xref ref-type="fig" rid="fig9">Figure 9</xref>). This combination produces a complex interplay between synaptic changes and reinforcer signals, enabling the model to not only learn to select the positive patterns but to reject incorrect ones (<xref ref-type="fig" rid="fig4">Figures 4B</xref> and <xref ref-type="fig" rid="fig6">6C</xref>). The resulting changes in MBON response to positive patterns during associative learning are consistent with the PE1 extrinsic neuron in the honeybee brain, which exhibits a lower response to the positive patterns (<xref ref-type="bibr" rid="bib55">Hancock et al., 2022</xref>; <xref ref-type="bibr" rid="bib39">Fiala and Kaun, 2024</xref>; <xref ref-type="bibr" rid="bib107">Okada et al., 2007</xref> ; <xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Spike-timing-dependent plasticity (STDP) curves.</title><p>(<bold>A</bold>) Classical spike-timing-dependent plasticity (STDP) curve showing relationship between synaptic weight change and the precise time difference between the Kenyon cells and mushroom body output neuron (MBON) spikes. The synaptic weight can be either depressed or potentiated. (<bold>B</bold>) STDP curve modulated by octopamine in the insect mushroom body. The Synaptic weights are depressed. The formula of these curves is described in <xref ref-type="disp-formula" rid="equ4 equ5">Equations 3 and 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89929-fig9-v1.tif"/></fig><p>However, further studies are required to investigate the novel combination of octopamine and dopamine modulation of STDP that is introduced in this study. Combining non-associative learning in the optic lobes with supervised learning in the mushroom bodies produced a model capable of not only discriminating simple patterns but also generalisation (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), and correct judgments in conflicting stimulus experiments (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). The real power of this approach is exemplified in the facial recognition task (<xref ref-type="fig" rid="fig6">Figure 6E</xref>), where the complexity of the human face is reduced to a set of sparse lobula neuron activations that can be learnt by the mushroom bodies. Moreover, the spatiotemporal receptive fields formed during non-associative learning respond differently for different faces, allowing fine differences to be encoded. Although real bees rarely have to discriminate between human faces, these processes likely enable bees to select rewarding flowers without requiring a complex visual memory within their miniature brains.</p><p>We used natural scenes with a statistical structure similar to those that visual systems have adapted to over evolutionary time (<xref ref-type="bibr" rid="bib42">Geisler, 2008</xref>; <xref ref-type="bibr" rid="bib61">Hyvärinen et al., 2009</xref>; <xref ref-type="bibr" rid="bib82">MaBouDi et al., 2016</xref>; <xref ref-type="bibr" rid="bib126">Simoncelli and Olshausen, 2001</xref>). Because bee navigation and foraging primarily involve locating food among a variety of flowers, our non-associative network was trained with a set of different flower images. As with all theoretical models, this is a simplification, since real bees navigate a 3D environment with a large field of view. Here, we assume that receptive field formation in real bees is comparable to our 2D simulations. Nevertheless, further studies are necessary to refine and expand our model based on a more comprehensive understanding of the function and structure of the bee eye components (<xref ref-type="bibr" rid="bib63">Juston et al., 2013</xref>; <xref ref-type="bibr" rid="bib67">Juusola et al., 2017</xref>; <xref ref-type="bibr" rid="bib71">Kemppainen et al., 2022b</xref>; <xref ref-type="bibr" rid="bib148">Viollet and Franceschini, 2010</xref>). Moreover, investigating the neural mechanisms underlying visual learning in the bee brain will allow us to fine-tune our model’s architecture and parameters, leading to a more faithful representation of the bee visual system.</p><p>In this study, we restricted the model’s input to green photoreceptors to align with the known visual processing mechanisms of honeybees. This decision was based on the hypothesis that bee pattern recognition primarily relies on the green component of visual input, as green-sensitive photoreceptors are the most abundant, comprising approximately two-thirds of the ommatidia in the compound eye (<xref ref-type="bibr" rid="bib15">Briscoe and Chittka, 2001</xref>; <xref ref-type="bibr" rid="bib44">Giger and Srinivasan, 1996</xref>; <xref ref-type="bibr" rid="bib130">Spaethe et al., 2001</xref>; <xref ref-type="bibr" rid="bib132">Spaethe and Briscoe, 2004</xref>). There is also empirical evidence that the green channel provides the predominant input to movement and edge detection, as well as detailed spatial information for visual discrimination (<xref ref-type="bibr" rid="bib44">Giger and Srinivasan, 1996</xref>; <xref ref-type="bibr" rid="bib131">Spaethe and Chittka, 2003</xref>). Moreover, natural images exhibit a strong correlation among colour channels, meaning that excluding certain channels does not substantially alter the structure of the visual scene. By focusing on the green photoreceptor input, our model remains biologically plausible while ensuring computational efficiency. Future work could explore the contributions of other photoreceptor types to assess their impact on visual pattern recognition and potential interactions between colour and spatial information processing in the insect visual system.</p><p>Our model provides a functional abstraction of the insect visual system, focusing on core computational principles rather than replicating the detailed structural connectivity available from recent connectome studies (<xref ref-type="bibr" rid="bib79">Lin et al., 2024</xref>; <xref ref-type="bibr" rid="bib117">Schlegel et al., 2024</xref>). By prioritising the identification of fundamental mechanisms underlying active vision and visual learning, our approach avoids the challenges associated with highly parameterised models that can be difficult to interpret mechanistically. Integrating known physiological properties and behavioural findings from bees, our model generates testable hypotheses on how motion-driven visual processing enhances pattern recognition. This functional simplification allows us to isolate key mechanisms that might otherwise be obscured in large-scale anatomical reconstructions. Additionally, our study emphasises the critical role of motion in visual recognition—an aspect often overlooked in static-image studies—and demonstrates how sequential visual input dynamically shapes neural encoding, reinforcing the importance of active vision in efficient sensory processing.</p><p>Recent studies have shown that bees often employ efficient, low-cost strategies to solve cognitive tasks (<xref ref-type="bibr" rid="bib26">Cope et al., 2018</xref>; <xref ref-type="bibr" rid="bib49">Guiraud et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Langridge et al., 2021</xref>; <xref ref-type="bibr" rid="bib87">MaBouDi et al., 2021a</xref>; <xref ref-type="bibr" rid="bib89">MaBouDi et al., 2023</xref>; <xref ref-type="bibr" rid="bib85">MaBouDi et al., 2020b</xref>; <xref ref-type="bibr" rid="bib113">Roper et al., 2017</xref>; <xref ref-type="bibr" rid="bib146">Vasas et al., 2019</xref>). Understanding these cognitive strategies not only advances our knowledge of neural computation in miniature brains but also provides a framework for improving artificial intelligence and autonomous systems. Our study highlights the minimal neural architectures required for visual learning and lays the foundation for bio-inspired, unsupervised machine learning algorithms. By emphasising active vision through movement-driven pattern recognition, our model offers insights into solutions for key AI challenges, such as visual invariance and robust 3D environmental understanding. Moreover, engineering implementations of eye micromovements have been shown to enhance edge and bar discrimination, improving the visual processing efficiency of flying robots (<xref ref-type="bibr" rid="bib63">Juston et al., 2013</xref>; <xref ref-type="bibr" rid="bib148">Viollet and Franceschini, 2010</xref>). Additionally, the non-associative learning model and local plasticity rules explored in this work closely align with unsupervised learning techniques, particularly sparse coding models, where sparsity constraints enhance efficiency by reducing redundancy and promoting selective coding. This bio-inspired framework enables the extraction of latent structures in high-dimensional temporal data, with applications ranging from sensory signal processing to more adaptive and robust autonomous perception. Bridging biological and machine intelligence through evolutionarily optimised computational strategies paves the way for the next generation of AI, driving advancements in robotics, autonomous navigation, and real-world learning systems (<xref ref-type="bibr" rid="bib29">de Croon et al., 2022</xref>; <xref ref-type="bibr" rid="bib94">Manoonpong et al., 2021</xref>; <xref ref-type="bibr" rid="bib124">Serres and Viollet, 2018</xref>; <xref ref-type="bibr" rid="bib153">Webb, 2020</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Network topology of active vision model</title><p>The model architecture of the bee visual pathway is illustrated in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. The bumblebee has a pair of compound eyes that are composed of ~5500 ommatidia (<xref ref-type="bibr" rid="bib131">Spaethe and Chittka, 2003</xref>; <xref ref-type="bibr" rid="bib138">Streinzer et al., 2013</xref>). Each eye contains three different types of photoreceptors, short, medium and long wavelength sensitive peaking in the UV, blue and the green, respectively (<xref ref-type="bibr" rid="bib98">Menzel and Blakers, 1976</xref>; <xref ref-type="bibr" rid="bib127">Skorupski et al., 2007</xref>). Since the green photoreceptors are those that predominantly mediate visual pattern recognition (<xref ref-type="bibr" rid="bib44">Giger and Srinivasan, 1996</xref>; <xref ref-type="bibr" rid="bib130">Spaethe et al., 2001</xref>), we modelled that 75 × 75 green photoreceptors in one eye component are activated by the pixel values of the input pattern. Photoreceptors then project to 625 (25 × 25) neurons in the lamina, which is the first centre of visual processing. In this model, each lamina neuron, <inline-formula><alternatives><mml:math id="inf1"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft1">\begin{document}$r_{l}^{La}$\end{document}</tex-math></alternatives></inline-formula>, receives input from a non-overlapping 3 × 3 grid of neighbouring photoreceptors, corresponding to adjust ommatidia. The response of a lamina neuron is computed as <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$r_{l}^{La}=f\left (\sum _{p=1}^{P=9}r_{p};A_{0},m,b\right)$\end{document}</tex-math></alternatives></inline-formula>. Here, the activation function <inline-formula><alternatives><mml:math id="inf3"><mml:mi>f</mml:mi></mml:math><tex-math id="inft3">\begin{document}$f$\end{document}</tex-math></alternatives></inline-formula> is defined as:<disp-formula id="equ1"><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle  f\left (r;A_{0},m,b\right)=A_{0}/\left (1+exp\left (mr+b\right),\right.$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <italic>r</italic> is the input to the lamina neuron <inline-formula><alternatives><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft4">\begin{document}$La$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="inft5">\begin{document}$A_{0}$\end{document}</tex-math></alternatives></inline-formula> represent the maximum possible activity of lamina neurons. The parameters <inline-formula><alternatives><mml:math id="inf6"><mml:mi>m</mml:mi></mml:math><tex-math id="inft6">\begin{document}$m$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf7"><mml:mi>b</mml:mi></mml:math><tex-math id="inft7">\begin{document}$b$\end{document}</tex-math></alternatives></inline-formula> define the shape of the activation function, controlling its steepness and midpoint, respectively. For simplicity, the activation function in our model is fixed with <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$A_{0}=1$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf9"><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:math><tex-math id="inft9">\begin{document}$m=- 1,$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf10"><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math><tex-math id="inft10">\begin{document}$b=0.5$\end{document}</tex-math></alternatives></inline-formula>. This function imposes a constraint where weak inputs result in low activity, while stronger inputs drive the response towards its maximum value in a sigmoidal manner. Each photoreceptor’s output,<inline-formula><alternatives><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub></mml:math><tex-math id="inft11">\begin{document}$r_{p,}$\end{document}</tex-math></alternatives></inline-formula> is derived directly from the pixel intensity at the corresponding location in the input image, representing the green channel’s brightness. The values are normalised between 0 and 1, ensuring a continuous response that reflects the natural variation in luminance.</p><p>In this study, each spiking neuron operates according to the integrate-and-fire model. The dynamics of the subthreshold membrane potential of a neuron, <inline-formula><alternatives><mml:math id="inf12"><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft12">\begin{document}$u\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> is described by the following standard conductance-based leaky integrate-and-fire model: <inline-formula><alternatives><mml:math id="inf13"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo>.</mml:mo><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft13">\begin{document}$\tau \frac{du\left (t\right)}{dt}=- u\left (t\right)+R.I\left (t\right)$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf14"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math><tex-math id="inft14">\begin{document}$R=10$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$\tau =10\, \rm ms$\end{document}</tex-math></alternatives></inline-formula> are the resistance and membrane time constant of the neuron, respectively. Here, the input <inline-formula><alternatives><mml:math id="inf16"><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft16">\begin{document}$I\left (t\right)$\end{document}</tex-math></alternatives></inline-formula> exhibit the total synaptic input to the cell from presynaptic neurons. The membrane potential is reset to the base activity, <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>80</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$v_{0}=- 80\, \rm mV$\end{document}</tex-math></alternatives></inline-formula>, if it exceeds the threshold, <inline-formula><alternatives><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$V_{T}=0\, \rm mV$\end{document}</tex-math></alternatives></inline-formula>.</p><p>Each medulla neuron is activated by the summed activity of lamina neurons through the synaptic connectivity matrix W. The input of the m—the medulla neuron, <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$I_{m}^{Me}$\end{document}</tex-math></alternatives></inline-formula>, is calculated <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$I_{m}^{Me}=\sum _{l=1}^{L}W_{m,l}\,r_{l}^{La}$\end{document}</tex-math></alternatives></inline-formula>. The value <inline-formula><alternatives><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft21">\begin{document}$W_{l,m}$\end{document}</tex-math></alternatives></inline-formula> specifies the strength of a synaptic input from the <italic>l</italic>th lamina neuron to the m—the medulla neuron. To account for the inherent variability in neural responses, we incorporated stochasticity by adding signal noise generated from a Poisson distribution to the output of each neuron. The Poisson distribution was chosen because it closely models the statistical fluctuations observed in biological neural firing, where the variance of the response scales with the mean activity. This noise was applied independently to each neuron, ensuring that the variability in responses remains biologically plausible while preserving the overall signal structure. By introducing this element, the model better reflects the natural dynamics of neural processing, capturing the probabilistic nature of spike generation and sensory encoding in biological systems.</p><p>We propose a temporal coding model that captures the interaction between medulla and lobula neurons in the visual pathway, incorporating sequential scanning of visual stimuli to optimise information processing. In this model, each wide-field lobula neuron receives synaptic input from <italic>M</italic> small-field medulla neurons, with a structured progressive delay <italic>T</italic> (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Here, <inline-formula><alternatives><mml:math id="inf22"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math><tex-math id="inft22">\begin{document}$M=5$\end{document}</tex-math></alternatives></inline-formula> represents the number of temporal instances within the model’s input sequence. Each medulla neuron is activated by visual information sampled from one of <italic>M</italic> overlapping segments of an image patch, determined by the scanning speed, and follows the hierarchical processing pathway from photoreceptors to lamina neurons (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). While individual medulla neurons encode only a fraction of the visual stimulus, the lobula neuron integrates input from all medulla neurons to generate spiking activity, thereby forming a holistic representation of the entire visual scene. The synaptic transmission between medulla and lobula neurons incorporates structured temporal delays at distinct instances <inline-formula><alternatives><mml:math id="inf23"><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft23">\begin{document}$\left (T,2T,3T,4T,5T\right)$\end{document}</tex-math></alternatives></inline-formula>, ensuring that sequentially acquired visual information is temporally aligned. This results in the synchronised activation of the lobula neuron at a single unified time point, effectively integrating spatially and temporally structured input into a cohesive internal representation. By simulating the dynamic interplay between spatial sampling and temporal integration, this model mirrors the way bees may optimise visual processing through active vision. The resulting alignment of visual signals enhances feature extraction and pattern recognition, providing a biologically plausible mechanism for encoding complex scenes efficiently (<xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig6">6</xref>).</p><p>The model incorporates lateral inhibitory connections between lobula neurons (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, depicted in red), <inline-formula><alternatives><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft24">\begin{document}$Q=\left [q_{i,j}\right]$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft25">\begin{document}$q_{i,j}$\end{document}</tex-math></alternatives></inline-formula> represents the lateral connectivity between <italic>i</italic>th and <italic>j</italic>th lobula neurons. This connectivity along with <inline-formula><alternatives><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft26">\begin{document}$W_{m,l}$\end{document}</tex-math></alternatives></inline-formula> are updated during a non-associative learning process, to reduce redundancy and decorrelate overlapping inputs (see next subsection). This inhibitory mechanism enhances contrast and improves pattern recognition by selectively amplifying novel spatial features (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig6">6</xref>).</p><p>The processed visual information is then transformed to the KCs in the mushroom body. The synaptic connectivity matrix <inline-formula><alternatives><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>K</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$S^{Lo\rightarrow KC}=\left [s_{o,k}\right]$\end{document}</tex-math></alternatives></inline-formula> determines the excitatory connections between lobula neurons and KCs in the mushroom body, following previous findings on sparse, random connectivity (<xref ref-type="bibr" rid="bib17">Caron et al., 2013</xref>; <xref ref-type="bibr" rid="bib140">Szyszka et al., 2005</xref>). KCs exhibit sparse activity, with fewer than 5% of KCs activated per stimulus, ensuring high selectivity for particular image features (<xref ref-type="bibr" rid="bib60">Honegger et al., 2011</xref>). This sparsity emerges naturally from the random connectivity and thresholder activation dynamics, reinforcing the sparseness in the model. Each connection weight <inline-formula><alternatives><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft28">\begin{document}$s_{o,k}$\end{document}</tex-math></alternatives></inline-formula> is randomly initialised from a uniform distribution in the range <inline-formula><alternatives><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft29">\begin{document}$\left [0,w_{max}\right]$\end{document}</tex-math></alternatives></inline-formula> , where <inline-formula><alternatives><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft30">\begin{document}$w_{max}$\end{document}</tex-math></alternatives></inline-formula> is a scaling factor ensuring the limitation of input to the KCs. For each simulated bee, the connectivity matrix <inline-formula><alternatives><mml:math id="inf31"><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mo>→</mml:mo><mml:mi>K</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft31">\begin{document}$S^{Lo\rightarrow KC}$\end{document}</tex-math></alternatives></inline-formula> is randomly reinitialised, ensuring that each instance of the model has a unique but statistically comparable connectivity structure. This reinitialisation reflects individual variations in synaptic wiring and allows us to assess the robustness of the model’s pattern recognition ability across different randomly generated network configurations.</p><p>All KCs project to a single MBON, which is the final output of the model. The input of the of the MBON, <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$I_{MBON},$\end{document}</tex-math></alternatives></inline-formula> is computed by the KC–MBON connections <inline-formula><alternatives><mml:math id="inf33"><mml:mi>D</mml:mi></mml:math><tex-math id="inft33">\begin{document}$D$\end{document}</tex-math></alternatives></inline-formula> such that <inline-formula><alternatives><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>B</mml:mi><mml:mi>O</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft34">\begin{document}$I_{MBON}=\sum _{k=1}^{K}D_{k}r_{k}^{KC}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$r_{k}^{KC}$\end{document}</tex-math></alternatives></inline-formula> is the spiking activity of the <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$k$\end{document}</tex-math></alternatives></inline-formula> th KCs. Finally, a reinforcement neuron makes reinforcement-modulated connections with the KCs and MBON in the presence of the positive and negative patterns (see next subsection).</p><p>Each neuron type is denoted with a superscript corresponding to its respective processing stage. For example, <inline-formula><alternatives><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft37">\begin{document}$r_{p}$\end{document}</tex-math></alternatives></inline-formula>, represents the response of an individual photoreceptor, while <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$r^{La}$\end{document}</tex-math></alternatives></inline-formula> denotes the activity of a lamina neuron, where ‘La’ refers to the lamina layer. Similarly, other superscripts—Me, Lo, KC, and MBON—correspond to specific network components: the medulla, lobula, KCs, and MBONs, respectively. This ensures consistency in notation throughout the model description.</p></sec><sec id="s4-2"><title>Training the network via a non-associative learning</title><p>We trained the model using 50,000 time-varying image patches randomly sampled from a dataset of 100 natural flowers and scene images. During each training step, the model received an input sequence of five sequential 75 × 75-pixel patches, extracted by shifting 15 pixels across the image from the left or right or the reverse orientation (<xref ref-type="fig" rid="fig1">Figures 1B</xref> and <xref ref-type="fig" rid="fig2">2B</xref>). This patchwise input simulates the sequential scanning behaviour of bees as they explore visual stimuli.</p><p>Using the described network architecture, each time-varying patch dynamically drives spiking activity in lobula neurons as the simulated movement progresses. At the start of training, all inhibitory connection strengths <inline-formula><alternatives><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft39">\begin{document}$Q$\end{document}</tex-math></alternatives></inline-formula> were initialised with values randomly drawn from a uniform distribution between 0 and 1. The feedforward synaptic weights <inline-formula><alternatives><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft40">\begin{document}$W$\end{document}</tex-math></alternatives></inline-formula> were initialised using Gaussian white noise <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$N\left (0,1\right)$\end{document}</tex-math></alternatives></inline-formula>. As training progressed, the evoked neural responses of lobula neurons to time-varying patches were used to iteratively update both the inhibitory weights (<inline-formula><alternatives><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft42">\begin{document}$Q$\end{document}</tex-math></alternatives></inline-formula>) and feedforward connections (<inline-formula><alternatives><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$W$\end{document}</tex-math></alternatives></inline-formula>) simultaneously (see Discussion).</p><p>After the image presentation, the feedforward weight <inline-formula><alternatives><mml:math id="inf44"><mml:mi>W</mml:mi></mml:math><tex-math id="inft44">\begin{document}$W$\end{document}</tex-math></alternatives></inline-formula> is updated according to Oja’s implementation of the Hebbian learning rule (<xref ref-type="bibr" rid="bib83">MaBouDi et al., 2017</xref>; <xref ref-type="bibr" rid="bib106">Oja, 1982</xref>) via<disp-formula id="equ2"><label>(1)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle  \Delta W_{i,j}=\gamma r_{j}^{Me}\left (r_{i}^{La}- r_{j}^{Me}W_{i,j}\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, the <inline-formula><alternatives><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft45">\begin{document}$r_{j}^{Me}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$r_{i}^{La}$\end{document}</tex-math></alternatives></inline-formula> represent the activities of the <italic>j</italic>th medulla and <italic>i</italic>th lamina neurons, respectively. The positive constant <italic>γ</italic> defines the learning rate.</p><p>At the same time of processing, the lateral inhibitory connectivity in the lobula is modified by iSTDP (<xref ref-type="bibr" rid="bib149">Vogels et al., 2011</xref>). Here, we model <italic>non-associative learning</italic> in the lobula by a symmetric iSTDP between presynaptic of the inhibitory neurons and postsynaptic lobula neurons. In this learning rule, both temporal ordering of pre- or post-synaptic spikes potentiates the connectivity and the synaptic strength of <italic>j</italic>th inhibitory neuron onto <italic>i</italic>th lobula neuron (<inline-formula><alternatives><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft47">\begin{document}$Q_{i,j}$\end{document}</tex-math></alternatives></inline-formula>) is updated as follows:<disp-formula id="equ3"><label>(2)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msubsup><mml:mo>∗</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle  \Delta Q_{i,j}=\eta \left (r_{i}^{Lo}\ast r_{j}^{In}- \alpha \right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf48"><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft48">\begin{document}$r_{i}^{Lo}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math><tex-math id="inft49">\begin{document}$r_{j}^{In}$\end{document}</tex-math></alternatives></inline-formula> exhibit the mean firing rate of the lobula and inhibitory neurons, respectively. The depression factor <inline-formula><alternatives><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft50">\begin{document}$\alpha $\end{document}</tex-math></alternatives></inline-formula> controls the target activity rate of the lobula neurons. Here, <inline-formula><alternatives><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft51">\begin{document}$\eta $\end{document}</tex-math></alternatives></inline-formula> is the learning rate. To simplify, a one-to-one connection between the inhibitory and lobula neuron is assumed in the model such that the activity of the <italic>j</italic>th inhibitory neuron is equal to the activity of the <italic>j</italic>th lobula neuron. The training is terminated when the synaptic weights over time are changed less than a small threshold (0.001). In the training process, synaptic weights were constrained within the range <inline-formula><alternatives><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft52">\begin{document}$\left [- 1,+1\right]$\end{document}</tex-math></alternatives></inline-formula> for the <inline-formula><alternatives><mml:math id="inf53"><mml:mi>W</mml:mi></mml:math><tex-math id="inft53">\begin{document}$W$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft54">\begin{document}$\left [0,+1\right]$\end{document}</tex-math></alternatives></inline-formula> for <inline-formula><alternatives><mml:math id="inf55"><mml:mi>Q</mml:mi></mml:math><tex-math id="inft55">\begin{document}$Q$\end{document}</tex-math></alternatives></inline-formula> to ensure stable convergence and to reflect biological limitations in synaptic transmission strength.</p></sec><sec id="s4-3"><title>Associative learning in mushroom bodies</title><p>To verify if the lobula neurons can reproduce empirical behavioural results in different visual tasks, the model is enriched with associative learning process in the mushroom bodies (a bio-inspired supervised learning). When the training process of the non-associative learning is terminated, we use a reward-based synaptic wright modification rule in KCs–MBON connection (D), such that, if a stimulus is rewarding (i.e. positive), the corresponding synapses between activated neurons will be weakened while for a stimulus paired with punishment (i.e. negative), activated synapses are strengthened (<xref ref-type="bibr" rid="bib19">Cassenaer and Laurent, 2012</xref>) (see Discussion). The model behaves as the activity of mushroom body neurons in decreasing their firing rate in responding to the positive stimuli during training (<xref ref-type="bibr" rid="bib107">Okada et al., 2007</xref>). In this model, two reinforcement neurons modulated strengths of synaptic connectivity at the output of the KCs in response to both reward and punishment. In the presence of the negative patterns, the synaptic strengths from the KCs to the MBON are modified, and modulated by dopamine, based on the classical STDP (<xref ref-type="bibr" rid="bib128">Song et al., 2000</xref>; <xref ref-type="bibr" rid="bib159">Zhang et al., 1998</xref>; <xref ref-type="fig" rid="fig9">Figure 9</xref>):<disp-formula id="equ4"><label>(3)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle  STDP_{Dop}\left (\Delta t\right)=\begin{cases} Ae^{- \Delta t/\tau }, \Delta t\gt 0\\- Ae^{\Delta t/\tau }, \Delta t\lt 0\end{cases} ,$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft56">\begin{document}$\Delta t=t_{post}- t_{pre}$\end{document}</tex-math></alternatives></inline-formula> implies the difference between the spike time of pre- and post-synaptic neurons. Further, applying the synaptic plasticity rule modulated by octopamine (octopamine-modulated STDP) observed in the presence of rewarding stimuli to the synapses between KCs and MBON (<xref ref-type="bibr" rid="bib19">Cassenaer and Laurent, 2012</xref>), the change in synaptic weight can be summarised as (<xref ref-type="fig" rid="fig4">Figure 4B</xref>):<disp-formula id="equ5"><label>(4)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>C</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  STDP_{OCT}\left (\Delta t\right)=\begin{cases} - Ae^{- \Delta t/\tau }, \Delta t\gt 0\\- Ae^{\Delta t/\tau }, \Delta t\lt 0\end{cases} ,$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf57"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:math><tex-math id="inft57">\begin{document}$A=0.01$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mstyle></mml:math><tex-math id="inft58">\begin{document}$\tau =20\,\rm ms$\end{document}</tex-math></alternatives></inline-formula> exhibit the maximum magnitude and time constant of the STDP function for the synaptic potentiation or depression.</p><p>To train the model in different conditions of scanning, the flight-scan forms of the positive and negative patterns were presented to the model. Each set of flight-scan input contained a set of five patches with size 75 × 75 pixels were selected from the test patterns by shifting 15 pixels over each pattern from the left to right (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The numbers of shifted pixels control the speed of scanning. The activity of the MBON was used to assess the performance of the model. Following the training, the performance of the model was calculated from a decrease in firing rate of the MBON to a pattern that had been rewarding and/or an increase in firing rate of MBON to a pattern that had been punishing in training. The bee’s final behavioural decision is proposed to come from a simple integration of these different valence-encoding neurons.</p></sec><sec id="s4-4"><title>Quantifying neural population sparseness and response separability</title><p>To quantify population sparseness in the lobula following training on natural images, we employed the Treves–Rolls sparseness index (<xref ref-type="bibr" rid="bib83">MaBouDi et al., 2017</xref>; <xref ref-type="bibr" rid="bib156">Willmore and Tolhurst, 2001</xref>), defined as:<disp-formula id="equ6"><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle  SI=\frac{\left (\sum _{j=1}^{N}r_{j}/N\right)^{2}}{\left (\sum _{j=1}^{N}r_{j}^{2}\right)/N}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft59">\begin{document}$r_{j}$\end{document}</tex-math></alternatives></inline-formula> represents the firing rate of the <italic>j</italic>th lobula neuron, and <italic>N</italic> is the total number of lobula neurons.</p><p>This metric provides insight into the population coding strategy of lobula neurons, distinguishing between broad distributed representations and sparse selective responses:</p><list list-type="bullet" id="list1"><list-item><p><italic>Maximum SI = 1 (low sparseness)</italic>: Achieved when all neurons respond equally, indicating a fully distributed code where the entire population is uniformly active across all stimuli.</p></list-item><list-item><p><italic>Minimum SI = 1/N (high sparseness)</italic>: Occurs when only a single neuron is active while all others remain silent, reflecting a highly selective encoding scheme.</p></list-item></list><p>To further assess the separability between response population of lobula neurons in response to different stimuli, we computed the angular distance (<inline-formula><alternatives><mml:math id="inf60"><mml:mi>θ</mml:mi></mml:math><tex-math id="inft60">\begin{document}$\theta $\end{document}</tex-math></alternatives></inline-formula>) using the cosine similarity formula: <inline-formula><alternatives><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft61">\begin{document}$\theta =cos^{- 1} \left (R_{1}.R_{2}/\left (\left |R_{1}\right |\left |R_{1}\right |\right)\right.$\end{document}</tex-math></alternatives></inline-formula><italic>,</italic> where <inline-formula><alternatives><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math><tex-math id="inft62">\begin{document}$R_{1} and\, R_{2}$\end{document}</tex-math></alternatives></inline-formula> represent the activity vectors of two neural populations, and <inline-formula><alternatives><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math><tex-math id="inft63">\begin{document}$\left |R_{1}\right | and \left |R_{2}\right |$\end{document}</tex-math></alternatives></inline-formula> denote their responses Euclidean norms. This measure captures the geometric distinction between response patterns, with larger angles indicating greater separability between neural representations of different stimuli. A higher angular distance suggests that the population responses are more distinct, reflecting improved stimulus discrimination within the lobula.</p></sec><sec id="s4-5"><title>Simulation and statistical analysis</title><p>To assess the model’s performance across experiments, we conducted 20 independent simulations for each condition, ensuring statistical reliability and robustness. In each simulation, the synaptic connectivity matrix between lobula neurons and KCs was randomly initialised using a uniform distribution within a biologically plausible range, mimicking the individual variability observed in real bees. This stochastic initialisation prevented bias in learning outcomes and allowed us to examine how the model generalises across different neural configurations.</p><p>Each simulated bee underwent multiple training exposures to visual stimuli. Following training, once the model parameters were fixed at the final stage of training, the model was tested across 50 repetitions per condition with testing patterns to account for variability in responses. The model’s performance was evaluated by averaging across simulations, with SEM reported in the figures to provide a statistically robust representation of discrimination accuracy.</p><p>Statistical analyses were conducted to compare pattern discrimination across conditions. To maintain clarity and focus on the modelling findings, detailed significance values are not reported in the main text. Instead, figures indicate p-values &lt;0.05 with ‘*’ and non-significant results with ‘n.s.’. Since data distributions did not always meet normality assumptions, we used the Wilcoxon signed-rank test for matched data and the Wilcoxon rank-sum test (Mann–Whitney <italic>U</italic> test) for independent samples. For comparisons across multiple groups, we applied the Kruskal–Wallis test, followed by Dunn’s post hoc test when necessary.</p></sec><sec id="s4-6"><title>Computing environment</title><p>All modelling and visualisation were performed using MATLAB (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_001622">SCR_001622</ext-link>) and Python (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_008394">SCR_008394</ext-link>). MATLAB was also used for statistical analysis.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>is affiliated with Ben Thorns Ltd</p></fn><fn fn-type="COI-statement" id="conf3"><p>is co-founder and Chief Scientific Officer at Opteran Technologies</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Funding acquisition, Validation, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Supervision, Funding acquisition, Validation, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-89929-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All figures were generated during the simulation. The code developed for this research project has been made openly accessible on <ext-link ext-link-type="uri" xlink:href="https://github.com/hadiimaboudi/Insect-Inspired-Neuromorphic-Model-for-Active-Vision">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib90">MaBouDi, 2025</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Paul Graham and Andrew Barron for valuable comments on manuscript and Alice Bridges for drawing the front view of the bumblebee presented in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. This research was financed by HFSP programme grant RGP0022/2014, EPSRC program grant Brains-on-Board EP/Poo6094/1 and by Horizon Europe Framework Programme grant NimbleAI. MJ was supported by BBSRC (BB/F012071/1 and BB/X006247/1), EPSRC (EP/X019705/1), and Leverhulme (RPG-2024-016). MGG was supported by ARC Discovery Projects DP230100006 and DP210100740 and Templeton World Charity Foundation Project Grant TWCF-2020-20539.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Synaptic plasticity: taming the beast</article-title><source>Nature Neuroscience</source><volume>3 Suppl</volume><fpage>1178</fpage><lpage>1183</lpage><pub-id pub-id-type="doi">10.1038/81453</pub-id><pub-id pub-id-type="pmid">11127835</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amin</surname><given-names>AA</given-names></name><name><surname>Kagioulis</surname><given-names>E</given-names></name><name><surname>Domcsek</surname><given-names>N</given-names></name><name><surname>Nowotny</surname><given-names>T</given-names></name><name><surname>Graham</surname><given-names>P</given-names></name><name><surname>Philippides</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Investigating the limits of familiarity-based navigation</article-title><source>Artificial Life</source><volume>31</volume><fpage>211</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1162/artl_a_00459</pub-id><pub-id pub-id-type="pmid">39485342</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>AG</given-names></name><name><surname>Ratnam</surname><given-names>K</given-names></name><name><surname>Roorda</surname><given-names>A</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>High-acuity vision from retinal image motion</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>34</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.7.34</pub-id><pub-id pub-id-type="pmid">32735342</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ardin</surname><given-names>P</given-names></name><name><surname>Peng</surname><given-names>F</given-names></name><name><surname>Mangan</surname><given-names>M</given-names></name><name><surname>Lagogiannis</surname><given-names>K</given-names></name><name><surname>Webb</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using an insect mushroom body circuit to encode route memory in complex natural environments</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004683</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004683</pub-id><pub-id pub-id-type="pmid">26866692</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arenas</surname><given-names>A</given-names></name><name><surname>Farina</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Age and rearing environment interact in the retention of early olfactory memories in honeybees</article-title><source>Journal of Comparative Physiology A</source><volume>194</volume><fpage>629</fpage><lpage>640</lpage><pub-id pub-id-type="doi">10.1007/s00359-008-0337-z</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arenz</surname><given-names>A</given-names></name><name><surname>Drews</surname><given-names>MS</given-names></name><name><surname>Richter</surname><given-names>FG</given-names></name><name><surname>Ammer</surname><given-names>G</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The temporal tuning of the <italic>Drosophila</italic> motion detectors is determined by the dynamics of their input elements</article-title><source>Current Biology</source><volume>27</volume><fpage>929</fpage><lpage>944</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.01.051</pub-id><pub-id pub-id-type="pmid">28343964</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aso</surname><given-names>Y</given-names></name><name><surname>Hattori</surname><given-names>D</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Johnston</surname><given-names>RM</given-names></name><name><surname>Iyer</surname><given-names>NA</given-names></name><name><surname>Ngo</surname><given-names>T-TB</given-names></name><name><surname>Dionne</surname><given-names>H</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Tanimoto</surname><given-names>H</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neuronal architecture of the mushroom body provides a logic for associative learning</article-title><source>eLife</source><volume>3</volume><elocation-id>e04577</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04577</pub-id><pub-id pub-id-type="pmid">25535793</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avarguès-Weber</surname><given-names>A</given-names></name><name><surname>Dyer</surname><given-names>AG</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Conceptualization of above and below relationships by an insect</article-title><source>Proceedings of the Royal Society London - B Biological Sciences</source><volume>278</volume><fpage>898</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1098/rspb.2010.1891</pub-id><pub-id pub-id-type="pmid">21068040</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname><given-names>B</given-names></name><name><surname>Graham</surname><given-names>P</given-names></name><name><surname>Husbands</surname><given-names>P</given-names></name><name><surname>Philippides</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A model of ant route navigation driven by scene familiarity</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002336</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002336</pub-id><pub-id pub-id-type="pmid">22241975</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><year iso-8601-date="1961">1961</year><chapter-title>Possible principles underlying the transformation of sensory messages</chapter-title><person-group person-group-type="editor"><name><surname>Rosenblith</surname><given-names>WA</given-names></name></person-group><source>Sensory Communication</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9780262518420.003.0013</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benard</surname><given-names>J</given-names></name><name><surname>Stach</surname><given-names>S</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Categorization of visual stimuli in the honeybee <italic>Apis mellifera</italic></article-title><source>Animal Cognition</source><volume>9</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1007/s10071-006-0032-9</pub-id><pub-id pub-id-type="pmid">16909238</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertrand</surname><given-names>OJN</given-names></name><name><surname>Doussot</surname><given-names>C</given-names></name><name><surname>Siesenop</surname><given-names>T</given-names></name><name><surname>Ravi</surname><given-names>S</given-names></name><name><surname>Egelhaaf</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Visual and movement memories steer foraging bumblebees along habitual routes</article-title><source>The Journal of Experimental Biology</source><volume>224</volume><elocation-id>jeb237867</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.237867</pub-id><pub-id pub-id-type="pmid">34115117</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boeddeker</surname><given-names>N</given-names></name><name><surname>Mertes</surname><given-names>M</given-names></name><name><surname>Dittmar</surname><given-names>L</given-names></name><name><surname>Egelhaaf</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bumblebee homing: the fine structure of head turning movements</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0135020</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0135020</pub-id><pub-id pub-id-type="pmid">26352836</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brinkworth</surname><given-names>RSA</given-names></name><name><surname>O’Carroll</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Robust models for optic flow coding in natural scenes inspired by insect biology</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000555</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000555</pub-id><pub-id pub-id-type="pmid">19893631</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Briscoe</surname><given-names>AD</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The evolution of color vision in insects</article-title><source>Annual Review of Entomology</source><volume>46</volume><fpage>471</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1146/annurev.ento.46.1.471</pub-id><pub-id pub-id-type="pmid">11112177</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burke</surname><given-names>CJ</given-names></name><name><surname>Huetteroth</surname><given-names>W</given-names></name><name><surname>Owald</surname><given-names>D</given-names></name><name><surname>Perisse</surname><given-names>E</given-names></name><name><surname>Krashes</surname><given-names>MJ</given-names></name><name><surname>Das</surname><given-names>G</given-names></name><name><surname>Gohl</surname><given-names>D</given-names></name><name><surname>Silies</surname><given-names>M</given-names></name><name><surname>Certel</surname><given-names>S</given-names></name><name><surname>Waddell</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Layered reward signalling through octopamine and dopamine in <italic>Drosophila</italic></article-title><source>Nature</source><volume>492</volume><fpage>433</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1038/nature11614</pub-id><pub-id pub-id-type="pmid">23103875</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caron</surname><given-names>SJC</given-names></name><name><surname>Ruta</surname><given-names>V</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Random convergence of olfactory inputs in the <italic>Drosophila</italic> mushroom body</article-title><source>Nature</source><volume>497</volume><fpage>113</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1038/nature12063</pub-id><pub-id pub-id-type="pmid">23615618</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cassenaer</surname><given-names>S</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hebbian STDP in mushroom bodies facilitates the synchronous flow of olfactory information in locusts</article-title><source>Nature</source><volume>448</volume><fpage>709</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1038/nature05973</pub-id><pub-id pub-id-type="pmid">17581587</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cassenaer</surname><given-names>S</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Conditional modulation of spike-timing-dependent plasticity for olfactory learning</article-title><source>Nature</source><volume>482</volume><fpage>47</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1038/nature10776</pub-id><pub-id pub-id-type="pmid">22278062</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Niven</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Are bigger brains better?</article-title><source>Current Biology</source><volume>19</volume><fpage>R995</fpage><lpage>R1008</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.08.023</pub-id><pub-id pub-id-type="pmid">19922859</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Active vision: a broader comparative perspective is needed</article-title><source>Constructivist Foundations</source><volume>13</volume><fpage>128</fpage><lpage>129</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><source>The Mind of a Bee</source><publisher-name>Princeton University Press</publisher-name><pub-id pub-id-type="doi">10.1515/9780691236247</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>DA</given-names></name><name><surname>Fitzgerald</surname><given-names>JE</given-names></name><name><surname>Ales</surname><given-names>JM</given-names></name><name><surname>Gohl</surname><given-names>DM</given-names></name><name><surname>Silies</surname><given-names>MA</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Flies and humans share a motion estimation strategy that exploits natural scene statistics</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>296</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nn.3600</pub-id><pub-id pub-id-type="pmid">24390225</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cognigni</surname><given-names>P</given-names></name><name><surname>Felsenberg</surname><given-names>J</given-names></name><name><surname>Waddell</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Do the right thing: neural network mechanisms of memory formation, expression and update in <italic>Drosophila</italic></article-title><source>Current Opinion in Neurobiology</source><volume>49</volume><fpage>51</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.12.002</pub-id><pub-id pub-id-type="pmid">29258011</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname><given-names>TS</given-names></name><name><surname>Fry</surname><given-names>SN</given-names></name><name><surname>Wehner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Sequence learning by honeybees</article-title><source>Journal of Comparative Physiology A</source><volume>172</volume><fpage>693</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1007/BF00195395</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cope</surname><given-names>AJ</given-names></name><name><surname>Vasilaki</surname><given-names>E</given-names></name><name><surname>Minors</surname><given-names>D</given-names></name><name><surname>Sabo</surname><given-names>C</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Abstract concept learning in a simple neural network inspired by the insect brain</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006435</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006435</pub-id><pub-id pub-id-type="pmid">30222735</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>AM</given-names></name><name><surname>Kaushik</surname><given-names>S</given-names></name><name><surname>Hige</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Dopamine-dependent plasticity is heterogeneously expressed by presynaptic calcium activity across individual boutons of the <italic>Drosophila</italic> mushroom body</article-title><source>eNeuro</source><volume>10</volume><elocation-id>ENEURO.0275-23.2023</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0275-23.2023</pub-id><pub-id pub-id-type="pmid">37848287</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dawkins</surname><given-names>MS</given-names></name><name><surname>Woodington</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Pattern recognition and active vision in chickens</article-title><source>Nature</source><volume>403</volume><fpage>652</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1038/35001064</pub-id><pub-id pub-id-type="pmid">10688200</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Croon</surname><given-names>G</given-names></name><name><surname>Dupeyroux</surname><given-names>JJG</given-names></name><name><surname>Fuller</surname><given-names>SB</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Insect-inspired AI for autonomous robots</article-title><source>Science Robotics</source><volume>7</volume><elocation-id>eabl6334</elocation-id><pub-id pub-id-type="doi">10.1126/scirobotics.abl6334</pub-id><pub-id pub-id-type="pmid">35704608</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doussot</surname><given-names>C</given-names></name><name><surname>Bertrand</surname><given-names>OJN</given-names></name><name><surname>Egelhaaf</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The critical role of head movements for spatial representation during bumblebees learning flight</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>14</volume><elocation-id>606590</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2020.606590</pub-id><pub-id pub-id-type="pmid">33542681</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyakova</surname><given-names>O</given-names></name><name><surname>Lee</surname><given-names>YJ</given-names></name><name><surname>Longden</surname><given-names>KD</given-names></name><name><surname>Kiselev</surname><given-names>VG</given-names></name><name><surname>Nordström</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A higher order visual neuron tuned to the spatial amplitude spectra of natural scenes</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8522</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9522</pub-id><pub-id pub-id-type="pmid">26439748</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyakova</surname><given-names>O</given-names></name><name><surname>Nordström</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Image statistics and their processing in insect vision</article-title><source>Current Opinion in Insect Science</source><volume>24</volume><fpage>7</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.cois.2017.08.002</pub-id><pub-id pub-id-type="pmid">29208226</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyakova</surname><given-names>O</given-names></name><name><surname>Müller</surname><given-names>MM</given-names></name><name><surname>Egelhaaf</surname><given-names>M</given-names></name><name><surname>Nordström</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Image statistics of the environment surrounding freely behaving hoverflies</article-title><source>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</source><volume>205</volume><fpage>373</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1007/s00359-019-01329-1</pub-id><pub-id pub-id-type="pmid">30937518</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyer</surname><given-names>AG</given-names></name><name><surname>Neumeyer</surname><given-names>C</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Honeybee (<italic>Apis mellifera</italic>) vision can discriminate between and recognise images of human faces</article-title><source>The Journal of Experimental Biology</source><volume>208</volume><fpage>4709</fpage><lpage>4714</lpage><pub-id pub-id-type="doi">10.1242/jeb.01929</pub-id><pub-id pub-id-type="pmid">16326952</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyer</surname><given-names>AG</given-names></name><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Reser</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Colour processing in complex environments: insights from the visual system of bees</article-title><source>Proceedings of the Royal Society London B - Biological Sciences</source><volume>278</volume><fpage>952</fpage><lpage>959</lpage><pub-id pub-id-type="doi">10.1098/rspb.2010.2412</pub-id><pub-id pub-id-type="pmid">21147796</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Egelhaaf</surname><given-names>M</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Lindemann</surname><given-names>JP</given-names></name><name><surname>Braun</surname><given-names>E</given-names></name><name><surname>Geurten</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><chapter-title>Active vision in blowflies: strategies and mechanisms of spatial orientation in</chapter-title><person-group person-group-type="editor"><name><surname>Floreano</surname><given-names>D</given-names></name><name><surname>Zufferey</surname><given-names>JC</given-names></name><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Ellington</surname><given-names>C</given-names></name></person-group><source>Flying Insects and Robots</source><publisher-name>Springer</publisher-name><fpage>51</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1007/978-3-540-89393-6_4</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehmer</surname><given-names>B</given-names></name><name><surname>Gronenberg</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Segregation of visual input to the mushroom bodies in the honeybee (<italic>Apis mellifera</italic>)</article-title><source>The Journal of Comparative Neurology</source><volume>451</volume><fpage>362</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1002/cne.10355</pub-id><pub-id pub-id-type="pmid">12210130</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fenk</surname><given-names>LM</given-names></name><name><surname>Avritzer</surname><given-names>SC</given-names></name><name><surname>Weisman</surname><given-names>JL</given-names></name><name><surname>Nair</surname><given-names>A</given-names></name><name><surname>Randt</surname><given-names>LD</given-names></name><name><surname>Mohren</surname><given-names>TL</given-names></name><name><surname>Siwanowicz</surname><given-names>I</given-names></name><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Muscles that move the retina augment compound eye vision in <italic>Drosophila</italic></article-title><source>Nature</source><volume>612</volume><fpage>116</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05317-5</pub-id><pub-id pub-id-type="pmid">36289333</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiala</surname><given-names>A</given-names></name><name><surname>Kaun</surname><given-names>KR</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>What do the mushroom bodies do for the insect brain? Twenty-five years of progress</article-title><source>Learning &amp; Memory</source><volume>31</volume><elocation-id>a053827</elocation-id><pub-id pub-id-type="doi">10.1101/lm.053827.123</pub-id><pub-id pub-id-type="pmid">38862175</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>YE</given-names></name><name><surname>Silies</surname><given-names>M</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Orientation selectivity sharpens motion detection in <italic>Drosophila</italic></article-title><source>Neuron</source><volume>88</volume><fpage>390</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.033</pub-id><pub-id pub-id-type="pmid">26456048</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>YE</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Octopamine enhances learning</article-title><source>National Science Review</source><volume>11</volume><elocation-id>nwae185</elocation-id><pub-id pub-id-type="doi">10.1093/nsr/nwae185</pub-id><pub-id pub-id-type="pmid">38953005</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual perception and the statistical properties of natural scenes</article-title><source>Annual Review of Psychology</source><volume>59</volume><fpage>167</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.58.110405.085632</pub-id><pub-id pub-id-type="pmid">17705683</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>The ecological approach to the visual perception of pictures</article-title><source>Leonardo</source><volume>11</volume><elocation-id>1574154</elocation-id><pub-id pub-id-type="doi">10.2307/1574154</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giger</surname><given-names>AD</given-names></name><name><surname>Srinivasan</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Pattern recognition in honeybees: chromatic properties of orientation analysis</article-title><source>Journal of Comparative Physiology A</source><volume>178</volume><fpage>763</fpage><lpage>769</lpage><pub-id pub-id-type="doi">10.1007/BF00225824</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Hammer</surname><given-names>M</given-names></name><name><surname>Stach</surname><given-names>S</given-names></name><name><surname>Stollhoff</surname><given-names>N</given-names></name><name><surname>Müller-deisig</surname><given-names>N</given-names></name><name><surname>Mizyrycki</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Pattern learning by honeybees: conditioning procedure and recognition strategy</article-title><source>Animal Behaviour</source><volume>57</volume><fpage>315</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1006/anbe.1998.0957</pub-id><pub-id pub-id-type="pmid">10049470</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Jenett</surname><given-names>A</given-names></name><name><surname>Menzel</surname><given-names>R</given-names></name><name><surname>Srinivasan</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The concepts of “sameness” and “difference” in an insect</article-title><source>Nature</source><volume>410</volume><fpage>930</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/35073582</pub-id><pub-id pub-id-type="pmid">11309617</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cognition with few neurons: higher-order learning in insects</article-title><source>Trends in Neurosciences</source><volume>36</volume><fpage>285</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2012.12.011</pub-id><pub-id pub-id-type="pmid">23375772</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gribakin</surname><given-names>FG</given-names></name></person-group><year iso-8601-date="1975">1975</year><chapter-title>Functional morphology of the compound eye of the bee</chapter-title><person-group person-group-type="editor"><name><surname>Horridge</surname><given-names>GA</given-names></name></person-group><source>The Compound Eye and Vision of Insects</source><publisher-name>Oxford University Press</publisher-name><fpage>154</fpage><lpage>176</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guiraud</surname><given-names>M</given-names></name><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High-speed videography reveals how honeybees can turn a spatial concept learning task into a simple discrimination task by stereotyped flight movements and sequential inspection of pattern elements</article-title><source>Frontiers in Psychology</source><volume>9</volume><elocation-id>1347</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2018.01347</pub-id><pub-id pub-id-type="pmid">30123157</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guiraud</surname><given-names>M</given-names></name><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Wolf</surname><given-names>S</given-names></name><name><surname>Woodgate</surname><given-names>JL</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Discrimination of edge orientation by bumblebees</article-title><source>PLOS ONE</source><volume>17</volume><elocation-id>e0263198</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0263198</pub-id><pub-id pub-id-type="pmid">35709295</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guiraud</surname><given-names>MG</given-names></name><name><surname>Gallo</surname><given-names>V</given-names></name><name><surname>Quinsal-Keel</surname><given-names>E</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2025">2025a</year><article-title>Bumble bee visual learning: simple solutions for complex stimuli</article-title><source>Animal Behaviour</source><volume>221</volume><elocation-id>123070</elocation-id><pub-id pub-id-type="doi">10.1016/j.anbehav.2024.123070</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guiraud</surname><given-names>MG</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Woodgate</surname><given-names>J</given-names></name><name><surname>Bates</surname><given-names>OK</given-names></name><name><surname>Rodriguez</surname><given-names>OR</given-names></name><name><surname>Gallo</surname><given-names>V</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2025">2025b</year><article-title>How bumblebees manage conflicting information seen on arrival and departure from flowers</article-title><source>Animal Cognition</source><volume>28</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1007/s10071-024-01926-x</pub-id><pub-id pub-id-type="pmid">39909894</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>An identified neuron mediates the unconditioned stimulus in associative olfactory learning in honeybees</article-title><source>Nature</source><volume>366</volume><fpage>59</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1038/366059a0</pub-id><pub-id pub-id-type="pmid">24308080</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammer</surname><given-names>M</given-names></name><name><surname>Menzel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Learning and memory in the honeybee</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>1617</fpage><lpage>1630</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-03-01617.1995</pub-id><pub-id pub-id-type="pmid">7891123</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hancock</surname><given-names>CE</given-names></name><name><surname>Rostami</surname><given-names>V</given-names></name><name><surname>Rachad</surname><given-names>EY</given-names></name><name><surname>Deimel</surname><given-names>SH</given-names></name><name><surname>Nawrot</surname><given-names>MP</given-names></name><name><surname>Fiala</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Visualization of learning-induced synaptic plasticity in output neurons of the <italic>Drosophila</italic> mushroom body γ-lobe</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>10421</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-14413-5</pub-id><pub-id pub-id-type="pmid">35729203</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardie</surname><given-names>RC</given-names></name><name><surname>Franze</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Photomechanical responses in <italic>Drosophila</italic> photoreceptors</article-title><source>Science</source><volume>338</volume><fpage>260</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1126/science.1222376</pub-id><pub-id pub-id-type="pmid">23066080</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heisenberg</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Mushroom body memoir: from maps to models</article-title><source>Nature Reviews Neuroscience</source><volume>4</volume><fpage>266</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1038/nrn1074</pub-id><pub-id pub-id-type="pmid">12671643</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertel</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>The effect of spectral light deprivation on the spectral sensitivity of the honey bee</article-title><source>Journal of Comparative Physiology A</source><volume>147</volume><fpage>365</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1007/BF00609670</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertel</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Change of synapse frequency in certain photoreceptors of the honeybee after chromatic deprivation</article-title><source>Journal of Comparative Physiology A</source><volume>151</volume><fpage>477</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1007/BF00605464</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honegger</surname><given-names>KS</given-names></name><name><surname>Campbell</surname><given-names>RAA</given-names></name><name><surname>Turner</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cellular-resolution population imaging reveals robust sparse coding in the <italic>Drosophila</italic> mushroom body</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>11772</fpage><lpage>11785</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1099-11.2011</pub-id><pub-id pub-id-type="pmid">21849538</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Hurri</surname><given-names>J</given-names></name><name><surname>Hoyer</surname><given-names>PO</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Natural Image Statistics: A Probabilistic Approach to Early Computational Vision</source><publisher-name>Springer Science &amp; Business Media</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-84882-491-1</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>James</surname><given-names>AC</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Characterisation of columnar neurons and visual signal processing in the medulla of the locust optic lobe by system identification techniques</article-title><source>Journal of Comparative Physiology A</source><volume>178</volume><fpage>183</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1007/BF00188161</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juston</surname><given-names>R</given-names></name><name><surname>Kerhuel</surname><given-names>L</given-names></name><name><surname>Franceschini</surname><given-names>N</given-names></name><name><surname>Viollet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hyperacute edge and bar detection in a bioinspired optical position sensing device</article-title><source>IEEE/ASME Transactions on Mechatronics</source><volume>19</volume><fpage>1025</fpage><lpage>1034</lpage><pub-id pub-id-type="doi">10.1109/TMECH.2013.2265983</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2001">2001a</year><article-title>Light adaptation in <italic>Drosophila</italic> photoreceptors: I. Response dynamics and signaling efficiency at 25 degrees C</article-title><source>The Journal of General Physiology</source><volume>117</volume><fpage>3</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1085/jgp.117.1.3</pub-id><pub-id pub-id-type="pmid">11134228</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2001">2001b</year><article-title>Light adaptation in <italic>Drosophila</italic> photoreceptors: II. Rising temperature increases the bandwidth of reliable signaling</article-title><source>The Journal of General Physiology</source><volume>117</volume><fpage>27</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1085/jgp.117.1.27</pub-id><pub-id pub-id-type="pmid">11134229</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>de Polavieja</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The rate of information transfer of naturalistic stimulation by graded potentials</article-title><source>The Journal of General Physiology</source><volume>122</volume><fpage>191</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1085/jgp.200308824</pub-id><pub-id pub-id-type="pmid">12860926</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Dau</surname><given-names>A</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Solanki</surname><given-names>N</given-names></name><name><surname>Rien</surname><given-names>D</given-names></name><name><surname>Jaciuch</surname><given-names>D</given-names></name><name><surname>Dongre</surname><given-names>SA</given-names></name><name><surname>Blanchard</surname><given-names>F</given-names></name><name><surname>de</surname><given-names>PG</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name><name><surname>Takalo</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Microsaccadic sampling of moving image information provides <italic>Drosophila</italic> hyperacute vision</article-title><source>eLife</source><volume>6</volume><elocation-id>e26117</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26117</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How a fly photoreceptor samples light information in time</article-title><source>The Journal of Physiology</source><volume>595</volume><fpage>5427</fpage><lpage>5437</lpage><pub-id pub-id-type="doi">10.1113/JP273645</pub-id><pub-id pub-id-type="pmid">28233315</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Takalo</surname><given-names>J</given-names></name><name><surname>Kemppainen</surname><given-names>J</given-names></name><name><surname>Haghighi</surname><given-names>KR</given-names></name><name><surname>Scales</surname><given-names>B</given-names></name><name><surname>McManus</surname><given-names>J</given-names></name><name><surname>Bridges</surname><given-names>A</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Theory of morphodynamic information processing: Linking sensing to behaviour</article-title><source>Vision Research</source><volume>227</volume><elocation-id>108537</elocation-id><pub-id pub-id-type="doi">10.1016/j.visres.2024.108537</pub-id><pub-id pub-id-type="pmid">39755072</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemppainen</surname><given-names>J</given-names></name><name><surname>Mansour</surname><given-names>N</given-names></name><name><surname>Takalo</surname><given-names>J</given-names></name><name><surname>Juusola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>High-speed imaging of light-induced photoreceptor microsaccades in compound eyes</article-title><source>Communications Biology</source><volume>5</volume><elocation-id>203</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-022-03142-0</pub-id><pub-id pub-id-type="pmid">35241794</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemppainen</surname><given-names>J</given-names></name><name><surname>Scales</surname><given-names>B</given-names></name><name><surname>Razban Haghighi</surname><given-names>K</given-names></name><name><surname>Takalo</surname><given-names>J</given-names></name><name><surname>Mansour</surname><given-names>N</given-names></name><name><surname>McManus</surname><given-names>J</given-names></name><name><surname>Leko</surname><given-names>G</given-names></name><name><surname>Saari</surname><given-names>P</given-names></name><name><surname>Hurcomb</surname><given-names>J</given-names></name><name><surname>Antohi</surname><given-names>A</given-names></name><name><surname>Suuronen</surname><given-names>JP</given-names></name><name><surname>Blanchard</surname><given-names>F</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Hampton</surname><given-names>M</given-names></name><name><surname>Eckermann</surname><given-names>M</given-names></name><name><surname>Westermeier</surname><given-names>F</given-names></name><name><surname>Frohn</surname><given-names>J</given-names></name><name><surname>Hoekstra</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>CH</given-names></name><name><surname>Huttula</surname><given-names>M</given-names></name><name><surname>Mokso</surname><given-names>R</given-names></name><name><surname>Juusola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022b</year><article-title>Binocular mirror-symmetric microsaccadic sampling enables <italic>Drosophila</italic> hyperacute 3D vision</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2109717119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2109717119</pub-id><pub-id pub-id-type="pmid">35298337</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuang</surname><given-names>X</given-names></name><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Rucci</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Temporal encoding of spatial information during active visual fixation</article-title><source>Current Biology</source><volume>22</volume><fpage>510</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.01.050</pub-id><pub-id pub-id-type="pmid">22342751</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Head movement of flies during visually guided flight</article-title><source>Nature</source><volume>243</volume><fpage>299</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1038/243299a0</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Motion and vision: why animals move their eyes</article-title><source>Journal of Comparative Physiology. A, Sensory, Neural, and Behavioral Physiology</source><volume>185</volume><fpage>341</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1007/s003590050393</pub-id><pub-id pub-id-type="pmid">10555268</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name><name><surname>Nilsson</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Animal Eyes</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199581139.001.0001</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langridge</surname><given-names>KV</given-names></name><name><surname>Wilke</surname><given-names>C</given-names></name><name><surname>Riabinina</surname><given-names>O</given-names></name><name><surname>Vorobyev</surname><given-names>M</given-names></name><name><surname>Hempel de Ibarra</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Approach direction prior to landing explains patterns of colour learning in bees</article-title><source>Frontiers in Physiology</source><volume>12</volume><elocation-id>697886</elocation-id><pub-id pub-id-type="doi">10.3389/fphys.2021.697886</pub-id><pub-id pub-id-type="pmid">34955870</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehrer</surname><given-names>M</given-names></name><name><surname>Collett</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Approaching and departing bees learn different cues to the distance of a landmark</article-title><source>Journal of Comparative Physiology A</source><volume>175</volume><fpage>171</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1007/BF00215113</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Egertová</surname><given-names>M</given-names></name><name><surname>Elphick</surname><given-names>MR</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Perry</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A possible structural correlate of learning performance on a colour discrimination task in the brain of the bumblebee</article-title><source>Proceedings. Biological Sciences</source><volume>284</volume><elocation-id>20171323</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2017.1323</pub-id><pub-id pub-id-type="pmid">28978727</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>R</given-names></name><name><surname>Dorkenwald</surname><given-names>S</given-names></name><name><surname>Matsliah</surname><given-names>A</given-names></name><name><surname>Sterling</surname><given-names>AR</given-names></name><name><surname>Schlegel</surname><given-names>P</given-names></name><name><surname>Yu</surname><given-names>S-C</given-names></name><name><surname>McKellar</surname><given-names>CE</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Eichler</surname><given-names>K</given-names></name><name><surname>Bates</surname><given-names>AS</given-names></name><name><surname>Eckstein</surname><given-names>N</given-names></name><name><surname>Funke</surname><given-names>J</given-names></name><name><surname>Jefferis</surname><given-names>GSXE</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Network statistics of the whole-brain connectome of <italic>Drosophila</italic></article-title><source>Nature</source><volume>634</volume><fpage>153</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07968-y</pub-id><pub-id pub-id-type="pmid">39358527</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Locatelli</surname><given-names>FF</given-names></name><name><surname>Fernandez</surname><given-names>PC</given-names></name><name><surname>Villareal</surname><given-names>F</given-names></name><name><surname>Muezzinoglu</surname><given-names>K</given-names></name><name><surname>Huerta</surname><given-names>R</given-names></name><name><surname>Galizia</surname><given-names>CG</given-names></name><name><surname>Smith</surname><given-names>BH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Nonassociative plasticity alters competitive interactions among mixture components in early olfactory processing</article-title><source>The European Journal of Neuroscience</source><volume>37</volume><fpage>63</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1111/ejn.12021</pub-id><pub-id pub-id-type="pmid">23167675</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name><name><surname>Hou</surname><given-names>X</given-names></name><name><surname>Edgecombe</surname><given-names>GD</given-names></name><name><surname>Strausfeld</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Complex brain and optic lobes in an early Cambrian arthropod</article-title><source>Nature</source><volume>490</volume><fpage>258</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1038/nature11495</pub-id><pub-id pub-id-type="pmid">23060195</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Shimazaki</surname><given-names>H</given-names></name><name><surname>Amari</surname><given-names>S</given-names></name><name><surname>Soltanian-Zadeh</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Representation of higher-order statistical structures in natural scenes via spatial phase distributions</article-title><source>Vision Research</source><volume>120</volume><fpage>61</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2015.06.009</pub-id><pub-id pub-id-type="pmid">26278166</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Shimazaki</surname><given-names>H</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Olfactory learning without the mushroom bodies: Spiking neural network models of the honeybee lateral antennal lobe tract reveal its capacities in odour memory tasks of varied complexities</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005551</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005551</pub-id><pub-id pub-id-type="pmid">28640825</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Galpayage Dona</surname><given-names>HS</given-names></name><name><surname>Gatto</surname><given-names>E</given-names></name><name><surname>Loukola</surname><given-names>OJ</given-names></name><name><surname>Buckley</surname><given-names>E</given-names></name><name><surname>Onoufriou</surname><given-names>PD</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Bumblebees use sequential scanning of countable items in visual patterns to solve numerosity tasks</article-title><source>Integrative and Comparative Biology</source><volume>60</volume><fpage>929</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1093/icb/icaa025</pub-id><pub-id pub-id-type="pmid">32369562</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Honeybees solve a multi-comparison ranking task by probability matching</article-title><source>Proceedings. Biological Sciences</source><volume>287</volume><elocation-id>20201525</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2020.1525</pub-id><pub-id pub-id-type="pmid">32873200</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Solvi</surname><given-names>C</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020c</year><article-title>Bumblebees learn a relational rule but switch to a win-stay/lose-switch heuristic after extensive training</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>14</volume><elocation-id>137</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2020.00137</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Honkanen</surname><given-names>M</given-names></name><name><surname>Loukola</surname><given-names>OJ</given-names></name><name><surname>Peng</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Cope</surname><given-names>A</given-names></name><name><surname>Vasilaki</surname><given-names>E</given-names></name><name><surname>Solvi</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Non-numerical strategies used by bees to solve numerical cognition tasks</article-title><source>Proceedings. Biological Sciences</source><volume>288</volume><elocation-id>20202711</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2020.2711</pub-id><pub-id pub-id-type="pmid">33593192</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Guiraud</surname><given-names>M</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Automated video tracking and flight analysis show how bumblebees solve a pattern discrimination task using active vision</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.03.09.434580</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Dearden</surname><given-names>N</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>How honey bees make fast and accurate decisions</article-title><source>eLife</source><volume>12</volume><elocation-id>e86176</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.86176</pub-id><pub-id pub-id-type="pmid">37365884</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Insect-Inspired-Neuromorphic-Model-for-Active-Vision</data-title><version designator="swh:1:rev:6f4adc155bd6c9149d12f99af8f3760c5574e918">swh:1:rev:6f4adc155bd6c9149d12f99af8f3760c5574e918</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:fb3577078fb05093c8945eede8bc178aa75f0b39;origin=https://github.com/hadiimaboudi/Insect-Inspired-Neuromorphic-Model-for-Active-Vision;visit=swh:1:snp:c1486a6e71f995ba9d745dde164398219adf240b;anchor=swh:1:rev:6f4adc155bd6c9149d12f99af8f3760c5574e918">https://archive.softwareheritage.org/swh:1:dir:fb3577078fb05093c8945eede8bc178aa75f0b39;origin=https://github.com/hadiimaboudi/Insect-Inspired-Neuromorphic-Model-for-Active-Vision;visit=swh:1:snp:c1486a6e71f995ba9d745dde164398219adf240b;anchor=swh:1:rev:6f4adc155bd6c9149d12f99af8f3760c5574e918</ext-link></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Richter</surname><given-names>J</given-names></name><name><surname>Guiraud</surname><given-names>M-G</given-names></name><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Active vision of bees in a simple pattern discrimination task</article-title><source>eLife</source><volume>14</volume><elocation-id>e106332</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.106332</pub-id><pub-id pub-id-type="pmid">40208660</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddess</surname><given-names>T</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>Horridge</surname><given-names>GA</given-names></name><name><surname>Levick</surname><given-names>WR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Adaptation of the motion-sensitive neuron H1 is generated locally and governed by contrast frequency</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><volume>225</volume><fpage>251</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1098/rspb.1985.0061</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddess</surname><given-names>T</given-names></name><name><surname>Yang</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Orientation-sensitive neurons in the brain of the honey bee (<italic>Apis mellifera</italic>)</article-title><source>Journal of Insect Physiology</source><volume>43</volume><fpage>329</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1016/s0022-1910(96)00111-4</pub-id><pub-id pub-id-type="pmid">12769894</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manoonpong</surname><given-names>P</given-names></name><name><surname>Patanè</surname><given-names>L</given-names></name><name><surname>Xiong</surname><given-names>X</given-names></name><name><surname>Brodoline</surname><given-names>I</given-names></name><name><surname>Dupeyroux</surname><given-names>J</given-names></name><name><surname>Viollet</surname><given-names>S</given-names></name><name><surname>Arena</surname><given-names>P</given-names></name><name><surname>Serres</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Insect-inspired robots: bridging biological and artificial systems</article-title><source>Sensors</source><volume>21</volume><elocation-id>7609</elocation-id><pub-id pub-id-type="doi">10.3390/s21227609</pub-id><pub-id pub-id-type="pmid">34833685</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Lübke</surname><given-names>J</given-names></name><name><surname>Frotscher</surname><given-names>M</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title><source>Science</source><volume>275</volume><fpage>213</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1126/science.275.5297.213</pub-id><pub-id pub-id-type="pmid">8985014</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsumoto</surname><given-names>Y</given-names></name><name><surname>Matsumoto</surname><given-names>CS</given-names></name><name><surname>Wakuda</surname><given-names>R</given-names></name><name><surname>Ichihara</surname><given-names>S</given-names></name><name><surname>Mizunami</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Roles of octopamine and dopamine in appetitive and aversive memory acquisition studied in olfactory conditioning of maxillary palpi extension response in crickets</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>230</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00230</pub-id><pub-id pub-id-type="pmid">26388749</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meeks</surname><given-names>JP</given-names></name><name><surname>Holy</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Pavlov’s moth: olfactory learning and spike timing-dependent plasticity</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1126</fpage><lpage>1127</lpage><pub-id pub-id-type="doi">10.1038/nn1008-1126</pub-id><pub-id pub-id-type="pmid">18818593</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menzel</surname><given-names>R</given-names></name><name><surname>Blakers</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Colour receptors in the bee eye - morphology and spectral sensitivity</article-title><source>Journal of Comparative Physiology? A</source><volume>108</volume><fpage>11</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1007/BF00625437</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menzel</surname><given-names>R</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dimensions of cognition in an insect, the honeybee</article-title><source>Behavioral and Cognitive Neuroscience Reviews</source><volume>5</volume><fpage>24</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1177/1534582306289522</pub-id><pub-id pub-id-type="pmid">16816091</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menzel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The honeybee as a model for understanding the basis of cognition</article-title><source>Nature Reviews Neuroscience</source><volume>13</volume><fpage>758</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1038/nrn3357</pub-id><pub-id pub-id-type="pmid">23080415</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menzel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>In search for the retrievable memory trace in an insect brain</article-title><source>Frontiers in Systems Neuroscience</source><volume>16</volume><elocation-id>876376</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2022.876376</pub-id><pub-id pub-id-type="pmid">35757095</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohammad</surname><given-names>F</given-names></name><name><surname>Mai</surname><given-names>Y</given-names></name><name><surname>Ho</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ott</surname><given-names>S</given-names></name><name><surname>Stewart</surname><given-names>JC</given-names></name><name><surname>Claridge-Chang</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Dopamine neurons that inform <italic>Drosophila</italic> olfactory memory have distinct, acute functions driving attraction and aversion</article-title><source>PLOS Biology</source><volume>22</volume><elocation-id>e3002843</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3002843</pub-id><pub-id pub-id-type="pmid">39556592</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Näher</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Pandinelli</surname><given-names>M</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Primate saccade rhythmicity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.09.27.559710</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nityananda</surname><given-names>V</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Can bees see at a glance?</article-title><source>The Journal of Experimental Biology</source><volume>217</volume><fpage>1933</fpage><lpage>1939</lpage><pub-id pub-id-type="doi">10.1242/jeb.101394</pub-id><pub-id pub-id-type="pmid">24625647</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odenthal</surname><given-names>L</given-names></name><name><surname>Doussot</surname><given-names>C</given-names></name><name><surname>Meyer</surname><given-names>S</given-names></name><name><surname>Bertrand</surname><given-names>OJN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Analysing head-thorax choreography during free-flights in bumblebees</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>14</volume><elocation-id>610029</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2020.610029</pub-id><pub-id pub-id-type="pmid">33510626</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oja</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>A simplified neuron model as a principal component analyzer</article-title><source>Journal of Mathematical Biology</source><volume>15</volume><fpage>267</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1007/BF00275687</pub-id><pub-id pub-id-type="pmid">7153672</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okada</surname><given-names>R</given-names></name><name><surname>Rybak</surname><given-names>J</given-names></name><name><surname>Manz</surname><given-names>G</given-names></name><name><surname>Menzel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning-related plasticity in PE1 and other mushroom body-extrinsic neurons in the honeybee brain</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>11736</fpage><lpage>11747</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2216-07.2007</pub-id><pub-id pub-id-type="pmid">17959815</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Principles of image representation in visual cortex</article-title><source>The Visual Neurosciences</source><volume>2</volume><fpage>1603</fpage><lpage>1615</lpage><pub-id pub-id-type="doi">10.7551/mitpress/7131.003.0123</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Gronenberg</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>Higher order visual input to the mushroom bodies in the bee, <italic>Bombus impatiens</italic></article-title><source>Arthropod Structure &amp; Development</source><volume>37</volume><fpage>443</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1016/j.asd.2008.03.002</pub-id><pub-id pub-id-type="pmid">18635397</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Phillips-Portillo</surname><given-names>J</given-names></name><name><surname>Dacks</surname><given-names>AM</given-names></name><name><surname>Fellous</surname><given-names>JM</given-names></name><name><surname>Gronenberg</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>The processing of color, motion, and stimulus timing are anatomically segregated in the bumblebee brain</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>6319</fpage><lpage>6332</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1196-08.2008</pub-id><pub-id pub-id-type="pmid">18562602</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Dacks</surname><given-names>AM</given-names></name><name><surname>Phillips-Portillo</surname><given-names>J</given-names></name><name><surname>Fellous</surname><given-names>JM</given-names></name><name><surname>Gronenberg</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Visual processing in the central bee brain</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>9987</fpage><lpage>9999</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1325-09.2009</pub-id><pub-id pub-id-type="pmid">19675233</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname><given-names>CJ</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural mechanisms of reward in insects</article-title><source>Annual Review of Entomology</source><volume>58</volume><fpage>543</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1146/annurev-ento-120811-153631</pub-id><pub-id pub-id-type="pmid">23020615</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Fernando</surname><given-names>C</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Insect bio-inspired neural network provides new evidence on how simple feature detectors can enable complex visual generalization and stimulus location invariance in the miniature brain of honeybees</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005333</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005333</pub-id><pub-id pub-id-type="pmid">28158189</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rucci</surname><given-names>M</given-names></name><name><surname>Iovin</surname><given-names>R</given-names></name><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Santini</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Miniature eye movements enhance fine spatial detail</article-title><source>Nature</source><volume>447</volume><fpage>851</fpage><lpage>854</lpage><pub-id pub-id-type="doi">10.1038/nature05866</pub-id><pub-id pub-id-type="pmid">17568745</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rucci</surname><given-names>M</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The unsteady eye: an information-processing stage, not a bug</article-title><source>Trends in Neurosciences</source><volume>38</volume><fpage>195</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2015.01.005</pub-id><pub-id pub-id-type="pmid">25698649</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruderman</surname><given-names>DL</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Statistics of natural images: Scaling in the woods</article-title><source>Physical Review Letters</source><volume>73</volume><fpage>814</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.73.814</pub-id><pub-id pub-id-type="pmid">10057546</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlegel</surname><given-names>P</given-names></name><name><surname>Yin</surname><given-names>Y</given-names></name><name><surname>Bates</surname><given-names>AS</given-names></name><name><surname>Dorkenwald</surname><given-names>S</given-names></name><name><surname>Eichler</surname><given-names>K</given-names></name><name><surname>Brooks</surname><given-names>P</given-names></name><name><surname>Han</surname><given-names>DS</given-names></name><name><surname>Gkantia</surname><given-names>M</given-names></name><name><surname>Dos Santos</surname><given-names>M</given-names></name><name><surname>Munnelly</surname><given-names>EJ</given-names></name><name><surname>Badalamente</surname><given-names>G</given-names></name><name><surname>Serratosa Capdevila</surname><given-names>L</given-names></name><name><surname>Sane</surname><given-names>VA</given-names></name><name><surname>Fragniere</surname><given-names>AMC</given-names></name><name><surname>Kiassat</surname><given-names>L</given-names></name><name><surname>Pleijzier</surname><given-names>MW</given-names></name><name><surname>Stürner</surname><given-names>T</given-names></name><name><surname>Tamimi</surname><given-names>IFM</given-names></name><name><surname>Dunne</surname><given-names>CR</given-names></name><name><surname>Salgarella</surname><given-names>I</given-names></name><name><surname>Javier</surname><given-names>A</given-names></name><name><surname>Fang</surname><given-names>S</given-names></name><name><surname>Perlman</surname><given-names>E</given-names></name><name><surname>Kazimiers</surname><given-names>T</given-names></name><name><surname>Jagannathan</surname><given-names>SR</given-names></name><name><surname>Matsliah</surname><given-names>A</given-names></name><name><surname>Sterling</surname><given-names>AR</given-names></name><name><surname>Yu</surname><given-names>SC</given-names></name><name><surname>McKellar</surname><given-names>CE</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name><name><surname>Hartenstein</surname><given-names>V</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>Jefferis</surname><given-names>G</given-names></name><collab>FlyWire Consortium</collab></person-group><year iso-8601-date="2024">2024</year><article-title>Whole-brain annotation and multi-connectome cell typing of <italic>Drosophila</italic></article-title><source>Nature</source><volume>634</volume><fpage>139</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07686-5</pub-id><pub-id pub-id-type="pmid">39358521</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmalz</surname><given-names>F</given-names></name><name><surname>El Jundi</surname><given-names>B</given-names></name><name><surname>Rössler</surname><given-names>W</given-names></name><name><surname>Strube-Bloss</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Categorizing visual information in subpopulations of honeybee mushroom body output neurons</article-title><source>Frontiers in Physiology</source><volume>13</volume><elocation-id>866807</elocation-id><pub-id pub-id-type="doi">10.3389/fphys.2022.866807</pub-id><pub-id pub-id-type="pmid">35574496</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwaerzel</surname><given-names>M</given-names></name><name><surname>Monastirioti</surname><given-names>M</given-names></name><name><surname>Scholz</surname><given-names>H</given-names></name><name><surname>Friggi-Grelin</surname><given-names>F</given-names></name><name><surname>Birman</surname><given-names>S</given-names></name><name><surname>Heisenberg</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Dopamine and octopamine differentiate between aversive and appetitive olfactory memories in <italic>Drosophila</italic></article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>10495</fpage><lpage>10502</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-33-10495.2003</pub-id><pub-id pub-id-type="pmid">14627633</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwegmann</surname><given-names>A</given-names></name><name><surname>Lindemann</surname><given-names>JP</given-names></name><name><surname>Egelhaaf</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal statistics of natural image sequences generated by movements with insect flight characteristics</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e110386</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0110386</pub-id><pub-id pub-id-type="pmid">25340761</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seelig</surname><given-names>JD</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Feature detection and orientation tuning in the <italic>Drosophila</italic> central complex</article-title><source>Nature</source><volume>503</volume><fpage>262</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nature12601</pub-id><pub-id pub-id-type="pmid">24107996</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selcho</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Octopamine in the mushroom body circuitry for learning and memory</article-title><source>Learning &amp; Memory</source><volume>31</volume><elocation-id>a053839</elocation-id><pub-id pub-id-type="doi">10.1101/lm.053839.123</pub-id><pub-id pub-id-type="pmid">38862169</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serbe</surname><given-names>E</given-names></name><name><surname>Meier</surname><given-names>M</given-names></name><name><surname>Leonhardt</surname><given-names>A</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comprehensive characterization of the major presynaptic elements to the <italic>Drosophila</italic> OFF motion detector</article-title><source>Neuron</source><volume>89</volume><fpage>829</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.01.006</pub-id><pub-id pub-id-type="pmid">26853306</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serres</surname><given-names>JR</given-names></name><name><surname>Viollet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Insect-inspired vision for autonomous vehicles</article-title><source>Current Opinion in Insect Science</source><volume>30</volume><fpage>46</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.cois.2018.09.005</pub-id><pub-id pub-id-type="pmid">30553484</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Severance</surname><given-names>E</given-names></name><name><surname>Washburn</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1907">1907</year><article-title>The loss of associative power in words after long fixation</article-title><source>The American Journal of Psychology</source><volume>18</volume><elocation-id>182</elocation-id><pub-id pub-id-type="doi">10.2307/1412411</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id><pub-id pub-id-type="pmid">11520932</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Döring</surname><given-names>TF</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Photoreceptor spectral sensitivity in island and mainland populations of the bumblebee, <italic>Bombus terrestris</italic></article-title><source>Journal of Comparative Physiology A</source><volume>193</volume><fpage>485</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1007/s00359-006-0206-6</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Competitive Hebbian learning through spike-timing-dependent synaptic plasticity</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>919</fpage><lpage>926</lpage><pub-id pub-id-type="doi">10.1038/78829</pub-id><pub-id pub-id-type="pmid">10966623</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Juusola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Refractory sampling links efficiency and costs of sensory encoding to stimulus statistics</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>7216</fpage><lpage>7237</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4463-13.2014</pub-id><pub-id pub-id-type="pmid">24849356</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spaethe</surname><given-names>J</given-names></name><name><surname>Tautz</surname><given-names>J</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Visual constraints in foraging bumblebees: flower size and color affect search time and flight behavior</article-title><source>PNAS</source><volume>98</volume><fpage>3898</fpage><lpage>3903</lpage><pub-id pub-id-type="doi">10.1073/pnas.071053098</pub-id><pub-id pub-id-type="pmid">11259668</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spaethe</surname><given-names>J</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Interindividual variation of eye optics and single object resolution in bumblebees</article-title><source>The Journal of Experimental Biology</source><volume>206</volume><fpage>3447</fpage><lpage>3453</lpage><pub-id pub-id-type="doi">10.1242/jeb.00570</pub-id><pub-id pub-id-type="pmid">12939375</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spaethe</surname><given-names>J</given-names></name><name><surname>Briscoe</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Early duplication and functional diversification of the opsin gene family in insects</article-title><source>Molecular Biology and Evolution</source><volume>21</volume><fpage>1583</fpage><lpage>1594</lpage><pub-id pub-id-type="doi">10.1093/molbev/msh162</pub-id><pub-id pub-id-type="pmid">15155799</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Lehrer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Spatial acuity of honeybee vision and its spectral properties</article-title><source>Journal of Comparative Physiology A</source><volume>162</volume><fpage>159</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1007/BF00606081</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Pattern recognition in the honeybee: Recent progress</article-title><source>Journal of Insect Physiology</source><volume>40</volume><fpage>183</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/0022-1910(94)90041-8</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Honey bees as a model for vision, perception, and cognition</article-title><source>Annual Review of Entomology</source><volume>55</volume><fpage>267</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1146/annurev.ento.010908.164537</pub-id><pub-id pub-id-type="pmid">19728835</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stach</surname><given-names>S</given-names></name><name><surname>Benard</surname><given-names>J</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Local-feature assembling in visual pattern recognition and generalization in honeybees</article-title><source>Nature</source><volume>429</volume><fpage>758</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1038/nature02594</pub-id><pub-id pub-id-type="pmid">15201910</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stach</surname><given-names>S</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The influence of training length on generalization of visual feature assemblies in honeybees</article-title><source>Behavioural Brain Research</source><volume>161</volume><fpage>8</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2005.02.008</pub-id><pub-id pub-id-type="pmid">15904705</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Streinzer</surname><given-names>M</given-names></name><name><surname>Brockmann</surname><given-names>A</given-names></name><name><surname>Nagaraja</surname><given-names>N</given-names></name><name><surname>Spaethe</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sex and caste-specific variation in compound eye morphology of five honeybee species</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e57702</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0057702</pub-id><pub-id pub-id-type="pmid">23460896</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strube-Bloss</surname><given-names>MF</given-names></name><name><surname>Nawrot</surname><given-names>MP</given-names></name><name><surname>Menzel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mushroom body output neurons encode odor-reward associations</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>3129</fpage><lpage>3140</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2583-10.2011</pub-id><pub-id pub-id-type="pmid">21414933</pub-id></element-citation></ref><ref id="bib140"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szyszka</surname><given-names>P</given-names></name><name><surname>Ditzen</surname><given-names>M</given-names></name><name><surname>Galkin</surname><given-names>A</given-names></name><name><surname>Galizia</surname><given-names>CG</given-names></name><name><surname>Menzel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Sparsening and temporal sharpening of olfactory representations in the honeybee mushroom bodies</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>3303</fpage><lpage>3313</lpage><pub-id pub-id-type="doi">10.1152/jn.00397.2005</pub-id><pub-id pub-id-type="pmid">16014792</pub-id></element-citation></ref><ref id="bib141"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>CH</given-names></name></person-group><year iso-8601-date="1911">1911</year><article-title>Experiments on pattern-vision of the honey bee</article-title><source>The Biological Bulletin</source><volume>21</volume><fpage>249</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.2307/1536017</pub-id></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Wait</surname><given-names>PB</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Pattern recognition in bees: orientation discrimination</article-title><source>Journal of Comparative Physiology A</source><volume>167</volume><fpage>649</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1007/BF00192658</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Theoretical predictions of spatiotemporal receptive fields of fly LMCs, and experimental validation</article-title><source>Journal of Comparative Physiology A</source><volume>171</volume><fpage>157</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1007/BF00188924</pub-id></element-citation></ref><ref id="bib144"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Processing of natural time series of intensities by the visual system of the blowfly</article-title><source>Vision Research</source><volume>37</volume><fpage>3407</fpage><lpage>3416</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00105-3</pub-id><pub-id pub-id-type="pmid">9425553</pub-id></element-citation></ref><ref id="bib145"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varella</surname><given-names>TT</given-names></name><name><surname>Takahashi</surname><given-names>DY</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Active sampling as an information seeking strategy in primate vocal interactions</article-title><source>Communications Biology</source><volume>7</volume><elocation-id>1098</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-024-06764-8</pub-id><pub-id pub-id-type="pmid">39242819</pub-id></element-citation></ref><ref id="bib146"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasas</surname><given-names>V</given-names></name><name><surname>Peng</surname><given-names>F</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Randomly weighted receptor inputs can explain the large diversity of colour-coding neurons in the bee visual system</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>8330</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-44375-0</pub-id><pub-id pub-id-type="pmid">31171814</pub-id></element-citation></ref><ref id="bib147"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vetter</surname><given-names>RS</given-names></name><name><surname>Visscher</surname><given-names>PK</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Influence of age on antennal response of male honey bees, <italic>Apis mellifera</italic>, to queen mandibular pheromone and alarm pheromone component</article-title><source>Journal of Chemical Ecology</source><volume>23</volume><fpage>1867</fpage><lpage>1880</lpage><pub-id pub-id-type="doi">10.1023/B:JOEC.0000006456.90528.94</pub-id></element-citation></ref><ref id="bib148"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viollet</surname><given-names>S</given-names></name><name><surname>Franceschini</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A hyperacute optical position sensor based on biomimetic retinal micro-scanning</article-title><source>Sensors and Actuators A</source><volume>160</volume><fpage>60</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.sna.2010.03.036</pub-id></element-citation></ref><ref id="bib149"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title><source>Science</source><volume>334</volume><fpage>1569</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1126/science.1211095</pub-id><pub-id pub-id-type="pmid">22075724</pub-id></element-citation></ref><ref id="bib150"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>von Frisch</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1914">1914</year><source>Der Farbensinn Und Formensinn Der Biene</source><publisher-name>Zoologische Jahrbücher (Physiologie)</publisher-name></element-citation></ref><ref id="bib151"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Washburn</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1908">1908</year><source>The Animal Mind: A Text-Book of Comparative Psychology</source><publisher-name>Macmillan</publisher-name></element-citation></ref><ref id="bib152"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Washburn</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1916">1916</year><source>Movement and Mental Imagery: Outlines of a Motor Theory of the Complexer Mental Processes</source><publisher-name>Houghton Mifflin</publisher-name></element-citation></ref><ref id="bib153"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Robots with insect brains</article-title><source>Science</source><volume>368</volume><fpage>244</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1126/science.aaz6869</pub-id><pub-id pub-id-type="pmid">32299938</pub-id></element-citation></ref><ref id="bib154"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>Pattern recognition in bees</article-title><source>Nature</source><volume>215</volume><fpage>1244</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.1038/2151244a0</pub-id><pub-id pub-id-type="pmid">6052722</pub-id></element-citation></ref><ref id="bib155"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Werner</surname><given-names>A</given-names></name><name><surname>Stürzl</surname><given-names>W</given-names></name><name><surname>Zanker</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Object recognition in flight: how do bees distinguish between 3D shapes?</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0147106</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0147106</pub-id><pub-id pub-id-type="pmid">26886006</pub-id></element-citation></ref><ref id="bib156"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>B</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Characterizing the sparseness of neural codes</article-title><source>Network</source><volume>12</volume><fpage>255</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1080/net.12.3.255.270</pub-id><pub-id pub-id-type="pmid">11563529</pub-id></element-citation></ref><ref id="bib157"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yarbus</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="1967">1967</year><source>Eye Movements and Vision</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4899-5379-7</pub-id></element-citation></ref><ref id="bib158"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>SW</given-names></name><name><surname>Horridge</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Pattern recognition in bees: size of regions in spatial layout</article-title><source>Philosophical Transactions of the Royal Society of London. Series B</source><volume>337</volume><fpage>65</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0084</pub-id></element-citation></ref><ref id="bib159"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>LI</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name><name><surname>Holt</surname><given-names>CE</given-names></name><name><surname>Harris</surname><given-names>WA</given-names></name><name><surname>Poo</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A critical window for cooperation and competition among developing retinotectal synapses</article-title><source>Nature</source><volume>395</volume><fpage>37</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1038/25665</pub-id></element-citation></ref><ref id="bib160"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>L</given-names></name><name><surname>de Polavieja</surname><given-names>GG</given-names></name><name><surname>Wolfram</surname><given-names>V</given-names></name><name><surname>Asyali</surname><given-names>MH</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name><name><surname>Juusola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Feedback network controls photoreceptor output at the layer of first visual synapses in <italic>Drosophila</italic></article-title><source>The Journal of General Physiology</source><volume>127</volume><fpage>495</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1085/jgp.200509470</pub-id><pub-id pub-id-type="pmid">16636201</pub-id></element-citation></ref><ref id="bib161"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmermann</surname><given-names>MJY</given-names></name><name><surname>Nevala</surname><given-names>NE</given-names></name><name><surname>Yoshimatsu</surname><given-names>T</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name><name><surname>Nilsson</surname><given-names>D-E</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Zebrafish differentially process color across visual space to match natural scenes</article-title><source>Current Biology</source><volume>28</volume><fpage>2018</fpage><lpage>2032</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.075</pub-id><pub-id pub-id-type="pmid">29937350</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89929.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Louis</surname><given-names>Matthieu</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California, Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2023.06.04.543620" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2023.06.04.543620"/></front-stub><body><p>Inspired by bee's visual behavior, this manuscript develops a model of visual scanning, processing, and pattern recognition learning. The work shows how pre-training with natural images creates spatiotemporal receptive fields in lobula neurons that enhance pattern discrimination through sparse encoding. The authors provide a solid analysis of neural responses, model performance across tasks, and the contributions of components like scanning strategies and lateral inhibition. While the model represents a functional circuit for active vision, its biological plausibility is somewhat limited by intentional simplifications. The systematic evaluation of necessary components and comparisons with bee behavioral data strengthen the findings. This important work offers insights into motion-driven visual processing in compact neural systems.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89929.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Louis</surname><given-names>Matthieu</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02t274463</institution-id><institution>University of California, Santa Barbara</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Fujiwara</surname><given-names>Terufumi</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sjwvz98</institution-id><institution>RIKEN</institution></institution-wrap><country>Japan</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>De Agrò</surname><given-names>Massimo</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05trd4x28</institution-id><institution>University of Trento</institution></institution-wrap><country>Italy</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2023.06.04.543620">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2023.06.04.543620v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A neuromorphic model of active vision reveals how spatio-temporal encoding in lobula neurons can aid pattern recognition in bees&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Claude Desplan as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential Revisions:</p><p>The reviewers agree that the model developed by the authors tackles an important problem in machine learning and visual neuroscience. The model is based on visual scanning, which represents a novel and exciting phenomenology in the bee. Unfortunately, the main conclusions of the work must be watered down until the authors demonstrate that alternative, equally plausible, models of the visual and mushroom body circuits are not sufficient to solve the tasks under consideration. We believe that the manuscript can be a valuable and important contribution to the field if the following weaknesses are thoroughly addressed.</p><p>1) The neural circuitry underlying the model does not adequately integrate the wealth of <italic>Drosophila</italic> connectome data that has been published during the past 10 years. While the model is definitely bio-inspired, layers of its architecture are built very differently from the connectivity of real insect brains. As a result, many features of the model's architecture appear to be arbitrary. Clarifications should be provided about circuit-function relationships of the bee MB versus the <italic>Drosophila</italic> MB, and their implications on the model.</p><p>2) Given the repeated claims that the authors present the &quot;minimal circuit&quot; required for the visual tasks explored, the work ought to rigorously and systematically assess the necessity and sufficiency of the different components included in the circuitry of the model. In particular, could a simpler learning rule be sufficient to explain discrimination? In what sense is the presented circuit &quot;minimal&quot;? Varying the number of lobula neurons of the model is a good first step, but the same should be done for other components of the model.</p><p>3) The presentation of model's results and its interpretation should explain the successes and failures of the model in reproducing the actual behavior. It should include a more in-depth comparison of the performances of the model and real bees.</p><p>4) The description of the methodology is incomplete, which prevents a proper interpretation of the model's results.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The introduction talks about natural scenes such that their specific features are critical to neural processing and pattern recognition. However, the manuscript does not thoroughly assess if the model is particularly suited for handling natural scenes. To demonstrate this, it appears to require using non-natural images for non-associative learning and comparing the performance with the model trained with natural scenes. Otherwise, I would recommend rephrasing the introduction.</p><p>Line 217: why does restricting the scan field improve the performance dramatically? This may be intuitively reasonable, but it would be nice to have an explicit explanation based on the model's structure.</p><p>Discussion:</p><p>The insect lobula is not necessarily only composed of wide-field neurons. It would be nice to have some discussion about how other types of neurons, such as small object-detecting neurons, could contribute to the same visual task.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Line 1 – title – is it really substantiated in this paper that the spatiotemporal lobula encoding &quot;aids&quot; pattern recognition? Relative to what? Can these tasks not be solved by models such as that in Ardin et al. (2016) that use low resolution pixel values as input to the KC and associate the corresponding sparse code with the MBON for selected images?</p><p>Line 6 – abstract – in what sense is the presented circuit 'minimal'? The paper explores reducing the number of lobula neurons, but not any other reduction in complexity.</p><p>Line 11 – the alignment to neurobiological observations does not seem all that compelling. It is already known that using non-associative adaptive processes that favour sparse coding, trained with natural images, produces output that resembles complex cell receptive fields. Does this study produce results that are notably more aligned with data from insect lobula recordings, for example?</p><p>Line 40 – the &quot;cognitive feats in visual learning&quot; explored in this paper do not seem all that &quot;remarkable&quot;.</p><p>Lines 52-72 This passage seems to interchangeably use three different senses of 'adaptive': adaptive in the sense of ongoing change in neurons due to the experience of the individual (lines 55-57); adaptive in the sense of being evolutionarily well adapted (lines 57-59); and adaptive in the sense of being versatile and robust (lines 59-61). It would be helpful to keep these differences clear, especially as the claim in this paper is that adaption in the first sense is needed to support adaption in the last sense.</p><p>Lines 84 and 92-93 It is not clear why it is stated that sampling &quot;builds up a representation/picture of the environment&quot;. Indeed the authors' own work here and previously clearly demonstrates how active sampling can be used to solve visual problems without &quot;building up&quot; a picture.</p><p>Line 99 – This is an explicit claim that the paper explores &quot;the necessary and minimally sufficient circuit&quot;. However, the paper does not demonstrate necessity or minimality of the circuit elements.</p><p>Line 104 – again a claim that the lobula encoding used here is &quot;necessary&quot;.</p><p>Line 110 – here and later it is claimed the lobula representation is &quot;efficient&quot; but efficiency is never explicitly defined or shown.</p><p>Lines 115-116 It seems extremely strange to cite no papers later than 2012 for &quot;neural mechanisms of associative learning in insect brains&quot;.</p><p>116-117 &quot;visual flight dynamics&quot; in this paper are hugely simplified to a five-step constant speed horizontal scan, so their influence on the model seems overstated here.</p><p>121-122 another list of citations going no later than 2013, in an area of very active research.</p><p>Line 143 is there &quot;recurrent neural connectivity&quot; between photoreceptors and the lamina (in the model or reality)?</p><p>147-148 If I have understood this correctly, the connectivity between medulla and lobula is fixed in advance to be exactly five inputs, arranged in space, and with delay times, to match the standard scanning process used in these experiments. So it assumes the movement of the bee is known? This seems a very arbitrary wiring, is there any evidence to support it? Possibly I have misunderstood but if the spatial extent and timing of these connections is actually created through the non-associative adaptation process, then this has not been well explained in the paper (including the methods).</p><p>Figure 1 caption – is it correct that there are random connections (not all-to-one connections) from KCs to the single MBON? Or is the meaning here that the connections have initial random weights? Please clarify.</p><p>Figure 2 (and 3) it would be nice to include (where possible) data from actual bee behaviour – how well do they perform relative to the model?</p><p>Figure 2 I assume the paired columns in C are similar to those in D, I.e. showing the result if the positive training is to one symbol or the other. If so, it would help to have the same pattern and legend in C.</p><p>210-212 I find it mystifying why this circuit should be unable to do the discrimination task when the whole pattern is scanned. Do the lobula neuron responses look the same for both stimuli in this case? Why? Isn't this a significant weakness for the model – that some types of (rather simple) patterns cannot be learned? Frankly, this is much more striking than the fact that the face stimuli can be learned. Please discuss.</p><p>Figure 3B the text says the test cases were &quot;a novel grating and a single bar&quot; but the picture appears to show a grating pair that were used in training.</p><p>Also, Figure 3, the caption says &quot;except for (A) all simulations were conducted at the default distance …etc.&quot; so what was used for A, and why not the default?</p><p>Line 241-242 it seems like an overinterpretation of these very mixed results to say &quot;the model was able to extract more than a single feature during its scan of the pattern&quot;.</p><p>Line 261 and following, there are several claims here that the lobula encoding is efficient. But how is efficiency defined and measured here? Similarly, line 282-284 says the representation is 'decorrelated and sparse' but the only evidence provided seems to be that in example 4B, only a few lobula neurons have high activity.</p><p>309-310 If variability between lobula neurons is reduced with fewer neurons, doesn't that argue against the claim that the adaptive process makes them 'decorrelated and sparse'?</p><p>See my public review for comments on the claims made in the Discussion that I believe to be insufficiently supported.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I believe that this very interesting manuscript would benefit greatly from a more in-depth consideration of the terminology and a clearer description of the model and the methods.</p><p>I will provide here some more specific points. Below that are the in-line comments.</p><p>General notes:</p><p>It can be confusing to show the whole pattern when the actual input to the network is only a part of it. A suggestion would be to show, in the graphs, only the actual input to the network.</p><p>Should the model architecture used for the results be specified earlier? First mention of number of neurons in lobula is at line 306 (maybe give a name/code to the model variants and refer to those in the method section)</p><p>The videos showing the evolution of the receptive fields over the training steps are appreciated, and they could benefit by including a title that describes what is being watched (similar to the caption for images). Possibly, also report the number of training examples over the total of training examples, to show how the receptive fields evolve over time (e.g. Video 5).</p><p>Methodology:</p><p>In general, the methodology is described somewhat sparsely. Some crucial steps and details are not reported fully, and the full mathematical model of the network is not immediately clear. This is a pity, as it affects the interpretation and may undermine the meaningfulness of the results.</p><p>The authors specify that the model considers only green photoreceptors. It is unclear whether this is obtained by processing only the green channel of an original RGB image or by other means.</p><p>Lines 488-489. The mathematical notation does not look coherent. Does $A_0$ refer to the $a$ in $f(x; a, b)$? Also, it is not clear whether $A_0$ and the other parameters of the sigmoid activation function ($m$ and $b$) have fixed values or are parametrized and learned. The reviewer assumes they are fixed. Finally, it can be slightly confusing what does $r_p$ represent in the equation. I assume it represents the activity of one green photoreceptor $p$, and that $P$ represents the total amount of photoreceptors (pixels in the image) considered as input to one lamina neuron (as such, P=9).</p><p>Line 486-489. The tiling of the receptive field of each Lamina unit is not specified. Given the reported numbers (of pixels and units), the reviewer assumes that the tiling is formed by 625 squares of 3x3 pixels, each adjacent but non-overlapping with the others.</p><p>It is not written how the output of each photoreceptor ($r_p$) is obtained from the input image, nor whether it is a continuous or discrete value.</p><p>The superscripts are never mentioned explicitly, and the reader is left to infer that they refer to the different components of the network architecture (e.g., $La$ = Lamina). Albeit not critical, this could become an issue when considering that other parts of the mathematical notation are also not detailed, leaving possibly too much room for interpretation.</p><p>Line 500-501. What is the parameter $\λ$ of the Poisson distribution used to generate the noise in the activation?</p><p>The topology of connection from Lamina units to Medulla units is unclear to this reviewer. Lines 505-507 specify that Medulla units have a small (receptive) field, each one being activated by a different region of the image patch. From Figure 1B, the image patch seems to be one full 75x75 frame. The manuscript, however, does not report what is the small field (selected from this patch) to which one Medulla unit is said to respond. The total number of Medulla units also seems not to be reported.</p><p>The topology of connections from Medulla units to Lobula units is unclear to this reviewer. Lines 503-505 state that a total of M Medulla units is connected to each Lobula unit. However, the Methods do not describe how these M units are selected from those in the Medulla. Are they adjacent to each other? If so, in what order is the temporal delay applied?</p><p>It is not clear whether, when observing half or a corner of a pattern, the amount of Medulla units changes to reflect a lower number of pixels in the image, or the original image is enlarged to keep the current network configuration, changing the scale of the observed features, or neither of the above. The lack of clarity about the topology of connections from Lamina to Medulla and from Medulla to Lobula makes it difficult to interpret what happens in this case.</p><p>It is not reported from which distribution is the random connectivity matrix S initialized, nor whether it is randomly reinitialized for each simulated bee.</p><p>It is not reported how the laterally inhibitory connections in the lobula, Q, mathematically affects the neurons activity.</p><p>It is not reported over which window of time is the mean firing rate computed, nor if it computed as a static condition of the leaky integrate-and-fire model assuming a fixed value of activity in the input layer (Medulla) during the current training step.</p><p>When the receptive fields are shown in figures, the weights seem to be clipped in the range [-1, +1]. However, no clipping is reported in the Methods.</p><p>A formal description of all the initialization, training, and testing steps is not reported, and it's left to be inferred from different parts of the manuscript.</p><p>Discussion</p><p>The Discussion section of this work is somewhat lacking when it comes to analyzing the variation of performance of the proposed network over the whole spectrum of tested conditions.</p><p>It is the belief of this reviewer that the underperformance of the network in those cases, and with some type of patterns (even in the best-performing scenario, such as with the gratings in Figure 3D), could be attributed to the receptive fields that are formed during the non-associative learning procedure. Specifically, the receptive fields shown seem to all be responsive to a specific orientation and velocity. However, they are all &quot;global&quot; (or &quot;large&quot; scale), in the sense that they all have only one big contiguous area of positive weights along one big contiguous area of negative weights.</p><p>The question that naturally arises is whether the non-associative learning employed here can produce more refined patterns in the receptive fields, or whether it can learn to be sensitive over different scales of features by combining (at the level of the lobula) different large-scale receptive fields in non-trivial ways. Also, why do patterns learned, when scanning whole images, perform best when not applied to a whole image.</p><p>The reviewer acknowledges the difficulty in showing the receptive fields when they include both a spatial and temporal component, and as such also that this sensitivity on a varying scale could already be present in the network, as a combination of larger scale receptive fields (albeit the results with different speeds and different distances from the image seems to suggest some specificity in the scale of visual feature that the network can identify). This could warrant a more in-depth study of the performance of the proposed network architecture when varying the training set and training conditions (e.g., speed).</p><p>In-Line comments</p><p>L134-136: but is the scanning order selected by the model, or is it fixed? At the moment it seems to be implied that the 5 frames are given. Is there reason to assume this is the optimal order? Is there any optimal order? P.S. I see that this is touched upon below. See comment to L210-219</p><p>L185-188: Videos 1 and 2 are missing. I assume they are present in MaBouDi et al. 2021, but if they are referenced here with this indication they should be included. Alternatively, you could add in the text a general reference to the previous paper.</p><p>L196-197: writing here &quot;during the initial experiment&quot; seems to refer to the first experiment you are doing in this paper. Instead, it seems you want to refer to your previous paper. This should be made more clear.</p><p>Figure 2B – In the caption, there seem to be typos on what is rewarding and what is punishing.</p><p>200-201 – This wording may be interpreted as the model having active control, which is however not the case.</p><p>L201-219: statistical analysis should be done and reported. In an attempt to comparing this model with living animals, I believe every step should be taken to follow the same procedure. How many simulated bees have been tested in the + vs X task? Are they 20 per shape or 20 per scanning pattern? Is the data collected after how many visits? Reporting percentage is I believe insufficient, and a binomial test against chance level should be performed. This is the case for all experiments in this paper.</p><p>Figure 2B: is this SD or CI?</p><p>L210-219: the authors here refer to an initial poor performance. In figure 2C, is this the last two bars of the graph? This should be made clear.</p><p>Overall, the experiments here described aimed at finding the best scanning procedure, but I am not sure if how this was evaluated is appropriate. First, how is the training on natural images organized? Are they all scanned in the bottom half, left to right? If that is the case, the highest efficiency of lower-half scanning may be linked to the highest similarity to training, not to the real efficiency of the technique, and it would as such suggest low generalizability of the model. If instead training is repeated for all experiments following that presentation pattern, and thus the scanning procedure has an effect of learning effectiveness, this may be a property of this network, but not necessarily of living bees (as L216 somewhat suggests). It could in fact be a self-fulfilling prophecy (behavioral experiments suggest the need for scanning, the network is designed with a recurrent layer to enable shape reconstruction, and the network is most effective with scanning).</p><p>If you want to suggest that scanning from the bottom left is indeed more effective, you need to also include conditions other than the confirmatory one. These could be scanning right to left, or scanning left to right in the top half, or scanning top to bottom, or even diagonally (which I suspect are going to produce identical results). As of now, the experimental conditions only allow us to conclude that scanning sections is more effective than seeing the whole image, which again is to me included as a property of the network. Also, I may be wrong about this, but bees visual field is not centered frontally on the animal, but points upward (https://www.researchgate.net/publication/326717773_Bumblebee_visual_allometry_results_in_locally_improved_resolution_and_globally_improved_sensitivity). Being this the case, a bee moving across the bottom of a stimulus wouldn't it actually be looking at it fully, with the visual field centered on the horizontal symmetry line?</p><p>A similar reasoning should be made for scanning speed. Velocity is tangled with stimulus size. 0.1m/s may work best with this size but will change drastically depending on how much of the stimulus occupies the virtual bee visual field.</p><p>I want to point out that none of these points are detrimental to the effectiveness of the model itself, which seems to present good performances. But if claims want to be made about the best scanning strategy, especially if confronted with real animals, these points should be tested and addressed. As of now, we can say that the current model best performs under certain conditions, but we can't generalize the effectiveness of such conditions to be the best for the task, nor the best for bees.</p><p>L221-252: I believe also here binomial statistics should be produced. I understand it seems to be redundant for performances nearing 100%, but this becomes more relevant for the 60% and 40% reported in Fig3E. On the same note, specific values should be reported, both for averages and SD.</p><p>246-251 – Repetition of a period.</p><p>It would be helpful if Figure 3 also reported the real bees data, as taken from the various papers. This would give a sense of how closely the model follows the bees behavior. Of course, bees are more complex and are subject to, among others, motivational effects which will make the choice percentage less clean, but I still think this would be appreciable.</p><p>311-312 – &quot;When the model is limited to only four neurons in the lobula, it lacks the capability to encode the entire spatio-temporal structure that is naturally present in the training patterns&quot;. This wording seems to suggest that with more neurons it can encode the entire spatio-temporal structure of the training patterns, which may be an overstatement.</p><p>L314-316: I agree that these neurons are sufficient for the discrimination task in hand, but I am unsure whether is appropriate to extend this to bees, as the paragraph title implies. Bees have to use the system to respond to much more complex patterns, like photorealistic ones. For example, is 16 neurons still enough for the face discrimination task?</p><p>355-356 – It is unclear how the study would suggest a crucial role of movement in the ability to efficiently analyze and encode the environment. In this work, movement of the input pattern is taken as a given condition under which the network is trained, and not as a tool that can be exploited to have an advantage in the encoding and analysis of the pattern itself.</p><p>Discussion: In general, I am not fully convinced that your model can say anything about the bees or the optimal performance in general, but should focus on the effectiveness of the model itself. This is because of what I have reported above about how the model performance is at least partially dependent on the model design, and not on how bees actually behave (which is hypothesized)</p><p>374-375 – In these lines, it is claimed that the model acts as a linear generative model, however, this is not shown in the results and these generative capabilities are not demonstrated.</p><p>487 – Calling $r_l^{La}$ as &quot;the output of one lamina neuron&quot; instead of &quot;one lamina neuron&quot; could improve clarity.</p><p>498 – I have not clear what &quot;however&quot; refers to, in this context</p><p>498 – Similarly. Rewording &quot;the input of the m −the medulla neuron is calculated&quot; to something like &quot;the input to the $m$-th medulla neuron, $I_m{Me}$, is calculated as&quot;</p><p>506-507 – Could reference to Figure 3B be a typo?</p><p>528-530 &quot;At each step of training, a set of five patches with size 75x75 pixels, selected by shifting 15 pixels over the image from the left or right or the reverse orientation (Figures1B, 2A), was considered as the input of the model.&quot; This wording could be a bit confusing, especially as, coincidentally, 15*5=75. It could be improved to make it clear that one input to the network is (a concatenation?) of 5 patches of 75x75 pixels each, obtained by shifting a window of 75x75 pixels by 15 pixels, 5 times (if this is actually the case).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The revised manuscript incorporates my comments well. The added analysis better clarified that local visual features are essential for learning using this scanning strategy. The description was also significantly revised, and the claim sounds reasonable now. I do not have further comments.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89929.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential Revisions:</p><p>The reviewers agree that the model developed by the authors tackles an important problem in machine learning and visual neuroscience. The model is based on visual scanning, which represents a novel and exciting phenomenology in the bee. Unfortunately, the main conclusions of the work must be watered down until the authors demonstrate that alternative, equally plausible, models of the visual and mushroom body circuits are not sufficient to solve the tasks under consideration. We believe that the manuscript can be a valuable and important contribution to the field if the following weaknesses are thoroughly addressed.</p></disp-quote><p>Thank you for compiling this detailed list of essential revisions. We appreciate it, as well as the recognition of the importance and novelty of our study. In response to the outlined revisions, we have substantially improved the manuscript by incorporating additional analyses, control models, deeper discussions, and methodological clarifications. Below, we summarise the key revisions and how they comprehensively address each of the essential concerns.</p><disp-quote content-type="editor-comment"><p>1) The neural circuitry underlying the model does not adequately integrate the wealth of <italic>Drosophila</italic> connectome data that has been published during the past 10 years. While the model is definitely bio-inspired, layers of its architecture are built very differently from the connectivity of real insect brains. As a result, many features of the model's architecture appear to be arbitrary. Clarifications should be provided about circuit-function relationships of the bee MB versus the Drosophila MB, and their implications on the model.</p></disp-quote><p>We acknowledge that our original manuscript did not fully incorporate recent <italic>Drosophila</italic> connectome data into the model’s design. However, this was an intentional simplification aimed at identifying core computational principles underlying visual learning, rather than precisely replicating the anatomical complexity of the insect brain. Below, we outline our reasoning and the broader implications of our approach.</p><list list-type="roman-upper" id="list2"><list-item><p>Simplification for Computational Insight</p></list-item></list><p>– A key objective of our study was to uncover the fundamental computational mechanisms underpinning active vision and visual pattern recognition in insects, with a particular focus on bees, as they are the only insect species that has been experimentally tested for active vision principles in visual pattern recognition, which is the central question of this study.</p><p>– Incorporating highly detailed connectome data would introduce a large number of additional parameters, making it difficult to isolate and interpret the contributions of individual components.</p><p>– By abstracting certain details, we can systematically assess how different computational principles—such as spatiotemporal encoding, sequential scanning, and non/associative learning—contribute to pattern recognition.</p><list list-type="roman-upper" id="list3"><list-item><label>II.</label><p>Limitations of Connectome Data for Functional Understanding</p></list-item></list><p>– While <italic>Drosophila</italic> connectome data provide an unprecedented level of structural detail, they do not directly reveal functional relationships or behavioural significance, although they have the potential to contribute to understanding these aspects.</p><p>– The functional role of specific circuit motifs in the mushroom body (MB) for visual pattern recognition remains largely untested; most of the available information for fruit flies relates to olfactory learning.</p><p>– Our model serves as a predictive tool, offering insights into how motion and spatiotemporal encoding contribute to visual learning—hypotheses that neurobiologists can test using connectomic and physiological approaches.</p><list list-type="roman-upper" id="list4"><list-item><label>III.</label><p>Importance of Motion in Visual Processing</p></list-item></list><p>– Most neuroscientific studies on insect vision have focused on either navigation or motion coding, but very few have directly addressed the neural correlates of pattern recognition.</p><p>– Our work highlights the importance of motion in visual recognition, suggesting that sequential visual input (as seen in active vision) fundamentally shapes neural encoding.</p><p>– This is particularly relevant because most visual studies in <italic>Drosophila</italic> have relied on static viewing condition, which do not account for the role of motion in shaping neural responses.</p><list list-type="roman-upper" id="list5"><list-item><label>IV.</label><p>Why We Focused on Bee Data</p></list-item></list><p>– Unlike <italic>Drosophila</italic>, bees have a longer history of research and a larger body of data on behavioural and neural aspects of visual learning and pattern recognition.</p><p>– While some level of abstraction is intended, our model does draw from established electrophysiological, computational, and behavioural studies in bees, aligning with known experimental findings in visual learning tasks.</p><p>– While connectome data from <italic>Drosophila</italic> provide valuable structural insights, there is no direct connectomic study in flies that tackles the neural correlates of pattern recognition.</p><p>In summary, we acknowledge the significance of connectome data in advancing our understanding of neural circuits; however, our approach prioritises computational abstraction to uncover the core principles of active vision and visual learning. Rather than fully replicating anatomical details, our model provides testable hypotheses that can inform future neurobiological investigations in bees, <italic>Drosophila</italic>, and other insect models. By emphasising motion-based visual processing, our study broadens the scope of insect vision research and highlights the importance of dynamic visual input—a factor often overlooked in traditional static-image studies. We believe this computational perspective can inspire new experimental approaches that integrate both static and dynamic visual cues in behavioural and neurophysiological studies. The simplifications inherent in our model have been thoroughly discussed in multiple sections of the Discussion, highlighted in blue, where we outline their implications and limitations while justifying our design choices.</p><disp-quote content-type="editor-comment"><p>2) Given the repeated claims that the authors present the &quot;minimal circuit&quot; required for the visual tasks explored, the work ought to rigorously and systematically assess the necessity and sufficiency of the different components included in the circuitry of the model. In particular, could a simpler learning rule be sufficient to explain discrimination? In what sense is the presented circuit &quot;minimal&quot;? Varying the number of lobula neurons of the model is a good first step, but the same should be done for other components of the model.</p></disp-quote><p>The reviewers rightly pointed out that our previous claim of a &quot;minimal circuit&quot; was not sufficiently justified. In response, we have significantly refined this aspect in the revised manuscript to ensure that our conclusions are well-supported and appropriately cautious.</p><p>Our primary aim was not to define an optimally minimised model, but rather to identify a functional computational circuit that enables visual coding and learning. Specifically, we sought to explore the role of key model components, such as non-associative learning, natural image statistics, the number of neurons involved in active vision, and scanning behaviour. This study represents an initial step in a novel approach, and we acknowledge that further refinements and investigations will be necessary. We have explicitly discussed the limitations of our model in Discussion section and made significant revisions regarding this claim, as summarised below:</p><p>Revisions and Improvements:</p><p>– We have removed overstatements about minimality and now clarify that our model represents one possible functional circuit rather than the absolute minimal configuration.</p><p>– To rigorously assess necessity and sufficiency, we conducted new control experiments:</p><p>– We revised the relevant sections to examine the influence of each model component on the final performance, rather than focusing solely on the number of lobula neurons.</p><p>– The revised manuscript includes a detailed justification for why specific mechanisms (e.g., spatiotemporal encoding, lateral inhibition, neuromodulation interactions) are necessary for task performance.</p><p>These revisions significantly strengthen the evidence for our model’s functional relevance while ensuring that our claims about minimality are well-contextualised and appropriately framed. Furthermore, we provide a clear rationale for our intentional simplifications in designing the network, allowing for a more interpretable assessment of computational principles in active vision.</p><disp-quote content-type="editor-comment"><p>3) The presentation of model's results and its interpretation should explain the successes and failures of the model in reproducing the actual behavior. It should include a more in-depth comparison of the performances of the model and real bees.</p></disp-quote><p>We apologise that the previous submission did not provide sufficient clarity on the conditions under which the model succeeds or fails. In response, the revised manuscript now presents a more comprehensive and in-depth analysis of performance variability, ensuring a clearer comparison between model outcomes and real bee behaviour. The following key revisions and improvements have been made:</p><p>– We have conducted a statistical comparison between the model's performance and compare them with real bee behaviour across multiple tasks, providing a more quantitative and systematic evaluation.</p><p>– The Results and Discussion sections have been substantially expanded to explicitly examine:</p><p>– Why certain complex patterns (e.g., spirals, faces) are successfully learned, whereas others (e.g., plus vs. multiplication symbols) can fail under specific scanning conditions. We have performed control experiments and deeper analysis to investigate the underlying mechanisms of success and failure, ensuring that these outcomes are well-explained. Additionally, further experiments highlight the conditions under which the current stage of the model succeeds or fails.</p><p>– How scanning strategies influence encoding and generalisation, drawing direct comparisons with bee vision behaviour to assess whether the observed results align with biological findings.</p><p>– The impact of scanning trajectory constraints, distinguishing between limitations imposed by model design and true behavioural optimisation strategies in bees.</p><p>– Figure 3 and Figure 5 have been revised to better illustrate model failures, providing clearer explanations of why specific conditions lead to reduced performance.</p><p>These revisions enhance the interpretability of the model results, ensuring that both successes and limitations are well-contextualised within their biological relevance.</p><disp-quote content-type="editor-comment"><p>4) The description of the methodology is incomplete, which prevents a proper interpretation of the model's results.</p></disp-quote><p>We apologise for the lack of methodological details in the previous submission, which may have limited the readers’ ability to fully interpret our results.</p><p>In response to the reviewers’ suggestions, we have substantially expanded the Methods section, providing a clearer and more comprehensive breakdown of:</p><p>– Network architecture, connectivity, and parameter choices, ensuring a detailed description of the model's structure.</p><p>– Training and testing procedures, incorporating new control experiments to further validate our findings.</p><p>– Training with natural images and its impact on receptive field formation, offering deeper insights into how the model learns visual patterns.</p><p>– Details on scanning sequences and how they were integrated into the model, improving transparency in the experimental design.</p><p>These revisions significantly enhance methodological clarity, ensuring that readers can better understand, evaluate, and reproduce our approach.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The introduction talks about natural scenes such that their specific features are critical to neural processing and pattern recognition. However, the manuscript does not thoroughly assess if the model is particularly suited for handling natural scenes. To demonstrate this, it appears to require using non-natural images for non-associative learning and comparing the performance with the model trained with natural scenes. Otherwise, I would recommend rephrasing the introduction.</p></disp-quote><p>We appreciate your insightful comment regarding the need to assess whether the model is particularly suited for handling natural scenes. To address this, we trained the model using shuffled natural images, in which the spatial structure was disrupted while preserving low-level statistical properties such as pixel values. This approach allowed us to isolate the effect of natural image structure on the model's performance. Our results demonstrate that the model trained on shuffled natural images failed to develop any structured receptive fields—its connectome remained random—resulting in a complete lack of discriminability and sparseness (Figure 3, Figure 2-suplementary figure 1B). This finding indicates that the specific spatial structure of natural scenes plays a crucial role in shaping the model’s learning and recognition capabilities. These results further support the premise introduced in the manuscript, reinforcing the argument that neural processing is particularly attuned to the statistical properties of natural scenes. We have now expanded this discussion in the manuscript, dedicating a full paragraph in the Discussion section and including an additional figure to illustrate these findings.</p><disp-quote content-type="editor-comment"><p>Line 217: why does restricting the scan field improve the performance dramatically? This may be intuitively reasonable, but it would be nice to have an explicit explanation based on the model's structure.</p></disp-quote><p>We apologise for the lack of clarity in our previous explanation. The restriction of the scan field significantly enhances performance by providing a more focused and high-contrast input, allowing for clearer and more relevant visual information to drive strong activation in lobula neurons. In contrast, when the entire image is processed, the response becomes less sparse, resulting in broader but weaker activation across the lobula. This diffused activation reduces stimulus discriminability in the subsequent associative layer, which relies on sparse and selective responses for effective differentiation.</p><p>We have now incorporated this explanation into the manuscript, expanding the discussion on the model’s structural dynamics and how scan restriction influences neural encoding and pattern recognition (Figure 5, 6F). Furthermore, our model predicts that bees may focus their scanning on smaller regions when confronted with more complex patterns, allowing them to extract simpler, more localised information. This aligns with experimental findings on bee visual behaviour (MaBouDi et al., 2021). For further clarification, please refer to our response to the Journal above.</p><disp-quote content-type="editor-comment"><p>Discussion:</p><p>The insect lobula is not necessarily only composed of wide-field neurons. It would be nice to have some discussion about how other types of neurons, such as small object-detecting neurons, could contribute to the same visual task.</p></disp-quote><p>Thank you for your suggestion. The involvement of other cell types in visual processing, beyond achromatic pattern recognition, has been discussed in the main text in lines 544- 558.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Line 1 – title – is it really substantiated in this paper that the spatiotemporal lobula encoding &quot;aids&quot; pattern recognition? Relative to what? Can these tasks not be solved by models such as that in Ardin et al. (2016) that use low resolution pixel values as input to the KC and associate the corresponding sparse code with the MBON for selected images?</p></disp-quote><p>Our study does not claim that spatiotemporal encoding is the only possible mechanism for pattern recognition; rather, we investigate how motion-driven visual processing and sequential scanning strategies influence neural coding and pattern recognition. Unlike models such as Ardin et al. (2016), which use low-resolution pixel values as direct input to Kenyon cells (KCs), our model incorporates a biologically inspired intermediate processing stage in the lobula, where visual patterns are dynamically encoded through temporal integration and lateral inhibition. This results in a sparse, decorrelated representation that aligns with known functional properties of insect visual neurons.</p><p>While simpler models, such as Ardin et al. (2016), may also solve specific pattern recognition tasks, they do not account for the role of active vision and require computationally expensive processing of all pixel values directly within the mushroom body. Our findings suggest that the visual system, particularly encoding within the lobula and lobula plate, generates a sparse, uncorrelated, and selective input to the learning centre, such as the MB. This compact, feature-selective representation reduces computational complexity, as only a small subset of highly selective neurons needs to be processed in the mushroom body and associated with reinforcement signals. Such an encoding strategy is not only more efficient but also more robust to input noise, potentially enhancing generalisability compared to direct pixel-wise input models.</p><p>Additionally, we discuss the broader implications of our findings on lines 560-578, particularly in visual navigation, by comparing the advantages of our approach to models like Ardin et al. (2016), which process raw pixel values without leveraging the benefits of motion-driven encoding and structured visual input filtering. These insights provide a more biologically plausible mechanism for efficient visual processing in insects while also offering potential applications for bio-inspired artificial vision systems.</p><disp-quote content-type="editor-comment"><p>Line 6 – abstract – in what sense is the presented circuit 'minimal'? The paper explores reducing the number of lobula neurons, but not any other reduction in complexity.</p></disp-quote><p>The abstract has been revised.</p><disp-quote content-type="editor-comment"><p>Line 11 – the alignment to neurobiological observations does not seem all that compelling. It is already known that using non-associative adaptive processes that favour sparse coding, trained with natural images, produces output that resembles complex cell receptive fields. Does this study produce results that are notably more aligned with data from insect lobula recordings, for example?</p></disp-quote><p>There is currently no population-level data on lobula neuron activity that would allow for a direct comparison of our model’s predictions on sparsity with empirical data. However, our model’s results were compared to functional recordings of lobula neurons involved in pattern recognition (Figure 2), demonstrating alignment with observed response properties. Given that early visual processing systems are widely recognised to support efficient coding by producing sparse and decorrelated activity, our findings are consistent with established neurobiological principles in primates. This has been further discussed in the Discussion section (lines 524-535).</p><disp-quote content-type="editor-comment"><p>Line 40 – the &quot;cognitive feats in visual learning&quot; explored in this paper do not seem all that &quot;remarkable&quot;.</p></disp-quote><p>Numerous studies have highlighted the remarkable visual learning abilities of bees, particularly given their small brain size compared to other animals. While face recognition is typically considered a complex cognitive task, our model—when aligned with previous bee experiments—demonstrates that bees can learn and discriminate human faces, a challenge that has been explored in larger-brained animals. This supports the view that even miniature neural circuits can achieve sophisticated visual discrimination.</p><disp-quote content-type="editor-comment"><p>Lines 52-72 This passage seems to interchangeably use three different senses of 'adaptive': adaptive in the sense of ongoing change in neurons due to the experience of the individual (lines 55-57); adaptive in the sense of being evolutionarily well adapted (lines 57-59); and adaptive in the sense of being versatile and robust (lines 59-61). It would be helpful to keep these differences clear, especially as the claim in this paper is that adaption in the first sense is needed to support adaption in the last sense.</p></disp-quote><p>Thank you for pointing this out. This paragraph has been revised to clarify the distinction between evolutionary adaptation and experience-dependent adaptation. Please see lines 64-89 for the updated version.</p><disp-quote content-type="editor-comment"><p>Lines 84 and 92-93 It is not clear why it is stated that sampling &quot;builds up a representation/picture of the environment&quot;. Indeed the authors' own work here and previously clearly demonstrates how active sampling can be used to solve visual problems without &quot;building up&quot; a picture.</p></disp-quote><p>This has been revised to &quot;build up a neural representation of their environment.&quot;</p><disp-quote content-type="editor-comment"><p>Line 99 – This is an explicit claim that the paper explores &quot;the necessary and minimally sufficient circuit&quot;. However, the paper does not demonstrate necessity or minimality of the circuit elements.</p></disp-quote><p>This has been revised to ‘Building on our previous work analysing bee flight paths during a simple visual task (MaBouDi et al., 2021b), we further investigated the main circuit elements that contribute to active vision in achromatic pattern recognition, focusing on a simplified yet biologically plausible model.’</p><disp-quote content-type="editor-comment"><p>Line 104 – again a claim that the lobula encoding used here is &quot;necessary&quot;.</p></disp-quote><p>The sentence has been revised to ‘We hypothesised that the bees’ scanning behaviours have co-adapted to sample complex visual features in a way that efficiently encodes them into spatiotemporal patterns of activity in the lobula neurons, facilitating distinct and specific representations that support learning in the bees’ compact brain.’</p><disp-quote content-type="editor-comment"><p>Line 110 – here and later it is claimed the lobula representation is &quot;efficient&quot; but efficiency is never explicitly defined or shown.</p></disp-quote><p>We apologise for the lack of clarity and have now revised the manuscript to provide a more precise and detailed explanation of this aspect. The efficiency of our lobula responses, in terms of sparsity, decorrelation, and selectivity, was further analysed and discussed in Figures 3 and 5 (new figures in the revised manuscript). Our results demonstrate that non-associative learning and its related components contribute to an optimised neural representation, reducing redundancy while preserving critical visual information. This aligns with principles of efficient coding, where neuronal activity is sparse and minimally correlated, allowing for a more compact yet informative representation of the environment.</p><disp-quote content-type="editor-comment"><p>Lines 115-116 It seems extremely strange to cite no papers later than 2012 for &quot;neural mechanisms of associative learning in insect brains&quot;.</p></disp-quote><p>We acknowledge the reviewer's concern. In this section, we specifically reference earlier papers that directly informed our model development. However, in the Discussion, we refer to more recent studies when discussing the plasticity rules in the mushroom bodies, providing a broader context for associative learning mechanisms in insect brains.</p><disp-quote content-type="editor-comment"><p>116-117 &quot;visual flight dynamics&quot; in this paper are hugely simplified to a five-step constant speed horizontal scan, so their influence on the model seems overstated here.</p></disp-quote><p>The limitations of our model have been addressed and highlighted in various paragraphs of the Discussion, particularly regarding the simplifications in visual flight dynamics. Additionally, we have outlined future directions, including the implementation of a more comprehensive visual flight dynamics model to further enhance our understanding of active vision.</p><disp-quote content-type="editor-comment"><p>121-122 another list of citations going no later than 2013, in an area of very active research.</p></disp-quote><p>The references have been updated to include recent research.</p><disp-quote content-type="editor-comment"><p>Line 143 is there &quot;recurrent neural connectivity&quot; between photoreceptors and the lamina (in the model or reality)?</p></disp-quote><p>This paragraph describes the model structure. However, it has been revised to improve clarity and reduce potential misunderstandings.</p><disp-quote content-type="editor-comment"><p>147-148 If I have understood this correctly, the connectivity between medulla and lobula is fixed in advance to be exactly five inputs, arranged in space, and with delay times, to match the standard scanning process used in these experiments. So it assumes the movement of the bee is known? This seems a very arbitrary wiring, is there any evidence to support it? Possibly I have misunderstood but if the spatial extent and timing of these connections is actually created through the non-associative adaptation process, then this has not been well explained in the paper (including the methods).</p></disp-quote><p>Apologies for the lack of clarity. Additional details have been added to the beginning of the result section and Methods section along with a paragraph discussing the potential mechanisms underlying this proposed network. The choice of five inputs is a model simplification to represent the scanning input to lobula neurons rather than an exact biological wiring. The model assumes prior knowledge of the bee’s speed and simplifies the structured visual input based on a part of observed scanning behaviour. The rationale behind this, as well as the optimisation and order of these inputs, is further discussed.</p><disp-quote content-type="editor-comment"><p>Figure 1 caption – is it correct that there are random connections (not all-to-one connections) from KCs to the single MBON? Or is the meaning here that the connections have initial random weights? Please clarify.</p></disp-quote><p>For each simulation, the model is set with a randomly weighted connectivity matrix, S. This has been revised in the figure caption and further explained in Methods.</p><disp-quote content-type="editor-comment"><p>Figure 2 (and 3) it would be nice to include (where possible) data from actual bee behaviour – how well do they perform relative to the model?</p></disp-quote><p>Following the advice of other reviewer, we applied statistical tests to the data and updated the figures to indicate significance with ‘*’ for statistically significant differences and ‘n.s.’ for non-significant results. The details of the statistical analysis have also been provided in the Methods section on lines 875- 893.</p><disp-quote content-type="editor-comment"><p>Figure 2 I assume the paired columns in C are similar to those in D, I.e. showing the result if the positive training is to one symbol or the other. If so, it would help to have the same pattern and legend in C.</p></disp-quote><p>Thank you for your suggestion. Panel C has now been updated and is presented as Panel D in the revised Figure 4.</p><disp-quote content-type="editor-comment"><p>210-212 I find it mystifying why this circuit should be unable to do the discrimination task when the whole pattern is scanned. Do the lobula neuron responses look the same for both stimuli in this case? Why? Isn't this a significant weakness for the model – that some types of (rather simple) patterns cannot be learned? Frankly, this is much more striking than the fact that the face stimuli can be learned. Please discuss.</p></disp-quote><p>Apologies for the lack of interpretation regarding the model’s performance. The lobula neuron activity has now been analysed in detail and is presented in Figures 3, 5, with further discussed in the result and Discussion section. In summary, when the entire pattern is scanned, a larger number of lobula neurons are activated; however, their responses are weaker and less selective, failing to provide a strong enough input for Kenyon cells to effectively associate the stimulus with reward or punishment, leading to poor discrimination performance.</p><p>This finding aligns with a key aspect of bee movement—bees may need to approach stimuli closely for effective scanning rather than making decisions from a distance (see MaBouDi et al., 2021). Close-range scanning enhances feature selectivity and learning, whereas scanning an entire pattern from a distance lead to diffuse, less discriminative activation, reducing the effectiveness of pattern differentiation. We have now explicitly addressed this interpretation and its implications in the Discussion section.</p><disp-quote content-type="editor-comment"><p>Figure 3B the text says the test cases were &quot;a novel grating and a single bar&quot; but the picture appears to show a grating pair that were used in training.</p></disp-quote><p>The model was tested on a novel pair of gratings with a single bar, different from those used in training.</p><disp-quote content-type="editor-comment"><p>Also, Figure 3, the caption says &quot;except for (A) all simulations were conducted at the default distance …etc.&quot; so what was used for A, and why not the default?</p></disp-quote><p>This has been corrected to: &quot;All simulations were conducted at a fixed distance of 2 cm and a scanning speed of 0.1 m/s, focusing on the lower half of the pattern.&quot;</p><disp-quote content-type="editor-comment"><p>Line 241-242 it seems like an overinterpretation of these very mixed results to say &quot;the model was able to extract more than a single feature during its scan of the pattern&quot;.</p></disp-quote><p>This has been revised.</p><disp-quote content-type="editor-comment"><p>Line 261 and following, there are several claims here that the lobula encoding is efficient. But how is efficiency defined and measured here? Similarly, line 282-284 says the representation is 'decorrelated and sparse' but the only evidence provided seems to be that in example 4B, only a few lobula neurons have high activity.</p></disp-quote><p>We have conducted further analysis to provide a clearer definition and quantification of efficiency, sparsity, and decorrelation in the model’s lobula encoding. Specifically, we now define efficiency in terms of sparse coding and reduced redundancy in neural responses, which enhances the selectivity and discriminability of visual features.</p><p>To support these claims, we have included additional quantitative measures, such as sparsity indices and correlation analyses, demonstrating how lobula neurons develop decorrelated and selective responses to visual stimuli. These results are now explicitly discussed in Figures 3 and 5, as well as in the revised text.</p><disp-quote content-type="editor-comment"><p>309-310 If variability between lobula neurons is reduced with fewer neurons, doesn't that argue against the claim that the adaptive process makes them 'decorrelated and sparse'?</p></disp-quote><p>We observed that reducing the number of lobula neurons may lead to lower variability in their responses, which could seem to contradict the claim that the adaptive process promotes decorrelation and sparsity. However, the relationship between neuron count, decorrelation, and sparsity is not strictly linear.</p><p>Our analysis shows that sparsity and decorrelation emerge as a function of competition among lobula neurons, which is driven by non-associative learning and lateral inhibition. When the number of lobula neurons is reduced, the diversity of receptive fields also decreases (Figures 2, 8), which limits the range of features that can be selectively encoded. This results in less decorrelation at the population level, as fewer neurons are available to distribute feature encoding efficiently.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I believe that this very interesting manuscript would benefit greatly from a more in-depth consideration of the terminology and a clearer description of the model and the methods.</p><p>I will provide here some more specific points. Below that are the in-line comments.</p></disp-quote><p>We appreciate your positive feedback and constructive suggestions. In response, we have carefully revised the manuscript to enhance clarity in both terminology and methodology. Specifically, we have refined key definitions to ensure consistency and precision, and we have expanded the model description to provide a more comprehensive and transparent account of its structure and underlying assumptions. We believe these revisions significantly strengthen the manuscript and greatly appreciate your valuable input. Please find our detailed responses below.</p><disp-quote content-type="editor-comment"><p>General notes:</p><p>It can be confusing to show the whole pattern when the actual input to the network is only a part of it. A suggestion would be to show, in the graphs, only the actual input to the network.</p></disp-quote><p>See answer below, please.</p><disp-quote content-type="editor-comment"><p>Should the model architecture used for the results be specified earlier? First mention of number of neurons in lobula is at line 306 (maybe give a name/code to the model variants and refer to those in the method section)</p></disp-quote><p>Thank you for your suggestion. The structure of the Results section has been revised, beginning with a summary of the network topology and an interpretation of the result of non-associative learning to provide greater clarity and coherence for the reader.</p><disp-quote content-type="editor-comment"><p>The videos showing the evolution of the receptive fields over the training steps are appreciated, and they could benefit by including a title that describes what is being watched (similar to the caption for images). Possibly, also report the number of training examples over the total of training examples, to show how the receptive fields evolve over time (e.g. Video 5).</p></disp-quote><p>Thank you for your suggestion. We have expanded the figure captions to provide clearer and more detailed information for the audience.</p><disp-quote content-type="editor-comment"><p>Methodology:</p><p>In general, the methodology is described somewhat sparsely. Some crucial steps and details are not reported fully, and the full mathematical model of the network is not immediately clear. This is a pity, as it affects the interpretation and may undermine the meaningfulness of the results.</p></disp-quote><p>We apologise for not providing sufficient details about the model in the previous version. The updated manuscript now includes more comprehensive descriptions and additional details.</p><disp-quote content-type="editor-comment"><p>The authors specify that the model considers only green photoreceptors. It is unclear whether this is obtained by processing only the green channel of an original RGB image or by other means.</p></disp-quote><p>Since we aim to compare the model’s output with bee behavioural data and the model is not designed for colour learning, we restricted it to only green photoreceptors. This decision is based on the hypothesis that bee pattern recognition primarily relies on the green component of visual input. Among the three types of photoreceptors in honeybees, green-sensitive photoreceptors are the most numerous, making them dominant in the bee's vision system.</p><p>Additionally, natural images exhibit a strong correlation between the red, green, and blue pixel values. Given this correlation, processing only the green channel should not significantly impact the final result. We have now clarified this point in the manuscript on lines 659-671.</p><disp-quote content-type="editor-comment"><p>Lines 488-489. The mathematical notation does not look coherent. Does $A_0$ refer to the $a$ in $f(x; a, b)$? Also, it is not clear whether $A_0$ and the other parameters of the sigmoid activation function ($m$ and $b$) have fixed values or are parametrized and learned. The reviewer assumes they are fixed. Finally, it can be slightly confusing what does $r_p$ represent in the equation. I assume it represents the activity of one green photoreceptor $p$, and that $P$ represents the total amount of photoreceptors (pixels in the image) considered as input to one lamina neuron (as such, P=9).</p></disp-quote><p>Sorry for the lack of clarity. We have now corrected this and provided a more detailed explanation in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Line 486-489. The tiling of the receptive field of each Lamina unit is not specified. Given the reported numbers (of pixels and units), the reviewer assumes that the tiling is formed by 625 squares of 3x3 pixels, each adjacent but non-overlapping with the others.</p></disp-quote><p>We have now clarified the tiling of the receptive fields. Each lamina neuron receives input from a non-overlapping 3×3 grid of neighbouring photoreceptors, ensuring full coverage of the visual field without redundancy. This structured arrangement has been explicitly stated in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>It is not written how the output of each photoreceptor ($r_p$) is obtained from the input image, nor whether it is a continuous or discrete value.</p></disp-quote><p>Each photoreceptor’s output is computed directly from the pixel intensity of the corresponding location in the input image, then normalized between 0 and 1, ensuring a smooth response that reflects the natural variation in luminance. This clarification has been added to the revised text.</p><disp-quote content-type="editor-comment"><p>The superscripts are never mentioned explicitly, and the reader is left to infer that they refer to the different components of the network architecture (e.g., $La$ = Lamina). Albeit not critical, this could become an issue when considering that other parts of the mathematical notation are also not detailed, leaving possibly too much room for interpretation.</p></disp-quote><p>Thank you for your suggestion. We have now explicitly defined all superscripts in the manuscript, ensuring clarity in their reference to different network components.</p><disp-quote content-type="editor-comment"><p>Line 500-501. What is the parameter $\λ$ of the Poisson distribution used to generate the noise in the activation?</p></disp-quote><p>Additional details have been added to the revised manuscript for further clarification.</p><disp-quote content-type="editor-comment"><p>The topology of connection from Lamina units to Medulla units is unclear to this reviewer. Lines 505-507 specify that Medulla units have a small (receptive) field, each one being activated by a different region of the image patch. From Figure 1B, the image patch seems to be one full 75x75 frame. The manuscript, however, does not report what is the small field (selected from this patch) to which one Medulla unit is said to respond. The total number of Medulla units also seems not to be reported.</p><p>The topology of connections from Medulla units to Lobula units is unclear to this reviewer. Lines 503-505 state that a total of M Medulla units is connected to each Lobula unit. However, the Methods do not describe how these M units are selected from those in the Medulla. Are they adjacent to each other? If so, in what order is the temporal delay applied?</p><p>It is not clear whether, when observing half or a corner of a pattern, the amount of Medulla units changes to reflect a lower number of pixels in the image, or the original image is enlarged to keep the current network configuration, changing the scale of the observed features, or neither of the above. The lack of clarity about the topology of connections from Lamina to Medulla and from Medulla to Lobula makes it difficult to interpret what happens in this case.</p></disp-quote><p>We apologise for the lack of clarity regarding the network topology. The relevant sections in both the Results and Methods have been substantially revised, providing greater detail and clarity. We believe the updated version offers a more comprehensive explanation for the reader.</p><disp-quote content-type="editor-comment"><p>It is not reported from which distribution is the random connectivity matrix S initialized, nor whether it is randomly reinitialized for each simulated bee.</p></disp-quote><p>Apologies for the ambiguity. Additional details have been added to the revised for further clarification.</p><disp-quote content-type="editor-comment"><p>It is not reported how the laterally inhibitory connections in the lobula, Q, mathematically affects the neurons activity.</p></disp-quote><p>Thank you for your suggestion. This now has been discussed in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>It is not reported over which window of time is the mean firing rate computed, nor if it computed as a static condition of the leaky integrate-and-fire model assuming a fixed value of activity in the input layer (Medulla) during the current training step.</p></disp-quote><p>This is now corrected, as the firing rates are already reported in spikes per second.</p><disp-quote content-type="editor-comment"><p>When the receptive fields are shown in figures, the weights seem to be clipped in the range [-1, +1]. However, no clipping is reported in the Methods.</p></disp-quote><p>In the training process, we restricted the synaptic weights to this specific range to facilitate faster convergence. Additionally, this constraint aligns with biological evidence suggesting limitations in synaptic transmission strength, preventing excessive potentiation or depression. We have now clarified this in the Methods section to ensure consistency with the figures presented.</p><disp-quote content-type="editor-comment"><p>A formal description of all the initialization, training, and testing steps is not reported, and it's left to be inferred from different parts of the manuscript.</p></disp-quote><p>Further details have now been included in the updated version.</p><disp-quote content-type="editor-comment"><p>Discussion</p><p>The Discussion section of this work is somewhat lacking when it comes to analyzing the variation of performance of the proposed network over the whole spectrum of tested conditions.</p></disp-quote><p>Further discussion has been added to the Discussion section to interpret and analyse the model’s performance and compare it with other studies. Additionally, please refer to our response to other comments for further details.</p><disp-quote content-type="editor-comment"><p>It is the belief of this reviewer that the underperformance of the network in those cases, and with some type of patterns (even in the best-performing scenario, such as with the gratings in Figure 3D), could be attributed to the receptive fields that are formed during the non-associative learning procedure. Specifically, the receptive fields shown seem to all be responsive to a specific orientation and velocity. However, they are all &quot;global&quot; (or &quot;large&quot; scale), in the sense that they all have only one big contiguous area of positive weights along one big contiguous area of negative weights.</p></disp-quote><p>We would like to highlight that while our model’s non-associative learning mechanism enables the self-organisation of spatiotemporal receptive fields, the extent to which it can refine patterns or develop multi-scale sensitivity remains an open question. Future work could explore whether large-scale receptive fields in the lobula can be combined or diversified to encode more complex, hierarchical features, thereby improving pattern discrimination.</p><p>Regarding the underperformance of the network in certain cases, such as gratings in Figure 3D (Figure 6D in the revised manuscript), we acknowledge that this may be attributed to the receptive fields formed during the non-associative learning procedure. As observed, the receptive fields tend to be global, with contiguous positive and negative weight areas, typically tuned to specific orientations and velocities. This structure may limit the network’s ability to capture finer-scale spatial features, potentially explaining reduced performance in tasks requiring higher spatial resolution. However, our results suggest that bees may approach stimuli for closer inspection and utilise sequential learning strategies when faced with more complex visual tasks, as observed in our previous studies. While this is a prediction of our model, it requires empirical validation in future studies. This aspect is discussed in interpreting the model’s failure to solve the task introduced in Figure 6F.</p><p>Additionally, our findings indicate that scanning a restricted image region enhances pattern discriminability, whereas whole-image scanning leads to reduced performance due to weaker and less selective activations in the lobula (Figure 5). This aligns with behavioural evidence suggesting that localised scanning strategies may improve visual encoding in bees, enabling them to extract more informative features from complex stimuli.</p><p>We have now expanded the discussion to address these points, emphasising the potential need for receptive field diversity, multi-scale feature encoding, and adaptive scanning strategies as promising future directions for refining the model.</p><disp-quote content-type="editor-comment"><p>The question that naturally arises is whether the non-associative learning employed here can produce more refined patterns in the receptive fields, or whether it can learn to be sensitive over different scales of features by combining (at the level of the lobula) different large-scale receptive fields in non-trivial ways. Also, why do patterns learned, when scanning whole images, perform best when not applied to a whole image.</p></disp-quote><p>Our results show that the model's non-associative learning mechanism enables lobula neurons to develop receptive fields that capture the main statistical features present in natural images, aligning with generative models of efficient coding. Through exposure to diverse natural scenes, the model learns to encode dominant spatial structures such as edges, orientations, and motion patterns, optimising its responses to the most frequently occurring visual features.</p><p>While the model successfully extracts key statistical features, the extent to which it can develop more refined receptive fields or achieve multi-scale sensitivity depends on the interactions between non-associative learning and scanning behaviour. In its current form, each lobula neuron integrates visual input over time, forming structured spatiotemporal receptive fields. However, the model has not yet been explicitly trained with different scanning behaviours that might enable it to capture multiple spatial scales simultaneously.</p><p>Our findings suggest that non-associative learning effectively captures the dominant statistical features of natural images, but further refinements are needed to enhance multi-scale feature sensitivity. Additionally, the improved performance in localised scanning conditions indicates that targeted feature selection enhances neural encoding, which may reflect an adaptive strategy in biological vision.</p><disp-quote content-type="editor-comment"><p>The reviewer acknowledges the difficulty in showing the receptive fields when they include both a spatial and temporal component, and as such also that this sensitivity on a varying scale could already be present in the network, as a combination of larger scale receptive fields (albeit the results with different speeds and different distances from the image seems to suggest some specificity in the scale of visual feature that the network can identify). This could warrant a more in-depth study of the performance of the proposed network architecture when varying the training set and training conditions (e.g., speed).</p></disp-quote><p>To provide a deeper interpretation of the spatiotemporal receptive field responses, we conducted additional analyses (Figure 3, 5), which have now been incorporated into the main text and discussion in lines relevant to Figures 3 ,5.</p><disp-quote content-type="editor-comment"><p>5 frames or or..</p></disp-quote><p>In this study, we used five fixed frames without explicitly optimizing their order, simplifying the model while aligning with the scanning speed observed in bumblebees. However, this number was tested in control models with different scanning speeds (faster and slower) to evaluate its impact. While we did not determine an optimal sequence, we hypothesize that the number and order of scanning windows feeding input to lobula neurons may vary across individuals and be influenced by their motor dynamics. The interaction between visual sampling and movement is likely an adaptive process, and future studies could investigate whether specific scanning sequences enhance pattern recognition efficiency. Further discussion has been added to the Discussion section to clarify our simplifications and outline future research directions. Additionally, please refer to our responses below for further details.</p><disp-quote content-type="editor-comment"><p>In-Line comments</p><p>L134-136: but is the scanning order selected by the model, or is it fixed? At the moment it seems to be implied that the 5 frames are given. Is there reason to assume this is the optimal order? Is there any optimal order? P.S. I see that this is touched upon below. See comment to L210-219</p></disp-quote><p>In this study, for simplicity, we used five fixed frames without explicitly optimising their order. This decision was based on the scanning speed observed in bumblebees during visual exploration. However, while the number of frames remained fixed throughout the experiment, the pixel shift between selected patches was varied in control models with different scanning speeds (faster and slower, Figure 4C) to assess its impact. Although we did not determine an optimal sequence, we hypothesise that the number and order of scanning windows feeding input to lobula neurons may vary among individuals and could be influenced by their motor dynamics. The interaction between visual sampling and movement is likely an adaptive process, and future studies could explore whether specific scanning sequences enhance pattern recognition efficiency (see Discussion).</p><disp-quote content-type="editor-comment"><p>L185-188: Videos 1 and 2 are missing. I assume they are present in MaBouDi et al. 2021, but if they are referenced here with this indication they should be included. Alternatively, you could add in the text a general reference to the previous paper.</p></disp-quote><p>Apologies for the omission. These lines were removed, but the main text still references the videos from MaBouDi et al. (2021).</p><disp-quote content-type="editor-comment"><p>L196-197: writing here &quot;during the initial experiment&quot; seems to refer to the first experiment you are doing in this paper. Instead, it seems you want to refer to your previous paper. This should be made more clear.</p></disp-quote><p>This has now been clarified.</p><disp-quote content-type="editor-comment"><p>Figure 2B – In the caption, there seem to be typos on what is rewarding and what is punishing.</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>200-201 – This wording may be interpreted as the model having active control, which is however not the case.</p></disp-quote><p>Corrected to sequential scanning.</p><disp-quote content-type="editor-comment"><p>L201-219: statistical analysis should be done and reported. In an attempt to comparing this model with living animals, I believe every step should be taken to follow the same procedure. How many simulated bees have been tested in the + vs X task? Are they 20 per shape or 20 per scanning pattern? Is the data collected after how many visits? Reporting percentage is I believe insufficient, and a binomial test against chance level should be performed. This is the case for all experiments in this paper.</p></disp-quote><p>Thank you for your suggestion. We have conducted various statistical comparisons to evaluate the model’s performance across different experiments and conditions. However, to maintain clarity and focus on the model’s key findings, we have opted to indicate statistical significance in the main text while reporting it in the figures using ‘*’ for p-values less than 0.05 and ‘n.s.’ for non-significant results. Additionally, we have included a dedicated section in the Methods detailing the simulation process (see new section Simulation and Statistical Analysis, in lines 875).</p><p>We acknowledge the challenge of directly comparing model outputs with bee behaviour, given the inherent differences between artificial simulations and biological systems. While living organisms exhibit natural variability due to genetic, environmental, and contextual factors, our model operates under controlled conditions with predefined parameters, making direct variability analysis difficult. However, despite these constraints, our model effectively captures key aspects of bee pattern recognition behaviour. Notably, the relative significance of responses—how strongly the model favours one pattern over another—closely aligns with behavioural data, regardless of absolute response variance.</p><p>Regarding your specific queries, in the + vs X task, each experimental condition was tested on 20 independent simulations per shape, with randomly initialised neural connectivity to account for individual variability (analogous to testing 20 bees). The data collection procedure follows behavioural study protocols, where each simulated bee undergoes multiple training exposures before performance is assessed. Additionally, each simulation was evaluated with test patterns multiple times, and averages along with the standard error of the mean (SEM) are now reported in the figures to provide a statistically robust representation of the results.</p><disp-quote content-type="editor-comment"><p>Figure 2B: is this SD or CI?</p></disp-quote><p>This has been clarified in the revised version as standard error of the mean (SEM).</p><disp-quote content-type="editor-comment"><p>L210-219: the authors here refer to an initial poor performance. In figure 2C, is this the last two bars of the graph? This should be made clear.</p></disp-quote><p>Apologies for the lack of clarity. This refers to the condition where the entire pattern is scanned. We have now clarified this in the main text.</p><disp-quote content-type="editor-comment"><p>Overall, the experiments here described aimed at finding the best scanning procedure, but I am not sure if how this was evaluated is appropriate. First, how is the training on natural images organized? Are they all scanned in the bottom half, left to right? If that is the case, the highest efficiency of lower-half scanning may be linked to the highest similarity to training, not to the real efficiency of the technique, and it would as such suggest low generalizability of the model. If instead training is repeated for all experiments following that presentation pattern, and thus the scanning procedure has an effect of learning effectiveness, this may be a property of this network, but not necessarily of living bees (as L216 somewhat suggests). It could in fact be a self-fulfilling prophecy (behavioral experiments suggest the need for scanning, the network is designed with a recurrent layer to enable shape reconstruction, and the network is most effective with scanning).</p></disp-quote><p>Regarding training on natural images, the model was not exclusively trained using lower-half scans. Instead, it was trained on full natural images captured at varying distances from natural objects such as flowers, with scanning applied as part of each image's presentation. The natural images were processed sequentially, with patches sampled from different locations over time, simulating the sequential nature of active vision. Importantly, scanning direction and region were not predetermined during training, ensuring that the model was exposed to a broad range of visual structures without inherent bias toward specific scanning trajectories. Given the variability in natural images, the model was effectively trained on a diverse dataset, allowing it to develop a generalised representation of visual patterns.</p><p>To further investigate how scanning influences neural representations, we analysed lobula neuron responses to whole-image inputs versus lower-half inputs. Our results show that restricting the input to the lower half led to greater neural discriminability and sparser responses, effectively providing a more selective and efficient signal to higher-level processing in the learning centre. This resulted in improved performance when simulated bees selectively scanned specific regions of the patterns. These findings suggest that localised scanning may enhance information encoding, optimising visual discrimination by reducing redundant inputs.</p><p>As discussed in the Discussion section, further studies are needed to determine the optimal scanning strategy that may provide even more efficient coding. Our approach serves as a foundation for future investigations into how dynamic scanning behaviours influence neural encoding and learning in both biological and artificial systems.</p><p>It is important to emphasise that our intent was not to assume that scanning is necessary but rather to test whether motion-driven encoding enhances visual discrimination. The model’s architecture does not inherently favour scanning—it is fully capable of processing static images. However, our findings indicate that scanning enhances feature encoding by leveraging spatiotemporal information, allowing for more efficient pattern recognition. This aligns with empirical studies in bees, which actively move to extract visual information rather than passively viewing static stimuli.</p><p>In summary, while our results indicate that sequential scanning improves discrimination, we do not claim that the specific trajectory tested represents the optimal strategy for bees. Instead, we propose that motion-driven encoding plays a fundamental role in shaping neural representations, and future studies should explore how different scanning strategies influence visual processing across both biological and artificial systems.</p><disp-quote content-type="editor-comment"><p>If you want to suggest that scanning from the bottom left is indeed more effective, you need to also include conditions other than the confirmatory one. These could be scanning right to left, or scanning left to right in the top half, or scanning top to bottom, or even diagonally (which I suspect are going to produce identical results). As of now, the experimental conditions only allow us to conclude that scanning sections is more effective than seeing the whole image, which again is to me included as a property of the network. Also, I may be wrong about this, but bees visual field is not centered frontally on the animal, but points upward (https://www.researchgate.net/publication/326717773_Bumblebee_visual_allometry_results_in_locally_improved_resolution_and_globally_improved_sensitivity). Being this the case, a bee moving across the bottom of a stimulus wouldn't it actually be looking at it fully, with the visual field centered on the horizontal symmetry line?</p></disp-quote><p>Thank you for raising these points. We acknowledge that our current experiments primarily demonstrate that scanning a specific section of the pattern enhances discrimination compared to viewing the entire image, rather than definitively establishing that scanning from the bottom left is the most effective strategy. To further investigate this, we have expanded our discussion (lines 505–518; 544–558) to emphasize the need for exploring more dynamic scanning trajectories to develop a more realistic model of active vision.</p><p>As highlighted in the Discussion section, a bee’s scanning strategy is likely influenced by multiple factors, including task complexity, stimulus properties, and environmental context. However, as the first computational study to examine the role of motion in shaping neural representations of the visual environment, our primary goal is to demonstrate how motion facilitates efficient spatiotemporal encoding. Our findings provide a computationally testable prediction that active scanning plays a fundamental role in structuring neural representations, laying the groundwork for future empirical and theoretical studies.</p><disp-quote content-type="editor-comment"><p>A similar reasoning should be made for scanning speed. Velocity is tangled with stimulus size. 0.1m/s may work best with this size but will change drastically depending on how much of the stimulus occupies the virtual bee visual field.</p></disp-quote><p>Please refer to our response to the previous comment.</p><disp-quote content-type="editor-comment"><p>I want to point out that none of these points are detrimental to the effectiveness of the model itself, which seems to present good performances. But if claims want to be made about the best scanning strategy, especially if confronted with real animals, these points should be tested and addressed. As of now, we can say that the current model best performs under certain conditions, but we can't generalize the effectiveness of such conditions to be the best for the task, nor the best for bees.</p></disp-quote><p>Thank you for your thoughtful feedback. We acknowledge the limitations regarding generalising the scanning strategy as the optimal one for bees. Our primary aim was to investigate how active scanning influences neural representations and pattern recognition rather than to establish a universally optimal scanning strategy.</p><p>We agree that determining the best scanning strategy requires a more comprehensive analysis, including testing alternative scanning trajectories and validating these against empirical bee behaviour. While our results demonstrate that the model performs best under specific conditions, we do not claim that these conditions necessarily represent the most effective strategy for bees in real-world settings. Instead, they provide a computationally grounded prediction that scanning behaviours influence neural encoding and recognition performance, which can be tested in future behavioural experiments.</p><p>As discussed in the revised manuscript (see Discussion, lines 544-558), scanning strategies in bees are likely influenced by multiple factors, including task complexity, environmental context, and individual variability. Given that bees adapt their behaviour to optimise foraging efficiency and learning, their scanning patterns may not be rigidly fixed but rather dynamically adjusted based on the specific demands of the task. Therefore, further empirical work is needed to explore how bees modulate their scanning strategies in different visual environments and whether certain trajectories lead to more efficient encoding.</p><disp-quote content-type="editor-comment"><p>L221-252: I believe also here binomial statistics should be produced. I understand it seems to be redundant for performances nearing 100%, but this becomes more relevant for the 60% and 40% reported in Fig3E. On the same note, specific values should be reported, both for averages and SD.</p></disp-quote><p>Following your suggestion, we have conducted additional statistical analyses on the model’s output, which are now reported in all result figures using ‘*’ for significance and ‘n.s.’ for non-significant results (see lines 875–893).</p><disp-quote content-type="editor-comment"><p>246-251 – Repetition of a period.</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>It would be helpful if Figure 3 also reported the real bees data, as taken from the various papers. This would give a sense of how closely the model follows the bees behavior. Of course, bees are more complex and are subject to, among others, motivational effects which will make the choice percentage less clean, but I still think this would be appreciable.</p></disp-quote><p>Directly comparing our model’s performance with all existing bee experimental data is beyond the scope of this study. Instead, we selected specific example studies, each addressing different aspects of pattern recognition challenges in bees. Given that our model represents a simplified visual system, it does not allow for a direct comparison with all behavioural data, particularly since bee responses are context-dependent and vary across experimental conditions.</p><p>However, our results suggest that motion plays a beneficial role in visual coding, providing a sparse and selective filtering of visual input before it reaches the learning centre. This structured encoding may facilitate associative processing by enhancing the link between visual patterns and reinforcement signals. While Figure 6 does not include direct behavioural data overlays, we compared them with the real data in the main text.</p><disp-quote content-type="editor-comment"><p>311-312 – &quot;When the model is limited to only four neurons in the lobula, it lacks the capability to encode the entire spatio-temporal structure that is naturally present in the training patterns&quot;. This wording seems to suggest that with more neurons it can encode the entire spatio-temporal structure of the training patterns, which may be an overstatement.</p></disp-quote><p>This has been revised and clarified to enhance understanding.</p><disp-quote content-type="editor-comment"><p>L314-316: I agree that these neurons are sufficient for the discrimination task in hand, but I am unsure whether is appropriate to extend this to bees, as the paragraph title implies. Bees have to use the system to respond to much more complex patterns, like photorealistic ones. For example, is 16 neurons still enough for the face discrimination task?</p></disp-quote><p>Determining the upper capacity of bees in pattern recognition is challenging, as defining visual pattern complexity remains an open question in vision science. Moreover, bee responses are highly context-dependent, influenced by environmental factors, prior experience, and task demands. While our model identifies a sufficient number of neurons for the specific discrimination tasks tested, this should not be directly extrapolated to all visual recognition challenges bees encounter in natural settings. Further behavioural neurobiological studies are needed to investigate whether a larger number of neurons is required to encode higher-dimensional visual features and support more complex pattern recognition tasks. However, we have examined this limitation of our model in Figure 6F and further expanded on it in the revised Discussion section.</p><disp-quote content-type="editor-comment"><p>355-356 – It is unclear how the study would suggest a crucial role of movement in the ability to efficiently analyze and encode the environment. In this work, movement of the input pattern is taken as a given condition under which the network is trained, and not as a tool that can be exploited to have an advantage in the encoding and analysis of the pattern itself.</p><p>Discussion: In general, I am not fully convinced that your model can say anything about the bees or the optimal performance in general, but should focus on the effectiveness of the model itself. This is because of what I have reported above about how the model performance is at least partially dependent on the model design, and not on how bees actually behave (which is hypothesized)</p></disp-quote><p>Our model does not claim to prove that movement is crucial for pattern recognition in bees; rather, it demonstrates how movement-driven spatiotemporal encoding influences neural representation and recognition efficiency.</p><p>– Movement is not merely a given condition in our approach; instead, it acts as a structural constraint that shapes encoding strategies in lobula neurons. The model shows that spatiotemporal integration from sequential scanning leads to a sparse and decorrelated representation, which enhances pattern recognition (Figures 2 and 5). Furthermore, experiments comparing different scanning speeds, and the absence of movement (Figure 4) reveal a significant impact on model performance, supporting the importance of motion in efficient visual processing.</p><p>– These findings align with experimental observations in bees, where active scanning has been shown to improve visual encoding, as opposed to processing entire images in a single fixation (see introduction). Our results suggest that this scanning strategy may enhance encoding efficiency, reinforcing previous behavioural and neurophysiological evidence.</p><p>Regarding the broader biological relevance of our model, we agree that its performance is influenced by our simplification in design choices, and we do not claim that it directly predicts optimal strategies in bees. Instead, our goal is to explore how motion-driven encoding affects visual learning and to generate testable hypotheses for future experimental studies, as discussed in the Discussion section (lines 505-518).</p><p>The revised manuscript more clearly distinguishes between model-intrinsic properties and biologically hypothesised mechanisms, ensuring that our claims are well-supported, appropriately framed, and accurately contextualised.</p><disp-quote content-type="editor-comment"><p>374-375 – In these lines, it is claimed that the model acts as a linear generative model, however, this is not shown in the results and these generative capabilities are not demonstrated.</p></disp-quote><p>Thank you for highlighting this point. We have now clarified this claim in the Discussion section (lines 524–535) by explicitly describing the model's generative properties and providing a more detailed explanation of how its spatiotemporal encoding framework captures key statistical features of natural scenes (Figures 2S, 3, 5).</p><disp-quote content-type="editor-comment"><p>487 – Calling $r_l^{La}$ as &quot;the output of one lamina neuron&quot; instead of &quot;one lamina neuron&quot; could improve clarity.</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>498 – I have not clear what &quot;however&quot; refers to, in this context</p></disp-quote><p>This section has been revised.</p><disp-quote content-type="editor-comment"><p>498 – Similarly. Rewording &quot;the input of the m −the medulla neuron is calculated&quot; to something like &quot;the input to the $m$-th medulla neuron, $I_m{Me}$, is calculated as&quot;</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>506-507 – Could reference to Figure 3B be a typo?</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>528-530 &quot;At each step of training, a set of five patches with size 75x75 pixels, selected by shifting 15 pixels over the image from the left or right or the reverse orientation (Figures1B, 2A), was considered as the input of the model.&quot; This wording could be a bit confusing, especially as, coincidentally, 15*5=75. It could be improved to make it clear that one input to the network is (a concatenation?) of 5 patches of 75x75 pixels each, obtained by shifting a window of 75x75 pixels by 15 pixels, 5 times (if this is actually the case).</p></disp-quote><p>Thank you for pointing it out. The entire Methods section, particularly this section, has been revised for improved clarity and explanation.</p></body></sub-article></article>