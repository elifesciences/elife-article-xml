<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">58237</article-id><article-id pub-id-type="doi">10.7554/eLife.58237</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature Article</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Meta-Research</subject></subj-group></article-categories><title-group><article-title>Questionable research practices may have little effect on replicability</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-184964"><name><surname>Ulrich</surname><given-names>Rolf</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8443-2705</contrib-id><email>ulrich@uni-tuebingen.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Rolf Ulrich</bold> is in the Department of Psychology, University of Tübingen, Tübingen, Germany</p></bio></contrib><contrib contrib-type="author" corresp="yes" id="author-187249"><name><surname>Miller</surname><given-names>Jeff</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2718-3153</contrib-id><email>miller@psy.otago.ac.nz</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Jeff Miller</bold> is in the Department of Psychology, University of Otago, Dunedin, New Zealand</p></bio></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, University of Tübingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Department of Psychology, University of Otago</institution><addr-line><named-content content-type="city">Dunedin</named-content></addr-line><country>New Zealand</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Senior Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>15</day><month>09</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e58237</elocation-id><history><date date-type="received" iso-8601-date="2020-04-24"><day>24</day><month>04</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-09-14"><day>14</day><month>09</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Ulrich and Miller</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Ulrich and Miller</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-58237-v2.pdf"/><abstract><p>This article examines why many studies fail to replicate statistically significant published results. We address this issue within a general statistical framework that also allows us to include various questionable research practices (QRPs) that are thought to reduce replicability. The analyses indicate that the base rate of true effects is the major factor that determines the replication rate of scientific results. Specifically, for purely statistical reasons, replicability is low in research domains where true effects are rare (e.g., search for effective drugs in pharmacology). This point is under-appreciated in current scientific and media discussions of replicability, which often attribute poor replicability mainly to QRPs.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>meta-research</kwd><kwd>p-hacking</kwd><kwd>base rate of true effects</kwd><kwd>replicability</kwd><kwd>false positives</kwd><kwd>mathematical modelling of research process</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Although questionable research practices inflate false positive rates, they have little effect on replicability because they also increase power.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>5</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p>Most sciences search for lawful data patterns or regularities to serve as the building blocks of theories (e.g., <xref ref-type="bibr" rid="bib10">Bunge, 1967</xref>; <xref ref-type="bibr" rid="bib14">Carnap, 1995</xref>; <xref ref-type="bibr" rid="bib55">Popper, 2002</xref>). Generally, such data patterns must not be singular findings (i.e., chance findings) but instead be replicable by other researchers under similar conditions in order to be scientifically meaningful (<xref ref-type="bibr" rid="bib55">Popper, 2002</xref>, p. 23). With this fundamental scientific premise as background, it is understandable that many researchers have become concerned that a surprisingly large number of published results cannot be replicated in independent studies and hence appear to represent chance findings or so-called false positive results (<xref ref-type="bibr" rid="bib3">Baker and Penny, 2016</xref>; <xref ref-type="bibr" rid="bib34">Ioannidis, 2005b</xref>; <xref ref-type="bibr" rid="bib54">Pashler and Harris, 2012</xref>; <xref ref-type="bibr" rid="bib61">Simmons et al., 2011</xref>; <xref ref-type="bibr" rid="bib77">Zwaan et al., 2018</xref>). For example, only less than 30% of results in social psychology and about 50% in cognitive psychology appear to be reproducible (<xref ref-type="bibr" rid="bib53">Open Science Collaboration, 2015</xref>). Similarly, the replication rate of 21 systematically selected experimental studies in the social sciences published between 2010 and 2015 in <italic>Nature</italic> and <italic>Science</italic> was estimated to be only about 62% (<xref ref-type="bibr" rid="bib13">Camerer et al., 2018</xref>). Low replication rates have also been reported in medical research (<xref ref-type="bibr" rid="bib7">Begley and Ellis, 2012</xref>; <xref ref-type="bibr" rid="bib33">Ioannidis, 2005a</xref>; <xref ref-type="bibr" rid="bib56">Prinz et al., 2011</xref>): for example, researchers at the biotechnology firm Amgen tried to confirm findings in 53 landmark studies in preclinical cancer research, but were able to do so for only six cases (<xref ref-type="bibr" rid="bib7">Begley and Ellis, 2012</xref>). The Reproducibility Project: Cancer Biology was set up to further explore the reproducibility of preclinical cancer research (<xref ref-type="bibr" rid="bib16">Errington et al., 2014</xref>).</p><sec id="s1"><title>Possible causes of low replication rates</title><p>Understanding the causes of these shockingly low replication rates has received much attention (e.g., <xref ref-type="bibr" rid="bib12">Button and Munafò, 2017</xref>; <xref ref-type="bibr" rid="bib54">Pashler and Harris, 2012</xref>; <xref ref-type="bibr" rid="bib60">Schmidt and Oh, 2016</xref>), and various possibilities have been discussed. First, scientists may fabricate data to support their hypotheses. However, surveys indicate that this is probably not a major cause because the prevalence of scientific fraud is low—probably smaller than 2% (see <xref ref-type="bibr" rid="bib18">Fanelli, 2009</xref>; <xref ref-type="bibr" rid="bib30">Gross, 2016</xref>; <xref ref-type="bibr" rid="bib67">Stroebe et al., 2012</xref>).</p><p>Second, <xref ref-type="bibr" rid="bib8">Benjamin et al., 2018</xref> recently argued that the traditional <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level of 5% is too large and thus produces too many false positives. These authors suggested changing the critical <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level to 0.5%, because this “would immediately improve the reproducibility of scientific research in many fields” (p. 6). Although this change would decrease the false positive rate, it would also <italic>increase</italic> the proportion of false negatives unless there were substantial increases in sample size (<xref ref-type="bibr" rid="bib19">Fiedler et al., 2012</xref>).</p><p>Third, another important factor seems to be the typically low statistical power in psychological research (<xref ref-type="bibr" rid="bib12">Button and Munafò, 2017</xref>; <xref ref-type="bibr" rid="bib66">Stanley et al., 2018</xref>). Some have reported average power estimates as high as 50% to detect a correlation of 0.2 (corresponding to Cohen’s <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.43</mml:mn></mml:mrow></mml:math></inline-formula>) in the field of social-personality psychology (<xref ref-type="bibr" rid="bib21">Fraley and Vazire, 2014</xref>). In a large survey of over 12,000 effect sizes, however, <xref ref-type="bibr" rid="bib66">Stanley et al., 2018</xref> reported that median power was about 36% and that only 8% of all studies had a power of about 80%. Even lower median power of about 21% has been reported for studies in the neurosciences (<xref ref-type="bibr" rid="bib11">Button et al., 2013</xref>). Low power within a research area reduces replicability for purely statistical reasons, because it reduces the ratio of true positives to false positives.</p><p>Fourth, the percentage or “base rate” <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of true effects within a research area strongly influences the replication rate (<xref ref-type="bibr" rid="bib44">Miller, 2009</xref>; <xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>; <xref ref-type="bibr" rid="bib74">Wilson and Wixted, 2018</xref>). When <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is small, the relative proportion of false positives within a given research domain will be high (<xref ref-type="bibr" rid="bib34">Ioannidis, 2005b</xref>; <xref ref-type="bibr" rid="bib52">Oberauer and Lewandowsky, 2019</xref>), and thus the replication rate will be low. This is easily seen: for <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> the relative proportion of false positives is 100%. In contrast, for <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, no false positives can occur so this proportion is zero. Consequently, replication rates must be higher when the base rate is relatively high than when it is low. For example, <xref ref-type="bibr" rid="bib74">Wilson and Wixted, 2018</xref> have argued that the fields of cognitive and social psychology differ in the base rate of real effects that are investigated, which they call the “prior odds.” On the basis of the results obtained by the <xref ref-type="bibr" rid="bib53">Open Science Collaboration, 2015</xref>, they estimated base rates of <inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mn>0.20</mml:mn></mml:mrow></mml:math></inline-formula> for cognitive psychology and <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:math></inline-formula> for social psychology, and these estimates are consistent with the finding that the replication rate is lower for social than cognitive psychology. Alternative analyses of replication rates and prediction markets also suggest similarly low base rates of about 10% (<xref ref-type="bibr" rid="bib15">Dreber et al., 2015</xref>; <xref ref-type="bibr" rid="bib37">Johnson et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>). More generally, it is reasonable to assume that base rates differ between discovery-oriented research and theory-testing research (<xref ref-type="bibr" rid="bib40">Lewandowsky and Oberauer, 2020</xref>; <xref ref-type="bibr" rid="bib52">Oberauer and Lewandowsky, 2019</xref>).</p><p>Finally, a certain percentage of false positive results is an unavoidable by-product of null hypothesis testing, and, more generally, of any uncertain dichotomous-choice situation in which one is required to choose between two alternatives, such as “accept” or “reject” a vaccine as beneficial in the fight against a certain infectious disease. In such situations, many have argued that replication rates are low because questionable research practices (QRPs) used by scientists chasing after statistically significant results produce an excess of false positive results beyond the usual nominal significance level of 5% (<xref ref-type="bibr" rid="bib35">Ioannidis and Trikalinos, 2007</xref>; <xref ref-type="bibr" rid="bib36">John et al., 2012</xref>; <xref ref-type="bibr" rid="bib61">Simmons et al., 2011</xref>). Such practices violate not only the basic assumptions of the null hypothesis significance testing (NHST) framework but also those underlying decision making within the Bayesian framework, where researchers could analogously use QRPs to obtain large Bayes factors (<xref ref-type="bibr" rid="bib62">Simonsohn, 2014</xref>).</p><p>Hence, a bias toward publication of significant results or large Bayes factors provides a strong incentive to use QRPs (<xref ref-type="bibr" rid="bib4">Bakker et al., 2012</xref>), especially when competing for academic promotion (<xref ref-type="bibr" rid="bib2">Asendorpf et al., 2013</xref>) or grant funding (<xref ref-type="bibr" rid="bib41">Lilienfeld, 2017</xref>). A survey conducted by <xref ref-type="bibr" rid="bib36">John et al., 2012</xref> identified several such practices, and the most frequent ones can be grouped into four categories (a) A researcher may capitalize on chance by performing multiple studies and using <italic>selective reporting</italic> of a significant result. For example, the researcher may conduct several similar experiments until one finally yields the hoped-for significant result, and then the researcher only reports the results of the one study that ‘worked’, putting negative results into the file drawer (<xref ref-type="bibr" rid="bib58">Rosenthal, 1979</xref>). There is convincing evidence that researchers conduct several studies to examine a hypothesis but only report those studies that yielded confirming results (<xref ref-type="bibr" rid="bib24">Francis, 2014</xref>; <xref ref-type="bibr" rid="bib25">Francis et al., 2014</xref>). (b) A researcher may measure multiple dependent measures and report only those that yield significant results. For example, a neuroscientist could record brain activity in hundreds of distinct brain areas and report the results only for those that were sensitive to a specific experimental manipulation (<xref ref-type="bibr" rid="bib73">Vul et al., 2009</xref>). With 10 moderately correlated dependent measures (i.e., <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>) and one-tailed tests, for example, this strategy of <italic>multiple testing</italic> raises the rate of false positives from 5% to 34%. (c) A researcher may monitor data collection, repeatedly testing for significant results, and stop data collection when a significant result is attained. This strategy of <italic>data peeking</italic> can easily raise the rate of false positives up to 20% (<xref ref-type="bibr" rid="bib61">Simmons et al., 2011</xref>). (d) Finally, <italic>selective outlier removal</italic> can also turn a nonsignificant result into a significant one (<xref ref-type="bibr" rid="bib72">Ulrich and Miller, 1994</xref>). For example, if an initial analysis produces nonsignificant results, a researcher may try different criteria for excluding outliers in the hope of getting significant results after the data have been ‘cleaned’.</p><p>With all four of these QRPs as well as other ones, the researcher exploits the degrees of freedom present in the research process to achieve a statistically significant result—a practice that has been referred as “<italic>p</italic>-hacking” (<xref ref-type="bibr" rid="bib63">Simonsohn et al., 2014a</xref>). This clearly inflates the rate of false positives, which would intuitively be expected to decrease replicability. What has received considerably less attention, however, is that <italic>p</italic>-hacking also increases the statistical power for detecting true effects, as noted recently by <xref ref-type="bibr" rid="bib75">Witt, 2019</xref>—a side-effect of <italic>p</italic>-hacking that might be termed <italic>power inflation</italic>. Since increasing power also increases replication rates, the influence of QRPs on power tends to counteract its influence on Type 1 error rate with respect to overall replicability. A quantitative model is therefore needed to assess the size of <italic>p</italic>-hacking’s overall effect on replicability.</p><p>In this paper, we consider in detail the prevailing claim that QRPs are a major cause of low replicability. However, <xref ref-type="bibr" rid="bib22">Francis, 2012a</xref> has noted the converse problem that in some circumstances QRPs can artificially increase replication rates. Specifically, this can happen when researchers use QRPs to significantly replicate their previous findings—usually with conceptual replications—to strengthen their theoretical position. Reanalyses of results from multi-experiment papers suggest that this does happen, because the rate of successful replication is unrealistically large in view of the studies’ power (e.g., <xref ref-type="bibr" rid="bib25">Francis et al., 2014</xref>; <xref ref-type="bibr" rid="bib23">Francis, 2012b</xref>). For example, when the power of a single experiment is 0.36, the probability that a series of five experiments would all result in positive outcomes is 0.36<sup>5</sup> = 0.006, so such a series of published findings would be too good to be true (i.e., an excess of positive results). Such a pattern would suggest the operation of one or more QRPs; for example, negative results may have been unreported, that is, put in the researcher’s file drawer. This situation could be called “motivated replication” and it is different from the situation in which an unbiased researcher tries to replicate a significant result, as in the Open Science Replication Project (<xref ref-type="bibr" rid="bib53">Open Science Collaboration, 2015</xref>), We shall focus on the situation with unbiased replications and assess the extent to which QRPs can reduce the rate of these.</p><p>In the present study, we develop a quantitative model of replication rate that simultaneously takes into account <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, power, the base rate of true effects, and <italic>p</italic>-hacking. This model allows us to assess the relative contributions of these factors to the replication rate, with a focus on the influence of QRPs. In contrast, the combined effects of <italic>p</italic>-hacking on Type 1 error rate and power have not previously been modelled at all, and previous studies have generally considered the effects of these factors on replicability one at a time (e.g., <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, power, base rate), making it difficult to see their relative contributions. Knowledge of the relative contributions of these different factors would increase our understanding of why the observed replication rate is so low and thus be useful in guiding efforts to improve the situation. Because the various different <italic>p</italic>-hacking strategies reviewed above may have different impacts on the replication rate, we conducted separate analyses for each strategy.</p></sec><sec id="s2"><title>Statistical analysis of the replication scenario</title><p>The analyses in this manuscript address replication scenarios in which researchers conduct direct replications of studies that reported a statistically significant positive outcome. An example is the Open Science Replication Project (<xref ref-type="bibr" rid="bib53">Open Science Collaboration, 2015</xref>), in which many independent research teams conducted high-powered studies attempting to directly replicate published results. <xref ref-type="fig" rid="fig1">Figure 1</xref> depicts these scenarios together with all statistically relevant parameters that must be taken into account when computing the rate of replicating significant results (<xref ref-type="bibr" rid="bib44">Miller, 2009</xref>; <xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>; <xref ref-type="bibr" rid="bib45">Miller and Schwarz, 2011</xref>). First, each original study tests either a true effect (i.e., <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is true) or a null effect (i.e., <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is true), with base rate probabilities <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula>, respectively, and these probabilities—sometimes called “pre-study probabilities” (<xref ref-type="bibr" rid="bib34">Ioannidis, 2005b</xref>) or “prior odds” (<xref ref-type="bibr" rid="bib74">Wilson and Wixted, 2018</xref>)—may vary across research fields (<xref ref-type="bibr" rid="bib74">Wilson and Wixted, 2018</xref>). If the original study tests a true effect, its statistical power is <inline-formula><mml:math id="inf17"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and the Type 2 error probability is equal to <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>. Thus, the compound probability of examining a true effect and rejecting the null hypothesis is <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; this outcome is called a “true positive.” In contrast, if the original study tests a null effect, its Type 1 error probability is <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>. Thus, the probability of testing a null effect and falsely rejecting <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is <inline-formula><mml:math id="inf22"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>; this outcome reflects a “false positive.” Note that, in keeping with accepted procedures for null hypothesis testing, we categorize studies as rejecting the null hypothesis or not based on an all-or-none comparison of computed <italic>p</italic>-values relative to an <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level cutoff. Such a discrete categorization is, for example, how most journals currently evaluate statistical results in publication decisions and how replication success or failure has mainly been operationalized in empirical studies of replication rates (<xref ref-type="bibr" rid="bib13">Camerer et al., 2018</xref>; <xref ref-type="bibr" rid="bib53">Open Science Collaboration, 2015</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Probability tree of the replication scenario.</title><p>The base rates of examining an alternative hypothesis <inline-formula><mml:math id="inf24"><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> or a null hypothesis <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> are <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf27"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula>, respectively. The statistical power and the Type 1 error rate of the original study are <inline-formula><mml:math id="inf28"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>. There are four possible outcomes of an original study, with the researcher deciding to reject the null hypothesis (i.e., ‘<inline-formula><mml:math id="inf30"><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>’) in two outcomes and failing to reject it (i.e., ‘<inline-formula><mml:math id="inf31"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>’) in the other two. If <inline-formula><mml:math id="inf32"><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is true, the outcomes associated with these decisions are called true positives and false negatives. By contrast, if <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> holds, they are called false positives and true negatives. Replication studies replicate original studies that reported a significant positive result. The statistical power and the Type 1 error probability of the replication study are <inline-formula><mml:math id="inf34"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, respectively. The replication study may either reject <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (which denotes a successful replication of the original positive result, <inline-formula><mml:math id="inf37"><mml:mi>R</mml:mi></mml:math></inline-formula>) or fail to reject it (which denotes a failure to replicate the original result, <inline-formula><mml:math id="inf38"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig1-v2.tif"/></fig><p>Only true positives and false positives enter into replication projects. The statistical power <inline-formula><mml:math id="inf39"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and Type 1 error probability <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> of the replication studies might differ from those of the original study, especially because replication studies are usually designed to have much higher power than the original studies. Thus, the compound probability of examining a true effect that yields a significant effect in the original and in the replication study is <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, whereas the compound probability of examining a null effect and finding significance in both the initial study and the replication study is <inline-formula><mml:math id="inf42"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. From the above compound probabilities, the rate of replication of initially significant results, <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, can be computed as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="fig" rid="fig2">Figure 2</xref> illustrates this equation by showing how <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> depends on <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> when the nominal alpha level and the statistical power of the replication studies are <inline-formula><mml:math id="inf48"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf49"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>. It can be seen in this figure that <inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> increases gradually with <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> from a minimum of <inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> to a maximum of <inline-formula><mml:math id="inf53"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>. For <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the proportion of significant results can only represent false positives, so <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> necessarily equals <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. For <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, in contrast, <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> merely reflects the power of the replication study. As is also illustrated in this figure, <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> grows faster when the power <inline-formula><mml:math id="inf60"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of the original studies is relatively large and their nominal alpha level <inline-formula><mml:math id="inf61"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is relatively small. Note that <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> must gradually increase with <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> from <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf65"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> even if the power in the original study were 100%. It is also instructive to note that worst-case <italic>p</italic>-hacking would imply <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. In this case it follows from <xref ref-type="disp-formula" rid="equ13">Equation 1</xref> that <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> approaches the line which runs from <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> at <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf71"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> at <inline-formula><mml:math id="inf72"><mml:mrow><mml:mi>π</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Rate of replication <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>R</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> as a function of base rate <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</title><p>Each line represents a different combination of the nominal alpha level <inline-formula><mml:math id="inf75"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and the statistical power <inline-formula><mml:math id="inf76"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> used by the original studies. The nominal alpha level and the power of the replication studies were always <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>% and <inline-formula><mml:math id="inf78"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig2-v2.tif"/></fig><p>If <italic>p</italic>-hacking is performed in the original study, this would increase the Type 1 error rate above the nominal significance level <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> (usually 5%) to, for example, 10% or even higher. Thus, when a researcher examines a null effect, <italic>p</italic>-hacking increases the proportion of false positives. The extent of this increase depends on the details of the <italic>p</italic>-hacking strategy that is used, as we examine in detail below for different strategies. However, and crucially for the analyses that will follow, when a true effect is present, <italic>p</italic>-hacking also increases the nominal power <inline-formula><mml:math id="inf80"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, for example, from 0.20 to 0.40 (i.e., power inflation, as mentioned above). With respect to the overall replication rate <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, this increase in power tends to compensate for the increased Type 1 error probability, making it difficult to determine intuitively how <italic>p</italic>-hacking would affect the replication rate <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>. Fortunately, however, <xref ref-type="disp-formula" rid="equ13">Equation 1</xref> can be used to assess this issue quantitatively.</p><p>Besides assessing the effect of these factors on replicability, we will also report computations of the rate of false positives <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, which is the proportion of false positive results among all significant results within a research area (sometimes also called <italic>false discovery rate</italic> or <italic>false positive report probability</italic>)<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In discussions about replicability—particularly replicability of published research findings—researchers often focus on this proportion (<xref ref-type="bibr" rid="bib11">Button et al., 2013</xref>; <xref ref-type="bibr" rid="bib54">Pashler and Harris, 2012</xref>) under the assumption that true positives are replicable but false positives are not. Therefore, it seems useful to include this rate in the analyses.</p><p>In the following, we model each of the four common <italic>p</italic>-hacking strategies that were described above. For each strategy, the inflated Type 1 error probability and the statistical power can be computed. These values are then inserted into <xref ref-type="disp-formula" rid="equ13">Equation 1</xref>, which allows one to evaluate the effects of base rates and <italic>p</italic>-hacking on the replication rate, for both true and null effects. In addition, we examined the effects on <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> of different levels of <inline-formula><mml:math id="inf85"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and statistical power, because—as mentioned above—several researchers have recently suggested lowering the <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level or increasing power in order to increase the replicability of scientific results (<xref ref-type="bibr" rid="bib8">Benjamin et al., 2018</xref>; <xref ref-type="bibr" rid="bib11">Button et al., 2013</xref>). This allows one to judge how these suggested measures would combat low replicability and to compare their effects with those of <italic>p</italic>-hacking and base rate.</p></sec><sec id="s3"><title>Selective reporting of significant studies</title><p>It has been often suspected that researchers tend to selectively report studies that yield positive results, that is, results that are in accordance with the researcher’s hypothesis (e.g., <xref ref-type="bibr" rid="bib36">John et al., 2012</xref>; <xref ref-type="bibr" rid="bib58">Rosenthal, 1979</xref>; <xref ref-type="bibr" rid="bib61">Simmons et al., 2011</xref>; <xref ref-type="bibr" rid="bib77">Zwaan et al., 2018</xref>). As noted earlier, this tendency will increase the number of reported false positives if researchers publish only the significant outcomes. This section models this <italic>p</italic>-hacking strategy and examines how it would influence the replication rate.</p><p>As a specific example, suppose that a researcher runs a series of experiments, each of which uses a slight variation of the same basic paradigm. This researcher terminates the series when a significant result emerges in support of the researcher’s hypothesis, and in this case the researcher tries to publish that result. However, if no significant result is obtained after conducting <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> experiments, the researcher abandons the project and concludes that the hypothesis is false. Thus, this researcher has <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> studies providing opportunities to test the hypothesis, and it would be misleading about the overall <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level to publish only the significant outcome but not mention the non-significant attempts (<xref ref-type="bibr" rid="bib24">Francis, 2014</xref>).</p><p>To model this scenario more concretely, assume that the researcher computes a <italic>z</italic>-value for the outcome of each experiment and considers the outcome to be statistically significant if any <italic>z</italic>-value exceeds a pre-specified criterion <italic>c</italic> (e.g., the critical <italic>z</italic> value of 1.96). In general, the probability of rejecting <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> can be computed for <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> with<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>because the outcomes of the <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> experiments are statistically independent if a new sample is recruited each time.</p><p><xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> depict the probability of rejecting <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> for two- and one-sample tests, respectively, as a function of <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and effect size <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.8</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. (Appendix 1 contains a detailed description of both tests.) In these examples, the group size is assumed to be <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (i.e., total <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula> for a two-sample test), a value that is typical for psychological research (<xref ref-type="bibr" rid="bib42">Marszalek et al., 2011</xref>, Table 3), though there is evidence that sample sizes have increased recently in the field of social-personality psychology (<xref ref-type="bibr" rid="bib21">Fraley and Vazire, 2014</xref>; <xref ref-type="bibr" rid="bib59">Sassenberg and Ditrich, 2019</xref>). The lines for <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> depict the effective Type 1 error probability. Of course, this probability is equal to <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf101"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, but it increases with <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> because of the greater number of opportunities for getting a significant result by chance when more studies are conducted. This increased Type 1 error probability is problematic because it tends to decrease replication rates (<xref ref-type="bibr" rid="bib8">Benjamin et al., 2018</xref>). In the worst of these cases, the inflated Type 1 error rate attains a value of about 0.34 with <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>. As one expects, decreasing the nominal <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level from 5% to 0.5% substantially diminishes the Type 1 error probability and thus correspondingly diminishes the probability of obtaining a false positive (<xref ref-type="bibr" rid="bib8">Benjamin et al., 2018</xref>). Even for <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> the Type 1 error rate would only be about 0.04 with this smaller nominal <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level. It must be stressed, however, that a larger sample would be required for <inline-formula><mml:math id="inf108"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> than for <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to achieve the same level of statistical power in both cases (<xref ref-type="bibr" rid="bib8">Benjamin et al., 2018</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Selective reporting of significant studies.</title><p>Each panel depicts the probability of rejecting <inline-formula><mml:math id="inf110"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> in at least one study as a function of the number of studies <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, nominal <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level, and effect size <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for a two-sample test with <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> participants in each sample.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Selective reporting of significant studies.</title><p>Each panel depicts the probability of rejecting <inline-formula><mml:math id="inf115"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> in at least one study as a function of the number of studies <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, nominal <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level, and effect size <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for a one-sample test with <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig3-figsupp1-v2.tif"/></fig></fig-group><p>The lines for <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> reveal the statistical power to reject <inline-formula><mml:math id="inf121"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> when it is false. When researchers follow good scientific practice, the statistical power associated with each value of <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> can be seen at <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. As is well known, power generally increases with <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and it is larger with <inline-formula><mml:math id="inf125"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> than with <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For the present purposes, however, the most important aspect of the <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> lines is the strong increase of statistical power with <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, which can be seen in both panels, especially when the single-experiment power is well below one. Since replication rates increase with power (<xref ref-type="bibr" rid="bib11">Button et al., 2013</xref>; <xref ref-type="bibr" rid="bib12">Button and Munafò, 2017</xref>), this power inflation will tend to compensate for the increased Type 1 error rate with respect to the overall influence of selective reporting on replication rate. It is therefore necessary to use a quantitative model to assess the net effect of this practice on the replication rate.</p><p>Using the above probabilities of rejecting <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, the proportion of false positives associated with this <italic>p</italic>-hacking scenario can be computed from <xref ref-type="disp-formula" rid="equ14">Equation 2</xref>. <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> highlight the false positive rate as a function of <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Dashed lines show the rates for researchers engaged in <italic>p</italic>-hacking. For comparison, the solid lines depict the rates for researchers who follow good scientific practice by just running a single experiment and reaching a conclusion based on its outcome (i.e., <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>). The rates for these researchers were also computed with <xref ref-type="disp-formula" rid="equ14">Equation 2</xref> by inserting the nominal value of <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and the single-experiment power for <inline-formula><mml:math id="inf136"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Selective reporting of significant studies.</title><p>False positive rate (FPR) as a function of base rate <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of studies <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf142"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>% and <inline-formula><mml:math id="inf143"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%. All results are based on <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> per group. Dashed lines give the results for <italic>p</italic>-hacking whereas solid lines depict the results of researchers who act in accord with good scientific practice. Note that the solid lines are the same in all rows of a single column because these constant reference lines do not depend on <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Selective reporting of significant studies.</title><p>False positive rate (FPR) as a function of base rate <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of studies <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf151"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>% and <inline-formula><mml:math id="inf152"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%. All results are based on one-sample tests with <inline-formula><mml:math id="inf153"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig4-figsupp1-v2.tif"/></fig></fig-group><p>Several effects can be observed in <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>: (a) As one expects, the false positive rate decreases from one to zero with increasing <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, because the proportion of true effects among all significant effects becomes larger when <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> increases (e.g., <xref ref-type="bibr" rid="bib34">Ioannidis, 2005b</xref>; <xref ref-type="bibr" rid="bib74">Wilson and Wixted, 2018</xref>). (b) Not surprisingly, the false positive rate becomes smaller when power increases due to larger <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib34">Ioannidis, 2005b</xref>). (c) Most interestingly and surprisingly, the increase in false positives produced by <italic>p</italic>-hacking is more pronounced with larger <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where statistical power is higher. This is presumably because <italic>p</italic>-hacking cannot increase statistical power much when it is already high (i.e., when <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is large), so there is little power inflation to compensate for the increased Type 1 error rate. Nevertheless, the effect of <italic>p</italic>-hacking is far from dramatic for <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, although it can be quite prominent for larger values of <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, especially with small base rates.</p><p><xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref> depict replication rates computed using the same parameters as in the previous figures. In addition, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref> augment these figures and specifically focus on decrease in RR (i.e., “shrinkage”) caused by <italic>p</italic>-hacking. Three features of these computations are especially noticeable. (a) Successful replication depends strongly on the base rate. As one might expect, all rates converge to the statistical power <inline-formula><mml:math id="inf161"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula> of the replication study, because when all significant effects are real, the replication rate simply reflects the statistical power of the replication study, whether <italic>p</italic>-hacking was involved in the first study or not. (b) The effect of <italic>p</italic>-hacking is modest for high base rates, for the smaller <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level, and interestingly also for smaller effect sizes and hence for low statistical power. (c) As emphasized by <xref ref-type="bibr" rid="bib8">Benjamin et al., 2018</xref>, the replication rate is considerably larger for <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> than for <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, especially for small base rates.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Selective reporting of significant studies.</title><p>Replication rate (RR) as a function of base rate <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of studies <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf170"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>% and <inline-formula><mml:math id="inf171"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%. All results are based on <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> per group. Dashed lines give the results for <italic>p</italic>-hacking whereas solid lines depict the results of researchers who act in accord with good scientific practice. Note that the solid lines are the same in all rows of a single column because these constant reference lines do not depend on <italic>k</italic>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Selective reporting of significant studies.</title><p>Shrinkage of the replication rate (i.e., the difference between the solid line and the dashed line in <xref ref-type="fig" rid="fig5">Figure 5</xref>) as a function of base rate <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of studies <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Selective reporting of significant studies.</title><p>Replication rate (RR) as a function of base rate <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of studies <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf182"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>% and <inline-formula><mml:math id="inf183"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%. All results are based on one-sample tests with <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Selective reporting of significant studies.</title><p>Shrinkage of the replication rate (i.e., the difference between the solid line and the dashed line in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>) as a function of base rate <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of studies <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig5-figsupp3-v2.tif"/></fig></fig-group><p>In summary, the above analysis casts doubt on the idea that this <italic>p</italic>-hacking strategy is a major contributor to low replicability, even though it seems to be one of the most frequent QRPs (e.g., <xref ref-type="bibr" rid="bib36">John et al., 2012</xref>). Instead, it seems that using this strategy would have little effect on replicability except in research scenarios where true effects were rare but there was high power to detect them when they were present. The strongest trends suggest that a low base rate of true effects is the major cause of low replicability (<xref ref-type="bibr" rid="bib74">Wilson and Wixted, 2018</xref>), since changes in base rate can cause replication rates to range across nearly the full 0–1 range.</p></sec><sec id="s4"><title>Failing to report all dependent measures</title><p>Failing to report all of a study’s dependent measures seems to be another common QRP (<xref ref-type="bibr" rid="bib20">Fiedler and Schwarz, 2016</xref>; <xref ref-type="bibr" rid="bib36">John et al., 2012</xref>). In this section, we analyze how this practice would affect the rate of replicating statistically significant results. In order to model this scenario, we assume that a researcher conducts a study to test a certain hypothesis using control and experimental conditions. After data collection, however, the researcher only reports the outcomes of those dependent measures whose tests surpass the statistical significance threshold and thereby confirm the proposed hypothesis. As examples, multiple dependent measures are usually measured and statistically evaluated in neurosciences and medical research, raising concerns about Type 1 error rates in those fields (e.g., <xref ref-type="bibr" rid="bib32">Hutton and Williamson, 2002</xref>; <xref ref-type="bibr" rid="bib73">Vul et al., 2009</xref>).</p><p>We again employed <italic>z</italic>-tests to model this scenario. Let <inline-formula><mml:math id="inf189"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> be the outcomes for all <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> dependent measures of a single study, with each <inline-formula><mml:math id="inf191"><mml:mi>Z</mml:mi></mml:math></inline-formula>-value representing the result of the control/experimental comparison for a single measure. Therefore, the probability of obtaining at least one significant result is equal to<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>with <italic>c</italic> being the critical cutoff value (see Appendix 1 for computational details). Because such measures are usually correlated across participants, our model incorporates correlations among the <inline-formula><mml:math id="inf192"><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> values.</p><p><xref ref-type="fig" rid="fig6">Figure 6</xref> illustrates the effects on Type 1 error probability (i.e., lines with <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) and statistical power (i.e., lines with <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) associated with this type of <italic>p</italic>-hacking. For this illustration, the pair-wise correlations of the different dependent measures were set to 0.2 and the sample size (per group) was set to 20, which are seemingly typical values in psychological research (<xref ref-type="bibr" rid="bib9">Bosco et al., 2015</xref>; <xref ref-type="bibr" rid="bib42">Marszalek et al., 2011</xref>). As expected, both the Type 1 error rate and power increase with the number of dependent measures, approximately as was found with selective reporting.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Failing to report all dependent measures.</title><p>Each panel depicts the probability of rejecting <inline-formula><mml:math id="inf195"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> as a function of the number of dependent measures <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, nominal <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level, and effect size <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for a two-sample test with <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> participants per group and dependent measure intercorrelations of 0.2.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Failing to report all dependent measures.</title><p>Each panel depicts the probability of rejecting <inline-formula><mml:math id="inf200"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> as a function of the number of dependent measures <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, nominal <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level, and effect size <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for a two-sample test with <inline-formula><mml:math id="inf204"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> participants per group and dependent measure intercorrelations of 0.8.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig6-figsupp1-v2.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig7">Figures 7</xref> and <xref ref-type="fig" rid="fig8">8</xref>, and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref> show the rate of false positives, rate of replications, and the shrinkage of the replication rate, respectively, resulting from this type of <italic>p</italic>-hacking. These results are quite similar to those seen with the selective reporting scenario (see <xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). In particular, both false positive rates and replication rates show strong expected effects of base rate and <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level, as well as a clear influence of effect size, <italic>d</italic>. The effects of <italic>p</italic>-hacking are again rather modest, however, especially when the effect size is small (i.e., <inline-formula><mml:math id="inf206"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>) so that increased power is especially helpful.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Failing to report all dependent measures.</title><p>False positive rate (FPR) as a function of base rate <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of dependent measures <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf212"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>% and <inline-formula><mml:math id="inf213"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%. All results are based on two-sample tests with <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> per group and dependent measure intercorrelations of 0.2. Dashed lines give the results for <italic>p</italic>-hacking whereas solid lines depict the results of researchers who act in accord with good scientific practice. Note that the solid lines are the same in all rows of a single column because these constant reference lines do not depend on <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Failing to Report all Dependent Measures.</title><p>False positive rate (FPR) as a function of base rate <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of dependent measures <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf221"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>% and <inline-formula><mml:math id="inf222"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%. All results are based on two-sample tests with <inline-formula><mml:math id="inf223"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> participants per group and dependent measure intercorrelations of 0.8.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig7-figsupp1-v2.tif"/></fig></fig-group><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Failing to report all dependent measures.</title><p>Replication rate (RR) as a function of base rate <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of dependent measures <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf229"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>% and <inline-formula><mml:math id="inf230"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%. All results are based on two-sample tests with <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> per group and dependent measure intercorrelations of 0.2. Dashed lines give the results for <italic>p</italic>-hacking whereas solid lines depict the results of researchers who act in accord with good scientific practice. Note that the solid lines are the same in all rows of a single column because these constant reference lines do not depend on <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Failing to report all dependent measures.</title><p>Shrinkage of the replication rate (i.e., the difference between the solid and dashed lines in <xref ref-type="fig" rid="fig8">Figure 8</xref>) as a function of base rate <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of dependent measures <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig8-figsupp1-v2.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>Failing to report all dependent measures.</title><p>Replication rate (RR) as a function of base rate <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of dependent measures <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf242"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>% and <inline-formula><mml:math id="inf243"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%. All results are based on two-sample tests with <inline-formula><mml:math id="inf244"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> participants per group and dependent measure intercorrelations of 0.8.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig8-figsupp2-v2.tif"/></fig><fig id="fig8s3" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 3.</label><caption><title>Failing to report all dependent measures.</title><p>Shrinkage of the replication rate (i.e., the difference between the solid and dashed lines in <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>) as a function of base rate <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of dependent measures <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig8-figsupp3-v2.tif"/></fig></fig-group><p>It should be noted that the extent of both Type 1 error rate inflation and power enhancement depend on the correlations among the different dependent measures. A correlation of zero would yield results identical to those of the scenario with selective reporting in the previous section, because in this case the outcomes for multiple dependent measures are independent just like the outcomes of multiple independent studies. In contrast, larger correlations (e.g., larger than the 0.2 used in <xref ref-type="fig" rid="fig7">Figures 7</xref> and <xref ref-type="fig" rid="fig8">8</xref> and <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>) weaken the effects of this <italic>p</italic>-hacking strategy, because the measures become increasingly redundant as the intercorrelations increase, and this lowers the possibility of capitalizing on chance. In other words, increasing the intercorrelations would decrease the inflation of both Type 1 error rate and power. Moreover, increased intercorrelations would decrease the false positive rate and increase the replication rate, that is, moving the dashed lines in <xref ref-type="fig" rid="fig7">Figures 7</xref> and <xref ref-type="fig" rid="fig8">8</xref> toward the solid reference lines (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>, and <xref ref-type="fig" rid="fig8s3">Figure 8—figure supplement 3</xref> for a parallel analysis with intercorrelations of 0.8).</p></sec><sec id="s5"><title>Data peeking</title><p>Another frequently-used QRP is data peeking (<xref ref-type="bibr" rid="bib20">Fiedler and Schwarz, 2016</xref>; <xref ref-type="bibr" rid="bib36">John et al., 2012</xref>). This practice occurs when a researcher collects additional data after finding that the results of initially collected data have not yielded statistical significance. A researcher may even peek at the results several times and increase the sample with additional observations each time a nonsignificant result is obtained. Data collection is finally terminated only if the study yields no significant result after <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> peeks. It is known that this practice increases the Type 1 error rate (<xref ref-type="bibr" rid="bib1">Armitage et al., 1969</xref>; <xref ref-type="bibr" rid="bib22">Francis, 2012a</xref>; <xref ref-type="bibr" rid="bib43">McCarroll et al., 1992</xref>; <xref ref-type="bibr" rid="bib61">Simmons et al., 2011</xref>; <xref ref-type="bibr" rid="bib68">Strube, 2006</xref>). For example, Monte-Carlo simulations by <xref ref-type="bibr" rid="bib61">Simmons et al., 2011</xref> revealed that this strategy can increase the error rate up to 14.3% with a first peek at <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> and four subsequent peeks (each time increasing the sample by 10 observations). However, this practice increases not only the Type 1 error rate but also the effective statistical power to reject a false <inline-formula><mml:math id="inf251"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib68">Strube, 2006</xref>), so a quantitative analysis is needed to determine its effect on replication rate.</p><p>An analysis similar to that of the preceding sections was conducted to examine how data peeking affects Type 1 error rates, power levels, false positive rates, and replication rates. Appendix 1 contains the computational details of this analysis, which follows an extension of Armitage's procedure (<xref ref-type="bibr" rid="bib1">Armitage et al., 1969</xref>). In brief, the probability of rejecting <inline-formula><mml:math id="inf252"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> with a maximum of <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> peeks at successive sample sizes <inline-formula><mml:math id="inf254"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is again given by the multivariate normal distribution for <italic>z</italic>-tests<disp-formula id="equ5"><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The correlations among the different <inline-formula><mml:math id="inf255"><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> values are determined by the amount of shared data used in computing them (e.g., all observations used in computing <inline-formula><mml:math id="inf256"><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> are also included in the computation of <inline-formula><mml:math id="inf257"><mml:msub><mml:mi>Z</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>).</p><p><xref ref-type="fig" rid="fig9">Figure 9</xref> depicts the probability of rejecting <inline-formula><mml:math id="inf258"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> for various effect sizes and two-sample tests. The abscissa represents the maximal number of peeks <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> at which a researcher would give up recruiting additional participants. For this example, it is assumed that data peeking occurs after 10, 15, 20, 25, 30, 35, 40, or 45 observations per group. Thus, a researcher with a maximum of <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> peeks will check statistical significance the first time at <inline-formula><mml:math id="inf261"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> and if the first peek does not reveal a significant result, the data will be examined a second and final time at <inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>. For <inline-formula><mml:math id="inf263"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, data will be examined a first time at <inline-formula><mml:math id="inf264"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> and—depending on the outcome of the first peek—a second time at <inline-formula><mml:math id="inf265"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula>; if the second peek also does not reveal a significant result, a final peek occurs at <inline-formula><mml:math id="inf266"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>.</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Data peeking.</title><p>Each panel depicts the probability of rejecting <inline-formula><mml:math id="inf267"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> as a function of the number of maximal peeks <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, nominal <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level, and effect size <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for a two-sample test.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig9-v2.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>Data peeking.</title><p>False positive rate (FPR) as a function of base rate <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of maximal data peeks <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf275"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf276"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>. Dashed lines give the results for <italic>p</italic>-hacking whereas solid lines depict the results of researchers who act in accord with good scientific practice. Note that the solid lines are the same in all rows of a single column because these constant reference lines do not depend on <italic>k</italic>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig9-figsupp1-v2.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig9">Figure 9</xref> shows quantitatively how the probability of rejecting <inline-formula><mml:math id="inf277"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> increases with the maximum number of peeks. In particular, the increase can be quite strong in situations with only moderate power (e.g., <inline-formula><mml:math id="inf278"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf279"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>) due to the extra chances of detecting the true effect. In contrast to the multiple dependent measures with intercorrelations of 0.2 as discussed in the previous section, the Type 1 error rate inflation is smaller in the present case, because <inline-formula><mml:math id="inf280"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are more strongly correlated under this scenario (cf. the correlation matrix in Appendix 1).</p><p>Given the probabilities of rejecting <inline-formula><mml:math id="inf281"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, the replication rate and false positive rate are again computed using <xref ref-type="disp-formula" rid="equ13 equ14">Equations 1 and 2</xref>, respectively. The results with respect to the false positive rate (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>) and the replication rate (<xref ref-type="fig" rid="fig10">Figure 10</xref> and <xref ref-type="fig" rid="fig10s1">Figure 10—figure supplement 1</xref>) are quite similar to those of the preceding scenarios. We compare this <italic>p</italic>-hacking strategy with researchers who conform to good scientific practice and thus examine the data only once at a preplanned <italic>n</italic>. In order to enable a conservative comparison with <italic>p</italic>-hackers, we used a preplanned <italic>n</italic> corresponding to the maximum number of observations a <italic>p</italic>-hacker would try when using the indicated number of peeks (i.e., this preplanned group size would be <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math></inline-formula> for the comparison with <inline-formula><mml:math id="inf283"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf284"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> for the comparison with <inline-formula><mml:math id="inf285"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, etc.). As can be seen, the pattern of results is quite comparable to the previous scenarios. Overall, the data peeking strategy again seems to have little effect on replication rate except in research scenarios where true effects are infrequent and there is high power to detect them when they do occur, just as with selective reporting.</p><fig-group><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Data peeking.</title><p>Replication Rate (RR) as a function of base rate <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of maximal data peeks <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf290"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf291"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>. Dashed lines give the results for <italic>p</italic>-hacking whereas solid lines depict the results of researchers who act in accord with good scientific practice. Note that the solid lines are the same in all rows of a single column because these constant reference lines do not depend on <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig10-v2.tif"/></fig><fig id="fig10s1" position="float" specific-use="child-fig"><label>Figure 10—figure supplement 1.</label><caption><title>Data peeking.</title><p>Shrinkage of the replication rate (i.e., the difference between the solid and dashed lines in <xref ref-type="fig" rid="fig10">Figure 10</xref>) as a function of base rate <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number of maximal peeks <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig10-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s6"><title>Selective outlier removal</title><p>Another QRP identified by <xref ref-type="bibr" rid="bib36">John et al., 2012</xref> is to analyze the same overall data set several times, each time excluding “outlier” data points identified by different criteria. The researcher may be tempted to conclude that a real effect has been found if any analysis yields a significant result, but this practice inflates the Type 1 error rate, because each of the analyses provides a further opportunity to obtain a significant result by chance. On the positive side, though, this practice again increases power, because each of the analyses also provides a further opportunity for detecting a real effect.</p><p>Because the effects of this type of <italic>p</italic>-hacking are not computable, we conducted Monte-Carlo simulations to see how multiple attempts at outlier removal would affect the Type 1 error rate, power, rate of false positives, and replication rate. Specifically, we examined the common practice of excluding scores more than a given number of standard deviations from the sample mean. We simulated researchers who carried out a sequence of at most five separate analyses on a single data set. The first three analyses included only scores within 3, 2.5, and 2 standard deviations of the mean, respectively, because these limits are most commonly employed in psychological research (<xref ref-type="bibr" rid="bib5">Bakker and Wicherts, 2014</xref>). The fourth analysis used the <xref ref-type="bibr" rid="bib70">Tukey, 1977</xref> “fences” method by including all scores within the range <inline-formula><mml:math id="inf297"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="4.2pt">,</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mn>1.5</mml:mn><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf298"><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf299"><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> are the 25 and 75% percentile points of the data set. The fifth analysis used a nonparametric test, which could potentially be used as an analysis in an attempt to minimize the influence of outliers even further.</p><p>We simulated experiments for both one- and two-sample tests, but only report the latter because the two simulations produced extremely similar results. There was a sample size of <inline-formula><mml:math id="inf300"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> per group using standard normally distributed scores and true effect sizes of <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, 0.2, 0.5, and 0.8. Researchers were modelled as using either <inline-formula><mml:math id="inf302"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> or 5%, one-tailed. The nonparametric test was the Mann-Whitney <inline-formula><mml:math id="inf303"><mml:mi>U</mml:mi></mml:math></inline-formula> test, and this test was used only if none of the previous analyses had produced significant results. We simulated 10,000 experiments with outliers by adding a random noise value to 5% of the data values, where these noise values came from a normal distribution with <inline-formula><mml:math id="inf304"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf305"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>. This simulation method has often been adopted to model contamination effects of outliers (e.g., <xref ref-type="bibr" rid="bib5">Bakker and Wicherts, 2014</xref>; <xref ref-type="bibr" rid="bib76">Zimmerman, 1998</xref>).</p><p><xref ref-type="fig" rid="fig11">Figure 11</xref> shows the probabilities of rejecting <inline-formula><mml:math id="inf306"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. As with the other <italic>p</italic>-hacking methods, this probability increases with the number of analyses conducted, increasing the probability of a Type 1 error when <inline-formula><mml:math id="inf307"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and increasing power when <inline-formula><mml:math id="inf308"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. <xref ref-type="fig" rid="fig12">Figures 12</xref> and <xref ref-type="fig" rid="fig13">13</xref> show the false positive and replication rates; <xref ref-type="fig" rid="fig13s1">Figure 13—figure supplement 1</xref> depicts the shrinkage of the replication rate. Interestingly, in some cases these measures even indicate slightly better results (i.e., lower false positive rates and higher replication rates) when researchers perform multiple analyses to remove the effects of possible outliers than when they do not. Most importantly, however, the present scenario also reveals that the major impact on the replication rate seems to come from the base rate.</p><fig-group><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Selective outlier removal.</title><p>Estimated probability of rejecting <inline-formula><mml:math id="inf309"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> as a function of the number <inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of outlier rejection methods attempted for various effect sizes <inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). Probability estimates were based on 10,000 simulated experiments. Simulated data included 5% outliers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig11-v2.tif"/></fig><fig id="fig11s1" position="float" specific-use="child-fig"><label>Figure 11—figure supplement 1.</label><caption><title>Selective outlier removal.</title><p>Estimated probability of rejecting <inline-formula><mml:math id="inf313"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> as a function of the number <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of outlier rejection methods attempted for various effect sizes <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). Probability estimates were based on 10,000 simulated experiments. Simulated data included no outliers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig11-figsupp1-v2.tif"/></fig></fig-group><fig-group><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Selective outlier removal.</title><p>False positive rate (FPR) as a function of the number <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of outlier rejection methods attempted, effect size <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study is <inline-formula><mml:math id="inf321"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf322"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>. Dashed lines gives the results for <italic>p</italic>-hacking whereas solid lines depict the results for researchers who act according to good scientific practice. Simulated data included 5% outliers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig12-v2.tif"/></fig><fig id="fig12s1" position="float" specific-use="child-fig"><label>Figure 12—figure supplement 1.</label><caption><title>Selective outlier removal.</title><p>False positive rate (FPR) as a function of the number <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of outlier rejection methods attempted, effect size <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study is <inline-formula><mml:math id="inf327"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf328"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>. Dashed lines give the results for <italic>p</italic>-hacking whereas solid lines depict the results for researchers who act according to good scientific practice. Simulated data included no outliers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig12-figsupp1-v2.tif"/></fig></fig-group><fig-group><fig id="fig13" position="float"><label>Figure 13.</label><caption><title>Selective outlier removal.</title><p>Replication rate (RR) as a function of the number <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of outlier rejection methods attempted, effect size <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study is <inline-formula><mml:math id="inf333"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf334"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>. Dashed lines gives the results for <italic>p</italic>-hacking whereas solid lines depict the results for researchers who act according to good scientific practice. Simulated data included 5% outliers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig13-v2.tif"/></fig><fig id="fig13s1" position="float" specific-use="child-fig"><label>Figure 13—figure supplement 1.</label><caption><title>Selective outlier removal.</title><p>Shrinkage of the replication rate (i.e., the difference between the solid and dashed lines in <xref ref-type="fig" rid="fig13">Figure 13</xref>) as a function of base rate <inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number <inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of outlier rejection methods attempted, effect size <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig13-figsupp1-v2.tif"/></fig><fig id="fig13s2" position="float" specific-use="child-fig"><label>Figure 13—figure supplement 2.</label><caption><title>Selective outlier removal.</title><p>Replication rate (RR) as a function of the number <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of outlier rejection methods attempted, effect size <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). The nominal <inline-formula><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level and power of the replication study are <inline-formula><mml:math id="inf343"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf344"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>. Dashed lines give the results for <italic>p</italic>-hacking whereas solid lines depict the results for researchers who act according to good scientific practice. Simulated data included no outliers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig13-figsupp2-v2.tif"/></fig><fig id="fig13s3" position="float" specific-use="child-fig"><label>Figure 13—figure supplement 3.</label><caption><title>Selective outlier removal.</title><p>Shrinkage of the replication rate (i.e., the difference between the solid and dashed lines in <xref ref-type="fig" rid="fig13s2">Figure 13—figure supplement 2</xref>) as a function of base rate <inline-formula><mml:math id="inf345"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, number <inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of outlier rejection methods attempted, effect size <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and nominal <inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level (0.5% or 5%). Simulated data included no outliers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-fig13-figsupp3-v2.tif"/></fig></fig-group><p>The present simulations assume that researchers try to remove outliers (i.e., apply a three-sigma rule) before they perform a <italic>t</italic>-test. Alternatively, however, researchers might first conduct a <italic>t</italic>-test on all data without excluding any extreme data points. If this test did not reveal statistical significance, they would then eliminate extreme data points before conducting one or more further <italic>t</italic>-tests. Under this alternative scenario, our simulations indicate that multiple analyses can produce notably better replication rates than the single analysis with all data points, apparently because the exclusion of outliers noticeably improves power relative to the analysis without exclusions. Moreover, the standard deviation of our outlier distribution was small compared to simulations of similar outlier scenarios (e.g., <xref ref-type="bibr" rid="bib5">Bakker and Wicherts, 2014</xref>; <xref ref-type="bibr" rid="bib76">Zimmerman, 1998</xref>). Our conclusion, of course, is that researchers should carefully examine their data for possible outliers before conducting any statistical tests, not that they should perform multiple tests with different outlier screening criteria—thereby inflating their Type 1 error rates—in order to maximize power.</p><p>Naturally, the story is different when no outliers are present in the data set. Making multiple attempts to remove outliers in this case would actually always increase the false positive rate and lower the replication rate (see <xref ref-type="fig" rid="fig11s1">Figure 11—figure supplement 1</xref>, <xref ref-type="fig" rid="fig12s1">Figure 12—figure supplement 1</xref>, <xref ref-type="fig" rid="fig13s2">Figure 13—figure supplement 2</xref>, and , <xref ref-type="fig" rid="fig13s3">Figure 13—figure supplement 3</xref> for a parallel simulation with no outliers). In fact, extreme data points in data sets without outliers appear to be especially diagnostic for testing the equality of locations between populations, as the Tukey pocket test demonstrates (<xref ref-type="bibr" rid="bib69">Tukey, 1959</xref>), so throwing away extreme observations that are not outliers reduces the information in the data set.</p></sec><sec id="s7"><title>General discussion</title><p>The ongoing reproducibility crisis concerns virtually all sciences and naturally prompts questions about how replication rates can be improved. Several measures have been advocated as ways to raise reproducibility, such as (a) preregistration of studies (<xref ref-type="bibr" rid="bib51">Nosek et al., 2018</xref>), (b) increasing the transparency of research by making data and research materials publicly available (e.g., <xref ref-type="bibr" rid="bib50">Nosek et al., 2015</xref>), (c) reducing <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib8">Benjamin et al., 2018</xref>), (d) increasing statistical power (<xref ref-type="bibr" rid="bib12">Button and Munafò, 2017</xref>), (e) improving statistical training (<xref ref-type="bibr" rid="bib2">Asendorpf et al., 2013</xref>), (f) adopting Bayesian approaches (<xref ref-type="bibr" rid="bib17">Etz and Vandekerckhove, 2016</xref>), and even (g) overhauling standard scientific methodology (<xref ref-type="bibr" rid="bib6">Barrett, 2020</xref>). The variety of these proposed measures demonstrates that replication failures can result from a multitude of causes that may come into play at various steps along the “entire analysis pipeline” (<xref ref-type="bibr" rid="bib39">Leek and Peng, 2015</xref>).</p><p>The present article focused on the statistical consequences of QRPs with respect to replication rate. The impacts of the various statistical factors affecting replication rate (i.e., <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, power, <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <italic>p</italic>-hacking) have typically been examined in isolation, which does not allow a complete assessment of their mutual influence and often leads to suggestions that are difficult to implement simultaneously, such as lowering <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and increasing power. In order to develop a better quantitative picture of the different influences on replicability, we modelled several apparently-frequent <italic>p</italic>-hacking strategies to examine their impacts on replication rate.</p><p>Our quantitative analyses suggest that <italic>p</italic>-hacking’s effects on replicability are unlikely to be massive. As noted previously, <italic>p</italic>-hacking inflates the effective Type 1 error rate (e.g., <xref ref-type="bibr" rid="bib61">Simmons et al., 2011</xref>), which tends to reduce replicability, but our analyses indicate that the corresponding increase in power (i.e., power inflation) substantially compensates for this inflation. Compared to the strong effect of the base rate on replicability, the reduction in replication rate caused by <italic>p</italic>-hacking appears rather small. Unsurprisingly, the impact is larger when <italic>p</italic>-hacking is more extensive (i.e., <inline-formula><mml:math id="inf353"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> rather than <inline-formula><mml:math id="inf354"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>). Moreover, <italic>p</italic>-hacking affects the replication rate most when the base rate is small. This makes sense, because <italic>p</italic>-hacking is harmful primarily when <inline-formula><mml:math id="inf355"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is true, which is more common with small base rates. The net influence of <italic>p</italic>-hacking on replicability appears to be smallest with small effect sizes, which is presumably the situation where <italic>p</italic>-hacking is most likely to be used. With small effects, the power increases associated with <italic>p</italic>-hacking are especially helpful for replicability. Finally and somewhat surprisingly, <italic>p</italic>-hacking tends to have a smaller effect on replicability when the nominal <inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level is 0.5% rather than 5%.</p><p>Of course, these conclusions are restricted to the limited extent of <italic>p</italic>-hacking (i.e., <inline-formula><mml:math id="inf357"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) that we examined, and more extensive <italic>p</italic>-hacking—or combining multiple <italic>p</italic>-hacking strategies—would presumably have larger effects on replicability. Nonetheless, we think that eight is a reasonable upper bound on the number of <italic>p</italic>-hacking attempts. The extent of <italic>p</italic>-hacking remains a controversial issue, with some arguing and providing evidence that ambitious <italic>p</italic>-hacking is too complicated and thus not plausible (<xref ref-type="bibr" rid="bib65">Simonsohn et al., 2015</xref>). Unfortunately, the exact extent of <italic>p</italic>-hacking is difficult to determine and might strongly depend on the field of research. For example, in areas with small effect sizes, <italic>p</italic>-hacking might be more extensive than in fields with medium or large effect sizes. But even without knowing the true <italic>p</italic>-hacking rates, our analyses are valuable because they clearly show that evidence of massive <italic>p</italic>-hacking is needed before one can conclude that it is a major contributor to the replication crisis. In addition, when estimating the actual effect of <italic>p</italic>-hacking on observed replication rates (e.g., <xref ref-type="bibr" rid="bib53">Open Science Collaboration, 2015</xref>), it is important to note that the effects shown in our figures are <italic>upper bounds</italic> that would only be approached if nearly all researchers employed these <italic>p</italic>-hacking methods. If only 10% of researchers use these methods, then the overall effects on empirical replication rates would be only 10% as large as those suggested by our model. Even the highest estimates of the prevalence of QRPs are only approximately 50% (<xref ref-type="bibr" rid="bib36">John et al., 2012</xref>), and these may be serious overestimates (<xref ref-type="bibr" rid="bib20">Fiedler and Schwarz, 2016</xref>).</p><p>Our quantitative analysis also assumed high-powered replication studies, that is, <inline-formula><mml:math id="inf358"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:math></inline-formula>%. This replication power was chosen as the best-scenario value close to the average replication power claimed by the <xref ref-type="bibr" rid="bib53">Open Science Collaboration, 2015</xref>. However, the power of the replication studies might not have been as high as they claimed. In particular, selective reporting of significant studies tends to overestimate true effect sizes, especially when these are small (<xref ref-type="bibr" rid="bib31">Hedges, 1984</xref>; <xref ref-type="bibr" rid="bib38">Lane and Dunlap, 1978</xref>; <xref ref-type="bibr" rid="bib71">Ulrich et al., 2018</xref>), so the effect size estimates used in the power computations of the <xref ref-type="bibr" rid="bib53">Open Science Collaboration, 2015</xref> may have been too large. As a consequence, their actual power levels may have been lower than the estimated 90%. To check whether our conclusions would still be valid with lower replication power, we reran our computations using a replication power of 50%. These computations revealed that <italic>p</italic>-hacking would even be slightly less harmful to replication rates with 50% rather than 90% replication power.</p><p>Our analyses were based on groups size of <inline-formula><mml:math id="inf359"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib42">Marszalek et al., 2011</xref>). Recent meta-analyses, however, have indicated an increase in sample size especially in social-personality research (<xref ref-type="bibr" rid="bib21">Fraley and Vazire, 2014</xref>; <xref ref-type="bibr" rid="bib59">Sassenberg and Ditrich, 2019</xref>). Therefore, one may ask whether our main conclusions still apply for larger samples. First, as discussed in the introduction, the replication rate increases gradually with base rate whether the statistical power of the original study is low or even 100%. Therefore, even large sample studies cannot avoid low replication rates when the base rate is small. Second, because the statistical power increases with both sample size and effect size, increasing the effect size mimics what would happen if one increases the sample size. In fact, additional computations with larger samples (i.e., group size of 50) revealed no meaningful changes that would alter our conclusions.</p><p>Another limitation concerning our conclusions is that our list of <italic>p</italic>-hacking strategies was not exhaustive. For example, we did not examine the possibility that researchers might try several covariates until a significant result is obtained (e.g., <xref ref-type="bibr" rid="bib64">Simonsohn et al., 2014b</xref>). As another example, suppose a researcher conducts a multi-factor analysis of variance (ANOVA) that invites the examination of multiple main effects and interactions, any one of which might be cherry picked as a “finding” in the absence of a specific a priori hypothesis. For instance, a three-factorial ANOVA allows the examination of seven potential effects (i.e., three main effects and four interactions). Assuming that all seven sources and their error terms are independent, the probability of at least one significant result when <inline-formula><mml:math id="inf360"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> holds in all cases is <inline-formula><mml:math id="inf361"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>—about 30% with <inline-formula><mml:math id="inf362"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>—which would simply emulate the multiple studies scenario that we analysed in this article. Thus, analyses similar to the present ones would be needed to analyze the consequences of these other strategies, but it would be surprising if the results were drastically different.</p><p>We supplemented the analyses reported in this manuscript by two further analyses (see Appendix 2), each of which approached the replication issue from a different angle.One supplementary analysis assessed the effect of <italic>p</italic>-hacking on power while controlling for the overall Type 1 error rate. The outcome of this analysis demonstrated that some <italic>p</italic>-hacking strategies can actually produce higher statistical power than good practice at each level of Type 1 error. This superiority can be explained by the fact that <italic>p</italic>-hacking sometimes involves the collection of additional data (e.g., as with data peeking or measuring additional variables), and in these cases the additional data can cause statistical power to increase faster than the Type 1 error rate. The other supplementary analysis compared the overall research payoff associated with good practice versus data peeking using the payoff model of <xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>. This analysis showed that the expected total payoff can actually be larger with data peeking than with good practice, evidently because data peeking tends to make more efficient use of limited sample sizes when true effects are common.</p><p>If <italic>p</italic>-hacking is not a major contributor to low replicability, then what is? In keeping with previous analyses (<xref ref-type="bibr" rid="bib15">Dreber et al., 2015</xref>; <xref ref-type="bibr" rid="bib37">Johnson et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Miller, 2009</xref>; <xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>; <xref ref-type="bibr" rid="bib74">Wilson and Wixted, 2018</xref>), our results suggest that low base rates of true effects—not too-large <inline-formula><mml:math id="inf363"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> levels, too-low power, or <italic>p</italic>-hacking—are most likely to be the major causes of poor replicability, so researchers concerned about replicability should pay special attention to the issue of base rates. Clearly, low base rates can lead to disappointingly low replication rates even in the absence of <italic>p</italic>-hacking (e.g., <xref ref-type="fig" rid="fig5">Figures 5</xref>, <xref ref-type="fig" rid="fig8">8</xref>, <xref ref-type="fig" rid="fig10">10</xref> and <xref ref-type="fig" rid="fig13">13</xref>, “good practice”). It follows from our analyses that research fields with inherently low base rates simply cannot improve their replication rates much by focusing exclusively on methodological issues. There are multiple lines of evidence that base rates are low in many fields (particularly those with low replication rates; e.g., <xref ref-type="bibr" rid="bib15">Dreber et al., 2015</xref>; <xref ref-type="bibr" rid="bib37">Johnson et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>; <xref ref-type="bibr" rid="bib47">Miller and Ulrich, 2019</xref>; <xref ref-type="bibr" rid="bib74">Wilson and Wixted, 2018</xref>), and it will be especially challenging to increase replicability in those fields.</p><p>In principle, researchers can increase base rates by testing hypotheses that are deduced from plausible, evidence-based theories rather than by looking for effects that would be particularly surprising and newsworthy. However, practical constraints may often make it difficult to increase base rates, especially in research areas where a deeper theoretical understanding is lacking (e.g., in the search for an effective vaccine against an infectious disease). In such areas, a haphazard approach to hypothesis selection may be the only option, which naturally implies a low base rate. In combination with publication bias and <italic>p</italic>-hacking, this low base rate may make it particularly challenging to establish scientific claims as facts (<xref ref-type="bibr" rid="bib49">Nissen et al., 2016</xref>).</p><p>Looking beyond replication rates, meta-scientists should consider exactly what measure of research productivity they want to optimize. For example, if the goal is to minimize false positives, they should use small <inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> levels and eliminate <italic>p</italic>-hacking. If the goal is to minimize false negatives, however, they should do exactly the opposite. The major problem in statistical decision making is that one cannot maximize all of the desirable goals at the same time. Thus, focusing on only one goal—even that of maximizing replicability—will not yield an optimal research strategy. Identifying the optimal strategy requires considering all of the goals simultaneously and integrating them into a composite measure of research productivity. One way to do this is to analyze the probabilities and payoffs for a set of possible research outcomes and to identify research parameters maximizing the expected research payoff (<xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>). This analysis must also take into account how limited research resources would be used under different strategies. Other things being equal, for example, fewer resources would be needed for replication studies with <inline-formula><mml:math id="inf365"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:math></inline-formula> than with <inline-formula><mml:math id="inf366"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, simply because initial studies would produce fewer significant outcomes as candidates for replication.</p></sec><sec id="s8"><title>Conclusion</title><p>We modelled different causes (alpha level, power, base rate of true effects, QRPs) of low replication rates within a general statistical framework. Our analyses indicate that a low rate of true effects—not <italic>p</italic>-hacking—is mainly responsible for low replication rates—a point that is often under-appreciated in current debates about how to improve replicability. Of course, we do not wish to transmit the message that <italic>p</italic>-hacking is tolerable just because it might increase power when a researcher examines a true effect. As has often been discussed previously (<xref ref-type="bibr" rid="bib61">Simmons et al., 2011</xref>), <italic>p</italic>-hacking should always be avoided because it inflates Type 1 error rates above stated levels and thus undermines scientific progress. Rather, our message is that scientists and others concerned about low replication rates should look beyond <italic>p</italic>-hacking for its primary causes. The current analyses suggest that even massive campaigns against <italic>p</italic>-hacking (e.g., researcher education, pre-registration initiatives) may produce only modest improvements in replicability. To make large changes in this important scientific measure, it will likely be necessary to address other aspects of the scientific culture. Unfortunately, that may not happen if attention and blame are focused too narrowly on <italic>p</italic>-hacking as a major cause of the current problems in this area.</p></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Eric-Jan Wagenmakers and all reviewers for helpful comments.</p></ack><sec id="s9" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Writing - original draft</p></fn></fn-group></sec><sec id="s10" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>Data peeking.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58237-code1-v2.m.zip"/></supplementary-material><supplementary-material id="scode2"><label>Source code 2.</label><caption><title>Demo.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58237-code2-v2.m.zip"/></supplementary-material><supplementary-material id="scode3"><label>Source code 3.</label><caption><title>Failing to report.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58237-code3-v2.m.zip"/></supplementary-material><supplementary-material id="scode4"><label>Source code 4.</label><caption><title>Outlier rejection.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58237-code4-v2.m.zip"/></supplementary-material><supplementary-material id="scode5"><label>Source code 5.</label><caption><title>Selective reporting of significant studies.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-58237-code5-v2.m.zip"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-58237-transrepform-v2.docx"/></supplementary-material></sec><sec id="s11" sec-type="data-availability"><title>Data availability</title><p>There are no empirical data because mathematical modelling was employed to assess the impact of various factors on the replication of significant results.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armitage</surname> <given-names>P</given-names></name><name><surname>McPherson</surname> <given-names>CK</given-names></name><name><surname>Rowe</surname> <given-names>BC</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Repeated significance tests on accumulating data</article-title><source>Journal of the Royal Statistical Society. Series A</source><volume>132</volume><fpage>235</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.2307/2343787</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asendorpf</surname> <given-names>JB</given-names></name><name><surname>Conner</surname> <given-names>M</given-names></name><name><surname>De Fruyt</surname> <given-names>F</given-names></name><name><surname>De Houwer</surname> <given-names>J</given-names></name><name><surname>Denissen</surname> <given-names>JJA</given-names></name><name><surname>Fiedler</surname> <given-names>K</given-names></name><name><surname>Fiedler</surname> <given-names>S</given-names></name><name><surname>Funder</surname> <given-names>DC</given-names></name><name><surname>Kliegl</surname> <given-names>R</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Perugini</surname> <given-names>M</given-names></name><name><surname>Roberts</surname> <given-names>BW</given-names></name><name><surname>Schmitt</surname> <given-names>M</given-names></name><name><surname>van Aken</surname> <given-names>MAG</given-names></name><name><surname>Weber</surname> <given-names>H</given-names></name><name><surname>Wicherts</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Recommendations for increasing replicability in psychology</article-title><source>European Journal of Personality</source><volume>27</volume><fpage>108</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1002/per.1919</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname> <given-names>M</given-names></name><name><surname>Penny</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Is there a reproducibility crisis?</article-title><source>Nature</source><volume>533</volume><fpage>452</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1038/533452a</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakker</surname> <given-names>M</given-names></name><name><surname>van Dijk</surname> <given-names>A</given-names></name><name><surname>Wicherts</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The rules of the game called psychological science</article-title><source>Perspectives on Psychological Science</source><volume>7</volume><fpage>543</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1177/1745691612459060</pub-id><pub-id pub-id-type="pmid">26168111</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakker</surname> <given-names>M</given-names></name><name><surname>Wicherts</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Outlier removal, sum scores, and the inflation of the type I error rate in independent samples t tests: the power of alternatives and recommendations</article-title><source>Psychological Methods</source><volume>19</volume><fpage>409</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1037/met0000014</pub-id><pub-id pub-id-type="pmid">24773354</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Forward into the past</article-title><source>APA Observer</source><volume>33</volume><fpage>5</fpage><lpage>7</lpage></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Begley</surname> <given-names>CG</given-names></name><name><surname>Ellis</surname> <given-names>LM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Raise standards for preclinical cancer research</article-title><source>Nature</source><volume>483</volume><fpage>531</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/483531a</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname> <given-names>DJ</given-names></name><name><surname>Berger</surname> <given-names>JO</given-names></name><name><surname>Johannesson</surname> <given-names>M</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Berk</surname> <given-names>R</given-names></name><name><surname>Bollen</surname> <given-names>KA</given-names></name><name><surname>Brembs</surname> <given-names>B</given-names></name><name><surname>Brown</surname> <given-names>L</given-names></name><name><surname>Camerer</surname> <given-names>C</given-names></name><name><surname>Cesarini</surname> <given-names>D</given-names></name><name><surname>Chambers</surname> <given-names>CD</given-names></name><name><surname>Clyde</surname> <given-names>M</given-names></name><name><surname>Cook</surname> <given-names>TD</given-names></name><name><surname>De Boeck</surname> <given-names>P</given-names></name><name><surname>Dienes</surname> <given-names>Z</given-names></name><name><surname>Dreber</surname> <given-names>A</given-names></name><name><surname>Easwaran</surname> <given-names>K</given-names></name><name><surname>Efferson</surname> <given-names>C</given-names></name><name><surname>Fehr</surname> <given-names>E</given-names></name><name><surname>Fidler</surname> <given-names>F</given-names></name><name><surname>Field</surname> <given-names>AP</given-names></name><name><surname>Forster</surname> <given-names>M</given-names></name><name><surname>George</surname> <given-names>EI</given-names></name><name><surname>Gonzalez</surname> <given-names>R</given-names></name><name><surname>Goodman</surname> <given-names>S</given-names></name><name><surname>Green</surname> <given-names>E</given-names></name><name><surname>Green</surname> <given-names>DP</given-names></name><name><surname>Greenwald</surname> <given-names>AG</given-names></name><name><surname>Hadfield</surname> <given-names>JD</given-names></name><name><surname>Hedges</surname> <given-names>LV</given-names></name><name><surname>Held</surname> <given-names>L</given-names></name><name><surname>Hua Ho</surname> <given-names>T</given-names></name><name><surname>Hoijtink</surname> <given-names>H</given-names></name><name><surname>Hruschka</surname> <given-names>DJ</given-names></name><name><surname>Imai</surname> <given-names>K</given-names></name><name><surname>Imbens</surname> <given-names>G</given-names></name><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name><name><surname>Jeon</surname> <given-names>M</given-names></name><name><surname>Jones</surname> <given-names>JH</given-names></name><name><surname>Kirchler</surname> <given-names>M</given-names></name><name><surname>Laibson</surname> <given-names>D</given-names></name><name><surname>List</surname> <given-names>J</given-names></name><name><surname>Little</surname> <given-names>R</given-names></name><name><surname>Lupia</surname> <given-names>A</given-names></name><name><surname>Machery</surname> <given-names>E</given-names></name><name><surname>Maxwell</surname> <given-names>SE</given-names></name><name><surname>McCarthy</surname> <given-names>M</given-names></name><name><surname>Moore</surname> <given-names>DA</given-names></name><name><surname>Morgan</surname> <given-names>SL</given-names></name><name><surname>Munafó</surname> <given-names>M</given-names></name><name><surname>Nakagawa</surname> <given-names>S</given-names></name><name><surname>Nyhan</surname> <given-names>B</given-names></name><name><surname>Parker</surname> <given-names>TH</given-names></name><name><surname>Pericchi</surname> <given-names>L</given-names></name><name><surname>Perugini</surname> <given-names>M</given-names></name><name><surname>Rouder</surname> <given-names>J</given-names></name><name><surname>Rousseau</surname> <given-names>J</given-names></name><name><surname>Savalei</surname> <given-names>V</given-names></name><name><surname>Schönbrodt</surname> <given-names>FD</given-names></name><name><surname>Sellke</surname> <given-names>T</given-names></name><name><surname>Sinclair</surname> <given-names>B</given-names></name><name><surname>Tingley</surname> <given-names>D</given-names></name><name><surname>Van Zandt</surname> <given-names>T</given-names></name><name><surname>Vazire</surname> <given-names>S</given-names></name><name><surname>Watts</surname> <given-names>DJ</given-names></name><name><surname>Winship</surname> <given-names>C</given-names></name><name><surname>Wolpert</surname> <given-names>RL</given-names></name><name><surname>Xie</surname> <given-names>Y</given-names></name><name><surname>Young</surname> <given-names>C</given-names></name><name><surname>Zinman</surname> <given-names>J</given-names></name><name><surname>Johnson</surname> <given-names>VE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Redefine statistical significance</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>6</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0189-z</pub-id><pub-id pub-id-type="pmid">30980045</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosco</surname> <given-names>FA</given-names></name><name><surname>Aguinis</surname> <given-names>H</given-names></name><name><surname>Singh</surname> <given-names>K</given-names></name><name><surname>Field</surname> <given-names>JG</given-names></name><name><surname>Pierce</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Correlational effect size benchmarks</article-title><source>Journal of Applied Psychology</source><volume>100</volume><fpage>431</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1037/a0038047</pub-id><pub-id pub-id-type="pmid">25314367</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bunge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1967">1967</year><source>Scientific Research II: The Search for Truth</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-48138-3</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Button</surname> <given-names>KS</given-names></name><name><surname>Ioannidis</surname> <given-names>JP</given-names></name><name><surname>Mokrysz</surname> <given-names>C</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Flint</surname> <given-names>J</given-names></name><name><surname>Robinson</surname> <given-names>ES</given-names></name><name><surname>Munafò</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Power failure: why small sample size undermines the reliability of neuroscience</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>365</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1038/nrn3475</pub-id><pub-id pub-id-type="pmid">23571845</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Button</surname> <given-names>KS</given-names></name><name><surname>Munafò</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Powering Reproducible Research</chapter-title><person-group person-group-type="editor"><name><surname>Lilienfeld</surname> <given-names>S. O</given-names></name><name><surname>Waldman</surname> <given-names>I. D</given-names></name></person-group><source>Psychological Science Under Scrutiny: Recent Challenges and Proposed Solutions</source><publisher-name>Wiley Online Library</publisher-name><fpage>22</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1002/9781119095910.ch2</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camerer</surname> <given-names>CF</given-names></name><name><surname>Dreber</surname> <given-names>A</given-names></name><name><surname>Holzmeister</surname> <given-names>F</given-names></name><name><surname>Ho</surname> <given-names>TH</given-names></name><name><surname>Huber</surname> <given-names>J</given-names></name><name><surname>Johannesson</surname> <given-names>M</given-names></name><name><surname>Kirchler</surname> <given-names>M</given-names></name><name><surname>Nave</surname> <given-names>G</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Pfeiffer</surname> <given-names>T</given-names></name><name><surname>Altmejd</surname> <given-names>A</given-names></name><name><surname>Buttrick</surname> <given-names>N</given-names></name><name><surname>Chan</surname> <given-names>T</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Forsell</surname> <given-names>E</given-names></name><name><surname>Gampa</surname> <given-names>A</given-names></name><name><surname>Heikensten</surname> <given-names>E</given-names></name><name><surname>Hummer</surname> <given-names>L</given-names></name><name><surname>Imai</surname> <given-names>T</given-names></name><name><surname>Isaksson</surname> <given-names>S</given-names></name><name><surname>Manfredi</surname> <given-names>D</given-names></name><name><surname>Rose</surname> <given-names>J</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Wu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>637</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0399-z</pub-id><pub-id pub-id-type="pmid">31346273</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Carnap</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1995">1995</year><source>An Introduction to the Philosophy of Science</source><publisher-name>General Publishing Company</publisher-name></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dreber</surname> <given-names>A</given-names></name><name><surname>Pfeiffer</surname> <given-names>T</given-names></name><name><surname>Almenberg</surname> <given-names>J</given-names></name><name><surname>Isaksson</surname> <given-names>S</given-names></name><name><surname>Wilson</surname> <given-names>B</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Johannesson</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Using prediction markets to estimate the reproducibility of scientific research</article-title><source>PNAS</source><volume>112</volume><fpage>15343</fpage><lpage>15347</lpage><pub-id pub-id-type="doi">10.1073/pnas.1516179112</pub-id><pub-id pub-id-type="pmid">26553988</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname> <given-names>TM</given-names></name><name><surname>Iorns</surname> <given-names>E</given-names></name><name><surname>Gunn</surname> <given-names>W</given-names></name><name><surname>Tan</surname> <given-names>FE</given-names></name><name><surname>Lomax</surname> <given-names>J</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An open investigation of the reproducibility of cancer biology research</article-title><source>eLife</source><volume>3</volume><elocation-id>e04333</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04333</pub-id><pub-id pub-id-type="pmid">25490932</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etz</surname> <given-names>A</given-names></name><name><surname>Vandekerckhove</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A Bayesian perspective on the Reproducibility Project: Psychology</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0149794</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0149794</pub-id><pub-id pub-id-type="pmid">26919473</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanelli</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e5738</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0005738</pub-id><pub-id pub-id-type="pmid">19478950</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiedler</surname> <given-names>K</given-names></name><name><surname>Kutzner</surname> <given-names>F</given-names></name><name><surname>Krueger</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The long way from α-error control to validity proper: problems with a short-sighted false-positive debate</article-title><source>Perspectives on Psychological Science</source><volume>7</volume><fpage>661</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1177/1745691612462587</pub-id><pub-id pub-id-type="pmid">26168128</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiedler</surname> <given-names>K</given-names></name><name><surname>Schwarz</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Questionable research practices revisited</article-title><source>Social Psychological and Personality Science</source><volume>7</volume><fpage>45</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1177/1948550615612150</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fraley</surname> <given-names>RC</given-names></name><name><surname>Vazire</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The N-pact factor: evaluating the quality of empirical journals with respect to sample size and statistical power</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e109019</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0109019</pub-id><pub-id pub-id-type="pmid">25296159</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Publication bias and the failure of replication in experimental psychology</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>19</volume><fpage>975</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.3758/s13423-012-0322-y</pub-id><pub-id pub-id-type="pmid">23055145</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Too good to be true: Publication bias in two prominent studies from experimental psychology</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>19</volume><fpage>151</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.3758/s13423-012-0227-9</pub-id><pub-id pub-id-type="pmid">22351589</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The frequency of excess success for articles in <italic>Psychological Science</italic></article-title><source>Psychonomic Bulletin &amp; Review</source><volume>21</volume><fpage>1180</fpage><lpage>1187</lpage><pub-id pub-id-type="doi">10.3758/s13423-014-0601-x</pub-id><pub-id pub-id-type="pmid">24638826</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname> <given-names>G</given-names></name><name><surname>Tanzman</surname> <given-names>J</given-names></name><name><surname>Matthews</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Excess success for psychology articles in the journal <italic>Science</italic></article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e114255</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0114255</pub-id><pub-id pub-id-type="pmid">25474317</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frick</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A better stopping rule for conventional statistical tests</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><volume>30</volume><fpage>690</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.3758/BF03209488</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Numerical computation of multivariate normal probabilities</article-title><source>Journal of Computational and Graphical Statistics</source><volume>1</volume><fpage>141</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-33507-0_13</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genz</surname> <given-names>A</given-names></name><name><surname>Bretz</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Numerical computation of multivariate <italic>t</italic> -probabilities with application to power calculation of multiple contrasts</article-title><source>Journal of Statistical Computation and Simulation</source><volume>63</volume><fpage>103</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1080/00949659908811962</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genz</surname> <given-names>A</given-names></name><name><surname>Bretz</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Comparison of methods for the computation of multivariate <italic>t</italic> probabilities</article-title><source>Journal of Computational and Graphical Statistics</source><volume>11</volume><fpage>950</fpage><lpage>971</lpage><pub-id pub-id-type="doi">10.1198/106186002394</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Scientific misconduct</article-title><source>Annual Review of Psychology</source><volume>67</volume><fpage>693</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122414-033437</pub-id><pub-id pub-id-type="pmid">26273897</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hedges</surname> <given-names>LV</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Estimation of effect size under nonrandom sampling: the effects of censoring studies yielding statistically insignificant mean differences</article-title><source>Journal of Educational Statistics</source><volume>9</volume><fpage>61</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.3102/10769986009001061</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutton</surname> <given-names>JL</given-names></name><name><surname>Williamson</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Bias in meta-analysis due to outcome variable selection within studies</article-title><source>Journal of the Royal Statistical Society: Series C</source><volume>49</volume><fpage>359</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1111/1467-9876.00197</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2005">2005a</year><article-title>Contradicted and initially stronger effects in highly cited clinical research</article-title><source>JAMA</source><volume>294</volume><fpage>218</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1001/jama.294.2.218</pub-id><pub-id pub-id-type="pmid">16014596</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2005">2005b</year><article-title>Why most published research findings are false</article-title><source>PLOS Medicine</source><volume>2</volume><elocation-id>e124</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.0020124</pub-id><pub-id pub-id-type="pmid">16060722</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name><name><surname>Trikalinos</surname> <given-names>TA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>An exploratory test for an excess of significant findings</article-title><source>Clinical Trials: Journal of the Society for Clinical Trials</source><volume>4</volume><fpage>245</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1177/1740774507079441</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>John</surname> <given-names>LK</given-names></name><name><surname>Loewenstein</surname> <given-names>G</given-names></name><name><surname>Prelec</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Measuring the prevalence of questionable research practices with incentives for truth telling</article-title><source>Psychological Science</source><volume>23</volume><fpage>524</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1177/0956797611430953</pub-id><pub-id pub-id-type="pmid">22508865</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>VE</given-names></name><name><surname>Payne</surname> <given-names>RD</given-names></name><name><surname>Wang</surname> <given-names>T</given-names></name><name><surname>Asher</surname> <given-names>A</given-names></name><name><surname>Mandal</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>On the reproducibility of psychological science</article-title><source>Journal of the American Statistical Association</source><volume>112</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1080/01621459.2016.1240079</pub-id><pub-id pub-id-type="pmid">29861517</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lane</surname> <given-names>DM</given-names></name><name><surname>Dunlap</surname> <given-names>WP</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Estimating effect size: bias resulting from the significance criterion in editorial decisions</article-title><source>British Journal of Mathematical and Statistical Psychology</source><volume>31</volume><fpage>107</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8317.1978.tb00578.x</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leek</surname> <given-names>JT</given-names></name><name><surname>Peng</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Statistics: p values are just the tip of the iceberg</article-title><source>Nature</source><volume>520</volume><elocation-id>612</elocation-id><pub-id pub-id-type="doi">10.1038/520612a</pub-id><pub-id pub-id-type="pmid">25925460</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewandowsky</surname> <given-names>S</given-names></name><name><surname>Oberauer</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Low replicability can support robust and efficient science</article-title><source>Nature Communications</source><volume>11</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-14203-0</pub-id><pub-id pub-id-type="pmid">31953411</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lilienfeld</surname> <given-names>SO</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Psychology's replication crisis and the grant culture: Righting the ship</article-title><source>Perspectives on Psychological Science</source><volume>12</volume><fpage>660</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1177/1745691616687745</pub-id><pub-id pub-id-type="pmid">28727961</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marszalek</surname> <given-names>JM</given-names></name><name><surname>Barber</surname> <given-names>C</given-names></name><name><surname>Kohlhart</surname> <given-names>J</given-names></name><name><surname>Holmes</surname> <given-names>CB</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sample size in psychological research over the past 30 years</article-title><source>Perceptual and Motor Skills</source><volume>112</volume><fpage>331</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.2466/03.11.PMS.112.2.331-348</pub-id><pub-id pub-id-type="pmid">21667745</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCarroll</surname> <given-names>D</given-names></name><name><surname>Crays</surname> <given-names>N</given-names></name><name><surname>Dunlap</surname> <given-names>WP</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Sequential ANOVAs and type I error rates</article-title><source>Educational and Psychological Measurement</source><volume>52</volume><fpage>387</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1177/0013164492052002014</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>What is the probability of replicating a statistically significant effect?</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>617</fpage><lpage>640</lpage><pub-id pub-id-type="doi">10.3758/PBR.16.4.617</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>J</given-names></name><name><surname>Schwarz</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Aggregate and individual replication probability within an explicit model of the research process</article-title><source>Psychological Methods</source><volume>16</volume><fpage>337</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1037/a0023347</pub-id><pub-id pub-id-type="pmid">21534683</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>J</given-names></name><name><surname>Ulrich</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Optimizing research payoff</article-title><source>Perspectives on Psychological Science</source><volume>11</volume><fpage>664</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1177/1745691616649170</pub-id><pub-id pub-id-type="pmid">27694463</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>J</given-names></name><name><surname>Ulrich</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The quest for an optimal alpha</article-title><source>PLOS ONE</source><volume>14</volume><elocation-id>e0208631</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0208631</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mood</surname> <given-names>AK</given-names></name><name><surname>Graybill</surname> <given-names>FA</given-names></name><name><surname>Boes</surname> <given-names>DC</given-names></name></person-group><year iso-8601-date="1974">1974</year><source>Introduction to the Theory of Statistics (3rd Edition)</source><publisher-name>McGraw-Hill</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nissen</surname> <given-names>SB</given-names></name><name><surname>Magidson</surname> <given-names>T</given-names></name><name><surname>Gross</surname> <given-names>K</given-names></name><name><surname>Bergstrom</surname> <given-names>CT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Publication bias and the canonization of false facts</article-title><source>eLife</source><volume>5</volume><elocation-id>e21451</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.21451</pub-id><pub-id pub-id-type="pmid">27995896</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Alter</surname> <given-names>G</given-names></name><name><surname>Banks</surname> <given-names>GC</given-names></name><name><surname>Borsboom</surname> <given-names>D</given-names></name><name><surname>Bowman</surname> <given-names>SD</given-names></name><name><surname>Breckler</surname> <given-names>SJ</given-names></name><name><surname>Buck</surname> <given-names>S</given-names></name><name><surname>Chambers</surname> <given-names>CD</given-names></name><name><surname>Chin</surname> <given-names>G</given-names></name><name><surname>Christensen</surname> <given-names>G</given-names></name><name><surname>Contestabile</surname> <given-names>M</given-names></name><name><surname>Dafoe</surname> <given-names>A</given-names></name><name><surname>Eich</surname> <given-names>E</given-names></name><name><surname>Freese</surname> <given-names>J</given-names></name><name><surname>Glennerster</surname> <given-names>R</given-names></name><name><surname>Goroff</surname> <given-names>D</given-names></name><name><surname>Green</surname> <given-names>DP</given-names></name><name><surname>Hesse</surname> <given-names>B</given-names></name><name><surname>Humphreys</surname> <given-names>M</given-names></name><name><surname>Ishiyama</surname> <given-names>J</given-names></name><name><surname>Karlan</surname> <given-names>D</given-names></name><name><surname>Kraut</surname> <given-names>A</given-names></name><name><surname>Lupia</surname> <given-names>A</given-names></name><name><surname>Mabry</surname> <given-names>P</given-names></name><name><surname>Madon</surname> <given-names>TA</given-names></name><name><surname>Malhotra</surname> <given-names>N</given-names></name><name><surname>Mayo-Wilson</surname> <given-names>E</given-names></name><name><surname>McNutt</surname> <given-names>M</given-names></name><name><surname>Miguel</surname> <given-names>E</given-names></name><name><surname>Paluck</surname> <given-names>EL</given-names></name><name><surname>Simonsohn</surname> <given-names>U</given-names></name><name><surname>Soderberg</surname> <given-names>C</given-names></name><name><surname>Spellman</surname> <given-names>BA</given-names></name><name><surname>Turitto</surname> <given-names>J</given-names></name><name><surname>VandenBos</surname> <given-names>G</given-names></name><name><surname>Vazire</surname> <given-names>S</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Wilson</surname> <given-names>R</given-names></name><name><surname>Yarkoni</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Promoting an open research culture</article-title><source>Science</source><volume>348</volume><fpage>1422</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1126/science.aab2374</pub-id><pub-id pub-id-type="pmid">26113702</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Ebersole</surname> <given-names>CR</given-names></name><name><surname>DeHaven</surname> <given-names>AC</given-names></name><name><surname>Mellor</surname> <given-names>DT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The preregistration revolution</article-title><source>PNAS</source><volume>115</volume><fpage>2600</fpage><lpage>2606</lpage><pub-id pub-id-type="doi">10.1073/pnas.1708274114</pub-id><pub-id pub-id-type="pmid">29531091</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberauer</surname> <given-names>K</given-names></name><name><surname>Lewandowsky</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Addressing the theory crisis in psychology</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>26</volume><fpage>1596</fpage><lpage>1618</lpage><pub-id pub-id-type="doi">10.3758/s13423-019-01645-2</pub-id><pub-id pub-id-type="pmid">31515732</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>aac4716</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id><pub-id pub-id-type="pmid">26315443</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pashler</surname> <given-names>H</given-names></name><name><surname>Harris</surname> <given-names>CR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Is the replicability crisis overblown? Three arguments examined</article-title><source>Perspectives on Psychological Science</source><volume>7</volume><fpage>531</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1177/1745691612463401</pub-id><pub-id pub-id-type="pmid">26168109</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Popper</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>The Logic of Scientific Discovery</source><publisher-name>Routledge Classics</publisher-name></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinz</surname> <given-names>F</given-names></name><name><surname>Schlange</surname> <given-names>T</given-names></name><name><surname>Asadullah</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Believe it or not: how much can we rely on published data on potential drug targets?</article-title><source>Nature Reviews Drug Discovery</source><volume>10</volume><fpage>712</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1038/nrd3439-c1</pub-id><pub-id pub-id-type="pmid">21892149</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Proschan</surname> <given-names>MA</given-names></name><name><surname>Lan</surname> <given-names>GKK</given-names></name><name><surname>Wittes</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Statistical Monitoring of Clinical Trials: A Unified Approach</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-44970-8</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenthal</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>The file drawer problem and tolerance for null results</article-title><source>Psychological Bulletin</source><volume>86</volume><fpage>638</fpage><lpage>641</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.86.3.638</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sassenberg</surname> <given-names>K</given-names></name><name><surname>Ditrich</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Research in social psychology changed between 2011 and 2016: larger sample sizes, more self-report measures, and more online studies</article-title><source>Advances in Methods and Practices in Psychological Science</source><volume>2</volume><fpage>107</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1177/2515245919838781</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname> <given-names>FL</given-names></name><name><surname>Oh</surname> <given-names>I-S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The crisis of confidence in research findings in psychology: Is lack of replication the real problem? Or is it something else?</article-title><source>Archives of Scientific Psychology</source><volume>4</volume><fpage>32</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1037/arc0000029</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname> <given-names>JP</given-names></name><name><surname>Nelson</surname> <given-names>LD</given-names></name><name><surname>Simonsohn</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant</article-title><source>Psychological Science</source><volume>22</volume><fpage>1359</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1177/0956797611417632</pub-id><pub-id pub-id-type="pmid">22006061</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonsohn</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Posterior-hacking: Selective reporting invalidates Bayesian results also</article-title><source>SSRN Electronic Journal</source><volume>1</volume><elocation-id>2374040</elocation-id><pub-id pub-id-type="doi">10.2139/ssrn.2374040</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonsohn</surname> <given-names>U</given-names></name><name><surname>Nelson</surname> <given-names>LD</given-names></name><name><surname>Simmons</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>p-Curve: A key to the file-drawer</article-title><source>Journal of Experimental Psychology: General</source><volume>143</volume><fpage>534</fpage><lpage>547</lpage><pub-id pub-id-type="doi">10.1037/a0033242</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonsohn</surname> <given-names>U</given-names></name><name><surname>Nelson</surname> <given-names>LD</given-names></name><name><surname>Simmons</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>p-Curve and effect size: Correcting for publication bias using only significant results</article-title><source>Perspectives on Psychological Science</source><volume>9</volume><fpage>666</fpage><lpage>681</lpage><pub-id pub-id-type="doi">10.1177/1745691614553988</pub-id><pub-id pub-id-type="pmid">26186117</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonsohn</surname> <given-names>U</given-names></name><name><surname>Simmons</surname> <given-names>JP</given-names></name><name><surname>Nelson</surname> <given-names>LD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Better p-curves: making p-curve analysis more robust to errors, fraud, and ambitious p-hacking, a reply to Ulrich and Miller (2015)</article-title><source>Journal of Experimental Psychology: General</source><volume>144</volume><fpage>1146</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1037/xge0000104</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanley</surname> <given-names>TD</given-names></name><name><surname>Carter</surname> <given-names>EC</given-names></name><name><surname>Doucouliagos</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What meta-analyses reveal about the replicability of psychological research</article-title><source>Psychological Bulletin</source><volume>144</volume><fpage>1325</fpage><lpage>1346</lpage><pub-id pub-id-type="doi">10.1037/bul0000169</pub-id><pub-id pub-id-type="pmid">30321017</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stroebe</surname> <given-names>W</given-names></name><name><surname>Postmes</surname> <given-names>T</given-names></name><name><surname>Spears</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Scientific misconduct and the myth of self-correction in science</article-title><source>Perspectives on Psychological Science</source><volume>7</volume><fpage>670</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.1177/1745691612460687</pub-id><pub-id pub-id-type="pmid">26168129</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strube</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>SNOOP: A program for demonstrating the consequences of premature and repeated null hypothesis testing</article-title><source>Behavior Research Methods</source><volume>38</volume><fpage>24</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.3758/BF03192746</pub-id><pub-id pub-id-type="pmid">16817510</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tukey</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="1959">1959</year><article-title>A quick, compact, two-sample test to Duckworth’s specifications</article-title><source>Technometrics : A Journal of Statistics for the Physical, Chemical, and Engineering Sciences</source><volume>1</volume><fpage>31</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.2307/1266308</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tukey</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="1977">1977</year><source>Exploratory Data Analysis </source><publisher-name>Addison-Wesley Publishing Company</publisher-name></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulrich</surname> <given-names>R</given-names></name><name><surname>Miller</surname> <given-names>J</given-names></name><name><surname>Erdfelder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Effect size estimation from <italic>t</italic>-statistics in the presence of publication bias</article-title><source>Zeitschrift für Psychologie</source><volume>226</volume><fpage>56</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1027/2151-2604/a000319</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulrich</surname> <given-names>R</given-names></name><name><surname>Miller</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Effects of truncation on reaction time analysis</article-title><source>Journal of Experimental Psychology: General</source><volume>123</volume><fpage>34</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.123.1.34</pub-id><pub-id pub-id-type="pmid">8138779</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vul</surname> <given-names>E</given-names></name><name><surname>Harris</surname> <given-names>C</given-names></name><name><surname>Winkielman</surname> <given-names>P</given-names></name><name><surname>Pashler</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Puzzlingly high correlations in fMRI studies of emotion, personality, and social cognition</article-title><source>Perspectives on Psychological Science</source><volume>4</volume><fpage>274</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1111/j.1745-6924.2009.01125.x</pub-id><pub-id pub-id-type="pmid">26158964</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>BM</given-names></name><name><surname>Wixted</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The prior odds of testing a true effect in cognitive and social psychology</article-title><source>Advances in Methods and Practices in Psychological Science</source><volume>1</volume><fpage>186</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1177/2515245918767122</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witt</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Insights into criteria for statistical significance from signal detection analysis</article-title><source>Meta-Psychology</source><volume>3</volume><elocation-id>871</elocation-id><pub-id pub-id-type="doi">10.15626/MP.2018.871</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmerman</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Invalidation of parametric and nonparametric statistical tests by concurrent violation of two assumptions</article-title><source>The Journal of Experimental Education</source><volume>67</volume><fpage>55</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1080/00220979809598344</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zwaan</surname> <given-names>RA</given-names></name><name><surname>Etz</surname> <given-names>A</given-names></name><name><surname>Lucas</surname> <given-names>RE</given-names></name><name><surname>Donnellan</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Making replication mainstream</article-title><source>Behavioral and Brain Sciences</source><volume>41</volume><fpage>01</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1017/S0140525X17001972</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><p>The analysis of <italic>p</italic>-hacking in the main article is based on one- and two-sample <italic>z</italic>-tests. Although researchers usually employ one- and two-sample <italic>t</italic>-tests rather than <italic>z</italic>-tests, we studied the latter tests because of their greater mathematical tractability. This should not have a big impact on the results, because <italic>z</italic>-tests closely resemble the results of the corresponding <italic>t</italic>-tests for realistic sample sizes.</p><sec id="s12" sec-type="appendix"><title>One-sample <italic>z</italic>-Test</title><p>This test proceeds from a random sample of <italic>n</italic> observations <inline-formula><mml:math id="inf367"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula><mml:math id="inf368"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Each observation <inline-formula><mml:math id="inf369"><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is a single measure taken from each of the <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> subjects. Let <inline-formula><mml:math id="inf371"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus <inline-formula><mml:math id="inf372"><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf373"><mml:mrow><mml:mrow><mml:mtext>SD</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>. In order to test the null hypothesis <inline-formula><mml:math id="inf374"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, one uses the test statistic<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which follows a standard normal distribution under the null hypothesis. For example, with a one-tailed test this hypothesis is rejected if <inline-formula><mml:math id="inf375"><mml:mi>Z</mml:mi></mml:math></inline-formula> exceeds the critical value <inline-formula><mml:math id="inf376"><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> that is associated with a pre-specified <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level. Moreover, the effect size of this test is<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>μ</mml:mi><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>There is an alternative application of the one-sample test that is worth mentioning. In this case <inline-formula><mml:math id="inf378"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents a difference score for the <italic>s</italic>-th subject, and the dependent measures <inline-formula><mml:math id="inf379"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf380"><mml:mi>Y</mml:mi></mml:math></inline-formula> are most likely correlated across subjects. Consequently, the variance of <inline-formula><mml:math id="inf381"><mml:mi>D</mml:mi></mml:math></inline-formula> is given by<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mtext>Cov</mml:mtext></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>If we let <inline-formula><mml:math id="inf382"><mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf383"><mml:mi>ϱ</mml:mi></mml:math></inline-formula> be the correlation between <inline-formula><mml:math id="inf384"><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf385"><mml:mi>Y</mml:mi></mml:math></inline-formula>, the preceding expression simplifies to<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ϱ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that for a moderate correlation, i.e., <inline-formula><mml:math id="inf386"><mml:mrow><mml:mi>ϱ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, the standard deviation of <inline-formula><mml:math id="inf387"><mml:mi>D</mml:mi></mml:math></inline-formula> becomes <inline-formula><mml:math id="inf388"><mml:mrow><mml:mrow><mml:mtext>SD</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mo>⋅</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and thus the test statistic under <inline-formula><mml:math id="inf389"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> of this alternative is<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>with effect size equal to <inline-formula><mml:math id="inf390"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Therefore, this alternative view of the one-sample test is equivalent to the aforementioned single-variable view.</p></sec><sec id="s13" sec-type="appendix"><title>Two-sample <italic>z</italic>-Test</title><p>The two-sample <italic>z</italic>-test proceeds from two independent samples <inline-formula><mml:math id="inf391"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf392"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To simplify matters, equal sample sizes are assumed. Without loss of generality, the first sample <inline-formula><mml:math id="inf393"><mml:mi>X</mml:mi></mml:math></inline-formula> is a random draw from <inline-formula><mml:math id="inf394"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the second sample <inline-formula><mml:math id="inf395"><mml:mi>Y</mml:mi></mml:math></inline-formula> from <inline-formula><mml:math id="inf396"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="inf397"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf398"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>; consequently, <inline-formula><mml:math id="inf399"><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf400"><mml:mrow><mml:mrow><mml:mtext>SD</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus the associated <italic>z</italic>-value of the statistic <inline-formula><mml:math id="inf401"><mml:mi>D</mml:mi></mml:math></inline-formula> for testing <inline-formula><mml:math id="inf402"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In addition, the effect size of this test is<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>μ</mml:mi><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s14" sec-type="appendix"><title>Multiple dependent measures</title><p>This section explains how to compute the probability of rejecting <inline-formula><mml:math id="inf403"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> when a researcher assesses <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> dependent measures for statistical significance. Assume that each of these dependent measures <inline-formula><mml:math id="inf405"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is converted into a <italic>z</italic>-value, that is,<disp-formula id="equ13"><label>(1)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>g</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>resulting in the random vector <inline-formula><mml:math id="inf406"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; <italic>g</italic> is equal to σ for a one-sample test and equal to <inline-formula><mml:math id="inf407"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> for a two-sample test. This vector has a multivariate distribution <inline-formula><mml:math id="inf408"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The mean of each <italic>z</italic>-value is given by<disp-formula id="equ14"><label>(2)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mfrac><mml:mo>⋅</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>and the variance of each <italic>z</italic>-value must be one; <inline-formula><mml:math id="inf409"><mml:msub><mml:mi>σ</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:math></inline-formula> equals 1 and <inline-formula><mml:math id="inf410"><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:math></inline-formula> for one- and two-sample tests, respectively.</p><p>The covariance matrix for the one-sample <italic>z</italic>-test can be derived as follows. Let the correlation coefficient of <inline-formula><mml:math id="inf411"><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf412"><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> be equal to <inline-formula><mml:math id="inf413"><mml:mrow><mml:msub><mml:mi>ϱ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ϱ</mml:mi></mml:mrow></mml:math></inline-formula>. Then the covariance of <inline-formula><mml:math id="inf414"><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf415"><mml:msub><mml:mi>Z</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mo>⋅</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mo>⋅</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mi>ϱ</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mi>ϱ</mml:mi><mml:mo>⋅</mml:mo><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϱ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Consequently, the off-diagonal elements in <inline-formula><mml:math id="inf416"><mml:mi mathvariant="bold">𝚺</mml:mi></mml:math></inline-formula> are equal to <inline-formula><mml:math id="inf417"><mml:mi>ϱ</mml:mi></mml:math></inline-formula> and those of the main diagonal are equal to 1.</p><p>For the two-sample test, the derivation of <inline-formula><mml:math id="inf418"><mml:mi mathvariant="bold">𝚺</mml:mi></mml:math></inline-formula> proceeds as follows<disp-formula id="equ16"><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:msqrt><mml:mo>⋅</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:msqrt><mml:mo>⋅</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ϱ</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt><mml:mo>+</mml:mo><mml:mi>ϱ</mml:mi><mml:mo>⋅</mml:mo><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>ϱ</mml:mi><mml:mo>⋅</mml:mo><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>ϱ</mml:mi><mml:mo>⋅</mml:mo><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϱ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>As a result, <inline-formula><mml:math id="inf419"><mml:mi mathvariant="bold">𝚺</mml:mi></mml:math></inline-formula> is identical to the covariance matrix of the one-sample test.</p><p>The rejection probability<disp-formula id="equ17"><label>(3)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>can be evaluated using routine <italic>mvncdf</italic> of MATLAB 2019a or function <italic>pmvnorm</italic> of the R package <italic>mvtnorm</italic> (<xref ref-type="bibr" rid="bib27">Genz, 1992</xref>; <xref ref-type="bibr" rid="bib28">Genz and Bretz, 1999</xref>; <xref ref-type="bibr" rid="bib29">Genz and Bretz, 2002</xref>).</p></sec><sec id="s15" sec-type="appendix"><title>Computing the probability of rejecting <inline-formula><mml:math id="inf420"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> with multiple peeks</title><p>This section shows how to compute the probability of rejecting <inline-formula><mml:math id="inf421"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> with a maximum of <inline-formula><mml:math id="inf422"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> peeks for one- or two-sample <italic>z</italic>-tests. Our procedure extends the standard approach originally suggested by <xref ref-type="bibr" rid="bib1">Armitage et al., 1969</xref>; (see also <xref ref-type="bibr" rid="bib57">Proschan et al., 2006</xref>, p. 78), which can be used to compute the probability of rejecting <inline-formula><mml:math id="inf423"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> for a one-sided test of a true null hypothesis. The extension also allows one to compute the probability of rejecting <inline-formula><mml:math id="inf424"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> when the null hypothesis is false (a somewhat similar mathematical approach is provided in <xref ref-type="bibr" rid="bib57">Proschan et al., 2006</xref>).</p><p>Assume that data are first checked for statistical significance (i.e., first “peek”) when <inline-formula><mml:math id="inf425"><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> observations have been collected for a one-sample design or in each group for a two-sample design. If no statistically significant result is observed, the per-group sample size will be increased to <inline-formula><mml:math id="inf426"><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> and again checked for statistical significance. This strategy is repeated until a significant result is obtained or terminated after <inline-formula><mml:math id="inf427"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> peeks when there has been no significant result at any peek. Thus, the sequence <inline-formula><mml:math id="inf428"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the different sample sizes at which the researcher tests the null hypothesis. In order to compute the probability of rejecting <inline-formula><mml:math id="inf429"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> with multiple peeks, we let <inline-formula><mml:math id="inf430"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> be the <italic>z</italic>-values associated with the various sample sizes <inline-formula><mml:math id="inf431"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. For a one-sided test the probability of rejecting <inline-formula><mml:math id="inf432"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> with a maximum of <inline-formula><mml:math id="inf433"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> peeks is given by<disp-formula id="equ18"><label>(4)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">j</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf434"><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the cumulative distribution function of the random vector <inline-formula><mml:math id="inf435"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that follows a multivariate normal <inline-formula><mml:math id="inf436"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝝁</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝚺</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf437"><mml:mrow><mml:mi mathvariant="bold-italic">𝝁</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and covariance matrix <inline-formula><mml:math id="inf438"><mml:mi mathvariant="bold">𝚺</mml:mi></mml:math></inline-formula>. In addition, the cutoff <italic>c</italic> corresponds to the <inline-formula><mml:math id="inf439"><mml:mrow><mml:mn>100</mml:mn><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>% percentile of the standard normal.</p><p>Under <inline-formula><mml:math id="inf440"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, the expected means of <inline-formula><mml:math id="inf441"><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are <inline-formula><mml:math id="inf442"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf443"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In contrast, under <inline-formula><mml:math id="inf444"><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> these means are<disp-formula id="equ19"><label>(5)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mo>*</mml:mo></mml:msub></mml:mfrac><mml:mo>⋅</mml:mo><mml:mpadded width="+1.7pt"><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msqrt></mml:mpadded></mml:mrow></mml:mrow><mml:mo rspace="22.5pt">,</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf445"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for a one-sample test and <inline-formula><mml:math id="inf446"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> for a two-sample test.</p><p>The covariance matrix <inline-formula><mml:math id="inf447"><mml:mi mathvariant="bold">𝚺</mml:mi></mml:math></inline-formula> is completely specified by the vector <inline-formula><mml:math id="inf448"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. It can be shown that the <inline-formula><mml:math id="inf449"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-th element for <inline-formula><mml:math id="inf450"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of this matrix is given by<disp-formula id="equ20"><label>(6)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:mrow><mml:mtext>Cov</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfrac></mml:msqrt></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In order to prove this equation, one makes use of the distributive property of covariances,<disp-formula id="equ21"><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msqrt><mml:mo>⋅</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msqrt><mml:mo>⋅</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⋅</mml:mo><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⋅</mml:mo><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⋅</mml:mo><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mo>,</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>term </mml:mtext><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mtext> 0</mml:mtext></mml:mrow></mml:munder></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⋅</mml:mo><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⋅</mml:mo><mml:msqrt><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msqrt><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfrac></mml:msqrt><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>For example, with peeks at <inline-formula><mml:math id="inf451"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>25</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>35</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, one obtains<disp-formula id="equ22"><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnalign="left left left left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1.0000</mml:mn></mml:mtd><mml:mtd><mml:mn>0.8944</mml:mn></mml:mtd><mml:mtd><mml:mn>0.8165</mml:mn></mml:mtd><mml:mtd><mml:mn>0.7559</mml:mn></mml:mtd><mml:mtd><mml:mn>0.7071</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.8944</mml:mn></mml:mtd><mml:mtd><mml:mn>1.0000</mml:mn></mml:mtd><mml:mtd><mml:mn>0.9129</mml:mn></mml:mtd><mml:mtd><mml:mn>0.8452</mml:mn></mml:mtd><mml:mtd><mml:mn>0.7906</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.8165</mml:mn></mml:mtd><mml:mtd><mml:mn>0.9129</mml:mn></mml:mtd><mml:mtd><mml:mn>1.0000</mml:mn></mml:mtd><mml:mtd><mml:mn>0.9258</mml:mn></mml:mtd><mml:mtd><mml:mn>0.8660</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.7559</mml:mn></mml:mtd><mml:mtd><mml:mn>0.8452</mml:mn></mml:mtd><mml:mtd><mml:mn>0.9258</mml:mn></mml:mtd><mml:mtd><mml:mn>1.0000</mml:mn></mml:mtd><mml:mtd><mml:mn>0.9354</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.7071</mml:mn></mml:mtd><mml:mtd><mml:mn>0.7906</mml:mn></mml:mtd><mml:mtd><mml:mn>0.8660</mml:mn></mml:mtd><mml:mtd><mml:mn>0.9354</mml:mn></mml:mtd><mml:mtd><mml:mn>1.0000</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Again with <inline-formula><mml:math id="inf452"><mml:mi mathvariant="bold-italic">𝝁</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf453"><mml:mi mathvariant="bold">𝚺</mml:mi></mml:math></inline-formula>, one can evaluate <xref ref-type="disp-formula" rid="equ18">Equation 4</xref> using routine <italic>mvncdf</italic> of MATLAB 2019a or function <italic>pmvnorm</italic> of the R package <italic>mvtnorm</italic> (<xref ref-type="bibr" rid="bib27">Genz, 1992</xref>; <xref ref-type="bibr" rid="bib28">Genz and Bretz, 1999</xref>; <xref ref-type="bibr" rid="bib29">Genz and Bretz, 2002</xref>).</p></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><sec id="s16" sec-type="appendix"><title>A comparison of research payoffs</title><p>Using the quantitative models of <italic>p</italic>-hacking developed in the main article, good practice and <italic>p</italic>-hacking can also be compared with respect to global measures of research effectiveness, in addition to the comparisons of Type 1 error rates, false positives, and replication rates. As an illustration, this section compares good practice versus the particular <italic>p</italic>-hacking strategy of data peeking based on the overall research payoff model of <xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>.</p><p>The payoff model assumes that a researcher tests a fixed total number of participants across a large number of studies (e.g., <inline-formula><mml:math id="inf454"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), with each study testing either a true or a false null hypothesis (i.e., <inline-formula><mml:math id="inf455"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf456"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). Each study produces one of four possible decision outcomes: a true positive (TP) in which <inline-formula><mml:math id="inf457"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is correctly rejected, a false positive (FP) in which <inline-formula><mml:math id="inf458"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is incorrectly rejected (i.e., Type 1 error), a true negative (TN) in which <inline-formula><mml:math id="inf459"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is correctly retained, or a false negative (FN) in which <inline-formula><mml:math id="inf460"><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is incorrectly retained (i.e., Type 2 error). According to the model, each outcome is associated with a given scientific payoff for the research area as a whole (i.e., <inline-formula><mml:math id="inf461"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf462"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf463"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf464"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, in arbitrary units). The expected net payoff for any given research strategy (e.g., data-peeking, good practice) is the weighted sum of the individual outcome payoffs, with weights corresponding to the expected number of studies within that strategy multiplied by the probabilities of the different outcomes [i.e., <inline-formula><mml:math id="inf465"><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf466"><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf467"><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf468"><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>]. The numbers of studies and outcome probabilities for researchers using good practice can be computed using standard techniques (e.g., <xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>), and they can be computed for data-peeking researchers using the outcome probabilities computed as described in the main article.</p><p><xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref> illustrates expected net payoffs for a simple scenario in which positive results are either helpful or harmful to the scientific field (i.e., <inline-formula><mml:math id="inf469"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf470"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), whereas negative results are basically uninformative (i.e., <inline-formula><mml:math id="inf471"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>), and several aspects are of interest. First, as was noted by <xref ref-type="bibr" rid="bib46">Miller and Ulrich, 2016</xref>, the expected net payoff increases strongly with the base rate of true effects, simply because the higher base rate increases the likelihood of obtaining true positive results. With a low base rate of true effects, the expected payoff can even be negative if the base rate is so low that FPs are more common than TPs. Second, as was emphasized by <xref ref-type="bibr" rid="bib47">Miller and Ulrich, 2019</xref>, payoffs can be larger for <inline-formula><mml:math id="inf472"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> than for <inline-formula><mml:math id="inf473"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo lspace="0pt" rspace="3.5pt">%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, especially when the base rate of true effects is not too small. This happens because the greater power provided by the larger <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level outweighs the associated increase in Type 1 errors. Third, payoffs depend little on sample size, again because of a trade-off: Although larger samples provide greater power, which tends to increase payoff, they also reduce the number of studies that can be conducted with the fixed total number of participants, which tends to reduce payoff.</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Expected payoff as a function of base rate and sample size.</title><p>The dashed lines give the expected payoffs for researchers using data peeking at sample sizes of 10, 15, 20, 25, and 30. The solid lines give the expected payoffs for researchers who act in accord with good scientific practice and only check the data once, at the indicated sample size. The panels on the left side reflect the results for one-sample tests, whereas those on the right for two-sample tests. The upper and lower panels give the results for a nominal one-tailed <inline-formula><mml:math id="inf475"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> levels of 0.5 and 5%, respectively, with different vertical scales used because of the different ranges of payoffs for the two <inline-formula><mml:math id="inf476"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> levels. All results are based on an effect size of <inline-formula><mml:math id="inf477"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>, individual outcome payoffs of <inline-formula><mml:math id="inf478"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf479"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf480"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf481"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and a total sample size of <inline-formula><mml:math id="inf482"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The results are similar for two-tailed testing (not shown).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-app2-fig1-v2.tif"/></fig><p>In the present context, however, the most interesting aspect of <xref ref-type="fig" rid="app2fig1">Appendix 2—figure 1</xref> is that the expected payoff can be larger with <italic>p</italic>-hacking by data peeking than with good practice. As was noted earlier, data peeking inflates the Type 1 error rate but also increases power, and these two consequences of peeking have counteracting effects on total payoff due to the opposite weighting of TPs and FPs (i.e., <inline-formula><mml:math id="inf483"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf484"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). When the base rate of true effects is large enough, the positive effects of increased power outweigh the negative effects of increased Type 1 errors. Moreover, with relatively large base rates (e.g., <inline-formula><mml:math id="inf485"><mml:mrow><mml:mi>π</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>), this can be true even when the cost of an FP is much larger than the gain associated with a TP (e.g., <inline-formula><mml:math id="inf486"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf487"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒫</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). Thus, under certain circumstances, data peeking would arguably be more effective than using the good-practice approach of fixing sample size in advance (e.g., <xref ref-type="bibr" rid="bib26">Frick, 1998</xref>).</p></sec><sec id="s17" sec-type="appendix"><title>Type 1 error rate versus power</title><p>Because there is an inherent trade-off between Type 1 error rate and power (i.e., larger Type 1 error rates tend to produce greater power), it is also useful to compare good practice and <italic>p</italic>-hacking procedures in a manner that takes both of these variables into account simultaneously. Similar comparisons are standard tools for determining the most powerful test (e.g., <xref ref-type="bibr" rid="bib48">Mood et al., 1974</xref>), under the assumption that a better procedure yields higher power for a given Type 1 error rate.</p><p><xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2</xref> shows examples of such comparisons, plotting power versus Type 1 error rate for good practice and for each of the <italic>p</italic>-hacking procedures. To trace out the each line in this figure, the nominal <inline-formula><mml:math id="inf488"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level of each procedure was varied between 0.001–0.2 in steps of 0.001. For good practice, the Type 1 error rate is simply the nominal <inline-formula><mml:math id="inf489"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level, and power is computed using standard methods for that <inline-formula><mml:math id="inf490"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, a given effect size <inline-formula><mml:math id="inf491"><mml:mrow><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, a given sample size, and a one- or two-sample design. The analogous Type 1 error rates and power values for each of the <italic>p</italic>-hacking procedures can be computed using the models described in the main article. For the <italic>p</italic>-hacking methods, the Type 1 error rates are greater than the nominal 0.001–0.2 <inline-formula><mml:math id="inf492"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> levels (i.e., Type 1 error rate inflation), and the curves for the different <italic>p</italic>-hacking methods are therefore stretched and shifted to the right. For example, with a nominal <inline-formula><mml:math id="inf493"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>—the maximum used in these calculations—the actual Type 1 error rate for multiple studies <italic>p</italic>-hacking is nearly 0.7.</p><p>Perhaps surprisingly, <xref ref-type="fig" rid="app2fig2">Appendix 2—figure 2</xref> shows that several of the <italic>p</italic>-hacking procedures have greater power than good practice at each actual Type 1 error rate. As an example, consider multiple studies <italic>p</italic>-hacking with <inline-formula><mml:math id="inf494"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> as shown in the figure. Taking inflation into account, a nominal <inline-formula><mml:math id="inf495"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level of 0.01 produces a Type 1 error rate of approximately 0.05. For <inline-formula><mml:math id="inf496"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> and one-sample testing, this nominal <inline-formula><mml:math id="inf497"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level yields power of 0.33. In contrast, using good practice with a nominal <inline-formula><mml:math id="inf498"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level of 0.05, which of course produces the same Type 1 error rate of 0.05, the power level is only 0.23. Thus, a multiple studies researcher using the stricter nominal <inline-formula><mml:math id="inf499"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> level of 0.01 would have the same rate of Type 1 errors as the good practice researcher and yet have higher power. As a consequence of its higher power and equal Type 1 error rate, multiple studies would also produce a higher replication rate than good practice for any fixed base rate of true effects. Thus, under certain circumstances, the <italic>p</italic>-hacking procedures would arguably be more effective than the good-practice approach.</p><fig id="app2fig2" position="float"><label>Appendix 2—figure 2.</label><caption><title>Power as a function of Type 1 error rate.</title><p>Power for one-tailed testing as a function of Type 1 error rate for researchers using good practice or one of the four <italic>p</italic>-hacking procedures considered in the main article: multiple studies (<inline-formula><mml:math id="inf500"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>), multiple DVs (<inline-formula><mml:math id="inf501"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> with intercorrelations of 0.2), data peeking after <inline-formula><mml:math id="inf502"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, 15, 20, 25, and 30, or multiple analyses (<inline-formula><mml:math id="inf503"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). Computations were based on a sample size of <inline-formula><mml:math id="inf504"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> (per group) for all procedures other than data-peeking.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-app2-fig2-v2.tif"/></fig><p>In retrospect, it seems obvious that some types of <italic>p</italic>-hacking would produce higher power than good practice, because they involve collecting more data. In these examples good practice involved testing 20 participants in the example one-sample design, whereas multiple studies <italic>p</italic>-hacking allowed testing up to 100 participants. Data peeking also involved testing more participants—up to a maximum of 30—when that was necessary to obtain significant results. Collecting multiple DVs also provides more data because there are more scores per participant. Only multiple analysis <italic>p</italic>-hacking involves collecting the same amount of data as good practice, and this type of <italic>p</italic>-hacking yields less power than good practice at a given Type 1 error rate.</p></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.58237.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Thompson</surname><given-names>William Hedley</given-names></name><role>Reviewer</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Francis</surname><given-names>Gregory</given-names> </name><role>Reviewer</role><aff><institution>Purdue University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>Thank you for submitting your article &quot;Meta Research: Replication of Significant Results - Modeling the Effects of p-Hacking&quot; to <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by the eLife Features Editor (Peter Rodgers). The following individuals involved in review of your submission have agreed to reveal their identity: William Hedley Thompson (Reviewer #2); Gregory Francis (Reviewer #3).</p><p>Summary:</p><p>The authors present an argument and simulations illustrating the impact of the replication rate of four different questionable research practices. I found the argument both interesting and convincing. The article also raises important points about interpreting replicability that are very much misunderstood by many practicing scientists. However, improvements could be made to clarify certain assumptions and aspects of the argument.</p><p>Essential revisions:</p><p>1) Equation (1) succinctly related replication rate (RR) to power and alpha. In some sense, almost everything follows from this equation. The details about how different QRPs affect alpha and beta are secondary (but useful) analyses. A further discussion of equation (1) might be worthwhile regardless of how alpha and beta take their values (through QRPs or otherwise).</p><p>2) It was not clear to me how the value for <italic>β<sub>2</sub></italic> was selected. Based on the figure captions, I think it was just set to be <italic>β<sub>2</sub></italic>=0.9, so 90% power. But the problem introduced by QRPs is that they tend to inflate standardized effect sizes. So, a replication of a QRP-influenced study might estimate power based on the effect size reported in the original study. Doing so will often lead to gross underestimation of power for the replication study (even though the replicator thinks they have 90% power, they might actually have 50% power). Also common, the replicator uses the same sample size as the original study, which again tends to lead to low power if the original study used QRPs. It is through these mechanisms that QRPs for an original study contribute to a low replicability rate (provided the replication study uses good practices).</p><p>3) As I read it, the headline result seemed to be that p-hacking doesn't have a large impact on replication, and therefore the explanation for surprisingly low replication must lie elsewhere. Unfortunately, the support for this claim hinges on the degree of p-hacking that one envisions, and it seems to me that the degree of p-hacking envisioned here is rather mild. My own experience suggests that most p-hacking flows from investigators' ignorance about what constitutes p-hacking, and in such cases, investigators can easily p-hack to a much greater degree than the simulations here suggest. I suspect that p-hacking investigators can easily conduct dozens if not hundreds of tests for every result ultimately published, which is a much more severe form of p-hacking than the simulations here envision. In these cases, p-hacking very well may be an important cause of non-reproducible results, thus overturning the major finding of this paper.</p><p>To be sure, the manuscript is abundantly aware that the degree to which p-hacking generates non-reproducible results depends on the degree of p-hacking, and both the results and text make that clear. So, I don't think the manuscript is 'wrong'. But, I fear that if one really wants to know how much p-hacking contributes to non-reproducible results, one has to know the extent to which p-hacked studies are indeed p-hacked.</p><p>-From the Features Editor: The article needs to explore levels of p-hacking higher than those explored in the current version and, if necessary, to revise the discussion and conclusions in the light of what these new analyses find.</p><p>Also, please consider discussing and citing the following paper:</p><p>Simonsohn et al 2015, <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/xge0000104">http://dx.doi.org/10.1037/xge0000104</ext-link></p><p>4) I also found the last simulation - meant to investigate the effect of removing outliers - unconvincing. As I understand it, the simulation generated hypothetical data sets that were the contaminated by outliers with a standard deviation ten-fold greater than the actual data generating process. It seems to me that this is a poor model for capturing outliers, because the outliers in this case are so anomalous that they corrupt the performance of the good-practice analysis. Thus we wind up with the head-scratching result that selective outlier removal improves FDR and RR. I don't think this set-up captures the actual hazards of selective outlier removal, and wouldn't put much stock in its results. To summarize, the simulation set-up needs to be more realistic and/or better justified.</p><p>5) A possible discussion point regarding the assumptions of the RR value. An interesting assumption in the RR value is that null/non-significant results are not replicated. And lowering the alpha threshold for statistical significance will increase the number of false negatives. So a possible outcome of focusing on improving RR with alpha thresholds that more false negatives go undetected and not replicated?</p><p>6) The authors use a group size of 20 (so total n=40) but sample size appears to be a key variable that will impact some of these measures (e.g. outlier exclusion). The researchers motivate their value by citing Marszalek, Barber, Kohlhart, &amp; Holmes, 2011, Table 3.</p><p>First, I think the authors may be referring to table 1 to get the value of 40 (I am unable to locate the value in Table 3)?</p><p>Second, others (e.g. Fraley &amp; Vazire 2014, 10.1371/journal.pone.0109019), found the average sample size (in social-personality) psychology research to be higher (here: 104). How dependent are the results and conclusions on the limited sample size? (especially for outlier exclusion). Also how dependent are the outlier exclusion results if more/less of the data points our outliers (currently 5% of data).</p><p>7) In Figure 3,4,6,9,11,12 and the associated text, there is no quantification about how much the &quot;p-hacking&quot; approach is worse and unprecise language is used, e.g. &quot;is modest for high base rates&quot; and the reader has to deduce the differences from the many-paneled figures. I think adding some summary numbers to the text (or an additional figure) to show the differences between methods would be useful (e.g. state RR difference when the base rate is 0.2 and 0.5 (with d=0.5, k=5) or maybe the total difference between the curves). This would be especially helpful when contrasting the differences between the &quot;p-hacking&quot; and &quot;good practice&quot; differences for the different thresholds where the reader has to deduce two differences from the graph and then compare those deduced differences in their heads.</p><p>8) One quite surprising result here was that selective outlier removal seems to increase the RR and perhaps needs a little more discussion. At the moment, a reader could read the paper and conclude that performing selective outlier removal is something that should be done to improve the RR. Is this the authors' position? If not, perhaps this should be explicitly stated.</p><p>9) The supplemental material (and a few places in the main text) suggest that QRPs might actually be favorable for scientific investigations because they increase the replicability rate. The text describes the situation properly, but I fear some readers will get the wrong impression. The favorable aspects very much depend on what a scientist wants (to avoid) out of their analyses. The supplemental material makes some claims about inflation and setting of Type I error rates and power that seem to contradict the Neyman-Pearson lemma. If not, then the multiple-studies researcher must using a larger sample size, so there are costs involved. This might be worth discussing.</p><p>10) In the Discussion the text suggests it will be difficult to increase replicability in fields with low base rates. To the contrary, I think it is easy: just increase the base rate. Scientists should do a better job picking hypotheses to test. They should not waste time testing hypotheses that would be surprising or counterintuitive. The text then goes onto discuss about how campaigns to reduce p-hacking may be ineffective. I get the point, but a field with a low base rate of hypotheses should have a low replication rate. Increasing replicability is not (or, should not) be the goal of scientific investigations.</p><p>11) The authors are familiar with some of my work on this topic (they cite several of my papers). There, the problem is not a low replication rate, but a &quot;too high&quot; replication rate. The problem is that if both original and replication scientists are using QRPs, then the replication rate is too high, compared to what would be expected with &quot;good practice&quot; analyses/experiments. In my view, this is the more serious problem with current practice, because it implies that the Type I error rate is higher than &quot;good practice&quot;. This suggests that scientists are not doing what they intended to do. This different viewpoint struck me while reading the introduction of the paper. There it is noted that some people suggest that QRPs lead to low replication rates. But this claim never really made sense (at least not without more discussion) because QRPs increase the probability of rejecting the null; so QRPs increase the replication rate. Indeed, if the simulations were revised so that both the original and replication scientists used QRPs, there would be quite an increase in the replication rate, even when the true effect is 0.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.58237.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[We repeat the points in the decision letter in italic, and give our responses in Roman.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Equation (1) succinctly related replication rate (RR) to power and alpha. In some sense, almost everything follows from this equation. The details about how different QRPs affect alpha and beta are secondary (but useful) analyses. A further discussion of equation (1) might be worthwhile regardless of how alpha and beta take their values (through QRPs or otherwise).</p></disp-quote><p>We agree and hence we have further discussed Equation 1 by illustrating its major features in a figure (Figure 2).</p><disp-quote content-type="editor-comment"><p>2) It was not clear to me how the value for β<sub>2</sub> was selected. Based on the figure captions, I think it was just set to be β<sub>2</sub>=0.9, so 90% power. But the problem introduced by QRPs is that they tend to inflate standardized effect sizes. So, a replication of a QRP-influenced study might estimate power based on the effect size reported in the original study. Doing so will often lead to gross underestimation of power for the replication study (even though the replicator thinks they have 90% power, they might actually have 50% power). Also common, the replicator uses the same sample size as the original study, which again tends to lead to low power if the original study used QRPs. It is through these mechanisms that QRPs for an original study contribute to a low replicability rate (provided the replication study uses good practices).</p></disp-quote><p>We agree that estimated effect sizes are overestimated by replicators in such scenarios, especially when non-significant results are put into the file drawer. The power of 90% was picked as the best-scenario value close to the average replication power (i.e. 92%) claimed by the OSF project, although we acknowledge that their actual power levels may have been lower for the reasons mentioned by the reviewer. In any case, we have rerun our computations with the above suggested power of 50% and checked if our conclusions are still valid under this condition. For example, <xref ref-type="fig" rid="respfig1">Author response image 1</xref> shows the rate of replication for a replication power of 50% instead of 90% (as in Figure 5 of the manuscript). It can be seen in this new figure that p-hacking would even be slightly less harmful to replication rates with low-powered replication studies than with high-powered replications. Thus low-powered replication studies would not change the conclusions of our paper. We now mention this point in the Discussion.</p><fig id="respfig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-resp-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>3) As I read it, the headline result seemed to be that p-hacking doesn't have a large impact on replication, and therefore the explanation for surprisingly low replication must lie elsewhere. Unfortunately, the support for this claim hinges on the degree of p-hacking that one envisions, and it seems to me that the degree of p-hacking envisioned here is rather mild. My own experience suggests that most p-hacking flows from investigators' ignorance about what constitutes p-hacking, and in such cases, investigators can easily p-hack to a much greater degree than the simulations here suggest. I suspect that p-hacking investigators can easily conduct dozens if not hundreds of tests for every result ultimately published, which is a much more severe form of p-hacking than the simulations here envision. In these cases, p-hacking very well may be an important cause of non-reproducible results, thus overturning the major finding of this paper.</p><p>To be sure, the manuscript is abundantly aware that the degree to which p-hacking generates non-reproducible results depends on the degree of p-hacking, and both the results and text make that clear. So, I don't think the manuscript is 'wrong'. But, I fear that if one really wants to know how much p-hacking contributes to non-reproducible results, one has to know the extent to which p-hacked studies are indeed p-hacked.</p><p>-From the Features Editor: The article needs to explore levels of p-hacking higher than those explored in the current version and, if necessary, to revise the discussion and conclusions in the light of what these new analyses find.</p><p>Also, please consider discussing and citing the following paper:</p><p>Simonsohn et al 2015, http://dx.doi.org/10.1037/xge0000104</p></disp-quote><p>We cannot rule out the possibility that some investigators p-hack to a more considerable degree than we have assumed in our computations. The extent of p-hacking remains a controversial issue, with some arguing and providing evidence that ambitious p-hacking is too complicated and thus not plausible (Simonsohn et al., 2015, p. 1149), and even that the frequency of any p-hacking has probably been overestimated (Fiedler &amp; Schwarz, 2016). Unfortunately, the exact extent of p-hacking is difficult to determine and might strongly depend on the field of research. For example, in areas with small effect sizes, p-hacking might be more extensive than in fields with medium or large effect sizes. But even without knowing the true p-hacking rates, our analyses are still valuable, because they clearly show that evidence of massive p-hacking is needed before one can conclude that p-hacking is a major contributor to the replication crisis. Nevertheless, for most p-hacking methods we now consider more extensive p-hacking to address this point (k to a maximum of 8). Because we cannot see how one could perform so many different selective outlier removal attempts, however, we did not consider more extensive p-hacking with this strategy. We also included this point in the General Discussion.</p><disp-quote content-type="editor-comment"><p>4) I also found the last simulation - meant to investigate the effect of removing outliers - unconvincing. As I understand it, the simulation generated hypothetical data sets that were the contaminated by outliers with a standard deviation ten-fold greater than the actual data generating process. It seems to me that this is a poor model for capturing outliers, because the outliers in this case are so anomalous that they corrupt the performance of the good-practice analysis. Thus we wind up with the head-scratching result that selective outlier removal improves FDR and RR. I don't think this set-up captures the actual hazards of selective outlier removal, and wouldn't put much stock in its results. To summarize, the simulation set-up needs to be more realistic and/or better justified.</p></disp-quote><p>The simulation scenario that we used to contaminate normally distributed scores has previously been employed for assessing the efficacy of outlier elimination methods (e.g., Bakker &amp; Wicherts, 2014; Zimmerman, 1998). For example, Bakker &amp; Wicherts and also Zimmermann sampled normally distributed scores from N(d,1), d=0, 0.2, 0.5, 0.8 with probability 0.95 and contaminated these scores with outliers that were drawn with probability 0.05 from a normal distribution with a standard deviation of 20—even larger than our standard deviation of 10 that the reviewer regards as unrealistically large. Nonetheless, in order to check how the results would change with a less extreme contamination distribution as suggested by the reviewer, we changed the standard deviation of the outlier distribution to 5. We found pretty similar results, that is, the primary factor limiting the replication rate remains the base rate (see <xref ref-type="fig" rid="respfig2">Author response image 2</xref> which can be compared to Figure 16 in the main text with a standard deviation of 10). Besides, the Z-score method for identifying outliers is commonly used in psychology as the meta-analysis by Bakker &amp; Wicherts has revealed; their meta-analysis also shows that absolute thresholds values for the Z scores of 2.0, 2.5 and 3.0 are common in psychological research. Thus, our simulations capture standard methods of data analysis. In the revision, we have stressed this point.</p><fig id="respfig2"><label>Author response image 2.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-resp-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>5) A possible discussion point regarding the assumptions of the RR value. An interesting assumption in the RR value is that null/non-significant results are not replicated. And lowering the alpha threshold for statistical significance will increase the number of false negatives. So a possible outcome of focusing on improving RR with alpha thresholds that more false negatives go undetected and not replicated?</p></disp-quote><p>Like others modeling replication rates, we assume that researchers only try to replicate significant results. As the reviewer notes, there will be fewer of these when alpha is lowered—whether a true effect is present or <italic>H</italic><sub>0</sub> is true—and this effect of alpha can clearly be seen in the probability of rejecting <italic>H</italic><sub>0</sub> (e.g., Figure 3). Naturally, our computations of RR take this effect into account, so the effects of alpha on RR can easily be seen (e.g., Figure 5). As the reviewer rightly notes, however, reducing alpha has an additional effect that is not evident in RR: namely, fewer replication studies will be needed, because fewer positive effects will be found. We have now mentioned this fact in the General Discussion, and we thank the reviewer for the suggestion.</p><disp-quote content-type="editor-comment"><p>6) The authors use a group size of 20 (so total n=40) but sample size appears to be a key variable that will impact some of these measures (e.g. outlier exclusion). The researchers motivate their value by citing Marszalek, Barber, Kohlhart, &amp; Holmes, 2011, Table 3.</p><p>First, I think the authors may be referring to table 1 to get the value of 40 (I am unable to locate the value in Table 3)?</p><p>Second, others (e.g. Fraley &amp; Vazire 2014, 10.1371/journal.pone.0109019), found the average sample size (in social-personality) psychology research to be higher (here: 104). How dependent are the results and conclusions on the limited sample size? (especially for outlier exclusion). Also how dependent are the outlier exclusion results if more/less of the data points our outliers (currently 5% of data).</p></disp-quote><p>Yes, we inferred this value from Table 3 (as indicated in the previous version) but not from Table 1, as the Reviewer seems to believe. Table 3 gives the group sample size.</p><p>First, the mean of the medians in Table 3 of Marszlek et al. (2011) is 18.9. The corresponding group size in the Open Science Replication Project (considering studies with significant t-tests) is 27.5 and thus not considerably off our value of 20. It is likely that sample sizes in published articles became larger or were already larger in the field of social psychology and personality (see Sassenberg &amp; Ditrich, 2019, Advances in Methods and Practices in Psychological Sciences). Note that sample and effect size determine the statistical power of the original study. Thus if we would employ a larger group size, this would merely increase the statistical power to an unrealistically high level --- the estimated median power level has been 36% for psychological studies and is even lower in the neurosciences. In order to keep the power of the original studies at a realistic level, we would have to reduce the effect sizes in our computations. We refrained from doing this because the effect sizes of 0.2, 0.5, and 0.8 seem appropriate theoretical choices for demonstrating the effect of p-hacking on the replication rate with small, medium, and large effects. Nevertheless, we have addressed the issue of group size in the General Discussion.</p><p>Second, Fraley and Vazire (2014) have reported an average sample size of 104 in their meta-analysis of articles published in social-personality psychology. Unfortunately, this size refers to the total sample size and not to group size. For example, consider a 2 x 2 between factorial design, then each group would be comprised of 21 subjects. Nevertheless, their article contained additional information that we found worth mentioning – for example, they estimated the average power in this area as 50% with an average effect size of d = 0.43, and that the false positive rate is (at least ) 28% for a base rate of 20%. These values fit well with our analyses. The revised paper includes Fraley and Vazire.</p><p>Third, we have rerun the simulations on outlier exclusion with group sizes of 50 (i.e., sample sizes of 100) using the new scenario (see also our response to Comment 8 below). <xref ref-type="fig" rid="respfig3">Author response image 3</xref> shows the result (which should be compared to Figure 16 in the main text with a group size of 20). It is clear that there are no major changes in the effects of p-hacking, alpha, or base rate, even though overall replicability has increased because of the greater power associated with larger samples.</p><fig id="respfig3"><label>Author response image 3.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-58237-resp-fig3-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>7) In Figure 3,4,6,9,11,12 and the associated text, there is no quantification about how much the &quot;p-hacking&quot; approach is worse and unprecise language is used, e.g. &quot;is modest for high base rates&quot; and the reader has to deduce the differences from the many-paneled figures. I think adding some summary numbers to the text (or an additional figure) to show the differences between methods would be useful (e.g. state RR difference when the base rate is 0.2 and 0.5 (with d=0.5, k=5) or maybe the total difference between the curves). This would be especially helpful when contrasting the differences between the &quot;p-hacking&quot; and &quot;good practice&quot; differences for the different thresholds where the reader has to deduce two differences from the graph and then compare those deduced differences in their heads.</p></disp-quote><p>We like the suggestion to include an additional figure that quantifies the shrinkage of the replication rate for various level of p-hacking. In the revised paper we have added such figures along with some text that describes the resulting shrinkage.</p><disp-quote content-type="editor-comment"><p>8) One quite surprising result here was that selective outlier removal seems to increase the RR and perhaps needs a little more discussion. At the moment, a reader could read the paper and conclude that performing selective outlier removal is something that should be done to improve the RR. Is this the authors' position? If not, perhaps this should be explicitly stated.</p></disp-quote><p>This is certainly not our position, although our simulations indicate that selective outlier removal can improve the replication rate and lower the false positive rate. In response to this comment, we have reconsidered our outlier removal simulation. In the previous version of the manuscript, the simulated researchers always started with “no removal” and then tried more and more removal. However, “no removal” is not the best method if there are outliers. It seems perhaps a more conventional practice to remove outliers before applying a statistical test (see the meta-analysis by Bakker &amp; Wicherts, 2014 that we cite now). Under this alternative scenario, subsequent “removals” might especially be prone to Type I error inflation and thus lower the replication rate. We have rerun our simulations under this scenario. The results indicate that multiple removals would lower the replication rate. Author response table 1 shows how we have changed the sequence of outlier removal attempts for the simulations in the revised paper. We have also modified the text in the manuscript accordingly.</p><p>Author response table 1.</p><p>We have also explicitly stated that outlier checks should be made before statistical testing and that multiple testing with different outlier criteria is unacceptable because of the increased Type 1 error rate, regardless of RR.</p><disp-quote content-type="editor-comment"><p>9) The supplemental material (and a few places in the main text) suggest that QRPs might actually be favorable for scientific investigations because they increase the replicability rate. The text describes the situation properly, but I fear some readers will get the wrong impression. The favorable aspects very much depend on what a scientist wants (to avoid) out of their analyses. The supplemental material makes some claims about inflation and setting of Type I error rates and power that seem to contradict the Neyman-Pearson lemma. If not, then the multiple-studies researcher must using a larger sample size, so there are costs involved. This might be worth discussing.</p></disp-quote><p>First, in the revision we have emphasized further that we do not suggest that QRPs might actually be favorable (e.g., see the Conclusions). Even in the section on outlier removal, we have now stressed that researchers should carefully examine their data before conducting any statistical tests in order to avoid inflating the Type 1 error rate. Second, the analysis in Appendix subsection “Type 1 error rate versus power” suggests that p-hacking can produce a larger power compared to good practice for the same level of Type I error. However, as the reviewer has correctly noted and as we have stated there, much larger sample sizes are the price of the potential superiority of this QRP. So the benefit actually comes from the larger samples involved with this QRP, not the QRP per se.</p><disp-quote content-type="editor-comment"><p>10) In the Discussion the text suggests it will be difficult to increase replicability in fields with low base rates. To the contrary, I think it is easy: just increase the base rate. Scientists should do a better job picking hypotheses to test. They should not waste time testing hypotheses that would be surprising or counterintuitive. The text then goes onto discuss about how campaigns to reduce p-hacking may be ineffective. I get the point, but a field with a low base rate of hypotheses should have a low replication rate. Increasing replicability is not (or, should not) be the goal of scientific investigations.</p></disp-quote><p>We have sympathy with this comment and agree that researchers should prefer to test hypotheses deduced from a plausible theory. However, we also see practical constraints that make it difficult to increase base rates, contrary to the reviewer’s suggestion that this would be easy. For example, consider research in clinical pharmacology aiming at discovering better medicines, such as the search for an effective vaccine against an infectious disease. Although we are not pharmacologists, we can imagine that the search for such a vaccine can be very haphazard. Pharmacological research often tests many ineffective drugs before an effective one is discovered. In such areas, the base rate could necessarily be low and only increased by a better theoretical understanding of the disease and how drugs interact with it. When such understanding is difficult to achieve, some black-box approach and the associated low base rate may be the only option. This more philosophical issue is beyond the aim of our paper, which focuses on the question of why significant results often do not replicate, but we now comment on the potential difficulty of increasing base rate in the Discussion.</p><disp-quote content-type="editor-comment"><p>11) The authors are familiar with some of my work on this topic (they cite several of my papers). There, the problem is not a low replication rate, but a &quot;too high&quot; replication rate. The problem is that if both original and replication scientists are using QRPs, then the replication rate is too high, compared to what would be expected with &quot;good practice&quot; analyses/experiments. In my view, this is the more serious problem with current practice, because it implies that the Type I error rate is higher than &quot;good practice&quot;. This suggests that scientists are not doing what they intended to do. This different viewpoint struck me while reading the introduction of the paper. There it is noted that some people suggest that QRPs lead to low replication rates. But this claim never really made sense (at least not without more discussion) because QRPs increase the probability of rejecting the null; so QRPs increase the replication rate. Indeed, if the simulations were revised so that both the original and replication scientists used QRPs, there would be quite an increase in the replication rate, even when the true effect is 0.</p></disp-quote><p>We agree that an excess of positive results due to QRPs—in both original studies and replication attempts—is another potential problem that could influence the replication rate. As the reviewer notes, the replication rate will be unrealistically high if QRPs are used to induce a significant result in the replication data. This situation, however, is different from the situation in which an unbiased researcher tries to replicate the results of an original study without using QRPs, and this is the situation producing the empirically low replication rates that have alarmed many researchers. We have now made clear that our analysis focuses on this latter replication situation.</p></body></sub-article></article>