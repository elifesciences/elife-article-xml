<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">71627</article-id><article-id pub-id-type="doi">10.7554/eLife.71627</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Motor memories of object dynamics are categorically organized</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-243635"><name><surname>Cesanek</surname><given-names>Evan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5335-6604</contrib-id><email>evan.cesanek@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-158429"><name><surname>Zhang</surname><given-names>Zhaoran</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4192-4088</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-244403"><name><surname>Ingram</surname><given-names>James N</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2567-504X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-137212"><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2011-2790</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-33247"><name><surname>Flanagan</surname><given-names>J Randall</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2760-6005</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University</institution><addr-line><named-content content-type="city">New York, NY</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Neuroscience, Columbia University</institution><addr-line><named-content content-type="city">New York, NY</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Department of Psychology and Centre for Neuroscience Studies, Queen’s University</institution><addr-line><named-content content-type="city">Kingston, ON</named-content></addr-line><country>Canada</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Crossley</surname><given-names>Matthew</given-names></name><role>Reviewing Editor</role><aff><institution>Macquarie University</institution><country>Australia</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Ivry</surname><given-names>Richard B</given-names></name><role>Senior Editor</role><aff><institution>University of California, Berkeley</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>19</day><month>11</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e71627</elocation-id><history><date date-type="received" iso-8601-date="2021-06-24"><day>24</day><month>06</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-11-18"><day>18</day><month>11</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-07-14"><day>14</day><month>07</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.07.13.452183"/></event></pub-history><permissions><copyright-statement>© 2021, Cesanek et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Cesanek et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-71627-v3.pdf"/><abstract><p>The ability to predict the dynamics of objects, linking applied force to motion, underlies our capacity to perform many of the tasks we carry out on a daily basis. Thus, a fundamental question is how the dynamics of the myriad objects we interact with are organized in memory. Using a custom-built three-dimensional robotic interface that allowed us to simulate objects of varying appearance and weight, we examined how participants learned the weights of sets of objects that they repeatedly lifted. We find strong support for the novel hypothesis that motor memories of object dynamics are organized categorically, in terms of families, based on covariation in their visual and mechanical properties. A striking prediction of this hypothesis, supported by our findings and not predicted by standard associative map models, is that outlier objects with weights that deviate from the family-predicted weight will never be learned despite causing repeated lifting errors.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>object manipulation</kwd><kwd>mechanical properties</kwd><kwd>motor learning</kwd><kwd>predictive control</kwd><kwd>memory</kwd><kwd>categories</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS117699</award-id><principal-award-recipient><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U19NS104649</award-id><principal-award-recipient><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>Natural Sciences and Engineering Research Council of Canada</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Flanagan</surname><given-names>J Randall</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000024</institution-id><institution>Canadian Institutes of Health Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Flanagan</surname><given-names>J Randall</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The motor-relevant properties of the myriad objects with which we interact on a daily basis are encoded in memory using categorical representations, or 'object families'.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Many theories about how objects are encoded in memory have been proposed (<xref ref-type="bibr" rid="bib33">Collins and Quillian, 1969</xref>; <xref ref-type="bibr" rid="bib128">Warrington and Taylor, 1978</xref>; <xref ref-type="bibr" rid="bib97">Mervis and Rosch, 1981</xref>; <xref ref-type="bibr" rid="bib114">Schacter and Cooper, 1993</xref>; <xref ref-type="bibr" rid="bib52">Gauthier et al., 1999</xref>; <xref ref-type="bibr" rid="bib26">Chao et al., 1999</xref>; <xref ref-type="bibr" rid="bib27">Chao and Martin, 2000</xref>; <xref ref-type="bibr" rid="bib68">Humphreys and Forde, 2001</xref>; <xref ref-type="bibr" rid="bib48">Freedman et al., 2001</xref>; <xref ref-type="bibr" rid="bib10">Ashby and Maddox, 2005</xref>; <xref ref-type="bibr" rid="bib81">Kemp and Tenenbaum, 2008</xref>; <xref ref-type="bibr" rid="bib88">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib85">Kourtzi and Connor, 2011</xref>; <xref ref-type="bibr" rid="bib90">Mahon and Caramazza, 2011</xref>; <xref ref-type="bibr" rid="bib69">Huth et al., 2012</xref>). These include theories concerned with the semantic, perceptual, and functional properties of objects. For example, a hammer may be semantically labeled as a tool, represented perceptually in terms of its shape, or evaluated functionally in the context of a particular task. However, the mechanical properties of objects, which are fundamentally important to human motor control, have received little attention in theories of object memory.</p><p>The majority of tasks we perform involve physical objects, and skilled interaction with these objects depends critically on our ability to predict their mechanical properties. For many of the objects that we interact with, dexterous performance requires accurate predictions of weight (<xref ref-type="bibr" rid="bib54">Gordon et al., 1991</xref>; <xref ref-type="bibr" rid="bib131">Wolpert and Flanagan, 2001</xref>; <xref ref-type="bibr" rid="bib45">Flanagan et al., 2006</xref>; <xref ref-type="bibr" rid="bib77">Johansson and Flanagan, 2009</xref>). For example, when lifting an object from a surface, weight prediction allows us to produce the vertical forces required to raise the object smoothly. When lifting an object for the first time, people will estimate its weight based on visual information about its size and material properties (<xref ref-type="bibr" rid="bib55">Gordon et al., 1993</xref>; <xref ref-type="bibr" rid="bib43">Flanagan and Beltzner, 2000</xref>; <xref ref-type="bibr" rid="bib13">Baugh et al., 2012</xref>; <xref ref-type="bibr" rid="bib14">Baugh et al., 2016</xref>). However, once an object has been lifted, a memory is formed of its actual (i.e. directly sensed) weight, and this memory can be used to guide subsequent lifts of the object (<xref ref-type="bibr" rid="bib13">Baugh et al., 2012</xref>; <xref ref-type="bibr" rid="bib14">Baugh et al., 2016</xref>; <xref ref-type="bibr" rid="bib76">Johansson and Westling, 1988</xref>; <xref ref-type="bibr" rid="bib44">Flanagan et al., 2001</xref>; <xref ref-type="bibr" rid="bib46">Flanagan et al., 2008</xref>). Thus, in addition to intact sensory and motor function, skilled manipulation—and thus the ability to perform most daily tasks—requires the capacity to form, and quickly access, representations of object weights in memory.</p><p>Here, we investigated how the mechanical properties of the myriad objects we interact with are organized in memory. To answer this question, we used a new three-dimensional robotic interface (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) that, in combination with a stereoscopic virtual reality system, allowed us to simulate objects of varying size, weight, and appearance (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Objects were presented on a carousel and, on each trial, the participant ‘lifted’ the presented (i.e. nearest) object by first applying an upward force to the object, which was fixed to the surface of the carousel and therefore could not move. When ready, the participant pressed a button with their other hand, which caused the portion of the carousel below the object to open, releasing the object so that it was free to move. The aim was to match the upward force to the weight of the object so that it would not move up or down when released. Therefore, by measuring the force just prior to release, we could precisely measure the participant’s weight prediction on every trial. Because the robot simulated the mechanics of the object, the participant received direct haptic and visual feedback about both the object’s weight and their motor error (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). At the end of the trial, the open portion of the carousel closed, and the participant replaced the object.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Object families and associative maps make different predictions for an outlier lifting task.</title><p>(<bold>a</bold>) Participants grasped the handle of a three-dimensional robotic interface (3BOT) with their right hand and viewed stereoscopic scenes (Oculus Rift). The 3BOT could track movement and simulate the haptic experience of manipulating objects. (<bold>b</bold>) Screenshots of the key stages of the lifting task. See text for details. (<bold>c</bold>) Load force and vertical position traces from an example trial, color-coded to match the numbers in (<bold>b</bold>). In this example, the anticipatory force was less than the weight of the object (dotted line), causing a downward movement of the hand and object. (<bold>d-f</bold>) Tasks used to examine family representations. In these tasks there were five visually similar objects of varying volume and mass. In the Linear+ condition (<bold>d</bold>), four of the objects had a linear relation between size and weight. A fifth object of intermediate size had a higher density (hence the + notation) and therefore was an outlier. Under the object families hypothesis, the four objects induce learning of the family structure (green line). Visually similar objects that fall within the category boundary for the family (shaded green region) are treated as family members. Because the outlier falls within the category boundary, its weight should be persistently misestimated based on the family structure (green circle). Under the associative map hypothesis, exposure to the outlier leads to partial learning of its actual weight (purple circle). In the Linear++ condition (<bold>e</bold>), the object families hypothesis predicts that when the outlier becomes sufficiently extreme, and crosses the family boundary, it will be categorized as an individual and its weight fully learned. The associative map hypothesis still predicts partial learning of this outlier. In the Uncorr+ condition (<bold>f</bold>), when size and weight are uncorrelated, the object families hypothesis predicts that the object weights will each be learned individually. Under the associative map hypothesis, there is no fundamental difference between this scenario and those depicted in (<bold>d, e</bold>), so the predictions for learning are similar to the object families hypothesis (i.e. all five objects will eventually be learned).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-71627-fig1-v3.tif"/></fig><p>Using this task, we developed a novel motor learning paradigm in which participants repeatedly lifted a set of five similar-looking objects of varying size and weight (<xref ref-type="fig" rid="fig1">Figure 1d–f</xref>; filled circles correspond to the objects in <xref ref-type="fig" rid="fig1">Figure 1b</xref>). In our key experiment (<xref ref-type="fig" rid="fig1">Figure 1d</xref>), these objects included four training objects (the two smallest and two largest) presented in an initial training phase, and an outlier object (the middle size) introduced later in a test phase. The training objects had a common density, and therefore had a linear relationship between size and weight. Although the size of the outlier was in the middle of the training objects, its weight was greater than would be expected under the assumption that it had the same density as the training objects. Using this lifting task, we could distinguish between two high-level hypotheses about memory organization.</p><p>First, the ‘object families’ hypothesis asserts that multiple objects are represented in memory by clustering them into categories, or families. This hypothesis posits that the training objects and the outlier will be represented as a single family (<xref ref-type="fig" rid="fig1">Figure 1d</xref>; green line), provided that the weight of the outlier falls within the family boundary (shaded green region). As a consequence, this hypothesis predicts that participants will fail to learn the actual weight of an outlier that falls within the family boundary, and will instead estimate the weight based on the family structure (open green circle). We refer to this predicted effect as the ‘family effect’. However, if the weight of the outlier is extreme and falls beyond the family boundary (<xref ref-type="fig" rid="fig1">Figure 1e</xref>), a separate memory will be formed for the outlier object. Thus, this model predicts an all-or-nothing pattern of learning whereby, depending on their family boundary, a participant will either fully learn the outlier weight or completely fail to learn it.</p><p>An alternative hypothesis is that object properties are encoded in an ‘associative map’. This idea comes from a well-known theoretical framework that has been successful in explaining how sensorimotor transformations for reaching, grasping, and saccades are encoded in memory (<xref ref-type="bibr" rid="bib132">Zipser and Andersen, 1988</xref>; <xref ref-type="bibr" rid="bib113">Salinas and Abbott, 1995</xref>; <xref ref-type="bibr" rid="bib103">Pouget and Sejnowski, 1997</xref>). In associative map models (<xref ref-type="fig" rid="fig1">Figure 1d and e</xref>; purple curve), experience with individual objects causes the visual and mechanical properties sensed during each interaction to become gradually associated. Additionally, memories of individual objects influence one another only through local generalization, producing smoothly varying mappings between visual size and expected weight. In associative map models, the predicted weight of the outlier (open purple circle) will become increasingly accurate with experience, such that an outlier of any weight will be at least partially learned. Note that associative map learning is also implemented by modular models, which have been proposed to account for learning in both motor (<xref ref-type="bibr" rid="bib130">Wolpert and Kawato, 1998</xref>) and non-motor (<xref ref-type="bibr" rid="bib79">Kalish et al., 2004</xref>) tasks.</p><p>These two hypotheses also make different predictions regarding how lifting the outlier will affect the four training objects during the test phase. Again, the object families hypothesis predicts an all-or-nothing pattern, depending on how the outlier is encoded. When encoded as a family member, the unexpectedly heavy weight of the outlier updates the family representation, causing the predicted weight to increase on a subsequent lift of a training object. However, once the outlier is classified as a separate individual, this outlier-to-family updating should be greatly suppressed. The associative map hypothesis, on the other hand, predicts that lifting the outlier will always update the estimated weights of similar-looking training objects.</p><p>Finally, the two hypotheses also make different predictions when there is no structured relationship between size and weight (<xref ref-type="fig" rid="fig1">Figure 1f</xref>). Under the object families hypothesis, each of these objects is learned as an individual (<xref ref-type="fig" rid="fig1">Figure 1f</xref>; separate green lines) and, as a consequence, the training objects will be learned more slowly than when they are learned as a family and there will be minimal single-trial generalization from the ‘outlier’ to the training objects. In contrast, in an associative map model, this scenario does not fundamentally differ from those depicted in <xref ref-type="fig" rid="fig1">Figure 1d and e</xref>.</p><p>Consistent with the object families hypothesis, we show that participants encode objects that covary in size and weight as a family, and that this representation exerts a powerful family effect on outlier objects, whose weights can differ markedly from the weights predicted by the family. In particular, we show that participants can completely fail to learn the weight of an outlier object, despite experiencing large, repeated movement errors; errors that, in the absence of the family, quickly drive learning. These findings address, for the first time, how motor-relevant properties of multiple objects are represented in memory.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Participants performed a lifting task in which they were required to predict the weights of five objects positioned around a carousel. <xref ref-type="fig" rid="fig1">Figure 1c</xref> shows the load force and vertical hand position in a single trial. The traces are color-coded to match the four trial phases depicted in <xref ref-type="fig" rid="fig1">Figure 1b</xref> and described above. We focused our analyses on the anticipatory force participants produced just prior to releasing the object by pressing a button with the non-lifting hand. This anticipatory force provides a precise and accurate measure of the participant’s motor memory of the object weight. In the trial shown in <xref ref-type="fig" rid="fig1">Figure 1c</xref>, the participant underestimated the weight of the object, and as a consequence when the participant pressed the button to release the object, the right hand and the object moved downward. (Note that the motion of the hand after the release of the object does not provide a robust measure of participants’ weight prediction because this motion depends on co-contraction and reflex responses in addition to the mismatch between vertical force and weight.)</p><sec id="s2-1"><title>Motor memories of objects are organized categorically</title><p>Our initial experiment was designed to critically evaluate the object families and associative map hypotheses by examining how participants learned the weight of a heavier-than-expected outlier object. We tested separate groups of participants in the three experimental designs depicted in <xref ref-type="fig" rid="fig1">Figure 1d–f</xref>. Participants completed a training phase, in which they interacted with the four training objects, followed by a test phase, in which the fifth test object was added. All objects were visually similar—cylinders of fixed diameter with varying heights.</p><p>In the Linear+ group (<xref ref-type="fig" rid="fig1">Figure 1d</xref>), the weights of the training objects were <italic>linearly</italic> related to their sizes and the test object was <italic>heavier</italic> (as denoted by the + sign) than expected based on the training objects. The weights and sizes of the training objects ranged from 0.6–1.2 kg and 400–800 cm<sup>3</sup>, respectively, and all had a density of 1.5 g/cm<sup>3</sup> (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). The size of the test object was 600 cm<sup>3</sup>, which was in the middle of the range of training object sizes. However, the weight of the test object, 1.2 kg, was equal to the heaviest training object, making it 0.3 kg greater than the weight that would be expected if it had the same density as the training objects.</p><p>The traces in <xref ref-type="fig" rid="fig2">Figure 2a</xref> show the anticipatory force generated for each object as a function of trial cycle (one lift of each object) across the training and test phases. The dotted horizontal lines (color-matched to the force traces) show the weights of the objects, and therefore the ideal anticipatory forces that would be generated with perfect learning. Participants in the Linear+ group very quickly learned the weights of the training objects. The scaling of forces to object weight observed in the first trial cycle suggests that participants rapidly learned the density (a family-level parameter) based on the first few objects lifted and then used this information, in conjunction with size, to predict the weights of the other objects. At the end of the training phase (final eight cycles), anticipatory force was strongly correlated with object weight (<italic>r</italic> = 0.76, 95% CI = [0.66, 0.83]).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Objects are encoded according to the object families hypothesis.</title><p>(<bold>a</bold>) Trial-by-trial anticipatory forces for the five objects over the course of the Linear+ condition (mean ± SEM). The training objects (thin lines) are experienced from the first trial cycle and the test object (thick line) is introduced on trial cycle 31 as the first trial of each cycle. Traces are color-coded with darker shades indicating larger objects and the dashed lines indicate the associated actual object weights (thick dashed line shows outlier weight). Rest breaks are indicated by gaps in the traces. (<bold>b</bold>) Anticipatory forces at the end of the test phase for the Linear+ condition (mean ± SEM). The abscissa shows the weights of the training objects and, for the outlier, the expected weight based on the family density. The weights of the training objects lie on the dotted unity line. Dashed horizontal line shows the weight of the outlier. Regression line shows the average of the participants’ linear regressions ± SEM. (<bold>c, d</bold>) Same as (<bold>a, b</bold>) for the Linear++ condition. (<bold>e, f</bold>) Same as (<bold>a, b</bold>) for the Uncorr+ condition. Note that for each participant, the uncorrelated mapping of size and weight for the training objects was randomly selected; the shading in (<bold>e</bold>) and (<bold>f</bold>) depicts one mapping. In (<bold>f</bold>) the outlier is plotted at the expected weight based on the family density in the Linear conditions. (<bold>g</bold>) Single-trial generalization in the first four cycles (Early) and last sixteen cycles (End) of the test phase of the Linear+ condition (mean ± SEM, see Materials and methods for details). (<bold>h, i</bold>) Same as (<bold>g</bold>) for the Linear++ and Uncorr+ conditions. (<bold>j</bold>) Response times averaged over objects in each trial cycle (mean ± SEM). The Linear+ and Linear++ groups are combined in the red trace, as they did not differ on this measure. All SEM are across participants.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Source data for Figure 2 (Linear+, Linear++, and Uncorr+ groups).</title></caption><media mimetype="application" mime-subtype="txt" xlink:href="elife-71627-fig2-data1-v3.txt"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-71627-fig2-v3.tif"/></fig><p>The thicker trace and dashed horizontal line, starting at trial cycle 31, show the anticipatory force and actual weight of the test object introduced in the test phase. On the first lift of the test object, the average anticipatory force was 9.00 N (95% CI = [7.68, 10.32]). This suggests that participants initially estimated that the test object would have the same density as the training objects and, therefore, that its weight would be close to the middle of the training object weights (8.83 N). Consequently, they experienced an error of approximately 300 g (~3 N), which is close to the weight of a full can of soda and represents fully a third of the anticipated weight. Remarkably, despite this large error, participants never learned the test object weight over the 40 cycles in the test phase (40 lifts of the test object interspersed with 40 lifts of each training object). That is, the average anticipatory force did not increase—remaining at the level predicted by the family—and, therefore, participants did not adapt to the actual weight of this pronounced outlier.</p><p>We calculated the anticipatory forces at the end of the test phase (final 16 cycles) as a function of mass for the four training objects, and as a function of expected mass based on the density of the training objects for the test object (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). To assess learning at the end of the test phase, we compared the average anticipatory force produced for the test object (9.15 N, 95% CI = [8.27, 10.03]) with the ‘family-predicted weight’ of the test object (9.09 N, 95% CI = [8.64, 9.54]), defined as the weight of the test object predicted from the best-fitting regression line through the training objects (thereby adjusting for any prediction error on the training objects). We found that the anticipatory force was not significantly greater than the family-predicted weight (<italic>t</italic>(13) = 0.17, p = 0.43).</p><p>The above results support the object families hypothesis by showing that even when the weight of an outlier object deviates markedly from its family-predicted weight, it continues to be encoded as a family member despite sensory evidence to the contrary. Next, we investigated whether there is a threshold to the family effect. We hypothesized that when the discrepancy between actual and family-predicted weight exceeds some threshold, the object will be encoded as an individual, separate from the family, despite its family-like appearance. To probe this threshold, we tested a Linear++ group, who completed the same task as the Linear+ group but with an <italic>even heavier</italic> outlier (hence the ++). Specifically, for the Linear++ group, the test object weighed 1.5 kg, making it 600 g heavier than if it had the same density as the training objects, and 300 g heavier than the heaviest training object (<xref ref-type="fig" rid="fig1">Figure 1e</xref>).</p><p><xref ref-type="fig" rid="fig2">Figure 2c</xref> shows the average anticipatory force timelines for the Linear++ group. As expected, at the end of the training phase, anticipatory force was strongly correlated with object weight (<italic>r</italic> = 0.85, 95% CI = [0.72, 0.92]). On the first lift of the test object, participants generated an average anticipatory force of 8.13 N (95% CI = [7.19, 9.08]), consistent with the density of the training objects. However, in contrast to the Linear+ group, over the following 5–10 cycles, participants increased their anticipatory force for the test object, reaching an asymptote just below the actual object weight (14.72 N). At the end of the test phase (<xref ref-type="fig" rid="fig2">Figure 2d</xref>), the anticipatory force for the test object (13.15 N, 95% CI = [11.56, 14.74]) was significantly greater (<italic>t</italic>(8) = 3.34, p = 0.0051) than the family-predicted weight (9.65 N, 95% CI = [8.62, 10.68]).</p><p>The results of the Linear++ group demonstrate that there is a limit to how deviant an outlier object can be, with respect to a known family, before it is ‘kicked out’ of that family and learned as a unique individual. That is, when the error signals received from a particular object are sufficiently large, they promote the formation of a separate memory. Note that the adaptation to the test object in the Linear++ group demonstrates that participants could visually distinguish the test object from the neighboring training objects. Thus, we can conclude that the striking failure to learn the test object in the Linear+ group is not due to an inability to visually identify the test object amongst the similar-looking training objects.</p><p>Lastly, we designed a third variant of the task, in which the test object was the same size and weight as in the Linear+ group but the training objects were not related by any family structure (<xref ref-type="fig" rid="fig1">Figure 1f</xref>). Specifically, in the Uncorr+ group, the sizes and weights were remapped (separately for each participant), such that size and weight of the training objects were either completely or close to completely uncorrelated (|<italic>r</italic>| &lt; 0.3). The object families hypothesis makes two key predictions for this condition. First, in the absence of structured covariation between visual and mechanical properties within the training set (i.e. when the training objects do not share a constant density), participants should be forced to form a separate memory for each training object, with no family-level representation. This, in turn, should result in slower initial learning of the training objects in comparison to the Linear groups, where all four training objects could be encoded as a family with a common density. Second, in the absence of a family representation, participants in the Uncorr+ group should be able to learn the weight of the 1.2 kg test object, unlike participants in the Linear+ group. In contrast, under the associative map hypothesis, the results of the Uncorr+ group should not fundamentally differ from the Linear+ group.</p><p><xref ref-type="fig" rid="fig2">Figure 2e</xref> shows the anticipatory force timelines for the Uncorr+ group. In the earliest trial cycles, there was poor differentiation of the object weights, showing that uncorrelated mappings are more difficult to learn than linear mappings. Nevertheless, by the end of the training phase the Uncorr+ group achieved accuracy comparable to the Linear groups, with anticipatory force being strongly correlated with object weight (<italic>r</italic> = 0.72, 95% CI = [0.62, 0.80]). On the first lift of the test object, participants produced 8.85 N (95% CI = [7.62, 10.08]) of anticipatory lift force, which is similar to the mean of the training object weights (8.83 N). Moreover, it is similar to the force generated by participants in the Linear+ group on their first lift of the test object (9.00 N). Thus, the initial weight estimation error for the test object was similar in the Linear+ and Uncorr+ groups. However, as can be seen in <xref ref-type="fig" rid="fig2">Figure 2e</xref>, during the test phase participants in the Uncorr+ group succeeded in adapting their anticipatory force for the test object. Unlike the Linear groups, the training objects in the Uncorr+ group did not have a common density, and therefore we compared the anticipatory force for the test object to the average weight of the training objects (as the test object was of intermediate volume). At the end of the test phase (<xref ref-type="fig" rid="fig2">Figure 2f</xref>), participants’ anticipatory force for the test object (10.48 N, 95% CI = [9.50, 11.46]) was significantly greater (<italic>t</italic>(11) = 4.06, p = 0.00094) than the average force for the training objects (8.68 N, 95% CI = [8.44, 8.92]). The learning of the test object observed in the Uncorr+ group confirms that the failure to learn the test object in the Linear+ group is due to the structured object family, rather than the lack of a sufficient error signal.</p><p>The object families hypothesis predicts that when lifting an object that is encoded as a family member, the experienced density will update the density estimate for the family, thereby biasing the anticipatory force on a subsequent lift of a training (i.e. family) object. Conversely, when lifting a test object that is encoded as an individual, the experienced density will not update the family estimate and the anticipatory force on a subsequent lift of a training object will be unaffected. Thus, at the end of the test phase, the object families hypothesis predicts strong generalization for the 1.2 kg outlier, but no generalization for the 1.5 kg outlier. In contrast, the associative map model predicts strong generalization for the 1.2 kg outlier, and even stronger generalization for the 1.5 kg outlier. To compare these predictions, we analyzed single-trial generalization at the start and end of the test phase (<xref ref-type="fig" rid="fig2">Figure 2g–i</xref>). Specifically, we examined how the anticipatory force applied to these training objects changed when they were lifted immediately after the test object, compared to when they were lifted in the final four trial cycles of the training phase, before the test object was introduced. Note that we opted to analyze only the trials immediately following the test object because generalization is washed out with each subsequent lift of a training object, such that including these subsequent trials weakens the analysis. For the Linear+ group (<xref ref-type="fig" rid="fig2">Figure 2g</xref>), we found significant generalization both at the start (<italic>t</italic>(13) = 5.47, p = 1.1e−4) and the end of the test phase (<italic>t</italic>(13) = 5.56, p = 9.2e−5), with no significant change (<italic>t</italic>(13) = 1.68, p = 0.12). That is, at both time points, there was an increase in anticipatory force on the trial after the test object, consistent with encoding the test object as a family member. For the Linear++ group (<xref ref-type="fig" rid="fig2">Figure 2h</xref>), there was significant generalization at the start of the test phase (<italic>t</italic>(8) = 5.61, p = 5.1e−4) that was greatly reduced at the end of the test phase, with a significant change over time (<italic>t</italic>(8) = 3.95, p = 0.0042). This shows that participants initially encoded the extreme outlier as a family member, but then formed a separate memory of this object. Note that although generalization was dramatically reduced in the Linear++ group, it remained significant at the end of the test phase (<italic>t</italic>(8) = 2.59, p = 0.032). For the Uncorr+ group (<xref ref-type="fig" rid="fig2">Figure 2i</xref>), we found no evidence of generalization at the start (<italic>t</italic>(11) = 1.90, p = 0.085) or the end of the test phase (<italic>t</italic>(11) = 0.45, p = 0.66), and no change over time (<italic>t</italic>(11) = 1.90, p = 0.084), consistent with encoding each object individually (<xref ref-type="fig" rid="fig2">Figure 2i</xref>).</p><p>We also analyzed the response time, defined as the time from object presentation to the button press that released the object, which is presumably linked to the time required to estimate the weight of the object. For this analysis, we combined the two Linear groups. As shown in <xref ref-type="fig" rid="fig2">Figure 2j</xref>, response times decreased during the training phase for both the linear and the uncorrelated size-weight mappings, but there was a consistent temporal cost associated with movement preparation when size and weight were uncorrelated as compared to linearly related. To assess these effects, we defined four epochs by splitting both the training and test phases into two equal parts. A two-way repeated-measures ANOVA on log-transformed response times revealed significant main effects of Group (<italic>F</italic>(1, 33) = 5.79, p = 0.022) and Epoch (<italic>F</italic>(3, 99) = 13.039, p = 0.30e−7), but no interaction (<italic>F</italic>(3, 99) = 0.80, p = 0.49). Separate <italic>t</italic>-tests on each epoch all showed significant Group effects (p &lt; 0.048 in all four epochs). These results show that, even at the end of the test phase, encoding each object individually resulted in a temporal cost compared to encoding the objects as a family.</p><p>The increased response time in the Uncorr+ condition could be a ‘switch cost’ related to loading a different category in motor memory. If there is a switch cost then we should observe an increase in response time in the Linear++ condition when lifting the outlier after lifting a family member, and when lifting a family member after the outlier, as both of these situations involve switching between categories. However, we found no evidence of increased response times on these ‘switch trials’. We first determined the predicted response times assuming there is no switch cost. These predictions were based on the response times observed when lifting a family member after having lifted a family member in the previous trial, that is, where the category did not change (see Materials and methods). In the Linear++ condition, the average response time when lifting the outlier after a family member group was only 3 ms longer (95% CI = [−156, 159]) than the predicted value, and the average response time when lifting a family member after the outlier was 25 ms faster (95% CI = [−129, 79]) than the predicted value. Thus, our data provide no evidence for the idea that the large temporal cost observed in the Uncorr+ condition is a switch cost. However, our data are consistent with the idea that there is a ‘selection cost’ associated with mapping the visual stimulus onto the appropriate category. In the Uncorr+ group, participants were required to map each visual stimulus onto one of five categories in memory, and thus we would expect a substantial selection cost. In contrast, in the Linear++ condition, only two categories were involved, and therefore the selection cost may be negligible.</p></sec><sec id="s2-2"><title>Re-organization of motor memories of objects</title><p>In the experiment described above, for the Linear groups we first introduced a set of objects with a common density, before adding in a test object, or outlier, with a higher density. We found a strong family effect such that participants never learned the weight of a test object that was 300 g heavier than expected. A key question is whether exposure to an object family can lead to the reorganization of an existing memory of an individual object. To address this question, we tested two new groups of participants on conditions in which the test object was experienced <italic>before</italic> the four common-density ‘family’ objects. Note that we used the same family and test objects as in our first experiment. We refer to these groups as the +Linear and ++Linear groups to denote the reversed order in which participants encountered the test object and the family objects. In the initial training phase, participants in the +Linear group lifted the 1.2 kg test object, and the ++Linear group lifted the 1.5 kg test object. For both groups, the four family objects were then introduced in the test phase.</p><p>As expected, both groups quickly and accurately learned the weight of the test object when it was presented individually during the training phase (<xref ref-type="fig" rid="fig3">Figure 3a and c</xref>). However, at the start of the test phase (beginning at trial cycle 31), it is evident that participants in both groups began to treat the outlier and the four family objects as a single family. Specifically, the estimated weight of the test object (i.e. the anticipatory force) decreased towards the family-predicted weight. At the same time, the estimated weights of the family members were initially overestimated. These results show that even brief exposure to an object family can reorganize the memory of a previously learned individual object, such that it is assimilated into the family.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>A memory of an individual is reorganized when an object family is introduced.</title><p>(<bold>a, c</bold>) Trial-by-trial anticipatory forces, as in <xref ref-type="fig" rid="fig2">Figure 2a and c</xref>, in a ‘reverse’ condition in which the outlier object was learned during the initial training phase, and the family objects were only introduced from trial cycle 31. Hence, we refer to these as +Linear and ++Linear. As the training phase trial cycles contained only one trial (the outlier), for clarity, the abscissa scale is compressed. After the test phase, in a ‘1:1’ phase the test object was presented four times in each trial cycle (rather than once as in the test phase), with each family member presented once (eight trials per cycle) such that the participant experienced the test object as often as a family member. For the 1:1 phase, we excluded trials from analysis in which the outlier object followed itself. (<bold>b, d</bold>) Average anticipatory forces at the end of the test phase, as in <xref ref-type="fig" rid="fig2">Figure 2b and d</xref>, but here plotted by volume.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Source data for Figure 3 (+Linear and ++Linear groups).</title></caption><media mimetype="application" mime-subtype="txt" xlink:href="elife-71627-fig3-data1-v3.txt"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-71627-fig3-v3.tif"/></fig><p>Following this assimilation of the test object, or outlier, into the family, the pattern of results is strikingly similar to that observed in our first experiment. Specifically, participants in the +Linear group never fully re-learned the actual weight of the outlier, whereas participants in the ++Linear group adapted their anticipatory force to the actual weight. At the end of the test phase (<xref ref-type="fig" rid="fig3">Figure 3b</xref>), the anticipatory force for the outlier in the +Linear group (10.01 N, 95% CI = [9.18, 10.84]) was not significantly greater (<italic>t</italic>(10) = 1.23, p = 0.12) than the family-predicted weight (9.49 N, 95% CI = [9.11, 9.86]). Thus, participants in the +Linear group did not re-learn the actual weight of the outlier after it was assimilated into the family. Therefore, the +Linear group, like the Linear+ group, exhibited a strong family effect. In the ++Linear group, the anticipatory force for the outlier at the end of the test phase (12.76 N, 95% CI = [11.14, 14.39]) was significantly greater (<italic>t</italic>(10) = 4.19, p = 0.00093) than the family-predicted weight (9.76 N, 95% CI = [9.47, 10.04]). Thus, as was the case for the Linear++ group, the ++Linear group exhibited learning (or re-learning) of the more extreme outlier.</p><p>The failure to learn the weight of the outlier in the Linear+ and +Linear groups could be due to the fact that the higher density outlier was lifted only once for every four lifts of the family objects. Thus, after the test phase we included a ‘1:1’ phase where the relative frequency with which the outlier and family objects were experienced was equivalent. Specifically, this phase consisted of ten cycles in which the outlier object was lifted four times per cycle and each family member was lifted only once, for a total of eight lifts per cycle with the outlier and family members randomly interleaved. As shown in <xref ref-type="fig" rid="fig3">Figure 3a</xref>, in the +Linear group there was minimal impact on learning in the 1:1 phase. In the ++Linear group, increasing the relative frequency of outlier lifts in the 1:1 phase did not further improve the separation between the anticipatory force for the outlier and its family-predicted weight. These findings demonstrate that the family effect cannot be accounted for by the greater relative frequency of the family objects.</p></sec><sec id="s2-3"><title>Category boundaries are flexible</title><p>In the first two experiments, we showed that participants failed to learn the weight of a test object, or outlier, that was 300 g (or 33%) greater than the weight predicted by the density of the family, but did learn the weight when the test object exceeded this weight by 600 g (or 67%). This suggests that there is a boundary, between these two weights, that determines whether the object is encoded as a family member or as a separate individual. A fundamental question is whether such boundaries are fixed or flexible. Research on both perceptual and conceptual categorization has shown that category boundaries may depend on within-category variability (<xref ref-type="bibr" rid="bib109">Rips, 1989</xref>; <xref ref-type="bibr" rid="bib70">Huttenlocher et al., 2000</xref>; <xref ref-type="bibr" rid="bib31">Clayards et al., 2008</xref>), and that category labeling can exhibit hysteresis whereby the point at which the perceived category changes depends on the direction of change (<xref ref-type="bibr" rid="bib129">Williams et al., 1986</xref>; <xref ref-type="bibr" rid="bib63">Hock et al., 1993</xref>; <xref ref-type="bibr" rid="bib101">Poltoratski and Tong, 2014</xref>). To examine this issue in relation to object categorization, we recruited two new groups of participants who initially experienced the same conditions as the Linear+ and Linear++ groups from our first experiment. That is, both groups completed a training phase in which they lifted the four family objects, followed by a test phase in which the test object was initially either 1.2 or 1.5 kg for 20 trial cycles. However, we then gradually changed the test object’s weight by steps of 50 g every eight trial cycles. In the Linear➚ group, the weight was gradually increased from 1.2 to 1.5 kg and in the Linear➘ group, the weight was gradually decreased from 1.5 to 1.2 kg.</p><p>The anticipatory force data for the Linear➚ and Linear➘ groups (<xref ref-type="fig" rid="fig4">Figure 4a and c</xref>) contain several features that replicate the key findings from our first experiment. First, both groups quickly and accurately learned the weights of the training objects, with anticipatory forces that were strongly correlated with actual object weights by the end of the training phase (<italic>r</italic> = 0.81, 95% CI = [0.73, 0.87] in Linear➚; <italic>r</italic> = 0.84, 95% CI = [0.77, 0.89] in Linear➘). Second, in both groups the anticipatory force generated on the first lift of the test object was close to the middle of the weights of the family objects (9.40 N, 95% CI = [8.10 10.70] in Linear➚; 8.07 N, 95% CI = [5.93, 10.22] in Linear➘). Third, at the end of the initial 20 cycles of the test phase, during which the test object weight remained at its initial value, learning of the 1.2 kg test object was not significant (Linear➚: <italic>t</italic>(8) = −0.58, p = 0.71), whereas learning of the 1.5 kg test object was significant (Linear➘: <italic>t</italic>(8) = 2.15, p = 0.032).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Family boundary depends on history of sensorimotor experience.</title><p>(<bold>a</bold>) Trial-by-trial anticipatory forces (same format as <xref ref-type="fig" rid="fig2">Figure 2a</xref>) in an ‘increasing’ condition (Linear➚) in which the outlier starts at the weight of the Linear+ group on trial cycle 31 and increases gradually to the weight of the Linear++ condition. (<bold>b</bold>) Anticipatory forces at the end of the test phase (same format as <xref ref-type="fig" rid="fig2">Figure 2b</xref>). (<bold>c,d</bold>) Same as (<bold>a,b</bold>) for a ‘decreasing’ condition (Linear➘) in which the outlier starts at the weight of the Linear++ condition and decreases gradually to the weight of the Linear+ condition.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Source data for Figure 4 (Linear➚ and Linear➘ groups).</title></caption><media mimetype="application" mime-subtype="txt" xlink:href="elife-71627-fig4-data1-v3.txt"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-71627-fig4-v3.tif"/></fig><p>For the Linear➚ group, the anticipatory force for the test object does appear to have slightly increased as its weight increased. However, the anticipatory force at the end of the test phase (11.02 N, 95% CI = [9.44, 12.60]) was not significantly greater (<italic>t</italic>(8) = 1.81, p = 0.054) than the family-predicted weight (9.72 N, 95% CI = [9.43 10.01]), and was still substantially less than the actual weight (14.72 N; <xref ref-type="fig" rid="fig4">Figure 4b</xref>). Thus, despite the fact that the test object weighed 1.5 kg at the end of the test phase, it was not ‘kicked out’ of the family, in contrast to the equally heavy test object experienced by the Linear++ group in our first experiment. A direct comparison between the Linear➚ and Linear++ groups showed a significant difference in the anticipatory force for the outlier object at the end of the test phase (<italic>t</italic>(16) = 2.20, p = 0.043).</p><p>As noted above, and as expected based on the Linear++ group, participants in the Linear➘ group increased their anticipatory force for the 1.5 kg test object from the start of the test phase, before its weight began decreasing. Then, as the anticipatory force increased and the actual weight of the test object gradually decreased, these two forces became closely matched, and remained so until the end of the test phase (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). At the end of the test phase, the anticipatory force (11.69 N, 95% CI = [10.35, 13.03]) was significantly greater (<italic>t</italic>(8) = 4.35, p = 0.0012) than the family-predicted weight (9.20 N, 95% CI = [8.65, 9.75]) and indistinguishable from the actual weight (11.77 N; <xref ref-type="fig" rid="fig4">Figure 4d</xref>). Thus, once a separate memory was formed for the test object, it continued to be encoded as an individual even when its weight deviation decreased to the level ( + 300 g, or 33%) that the Linear+ group failed to learn. A direct comparison between the Linear➘ and Linear+ groups showed a significant difference in the anticipatory force for the outlier object at the end of the test phase (<italic>t</italic>(21) = −3.68, p = 0.0014). Overall, the results from both groups demonstrate that the threshold for categorizing an object as either a family member or an individual object is flexible and depends on past sensorimotor experience. Mechanisms that could potentially give rise to these effects are discussed below.</p></sec><sec id="s2-4"><title>All-or-nothing learning of outlier weight</title><p>According to the object families hypothesis, an outlier object is encoded categorically as either a family member or an individual. As a consequence, a given participant should either fully learn the weight of an outlier object or not learn at all, depending on their particular threshold for ‘kicking out’ an object from a family. Assuming that the threshold weight at which an outlier is kicked out of a family varies across participants, the object families hypothesis predicts that for certain outliers, there will be a bimodal distribution of estimated weights across participants (separating learners from non-learners). In contrast, the associative map hypothesis predicts that partial learning will be observed and that, assuming learning rates across participants are normally distributed, there will be a unimodal distribution in the amount of learning, regardless of the weight of the outlier.</p><p>With the aim of examining distributions across participants, we performed a web-based experiment in which we recruited a large number of participants (N = 196), divided into four groups that varied in how the outlier deviated from a linear family. As in our first experiment, we tested groups who were presented with an outlier object that was heavier (Linear+) or much heavier (Linear++) than the weight predicted by the density of the training objects. In addition, to assess the generality of our findings, we tested groups who were presented with an outlier that was lighter (Linear-) or much lighter (Linear--) than the weight predicted by the density of the training objects.</p><p>Based on the object families hypothesis, we expected that the participants in the groups with less deviant outliers (Linear+ and Linear-) would form a single distribution of non-learners, with anticipatory forces centered on the family-predicted weight. In contrast, we predicted that participants in the more deviant outlier groups (Linear++ and Linear--) would cluster into distinct distributions of learners and non-learners, with anticipatory forces centered on the actual and family-predicted weights of the outlier, respectively.</p><p>The web-based task was designed to closely mirror the laboratory task. The visual scene consisted of five cylindrical objects each with a spring attached to its top (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). The objects were clamped in place by a ring that rotated before each trial to bring one of the objects to the foremost position. Participants used their mouse or trackpad to stretch the spring upwards in an attempt to generate a lifting force on the object that matched its weight (trial phase 1). Then, they pressed a key with their other hand to release the clamp (trial phase 2). From this point on, the object’s motion was simulated as a mass-spring-damper system, thus providing visual feedback about the participant’s performance. If the spring was stretched too much (or too little), the object would rise (or fall) and then oscillate until coming to rest (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, rightmost panel). The oscillation time depended on the mismatch between the estimated and actual object weight, creating a natural time penalty. Note that although the exact nature of the sensory information in the web-based task differs from the laboratory task, both are fundamentally motor control tasks as they test how individuals translate sensory information into continuous motor commands to achieve an action goal.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Individual differences show that outliers are either fully learned or not learned at all.</title><p>(<bold>a</bold>) Web-based lifting experiment. (1) Five visually similar objects were clamped onto a ring, which rotated to bring the target object to the front. Participants clicked and dragged upward using their mouse or trackpad to stretch a spring, thereby applying a lifting force to the object. (2) When ready, they pressed a key on the keyboard with their other hand to release the object from the ring. The object and spring were simulated as a mass-spring-damper providing visual feedback about performance, with greater errors giving rise to larger oscillations, which also took longer to decay. As in the laboratory experiments, the goal was to prevent the object from moving after the key press. Right column shows the spring length (i.e. lift force, gray) and object position (orange) traces for an example trial in which the anticipatory force was less than the object weight. (<bold>b, e, h, k</bold>) Trial-by-trial anticipatory forces (formatted as in <xref ref-type="fig" rid="fig2">Figure 2a</xref>) for four conditions: two with a heavy outlier (Linear+ and Linear++, as in <xref ref-type="fig" rid="fig2">Figure 2</xref>) and the others with a lighter (Linear-) or much lighter (Linear--) outlier. (<bold>c, f, i, l</bold>) Histograms show the distribution across participants of the average anticipatory force for the outlier object at the end of the test phase. Blue and green curves show the fits of a single-Gaussian and a two-Gaussian mixture model, respectively. (<bold>d, g, j, m</bold>) Anticipatory forces at the end of the test phase (as in <xref ref-type="fig" rid="fig2">Figure 2b</xref>). The mean of each Gaussian component of the two-Gaussian mixture model is plotted as a green square, with standard error estimated via parametric bootstrap.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Source data for Figure 5 (web-based experiment).</title></caption><media mimetype="application" mime-subtype="txt" xlink:href="elife-71627-fig5-data1-v3.txt"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-71627-fig5-v3.tif"/></fig><p>The results for the Linear+ and Linear++ groups in the web-based experiment (<xref ref-type="fig" rid="fig5">Figure 5b and e</xref>) were very similar to those observed for the corresponding groups in our first experiment. This indicates that similar learning processes were engaged despite the use of visual dynamics without haptic feedback (<xref ref-type="bibr" rid="bib38">Danion et al., 2012</xref>). On average, the Linear+ group did not learn the outlier, whereas the Linear++ group exhibited substantial, but not complete, learning. Our analysis, however, focused on the distributions of anticipatory forces for the outlier object at the end of the test phase (final five cycles) across participants in each group (<xref ref-type="fig" rid="fig5">Figure 5c and f</xref>). For each distribution, we fit a single-Gaussian and a two-Gaussian mixture model (blue and green curves, respectively). To compare these models, we computed the difference in the Akaike Information Criteria (ΔAIC), with positive values in favor of the two-Gaussian mixture, and we report the relative likelihood for the favored model. As expected, for the Linear+ group, in which learning of the weight of the outlier was not observed, the single-Gaussian model was favored (ΔAIC = −4.6; relative likelihood = 10.0). In contrast, for the Linear++ group, the distribution was clearly bimodal, separating participants who either did or did not learn the outlier weight. This bimodal distribution was better captured by the two-Gaussian model (ΔAIC = 7.0, relative likelihood = 33.1).</p><p>For the Linear+ and Linear++ groups, the average anticipatory forces applied to the five objects at the end of the test phase are shown by the filled circles in <xref ref-type="fig" rid="fig5">Figure 5d and g</xref>. The mean of each Gaussian component of the two-Gaussian mixture is shown as a green square. In the Linear++ group, the greater of these two means (8.48 N, 95% CI = [7.98, 8.89])—representing the learners—lies almost perfectly on the actual outlier weight (dashed line, 8.83 N), whereas the lesser of the two means (5.43 N, 95% CI = [4.91, 6.08])—representing the non-learners—is very close to the family-predicted weight (4.91 N, 95% CI = [4.79, 5.03]). Surprisingly, although the single-Gaussian model was favored for the Linear+ group, one can nevertheless see two peaks in the two-Gaussian model (6.59 N, 95% CI = [5.58, 7.18] and 4.93 N, 95% CI = [3.66, 5.28]) that, respectively, closely match the actual weight (6.87 N) and family-predicted weight (4.93 N, 95% CI = [4.72, 5.13]) of the outlier. Thus, while most participants in the Linear+ group did not learn the outlier weight at all, there was a small subgroup who fully learned this weight.</p><p>The same pattern of results was observed for the Linear- and Linear-- groups (<xref ref-type="fig" rid="fig5">Figure 5i–j and l–m</xref>). For the Linear- group, the distribution of anticipatory forces for the outlier object at the end of the test phase were best fit by the single-Gaussian model (ΔAIC = −3.7, relative likelihood = 6.4), whereas the two-Gaussian model was preferred for the Linear-- group (ΔAIC = 29.3, relative likelihood = 2.3e + 6). For the Linear-- group, the means of the two components of the two-Gaussian model (1.05 N, 95% CI = [0.91, 1.22] and 4.10 N, 95% CI = [3.47, 4.68]) were, respectively, very close to the actual weight (0.98 N) and family-predicted weight (4.58 N, 95% CI = [4.36 4.79]) of the outlier. As was the case for the Linear+ group, the two-Gaussian mixture model fit to the Linear- group picked out a cluster of non-learners and a smaller cluster of learners, whose means (3.08 N, 95% CI = [2.75, 3.77] and 4.26 N, 95% CI = [4.07, 4.73]) respectively correspond to the actual weight (2.94 N) and family-predicted weight (4.66 N, 95% CI = [4.52, 4.79]) of the outlier. Overall, the results of this large-sample web-based experiment clearly support the object families hypothesis over the associative map hypothesis. At the level of single participants, the outlier was either encoded as a family member, in which case lift errors were ignored, or it was identified as a distinct individual, in which case lift errors drove complete learning of the outlier’s weight.</p><p>Notably, similar bimodality was also observed in the laboratory experiments. Revisiting these data, we applied the same mixture model analysis to individual participants’ final outlier learning (i.e. the difference between the anticipatory force produced for the outlier and the family-predicted weight of the outlier). To obtain sufficient sample sizes to fit the models, we combined participants from the Linear+ and +Linear groups, and from the Linear++ and ++Linear groups. The two-Gaussian model outperformed the single-Gaussian model for the combined Linear+ and +Linear group (ΔAIC = 13.3, relative likelihood = 772.8) and the resulting clustering yielded 20 non-learners and five learners. In the combined Linear++ and ++Linear group, the two-Gaussian model outperformed the single-Gaussian model (ΔAIC = 9.8, relative likelihood = 134.3), with the resulting clustering yielding six non-learners and 14 learners. When considering only the non-learners in the combined Linear+ and +Linear group, the anticipatory force produced for the outlier at the end of the test phase (8.98 N, 95% CI = [8.50, 9.47]) was very similar to the family-predicted weight (9.33 N, 95% CI = [8.97, 9.68]), as expected. Strikingly, when considering the small subset of learners in the combined Linear+ and +Linear group, the anticipatory force for the outlier at the end of the test phase was 11.71 N (95% CI = [11.34, 12.08]), showing that these participants fully learned the actual weight of the outlier (11.77 N). The same pattern of all-or-nothing learning in the two clusters was found for the combined Linear++ and ++Linear group. Specifically, the anticipatory force for the outlier produced by non-learners (10.17 N, 95% CI = [8.94, 11.40]) was very similar to the family-predicted weight (10.52 N, 95% CI = [9.50, 11.54]). Note that the family-predicted weight is slightly increased for the non-learners in these conditions. This is because the family shifted to partially accommodate the extreme weight of the outlier, but it was not kicked out of the family. The anticipatory force produced by the learners in the combined Linear++ and ++Linear group (14.12 N, 95% CI = [13.39, 14.86]) was very similar to the actual weight of the outlier (14.72 N). Thus, these results strengthen our main conclusion that memories of the motor properties of objects are organized categorically.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have examined how the mechanical properties of objects we interact with are represented in memory. In a series of experiments, we provide evidence that ‘motor memories’ of objects are organized in terms of families. More specifically, we show that when encountering a set of new objects whose size and weight covary, participants have a strong propensity to encode the objects as a family. The consequence of this encoding is that an object that appears to be part of a previously learned family, but is an outlier in terms of weight, may nevertheless be classified as a family member. In this case, participants predict the outlier’s weight based on the family and never learn its actual weight. This ‘family effect’ on the outlier can be anterograde, such that the family interferes with learning the weight of a newly introduced outlier, or retrograde, such that an already-learned outlier weight will be forgotten when the family is introduced. We also show that there is a weight threshold at which a sufficiently deviant outlier will ‘escape’ the family and be learned as an individual object. Moreover, we show that the error experienced when lifting an outlier that is encoded as a family member updates the estimated weights of the other family members. However, if the outlier has been learned as an individual, such updating is not observed. Additionally, we show that the threshold that determines whether an outlier is classified as an individual or a family member depends on recent sensorimotor experience.</p><p>Two broad approaches have been used in motor control to examine how dynamics, experienced during arm and hand movements, are represented in memory. The first approach involves applying novel dynamics, or ‘force fields’, to the hand. Typically this has been done by asking participants to move a handle, which is attached to a robotic manipulandum and visually represented as a cursor, between visual targets located in a horizontal plane. This work has focused on the reference frame in which individual force fields are represented (<xref ref-type="bibr" rid="bib117">Shadmehr and Mussa-Ivaldi, 1994</xref>; <xref ref-type="bibr" rid="bib87">Krakauer et al., 2000</xref>; <xref ref-type="bibr" rid="bib91">Malfait et al., 2002</xref>; <xref ref-type="bibr" rid="bib39">Davidson et al., 2005</xref>; <xref ref-type="bibr" rid="bib15">Berniker et al., 2014</xref>), and on contextual factors that enable people to learn two different force fields that apply forces in opposite directions (<xref ref-type="bibr" rid="bib39">Davidson et al., 2005</xref>; <xref ref-type="bibr" rid="bib17">Brashers-Krug et al., 1996</xref>; <xref ref-type="bibr" rid="bib51">Gandolfo et al., 1996</xref>; <xref ref-type="bibr" rid="bib86">Krakauer et al., 1999</xref>; <xref ref-type="bibr" rid="bib80">Karniel and Mussa-Ivaldi, 2002</xref>; <xref ref-type="bibr" rid="bib123">Tong et al., 2002</xref>; <xref ref-type="bibr" rid="bib23">Caithness et al., 2004</xref>; <xref ref-type="bibr" rid="bib100">Osu et al., 2004</xref>; <xref ref-type="bibr" rid="bib99">Nozaki et al., 2006</xref>; <xref ref-type="bibr" rid="bib64">Howard et al., 2008</xref>; <xref ref-type="bibr" rid="bib1">Addou et al., 2011</xref>; <xref ref-type="bibr" rid="bib65">Howard et al., 2012</xref>; <xref ref-type="bibr" rid="bib66">Howard et al., 2013</xref>; <xref ref-type="bibr" rid="bib118">Sheahan et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Heald et al., 2018</xref>; <xref ref-type="bibr" rid="bib95">McGarity-Shipley et al., 2020</xref>). Although force fields may, arguably, be viewed as objects (at least in some contexts; <xref ref-type="bibr" rid="bib34">Cothros et al., 2006</xref>; <xref ref-type="bibr" rid="bib35">Cothros et al., 2009</xref>; <xref ref-type="bibr" rid="bib84">Kluzik et al., 2008</xref>), this previous work has not examined how memories of multiple objects might be organized. The second approach to investigating how dynamics are represented in memory focuses on weight prediction when lifting objects, which is critical for dexterous manipulation. This work has shown that people can exploit learned associations, or ‘priors’, between size and weight, and between material and weight, to estimate the weight of an object (<xref ref-type="bibr" rid="bib54">Gordon et al., 1991</xref>; <xref ref-type="bibr" rid="bib55">Gordon et al., 1993</xref>; <xref ref-type="bibr" rid="bib13">Baugh et al., 2012</xref>; <xref ref-type="bibr" rid="bib32">Cole, 2008</xref>; <xref ref-type="bibr" rid="bib21">Buckingham et al., 2009</xref>). Although such priors are often useful, for many objects that we interact with they do not provide accurate weight predictions. Importantly, once an object has been lifted, people can form a long-lasting ‘object-specific’ memory of the object’s actual weight (<xref ref-type="bibr" rid="bib54">Gordon et al., 1991</xref>; <xref ref-type="bibr" rid="bib55">Gordon et al., 1993</xref>; <xref ref-type="bibr" rid="bib13">Baugh et al., 2012</xref>; <xref ref-type="bibr" rid="bib76">Johansson and Westling, 1988</xref>; <xref ref-type="bibr" rid="bib44">Flanagan et al., 2001</xref>; <xref ref-type="bibr" rid="bib46">Flanagan et al., 2008</xref>). However, the question of how motor memories of the myriad objects we interact with are represented and organized has not been addressed.</p><p>Where in the brain might motor memories of objects be stored? According to a well-known neuroanatomical framework for understanding visual processing in the primate brain, the dorsal visual pathway, in parietofrontal cortex, supports visual processing for action, whereas the ventral visual pathway, in ventrotemporal cortex, supports visual processing for perception (<xref ref-type="bibr" rid="bib53">Goodale and Milner, 1992</xref>). This framework arose primarily from studies examining reaching and grasping movements directed towards objects, where the relevant object properties (e.g. size, shape, location) can be directly appreciated through vision. The control of these actions involves mapping these visual features onto motor commands to move and shape the hand (<xref ref-type="bibr" rid="bib113">Salinas and Abbott, 1995</xref>; <xref ref-type="bibr" rid="bib7">Arbib, 1981</xref>; <xref ref-type="bibr" rid="bib73">Jeannerod, 1981</xref>; <xref ref-type="bibr" rid="bib104">Pouget and Snyder, 2000</xref>), and there is abundant evidence that parietofrontal cortex is engaged in such computations (<xref ref-type="bibr" rid="bib74">Jeannerod et al., 1995</xref>; <xref ref-type="bibr" rid="bib110">Rizzolatti and Luppino, 2001</xref>; <xref ref-type="bibr" rid="bib25">Castiello and Begliomini, 2008</xref>; <xref ref-type="bibr" rid="bib56">Grafton, 2010</xref>; <xref ref-type="bibr" rid="bib12">Battaglia-Mayer and Caminiti, 2018</xref>). However, as emphasized above, skilled object manipulation requires knowledge of mechanical properties, which cannot be directly appreciated through vision and must instead be estimated based on object memories linking visual and mechanical properties. Some evidence suggests that such memories could involve parietal and premotor regions of the dorsal pathway (<xref ref-type="bibr" rid="bib28">Chouinard et al., 2005</xref>; <xref ref-type="bibr" rid="bib75">Jenmalm et al., 2006</xref>; <xref ref-type="bibr" rid="bib29">Chouinard et al., 2009</xref>; <xref ref-type="bibr" rid="bib49">Freedman and Assad, 2009</xref>; <xref ref-type="bibr" rid="bib125">van Nuenen et al., 2012</xref>). However, the maintenance of durable memory representations of objects is more commonly associated with the ventral visual pathway (<xref ref-type="bibr" rid="bib20">Bruce et al., 1981</xref>; <xref ref-type="bibr" rid="bib124">Ungerleider and Haxby, 1994</xref>; <xref ref-type="bibr" rid="bib108">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="bib58">Grill-Spector et al., 2001</xref>; <xref ref-type="bibr" rid="bib41">Erez et al., 2016</xref>). Given that category selectivity is a well-established organizational feature of ventrotemporal cortex (<xref ref-type="bibr" rid="bib88">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="bib59">Grill-Spector and Weiner, 2014</xref>), it seems plausible that the ventral pathway also plays a role in categorizing the mechanical properties of objects. Consistent with this view, it has been shown that, in the context of lifting, object weight is represented in the lateral occipital complex (LOC) (<xref ref-type="bibr" rid="bib50">Gallivan et al., 2014</xref>), an object-selective ventral region also known to be active during reaching and grasping (<xref ref-type="bibr" rid="bib37">Culham et al., 2003</xref>; <xref ref-type="bibr" rid="bib98">Monaco et al., 2014</xref>). On the other hand, LOC does not appear to represent object mass that can be inferred when simply viewing objects interacting (<xref ref-type="bibr" rid="bib115">Schwettmann et al., 2019</xref>).</p><p>Beyond the dorsal and ventral visual pathways, several other candidate brain regions may be involved in learning object families in the service of dexterous manipulation. For instance, predictive encoding of object weight has also been demonstrated in single-cell recordings of Purkinje neurons (<xref ref-type="bibr" rid="bib2">Allan et al., 2015</xref>; <xref ref-type="bibr" rid="bib93">Mason et al., 2006</xref>), which may arise from cerebellar internal models of the dynamics of different types of objects (<xref ref-type="bibr" rid="bib131">Wolpert and Flanagan, 2001</xref>; <xref ref-type="bibr" rid="bib71">Imamizu et al., 2000</xref>; <xref ref-type="bibr" rid="bib22">Bursztyn et al., 2006</xref>). Likewise, there is considerable evidence from human imaging studies and non-human primate neurophysiological studies for the role of prefrontal cortex and the striatum in perceptual category learning (<xref ref-type="bibr" rid="bib48">Freedman et al., 2001</xref>; <xref ref-type="bibr" rid="bib10">Ashby and Maddox, 2005</xref>; <xref ref-type="bibr" rid="bib107">Reber et al., 1998</xref>; <xref ref-type="bibr" rid="bib127">Vogels et al., 2002</xref>; <xref ref-type="bibr" rid="bib116">Seger and Miller, 2010</xref>; <xref ref-type="bibr" rid="bib5">Antzoulatos and Miller, 2011</xref>; <xref ref-type="bibr" rid="bib6">Antzoulatos and Miller, 2014</xref>; <xref ref-type="bibr" rid="bib16">Bowman and Zeithamova, 2018</xref>; <xref ref-type="bibr" rid="bib106">Raz and Saxe, 2020</xref>), but it remains unknown whether these areas are also recruited in organizing objects based on their learned motor properties.</p><p>Current theories of motor learning often focus on graded generalization of learning across various stimulus and motor parameters as a revealing feature of the underlying computations (<xref ref-type="bibr" rid="bib87">Krakauer et al., 2000</xref>; <xref ref-type="bibr" rid="bib122">Thoroughman and Shadmehr, 2000</xref>; <xref ref-type="bibr" rid="bib40">Donchin et al., 2003</xref>; <xref ref-type="bibr" rid="bib72">Ingram et al., 2017</xref>). In particular, graded patterns of generalization have been taken as evidence that motor learning fundamentally involves associating contextual features of a movement with the target motor parameters in a continuous multi-dimensional space, often termed an associative map. The theoretical significance of our study is that it provides multiple, converging pieces of evidence for a fundamentally different type of organization—motor memories of objects are organized categorically, into families. Our key result is the family effect itself, wherein an outlier object is persistently encoded as a family member, despite greatly deviating from its expected weight. In contrast, the prediction of an associative map account is that these outliers would eventually be learned, since they are visually and haptically discriminable from the family (as shown by the accurate learning in the Uncorr+ condition).</p><p>In our experiments, we generally observed incomplete learning of the outlier when averaging anticipatory forces across participants. At first glance, partial learning could be explained by an associative map model where the neighboring objects reduce the estimated weight of the outlier by local generalization. However, seemingly partial learning is also consistent with the object families hypothesis. In particular, partial learning in the group averages could result from averaging together a subgroup of highly accurate learners with a separate subgroup of complete non-learners, who differ in their threshold for reclassifying the outlier as an individual. This latter interpretation was confirmed by our large-sample, web-based experiment, which revealed that individual differences in outlier learning followed an all-or-nothing pattern. At the end of the experiment, participants had either learned to classify the outlier as a unique individual and accurately estimated its weight, or they still encoded it as a family member and incorrectly estimated its weight based on the family representation.</p><p>We found remarkably similar results in the laboratory and web-based tasks. These two motor tasks are similar in that they both involve translating sensory information into continuous motor commands to achieve an action goal. However, the precise nature of the sensory information used for control differs between the tasks, with the laboratory task primarily relying on haptic feedback and the web-based task relying on visual feedback. Our web-based task required participants to map their weight predictions onto an arm movement that set the visual length of a spring. This ‘visual’ lifting is similar to many motor tasks in which the initial conditions of an interaction are likewise adjusted based on visual feedback; for example, when aiming in archery, lining up a putt in golf, or pulling back the plunger to launch a pinball. In these examples, the motor error is also provided strictly through visual feedback as in our web-based task. The fact that we observe similar categorical encoding in both the laboratory task and the web-based task demonstrates the generalizability of our findings.</p><p>Our generalization results also favor a categorical organization of motor memory over a continuous, associative map. We found that the way that the outlier object was classified—either as a family member or an individual—had a dramatic effect on outlier-to-family generalization. When the outlier object was classified as a family member, strong generalization was observed, whereas when it was classified as an individual, generalization was substantially reduced. This qualitative change in generalization was observed across participants in different conditions, as well as within the same participants who, during learning, reclassified the outlier from a family member to an individual. These results strongly support the idea that motor memories of objects are organized categorically, rather than continuously, which would predict graded generalization as a function of error magnitude and sensory similarity. By eliciting separate visual classification of the outlier and the family objects, we were able to suddenly ‘shut off’ inter-object error generalization.</p><p>We also found that when the weight of the outlier was gradually increased from 1.2 to 1.5 kg, participants generally failed to learn its weight, even though it reached the same weight as the outlier that, when introduced abruptly, was learned. One interpretation of this finding is that first experiencing the 1.2 kg outlier, and then experiencing incrementally increasing weights, broadened the category by increasing the within-category variability, as shown in perceptual and conceptual categorization (<xref ref-type="bibr" rid="bib109">Rips, 1989</xref>; <xref ref-type="bibr" rid="bib70">Huttenlocher et al., 2000</xref>; <xref ref-type="bibr" rid="bib31">Clayards et al., 2008</xref>). Another possible account for this finding is that category labels are ‘sticky’, and that once the test object was labeled as a family member, there was resistance to relabeling it as an individual, similar to the hysteretic effects reported in perceptual categorization (<xref ref-type="bibr" rid="bib129">Williams et al., 1986</xref>; <xref ref-type="bibr" rid="bib63">Hock et al., 1993</xref>; <xref ref-type="bibr" rid="bib101">Poltoratski and Tong, 2014</xref>). However, it seems plausible that the 1.5 kg outlier was initially labeled as a family member as participants’ anticipatory forces on the first lift of this object were based on the density of the family. If so, then relabeling occurred when this extreme outlier was learned, arguing against the ‘stickiness’ account. On the other hand, the stickiness hypothesis could account for the results we observed when the outlier weight was initially set to 1.5 kg and then gradually decreased to 1.2 kg. In this case, participants initially learned the extreme outlier and continued to accurately predict its weight—and hence to categorize it as an individual—even as its weight decreased to a level that, when introduced abruptly, was not learned. Alternatively, it is possible that learning the extreme 1.5 kg outlier as a distinct individual object caused the category boundary for the training objects to contract, such that a 1.2 kg outlier remained outside the learned family, perhaps because the individuated outlier effectively forms a competing category. Note that work on sensorimotor adaptation has shown that participants do not become aware of visual or force perturbations that are introduced gradually (<xref ref-type="bibr" rid="bib78">Kagerer et al., 1997</xref>; <xref ref-type="bibr" rid="bib92">Malfait and Ostry, 2004</xref>; <xref ref-type="bibr" rid="bib82">Klassen et al., 2005</xref>; <xref ref-type="bibr" rid="bib112">Saijo and Gomi, 2010</xref>; <xref ref-type="bibr" rid="bib36">Criscimagna-Hemminger et al., 2010</xref>; <xref ref-type="bibr" rid="bib111">Roemmich and Bastian, 2015</xref>). Since participants adapt to these gradually increasing perturbations, they never see large errors, which presumably explains why they do not become aware of the perturbation. In contrast, in our experiment with a gradually increasing outlier weight, participants did not adapt (i.e. they continued to predict the outlier weight based on the family density). Thus, they experienced larger and larger errors, ultimately experiencing the same error that drove learning when the 1.5 kg outlier was introduced abruptly. The reason that participants learned the 1.5 kg outlier when introduced abruptly, but not when introduced gradually, may be that they are sensitive to the change in error, as opposed to error per se.</p><p>Although the formation of motor memories has historically been viewed as a largely implicit process, recent research on motor learning and adaptation has emphasized the role of explicit processes. For example, when reaching under a visuomotor rotation, participants often learn to use an explicit re-aiming strategy to reduce movement errors (<xref ref-type="bibr" rid="bib94">Mazzoni and Krakauer, 2006</xref>; <xref ref-type="bibr" rid="bib4">Anguera et al., 2010</xref>; <xref ref-type="bibr" rid="bib42">Fernandez-Ruiz et al., 2011</xref>), and can quickly recall and implement this strategy when re-exposed to the rotation at a later time (<xref ref-type="bibr" rid="bib120">Taylor et al., 2014</xref>; <xref ref-type="bibr" rid="bib67">Huberdeau et al., 2015</xref>). In the context of object manipulation, it is clear that people often have explicit knowledge of the weights of objects they interact with. That is, if asked, people can provide an estimate of the weight of an object. When lifting familiar objects, these estimates can be quite accurate, although research on weight illusions shows that these estimates are biased by expected weight (<xref ref-type="bibr" rid="bib43">Flanagan and Beltzner, 2000</xref>; <xref ref-type="bibr" rid="bib46">Flanagan et al., 2008</xref>). However, whether and, if so, how explicit knowledge is used when generating lift forces is unclear. In the current study, we did not ask participants to provide verbal estimates of the weights of the objects before lifting them. We suspect that these estimates would have been consistent with the actual forces produced when lifting. Requiring participants to provide such estimates, however, may also alter the category boundary used to classify objects as family members or individuals. The use of explicit knowledge is believed to require working memory and previous research has shown that greater working memory resources are required when lifting unusually weighted objects than when lifting normally weighted objects (<xref ref-type="bibr" rid="bib14">Baugh et al., 2016</xref>; <xref ref-type="bibr" rid="bib46">Flanagan et al., 2008</xref>). Based on these findings, we speculate that when participants lift objects that are encoded as a family, this categorical encoding is largely implicit, requiring little to no explicit processing. In contrast, we suspect that explicit processing does contribute to encoding an object that is similar in appearance to the family as an outlier. Likewise, explicit processing may be required when learning the weights of multiple individual objects, as in our Uncorr+ condition. In this condition, response times were significantly greater than the Linear conditions, consistent with the notion that explicit processes were engaged. In general, as people become more experienced at classifying objects into separate families, the contribution of explicit knowledge will likely diminish and lifting will become more implicit and automated. As noted above, object manipulation tasks engage multiple sensorimotor and cognitive processes, including categorization, and can depend on implicit and explicit memories. From our perspective, any form of memory engaged in a motor control task can be considered as a ‘motor memory’, whether that is an explicit declarative memory or an implicit procedural memory.</p><p>By showing that dexterous object manipulation relies on learned representations of categories (and individuals), our findings open the door for future work that connects theories of human category learning, developed in the context of perception and cognition, with theories of motor control. The vast literature on category learning has identified and debated a variety of key issues, including why certain categorizations are harder to learn than others (<xref ref-type="bibr" rid="bib10">Ashby and Maddox, 2005</xref>; <xref ref-type="bibr" rid="bib119">Shepard et al., 1961</xref>), whether category knowledge is encoded using prototype, exemplar, or decision-bound representations (<xref ref-type="bibr" rid="bib102">Posner and Keele, 1968</xref>; <xref ref-type="bibr" rid="bib96">Medin and Schaffer, 1978</xref>; <xref ref-type="bibr" rid="bib8">Ashby and Townsend, 1986</xref>), and how the relative contributions of explicit ‘rule-based’ and implicit ‘information-integration’ processes are modulated by the relevant perceptual dimensions and category structure of a stimulus domain (<xref ref-type="bibr" rid="bib10">Ashby and Maddox, 2005</xref>; <xref ref-type="bibr" rid="bib9">Ashby et al., 1998</xref>; <xref ref-type="bibr" rid="bib11">Ashby and Maddox, 2011</xref>). A detailed review of how the pertinent findings from this literature might inform our understanding of dexterous object manipulation (and vice versa) is well beyond the scope of this article, but it is nonetheless clear that there is a pressing need for greater attention to these connections. However, focusing more narrowly on accounting for the present findings, it is notable that many existing process-level (i.e. trial-by-trial) models of category learning posit a mechanism that allows for the creation of a new category in memory when an observation deviates sufficiently from previously learned categories (<xref ref-type="bibr" rid="bib10">Ashby and Maddox, 2005</xref>; <xref ref-type="bibr" rid="bib60">Hartigan, 1975</xref>; <xref ref-type="bibr" rid="bib24">Carpenter and Grossberg, 1987</xref>; <xref ref-type="bibr" rid="bib30">Clapper and Bower, 1991</xref>; <xref ref-type="bibr" rid="bib3">Anderson, 1991</xref>; <xref ref-type="bibr" rid="bib89">Love et al., 2004</xref>; <xref ref-type="bibr" rid="bib126">Vanpaemel et al., 2005</xref>). These various treatments can all be viewed as instances of non-parametric Bayesian models that leverage the hierarchical Dirichlet process, a statistically principled approach to clustering data into a theoretically infinite number of components (<xref ref-type="bibr" rid="bib121">Teh et al., 2005</xref>; <xref ref-type="bibr" rid="bib57">Griffiths et al., 2007</xref>). A recent motor control model has been developed based on this approach (<xref ref-type="bibr" rid="bib62">Heald et al., 2020</xref>). However, at present this model cannot account for our results because there is no mechanism through which the visual properties of the objects—which are encoded as discrete cues—can be linked together to form a family.</p><p>In general, learning a family of objects based on covarying size and weight, as in this study, is presumably just one example of a more general tendency to compactly encode the covariability of observable sensory features and latent mechanical properties. Previous work has shown that people can learn more complex ‘structures’ in motor control tasks (e.g. visuomotor rotations and skews), but has not distinguished between categorical and associative representations (<xref ref-type="bibr" rid="bib18">Braun et al., 2009</xref>; <xref ref-type="bibr" rid="bib19">Braun et al., 2010</xref>). Categorical encoding amounts to carving the sparse, high-dimensional space of sensorimotor information into circumscribed, lower-dimensional object categories, providing a number of benefits. First, it allows for more robust interpolation and extrapolation from past sensorimotor experience by shoehorning ambiguous new items into predictable categories. Second, it reduces the temporal costs associated with specifically identifying objects, which would involve deeper traversal into object memory. Third, when working with multiple objects from the same family, this strategy conserves working memory resources that would otherwise be expended on object individuation. Lastly, categorical organization also conserves long-term memory resources by maintaining only abstract descriptions of relevant family structure, rather than a detailed map of all sensorimotor properties, helping to address the curse of dimensionality. In contrast, although learning about individual objects may increase accuracy in some circumstances, this would come at the cost of significantly increased demands on attention (for visual recognition), cognitive control (for switching between memories), and memory (for storage). Therefore, in combination with context-sensitive reflexes and other rapid corrective mechanisms, a categorical memory of object properties affords tradeoffs between accuracy and memory that can be balanced as needed to support our unmatched ability to skillfully manipulate many different kinds of objects.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>We first describe the in-laboratory experiments before describing the web-based experiments.</p><sec id="s4-1"><title>Laboratory experiments</title><sec id="s4-1-1"><title>Participants</title><p>A total of 80 participants (42 males, 38 females) aged 18–45 years old (median 24) were recruited for the laboratory experiments. Participants were right-handed according to the Edinburgh handedness questionnaire, and reported that they had normal or corrected-to-normal vision and no prior diagnosis of a movement disorder. They were compensated at a rate of $17 per hour. All experiments were conducted in accordance with the 1964 Declaration of Helsinki, following protocol approved by the Columbia University Institutional Review Board. Written informed consent was obtained from all participants prior to their participation.</p></sec><sec id="s4-1-2"><title>Apparatus</title><p>Experiments were performed using a 3BOT three-dimensional robotic manipulandum and an Oculus Rift DK2 (Menlo Park, CA) virtual reality headset, as well as a 2-button USB response pad (The Black Box ToolKit Ltd., Sheffield, UK). The position of the 3BOT handle was measured using optical encoders sampled at 5 kHz, and torque motors allowed forces (also updated at 5 kHz) to be generated on the handle. Participants sat on a height-adjustable stool in front of a tabletop workspace and grasped the 3BOT handle with their right hand (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The virtual reality headset was rigidly fixed to an aluminum crossbeam and angled downwards by 30°. Stereoscopic visual stimuli were rendered on the headset using custom OpenGL routines and the Psychophysics Toolbox (<xref ref-type="bibr" rid="bib83">Kleiner et al., 2007</xref>). Auditory cues were provided through Sennheiser HD201 (Old Lyme, CT) over-ear headphones.</p></sec><sec id="s4-1-3"><title>Task</title><p>In our object ‘lifting’ task, the participant generates an upward force on an object that is initially fixed to the surface beneath it, such that the object cannot move. The participant then presses a button, at which time the surface disappears, releasing the object so that it is then free to move. The goal for the participant is to match the upward force to the weight of the object so that the object does not move when it is released. Participants performed this lifting task with five cylinders of equal radius (4.61 cm), but of different heights (6, 7.5, 9, 10.5, and 12 cm), leading to five equally spaced volumes (400, 500, 600, 700, and 800 cm<sup>3</sup>). Each cylinder was shaded, from smallest to largest, between orange and red according to the Munsell color system (Hue: 10 R, Value/Chroma: 3/10, 4/12, 5/14, 6/16, and 7/16). All objects were visible throughout the task, except during rest breaks. The objects were positioned evenly around the edge of a gray, semi-transparent carousel with a radius of 20 cm (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). The weight of each object varied across the experimental conditions (see below).</p><p>Before each trial, the 3BOT moved the participant’s hand passively to a start position 11 cm in front of and 19 cm below the cyclopean eye (in gravity-oriented space) and clamped it there by a simulated stiff spring (spring constant: 4000 N m<sup>−1</sup>, damping coefficient: 2 N m s<sup>−1</sup>, both acting in all directions). The participant saw a stereoscopically rendered view of the five objects and the circular carousel (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). The carousel rotated smoothly (750 ms) to bring a target object to the front and a 500 ms tone then signaled the start of the trial. Note that at this point, the hand (i.e. the center of the 3BOT handle) was located at the center of the base of the target object. The participant then generated an upward lifting force on the object (i.e. against the simulated stiff spring) attempting to match its weight. When ready, the participant pressed a button with their left hand that caused a portion of the carousel below the object to open, thus releasing the object so that it was free to move. The physical interaction between the hand and the object was then simulated haptically using the 3BOT. We simulated the object as a point-mass acted upon by gravity and attached by a stiff, damped spring (acting in all three dimensions) to the center of the handle. The spring constant was 4000 N m<sup>–1</sup> and the damping coefficient was 2 N m s<sup>−1</sup> with gravity set at −9.81 m s<sup>–2</sup>. We updated the location of the object both haptically and visually and generated the appropriate forces on the hand. This method produces a stable, compelling haptic percept of a handheld inertial mass. If the anticipatory force was more or less than the weight of the object then the handle would move upward or downward, respectively, until corrective motor commands re-stabilized the arm posture. To encourage accurate performance, thin horizontal gray bars (2 mm radius, purely visual and not haptic) were visible just above and below the target object from the start of the trial (not depicted in <xref ref-type="fig" rid="fig1">Figure 1b</xref>). If the object remained between the horizontal bars for 500 ms, the bars disappeared, and the participant completed the trial by raising the object at least 3 cm above the start position and replacing it on the carousel, where a virtual haptic surface was now simulated to allow full unloading of lift forces prior to the next trial. However, if the object crossed one of the bars, it turned red and a white-noise audio burst was played. The object had to be brought back within the bars before they would disappear, and only then could the participant complete the trial by raising and replacing the object on the carousel. The distance of the bars from the top and bottom edges of the object (i.e. the amount of tolerated object movement) varied according to the participant’s performance: the demarcated region became 1 mm larger following an trial where the object crossed a bar, up to a maximum tolerated deviation of ±13 mm (this was also the initial width), and became 1 mm smaller after five consecutive trials where the object stayed within the bars, down to a minimum tolerated deviation of ±2 mm.</p><p>Feedback was also provided in the form of a per-trial score that depended on the absolute error between the anticipatory force at the moment of the button press and the required force to support the object, with score = max(0, 100–13*|error|). The participant’s cumulative score was displayed throughout the experiment. The five highest-scoring previous participants’ scores from the same condition were displayed in a leaderboard beside their own score. This leaderboard was initially seeded based on the score of a pilot run, which was multiplied by 1, 0.9, 0.8, 0.75, and 0.7 to produce five scores. These seed scores were erased one by one as data were collected from the first five participants in each condition.</p></sec></sec><sec id="s4-2"><title>Paradigm</title><sec id="s4-2-1"><title>Linear+ condition</title><p>Fifteen participants (of an initial sample of 30) were randomly assigned to the Linear+ condition; the other fifteen were assigned to the Uncorr+ condition (see below). The training objects (the two smallest and two largest objects by volume) weighed 600, 750, 1050, and 1200 g, respectively, corresponding to a constant density of 1.5 g cm<sup>−3</sup> (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). The test object (or ‘outlier’) was the mid-size cylinder and weighed 1200 g, corresponding to a density of 2.0 g cm<sup>−3</sup>.</p><p>All participants were informed that the purpose of the experiment was to test their ability to learn and recall the weights of a novel set of objects. The Linear+ condition began with a 120-trial training phase in which the participant interacted only with the four training objects. The order of presentation was pseudo-randomized in cycles where each object was presented once before any object was repeated, and subject to the additional constraint that the first object presented in one cycle could not be the same as the last object presented on the previous cycle. Following training, the test object (also called the outlier object when introduced amongst a linear object family) was introduced for a 200-trial test phase. During the test phase, in each five-trial cycle, the test object was always presented first, followed by the four training objects in pseudo-random order, but subject to the additional constraint that for every four cycles, each of the four training objects would be presented immediately after the test object (i.e. on the second trial of the cycle) exactly once.</p><p>To reduce the effects of fatigue, participants were required to take occasional 30 second breaks. During these breaks, participants stopped holding the 3BOT handle, came out of the virtual reality headset, and were encouraged to stretch their right arm and hand. These breaks occurred after trials 60, 120, and 200. The experiment had a total of 320 trials and lasted approximately 45 minutes.</p><p>Prior to the experiment, the experimenter demonstrated the task by performing 10 or 15 trials of a familiarization condition while the participant watched. The visual scene was displayed on a nearby monitor so the participant could follow along. The participant then completed 30 trials of task familiarization, where the object stimuli were three spheres (5 cm radius) that were blue, red, and green (7.5B 6/8, 7.5 R 6/18, 7.5 GY 6/10) and weighed 500, 900, and 1300 g, respectively. During task familiarization, the experimenter could choose to display or hide a bar graph that showed the real-time load force on the handle. This visual aid helped participants calibrate to the range of forces they would be asked to produce in the experiment, and prevented them from producing unnecessarily large forces. Approximately ten familiarization trials were performed with full view of this visual feedback, followed by approximately ten trials with short glimpses of the feedback prior to the button press, followed by approximately ten trials without the visual feedback as in the actual experiment.</p></sec><sec id="s4-2-2"><title>Uncorr+ condition</title><p>Fifteen participants were randomly assigned to the Uncorr+ condition. The Uncorr+ condition was similar to the Linear+ condition, except the four training object weights (600, 750, 1050, and 1200 g) were assigned randomly to the four training objects (<xref ref-type="fig" rid="fig1">Figure 1f</xref>), subject to the constraint that the absolute value of the Pearson correlation coefficient between volume and mass could not exceed 0.3. The test object had the same weight as in the Linear+ condition.</p></sec><sec id="s4-2-3"><title>Linear++ condition</title><p>In the Linear++ condition, we recruited participants until we obtained a sample size of 9 after excluding non-learners. The Linear++ condition was identical to the Linear+ condition, except the outlier object weighed 1500 g (rather than 1200 g; <xref ref-type="fig" rid="fig1">Figure 1e</xref>).</p></sec><sec id="s4-2-4"><title>+Linear condition</title><p>In the +Linear condition, we recruited participants until we obtained a sample size of 11 after excluding non-learners. In the +Linear condition, the experiment began with a 30-trial training phase where participants interacted only with the test object which weighed 1200 g. This was followed by a 200-trial test phase identical to the Linear+ condition in which all five objects were lifted in each cycle. This was followed by the 1:1 phase, which was a block of 10 cycles where, in each cycle, the test object was presented four times and each of the four family objects was lifted once, for a total of ight trials per cycle. To limit the number of consecutive presentations of the test object in the 1:1 phase, we pseudorandomized the trial sequence such that consecutive presentations of the test object occurred exactly 13 times, while presentations of the test object with one, two, or three intervening trials from the last presentation of the test object occurred exactly 15, 8, and three times, respectively. The +Linear condition had a total of 310 trials and rest breaks occurred after trials 90 and 190.</p></sec><sec id="s4-2-5"><title>++Linear condition</title><p>The ++Linear condition was identical to the +Linear condition except the outlier object weighed 1500 g (rather than 1200 g).</p></sec><sec id="s4-2-6"><title>Linear➚ condition</title><p>In the Linear➚ condition, we recruited participants until we obtained a sample size of 9 after excluding non-learners. The Linear➚ condition was identical to the Linear+ condition except that the outlier object’s weight (initially 1200 g) was iteratively increased by 50 g on trials 221, 261, 301, 341, 381, and 421, up to a maximum of 1500 g. The length of the test phase was also increased to 340 trials, leading to a total of 460 trials. Rest breaks occurred after trials 60, 120, 220, 300, and 380.</p></sec><sec id="s4-2-7"><title>Linear➘ condition</title><p>The Linear➘ condition was identical to the Linear➚ condition except that the outlier initially weighed 1500 g and its weight was iteratively decreased by 50 g to 1200 g.</p></sec></sec><sec id="s4-3"><title>Analysis</title><sec id="s4-3-1"><title>Data preprocessing</title><p>The anticipatory force was taken as the average force applied in the upward direction over the final ten samples (10 ms) of the clamp phase (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, trial phase 2). Response times were measured as the duration from trial onset (defined as the beginning of trial phase 2, when the object carousel stopped rotating) to the button press.</p><p>We excluded 322 anticipatory forces (1.15%) that were less than or equal to 1 N (typically due to an accidental button press) or more than 3.5 scaled median absolute deviations away from the median anticipatory force applied by a given participant for a given object. Similarly, we excluded 392 response times (1.40%) that, following a log transformation, were more than 3.5 scaled median absolute deviations from the median log-transformed response time. We then imputed the mean anticipatory force or reaction time produced on non-outlying trials by other participants for the same object, cycle, and condition.</p><p>We also excluded participants (and hence recruited additional participants) who failed to learn the weights of the training objects, as the goal of the experiment was to observe how learning of a new object is affected by existing knowledge of object weights. Non-learners were defined as those whose anticipatory forces during the final 15 cycles of the training phase did not show a highly significant (<italic>α</italic> = 0.01) positive correlation with the weights of the objects. In the Uncorr+ group, three participants were excluded by this criterion. In the Linear+ and Linear➚ groups, one participant from each group was excluded by this criterion. This criterion was not applied in the +Linear and ++Linear groups because the training phase involved only the test object.</p></sec></sec><sec id="s4-4"><title>Statistical analysis</title><p>In most motor learning experiments, there are between eight and twelve participants per experimental group. This sample size provides sufficient power to detect the large effects typical of motor learning experiments, where the effect of interest is observed in most if not all participants. As this was a new experimental paradigm, in the first two experimental groups (Linear+ and Uncorr+) we recruited a sample size of fifteen. In the Uncorr+ group, we observed significant learning of the outlier object with a large effect size (Cohen’s <italic>d</italic> = 1.17). Based on this value, we adopted a sample size of nine for the Linear++, Linear➚, and Linear➘ groups, aiming to achieve a statistical power exceeding 0.90 in our one-tailed <italic>t</italic>-tests of outlier learning. In the +Linear and ++Linear conditions, we could not exclude individual participants as non-learners as in the other conditions (see above). We therefore estimated a slightly reduced effect size for sample size estimation (Cohen’s <italic>d</italic> = 1.00), leading us to adopt a sample size of eleven in order to achieve at least 0.90 power in these groups. Post-hoc power analyses of groups with significant outlier learning confirmed that we achieved the desired power (Uncorr+: 0.98, Linear++: 0.92, ++Linear: 0.96, Linear➘: 0.99).</p><p>In the Linear+, Linear++, Uncorr+, Linear➚, and Linear➘ groups, learning of the training set at the end of the training phase was measured using the Pearson correlation between actual object weight and anticipatory force on trials between trial cycles 23 and 30. The Fisher <italic>z</italic>-transformation was used to compute 95% confidence intervals.</p><p>To assess learning of the test object relative to the training objects, we compared the anticipatory force for the test object to the force that would be expected based on the anticipatory forces for the four training objects (i.e. the ‘family-predicted’ weight). To do this, we fit a linear regression to the anticipatory forces for the training objects as a function of volume in the final 16 trial cycles of the test phase. We calculated the family-predicted weight of the test object based on the regression and the test object’s volume. Note that because the test object’s volume was always in the middle of the training objects, the family-predicted weight is equivalent to the mean anticipatory force produced for the four training objects, hence the logic is also appropriate for the Uncorr+ condition. We used one-tailed <italic>t</italic>-tests to evaluate the null hypothesis that the test object weight would not be learned. One-tailed tests are justified because failure to learn the test object weight is a directional hypothesis, which includes the case where the anticipatory force for the test object does not differ from the family-predicted weight, as well as the case where it is less than the family-predicted weight. In the Linear➚ and Linear➘ groups, we also conducted this analysis for the final four trial cycles of the initial portion of the test phase during which the test object weight did not change.</p><p>In the first experiment, we analyzed how lifting the test object generalized to the training objects in the subsequent trial (<xref ref-type="fig" rid="fig2">Figure 2g–i</xref>). For this analysis, we focused only on trials that immediately followed a lift of the test object (i.e. the second trial of each trial cycle), as any generalization would only be reduced in the third, fourth, and fifth trials of each cycle due to washout from lifting the training objects. Generalization was measured by comparing the anticipatory force on these trials, during either the early test phase (trial cycles 31–34) or the end of the test phase (trial cycles 55–70), with the average anticipatory force at the end of the training phase (trial cycles 27–30), and expressed as a percentage of the difference between the actual and expected weight of the test object (2.94 N in Linear+ and Uncorr+ versus 5.89 N in Linear++). Due to the constrained trial order in the test phase (see Paradigm), all four training objects contribute equally to this analysis. For each group, we conducted two-tailed <italic>t</italic>-tests on this generalization metric in each portion of the test phase, and also on the change in this generalization metric from the early portion to the late portion of the test phase.</p><p>In the first experiment, we conducted a two-way repeated-measures ANOVA on log-transformed response times, with factors Group (two levels: Linear+ combined with Linear++ versus Uncorr+) and Epoch (four levels: trial cycles 1–15, 16–30, 31–50, 51–70), and performed four follow-up one-tailed <italic>t</italic>-tests to examine whether the main effect of Group was present in all four Epochs individually. In each of these groups, we also tested for single-trial generalization at the start (first four cycles) and the end (final sixteen cycles) of the test phase, as well as the change from start to end, using two-tailed <italic>t</italic>-tests. In the Linear++ group, where we observed learning of separate categories, we analyzed whether response times were longer on trials involving a category switch (i.e. trials with the test object or trials immediately after the test object). Not surprisingly, participants in our experiments took longer to generate larger anticipatory forces. To account for this uninteresting component of the response time, we fit a linear regression to response time as a function of object weight using data from ‘non-switch’ trials in the Linear+ and Linear++ conditions (i.e. excluding trials with the test object and immediately subsequent trials). This allowed us to determine the predicted response time, based solely on weight, which we then compared to the actual response time to test for additional temporal costs (e.g. associated with switching categories). We found a significant slope of 26.2 ms per Newton of anticipatory force. Thus, when analyzing test object trials in the Linear++ condition, we computed the predicted response time as the average response time for the family objects in ‘non-switch’ trials during the test phase plus 154 ms, as the test object was 5.89 N heavier than the average family object weight. When analyzing trials immediately after the test object, we computed the predicted response time as the average response time for the family objects in ‘non-switch’ trials during the test phase, as the object weight in these trials was, on average, equivalent to the average family object weight.</p><p>We also directly compared the Linear➚ with the Linear++ group, and the Linear➘ with the Linear+ group, using two-tailed, two-sample <italic>t</italic>-tests on the anticipatory force for the test object in the final 16 trial cycles of the test phase, when the outlier weight was similar for each pair of groups.</p></sec><sec id="s4-5"><title>Web-based experiment</title><p>For the web-based experiments, we obtained complete data associated with 196 unique Amazon Mechanical Turk Worker IDs (135 males, 60 females, 1 non-binary) aged 19–70 years old (median 31.5). These workers were paid $1.50 upon successful submission of a complete dataset, and received an additional bonus payment determined by dividing their final score by 100 (max bonus = $0.01/trial = $1.60). Of these participants, 185 individuals reported using their right hand to control their input device and 11 reported using their left hand. They were not screened for visual impairment or prior diagnosis of movement disorder.</p><p>The web-based experiments were designed so that they could only be completed by individuals using the Google Chrome web browser, in full-screen mode and with pointer lock enabled, on a computer with graphics hardware that supports WebGL 2.0, and with a mouse (172 participants) or trackpad (24 participants). Dimensions of the full-screen window displaying the task ranged from (1093, 576) to (2560, 1410) pixels; actual monitor sizes were not collected.</p><p>The objects in the web-based experiments had radii of 2 cm and heights of 3, 4, 5, 6, and 7 cm. They were arranged around a gray metallic ring, had springs attached to their tops, and were rendered via perspective projection to a camera 40 cm behind and 10 cm above the top-center of the foremost object. Since there was no haptic interface, feedback about object weight was provided through vision of the simulated dynamics of a spring-mass-damper system (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). In the web-based Linear++, Linear+, Linear-, and Linear-- conditions, the training objects always weighed 300, 400, 600, and 700 g, while the test object weighed 900, 700, 300, or 100 g, respectively.</p><p>Trials of the web-based experiments were similar to the laboratory experiment, but simplified. There were no auditory cues, haptic feedback, bars above and below the object, or a leaderboard. Each trial consisted of two main phases (<xref ref-type="fig" rid="fig5">Figure 5a</xref>): the clamp phase (trial phase 1), in which the participant clicked and dragged to stretch the spring on top of the object, and the release phase, which was triggered by pressing the Shift key with the spring stretched to a certain distance, and portrayed a simulation of the spring-mass-damper dynamics that would result from the initial conditions created by the spring length (spring constant: 1, damping coefficient: 0.01). The per-trial score <italic>y</italic> was related to the spring-length error in centimeters <italic>e</italic> by <italic>y</italic> = max(0, 1−<italic>e<sup>2</sup></italic>/2.25)*100. The duration of the release phase in seconds <italic>t</italic> (i<italic>.</italic>e. the inter-trial interval, which serves as a time penalty) was modulated according to the spring-length error: <italic>t</italic> = min(0.4*<italic>e<sup>2</sup></italic>, 12). This time penalty was correlated with, but not exactly equal to, the decay time of the oscillations in the visual feedback of the spring.</p><p>Participants received task familiarization through a single, repeatable demo trial that provided an instructed walkthrough of a single trial with the largest of the four training objects. The total number of trials was reduced by half compared to the in-laboratory Linear+ condition, with 60 training trials and 100 test trials. Rest breaks were not required.</p><p>The anticipatory force was measured as the amount of force exerted on the object by the visually simulated spring on the final frame of the clamp phase (<xref ref-type="fig" rid="fig5">Figure 5a</xref>, trial phase 1). Non-learners were defined as those whose anticipatory forces for the training objects during the final five cycles of the training phase <italic>or</italic> the final five cycles of the test phase did not show a mild positive correlation with the simulated weights (<italic>α</italic> = 0.10). Forty-seven participants were excluded from the four groups of the web-based experiment by this criterion, resulting in sample sizes of 37, 36, 37, and 39 individuals, respectively, in the Linear++, Linear+, Linear-, and Linear-- groups. This high rate of exclusion was not due to task difficulty, but to the fact that many participants in the web-based experiment adopted strategies that minimized effort at the expense of time and accuracy. Additionally, we excluded as outliers any anticipatory forces that were more than four scaled median absolute deviations from the median anticipatory force applied by a given participant to a given object, resulting in 1398 exclusions (4.46%).</p><p>To estimate required sample sizes for the web-based experiments, we simulated bimodal distributions of ‘learners’ and ‘non-learners’ with different sample sizes and calculated the proportion of simulations in which the two-Gaussian mixture model outperformed the single Gaussian model. We estimated that the learner and non-learner group means would be separated by 3.5 standard deviations, and we assumed that learners and non-learners are normally distributed, have equal variance, and occur in equal proportions. We found that a sample size of 36 participants led the two-Gaussian model to be correctly favored by AIC in 85 % of our simulations.</p><p>We analyzed the distributions of anticipatory forces produced for the outlier in the final five cycles of the test phase. We fit both a single-Gaussian and a two-Gaussian mixture model using the R package <italic>mclust</italic> (<xref ref-type="bibr" rid="bib105">R Development Core Team, 2020</xref>; <xref ref-type="bibr" rid="bib47">Fraley and Raftery, 2002</xref>), and estimated confidence intervals on the fit parameters by parametric bootstrap with 10,000 samples. Model comparisons based on AIC and BIC yielded the same pattern of results; we report only AIC in the text. The same analysis of bimodality was conducted for the laboratory experiments by combining the Linear+ and +Linear groups in one analysis, and the Linear++ and ++Linear groups in another analysis. For the laboratory experiments, we fit the single-Gaussian and two-Gaussian models to the distributions of the difference between anticipatory force for the outlier and the family-predicted weight of the outlier in the final 16 trial cycles of the test phase.</p><p>All source data, analysis code, and figure generation code is available in the supplementary files.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Project administration, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Methodology, Resources, Supervision, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Methodology, Supervision, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All experiments were conducted in accordance with the 1964 Declaration of Helsinki, following protocol approved by the Columbia University Institutional Review Board (IRB-AAAR9148). Written informed consent was obtained from all participants prior to their participation.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-71627-transrepform1-v3.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All source data, analysis code, and figure generation code is available in the supplementary files.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Ian Howard for the design of the 3BOT manipulandum.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addou</surname><given-names>T</given-names></name><name><surname>Krouchev</surname><given-names>N</given-names></name><name><surname>Kalaska</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Colored context cues can facilitate the ability to learn and to switch between multiple dynamical force fields</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>163</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1152/jn.00869.2010</pub-id><pub-id pub-id-type="pmid">21490278</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allan</surname><given-names>MS</given-names></name><name><surname>Clause</surname><given-names>D</given-names></name><name><surname>Pierre</surname><given-names>F</given-names></name><name><surname>John</surname><given-names>K</given-names></name><name><surname>Nathalie</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Comparing Cerebellar and Motor Cortical Activity in Reaching and Grasping</article-title><source>Canadian Journal of Neurological Sciences / Journal Canadien Des Sciences Neurologiques</source><volume>20</volume><fpage>S53</fpage><lpage>S61</lpage><pub-id pub-id-type="doi">10.1017/S0317167100048538</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The adaptive nature of human categorization</article-title><source>Psychological Review</source><volume>98</volume><fpage>409</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.98.3.409</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anguera</surname><given-names>JA</given-names></name><name><surname>Reuter-Lorenz</surname><given-names>PA</given-names></name><name><surname>Willingham</surname><given-names>DT</given-names></name><name><surname>Seidler</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Contributions of spatial working memory to visuomotor learning</article-title><source>Journal of Cognitive Neuroscience</source><volume>22</volume><fpage>1917</fpage><lpage>1930</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21351</pub-id><pub-id pub-id-type="pmid">19803691</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antzoulatos</surname><given-names>EG</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Differences between neural activity in prefrontal cortex and striatum during learning of novel abstract categories</article-title><source>Neuron</source><volume>71</volume><fpage>243</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.040</pub-id><pub-id pub-id-type="pmid">21791284</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antzoulatos</surname><given-names>EG</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Increases in functional connectivity between prefrontal cortex and striatum during category learning</article-title><source>Neuron</source><volume>83</volume><fpage>216</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.05.005</pub-id><pub-id pub-id-type="pmid">24930701</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Arbib</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1981">1981</year><chapter-title>Perceptual structures and distributed motor control</chapter-title><person-group person-group-type="editor"><name><surname>Brooks</surname><given-names>VB</given-names></name></person-group><source>Handbook of Physiology-The Nervous System II</source><publisher-name>American Physiological Society</publisher-name><fpage>1449</fpage><lpage>1480</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Townsend</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Varieties of perceptual independence</article-title><source>Psychological Review</source><volume>93</volume><fpage>154</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.93.2.154</pub-id><pub-id pub-id-type="pmid">3714926</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Alfonso-Reese</surname><given-names>LA</given-names></name><name><surname>Turken</surname><given-names>AU</given-names></name><name><surname>Waldron</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A neuropsychological theory of multiple systems in category learning</article-title><source>Psychological Review</source><volume>105</volume><fpage>442</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.105.3.442</pub-id><pub-id pub-id-type="pmid">9697427</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Maddox</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Human category learning</article-title><source>Annual Review of Psychology</source><volume>56</volume><fpage>149</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.56.091103.070217</pub-id><pub-id pub-id-type="pmid">15709932</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Maddox</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Human category learning 2.0</article-title><source>Annals of the New York Academy of Sciences</source><volume>1224</volume><fpage>147</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2010.05874.x</pub-id><pub-id pub-id-type="pmid">21182535</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Battaglia-Mayer</surname><given-names>A</given-names></name><name><surname>Caminiti</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Parieto-frontal networks for eye–hand coordination and movements</chapter-title><person-group person-group-type="editor"><name><surname>Vallar</surname><given-names>G</given-names></name><name><surname>Coslett</surname><given-names>HB</given-names></name></person-group><source>Handbook of Clinical Neurology-The Parietal Lobe</source><publisher-name>Elsevier</publisher-name><fpage>499</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1016/B978-0-444-63622-5.00026-7</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baugh</surname><given-names>LA</given-names></name><name><surname>Kao</surname><given-names>M</given-names></name><name><surname>Johansson</surname><given-names>RS</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Material evidence: interaction of well-learned priors and sensorimotor memory when lifting objects</article-title><source>Journal of Neurophysiology</source><volume>108</volume><fpage>1262</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1152/jn.00263.2012</pub-id><pub-id pub-id-type="pmid">22696542</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baugh</surname><given-names>LA</given-names></name><name><surname>Yak</surname><given-names>A</given-names></name><name><surname>Johansson</surname><given-names>RS</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Representing multiple object weights: competing priors and sensorimotor memories</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>1615</fpage><lpage>1625</lpage><pub-id pub-id-type="doi">10.1152/jn.00282.2016</pub-id><pub-id pub-id-type="pmid">27385795</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berniker</surname><given-names>M</given-names></name><name><surname>Franklin</surname><given-names>DW</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Kording</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Motor learning of novel dynamics is not represented in a single global coordinate system: evaluation of mixed coordinate representations and local learning</article-title><source>Journal of Neurophysiology</source><volume>111</volume><fpage>1165</fpage><lpage>1182</lpage><pub-id pub-id-type="doi">10.1152/jn.00493.2013</pub-id><pub-id pub-id-type="pmid">24353296</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowman</surname><given-names>CR</given-names></name><name><surname>Zeithamova</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Abstract Memory Representations in the Ventromedial Prefrontal Cortex and Hippocampus Support Concept Generalization</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2605</fpage><lpage>2614</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2811-17.2018</pub-id><pub-id pub-id-type="pmid">29437891</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brashers-Krug</surname><given-names>T</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Consolidation in human motor memory</article-title><source>Nature</source><volume>382</volume><fpage>252</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1038/382252a0</pub-id><pub-id pub-id-type="pmid">8717039</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname><given-names>DA</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Mehring</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Motor task variation induces structural learning</article-title><source>Current Biology</source><volume>19</volume><fpage>352</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.01.036</pub-id><pub-id pub-id-type="pmid">19217296</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname><given-names>DA</given-names></name><name><surname>Mehring</surname><given-names>C</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Structure learning in action</article-title><source>Behavioural Brain Research</source><volume>206</volume><fpage>157</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2009.08.031</pub-id><pub-id pub-id-type="pmid">19720086</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>C</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Visual properties of neurons in a polysensory area in superior temporal sulcus of the macaque</article-title><source>Journal of Neurophysiology</source><volume>46</volume><fpage>369</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1152/jn.1981.46.2.369</pub-id><pub-id pub-id-type="pmid">6267219</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckingham</surname><given-names>G</given-names></name><name><surname>Cant</surname><given-names>JS</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Living in a material world: how visual cues to material properties affect the way that we lift objects and perceive their weight</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>3111</fpage><lpage>3118</lpage><pub-id pub-id-type="doi">10.1152/jn.00515.2009</pub-id><pub-id pub-id-type="pmid">19793879</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bursztyn</surname><given-names>LLCD</given-names></name><name><surname>Ganesh</surname><given-names>G</given-names></name><name><surname>Imamizu</surname><given-names>H</given-names></name><name><surname>Kawato</surname><given-names>M</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural correlates of internal-model loading</article-title><source>Current Biology</source><volume>16</volume><fpage>2440</fpage><lpage>2445</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2006.10.051</pub-id><pub-id pub-id-type="pmid">17174919</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caithness</surname><given-names>G</given-names></name><name><surname>Osu</surname><given-names>R</given-names></name><name><surname>Bays</surname><given-names>P</given-names></name><name><surname>Chase</surname><given-names>H</given-names></name><name><surname>Klassen</surname><given-names>J</given-names></name><name><surname>Kawato</surname><given-names>M</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Failure to consolidate the consolidation theory of learning for sensorimotor adaptation tasks</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>8662</fpage><lpage>8671</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2214-04.2004</pub-id><pub-id pub-id-type="pmid">15470131</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>GA</given-names></name><name><surname>Grossberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>ART 2: self-organization of stable category recognition codes for analog input patterns</article-title><source>Applied Optics</source><volume>26</volume><fpage>4919</fpage><lpage>4930</lpage><pub-id pub-id-type="doi">10.1364/AO.26.004919</pub-id><pub-id pub-id-type="pmid">20523470</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castiello</surname><given-names>U</given-names></name><name><surname>Begliomini</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The cortical control of visually guided grasping</article-title><source>The Neuroscientist</source><volume>14</volume><fpage>157</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1177/1073858407312080</pub-id><pub-id pub-id-type="pmid">18219055</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>LL</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Attribute-based neural substrates in temporal cortex for perceiving and knowing about objects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>913</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1038/13217</pub-id><pub-id pub-id-type="pmid">10491613</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>LL</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Representation of manipulable man-made objects in the dorsal stream</article-title><source>NeuroImage</source><volume>12</volume><fpage>478</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.1006/nimg.2000.0635</pub-id><pub-id pub-id-type="pmid">10988041</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chouinard</surname><given-names>PA</given-names></name><name><surname>Leonard</surname><given-names>G</given-names></name><name><surname>Paus</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Role of the primary motor and dorsal premotor cortices in the anticipation of forces during object lifting</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>2277</fpage><lpage>2284</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4649-04.2005</pub-id><pub-id pub-id-type="pmid">15745953</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chouinard</surname><given-names>PA</given-names></name><name><surname>Large</surname><given-names>ME</given-names></name><name><surname>Chang</surname><given-names>EC</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dissociable neural mechanisms for determining the perceived heaviness of objects and the predicted weight of objects during lifting: an fMRI investigation of the size-weight illusion</article-title><source>NeuroImage</source><volume>44</volume><fpage>200</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.08.023</pub-id><pub-id pub-id-type="pmid">18801445</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clapper</surname><given-names>JP</given-names></name><name><surname>Bower</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Learning and applying category knowledge in unsupervised domains</article-title><source>The Psychology of Learning and Motivation</source><volume>27</volume><fpage>65</fpage><lpage>108</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clayards</surname><given-names>M</given-names></name><name><surname>Tanenhaus</surname><given-names>MK</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name><name><surname>Jacobs</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Perception of speech reflects optimal use of probabilistic speech cues</article-title><source>Cognition</source><volume>108</volume><fpage>804</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2008.04.004</pub-id><pub-id pub-id-type="pmid">18582855</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Lifting a familiar object: visual size analysis, not memory for object weight, scales lift force</article-title><source>Experimental Brain Research</source><volume>188</volume><fpage>551</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1007/s00221-008-1392-y</pub-id><pub-id pub-id-type="pmid">18443767</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AM</given-names></name><name><surname>Quillian</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Retrieval time from semantic memory</article-title><source>Journal of Verbal Learning and Verbal Behavior</source><volume>8</volume><fpage>240</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/S0022-5371(69)80069-1</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cothros</surname><given-names>N</given-names></name><name><surname>Wong</surname><given-names>JD</given-names></name><name><surname>Gribble</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Are there distinct neural representations of object and limb dynamics?</article-title><source>Experimental Brain Research</source><volume>173</volume><fpage>689</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1007/s00221-006-0411-0</pub-id><pub-id pub-id-type="pmid">16525798</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cothros</surname><given-names>N</given-names></name><name><surname>Wong</surname><given-names>J</given-names></name><name><surname>Gribble</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Visual cues signaling object grasp reduce interference in motor learning</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>2112</fpage><lpage>2120</lpage><pub-id pub-id-type="doi">10.1152/jn.00493.2009</pub-id><pub-id pub-id-type="pmid">19657075</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Criscimagna-Hemminger</surname><given-names>SE</given-names></name><name><surname>Bastian</surname><given-names>AJ</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Size of error affects cerebellar contributions to motor learning</article-title><source>Journal of Neurophysiology</source><volume>103</volume><fpage>2275</fpage><lpage>2284</lpage><pub-id pub-id-type="doi">10.1152/jn.00822.2009</pub-id><pub-id pub-id-type="pmid">20164398</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culham</surname><given-names>JC</given-names></name><name><surname>Danckert</surname><given-names>SL</given-names></name><name><surname>DeSouza</surname><given-names>JFX</given-names></name><name><surname>Gati</surname><given-names>JS</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Visually guided grasping produces fMRI activation in dorsal but not ventral stream brain areas</article-title><source>Experimental Brain Research</source><volume>153</volume><fpage>180</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1007/s00221-003-1591-5</pub-id><pub-id pub-id-type="pmid">12961051</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danion</surname><given-names>F</given-names></name><name><surname>Diamond</surname><given-names>JS</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The role of haptic feedback when manipulating nonrigid objects</article-title><source>Journal of Neurophysiology</source><volume>107</volume><fpage>433</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1152/jn.00738.2011</pub-id><pub-id pub-id-type="pmid">22013237</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>PR</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Scott</surname><given-names>SH</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Common encoding of novel dynamic loads applied to the hand and arm</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>5425</fpage><lpage>5429</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0429-05.2005</pub-id><pub-id pub-id-type="pmid">15930392</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donchin</surname><given-names>O</given-names></name><name><surname>Francis</surname><given-names>JT</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Quantifying generalization from trial-by-trial behavior of adaptive systems that learn with basis functions: theory and experiments in human motor control</article-title><source>The Journal of Neuroscience</source><volume>23</volume><fpage>9032</fpage><lpage>9045</lpage><pub-id pub-id-type="pmid">14534237</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erez</surname><given-names>J</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Kendall</surname><given-names>W</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Conjunctive Coding of Complex Object Features</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>2271</fpage><lpage>2282</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv081</pub-id><pub-id pub-id-type="pmid">25921583</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez-Ruiz</surname><given-names>J</given-names></name><name><surname>Wong</surname><given-names>W</given-names></name><name><surname>Armstrong</surname><given-names>IT</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Relation between reaction time and reach errors during visuomotor adaptation</article-title><source>Behavioural Brain Research</source><volume>219</volume><fpage>8</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2010.11.060</pub-id><pub-id pub-id-type="pmid">21138745</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Beltzner</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Independence of perceptual and sensorimotor predictions in the size-weight illusion</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>737</fpage><lpage>741</lpage><pub-id pub-id-type="doi">10.1038/76701</pub-id><pub-id pub-id-type="pmid">10862708</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>King</surname><given-names>S</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Johansson</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Sensorimotor prediction and memory in object manipulation</article-title><source>Canadian Journal of Experimental Psychology = Revue Canadienne de Psychologie Experimentale</source><volume>55</volume><fpage>87</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1037/h0087355</pub-id><pub-id pub-id-type="pmid">11433790</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Bowman</surname><given-names>MC</given-names></name><name><surname>Johansson</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Control strategies in object manipulation tasks</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>650</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.10.005</pub-id><pub-id pub-id-type="pmid">17084619</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Bittner</surname><given-names>JP</given-names></name><name><surname>Johansson</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Experience can change distinct size-weight priors engaged in lifting objects and judging their weights</article-title><source>Current Biology</source><volume>18</volume><fpage>1742</fpage><lpage>1747</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.09.042</pub-id><pub-id pub-id-type="pmid">19026545</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fraley</surname><given-names>C</given-names></name><name><surname>Raftery</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Enhanced Model-Based Clustering, Density Estimation, and Discriminant Analysis Software: MCLUST</article-title><source>Journal of Classification</source><volume>20</volume><fpage>263</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.21236/ada459792</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Categorical representation of visual stimuli in the primate prefrontal cortex</article-title><source>Science</source><volume>291</volume><fpage>312</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1126/science.291.5502.312</pub-id><pub-id pub-id-type="pmid">11209083</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Assad</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Distinct encoding of spatial and nonspatial visual information in parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>5671</fpage><lpage>5680</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2878-08.2009</pub-id><pub-id pub-id-type="pmid">19403833</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Cant</surname><given-names>JS</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Representation of object weight in human ventral visual cortex</article-title><source>Current Biology</source><volume>24</volume><fpage>1866</fpage><lpage>1873</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.06.046</pub-id><pub-id pub-id-type="pmid">25065755</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gandolfo</surname><given-names>F</given-names></name><name><surname>Mussa-Ivaldi</surname><given-names>FA</given-names></name><name><surname>Bizzi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Motor learning by field approximation</article-title><source>PNAS</source><volume>93</volume><fpage>3843</fpage><lpage>3846</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.9.3843</pub-id><pub-id pub-id-type="pmid">8632977</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>I</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Anderson</surname><given-names>AW</given-names></name><name><surname>Skudlarski</surname><given-names>P</given-names></name><name><surname>Gore</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Activation of the middle fusiform “face area” increases with expertise in recognizing novel objects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>568</fpage><lpage>573</lpage><pub-id pub-id-type="doi">10.1038/9224</pub-id><pub-id pub-id-type="pmid">10448223</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Separate visual pathways for perception and action</article-title><source>Trends in Neurosciences</source><volume>15</volume><fpage>20</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(92)90344-8</pub-id><pub-id pub-id-type="pmid">1374953</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>AM</given-names></name><name><surname>Forssberg</surname><given-names>H</given-names></name><name><surname>Johansson</surname><given-names>RS</given-names></name><name><surname>Westling</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Visual size cues in the programming of manipulative forces during precision grip</article-title><source>Experimental Brain Research</source><volume>83</volume><fpage>477</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1007/BF00229824</pub-id><pub-id pub-id-type="pmid">2026190</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>AM</given-names></name><name><surname>Westling</surname><given-names>G</given-names></name><name><surname>Cole</surname><given-names>KJ</given-names></name><name><surname>Johansson</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Memory representations underlying motor commands used during manipulation of common and novel objects</article-title><source>Journal of Neurophysiology</source><volume>69</volume><fpage>1789</fpage><lpage>1796</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.69.6.1789</pub-id><pub-id pub-id-type="pmid">8350123</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grafton</surname><given-names>ST</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The cognitive neuroscience of prehension: recent developments</article-title><source>Experimental Brain Research</source><volume>204</volume><fpage>475</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1007/s00221-010-2315-2</pub-id><pub-id pub-id-type="pmid">20532487</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Canini</surname><given-names>KR</given-names></name><name><surname>Sanborn</surname><given-names>AN</given-names></name><name><surname>Navarro</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Unifying rational models of categorization via the hierarchical Dirichlet process</article-title><conf-name>Proceedings of the 29th Annual Conference of the Cognitive Science Society</conf-name><fpage>323</fpage><lpage>328</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The lateral occipital complex and its role in object recognition</article-title><source>Vision Research</source><volume>41</volume><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00073-6</pub-id><pub-id pub-id-type="pmid">11322983</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id><pub-id pub-id-type="pmid">24962370</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hartigan</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1975">1975</year><source>Clustering Algorithms</source><publisher-name>John Wiley &amp; Sons</publisher-name></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heald</surname><given-names>JB</given-names></name><name><surname>Ingram</surname><given-names>JN</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multiple motor memories are learned to control different points on a tool</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>300</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0324-5</pub-id><pub-id pub-id-type="pmid">29736420</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Heald</surname><given-names>JB</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Contextual Inference Underlies the Learning of Sensorimotor Repertoires</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.11.23.394320</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hock</surname><given-names>HS</given-names></name><name><surname>Kelso</surname><given-names>JA</given-names></name><name><surname>Schöner</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Bistability and hysteresis in the organization of apparent motion patterns</article-title><source>Journal of Experimental Psychology</source><volume>19</volume><fpage>63</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.19.1.63</pub-id><pub-id pub-id-type="pmid">8440989</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>IS</given-names></name><name><surname>Ingram</surname><given-names>JN</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Composition and decomposition in bimanual dynamic learning</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>10531</fpage><lpage>10540</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3473-08.2008</pub-id><pub-id pub-id-type="pmid">18923029</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>IS</given-names></name><name><surname>Ingram</surname><given-names>JN</given-names></name><name><surname>Franklin</surname><given-names>DW</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Gone in 0.6 seconds: the encoding of motor memories depends on recent sensorimotor states</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>12756</fpage><lpage>12768</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5909-11.2012</pub-id><pub-id pub-id-type="pmid">22972999</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>IS</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Franklin</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The effect of contextual cues on the encoding of motor memories</article-title><source>Journal of Neurophysiology</source><volume>109</volume><fpage>2632</fpage><lpage>2644</lpage><pub-id pub-id-type="doi">10.1152/jn.00773.2012</pub-id><pub-id pub-id-type="pmid">23446696</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huberdeau</surname><given-names>DM</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Haith</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dual-process decomposition in human sensorimotor adaptation</article-title><source>Current Opinion in Neurobiology</source><volume>33</volume><fpage>71</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.03.003</pub-id><pub-id pub-id-type="pmid">25827272</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphreys</surname><given-names>GW</given-names></name><name><surname>Forde</surname><given-names>EME</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Hierarchies, similarity, and interactivity in object recognition: “Category-specific” neuropsychological deficits</article-title><source>Behavioral and Brain Sciences</source><volume>24</volume><fpage>453</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1017/S0140525X01004150</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title><source>Neuron</source><volume>76</volume><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id><pub-id pub-id-type="pmid">23259955</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huttenlocher</surname><given-names>J</given-names></name><name><surname>Hedges</surname><given-names>LV</given-names></name><name><surname>Vevea</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Why do categories affect stimulus judgment?</article-title><source>Journal of Experimental Psychology. General</source><volume>129</volume><fpage>220</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1037//0096-3445.129.2.220</pub-id><pub-id pub-id-type="pmid">10868335</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imamizu</surname><given-names>H</given-names></name><name><surname>Miyauchi</surname><given-names>S</given-names></name><name><surname>Tamada</surname><given-names>T</given-names></name><name><surname>Sasaki</surname><given-names>Y</given-names></name><name><surname>Takino</surname><given-names>R</given-names></name><name><surname>Pütz</surname><given-names>B</given-names></name><name><surname>Yoshioka</surname><given-names>T</given-names></name><name><surname>Kawato</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Human cerebellar activity reflecting an acquired internal model of a new tool</article-title><source>Nature</source><volume>403</volume><fpage>192</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1038/35003194</pub-id><pub-id pub-id-type="pmid">10646603</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ingram</surname><given-names>JN</given-names></name><name><surname>Sadeghi</surname><given-names>M</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Haith</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An error-tuned model for sensorimotor learning</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005883</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005883</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeannerod</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1981">1981</year><chapter-title>Intersegmental coordination during reaching at natural visual objects</chapter-title><person-group person-group-type="editor"><name><surname>Long</surname><given-names>J</given-names></name><name><surname>Baddeley</surname><given-names>A</given-names></name></person-group><source>Attention and Performance IX</source><publisher-name>Erlbaum</publisher-name><fpage>153</fpage><lpage>169</lpage></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeannerod</surname><given-names>M</given-names></name><name><surname>Arbib</surname><given-names>MA</given-names></name><name><surname>Rizzolatti</surname><given-names>G</given-names></name><name><surname>Sakata</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Grasping objects: the cortical mechanisms of visuomotor transformation</article-title><source>Trends in Neurosciences</source><volume>18</volume><fpage>314</fpage><lpage>320</lpage><pub-id pub-id-type="pmid">7571012</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenmalm</surname><given-names>P</given-names></name><name><surname>Schmitz</surname><given-names>C</given-names></name><name><surname>Forssberg</surname><given-names>H</given-names></name><name><surname>Ehrsson</surname><given-names>HH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Lighter or heavier than predicted: neural correlates of corrective mechanisms during erroneously programmed lifts</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>9015</fpage><lpage>9021</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5045-05.2006</pub-id><pub-id pub-id-type="pmid">16943559</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johansson</surname><given-names>RS</given-names></name><name><surname>Westling</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Coordinated isometric muscle commands adequately and erroneously programmed for the weight during lifting task with precision grip</article-title><source>Experimental Brain Research</source><volume>71</volume><fpage>59</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1007/BF00247522</pub-id><pub-id pub-id-type="pmid">3416958</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johansson</surname><given-names>RS</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Coding and use of tactile signals from the fingertips in object manipulation tasks</article-title><source>Nature Reviews. Neuroscience</source><volume>10</volume><fpage>345</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1038/nrn2621</pub-id><pub-id pub-id-type="pmid">19352402</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kagerer</surname><given-names>FA</given-names></name><name><surname>Contreras-Vidal</surname><given-names>JL</given-names></name><name><surname>Stelmach</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Adaptation to gradual as compared with sudden visuo-motor distortions</article-title><source>Experimental Brain Research</source><volume>115</volume><fpage>557</fpage><lpage>561</lpage><pub-id pub-id-type="doi">10.1007/pl00005727</pub-id><pub-id pub-id-type="pmid">9262212</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalish</surname><given-names>ML</given-names></name><name><surname>Lewandowsky</surname><given-names>S</given-names></name><name><surname>Kruschke</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Population of linear experts: knowledge partitioning and function learning</article-title><source>Psychological Review</source><volume>111</volume><fpage>1072</fpage><lpage>1099</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.111.4.1072</pub-id><pub-id pub-id-type="pmid">15482074</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karniel</surname><given-names>A</given-names></name><name><surname>Mussa-Ivaldi</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Does the motor control system use multiple models and context switching to cope with a variable environment?</article-title><source>Experimental Brain Research</source><volume>143</volume><fpage>520</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1007/s00221-002-1054-4</pub-id><pub-id pub-id-type="pmid">11914799</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemp</surname><given-names>C</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The discovery of structural form</article-title><source>PNAS</source><volume>105</volume><fpage>10687</fpage><lpage>10692</lpage><pub-id pub-id-type="doi">10.1073/pnas.0802631105</pub-id><pub-id pub-id-type="pmid">18669663</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klassen</surname><given-names>J</given-names></name><name><surname>Tong</surname><given-names>C</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Learning and recall of incremental kinematic and dynamic sensorimotor transformations</article-title><source>Experimental Brain Research</source><volume>164</volume><fpage>250</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1007/s00221-005-2247-4</pub-id><pub-id pub-id-type="pmid">15947919</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in Psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1068/v070821</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kluzik</surname><given-names>J</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name><name><surname>Bastian</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Reach adaptation: what determines whether we learn an internal model of the tool or adapt the model of our arm?</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>1455</fpage><lpage>1464</lpage><pub-id pub-id-type="doi">10.1152/jn.90334.2008</pub-id><pub-id pub-id-type="pmid">18596187</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neural representations for object perception: structure, category, and adaptive coding</article-title><source>Annual Review of Neuroscience</source><volume>34</volume><fpage>45</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-060909-153218</pub-id><pub-id pub-id-type="pmid">21438683</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ghilardi</surname><given-names>MF</given-names></name><name><surname>Ghez</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Independent learning of internal models for kinematic and dynamic control of reaching</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1026</fpage><lpage>1031</lpage><pub-id pub-id-type="doi">10.1038/14826</pub-id><pub-id pub-id-type="pmid">10526344</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Pine</surname><given-names>ZM</given-names></name><name><surname>Ghilardi</surname><given-names>MF</given-names></name><name><surname>Ghez</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Learning of visuomotor transformations for vectorial planning of reaching trajectories</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>8916</fpage><lpage>8924</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-23-08916.2000</pub-id><pub-id pub-id-type="pmid">11102502</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id><pub-id pub-id-type="pmid">19109916</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Medin</surname><given-names>DL</given-names></name><name><surname>Gureckis</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>SUSTAIN: a network model of category learning</article-title><source>Psychological Review</source><volume>111</volume><fpage>309</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.111.2.309</pub-id><pub-id pub-id-type="pmid">15065912</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname><given-names>BZ</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What drives the organization of object knowledge in the brain?</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>97</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.01.004</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malfait</surname><given-names>N</given-names></name><name><surname>Shiller</surname><given-names>DM</given-names></name><name><surname>Ostry</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Transfer of motor learning across arm configurations</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>9656</fpage><lpage>9660</lpage><pub-id pub-id-type="pmid">12427820</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malfait</surname><given-names>N</given-names></name><name><surname>Ostry</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Is interlimb transfer of force-field adaptation a cognitive response to the sudden introduction of load?</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>8084</fpage><lpage>8089</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1742-04.2004</pub-id><pub-id pub-id-type="pmid">15371509</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mason</surname><given-names>CR</given-names></name><name><surname>Hendrix</surname><given-names>CM</given-names></name><name><surname>Ebner</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Purkinje cells signal hand shape and grasp force during reach-to-grasp in the monkey</article-title><source>Journal of Neurophysiology</source><volume>95</volume><fpage>144</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1152/jn.00492.2005</pub-id><pub-id pub-id-type="pmid">16162833</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazzoni</surname><given-names>P</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An implicit plan overrides an explicit strategy during visuomotor adaptation</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>3642</fpage><lpage>3645</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5317-05.2006</pub-id><pub-id pub-id-type="pmid">16597717</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGarity-Shipley</surname><given-names>MR</given-names></name><name><surname>Heald</surname><given-names>JB</given-names></name><name><surname>Ingram</surname><given-names>JN</given-names></name><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Motor memories in manipulation tasks are linked to contact goals between objects</article-title><source>Journal of Neurophysiology</source><volume>124</volume><fpage>994</fpage><lpage>1004</lpage><pub-id pub-id-type="doi">10.1152/jn.00252.2020</pub-id><pub-id pub-id-type="pmid">32816611</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Medin</surname><given-names>DL</given-names></name><name><surname>Schaffer</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Context theory of classification learning</article-title><source>Psychological Review</source><volume>85</volume><fpage>207</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.3.207</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mervis</surname><given-names>CB</given-names></name><name><surname>Rosch</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Categorization of Natural Objects</article-title><source>Annual Review of Psychology</source><volume>32</volume><fpage>89</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1146/annurev.ps.32.020181.000513</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monaco</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Medendorp</surname><given-names>WP</given-names></name><name><surname>Crawford</surname><given-names>JD</given-names></name><name><surname>Fiehler</surname><given-names>K</given-names></name><name><surname>Henriques</surname><given-names>DYP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Functional magnetic resonance imaging adaptation reveals the cortical networks for processing grasp-relevant object properties</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>1540</fpage><lpage>1554</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht006</pub-id><pub-id pub-id-type="pmid">23362111</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaki</surname><given-names>D</given-names></name><name><surname>Kurtzer</surname><given-names>I</given-names></name><name><surname>Scott</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Limited transfer of learning between unimanual and bimanual skills within the same limb</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1364</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1038/nn1785</pub-id><pub-id pub-id-type="pmid">17028583</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osu</surname><given-names>R</given-names></name><name><surname>Hirai</surname><given-names>S</given-names></name><name><surname>Yoshioka</surname><given-names>T</given-names></name><name><surname>Kawato</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Random presentation enables subjects to adapt to two opposing forces on the hand</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>111</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1038/nn1184</pub-id><pub-id pub-id-type="pmid">14745452</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poltoratski</surname><given-names>S</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hysteresis in the dynamic perception of scenes and objects</article-title><source>Journal of Experimental Psychology. General</source><volume>143</volume><fpage>1875</fpage><lpage>1892</lpage><pub-id pub-id-type="doi">10.1037/a0037365</pub-id><pub-id pub-id-type="pmid">25150947</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Posner</surname><given-names>MI</given-names></name><name><surname>Keele</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>On the genesis of abstract ideas</article-title><source>Journal of Experimental Psychology</source><volume>77</volume><fpage>353</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1037/h0025953</pub-id><pub-id pub-id-type="pmid">5665566</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Spatial transformations in the parietal cortex using basis functions</article-title><source>Journal of Cognitive Neuroscience</source><volume>9</volume><fpage>222</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1162/jocn.1997.9.2.222</pub-id><pub-id pub-id-type="pmid">23962013</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Snyder</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Computational approaches to sensorimotor transformations</article-title><source>Nature Neuroscience</source><volume>3 Suppl</volume><fpage>1192</fpage><lpage>1198</lpage><pub-id pub-id-type="doi">10.1038/81469</pub-id><pub-id pub-id-type="pmid">11127837</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Development Core Team</collab></person-group><year iso-8601-date="2020">2020</year><data-title>R: A language and environment for statistical computing</data-title><version designator="2.6.2">2.6.2</version><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name><ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raz</surname><given-names>G</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning in Infancy Is Active, Endogenously Motivated, and Depends on the Prefrontal Cortices</article-title><source>Annual Review of Developmental Psychology</source><volume>2</volume><fpage>247</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1146/annurev-devpsych-121318-084841</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reber</surname><given-names>PJ</given-names></name><name><surname>Stark</surname><given-names>CEL</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical areas supporting category learning identified using functional MRI</article-title><source>PNAS</source><volume>95</volume><fpage>747</fpage><lpage>750</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.2.747</pub-id><pub-id pub-id-type="pmid">9435264</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rips</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="1989">1989</year><chapter-title>Similarity, typicality, and categorization</chapter-title><person-group person-group-type="editor"><name><surname>Vosniadou</surname><given-names>S</given-names></name><name><surname>Ortony</surname><given-names>A</given-names></name></person-group><source>Similarity and Analogical Reasoning</source><publisher-name>Cambridge University Press</publisher-name><fpage>21</fpage><lpage>59</lpage></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G</given-names></name><name><surname>Luppino</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The cortical motor system</article-title><source>Neuron</source><volume>31</volume><fpage>889</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00423-8</pub-id><pub-id pub-id-type="pmid">11580891</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roemmich</surname><given-names>RT</given-names></name><name><surname>Bastian</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Two ways to save a newly learned motor pattern</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>3519</fpage><lpage>3530</lpage><pub-id pub-id-type="doi">10.1152/jn.00965.2014</pub-id><pub-id pub-id-type="pmid">25855699</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saijo</surname><given-names>N</given-names></name><name><surname>Gomi</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multiple motor learning strategies in visuomotor rotation</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e9399</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0009399</pub-id><pub-id pub-id-type="pmid">20195373</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salinas</surname><given-names>E</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Transfer of coded information from sensory to motor networks</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>6461</fpage><lpage>6474</lpage><pub-id pub-id-type="pmid">7472409</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schacter</surname><given-names>DL</given-names></name><name><surname>Cooper</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Implicit and explicit memory for novel visual objects: structure and function</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>19</volume><fpage>995</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1037//0278-7393.19.5.995</pub-id><pub-id pub-id-type="pmid">8409854</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwettmann</surname><given-names>S</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Invariant representations of mass in the human brain</article-title><source>eLife</source><volume>8</volume><elocation-id>e46619</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46619</pub-id><pub-id pub-id-type="pmid">31845887</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seger</surname><given-names>CA</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Category learning in the brain</article-title><source>Annual Review of Neuroscience</source><volume>33</volume><fpage>203</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135546</pub-id><pub-id pub-id-type="pmid">20572771</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadmehr</surname><given-names>R</given-names></name><name><surname>Mussa-Ivaldi</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Adaptive representation of dynamics during learning of a motor task</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>3208</fpage><lpage>3224</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-05-03208.1994</pub-id><pub-id pub-id-type="pmid">8182467</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheahan</surname><given-names>HR</given-names></name><name><surname>Franklin</surname><given-names>DW</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Motor Planning, Not Execution, Separates Motor Memories</article-title><source>Neuron</source><volume>92</volume><fpage>773</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.017</pub-id><pub-id pub-id-type="pmid">27817979</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>RN</given-names></name><name><surname>Hovland</surname><given-names>CI</given-names></name><name><surname>Jenkins</surname><given-names>HM</given-names></name></person-group><year iso-8601-date="1961">1961</year><article-title>Learning and memorization of classifications</article-title><source>Psychological Monographs</source><volume>75</volume><fpage>1</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1037/h0093825</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>JA</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Explicit and implicit contributions to learning in a sensorimotor adaptation task</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>3023</fpage><lpage>3032</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3619-13.2014</pub-id><pub-id pub-id-type="pmid">24553942</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Teh</surname><given-names>YW</given-names></name><name><surname>Jordan</surname><given-names>M</given-names></name><name><surname>Beal</surname><given-names>MJ</given-names></name><name><surname>Blei</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Sharing Clusters Among Related Groups: Hierarchical Dirichlet Processes</article-title><conf-name>Advances in Neural Information Processing Systems 17</conf-name></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thoroughman</surname><given-names>KA</given-names></name><name><surname>Shadmehr</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Learning of action through adaptive combination of motor primitives</article-title><source>Nature</source><volume>407</volume><fpage>742</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1038/35037588</pub-id><pub-id pub-id-type="pmid">11048720</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>C</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Kinematics and dynamics are not represented independently in motor working memory: evidence from an interference study</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>1108</fpage><lpage>1113</lpage><pub-id pub-id-type="pmid">11826139</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Haxby</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>What’ and ‘where’ in the human brain</article-title><source>Current Opinion in Neurobiology</source><volume>4</volume><fpage>157</fpage><lpage>165</lpage></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Nuenen</surname><given-names>BFL</given-names></name><name><surname>Kuhtz-Buschbeck</surname><given-names>J</given-names></name><name><surname>Schulz</surname><given-names>C</given-names></name><name><surname>Bloem</surname><given-names>BR</given-names></name><name><surname>Siebner</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Weight-specific anticipatory coding of grip force in human dorsal premotor cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>5272</fpage><lpage>5283</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5673-11.2012</pub-id><pub-id pub-id-type="pmid">22496573</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vanpaemel</surname><given-names>W</given-names></name><name><surname>Storms</surname><given-names>G</given-names></name><name><surname>Ons</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Language Evolution and Computation Bibliography</article-title><conf-name>Proceedings of the 27th Annual Conference of the Cognitive Science Society</conf-name><fpage>2277</fpage><lpage>2282</lpage></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>R</given-names></name><name><surname>Sary</surname><given-names>G</given-names></name><name><surname>Dupont</surname><given-names>P</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Human brain regions involved in visual categorization</article-title><source>NeuroImage</source><volume>16</volume><fpage>401</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1109</pub-id><pub-id pub-id-type="pmid">12030825</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warrington</surname><given-names>EK</given-names></name><name><surname>Taylor</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Two categorical stages of object recognition</article-title><source>Perception</source><volume>7</volume><fpage>695</fpage><lpage>705</lpage><pub-id pub-id-type="doi">10.1068/p070695</pub-id><pub-id pub-id-type="pmid">740510</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Phillips</surname><given-names>G</given-names></name><name><surname>Sekuler</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Hysteresis in the perception of motion direction as evidence for neural cooperativity</article-title><source>Nature</source><volume>324</volume><fpage>253</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1038/324253a0</pub-id><pub-id pub-id-type="pmid">3785395</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Kawato</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Multiple paired forward and inverse models for motor control</article-title><source>Neural Networks</source><volume>11</volume><fpage>1317</fpage><lpage>1329</lpage><pub-id pub-id-type="doi">10.1016/s0893-6080(98)00066-5</pub-id><pub-id pub-id-type="pmid">12662752</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Motor prediction</article-title><source>Current Biology</source><volume>11</volume><fpage>729</fpage><lpage>732</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(01)00432-8</pub-id><pub-id pub-id-type="pmid">11566114</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zipser</surname><given-names>D</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons</article-title><source>Nature</source><volume>331</volume><fpage>679</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.1038/331679a0</pub-id><pub-id pub-id-type="pmid">3344044</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71627.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Crossley</surname><given-names>Matthew</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Macquarie University</institution><country>Australia</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.07.13.452183" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.13.452183"/></front-stub><body><p>This paper provides compelling evidence from several behavioural experiments that recently learned estimates of the mass of novel objects possess a categorical structure in memory. It further links this categorical structure to important aspects of motor control, and provides a compelling window through which to consider the role of multiple systems in the learning and memory of novel object dynamics.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71627.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Crossley</surname><given-names>Matthew</given-names></name><role>Reviewing Editor</role><aff><institution>Macquarie University</institution><country>Australia</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Crossley</surname><given-names>Matthew</given-names></name><role>Reviewer</role><aff><institution>Macquarie University</institution><country>Australia</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.13.452183">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.07.13.452183v1.full">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Motor memories of object dynamics are categorically organized&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Matthew Crossley as Reviewer #1 and Reviewing Editor, and the evaluation has been overseen by Richard Ivry as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>We all agree that the experiments are well designed and well executed, and the results confidently point to the presence of a categorical effect. However, we also agree that the points discussed below represent essential revisions.</p><p>1. We are unsure how clearly these data can be anchored to motor learning and memory. R2 points out that the categorical effect could arise from declarative memory and R3 suggests that the categorical effect could arise from a combination of map-learning + heuristic category judgement (which would likely be declarative or otherwise non-motor).</p><p>This raises a fundamental question: Does this paper demonstrate a categorical structure to motor memory, or is it telling us something about non-motor cognition?</p><p>Please clearly address this possibility in your revision. In so doing, please explicitly state your views on how motor or non-motor both the lab-based and the web-based task are, and please also state to what degree you feel the web-based task is measuring the same phenomena as the lab-based task.</p><p>One observation that may be helpful in considering how to structure your revision is that since the family bias appears to effectively block learning of the true weight, it seems that even if the categorical effect is driven by non-motor systems, it can nevertheless have an important impact on motor systems.</p><p>2. If we accept that the categorical effect is coming from motor systems, then the degree to which it is novel and / or counter to the predictions of leading models should be made clearer. For example, my own intuition is that the categorical effect could arise from switching between &quot;multiple paired forward and inverse models for motor control&quot; (Wolpert and Kawato, 1998). I believe that this would be best addressed by adding formal modelling to your paper, but I am open to the idea that simply rewriting may be sufficient.</p><p>3. The basic model predictions are reasonably clear as is, but I agree with R3 that it isn't as clear as it could be whether or not block-level generalisation effects should exist in your data. Again, adding formal modelling would strongly address this ambiguity, but may not be necessary. If block level generalisation effects are predicted, then please report them in your revision.</p><p>4. R2 and R3 both raise the issue that there are useful reaction time predictions that should be investigated. In essence, we should see RT switch costs wherever a categorical effect is established and not elsewhere. In the revision, please test these predictions. If they are invalid, or the analysis isn't a good idea for some other reason, please explain why.</p><p>5. R2 and R3 both raise issues related to whether the lab-based task was bimodal. Please report this analysis in your revision, or else clearly outline why it isn't appropriate to do so (e.g., insufficient sample size etc.).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>I have only two major recommendations for the authors.</p><p>1. The paper is currently framed by asking &quot;how memories of multiple objects might be organized&quot;, but I believe the paper would be clearer by using the following two passages from the discussion as anchors:</p><p>&quot;Current theories of motor learning often focus on graded generalization of learning across various stimulus and motor parameters as a revealing feature of the underlying computations 38,97-99. In particular, graded patterns of generalization have been taken as evidence that motor learning fundamentally involves associating contextual features of a movement with the target motor parameters in a continuous multi-dimensional space, often termed an associative map. The theoretical significance of our study is that it provides multiple, converging pieces of evidence for a fundamentally different type of organization-motor memories of objects are organized categorically, into families&quot;</p><p>&quot;These various treatments can all be viewed as instances of non-parametric Bayesian models that leverage the hierarchical Dirichlet process, a statistically principled approach to clustering data into a theoretically infinite number of components 123,124. Importantly, this approach has recently been applied to successfully account for an unprecedented range of phenomena in motor learning 125, suggesting that similar computations could also underlie the (in)ability to learn the weight of an outlier object in our lifting task.&quot;</p><p>I think this framing would give you the opportunity explicitly walk the reader through the modelling of Wolpert and Kawato (1998) -- and any more recent motor learning theories that carry on in the internal model tradition and / or the context inference tradition -- such that we understand right from the beginning how the existence of categorical encoding would demand revision of our current thinking. If it turns out that the present data are completely consistent with current models, then you could use this framing to explain why the present data is nonetheless important. This would presumably entail a cogent explanation as to why the present data is itself novel. That is, I would like a clear description of the context effects studied in force fields and in any other relevant motor task such that the categorical effect you report can be seen to stand in sharp contrast.</p><p>2. I believe the strength of the paper would be greatly improved if you performed modelling showing how and why the ideas of Wolpert and Kawato (1998) and related models are inadequate to capture the present data.</p><p>Wolpert, D. M., and Kawato, M. (1998). Multiple paired forward and inverse models for motor control. Neural Networks, 11(7-8), 1317-1329.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I would like to raise several issues.</p><p>1. In figure 2a (Linear+ group), the data of the 1st test trial (31st cycle) for the test object located in the just middle of the four training objects (i.e., ~9N). This result was reasonable from the idea of family effect: The motor system might estimate the weight by the interpolation. However, in figure 2c (Linear++ group), the participants failed to estimate the weight for the test object from the 1st test trial. This result was puzzling. Is there any reasonable explanation? Was there any possibility that this failure triggered to develop the different motor memory?</p><p>2. Line 280-282 &quot;These results suggest that….&quot;: If the processing objects in a family contribute to reducing the response time, I expect that the response time for the test object in Linear++ group should also exhibit a longer response time. Did the results support this prediction?</p><p>3. Line 390- &quot;All-or-nothing learning of outlier weight&quot;: The results of the web-based task clearly indicated that the estimated weight distribution for Linear++ or Linear-- group was bimodal. Was bimodality also observed in the laboratory task? The number of participants for the Linear++ and ++Linear groups was sufficient to examine if the distribution was bimodal.</p><p>4. The web-based task: This is an innovative way of collecting data from many subjects. However, I wondered if this web-based task can be regarded as a motor learning task. In this task, the participants could directly see the motor output by the spring length. This was totally different from the laboratory experiment in which the participants could only use their haptic sensation. Considering that the participants had only to associate the object size with the spring length, the web-based task was like a test for declarative memory. If the web-based task was not a motor task, the object family effect could not be a property of motor memory but a property of declarative memory.</p><p>5. I was curious about how the participants recognized the error when lifting the test object. The difference in error awareness between Linear++ and Linear+ groups contributed to either creating a new memory or consistently using the same memory. I would also like to know if the participants noticed gradual weight change in Linear_up and Linear_down groups (Figure 4).</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>In this paper, Cesanek et al., use a novel object lifting task to investigate the &quot;format&quot; of memories for object dynamics. Namely, they ask if those memories are organized according to a smooth, local map, or discrete categories ('families'). They pit these competing models against one another across several experiments, asking if subjects' predicted weights of objects follow the family model or a smooth map. This was tested by having people train on objects of varying volumes/masses that were either consistent with a linear mapping between volume/mass, or where those dimensions were uncorrelated. This training phase was either preceded by, or preceded, a testing phase where a novel object with a deviant mass (but a medium-size volume) was introduced. As the authors expected, individuals trained on the linear mappings treated novel objects that were relatively close to the &quot;family&quot; average mass as a member of the family, and thus obligatorily interpolated to compute the expected mass of that object (i.e., under-predicting its true mass); conversely, when a novel object's mass was a substantial outlier w/r/t the training items, it was treated as a singleton and thus lifted with close to the correct force. Additional variations of this experiment provided further evidence that people tend to treat an object's dynamical features as a category label, rather than simply forming local associative representations. These findings offer a novel perspective on how people learn and remember the dynamics of objects in the world.</p><p>Overall, I found this study to be both rigorous and creative. The experimental logic is refreshingly clear, and the results, which are replicated and extended several times in follow-up experiments, are rather convincing. I do think some additional analyses could be done, and data presentation could be improved. I also thought the generalization analysis, as I interpreted it, was difficult to align with the initial predictions.</p><p>I think sequential effects in the test phase could be analyzed more closely. This is especially pertinent as the generalization metric appears to be based primarily on sequential effects (if I'm interpreting it correctly). Moreover, wouldn't the family model predict a block-level generalization effect? That is, if the family representation is a discrete memory, and is altered by incorporating the new, heavier object, wouldn't all items be influenced at a block-level, not just items nearby in volume/time? I think overall the connection between the family model and the particular implementation of the generalization analysis didn't fully click for me.</p><p>I had trouble figuring out why there are different predictions for the family/association models in Figure 1F?</p><p>Shouldn't response times for the Linear+/Linear++ conditions be different when subjects are faced with the novel object? (i.e., a memory &quot;switch&quot; cost?). This is hard to see here as they have been collapsed. Moreover, it could be useful to also analyze response times from stimulus appearance to initial force application (planning?).</p><p>The consistent undershoot in the novel object's mass for the Linear++/++Linear groups is treated, retrospectively, as the result of individual differences rather than a &quot;true&quot; undershoot (i.e., via the online experiment). Could this be confirmed in the original in-lab samples? I understand they are smaller Ns; at least it could be nice to see the individual-level data if the model fit is underpowered. (Indeed, individuals could be added to several of the summary statistic figures).</p><p>It seems plausible that the computation of an anticipatory force could come from parallel learning processes (e.g., very slow local map-learning + heuristic category judgment) within a single observer. Couldn't this also lead to the observed undershoot in the Linear ++ condition? While not critical, I think this point could be discussed somewhere.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.71627.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>We all agree that the experiments are well designed and well executed, and the results confidently point to the presence of a categorical effect. However, we also agree that the points discussed below represent essential revisions.</p><p>1. We are unsure how clearly these data can be anchored to motor learning and memory. R2 points out that the categorical effect could arise from declarative memory and R3 suggests that the categorical effect could arise from a combination of map-learning + heuristic category judgement (which would likely be declarative or otherwise non-motor).</p><p>This raises a fundamental question: Does this paper demonstrate a categorical structure to motor memory, or is it telling us something about non-motor cognition?</p><p>Please clearly address this possibility in your revision. In so doing, please explicitly state your views on how motor or non-motor both the lab-based and the web-based task are, and please also state to what degree you feel the web-based task is measuring the same phenomena as the lab-based task.</p><p>One observation that may be helpful in considering how to structure your revision is that since the family bias appears to effectively block learning of the true weight, it seems that even if the categorical effect is driven by non-motor systems, it can nevertheless have an important impact on motor systems.</p></disp-quote><p>The reviewers ask us to discuss whether the categorical memory organization we observe for object weights should be viewed as motor memory or non-motor (or ‘cognitive’) memory. We feel that, ultimately, memory can be designated ‘motor memory’ if it is used to control a motor task. This includes explicit or declarative memories that are engaged in motor tasks. Consider, for example, the explicit re-aiming that is often observed when adapting to a visuomotor rotation. It seems clear that re-aiming draws on similar explicit processing as mental rotation (<italic>i.e.</italic>, mentally rotating the visual target around the starting position of the hand). However, it is not obvious to us that this processing should necessarily be characterized as non-motor. Rather, it is a capacity that can support both motor and non-motor tasks. Similarly, while categorical encoding of object weights (and mechanical properties more generally) is clearly important for motor control, it is conceivable that it may also be important in non-motor tasks (<italic>e.g.</italic>, tasks involving perception or visual cognition without action). Again, even if such encoding was used in a non-motor task, it is not clear to us why we would label such encoding—in general—as being strictly non-motor. In the revised Discussion, we now clarify our view on why the categorical encoding we observe is related to the organization of motor memory:</p><p>L677: “As noted above, object manipulation tasks engage multiple sensorimotor and cognitive processes, including categorization, and can depend on implicit and explicit memories. From our perspective, any form of memory engaged in a motor control task can be considered as a ‘motor memory’, whether that is an explicit declarative memory or an implicit procedural memory.”</p><p>The reviewers also ask whether the categorical encoding we observe involves explicit (or declarative) memory or implicit (or procedural) memory. We know from research in other domains that categorical encoding may be explicit or implicit. For example, researchers who work on categorization distinguish between rule-based categories, thought to be learned explicitly, and information-integration categories, thought to be learned implicitly.</p><p>In our experiment, we did not attempt to measure whether the generation of lift forces involves explicit processes, implicit processes, or both. Although we can speculate about the processes that might be involved, we feel that this is a challenging issue. We know that if we ask people to verbally estimate the weights of objects before lifting them, they can generally give reasonable answers. However, we also know that these explicit judgments are not estimates of <italic>absolute</italic> object weight, but are biased by expected weight (as clearly demonstrated by the size-weight illusion). In other words, explicit weight estimates are partially comparative. We suspect that if we had asked participants who did not learn the outlier to explicitly estimate its weight prior to lifting, they would have given responses consistent with the family-predicted weight. Conversely, we suspect that participants who did learn the outlier would have given estimates close to the actual weight. That is, we think that participants would likely exhibit explicit knowledge of the outlier in cases where correct lift forces are learned for the outlier. (Note that we also suspect that asking participants to provide verbal estimates of weight might shift the family boundary; <italic>e.g.</italic>, participants might be more likely to generate new categories if asked to reflect on the weight.)</p><p>Even though people can give explicit (albeit biased) estimates of object weight, it is unclear how or whether this explicit knowledge is used to generate forces when lifting. The use of explicit knowledge to perform a task is often considered to require working memory resources, whereas the use of implicit knowledge is more automated and does not require working memory resources. Thus, one way to probe the involvement of explicit memory in object lifting would be to consider the working memory resources required to perform the task. We have previously provided evidence that lifting unusually weighted objects taxes working memory resources (Baugh et al., 2016, J. Neurophysiology). Specifically, having to perform a secondary task that draws on working memory disrupts predictive scaling of lift forces when lifting unusually weighted objects, but not when lifting normally weighted objects.</p><p>Based on these findings, we speculate that when participants lift objects that are all encoded as members of the same family, this categorical encoding occurs with very little engagement of explicit processing. In contrast, we suspect that explicit processing contributes to encoding an object that is similar in appearance to the family as an outlier. In particular, explicit processing may be required to inhibit automatic category encoding and recognize that the object does not belong to the family. It is possible that this explicit processing could be reduced under some conditions, such as with extended practice, by introducing additional objects that form a family with the outlier, and/or with greater visual discriminability between the family objects and the outlier.</p><p>In order to clarify these issues in our manuscript, we have considerably modified the paragraph in the Discussion that deals with explicit and implicit contributions to object lifting (red text is new):</p><p>L654: “Although the formation of motor memories has historically been viewed as a largely implicit process, recent research on motor learning and adaptation has emphasized the role of explicit processes. […] In general, as people become more experienced at classifying objects into separate families, the contribution of explicit knowledge will likely diminish and lifting will become more implicit and automated.”</p><p>Finally, the reviewers ask whether the web-based task tests the same phenomena as the lab-based task, and Reviewer 2 questions whether the former is really a motor learning task. For us, there is no question that both tasks are motor learning tasks. Both tasks examine how individuals translate sensory information into continuous motor commands to achieve an action goal. Although the precise nature of the sensory information used for control differs between the tasks, both tasks engage similar control mechanisms.</p><p>In our web-based task, participants’ weight predictions were mapped onto the visual length of a spring (rather than a haptic sensation). However, there are many motor tasks in which the initial conditions of an interaction are adjusted based on visual feedback, like when aiming in archery, or lining up a putt in golf. In these examples, the motor error is also provided strictly through visual feedback as in our web-based task.</p><p>In the revised paper, we have added a sentence when introducing the web-based tasks that emphasizes this similarity:</p><p>L442: “Note that although the exact nature of the sensory information in the web-based task differs from the laboratory task, both are fundamentally motor control tasks because they test how individuals translate sensory information into continuous motor commands to achieve an action goal.”</p><p>We have also added the following paragraph to the Discussion:</p><p>L606: “We found remarkably similar results in the laboratory and web-based tasks. […] The fact that we observe similar categorical encoding in both the laboratory task and the web-based task demonstrates the generalizability of our findings.”</p><disp-quote content-type="editor-comment"><p>2. If we accept that the categorical effect is coming from motor systems, then the degree to which it is novel and / or counter to the predictions of leading models should be made clearer. For example, my own intuition is that the categorical effect could arise from switching between &quot;multiple paired forward and inverse models for motor control&quot; (Wolpert and Kawato, 1998). I believe that this would be best addressed by adding formal modelling to your paper, but I am open to the idea that simply rewriting may be sufficient.</p></disp-quote><p>We have carefully considered how our results fit with modular models of motor control including Wolpert and Kawato’s (1998) MOSAIC model and we have concluded that such models cannot explain our data. In what follows, we explain why the MOSAIC model cannot account for our results and then provide a simulation, based on the model, demonstrating this finding. The MOSAIC model assumes that the motor system has at its disposal an effectively unlimited supply of learning modules (<italic>i.e.</italic>, paired forward and inverse models) that will be used as needed. To be concrete, with respect to the Linear+ experiment, the MOSAIC model should recruit four learning modules, each primarily responsible for one of the four unique weights (recall that in Linear+ the outlier object weighs the same as the largest training object, so there are five objects but four weights). This is because the modules become differentiated only by having different forward model predictions and, in our task, these predictions depend only on weight. As a result, each of the four modules would learn to predict a high responsibility for one of the four weights, and each module’s inverse model would simply generate the motor command equal to the weight that is well predicted by its paired forward model. Note that when the outlier is introduced later in the experiment, it will already be well predicted by one of these modules, as it shares the same weight as another object.</p><p>However, producing the correct motor command for the test object first requires that the responsibility predictor for this module learns to predict a high responsibility based on the observed sensory cue. In our task, the only relevant sensory cue was the height of the object. At the start of each trial, the responsibility predictors determine the weighting of the inverse model outputs of all the modules shaping the final motor command. Over time, each responsibility predictor learns to predict, from sensory cues alone, the responsibility based on the forward model errors, so high responsibility is predicted for modules with low forward model error. Initially, the responsibility predictor for the module that is already tuned to the weight of the outlier will not predict a high responsibility because it was trained on the largest height, whereas the outlier is medium height. Note that in order to learn to predict the same high responsibility for the medium and largest heights, this responsibility predictor must implement a non-linear mapping, which should be possible as neural networks are universal function approximators. The Uncorr+ condition suggests that in the MOSAIC framework, at least four modules can be recruited, learned and accessed by height (<italic>i.e.</italic>, the responsibility predictor can implement a highly nonlinear mapping).</p><p>Thus, a MOSAIC model, if trained properly to reach the optimal solution with minimum average error across the task, would learn the weights of all objects, failing to account for our main finding. Indeed, we show in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref> that this is exactly what happens when a MOSAIC model is given our task.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>A MOSAIC model learns the weight of the Linear+ outlier in 200 trials.</title><p>Our model consists of five learning modules, each containing a responsibility predictor (a 1-10-1 feedforward neural network), an inverse model (with one scalar parameter as the motor output, <italic>i.e.</italic> the anticipatory force), and a forward model (with one scalar parameter as the predicted weight, subtracted from the final weighted motor output to predict the observed outcome, <italic>i.e.</italic> the force error). We first trained the model on the four training objects, which caused four of the five learning modules to learn each of the four different weights. The fifth module was extraneous and tended to learn to imitate one of the other modules. After the MOSAIC model learned to produce the correct forces for the four training objects, we introduced the outlier, randomly interspersed among the training objects for 200 trials (the same number of trials as the test phase in our first experiment). Here, in the same format as Figure 2b, we see that the model has learned the outlier, unlike our participants. The force output for the outlier is much greater than the force expected by interpolating within the family. Moreover, examining the internal states of the individual model components shows that they are fully consistent with the explanation above: the module responsible for lifting the heaviest object rapidly comes to dominate the motor response to the outlier as well.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-71627-sa2-fig1-v3.tif"/></fig><p>Although we could include this rather lengthy exposition of the MOSAIC model in our paper, we feel it would be distracting. Importantly, the main conclusion of this exercise is quite straightforward: the MOSAIC model effectively implements an associative map. Indeed, to our knowledge all current models of motor control predict associative map learning and do not account for categorical encoding. We recognize that this was not clear in our manuscript, especially for modular models like MOSAIC. In our revision, we have added a sentence to the Introduction clearly indicating that modular models implement associative map learning:</p><p>L99: “Note that associative map learning is also implemented by modular models, which have been proposed to account for learning in both motor <sup>30</sup> and non-motor <sup>31</sup> tasks.”</p><p>Additionally, we revisit this issue in the Discussion, where we have added the following text to point out that the recent developed COIN model also cannot, at present, explain our findings:</p><p>L694: “These various treatments can all be viewed as instances of non-parametric Bayesian models that leverage the hierarchical Dirichlet process, a statistically principled approach to clustering data into a theoretically infinite number of components <sup>125,126</sup>. A recent motor control model has been developed based on this approach <sup>127</sup>. However, at present this model cannot account for our results because there is no mechanism through which the visual properties of the objects—which are encoded as discrete cues—can be linked together to form a family.”</p><disp-quote content-type="editor-comment"><p>3. The basic model predictions are reasonably clear as is, but I agree with R3 that it isn't as clear as it could be whether or not block-level generalisation effects should exist in your data. Again, adding formal modelling would strongly address this ambiguity, but may not be necessary. If block level generalisation effects are predicted, then please report them in your revision.</p></disp-quote><p>We agree that the generalization analysis in our original paper was difficult to follow and unnecessarily complicated. In our original analysis, we compared the anticipatory force for the family objects in trials that came <italic>immediately after</italic> the outlier with a prediction of that force (using a linear model) based on other lifts of the family objects in the test phase. Moreover, we restricted our analysis to the two objects that were most visually similar to the outlier.</p><p>The reviewers suggested a far simpler “block-level” generalization analysis whereby we compare the anticipatory force produced for <italic>all lifts</italic> of the family objects during the test phase with the anticipatory force produced for the family objects at the end of the training phase. However, there is a problem with this approach. Any generalization from the outlier object to the family objects would be expected to be observed on the <italic>first lift</italic> of a family object following the lift of the outlier. However, once that family object has been lifted, any generalization from the outlier on additional lifts of family objects would be expected to be diminished. Indeed, in our response to Reviewer 3 below, we show this ‘wash-out’ of generalization in our data. Therefore, in our new generalization analysis, we focus on lifts of family objects that immediately followed the outlier, and compare the anticipatory force on these test phase lifts with corresponding lifts in the late training phase. We like this new approach because it is both sensible and straightforward. Note that the results of this new analysis are very clear: strong generalization is observed throughout the test phase in the Linear+ condition, whereas it is strongly diminished in the Linear++ condition, and absent in the Uncorr+ condition.</p><p>In the new analysis, we compare the anticipatory force on trials immediately following the test object with the average force from the final four bins of the training phase. Note that the trial order was controlled so that, every four trial cycles, each family object followed the outlier exactly once. As a result, the presentation frequency of the family objects is balanced, within each participant, for this analysis. Additionally, we have normalized our metric of generalization against the magnitude of the outlier (<italic>i.e.</italic>, we divide the observed increase in force by 2.94 N in the Linear+ and Uncorr+ groups, 5.89 N in the Linear++ group), in order to facilitate comparing the groups. We have modified Figure 2g-i and the accompanying text in the Results section to present this improved analysis:</p><p>L249: “The object families hypothesis predicts that when lifting an object that is encoded as a family member, the experienced density will update the density estimate for the family, thereby biasing the anticipatory force on a subsequent lift of a training (<italic>i.e.</italic>, family) object. […] For the Uncorr+ group (Figure 2i), we found no evidence of generalization at the start (<italic>t</italic>(11) = 1.90, p = 0.085) or the end of the test phase (<italic>t</italic>(11) = 0.45, p = 0.66), and no change over time (<italic>t</italic>(11) = 1.90, p = 0.084), consistent with encoding each object individually (Figure 2i).”</p><p>We have also replaced the previous description of the generalization analysis in the Methods with the following paragraph:</p><p>L909: “In the first experiment, we analyzed how lifting the test object generalized to the training objects in the subsequent trial (Figure 2g-i). […] For each group, we conducted two-tailed <italic>t</italic>-tests on this generalization metric in each portion of the test phase, and also on the change in this generalization metric from the early portion to the late portion of the test phase.”</p><disp-quote content-type="editor-comment"><p>4. R2 and R3 both raise the issue that there are useful reaction time predictions that should be investigated. In essence, we should see RT switch costs wherever a categorical effect is established and not elsewhere. In the revision, please test these predictions. If they are invalid, or the analysis isn't a good idea for some other reason, please explain why.</p></disp-quote><p>In our paper we reported that response times were greater in the Uncorr+ condition than in the Linear+ and Linear++ conditions combined. However, the source of this effect is not clear. The increased response time in the Uncorr+ condition could be a ‘switch’ cost related to loading a different category into motor memory. Alternatively, the increased response time could be a ‘selection’ cost associated with mapping the visual stimulus onto the appropriate category. Note that a selection cost would be expected to be considerably larger when lifting objects from five different categories (in the Uncorr+ condition) than when lifting objects from just two categories (in the Linear++ condition). However, a switch cost would not depend on the number of categories.</p><p>We can test between these explanations by examining the response time when lifting the outlier in the Linear++ condition. Note that, not surprisingly, we found that participants took longer to generate larger anticipatory forces in our task. To account for this uninteresting component of the response time, we fit a linear regression to response time as a function of object weight using data from the Linear+ and Linear++ conditions, and excluding trials with the test object and each immediately subsequent trial. This allowed us to determine the predicted response time, based solely on weight, which we then compared to the actual response time to test for additional temporal costs (e.g., associated with switching categories). We found a significant slope of 26.2 ms per Newton of anticipatory force. Therefore, when lifting the 1.5-kg test object in the Linear++ condition, we should expect the response time to be 154 ms longer than the average response time for the family objects (as the test object is 5.89 N heavier than the average family object weight). The actual response time for the 1.5-kg test object was 157 ms longer than the average response time for the family objects. In other words, the response time was 3 ms slower than the predicted response time based on weight. On trials that immediately followed the test object, which involved switching back to the memory of the family, the response time (averaging across objects) was 25 ms faster than the predicted response time. Neither of these differences was even close to being significant. As there was a category switch in motor memory in both of these cases, our data provide no evidence for the idea that the large temporal cost observed in the Uncorr+ condition is a switch cost. However, these results are consistent with a selection cost, which could be very large when dealing with five categories, but may be negligible in the trivial case involving only two categories.</p><p>We have included these additional response time analyses for the Linear++ group, along with our interpretation, in a new paragraph in the Results:</p><p>L286: “The increased response time in the Uncorr+ condition could be a ‘switch cost’ related to loading a different category in motor memory. […] In contrast, in the Linear++ condition, only two categories were involved, and therefore the selection cost may be negligible.”</p><p>We have also added the following text to the Methods, describing this response time analysis in detail:</p><p>L926: “In the Linear++ group, where we observed learning of separate categories, we analyzed whether response times were longer on trials involving a category switch (<italic>i.e.</italic>, trials with the test object or trials immediately after the test object). […] When analyzing trials immediately after the test object, we computed the predicted response time as the average response time for the family objects in ‘non-switch’ trials during the test phase, as the object weight in these trials was, on average, equivalent to the average family object weight.”</p><disp-quote content-type="editor-comment"><p>5. R2 and R3 both raise issues related to whether the lab-based task was bimodal. Please report this analysis in your revision, or else clearly outline why it isn't appropriate to do so (e.g., insufficient sample size etc.).</p></disp-quote><p>Thanks to a suggestion by Reviewer 2, we combined groups from the laboratory experiments to achieve sufficient sample sizes for this analysis. The results confirm that bimodal distributions of learners and non-learners were also observed in the laboratory experiments. We have included the following text after reporting the analysis for the web-based experiments:</p><p>L502: “Notably, similar bimodality was also observed in the laboratory experiments. Revisiting these data, we applied the same mixture model analysis to individual participants’ final outlier learning, (<italic>i.e.</italic>, the difference between the anticipatory force produced for the outlier and the family-predicted weight of the outlier). […] Thus, these results strengthen our main conclusion that memories of the motor properties of objects are organized categorically.”</p><p>We also added the following text to the Methods, after describing the analysis for the web-based task:</p><p>L1002: “The same analysis of bimodality was conducted for the laboratory experiments by combining the Linear+ and +Linear groups in one analysis, and the Linear++ and ++Linear groups in another analysis. For the laboratory experiments, we fit the single-Gaussian and two-Gaussian models to the distributions of the difference between anticipatory force for the outlier and the family-predicted weight of the outlier in the final 16 trial cycles of the test phase.”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>I have only two major recommendations for the authors.</p><p>1. The paper is currently framed by asking &quot;how memories of multiple objects might be organized&quot;, but I believe the paper would be clearer by using the following two passages from the discussion as anchors:</p><p>&quot;Current theories of motor learning often focus on graded generalization of learning across various stimulus and motor parameters as a revealing feature of the underlying computations 38,97-99. In particular, graded patterns of generalization have been taken as evidence that motor learning fundamentally involves associating contextual features of a movement with the target motor parameters in a continuous multi-dimensional space, often termed an associative map. The theoretical significance of our study is that it provides multiple, converging pieces of evidence for a fundamentally different type of organization-motor memories of objects are organized categorically, into families&quot;</p><p>&quot;These various treatments can all be viewed as instances of non-parametric Bayesian models that leverage the hierarchical Dirichlet process, a statistically principled approach to clustering data into a theoretically infinite number of components 123,124. Importantly, this approach has recently been applied to successfully account for an unprecedented range of phenomena in motor learning 125, suggesting that similar computations could also underlie the (in)ability to learn the weight of an outlier object in our lifting task.&quot;</p><p>I think this framing would give you the opportunity explicitly walk the reader through the modelling of Wolpert and Kawato (1998) -- and any more recent motor learning theories that carry on in the internal model tradition and / or the context inference tradition -- such that we understand right from the beginning how the existence of categorical encoding would demand revision of our current thinking. If it turns out that the present data are completely consistent with current models, then you could use this framing to explain why the present data is nonetheless important. This would presumably entail a cogent explanation as to why the present data is itself novel. That is, I would like a clear description of the context effects studied in force fields and in any other relevant motor task such that the categorical effect you report can be seen to stand in sharp contrast.</p></disp-quote><p>We appreciate the idea of laying out relevant theoretical models at the beginning of a paper if they make opposing predictions, one of which is supported by the data. However, this is not how our findings relate to the two models discussed above. As explained in response to Essential Revision #2, the MOSAIC model is effectively an associative map model, and the existing Introduction addresses associative models and why they predict at least partial learning in all of our conditions. On the other hand, the relation to clustering models, such as the recently developed COIN model (Ref. 125 in the passage quoted above), is less straightforward. The COIN model is capable of generating new memories as needed; however, there is nothing in the model that links memories to categories and therefore the model, as it stands, cannot account for our data. Specifically, because sensory cues in the model are discrete labels, there is no notion of similarity between sensory cues (<italic>e.g.</italic>, objects of continuously varying height), and therefore no basis for forming families. It is possible that future extensions of the COIN model could explain the data, but that is not something we would like to speculate on in the current paper. Given these considerations, we feel strongly that the current approach to the Introduction, which gives a clear and simple intuition for the object families hypothesis and contrasts it with the associative map learning predicted by current models of motor control, is better than attempting to link the object families hypothesis to specific modeling concepts that, in any case, have not yet been sufficiently developed to account for our findings.</p><disp-quote content-type="editor-comment"><p>2. I believe the strength of the paper would be greatly improved if you performed modelling showing how and why the ideas of Wolpert and Kawato (1998) and related models are inadequate to capture the present data.</p><p>Wolpert, D. M., and Kawato, M. (1998). Multiple paired forward and inverse models for motor control. Neural Networks, 11(7-8), 1317-1329.</p></disp-quote><p>As explained in response to Essential Revision #2, despite being modular, the MOSAIC model is essentially an associative map model that does not implement categorical encoding. Although it is possible that the model could be extended to accommodate categorical encoding, it is not clear to us how this could be done in a principled way. We feel it is most appropriate to leave this to future modeling work that carefully arbitrates between various implementations of categorical encoding. We do, however, explicitly point out in our revised manuscript that current models of motor control, including MOSAIC, predict associative map learning, and cannot account for categorical effects.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I would like to raise several issues.</p><p>1. In figure 2a (Linear+ group), the data of the 1st test trial (31st cycle) for the test object located in the just middle of the four training objects (i.e., ~9N). This result was reasonable from the idea of family effect: The motor system might estimate the weight by the interpolation. However, in figure 2c (Linear++ group), the participants failed to estimate the weight for the test object from the 1st test trial. This result was puzzling. Is there any reasonable explanation? Was there any possibility that this failure triggered to develop the different motor memory?</p></disp-quote><p>The Linear+ and Linear++ conditions were identical up to the first test trial, so there is no explanation for the apparent difference on the first test trial other than between-group differences due to random sampling. Indeed, the first test trial in Linear+ does not significantly differ from that in Linear++, nor does the first test trial in Linear++ significantly differ from the family-predicted weight. Thus, although the first test trial in the Linear++ condition does not appear to perfectly interpolate between the family members, our data are in fact consistent with interpolation.</p><disp-quote content-type="editor-comment"><p>2. Line 280-282 &quot;These results suggest that….&quot;: If the processing objects in a family contribute to reducing the response time, I expect that the response time for the test object in Linear++ group should also exhibit a longer response time. Did the results support this prediction?</p></disp-quote><p>This is a reasonable prediction that we have now tested. Our analysis of the response times on ‘switch trials’ in the Linear++ group provided no evidence for increased response times (after accounting for the greater weight of the test object). As explained in detail in our response to Essential Revision #4, this analysis shows that the temporal cost observed in the Uncorr+ condition is not simply due to the process of loading a different category in motor memory. Instead, we argue that it is related to the process of mapping the visual stimulus onto the appropriate category (termed a ‘selection cost’). In the Uncorr+ group, participants were required to map each visual stimulus onto one of five categories in memory, and thus we would expect a substantial selection cost. In contrast, in the Linear++ condition, only two categories were involved, and therefore the selection cost may be negligible.</p><disp-quote content-type="editor-comment"><p>3. Line 390- &quot;All-or-nothing learning of outlier weight&quot;: The results of the web-based task clearly indicated that the estimated weight distribution for Linear++ or Linear-- group was bimodal. Was bimodality also observed in the laboratory task? The number of participants for the Linear++ and ++Linear groups was sufficient to examine if the distribution was bimodal.</p></disp-quote><p>Thank you for calling this to our attention. We had overlooked the possibility of combining groups from the laboratory experiments to enable this analysis. As noted in our response to Essential Revision #5, we now include the results of this analysis, confirming that bimodal distributions of learners and non-learners were also observed in the laboratory experiments.</p><disp-quote content-type="editor-comment"><p>4. The web-based task: This is an innovative way of collecting data from many subjects. However, I wondered if this web-based task can be regarded as a motor learning task. In this task, the participants could directly see the motor output by the spring length. This was totally different from the laboratory experiment in which the participants could only use their haptic sensation. Considering that the participants had only to associate the object size with the spring length, the web-based task was like a test for declarative memory. If the web-based task was not a motor task, the object family effect could not be a property of motor memory but a property of declarative memory.</p></disp-quote><p>As explained in our response to Essential Revision #1, both tasks examine how individuals translate sensory information into continuous motor commands to achieve an action goal. Although the precise nature of the sensory information used for control differs between the tasks, both tasks engage very similar control mechanisms. We therefore feel strongly that the web-based task can be regarded as a motor task. We have added text to the Results and Discussion that explains the fundamental similarity between the tasks, and also points out that the use of visual feedback to set the initial conditions of a motor interaction is not uncommon in natural motor tasks such as archery, golf, and pinball.</p><p>Additionally, as we also explained in our response to Essential Revision #1, in our view, any memory, declarative or otherwise, used in the context of a motor task, can be viewed as a ‘motor memory’. In our revised manuscript, we now discuss the possible contributions of explicit and implicit processing in the encoding and use of categorical knowledge of object motor properties.</p><disp-quote content-type="editor-comment"><p>5. I was curious about how the participants recognized the error when lifting the test object. The difference in error awareness between Linear++ and Linear+ groups contributed to either creating a new memory or consistently using the same memory. I would also like to know if the participants noticed gradual weight change in Linear_up and Linear_down groups (Figure 4).</p></disp-quote><p>As explained in our response to Essential Revision #1, we did not ask participants to provide their explicit beliefs about each object’s weight during the course of the experiment, as this could have interfered with the processes we aimed to examine. As a result, there is little we can say regarding explicit awareness of errors and weights. We suspect that participants are aware of their error when lifting the test object even in the Linear+ condition, given that a 3-Newton error is very large, but do not attribute the error to the object. In general, how errors are credited likely involves a trade-off between acceptable errors and the cost of maintaining multiple categories in memory. In the revised Discussion, we comment briefly on the roles of implicit and explicit processes in weight prediction and memory.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>In this paper, Cesanek et al., use a novel object lifting task to investigate the &quot;format&quot; of memories for object dynamics. Namely, they ask if those memories are organized according to a smooth, local map, or discrete categories ('families'). They pit these competing models against one another across several experiments, asking if subjects' predicted weights of objects follow the family model or a smooth map. This was tested by having people train on objects of varying volumes/masses that were either consistent with a linear mapping between volume/mass, or where those dimensions were uncorrelated. This training phase was either preceded by, or preceded, a testing phase where a novel object with a deviant mass (but a medium-size volume) was introduced. As the authors expected, individuals trained on the linear mappings treated novel objects that were relatively close to the &quot;family&quot; average mass as a member of the family, and thus obligatorily interpolated to compute the expected mass of that object (i.e., under-predicting its true mass); conversely, when a novel object's mass was a substantial outlier w/r/t the training items, it was treated as a singleton and thus lifted with close to the correct force. Additional variations of this experiment provided further evidence that people tend to treat an object's dynamical features as a category label, rather than simply forming local associative representations. These findings offer a novel perspective on how people learn and remember the dynamics of objects in the world.</p><p>Overall, I found this study to be both rigorous and creative. The experimental logic is refreshingly clear, and the results, which are replicated and extended several times in follow-up experiments, are rather convincing. I do think some additional analyses could be done, and data presentation could be improved. I also thought the generalization analysis, as I interpreted it, was difficult to align with the initial predictions.</p><p>I think sequential effects in the test phase could be analyzed more closely. This is especially pertinent as the generalization metric appears to be based primarily on sequential effects (if I'm interpreting it correctly). Moreover, wouldn't the family model predict a block-level generalization effect? That is, if the family representation is a discrete memory, and is altered by incorporating the new, heavier object, wouldn't all items be influenced at a block-level, not just items nearby in volume/time? I think overall the connection between the family model and the particular implementation of the generalization analysis didn't fully click for me.</p></disp-quote><p>Thank you for this comment. We thoroughly examined the sequential effects in the test phase and arrived at a considerably improved generalization analysis, as explained in our response to Essential Revision #3. Please see that response for full details on the updated analysis, and the reasoning behind them.</p><p>This comment correctly points out that any generalization should affect all members of the family, not just the ones nearby in volume to the outlier. In the new generalization analysis, we now include all four family objects. However, our new analysis focuses on the lifts of the family objects that immediately follow the test object, and not the other lifts of the family objects (<italic>i.e.</italic>, lifts that occurred 2-4 lifts after the lift of the test object). We adopted this approach because generalization effects from the test object would be expected to diminish, thereby weakening our analysis. To verify this assumption, we analyzed the shape of the wash-out function in our experiment and found that although there are lingering effects of generalization in the second, third, and fourth trials, they are negligible compared to the first trial, and therefore it makes little sense to include them in the analysis (see ).</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>Generalization in each of the four trials after lifting the test object.</title><p>The depicted mean +/- SEM for Trial #1 is the same as shown in Figure 2g-i. This figure demonstrates how the generalization effect rapidly washes out in the second, third, and fourth lifts after the test object, and thus why it is not desirable to include these trials in the generalization analysis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-71627-sa2-fig2-v3.tif"/></fig><disp-quote content-type="editor-comment"><p>I had trouble figuring out why there are different predictions for the family/association models in Figure 1F?</p></disp-quote><p>The pictorial differences in Figure 1F are intended only to reflect the qualitative distinction in how the two models encode the five objects, but the predictions for the two models are the same. We now briefly note this in the figure caption, and we have added some clarifying information to the relevant paragraph in the manuscript:</p><p>L88: “Under the associative map hypothesis, there is no fundamental difference between this scenario and those depicted in (d, e), so the prediction for learning is similar to the object families hypothesis (<italic>i.e.</italic>, all five objects will eventually be learned).”</p><p>L111: “Under the object families hypothesis, each of these objects is learned as an individual (Figure 1f; separate green lines) and, as a consequence, the training objects will be learned more slowly than when they are learned as a family and there will be minimal single-trial generalization from the ‘outlier’ to the training objects.”</p><disp-quote content-type="editor-comment"><p>Shouldn't response times for the Linear+/Linear++ conditions be different when subjects are faced with the novel object? (i.e., a memory &quot;switch&quot; cost?). This is hard to see here as they have been collapsed. Moreover, it could be useful to also analyze response times from stimulus appearance to initial force application (planning?).</p></disp-quote><p>This is a reasonable prediction, however our analysis of the response times on ‘switch trials’ in the Linear++ group provided no evidence for increased response times (after accounting for the greater weight of the test object). As explained in our response to Essential Revision #4, this analysis shows that the temporal cost observed in the Uncorr+ condition is not simply due to the process of loading a different category in motor memory. Instead, we argue that it is related to the process of selecting the appropriate memory from multiple candidates. This account explains the large effect in the Uncorr+ condition as well as the absence of a detectable effect in the Linear++ condition based on the notion that the selection process is trivial and rapid when there are only two categories (Linear++) but becomes time-consuming when there are five categories (Uncorr+).</p><p>Please note that participants’ force application prior to the button press was not constrained in any way and, as a result, it is impractical to attempt to measure reaction times as suggested.</p><disp-quote content-type="editor-comment"><p>The consistent undershoot in the novel object's mass for the Linear++/++Linear groups is treated, retrospectively, as the result of individual differences rather than a &quot;true&quot; undershoot (i.e., via the online experiment). Could this be confirmed in the original in-lab samples? I understand they are smaller Ns; at least it could be nice to see the individual-level data if the model fit is underpowered. (Indeed, individuals could be added to several of the summary statistic figures).</p></disp-quote><p>Based on a suggestion from Reviewer 2, we combined the Linear++ group with the ++Linear group and the Linear+ group with the +Linear group to achieve sufficient sample sizes to fit the mixture models. The results confirm that bimodal distributions of learners and non-learners were also observed in the laboratory experiments. Please see our response to Essential Revision #5 for the relevant text that has been included in the revision.</p><disp-quote content-type="editor-comment"><p>It seems plausible that the computation of an anticipatory force could come from parallel learning processes (e.g., very slow local map-learning + heuristic category judgment) within a single observer. Couldn't this also lead to the observed undershoot in the Linear ++ condition? While not critical, I think this point could be discussed somewhere.</p></disp-quote><p>In the revised Discussion, we now state that explicit processing is likely engaged when learning to re-classify the outlier object as an individual. In the same section, we point out that it is unclear whether explicit knowledge of object weight, which is known to be at least partially relative, is sufficient to produce lifting forces, which are absolute. Thus, while we do not discuss the specific possibility mentioned here (slow local map-learning + heuristic category judgment), we feel that this new Discussion paragraph addresses this comment by acknowledging potentially separate contributions of explicit and implicit processes in this task (<italic>i.e.</italic>, that explicit memory is likely engaged in object re-classification while implicit memory may be needed to map from category labels to absolute forces).</p></body></sub-article></article>