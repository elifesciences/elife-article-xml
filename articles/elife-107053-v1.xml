<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">107053</article-id><article-id pub-id-type="doi">10.7554/eLife.107053</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107053.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Feedback of peripheral saccade targets to early foveal cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Kämmer</surname><given-names>Luca</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0009-8046-5724</contrib-id><email>kaemmer@cbs.mpg.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Kroell</surname><given-names>Lisa M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3508-5214</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Knapen</surname><given-names>Tomas</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Rolfs</surname><given-names>Martin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8214-8556</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="aff" rid="aff10">10</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Hebart</surname><given-names>Martin N</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7257-428X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff11">11</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Vision and Computational Cognition Group, Max Planck Institute of Human Cognitive and Brain Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Department of Psychology, Humboldt University Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Department of Medicine, Justus Liebig University Giessen</institution></institution-wrap><addr-line><named-content content-type="city">Giessen</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05kgbsy64</institution-id><institution>Spinoza Center for Neuroimaging, KNAW Netherlands</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05csn2x06</institution-id><institution>Computational Cognitive Neuroscience and Neuroimaging, Netherlands Institute for Neuroscience</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008xxew50</institution-id><institution>Experimental and Applied Psychology, Vrije University Amsterdam</institution></institution-wrap><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Berlin School of Mind and Brain, Humboldt University Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v4gjf40</institution-id><institution>Exzellenzcluster Science of Intelligence, Technical University Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ewdps05</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff10"><label>10</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ewdps05</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff11"><label>11</label><institution>Center for Mind, Brain and Behavior, Universities of Marburg, Giessen, and Darmstadt, Germany</institution><addr-line><named-content content-type="city">Marburg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kok</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/001mm6w73</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>01</month><year>2026</year></pub-date><volume>14</volume><elocation-id>RP107053</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-04-22"><day>22</day><month>04</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2025-04-22"><day>22</day><month>04</month><year>2025</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.02.20.639262"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-08-20"><day>20</day><month>08</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107053.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-12-17"><day>17</day><month>12</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107053.2"/></event></pub-history><permissions><copyright-statement>© 2025, Kämmer et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Kämmer et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-107053-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-107053-figures-v1.pdf"/><abstract><p>Human vision is characterized by frequent eye movements and constant shifts in visual input, yet our perception of the world remains remarkably stable. Here, we directly demonstrate image-specific foveal feedback to primary visual cortex in the context of saccadic eye movements. To this end, we used a gaze-contingent fMRI paradigm, in which peripheral saccade targets disappeared before they could be fixated. Despite no direct foveal stimulation, we were able to decode peripheral saccade targets from foveal retinotopic areas, demonstrating that image-specific feedback during saccade preparation may underlie this effect. Decoding was sensitive to shape but not semantic category of natural images, indicating feedback of only low-to-mid-level information. Cross-decoding to a control condition with foveal stimulus presentation indicates a shared representational format between foveal feedback and direct stimulation. Moreover, eccentricity-dependent analyses showed a U-shaped decoding curve, confirming that these results are not explained by spillover of peripheral activity or large receptive fields. Finally, fluctuations in foveal decodability covaried with activity in the intraparietal sulcus, thus providing a candidate region for driving foveal feedback. These findings suggest that foveal cortex predicts the features of incoming stimuli through feedback from higher cortical areas, which offers a candidate mechanism underlying stable perception.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>fMRI</kwd><kwd>foveal feedback</kwd><kwd>decoding</kwd><kwd>eye movements</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05xwwfy96</institution-id><institution>German National Academic Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Kämmer</surname><given-names>Luca</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hhn8329</institution-id><institution>Max Planck Society</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.3030/101039712</award-id><principal-award-recipient><name><surname>Hebart</surname><given-names>Martin N</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0472cxd90</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.3030/865715</award-id><principal-award-recipient><name><surname>Rolfs</surname><given-names>Martin</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. Open access funding provided by Max Planck Society.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Low-level features of peripheral saccade targets are fed back to early foveal retinotopic cortex in a signal that resembles activation elicited by direct foveal presentation.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Human vision relies heavily on foveal processing. Even though this region of the retina covers only a fraction of the visual field, it occupies a disproportionately large portion of neurons in the early visual cortex (<xref ref-type="bibr" rid="bib19">Curcio et al., 1990</xref>; <xref ref-type="bibr" rid="bib18">Curcio and Allen, 1990</xref>; <xref ref-type="bibr" rid="bib38">Hendrickson, 2005</xref>; <xref ref-type="bibr" rid="bib67">Schira et al., 2009</xref>). To take advantage of the fovea’s high resolution, humans perform several rapid eye movements per second, with each saccade bringing an object of interest into the fovea (<xref ref-type="bibr" rid="bib58">O’Regan, 1992</xref>; <xref ref-type="bibr" rid="bib10">Carpenter, 2000</xref>). Despite these frequent disruptions and dramatic shifts in retinal input, our perception of the visual environment remains stable, coherent, and continuous (<xref ref-type="bibr" rid="bib32">Golomb and Mazer, 2021</xref>; <xref ref-type="bibr" rid="bib53">Melcher and Colby, 2008</xref>; <xref ref-type="bibr" rid="bib8">Burr et al., 1994</xref>). This perceptual stability, while often taken for granted, points toward sophisticated neural mechanisms that integrate visual information across separate gaze fixations (<xref ref-type="bibr" rid="bib75">Wurtz, 2008</xref>; <xref ref-type="bibr" rid="bib11">Cavanagh et al., 2010</xref>; <xref ref-type="bibr" rid="bib54">Merriam et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Denagamage et al., 2024</xref>).</p><p>In the domain of visual perception, predictions may critically contribute to maintaining perceptual stability across rapid shifts in gaze (<xref ref-type="bibr" rid="bib64">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib22">de Lange et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Clark, 2013</xref>). One compelling hypothesis suggests that perceptual continuity arises through predictive feedback of peripheral information from higher cortical areas to foveal retinotopic regions. This feedback informs foveal regions about the expected visual features of a stimulus prior to its direct fixation to prepare for the shift in visual input (<xref ref-type="bibr" rid="bib49">Kroell and Rolfs, 2022</xref>). Support for this foveal-prediction hypothesis comes from psychophysical studies, which find that features of peripheral saccade targets are enhanced in the presaccadic fovea (<xref ref-type="bibr" rid="bib49">Kroell and Rolfs, 2022</xref>; <xref ref-type="bibr" rid="bib50">Kroell and Rolfs, 2025</xref>).</p><p>Current evidence for the role of feedback in foveal prediction is indirect, relying mostly on the interpretation of behavioral reports. While multiple studies have shown feedback of peripheral information to the fovea during fixation (<xref ref-type="bibr" rid="bib74">Williams et al., 2008</xref>; <xref ref-type="bibr" rid="bib28">Fan et al., 2016</xref>; <xref ref-type="bibr" rid="bib16">Costantino et al., 2025</xref>; <xref ref-type="bibr" rid="bib69">Stewart et al., 2020</xref>), this effect was only shown when features of the target stimulus were task relevant. The relationship of these foveal feedback signals to foveal prediction during saccade preparation has remained unclear. Furthermore, the nature of such feedback signals and whether they resemble activity patterns elicited by direct foveal stimulation has remained poorly understood.</p><p>In this study, we directly tested saccade-related foveal feedback in the brain to address (1) whether feedback is specific to stimulus features and which features are fed back, (2) whether feedback activation resembles activation elicited by direct stimulus presentation, and (3) which brain regions mediate this effect. To systematically address these open questions, we developed a gaze-contingent functional magnetic resonance imaging (fMRI) paradigm that allowed us to disentangle neural activity attributable to direct visual input from activity exclusively related to foveal feedback. By removing peripheral saccade targets before participants could fixate them, we ensured that observed neural activation within foveal retinotopic regions must originate from feedback rather than direct foveal stimulation. Furthermore, by employing naturalistic stimuli whose visual shape and semantic content were independently manipulated, we could explicitly assess the specificity and content of the signals fed back to early foveal cortex.</p><p>Our findings robustly demonstrate the presence of feedback in early visual areas, including primary visual cortex, indicated by reliable decoding of peripheral saccade targets from foveal retinotopic areas, despite the absence of direct foveal stimulation. Critically, this decoding was selective for stimulus shape and not influenced by semantic category, indicating a predominantly low-to-mid-level visual representation. Eccentricity-dependent analyses showed a U-shaped decoding curve, demonstrating that these results cannot be explained by spillover of peripheral activity or large receptive fields. Cross-decoding analyses further confirmed the similarity between feedback and direct visual representations in the fovea, reinforcing the shared nature of neural codes. Finally, exploratory analyses identified the intraparietal sulcus (IPS), a brain area integral to visuomotor coordination and eye movement planning, as a likely candidate involved in driving foveal feedback. These findings reveal a plausible neural implementation for perceptual continuity and may also facilitate object recognition across saccades (<xref ref-type="bibr" rid="bib40">Herwig and Schneider, 2014</xref>; <xref ref-type="bibr" rid="bib6">Blom et al., 2020</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Given the fast time scale of saccade-related processes, it is challenging to investigate foveal feedback using fMRI, which is based mostly on a sluggish hemodynamic response. To address this challenge and dissociate neural processes elicited by direct visual input from those related to foveal feedback, we designed a gaze-contingent functional MRI study where the saccade target was removed before it could reach the central 2 degrees of visual angle (dva) of the fovea (1 dva radius), from which we decoded. To maximise statistical power, we implemented the paradigm using a block design (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). During a block, participants fixated a fixation point until a stimulus appeared in the periphery, cueing them to execute a saccade toward the stimulus. As soon as their gaze came within 6.5 dva of the saccade target, the stimulus disappeared, ensuring that no part of the stimulus ever appeared in the central 2 dva of the fovea. On subsequent trials of the block, the same stimulus appeared in the participant’s periphery to cue another saccade, a process which was repeated until the end of a block. The presentation time of each stimulus depended on the saccade latency (M=240.70 ms). While the fovea is commonly defined as the central 5 dva of the visual field (<xref ref-type="bibr" rid="bib18">Curcio and Allen, 1990</xref>; <xref ref-type="bibr" rid="bib39">Hendrickson, 2009</xref>; <xref ref-type="bibr" rid="bib38">Hendrickson, 2005</xref>), we focused here on the central part of the fovea, commonly referred to as the foveola (<xref ref-type="bibr" rid="bib63">Poletti et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Heckenlively and Arden, 2006</xref>), within 2 dva (1 dva radius), to avoid any overlap of the stimulus and the area from which we decoded. This narrow definition also allowed for enough space to execute a saccade without the stimulus breaching this region. <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows that this paradigm was successful in preventing the target from reaching the central 2 dva of the fovea in 99.27% of saccades. All blocks in which the target could have appeared in this region were removed from further analysis (see methods section quantification and statistical analysis).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental setup.</title><p>(<bold>A</bold>) Illustration of one block of the experimental and control conditions, respectively. During each block in the experimental condition, a peripheral saccade target was shown, which participants were instructed to fixate. The target disappeared before it could be foveated, and once fixation was achieved, a new target appeared, until the block was over (duration: 11 s). The timing of target appearance and disappearance from the experimental condition was recorded and used in the control condition, where targets appeared at fixation. (<bold>B</bold>) Depiction of the four stimuli used in the experiment, which were matched in either visual shape (horizontal/vertical) or semantic category (animal/instrument). Each stimulus appeared equally often in both conditions. (<bold>C</bold>) Histogram of the gaze distance from the stimulus right after stimulus disappearance, including all trial from all 28 participants . The blocks in which the stimulus edge may have appeared in the participants’ fovea during at least one saccade were excluded.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107053-fig1-v1.tif"/></fig><p>To compare the experimental condition to activation elicited by direct foveal input, we included a control condition in which participants were instructed to fixate a point at the center of the screen, with the stimulus appearing directly in the center of the fovea (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, bottom). To keep visual stimulation frequency comparable, the timing of stimulus appearance and disappearance was recorded for each participant in the experimental condition and replayed in the control condition. To elucidate the content of the stimulus information fed back to foveal retinotopic areas, we used four different natural stimuli (one stimulus per block) allowing us to disentangle the nature of the representation in the foveal retinotopic cortex (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The stimuli were manipulated to match in either shape (horizontal vs. vertical) or semantic category (animals vs. instruments).</p><sec id="s2-1"><title>Decoding foveal feedback</title><p>To test for the presence of stimulus-specific activation in foveal regions of early visual cortex, we used cross-validated multivariate decoding (<xref ref-type="bibr" rid="bib36">Hebart and Baker, 2018</xref>), which reveals information that allows discriminating between different stimuli. We compared all pairs of stimuli in the experimental and control conditions, respectively (chance level: 50%). In the experimental condition, we found above-chance decoding in central foveal V1 (t(27) = 8.81, <italic>p</italic>&lt;0.001, mean = 57.43%) (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). This result demonstrates that information about the peripheral saccade targets is present in central foveal regions of V1, despite never appearing in the corresponding part of the fovea. To compare this finding to direct foveal stimulus presentation, we repeated the same analysis for the control condition, where we also found strong significant decoding (t(27) = 19.92, <italic>p</italic>&lt;0.001, mean = 84.06%) (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). To further examine the nature of the neural representation elicited by foveal feedback, we cross-decoded from experimental to control trials by training a classifier on data from central foveal V1 in the experimental condition and testing it on the control condition. Decoding was significantly above chance (t(27) = 5.22, <italic>p</italic>&lt;0.001, mean = 57.2%), indicating a similar representational format between the neural representation elicited by direct presentation of the stimulus in the fovea and that elicited by foveal feedback (<xref ref-type="fig" rid="fig2">Figure 2A</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Foveal feedback can be decoded from V1.</title><p>(<bold>A</bold>) Average decoding accuracy over all pairwise comparisons for control (t(27) = 19.92, <italic>p</italic>&lt;0.001, mean = 84.06%) and experimental (t(27) = 8.81, <italic>p</italic>&lt;0.001, mean = 57.43%) conditions and for cross-decoding from experimental to control condition (t(27) = 5.22, <italic>p</italic>&lt;0.001, mean = 57.2%). (<bold>B</bold>) Average decoding accuracies for all early visual areas as a function of eccentricity for both experimental and control conditions. Error bars represent standard error of the mean. Note that the graphs have different scales of decoding accuracy. The central eccentricities (1–5 dva) were measured using a retinotopic localizer, the outer ones (6–10 dva) were inferred from structural data using Neuropythy (<xref ref-type="bibr" rid="bib5">Benson and Winawer, 2018</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107053-fig2-v1.tif"/></fig><p>This pattern of results may alternatively be explained by spillover from peripheral regions or large receptive fields in the fovea reaching into the periphery (cf <xref ref-type="bibr" rid="bib74">Williams et al., 2008</xref>). To address this issue, we investigated decoding as a function of eccentricity in early visual regions (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Foveal decoding due to peripheral spillover would predict a monotonic relationship between peripheral and foveal decoding in the experimental condition. Instead, we found a U-shaped relationship, with stronger decoding from peripheral and foveal regions compared to parafoveal regions. We tested this relationship using a weighted quadratic regression and found significant positive curvature for decoding in all early visual areas (V1: t(27) = 3.98, <italic>p</italic>=0.008, V2: t(27) = 3.03, <italic>p</italic>=0.02, V3: t(27) = 2.776, <italic>p</italic>=0.025, one-sided). These results highlight the spatial pattern of foveal feedback, separating decoding due to direct stimulus presentation in the periphery and decoding due to feedback to foveal regions. As expected, in the control condition, decoding was the highest in the center of gaze and dropped off towards the periphery.</p></sec><sec id="s2-2"><title>Foveal feedback is sensitive to stimulus shape, not semantic category</title><p>Our use of natural stimuli allowed us to test the effects of shape and category on decoding accuracy to better understand the nature of the information fed back to foveal areas (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Decoding was assessed between visually similar yet semantically dissimilar stimuli (across category), between semantically similar yet visually dissimilar stimuli (across shape), and between visually and semantically dissimilar stimuli (across both). The latter comparison served as a baseline, assessing how good decoding is for maximally different stimuli.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Foveal feedback is sensitive to stimulus shape, not semantic category.</title><p>(<bold>A</bold>) Schematic depiction of comparisons to assess the information content of the neural representations. Comparisons across categories assess similarity of representations in terms of visual stimulus properties, with lower decoding accuracies indicating coding for visual information. Similarly, comparing across shape assesses categorical information. Comparing across both serves as a baseline, showing how high decoding accuracy is between maximally different stimuli. (<bold>B</bold>) Decoding accuracies for all comparisons using data from foveal regions of V1 and from the lateral occipital area (LO) (n=28). Error bars represent the standard error from the mean. Note that the graphs have different y-axes of decoding accuracy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107053-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Decoding stimulus content from all early foveal areas.</title><p>Decoding accuracies for all comparisons using data from foveal regions of V1, V2, and V3 for all 28 participants. Error bars represent the standard error from the mean. These graphs show that the results from V1 outlined in this publication generalize to other regions of the early visual cortex.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107053-fig3-figsupp1-v1.tif"/></fig></fig-group><p>A similar pattern of decoding accuracies emerged in both experimental and control conditions: Decoding from foveal V1 across category dropped significantly relative to baseline (experimental condition: t(27) = 2.25, <italic>p</italic>=0.033, difference = 3.03%; control condition: t(27) = 14.74, <italic>p</italic>&lt;0.001, difference = 16.64%), while decoding across shape remained high (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). This pattern indicates that the nature of the feedback signal in the experimental condition was related to stimulus shape information and not semantic category. The fact that the overall pattern of results across conditions looks similar in the experimental and control conditions is in line with the notion that direct stimulus presentation and foveal feedback elicit similar neural representation, as suggested by the cross-decoding results described above (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). To test the degree to which the classifier was able to pick up on category information, we repeated the same analysis in the lateral occipital area (LO), which has been shown to capture higher-level information about object category (<xref ref-type="bibr" rid="bib33">Grill-Spector et al., 2001</xref>). In this area, the pattern was reversed: Decoding dropped across shape relative to baseline (experimental condition: t(27) = 3.41, <italic>p</italic>=0.002, difference = 5.31%; control condition: t(27) = 7.25, <italic>p</italic>&lt;0.001, difference = 6.7%), while it remained high across category, which suggests that this activation more strongly reflects information about the semantic category than stimulus shape.</p></sec><sec id="s2-3"><title>The role of IPS in mediating foveal feedback</title><p>While the previous analyses revealed the pattern of foveal feedback in early visual regions, they left open which neural regions might be involved in driving or mediating this effect. To this end, we conducted an exploratory analysis looking at the block-by-block fluctuations in foveal decodability in the experimental condition. As a measure of decodability, we used the continuous decision value of the classifier, which signifies the distance to the classifier’s hyperplane on a given trial. Using a parametric modulation analysis, we explored which brain region’s activity increased or decreased as a function of foveal decodability. To control for the effects of the direct peripheral presentation of the stimulus, we used the block-by-block fluctuations of peripheral decoding in the experimental condition as a baseline.</p><p>Since foveal feedback is a process tightly linked to saccadic eye movements (<xref ref-type="bibr" rid="bib49">Kroell and Rolfs, 2022</xref>), we hypothesized that regions associated with eye movements are most likely involved in driving this effect. We focused on three regions that have consistently been associated with eye movements and object representations: Frontal eye fields (FEF) (<xref ref-type="bibr" rid="bib59">Paus, 1996</xref>; <xref ref-type="bibr" rid="bib71">Vernet et al., 2014</xref>), intraparietal sulcus (IPS) (<xref ref-type="bibr" rid="bib3">Andersen et al., 1992</xref>; <xref ref-type="bibr" rid="bib2">Andersen, 1989</xref>), and lateral occipital area (LO) (<xref ref-type="bibr" rid="bib44">Kawawaki et al., 2006</xref>; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Not all voxels in these regions were expected to be functionally active. Therefore, to focus analyses on the most relevant voxels, we selected the 100 voxels in each of these regions that activated most strongly in the experimental condition in general. The region of interest (ROI) analyses showed that all of these areas were significantly related to both foveal and peripheral decoding in the experimental condition (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Specifically, IPS showed significantly larger activation related to foveal decoding compared to peripheral decoding (t(27) = 2.53, <italic>p</italic>=0.026, difference = 4.22), suggesting that IPS may be involved in foveal feedback. While effects in the other regions went in the expected direction, they remained non-significant after correcting for multiple comparisons (LO: t(27) = 0.67, <italic>p</italic>=0.767, difference = 0.53; FEF: t(27) = 2.07, <italic>p</italic>=0.072, difference = 1.98). We conducted the same analysis in the control condition, which showed a significant decrease in FEF (t(27) = –4.62, <italic>p</italic>&lt;0.001, difference = 10.36) and IPS (t(27) = –3.61, <italic>p</italic>=0.004, difference = 11.6) and an increase in LO (t(27) = 5.11, <italic>p</italic>&lt;0.001, difference = 16.89) in association with foveal decoding compared to peripheral decoding. In contrast, we found no significant association between peripheral decoding and any of the ROIs (FEF: t(27) = 0.49, <italic>p</italic>=1.0; IPS: t(27) = 1.35, <italic>p</italic>=0.56; LO: t(27) = 2.43, <italic>p</italic>=0.07). These findings confirm that our main effects were not simply explained by global brain fluctuations or signal-to-noise ratio, since under those conditions, we would have expected a similar relationship between foveal and peripheral decoding as in the experimental condition (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Correlation of foveal decoding with region of interest (ROI) activation.</title><p>(<bold>A</bold>) Cortical masks used in the ROI analyses. These masks were generated using Neurosynth (<xref ref-type="bibr" rid="bib78">Yarkoni et al., 2011</xref>) with the keyword ‘eye movements.’ For later analyses, only the 100 most activating voxels were selected in each area. (<bold>B</bold>) Results of the ROI analyses comparing neural activation as a function of foveal and peripheral decoding in three key areas related to eye movements (n=28). Activation in the intraparietal sulcus (IPS) was significantly higher in association with foveal decoding compared to peripheral decoding (t(27) = 2.53, <italic>p</italic>=0.026, difference = 4.22).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107053-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Parametric modulation analysis in the control condition.</title><p>We conducted the same parametric modulation analysis on the control condition (n=28). Foveal decoding was associated with significantly decreased activation in frontal eye field (FEF) (t(27) = –4.62, <italic>p</italic>&lt;0.001, difference = 10.36) and intraparietal sulcus (IPS) (t(27) = –3.61, <italic>p</italic>=0.004, difference = 11.6), likely because eye movements in the control condition decrease foveal stimulation. We also found increased activation in lateral occipital area (LO) (t(27) = 5.11, <italic>p</italic>&lt;0.001, difference = 16.89). Peripheral decoding was not associated with significant changes in any of the region of interests (ROIs) (FEF: t(27) = 0.49, <italic>p</italic>=1.0; IPS: t(27) = 1.35, <italic>p</italic>=0.56; LO: t(27) = 2.43, <italic>p</italic>=0.07).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107053-fig4-figsupp1-v1.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The present preregistered study provides evidence that early foveal retinotopic areas are involved in the processing of peripheral saccade targets, even if the stimuli are never presented in the central part of the fovea. Using a combination of fMRI and eye-tracking, we were able to decode the identity of saccade targets from foveal regions of early visual cortex (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), suggesting that shape-specific information about peripheral targets is fed back to foveal areas during saccade preparation. We also showed that this foveal feedback is unlikely to be caused by spillover from peripheral regions (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) and that it has a similar neural representation as direct stimulus presentation, as shown by the above-chance cross-decoding (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). This cross-decoding alleviates concerns about the type of information picked up by the decoder in the experimental condition. That is, it cannot be explained by the decoder picking up small changes in eye movements between different stimuli (<xref ref-type="bibr" rid="bib74">Williams et al., 2008</xref>), since the representation - at least in part - transfers to the control condition, where participants did not move their eyes. Furthermore, we showed that foveal feedback is sensitive to the shape, but not the semantic category of the stimulus (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), which suggests that foveal feedback in early visual cortex is rather rudimentary and does not convey a category-invariant stimulus representation. These findings are in line with recent work showing that feedback to primary visual cortex in a fixation condition may primarily carry low-level perceptual information (<xref ref-type="bibr" rid="bib16">Costantino et al., 2025</xref>). Lastly, in an exploratory analysis, we identified the intraparietal sulcus (IPS) as a candidate region for driving foveal feedback (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><sec id="s3-1"><title>Foveal feedback during saccade preparation</title><p>While previous studies have shown that peripheral information can be decoded from foveal regions of the visual cortex and can affect foveal processing, this effect has primarily been studied during passive fixation and under specific task conditions related to distinctive spatial features of the peripheral stimulus (<xref ref-type="bibr" rid="bib74">Williams et al., 2008</xref>; <xref ref-type="bibr" rid="bib16">Costantino et al., 2025</xref>; <xref ref-type="bibr" rid="bib28">Fan et al., 2016</xref>; <xref ref-type="bibr" rid="bib79">Yu and Shim, 2016</xref>; <xref ref-type="bibr" rid="bib12">Chambers et al., 2013</xref>). In the absence of target-specific tasks, or using a different control task, foveal feedback was not observed (<xref ref-type="bibr" rid="bib74">Williams et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">Knapen et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Fan et al., 2016</xref>). The present study employed a paradigm in which participants performed a saccade towards the target—irrespective of target features or a specific task on the target. Despite the absence of a target-specific task, we still observed robust decoding from foveal regions. This task independence is reminiscent of the saccade-based recruitment of feature-based attention in area V4 of the macaque brain (<xref ref-type="bibr" rid="bib9">Burrows et al., 2014</xref>), showing that neurons tuned to the features of an imminent saccade target increase their responsiveness. However, in contrast to such spatially invariant feature selection, our results show clear spatial selectivity for foveal locations. Furthermore, we found a specific positive association of IPS activity with the experimental condition, not the control condition, which is in line with the idea that the foveal feedback effect reported in this study is related to saccade preparation (<xref ref-type="bibr" rid="bib20">Curtis and Connolly, 2008</xref>; <xref ref-type="bibr" rid="bib29">Gaymard et al., 1998</xref>). Since humans invariably perform saccades to bring relevant objects into foveal view, instead of scrutinizing them peripherally, foveal feedback during fixation (e.g. <xref ref-type="bibr" rid="bib74">Williams et al., 2008</xref>) could possibly be the result of the preparation of eye movements that are not executed (cf. <xref ref-type="bibr" rid="bib49">Kroell and Rolfs, 2022</xref>; <xref ref-type="bibr" rid="bib28">Fan et al., 2016</xref>; <xref ref-type="bibr" rid="bib12">Chambers et al., 2013</xref>; <xref ref-type="bibr" rid="bib79">Yu and Shim, 2016</xref>). The function of predictive foveal feedback in this context would be to support continuity of visual processing across eye movements that routinely change objects’ locations in retinotopic coordinates (cf. <xref ref-type="bibr" rid="bib49">Kroell and Rolfs, 2022</xref>). Indeed, covert attention and saccade preparation are strongly coupled processes (<xref ref-type="bibr" rid="bib52">Li et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Kowler et al., 1995</xref>; <xref ref-type="bibr" rid="bib24">Deubel and Schneider, 1996</xref>; <xref ref-type="bibr" rid="bib56">Montagnini and Castet, 2007</xref>; <xref ref-type="bibr" rid="bib66">Rolfs and Carrasco, 2012</xref>; <xref ref-type="bibr" rid="bib65">Rolfs et al., 2011</xref>). The task used by <xref ref-type="bibr" rid="bib74">Williams et al., 2008</xref> would, in natural vision, likely involve an eye movement to the peripheral targets. While these findings offer a plausible alternative understanding of the results of <xref ref-type="bibr" rid="bib74">Williams et al., 2008</xref>, this interpretation remains speculative, and more research is needed to determine whether our findings and theirs result from the same underlying mechanism.</p></sec><sec id="s3-2"><title>Saccadic remapping or foveal prediction</title><p>Saccadic remapping, that is, the increase of activity of neurons in anticipation of a stimulus entering their receptive field, has been observed all over the visual cortex (<xref ref-type="bibr" rid="bib32">Golomb and Mazer, 2021</xref>; <xref ref-type="bibr" rid="bib25">Duhamel et al., 1992</xref>; <xref ref-type="bibr" rid="bib54">Merriam et al., 2007</xref>; <xref ref-type="bibr" rid="bib55">Mirpour and Bisley, 2012</xref>), including primary visual cortex (<xref ref-type="bibr" rid="bib57">Nakamura and Colby, 2002</xref>; <xref ref-type="bibr" rid="bib47">Knapen et al., 2016</xref>). While this effect has been widely reported, there is little evidence that saccadic remapping also encodes feature information in humans (<xref ref-type="bibr" rid="bib64">Rao and Ballard, 1999</xref>; <xref ref-type="bibr" rid="bib76">Xiao et al., 2024</xref>; <xref ref-type="bibr" rid="bib45">Knapen et al., 2009</xref>; <xref ref-type="bibr" rid="bib46">Knapen et al., 2010</xref>; <xref ref-type="bibr" rid="bib51">Lescroart et al., 2016</xref>; <xref ref-type="bibr" rid="bib77">Yao et al., 2016</xref>), but see <xref ref-type="bibr" rid="bib23">Denagamage et al., 2024</xref>. One exception is the presaccadic integration of features across two peripheral locations, provided they are the current and future location of an attended stimulus (<xref ref-type="bibr" rid="bib35">Harrison et al., 2013</xref>; <xref ref-type="bibr" rid="bib70">Szinte et al., 2015</xref>). Such integration, however, could be explained by presaccadic updating of spatial attention pointers (<xref ref-type="bibr" rid="bib65">Rolfs et al., 2011</xref>) that link two retinotopic locations to one object, rather than remapping of feature information per se (<xref ref-type="bibr" rid="bib62">Pelli and Cavanagh, 2013</xref>), although this is a topic of ongoing discussion (<xref ref-type="bibr" rid="bib32">Golomb and Mazer, 2021</xref>). This explanation does not apply to the effect observed in the present study, since there never was a stimulus presented in the fovea, so remapping of spatial attention to the fovea (as in <xref ref-type="bibr" rid="bib65">Rolfs et al., 2011</xref>, Figure 5) would not suffice to explain the data.</p><p>Foveal prediction, on the other hand, genuinely involves the transfer of information of saccade target features. In their psychophysical experiments, <xref ref-type="bibr" rid="bib49">Kroell and Rolfs, 2022</xref>; <xref ref-type="bibr" rid="bib50">Kroell and Rolfs, 2025</xref> found that, during saccade preparation, features of the peripheral saccade target were enhanced in the pre-saccadic fovea. In contrast, three independent studies found no automatic selection of saccade target features at peripheral locations (<xref ref-type="bibr" rid="bib7">Born et al., 2013</xref>; <xref ref-type="bibr" rid="bib42">Jonikaitis and Theeuwes, 2013</xref>; <xref ref-type="bibr" rid="bib73">White et al., 2013</xref>). Thus, while saccade-based feature-based attention is evident in visual cortex (<xref ref-type="bibr" rid="bib9">Burrows et al., 2014</xref>), it is not sufficient to explain feature predictions before saccades. These findings led (<xref ref-type="bibr" rid="bib49">Kroell and Rolfs, 2022</xref>; <xref ref-type="bibr" rid="bib50">Kroell and Rolfs, 2025</xref>) to conclude that the fovea plays a unique role in maintaining perceptual continuity by predicting future inputs during saccade preparation. Our results support this view by showing shape-sensitive decoding, as well as cross-decoding from experimental to control condition, indicating that foveal feedback may lead early foveal regions to share features with the peripheral target stimulus in anticipation of an upcoming saccade. In line with this, <xref ref-type="bibr" rid="bib51">Lescroart et al., 2016</xref> found no evidence for periphery-to-periphery feature remapping, and <xref ref-type="bibr" rid="bib13">Chiu and Golomb, 2025</xref> found supporting evidence for remapping of object-location binding from periphery to fovea but not periphery to periphery. Together, these findings suggest that foveal processing is uniquely equipped for predicting feature information of upcoming stimuli.</p></sec><sec id="s3-3"><title>Comparing foveal feedback to direct presentation</title><p>The decoding patterns in the present study revealed that the information about the saccade targets that is fed back to the foveal cortex may reflect shape information but does not contain higher-level categorical information. These decoding patterns resemble the ones we found in the control condition, where the stimuli were presented directly in the fovea (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Lastly, it was possible to cross-decode by training a decoder on foveal V1 data from the experimental condition and decoding stimulus identity from the same regions in the control condition at above chance level (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). These results are in line with behavioral studies showing that presenting a foveal foil stimulus identical to the peripheral target shortly after target onset improves visual discrimination of the peripheral target (<xref ref-type="bibr" rid="bib79">Yu and Shim, 2016</xref>). Together, these findings suggest shared representational formats in early visual areas between foveal prediction and direct stimulus presentation, which indicates that foveal feedback reflects low-to-mid-level features of the target, similarly to the direct presentation of the stimulus.</p></sec><sec id="s3-4"><title>IPS as a candidate modulator of foveal prediction</title><p>In a parametric modulation analysis, we found that the intraparietal sulcus (IPS) was significantly more active in association with foveal decoding compared to peripheral decoding in the experimental condition (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). This area has been described as neither a strictly visual nor motor area but instead as performing visuomotor integration functions, such as determining the spatial location of saccade targets and forming plans to make eye movements (<xref ref-type="bibr" rid="bib3">Andersen et al., 1992</xref>; <xref ref-type="bibr" rid="bib2">Andersen, 1989</xref>). Further research has shown that this region represents salient stimuli, relative to the center of gaze (<xref ref-type="bibr" rid="bib15">Colby and Duhamel, 1996</xref>). This integrative function makes the IPS an ideal candidate for modulating feature-specific feedback to foveal areas during saccade preparation. While this analysis is exploratory, it offers yet another indication that foveal feedback is inherently linked to saccadic eye movements, and that IPS could play an important role in driving this effect. Future hypothesis-driven research could specifically target this region to more clearly determine its role in foveal feedback.</p></sec><sec id="s3-5"><title>Limitations and future directions</title><p>Despite the insights gained in the present study, several open questions remain. We did not specifically test whether we can find foveal feedback from peripheral targets without any stimulus-specific task (e.g. no eye movements), which is relevant to showing that foveal feedback is task-dependent. However, similar control conditions have been run by <xref ref-type="bibr" rid="bib74">Williams et al., 2008</xref> and <xref ref-type="bibr" rid="bib47">Knapen et al., 2016</xref>. Neither found any stimulus-specific foveal activation with peripheral target presentation in the absence of a target-specific task. Additionally, while the stimulus-specific effects reported in the present study were robust, the results were limited to four different stimuli, since the addition of further conditions would have led to a reduction in statistical power. Future studies could expand upon the present approach by increasing the number of stimuli, possibly collecting data across multiple sessions to achieve sufficiently large effects. Furthermore, while using an fMRI block-design paradigm allowed us to conduct precise spatial analyses of different retinotopic regions with high statistical power, it did not allow for any temporal analyses. Under these circumstances, we cannot fully rule out that the observed effects were influenced by working memory (<xref ref-type="bibr" rid="bib34">Harrison and Tong, 2009</xref>) or mental imagery (<xref ref-type="bibr" rid="bib1">Albers et al., 2013</xref>). However, given previous psychophysical work (<xref ref-type="bibr" rid="bib49">Kroell and Rolfs, 2022</xref>) and the fact that stimulus features were not task-relevant, participants had no incentive to engage in these processes, making it unlikely that they played a strong role in our findings. Another limitation of our work is that, while our results are consistent with psychophysical studies defining the temporal onset of foveal feedback during saccade preparation (<xref ref-type="bibr" rid="bib50">Kroell and Rolfs, 2025</xref>; <xref ref-type="bibr" rid="bib28">Fan et al., 2016</xref>), we cannot rule out that post-saccadic processes might have also influenced the observed effects (<xref ref-type="bibr" rid="bib12">Chambers et al., 2013</xref>). Lastly, an additional condition in which participants make a saccade to a neighboring stimulus could elucidate if foveal prediction is exclusive to the target of a saccade. While these questions offer exciting research avenues for future studies, our results demonstrate the importance of foveal feedback during saccadic eye movements, offering a plausible candidate mechanism for our ability to integrate visual information across saccades. They also pave the way for future research about how foveal prediction may facilitate object recognition by giving object processing a head start before fixation onset.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>This study was pre-registered on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/rxacd/">https://osf.io/rxacd/</ext-link>), detailing the hypotheses, methodology, and planned analyses prior to data collection, with the exception of the exploratory parametric modulation analysis. While the preregistration was initiated and written before conducting the study, due to an error on the authors’ side, it was only submitted after submission of the manuscript. However, timestamps on the website document demonstrate that the preregistration text was complete before data collection and not altered afterwards.</p><p>The dataset collected for this study is available on OpenNeuro: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds005933.v1.0.0">https://doi.org/10.18112/openneuro.ds005933.v1.0.0</ext-link>. The experimental and analysis code are available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/Lucakaemmer/FovealFeedback">https://github.com/Lucakaemmer/FovealFeedback</ext-link> (copy archived at <xref ref-type="bibr" rid="bib43">Kaemmer, 2026</xref>).</p><sec id="s4-1"><title>Experimental model and study participant details</title><p>The experiment was performed with human participants recruited by the recruitment system of the Max Planck Institute for Cognitive and Brain Sciences in Leipzig, where the experiment was conducted. Thirty participants were tested, which is similar to comparable studies. Two participants were excluded. One person had poor eye-tracking performance, which impaired the functioning of the gaze-contingent aspect of the experiment (fewer than 100 valid saccades in the experiment). The other was excluded based on poor fMRI data, likely due to drowsiness, which made it impossible to generate retinotopic masks. The remaining 28 participants (14 male and 14 female) were 18–40 years old (average 27.54 years), healthy, right-handed, and had normal or corrected-to-normal vision. Participants were compensated with €12 per hour. The experiment was conducted in accordance with the declaration of Helsinki, and the experimental procedure was approved by the ethics review board of the University of Leipzig (reference number: 421/23-ek). The participants gave written informed consent before taking part in the study.</p></sec><sec id="s4-2"><title>Method details</title><sec id="s4-2-1"><title>Stimuli</title><p>The stimuli were four different object images taken from the Hemera object dataset (H. Technologies. Hemera photo objects). These stimuli were chosen to match either regarding their visual shape (horn and tiger, guitar and kangaroo) or their semantic category (horn and guitar, tiger, and kangaroo). The stimuli that were visually similar were also semantically dissimilar, and vice versa (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). All stimuli were normalized regarding their overall luminance and root mean square contrast and were presented at a size of 3 dva.</p></sec><sec id="s4-2-2"><title>Behavioral task</title><p>In the experimental condition, objects appeared in eight different locations arranged in a circle around the screen, each of them 4 dva away from the center of the screen (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Experimental blocks started with a white fixation point appearing in one of these eight locations. After 4 s, the fixation point turned black, and one of the four stimuli appeared in one of the three locations on the opposite side of the screen, 7.4 dva or 8 dva away from the fixation point. The stimulus only appeared if participants were fixating on the fixation point. Stimulus appearance was the cue for the participant to perform a saccade. As soon as their gaze came within 6.5 dva of the stimulus during the saccade, the stimulus disappeared to prevent it from appearing in the participant’s fovea. After an inter-trial interval of 0.5 s, the same stimulus appeared again at the next location. This pattern repeated for 11 s, concluding one experimental block. One run consisted of 20 blocks, switching to a different stimulus in each block. The experiment included five experimental runs and five control runs.</p><p>In the control condition, stimuli were presented at the center of the fovea, with fixation at the center of the screen (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The overall timing of the stimulus presentation was the same as in the experimental condition. To this end, the timing of stimulus appearance and disappearance in the experimental condition, which depended on the participants’ eye movements, was recorded and used in the control condition to provide the same exposure duration to the stimulus across conditions.</p><p>Lastly, an eccentricity retinotopic localizer and standard object localizer task were used to create retinotopic mapping and to identify object-processing areas in each participant. The retinotopic localizer consisted of six iterations of contracting or expanding rings to map each participant’s eccentricity in early visual areas. The object localizer consisted of blocks of either object images or scrambled images to separate object-processing areas from lower-level visual areas.</p></sec><sec id="s4-2-3"><title>Imaging</title><p>fMRI scanning was performed at the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany, using a Siemens Magnetom Skyra 3T scanner equipped with a 32-channel head coil (Siemens, Erlangen). Whole-brain T2*-weighted echo-planar images (EPI) were collected with the following parameters: repetition time (TR)=2000 ms, echo time (TE)=23.6 ms, flip angle = 80°, field of view (FOV)=204 mm², voxel size = 2×2×2 mm³, and 69 transverse slices with no gap. An interleaved slice acquisition was used with a multiband acceleration factor of 3 and no in-plane acceleration, and the phase encoding direction was anterior to posterior. High-resolution T1-weighted anatomical images were acquired at the end of the scanning session using a standard magnetization-prepared rapid gradient echo (MPRAGE) sequence. Parallel imaging with GRAPPA was utilized for the T1-weighted MPRAGE sequence with an acceleration factor of 2.</p><p>For eye tracking, we used an EyeLink 1000+ Eyetracker, sampling the right eye at 1000 Hz. The eye tracker was calibrated using a 13-point setup to ensure high eye tracking quality. The stimuli were presented using a VPixx PROPixx projector, set to a real-time refresh rate of 180 Hz to allow for the gaze-contingent disappearance of the stimulus before participants could fixate on it. The presentation was programmed in Python using PsychoPy (<xref ref-type="bibr" rid="bib61">Peirce et al., 2019</xref>).</p><p>Each participant completed one scanning session, starting with 10 experimental runs (five gaze-contingent and five fixation runs), which took 320 s each. The retinotopic localizer run took 206 s, and the object localizer run took 426 s, bringing the overall time inside the MRI scanner to around 80 min, including structural scans, breaks, and the calibration of the eye tracker.</p></sec></sec><sec id="s4-3"><title>Quantification and statistical analysis</title><p>The fMRI data were preprocessed using fMRIPrep version 20.2.0 (<xref ref-type="bibr" rid="bib27">Esteban et al., 2019</xref>). Brain surfaces were reconstructed using recon-all from FreeSurfer v6.0.1 (<xref ref-type="bibr" rid="bib21">Dale et al., 1999</xref>). Functional data were slice-time corrected using 3dTshift from AFNI v16.2.07 (<xref ref-type="bibr" rid="bib17">Cox, 1996</xref>) and motion-corrected using mcflirt (<xref ref-type="bibr" rid="bib41">Jenkinson et al., 2012</xref>). The functional data used for the decoding analysis remained in each participant’s native space, without registering to a brain template.</p><p>Univariate analyses were conducted on all experimental and control runs using FSL’s FEAT (v6.00) to perform a general linear model analysis (<xref ref-type="bibr" rid="bib41">Jenkinson et al., 2012</xref>). The resulting beta values for each block were then used in the subsequent multivariate decoding analysis. We chose pairwise classification over multiclass classification (1) since our intent was not to build a classifier for real-world applications but to infer from the presence of above-chance accuracies the presence of discriminative information (<xref ref-type="bibr" rid="bib36">Hebart and Baker, 2018</xref>) and (2) since most common multiclass classification approaches implicitly rely on the comparison of multiple pairwise classifiers anyway. For the decoding analyses and statistical tests, a custom-written Python pipeline was used; a linear support vector machine with fivefold cross-validation was used for decoding, provided by the Python package sklearn (<xref ref-type="bibr" rid="bib60">Pedregosa et al., 2011</xref>). Decoding accuracies were compared with t-tests using the Python package SciPy (<xref ref-type="bibr" rid="bib72">Virtanen et al., 2020</xref>). One-sample t-tests were used to assess if decoding accuracies were significantly different from chance. Since a within-subjects design was used in which all participants went through all the conditions, paired, two-sided t-tests were used to compare decoding accuracies (across shape, across category, across both). The significance threshold was set at <italic>p</italic>&lt;0.05. Significance levels in figures are indicated as follows: *<italic>p</italic>&lt;0.05, **<italic>p</italic>&lt;0.01, ***<italic>p</italic>&lt;0.001.</p><p>Anatomical masks for V1, V2, and V3 were generated for each participant using Neuropythy (<xref ref-type="bibr" rid="bib5">Benson and Winawer, 2018</xref>). The same package was used to estimate the outer retinotopic eccentricities from 6 to 10 dva. The inner eccentricities from 1 to 5 degrees were estimated using the functional retinotopic localizer. A univariate analysis was used to estimate the voxels responding to a certain range of eccentricity. Similarly, anatomical masks for LO were generated for each participant using the functional object localizer.</p><p>The parametric modulation analysis was conducted with FSL’s FEAT. Using the decoders’ block-by-block fluctuations in decision scores as regressors, the parametric modulation analysis was run on the preprocessed data from the experimental runs, which was converted to MNI space to allow for second-level analyses across participants. Masks for the eye-tracking ROIs were generated using the website Neurosynth (<xref ref-type="bibr" rid="bib78">Yarkoni et al., 2011</xref>), which synthesizes data from thousands of fMRI studies to create functional cortical masks in MNI space. Using the keyword ‘eye movements,’ we generated masks of regions known to be associated with eye movements: Frontal eye fields (FEF) (<xref ref-type="bibr" rid="bib59">Paus, 1996</xref>; <xref ref-type="bibr" rid="bib71">Vernet et al., 2014</xref>), intraparietal sulcus (IPS) (<xref ref-type="bibr" rid="bib3">Andersen et al., 1992</xref>; <xref ref-type="bibr" rid="bib2">Andersen, 1989</xref>), and lateral occipital area (LO) (<xref ref-type="bibr" rid="bib44">Kawawaki et al., 2006</xref>; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). We then isolated the 100 voxels per area that activated most strongly in the experimental condition overall, independent of which stimulus was shown. Beta estimates for these 100 voxels in each area were then compared with paired, one-sided t-tests using SciPy to test our hypothesis that foveal decoding was associated with larger neural activation in our ROIs. These tests were corrected for multiple comparisons using Bonferroni correction.</p><p>To assure that there was no direct foveal stimulation in the experimental condition, we excluded all the experimental blocks in which any part of the stimulus may have appeared in the participants’ fovea during at least one saccade, which happened in 0.73% of all saccades. Since entire blocks were removed when they contained a single such saccade, this led to an exclusion of 8.49% of experimental blocks. To keep the balance between experimental and control conditions, we also excluded the corresponding blocks from the control condition.</p><p>The processing of offline eye movement data was performed in Matlab 2018b (Mathworks, Natick, MA, USA). Within an experimental block, a trial only started (characterized by the appearance of a stimulus) once participants fixated within 1 dva of the initial fixation point and only ended once they fixated within 1 dva of the target fixation point. In the offline analysis, a saccade was detected when 2D velocity exceeded 5 standard deviations from the median for a minimum of 5 ms (<xref ref-type="bibr" rid="bib26">Engbert and Mergenthaler, 2006</xref>). Saccade candidates that were less than 10 ms apart usually resulting from post-saccadic oscillations (<xref ref-type="bibr" rid="bib68">Schweitzer and Rolfs, 2022</xref>) were merged into a single saccade. On average, participants performed 1084 (±206.17) trials over the course of the experiment, which amounts to 10.84 trials per experimental block. Overall, participants reached the target area (within 2 dva of stimulus center) in a single saccade in 76.07% of all stimulus appearances, and in 16.94% in more than one saccade. Note that humans can plan two saccades in advance and still allocate attention to the final goal ahead of the first saccade (<xref ref-type="bibr" rid="bib65">Rolfs et al., 2011</xref>; <xref ref-type="bibr" rid="bib4">Baldauf and Deubel, 2008</xref>; <xref ref-type="bibr" rid="bib30">Gersch et al., 2004</xref>; <xref ref-type="bibr" rid="bib31">Godijn and Theeuwes, 2003</xref>). In 6.99% of trials, no saccade was detected in the offline analysis, probably due to tracking noise in the MRI scanner. In the worst case, these trials would only have added noise. Average saccade latency was 240.70 ms, and average saccade amplitude was 6.76 dva. All the figures were prepared using Python and PowerPoint.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Visualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis</p></fn><fn fn-type="con" id="con3"><p>Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The experiment was conducted in accordance with the declaration of Helsinki, and the experimental procedure was approved by the ethics review board of the University of Leipzig (reference number: 421/23-ek). The participants gave written informed consent before taking part in the study.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-107053-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The dataset collected for this study is available on OpenNeuro: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds005933.v1.0.0">https://doi.org/10.18112/openneuro.ds005933.v1.0.0</ext-link>. The experimental and analysis code are available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/Lucakaemmer/FovealFeedback">https://github.com/Lucakaemmer/FovealFeedback</ext-link> (copy archived at <xref ref-type="bibr" rid="bib43">Kaemmer, 2026</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Kaemmer</surname><given-names>L</given-names></name><name><surname>Hebart</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Foveal Feedback</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds005933.v1.0.0</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We would like to thank the members of the Vision and Computational Cognition Group at the Max Planck Institute for Human Cognitive and Brain Sciences for their valuable feedback and contributions to this study. We would also like to thank the staff in the MRI department at the MPI-CBS that supported the data collection. This work was supported by a doctoral student scholarship awarded to LK by the German Academic Scholarship Foundation ('Studienstiftung des Deutschen Volkes'), a research group grant by the Max Planck Society awarded to MNH, the ERC Starting Grant project COREDIM (ERC-StG-2021–101039712) and the Hessian Ministry of Higher Education, Science, Research and Art (LOEWE Start Professorship to MNH and Excellence Program ‘The Adaptive Mind’). MR was supported by the ERC Consolidator Grant project VIS-A-VIS (grant agreement 865715). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript. Open access funding provided by Max Planck Society.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albers</surname><given-names>AM</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name><name><surname>Dijkerman</surname><given-names>HC</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Shared representations for working memory and mental imagery in early visual cortex</article-title><source>Current Biology</source><volume>23</volume><fpage>1427</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id><pub-id pub-id-type="pmid">23871239</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Visual and eye movement functions of the posterior parietal cortex</article-title><source>Annual Review of Neuroscience</source><volume>12</volume><fpage>377</fpage><lpage>403</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.12.030189.002113</pub-id><pub-id pub-id-type="pmid">2648954</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Brotchie</surname><given-names>PR</given-names></name><name><surname>Mazzoni</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Evidence for the lateral intraparietal area as the parietal eye field</article-title><source>Current Opinion in Neurobiology</source><volume>2</volume><fpage>840</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(92)90143-9</pub-id><pub-id pub-id-type="pmid">1477549</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldauf</surname><given-names>D</given-names></name><name><surname>Deubel</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Properties of attentional selection during the preparation of sequential saccades</article-title><source>Experimental Brain Research</source><volume>184</volume><fpage>411</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1007/s00221-007-1114-x</pub-id><pub-id pub-id-type="pmid">17846754</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bayesian analysis of retinotopic maps</article-title><source>eLife</source><volume>7</volume><elocation-id>e40224</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.40224</pub-id><pub-id pub-id-type="pmid">30520736</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blom</surname><given-names>T</given-names></name><name><surname>Feuerriegel</surname><given-names>D</given-names></name><name><surname>Johnson</surname><given-names>P</given-names></name><name><surname>Bode</surname><given-names>S</given-names></name><name><surname>Hogendoorn</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Predictions drive neural representations of visual events ahead of incoming sensory information</article-title><source>PNAS</source><volume>117</volume><fpage>7510</fpage><lpage>7515</lpage><pub-id pub-id-type="doi">10.1073/pnas.1917777117</pub-id><pub-id pub-id-type="pmid">32179666</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Born</surname><given-names>S</given-names></name><name><surname>Ansorge</surname><given-names>U</given-names></name><name><surname>Kerzel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predictability of spatial and non-spatial target properties improves perception in the pre-saccadic interval</article-title><source>Vision Research</source><volume>91</volume><fpage>93</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2013.08.003</pub-id><pub-id pub-id-type="pmid">23954813</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burr</surname><given-names>DC</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name><name><surname>Ross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Selective suppression of the magnocellular visual pathway during saccadic eye movements</article-title><source>Nature</source><volume>371</volume><fpage>511</fpage><lpage>513</lpage><pub-id pub-id-type="doi">10.1038/371511a0</pub-id><pub-id pub-id-type="pmid">7935763</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burrows</surname><given-names>BE</given-names></name><name><surname>Zirnsak</surname><given-names>M</given-names></name><name><surname>Akhlaghpour</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Global selection of saccadic target features by neurons in area V4</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>6700</fpage><lpage>6706</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0867-13.2014</pub-id><pub-id pub-id-type="pmid">24806696</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The neural control of looking</article-title><source>Current Biology</source><volume>10</volume><fpage>R291</fpage><lpage>R293</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(00)00430-9</pub-id><pub-id pub-id-type="pmid">10801426</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname><given-names>P</given-names></name><name><surname>Hunt</surname><given-names>AR</given-names></name><name><surname>Afraz</surname><given-names>A</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual stability based on remapping of attention pointers</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>147</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.007</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>CD</given-names></name><name><surname>Allen</surname><given-names>CPG</given-names></name><name><surname>Maizey</surname><given-names>L</given-names></name><name><surname>Williams</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Is delayed foveal feedback critical for extra-foveal perception?</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>49</volume><fpage>327</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2012.03.007</pub-id><pub-id pub-id-type="pmid">22503283</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chiu</surname><given-names>TY</given-names></name><name><surname>Golomb</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>The influence of saccade target status on the reference frame of object-location binding</article-title><source>Journal of Experimental Psychology. General</source><volume>154</volume><fpage>1183</fpage><lpage>1200</lpage><pub-id pub-id-type="doi">10.1037/xge0001718</pub-id><pub-id pub-id-type="pmid">40080559</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title><source>The Behavioral and Brain Sciences</source><volume>36</volume><fpage>181</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id><pub-id pub-id-type="pmid">23663408</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colby</surname><given-names>CL</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Spatial representations for action in parietal cortex</article-title><source>Brain Research. Cognitive Brain Research</source><volume>5</volume><fpage>105</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/s0926-6410(96)00046-8</pub-id><pub-id pub-id-type="pmid">9049076</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costantino</surname><given-names>AI</given-names></name><name><surname>Pelzer</surname><given-names>BO</given-names></name><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Crossley</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Partial information transfer from peripheral visual streams to foveal visual streams may be mediated through local primary visual circuits</article-title><source>NeuroImage</source><volume>311</volume><elocation-id>121147</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2025.121147</pub-id><pub-id pub-id-type="pmid">40154647</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research, an International Journal</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curcio</surname><given-names>CA</given-names></name><name><surname>Allen</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Topography of ganglion cells in human retina</article-title><source>The Journal of Comparative Neurology</source><volume>300</volume><fpage>5</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/cne.903000103</pub-id><pub-id pub-id-type="pmid">2229487</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curcio</surname><given-names>CA</given-names></name><name><surname>Sloan</surname><given-names>KR</given-names></name><name><surname>Kalina</surname><given-names>RE</given-names></name><name><surname>Hendrickson</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Human photoreceptor topography</article-title><source>The Journal of Comparative Neurology</source><volume>292</volume><fpage>497</fpage><lpage>523</lpage><pub-id pub-id-type="doi">10.1002/cne.902920402</pub-id><pub-id pub-id-type="pmid">2324310</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Connolly</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Saccade preparation signals in the human frontal and parietal cortices</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>133</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1152/jn.00899.2007</pub-id><pub-id pub-id-type="pmid">18032565</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denagamage</surname><given-names>S</given-names></name><name><surname>Morton</surname><given-names>MP</given-names></name><name><surname>Hudson</surname><given-names>NV</given-names></name><name><surname>Nandy</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Widespread receptive field remapping in early primate visual cortex</article-title><source>Cell Reports</source><volume>43</volume><elocation-id>114557</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2024.114557</pub-id><pub-id pub-id-type="pmid">39058592</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deubel</surname><given-names>H</given-names></name><name><surname>Schneider</surname><given-names>WX</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Saccade target selection and object recognition: evidence for a common attentional mechanism</article-title><source>Vision Research</source><volume>36</volume><fpage>1827</fpage><lpage>1837</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(95)00294-4</pub-id><pub-id pub-id-type="pmid">8759451</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duhamel</surname><given-names>JR</given-names></name><name><surname>Colby</surname><given-names>CL</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The updating of the representation of visual space in parietal cortex by intended eye movements</article-title><source>Science</source><volume>255</volume><fpage>90</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1126/science.1553535</pub-id><pub-id pub-id-type="pmid">1553535</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Mergenthaler</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Microsaccades are triggered by low retinal image slip</article-title><source>PNAS</source><volume>103</volume><fpage>7192</fpage><lpage>7197</lpage><pub-id pub-id-type="doi">10.1073/pnas.0509557103</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Shao</surname><given-names>H</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name><name><surname>He</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Temporally flexible feedback signal to foveal cortex for peripheral object recognition</article-title><source>PNAS</source><volume>113</volume><fpage>11627</fpage><lpage>11632</lpage><pub-id pub-id-type="doi">10.1073/pnas.1606137113</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaymard</surname><given-names>B</given-names></name><name><surname>Ploner</surname><given-names>CJ</given-names></name><name><surname>Rivaud</surname><given-names>S</given-names></name><name><surname>Vermersch</surname><given-names>AI</given-names></name><name><surname>Pierrot-Deseilligny</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical control of saccades</article-title><source>Experimental Brain Research</source><volume>123</volume><fpage>159</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1007/s002210050557</pub-id><pub-id pub-id-type="pmid">9835405</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gersch</surname><given-names>TM</given-names></name><name><surname>Kowler</surname><given-names>E</given-names></name><name><surname>Dosher</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dynamic allocation of visual attention during the execution of sequences of saccades</article-title><source>Vision Research</source><volume>44</volume><fpage>1469</fpage><lpage>1483</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2003.12.014</pub-id><pub-id pub-id-type="pmid">15066405</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Godijn</surname><given-names>R</given-names></name><name><surname>Theeuwes</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Parallel allocation of attention prior to the execution of saccade sequences</article-title><source>Journal of Experimental Psychology</source><volume>29</volume><fpage>882</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.29.5.882</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golomb</surname><given-names>JD</given-names></name><name><surname>Mazer</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Visual Remapping</article-title><source>Annual Review of Vision Science</source><volume>7</volume><fpage>257</fpage><lpage>277</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-032321-100012</pub-id><pub-id pub-id-type="pmid">34242055</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The lateral occipital complex and its role in object recognition</article-title><source>Vision Research</source><volume>41</volume><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00073-6</pub-id><pub-id pub-id-type="pmid">11322983</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>SA</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nature</source><volume>458</volume><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1038/nature07832</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>WJ</given-names></name><name><surname>Mattingley</surname><given-names>JB</given-names></name><name><surname>Remington</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Eye movement targets are released from visual crowding</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>2927</fpage><lpage>2933</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4172-12.2013</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deconstructing multivariate decoding for the study of brain function</article-title><source>NeuroImage</source><volume>180</volume><fpage>4</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.005</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Heckenlively</surname><given-names>JR</given-names></name><name><surname>Arden</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Principles and Practice of Clinical Electrophysiology of Vision</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/5557.001.0001</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hendrickson</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>Organization of the adult primate fovea</chapter-title><person-group person-group-type="editor"><name><surname>Penfold</surname><given-names>PL</given-names></name><name><surname>Provis</surname><given-names>JM</given-names></name></person-group><source>Macular Degeneration</source><publisher-name>Springer-Verlag</publisher-name><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1007/3-540-26977-0_1</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hendrickson</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><chapter-title>Fovea: primate</chapter-title><person-group person-group-type="editor"><name><surname>Hendrickson</surname><given-names>A</given-names></name></person-group><source>Encyclopedia of Neuroscience</source><publisher-name>Elsevier</publisher-name><fpage>49</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/B978-008045046-9.00920-7</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herwig</surname><given-names>A</given-names></name><name><surname>Schneider</surname><given-names>WX</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Predicting object features across saccades: evidence from object recognition and visual search</article-title><source>Journal of Experimental Psychology. General</source><volume>143</volume><fpage>1903</fpage><lpage>1922</lpage><pub-id pub-id-type="doi">10.1037/a0036781</pub-id><pub-id pub-id-type="pmid">24820249</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FSL</article-title><source>NeuroImage</source><volume>62</volume><fpage>782</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonikaitis</surname><given-names>D</given-names></name><name><surname>Theeuwes</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissociating oculomotor contributions to spatial and feature-based selection</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>1525</fpage><lpage>1534</lpage><pub-id pub-id-type="doi">10.1152/jn.00275.2013</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kaemmer</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2026">2026</year><data-title>Foveal feedback</data-title><version designator="swh:1:rev:0bbeacd3bd64fdc3204784a45c7d08a60db97ed7">swh:1:rev:0bbeacd3bd64fdc3204784a45c7d08a60db97ed7</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:2ab298b54117fdcad8252c4d81cf0d24798c70d3;origin=https://github.com/Lucakaemmer/FovealFeedback;visit=swh:1:snp:dfd1c09dde1d1028b9076bbc903c96e1e67bae0a;anchor=swh:1:rev:0bbeacd3bd64fdc3204784a45c7d08a60db97ed7">https://archive.softwareheritage.org/swh:1:dir:2ab298b54117fdcad8252c4d81cf0d24798c70d3;origin=https://github.com/Lucakaemmer/FovealFeedback;visit=swh:1:snp:dfd1c09dde1d1028b9076bbc903c96e1e67bae0a;anchor=swh:1:rev:0bbeacd3bd64fdc3204784a45c7d08a60db97ed7</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawawaki</surname><given-names>D</given-names></name><name><surname>Shibata</surname><given-names>T</given-names></name><name><surname>Goda</surname><given-names>N</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name><name><surname>Kawato</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Anterior and superior lateral occipito-temporal cortex responsible for target motion prediction during overt and covert visual pursuit</article-title><source>Neuroscience Research</source><volume>54</volume><fpage>112</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2005.10.015</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The reference frame of the motion aftereffect is retinotopic</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>16</elocation-id><pub-id pub-id-type="doi">10.1167/9.5.16</pub-id><pub-id pub-id-type="pmid">19757894</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name><name><surname>Wexler</surname><given-names>M</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The reference frame of the tilt aftereffect</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/10.1.8</pub-id><pub-id pub-id-type="pmid">20143901</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Swisher</surname><given-names>JD</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Oculomotor remapping of visual information to foveal retinotopic cortex</article-title><source>Frontiers in Systems Neuroscience</source><volume>10</volume><elocation-id>54</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2016.00054</pub-id><pub-id pub-id-type="pmid">27445715</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kowler</surname><given-names>E</given-names></name><name><surname>Anderson</surname><given-names>E</given-names></name><name><surname>Dosher</surname><given-names>B</given-names></name><name><surname>Blaser</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The role of attention in the programming of saccades</article-title><source>Vision Research</source><volume>35</volume><fpage>1897</fpage><lpage>1916</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)00279-u</pub-id><pub-id pub-id-type="pmid">7660596</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kroell</surname><given-names>LM</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Foveal vision anticipates defining features of eye movement targets</article-title><source>eLife</source><volume>11</volume><elocation-id>e78106</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.78106</pub-id><pub-id pub-id-type="pmid">36082940</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kroell</surname><given-names>LM</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>The magnitude and time course of pre-saccadic foveal prediction depend on the conspicuity of the saccade target</article-title><source>eLife</source><volume>12</volume><elocation-id>RP91236</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.91236</pub-id><pub-id pub-id-type="pmid">40637717</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lescroart</surname><given-names>MD</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Golomb</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>No evidence for automatic remapping of stimulus features or location found with fMRI</article-title><source>Frontiers in Systems Neuroscience</source><volume>10</volume><elocation-id>53</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2016.00053</pub-id><pub-id pub-id-type="pmid">27378866</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>HH</given-names></name><name><surname>Hanning</surname><given-names>NM</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>To look or not to look: dissociating presaccadic and covert spatial attention</article-title><source>Trends in Neurosciences</source><volume>44</volume><fpage>669</fpage><lpage>686</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2021.05.002</pub-id><pub-id pub-id-type="pmid">34099240</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Melcher</surname><given-names>D</given-names></name><name><surname>Colby</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Trans-saccadic perception</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>466</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.09.003</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Genovese</surname><given-names>CR</given-names></name><name><surname>Colby</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Remapping in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>1738</fpage><lpage>1755</lpage><pub-id pub-id-type="doi">10.1152/jn.00189.2006</pub-id><pub-id pub-id-type="pmid">17093130</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirpour</surname><given-names>K</given-names></name><name><surname>Bisley</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Anticipatory remapping of attentional priority across the entire visual field</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>16449</fpage><lpage>16457</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2008-12.2012</pub-id><pub-id pub-id-type="pmid">23152627</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montagnini</surname><given-names>A</given-names></name><name><surname>Castet</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Spatiotemporal dynamics of visual attention during saccade preparation: Independence and coupling between attention and movement planning</article-title><source>Journal of Vision</source><volume>7</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/7.14.8</pub-id><pub-id pub-id-type="pmid">18217803</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakamura</surname><given-names>K</given-names></name><name><surname>Colby</surname><given-names>CL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Updating of the visual representation in monkey striate and extrastriate cortex during saccades</article-title><source>PNAS</source><volume>99</volume><fpage>4026</fpage><lpage>4031</lpage><pub-id pub-id-type="doi">10.1073/pnas.052379899</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Regan</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Solving the “real” mysteries of visual perception: The world as an outside memory</article-title><source>Canadian Journal of Psychology / Revue Canadienne de Psychologie</source><volume>46</volume><fpage>461</fpage><lpage>488</lpage><pub-id pub-id-type="doi">10.1037/h0084327</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paus</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Location and function of the human frontal eye-field: a selective review</article-title><source>Neuropsychologia</source><volume>34</volume><fpage>475</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(95)00134-4</pub-id><pub-id pub-id-type="pmid">8736560</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name><name><surname>Passos</surname><given-names>A</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J</given-names></name><name><surname>Gray</surname><given-names>JR</given-names></name><name><surname>Simpson</surname><given-names>S</given-names></name><name><surname>MacAskill</surname><given-names>M</given-names></name><name><surname>Höchenberger</surname><given-names>R</given-names></name><name><surname>Sogo</surname><given-names>H</given-names></name><name><surname>Kastman</surname><given-names>E</given-names></name><name><surname>Lindeløv</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PsychoPy2: Experiments in behavior made easy</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id><pub-id pub-id-type="pmid">30734206</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Object recognition: visual crowding from a distance</article-title><source>Current Biology</source><volume>23</volume><fpage>R478</fpage><lpage>R479</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.04.022</pub-id><pub-id pub-id-type="pmid">23743412</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Rucci</surname><given-names>M</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Selective attention within the foveola</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1413</fpage><lpage>1417</lpage><pub-id pub-id-type="doi">10.1038/nn.4622</pub-id><pub-id pub-id-type="pmid">28805816</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RPN</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolfs</surname><given-names>M</given-names></name><name><surname>Jonikaitis</surname><given-names>D</given-names></name><name><surname>Deubel</surname><given-names>H</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Predictive remapping of attention across eye movements</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>252</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1038/nn.2711</pub-id><pub-id pub-id-type="pmid">21186360</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolfs</surname><given-names>M</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rapid simultaneous enhancement of visual sensitivity and perceived contrast during saccade preparation</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>13744</fpage><lpage>52a</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2676-12.2012</pub-id><pub-id pub-id-type="pmid">23035086</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schira</surname><given-names>MM</given-names></name><name><surname>Tyler</surname><given-names>CW</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Spehar</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The foveal confluence in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>9050</fpage><lpage>9058</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1760-09.2009</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schweitzer</surname><given-names>R</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><chapter-title>Definition, modeling, and detection of saccades in the face of post-saccadic oscillations</chapter-title><person-group person-group-type="editor"><name><surname>Stuart</surname><given-names>S</given-names></name></person-group><source>Eye Tracking</source><publisher-name>Springer</publisher-name><fpage>69</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1007/978-1-0716-2391-6_5</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>EEM</given-names></name><name><surname>Valsecchi</surname><given-names>M</given-names></name><name><surname>Schütz</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A review of interactions between peripheral and foveal vision</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.12.2</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szinte</surname><given-names>M</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attentional trade-offs maintain the tracking of moving objects across saccades</article-title><source>Journal of Neurophysiology</source><volume>113</volume><fpage>2220</fpage><lpage>2231</lpage><pub-id pub-id-type="doi">10.1152/jn.00966.2014</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vernet</surname><given-names>M</given-names></name><name><surname>Quentin</surname><given-names>R</given-names></name><name><surname>Chanes</surname><given-names>L</given-names></name><name><surname>Mitsumasu</surname><given-names>A</given-names></name><name><surname>Valero-Cabré</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Frontal eye field, where art thou? Anatomy, function, and non-invasive manipulation of frontal regions involved in eye movements and associated cognitive operations</article-title><source>Frontiers in Integrative Neuroscience</source><volume>8</volume><elocation-id>66</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2014.00066</pub-id><pub-id pub-id-type="pmid">25202241</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><name><surname>Vijaykumar</surname><given-names>A</given-names></name><name><surname>Bardelli</surname><given-names>AP</given-names></name><name><surname>Rothberg</surname><given-names>A</given-names></name><name><surname>Hilboll</surname><given-names>A</given-names></name><name><surname>Kloeckner</surname><given-names>A</given-names></name><name><surname>Scopatz</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>A</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Woods</surname><given-names>CN</given-names></name><name><surname>Fulton</surname><given-names>C</given-names></name><name><surname>Masson</surname><given-names>C</given-names></name><name><surname>Häggström</surname><given-names>C</given-names></name><name><surname>Fitzgerald</surname><given-names>C</given-names></name><name><surname>Nicholson</surname><given-names>DA</given-names></name><name><surname>Hagen</surname><given-names>DR</given-names></name><name><surname>Pasechnik</surname><given-names>DV</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name><name><surname>Martin</surname><given-names>E</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Silva</surname><given-names>F</given-names></name><name><surname>Lenders</surname><given-names>F</given-names></name><name><surname>Wilhelm</surname><given-names>F</given-names></name><name><surname>Young</surname><given-names>G</given-names></name><name><surname>Price</surname><given-names>GA</given-names></name><name><surname>Ingold</surname><given-names>G-L</given-names></name><name><surname>Allen</surname><given-names>GE</given-names></name><name><surname>Lee</surname><given-names>GR</given-names></name><name><surname>Audren</surname><given-names>H</given-names></name><name><surname>Probst</surname><given-names>I</given-names></name><name><surname>Dietrich</surname><given-names>JP</given-names></name><name><surname>Silterra</surname><given-names>J</given-names></name><name><surname>Webber</surname><given-names>JT</given-names></name><name><surname>Slavič</surname><given-names>J</given-names></name><name><surname>Nothman</surname><given-names>J</given-names></name><name><surname>Buchner</surname><given-names>J</given-names></name><name><surname>Kulick</surname><given-names>J</given-names></name><name><surname>Schönberger</surname><given-names>JL</given-names></name><name><surname>de Miranda Cardoso</surname><given-names>JV</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Harrington</surname><given-names>J</given-names></name><name><surname>Rodríguez</surname><given-names>JLC</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J</given-names></name><name><surname>Kuczynski</surname><given-names>J</given-names></name><name><surname>Tritz</surname><given-names>K</given-names></name><name><surname>Thoma</surname><given-names>M</given-names></name><name><surname>Newville</surname><given-names>M</given-names></name><name><surname>Kümmerer</surname><given-names>M</given-names></name><name><surname>Bolingbroke</surname><given-names>M</given-names></name><name><surname>Tartre</surname><given-names>M</given-names></name><name><surname>Pak</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Nowaczyk</surname><given-names>N</given-names></name><name><surname>Shebanov</surname><given-names>N</given-names></name><name><surname>Pavlyk</surname><given-names>O</given-names></name><name><surname>Brodtkorb</surname><given-names>PA</given-names></name><name><surname>Lee</surname><given-names>P</given-names></name><name><surname>McGibbon</surname><given-names>RT</given-names></name><name><surname>Feldbauer</surname><given-names>R</given-names></name><name><surname>Lewis</surname><given-names>S</given-names></name><name><surname>Tygier</surname><given-names>S</given-names></name><name><surname>Sievert</surname><given-names>S</given-names></name><name><surname>Vigna</surname><given-names>S</given-names></name><name><surname>Peterson</surname><given-names>S</given-names></name><name><surname>More</surname><given-names>S</given-names></name><name><surname>Pudlik</surname><given-names>T</given-names></name><name><surname>Oshima</surname><given-names>T</given-names></name><name><surname>Pingel</surname><given-names>TJ</given-names></name><name><surname>Robitaille</surname><given-names>TP</given-names></name><name><surname>Spura</surname><given-names>T</given-names></name><name><surname>Jones</surname><given-names>TR</given-names></name><name><surname>Cera</surname><given-names>T</given-names></name><name><surname>Leslie</surname><given-names>T</given-names></name><name><surname>Zito</surname><given-names>T</given-names></name><name><surname>Krauss</surname><given-names>T</given-names></name><name><surname>Upadhyay</surname><given-names>U</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Vázquez-Baeza</surname><given-names>Y</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>AL</given-names></name><name><surname>Rolfs</surname><given-names>M</given-names></name><name><surname>Carrasco</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Adaptive deployment of spatial and feature-based attention before saccades</article-title><source>Vision Research</source><volume>85</volume><fpage>26</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2012.10.017</pub-id><pub-id pub-id-type="pmid">23147690</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Shim</surname><given-names>WM</given-names></name><name><surname>Dang</surname><given-names>S</given-names></name><name><surname>Triantafyllou</surname><given-names>C</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Feedback of visual object information to foveal retinotopic cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1439</fpage><lpage>1445</lpage><pub-id pub-id-type="doi">10.1038/nn.2218</pub-id><pub-id pub-id-type="pmid">18978780</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurtz</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neuronal mechanisms of visual stability</article-title><source>Vision Research</source><volume>48</volume><fpage>2070</fpage><lpage>2089</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.03.021</pub-id><pub-id pub-id-type="pmid">18513781</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>W</given-names></name><name><surname>Sharma</surname><given-names>S</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Feature-selective responses in macaque visual cortex follow eye movements during natural vision</article-title><source>Nature Neuroscience</source><volume>27</volume><fpage>1157</fpage><lpage>1166</lpage><pub-id pub-id-type="doi">10.1038/s41593-024-01631-5</pub-id><pub-id pub-id-type="pmid">38684892</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>T</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Krishna</surname><given-names>BS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An attention-sensitive memory trace in macaque mt following saccadic eye movements</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002390</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002390</pub-id><pub-id pub-id-type="pmid">26901857</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title><source>Nature Methods</source><volume>8</volume><fpage>665</fpage><lpage>670</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1635</pub-id><pub-id pub-id-type="pmid">21706013</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Q</given-names></name><name><surname>Shim</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Modulating foveal representation can influence visual discrimination in the periphery</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>15</elocation-id><pub-id pub-id-type="doi">10.1167/16.3.15</pub-id><pub-id pub-id-type="pmid">26885627</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107053.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kok</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University College London</institution><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study addresses a question related to how we achieve visual stability across saccadic eye movements. The authors' gaze-contingent fMRI design provides <bold>convincing</bold> evidence that peripherally presented visual stimuli are represented in foveal visual cortex prior to a saccade. The results will be of interest to vision scientists and behavioural neuroscientists.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107053.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study investigated whether the identity of a peripheral saccade target object is fed back to the foveal retinotopic cortex during saccade preparation, a critical prediction of the foveal prediction hypothesis proposed by Kroell &amp; Rolfs (2022). To achieve this, the authors leveraged a gaze-contingent fMRI paradigm, where the peripheral saccade target was removed before the eyes landed near it, and used multivariate decoding analysis to quantify identity information in the foveal cortex. The results showed that the identity of the saccade target object can be decoded based on foveal cortex activity, despite the fovea never directly viewing the object, and that the foveal feedback representation was similar to passive viewing and not explained by spillover effects. Additionally, exploratory analysis suggested IPS as a candidate region mediating such foveal decodability. Overall, these findings provide neural evidence for the foveal cortex processing the features of the saccade target object, potentially supporting the maintenance of perceptual stability across saccadic eye movements.</p><p>Strengths:</p><p>This study is well-motivated by previous theoretical findings (Kroell &amp; Rolfs, 2022), aiming to provide neural evidence for a potential neural mechanism of trans-saccadic perceptual stability. The question is important, and the gaze-contingent fMRI paradigm is a solid methodological choice for the research goal. The use of stimuli allowing orthogonal decoding of stimulus category vs stimulus shape is a nice strength, and the resulting distinctions in decoded information by brain region are clean. The results will be of interest to readers in the field, and they fill in some untested questions regarding pre-saccadic remapping and foveal feedback.</p><p>Weaknesses:</p><p>The authors have done a nice job addressing the previous weaknesses. The remaining weaknesses / limitations are appropriately discussed in the manuscript. E.g., the use of only 4 unique stimuli in the experiment. The findings are intriguing and relevant to saccadic remapping and foveal feedback, but somewhat limited in terms of the ability to draw theoretical distinctions between these related phenomena.</p><p>Specifics:</p><p>The revised manuscript is much improved in terms of framing and discussion of the prior literature, and the theoretical claims are now stated with appropriate nuance.</p><p>I have two remaining minor suggestions/comments, which the authors may optionally respond to:</p><p>(1) In the parametric modulation analysis, the authors' additional analyses nicely addresses my concern and strengthens the claim. However, the description in the revised manuscript (Pg 7 Ln 190-191) is minimal and may be difficult to grasp what the control analysis is about and how it rules out alternative explanations to the IPS findings. The authors may wish to elaborate on the description in the text.</p><p>(2) Out of curiosity (not badgering): The authors argued that the findings of Harrison et al. (2013) and Szinte et al. (2015) can be explained by feature integration between the currently attended location and its future, post-saccadic location. Couldn't the same argument apply in the current paradigm, where attention at the saccade target gets remapped to the pre-saccadic fovea (see also Rolfs et al., 2011 Fig 5), thus leading to the observed feature remapping?</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107053.3.sa2</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this paper the authors used fMRI to determine whether peripherally-viewed objects could be decoded from foveal cortex, even when the objects themselves were never viewed foveally. Specifically they investigated whether pre-saccadic target attributes (shape, semantic category) could be decoded from foveal cortex. They found that object shape, but not semantic category could be decoded, providing evidence that foveal feedback relies on low-mid-level information. The authors claim that this provides evidence for a mechanism underlying visual stability and object recognition across saccades.</p><p>Strengths:</p><p>I think this is another nice demonstration that peripheral information can be decoded from / is processed in foveal cortex - the methods seem appropriate, and the experiments and analyses carefully conducted, and the main results seem convincing. The paper itself was very clear and well-written.</p><p>Weaknesses:</p><p>Given that foveal feedback has been found in previous studies that don't incorporate saccades, it is still unclear how this mechanism might specifically contribute to stability across saccades, rather than just being a general mechanism that aids the processing/discrimination of peripherally-viewed stimuli. The authors address this point, but I guess whether foveal feedback during fixation and saccade prep are really the same, is ultimately a question that needs more experimental work to disentangle.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107053.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kämmer</surname><given-names>Luca</given-names></name><role specific-use="author">Author</role><aff><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution><addr-line><named-content content-type="city">Leipzig</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Kroell</surname><given-names>Lisa M</given-names></name><role specific-use="author">Author</role><aff><institution>Max Planck Institute for Biological Intelligence</institution><addr-line><named-content content-type="city">Munich</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Knapen</surname><given-names>Tomas</given-names></name><role specific-use="author">Author</role><aff><institution>Vrije Universiteit Amsterdam</institution><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Rolfs</surname><given-names>Martin</given-names></name><role specific-use="author">Author</role><aff><institution>Humboldt University Berlin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Hebart</surname><given-names>Martin N</given-names></name><role specific-use="author">Author</role><aff><institution>Justus Liebig University Giessen</institution><addr-line><named-content content-type="city">Giessen</named-content></addr-line><country>Germany</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>The main contributions of this paper are: (1) a replication of the surprising prior finding that information about peripherally-presented stimuli can be decoded from foveal V1 (Williams et al 2008), (2) a new demonstration of cross-decoding between stimuli presented in the periphery and stimuli presented at the fovea, (3) a demonstration that the information present in the fovea is based on shape not semantic category, and (4) a demonstration that the strength of foveal information about peripheral targets is correlated with the univariate response in the same block in IPS.</p><p>Strengths:</p><p>The design and methods appear sound, and finding (2) above is new, and importantly constrains our understanding of this surprising phenomenon. The basic effect investigated here is so surprising that even though it has been replicated several times since it was first reported in 2008, it is useful to replicate it again.</p></disp-quote><p>We thank the reviewer for their summary. While we agree with many points, we would like to respectfully push back on the notion that this work is a replication of Williams et al. (2008). What our findings share with those of Williams is a report of surprising decoding at the fovea without foveal stimulation. Beyond this similarity, we treat these as related but clearly separate findings, for the following reasons:</p><p>(1) Foveal feedback, as shown by Williams et al. (2008) and others during fixation, was only observed during a shape discrimination task, specific to the presented stimulus. Control experiments without such a task (or a color-related task) did not show effects of foveal feedback. In contrast, in the present study, the participants’ task was merely to perform saccades towards stimuli, independently of target features. We thus show that foveal feedback can occur independently of a task related to stimulus features. This dissociation demonstrates that our study must be tapping into something different than reported by Williams.</p><p>(2) In a related study, Kroell and Rolfs (2022, 2025) demonstrated a connection between foveal feedback and saccade preparation, including the temporal details of the onset of this effect before saccade execution, highlighting the close link of this effect to saccade preparation. Here we used a very similar behavioral task to capture this saccade-related effect in neural recordings and investigate how early it occurs and what its nature is. Thus, there is a clear motivation for this study in the context of eye movement preparation that is separate from the previous work by Williams.</p><p>(3) Lastly, decoding in the experimental task was positively associated with activity in FEF and IPS, areas that have been reliably linked to saccade preparation. We have now also performed an additional analysis (see our response to Specific point 2 of Reviewer 2) showing that decoding in the control condition did not show the same association, further supporting the link of foveal feedback to saccade preparation.</p><p>Despite our emphasis on these critical differences in studies, covert peripheral attention, as required by the task in Williams et al., and saccade preparation in natural vision, as in our study, are tightly coupled processes. Indeed, the task in Williams et al. would, during natural vision, likely involve an eye movement to the peripheral target. While speculative, a parsimonious and ecologically valid explanation is that both ours and earlier studies involve eye movement preparation, for which execution is suppressed, however, in studies enforcing fixation (e.g., Williams et al., 2008). We now discuss this idea of a shared underlying mechanism more extensively in the revised manuscript (pg 8 ln 228-240).</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) The paper, including in the title (&quot;Feedback of peripheral saccade targets to early foveal cortex&quot;) seems to assume that the feedback to foveal cortex occurs in conjunction with saccade preparation. However, participants in the original Williams et al (2008) paper never made saccades to the peripheral stimuli. So, saccade preparation is not necessary for this effect to occur. Some acknowledgement and discussion of this prior evidence against the interpretation of the effect as due to saccade preparation would be useful. (e.g., one might argue that saccade preparation is automatic when attending to peripheral stimuli.)</p></disp-quote><p>We agree that the effects Williams et al. showed were not sufficiently discussed in the first version of this manuscript. To more clearly engage with these findings we now introduce saccade related foveal feedback (foveal prediction) and foveal feedback during fixation separately in the introduction (pg 2 ln 46-59).</p><p>We further added another section in the discussion called “Foveal feedback during saccade preparation” in which we discuss how our findings are related to Williams et al. and how they differ (pg 8 ln 211-240).</p><p>As described in our previous response, we believe that our findings go beyond those described by Williams et al. (2008) and others in significant ways. However, during natural vision, the paradigm used by Williams et al. (2008) would likely be solved using an eye movement. Thus, while participants in Williams et al. (2008) did not execute saccades, it appears plausible that they have prepared saccades. Given the fact that covert peripheral attention and saccade preparation are tightly coupled processes (Kowler et al., 1995, Vis Res; Deubel &amp; Schneider, 1996, Vis Res; Montagnini &amp; Castet, 2007, J Vis; Rolfs &amp; Carrasco, 2012, J Neurosci; Rolfs et al., 2011, Nat Neurosci), their results are parsimoniously explained by saccade preparation (but not execution) to a behaviorally relevant target.</p><disp-quote content-type="editor-comment"><p>(2) The most important new finding from this paper is the cross-decodability between stimuli presented in the fovea and stimuli presented in the periphery. This finding should be related to the prior behavioral finding (Yu &amp; Shim, 2016) that when a foveal foil stimulus identical to a peripheral target is presented 150 ms after the onset of the peripheral target, visual discrimination of the peripheral target is improved, and this congruency effect occurred even though participants did not consciously perceive the foveal stimulus (Yu, Q., &amp; Shim, W. M., 2016). Modulating foveal representation can influence visual discrimination in the periphery (Journal of Vision, 16(3), 15-15).</p></disp-quote><p>We thank the reviewer for highlighting this highly relevant reference. In the revised version of the manuscript, we now put more emphasis on the finding of cross-decodability (pg 2 ln 60-61). We now also discuss Yu et al.’s finding, which support our conclusion that foveal feedback and direct stimulus presentation share representational formats in early visual areas (pg 9 ln 277-279).</p><disp-quote content-type="editor-comment"><p>(3) The prior literature should be laid out more clearly. For example, most readers will not realize that the basic effect of decodability of peripherally-presented stimuli in the fovea was first reported in 2008, and that that original paper already showed that the effect cannot arise from spillover effects from peripheral retinotopic cortex because it was not present in a retinotopic location between the cortical locus corresponding to the peripheral target and the fovea. (For example, this claim on lines 56-57 is not correct: &quot;it remains unknown (1) whether information is fed back all the way to early visual areas&quot;). What is needed is a clear presentation of the prior findings in one place in the introduction to the paper, followed by an articulation and motivation of the new questions addressed in this paper. If I were writing the paper, I would focus on the cross-decodability between foveal and peripheral stimuli, as I think that is the most revealing finding.</p></disp-quote><p>We agree that the structure of the introduction did not sufficiently place our work in the context of prior literature. We have now expanded upon our Introduction section to discuss past studies of saccade- and fixation-related foveal feedback (pg 2 ln 49-59), laying out how this effect has been studied previously. We also removed the claim that &quot;it remains unknown (1) whether information is fed back all the way to early visual areas&quot;, where our intention was to specifically focus on foveal prediction. We realize that this was not clear and hence removed this section. Instead, we now place a stronger focus on the cross-decodability finding (pg 2 ln 60-61).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>This study investigated whether the identity of a peripheral saccade target object is predictively fed back to the foveal retinotopic cortex during saccade preparation, a critical prediction of the foveal prediction hypothesis proposed by Kroell &amp; Rolfs (2022). To achieve this, the authors leveraged a gaze-contingent fMRI paradigm, where the peripheral saccade target was removed before the eyes landed near it, and used multivariate decoding analysis to quantify identity information in the foveal cortex. The results showed that the identity of the saccade target object can be decoded based on foveal cortex activity, despite the fovea never directly viewing the object, and that the foveal feedback representation was similar to passive viewing and not explained by spillover effects. Additionally, exploratory analysis suggested IPS as a candidate region mediating such foveal decodability. Overall, these findings provide neural evidence for the foveal cortex processing the features of the saccade target object, potentially supporting the maintenance of perceptual stability across saccadic eye movements.</p><p>Strengths:</p><p>This study is well-motivated by previous theoretical findings (Kroell &amp; Rolfs, 2022), aiming to provide neural evidence for a potential neural mechanism of trans-saccadic perceptual stability. The question is important, and the gaze-contingent fMRI paradigm is a solid methodological choice for the research goal. The use of stimuli allowing orthogonal decoding of stimulus category vs stimulus shape is a nice strength, and the resulting distinctions in decoded information by brain region are clean. The results will be of interest to readers in the field, and they fill in some untested questions regarding pre-saccadic remapping and foveal feedback.</p></disp-quote><p>We thank the reviewer for the positive assessment of our study.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>The conclusions feel a bit over-reaching; some strong theoretical claims are not fully supported, and the framing of prior literature is currently too narrow. A critical weakness lies in the inability to test a distinction between these findings (claiming to demonstrate that &quot;feedback during saccade preparation must underlie this effect&quot;) and foveal feedback previously found during passive fixation (Williams et al., 2008). Discussions (and perhaps control analysis/experiments) about how these findings are specific to the saccade target and the temporal constraints on these effects are lacking. The relationship between the concepts of foveal prediction, foveal feedback, and predictive remapping needs more thorough treatment. The choice to use only 4 stimuli is justified in the manuscript, but remains an important limitation. The IPS results are intriguing but could be strengthened by additional control analysis. Finally, the manuscript claims the study was pre-registered (&quot;detailing the hypotheses, methodology, and planned analyses prior to data collection&quot;), but on the OSF link provided, there is just a brief summary paragraph, and the website says &quot;there have been no completed registrations of this project&quot;.</p></disp-quote><p>We thank the reviewer for these helpful considerations. We agree that some of the claims were not sufficiently supported by the evidence, and in the revised manuscript, we added nuance to those claims (pg 8 ln 211-240). Furthermore, we now address more directly the distinction between foveal feedback during fixation and foveal feedback (foveal prediction) during saccade preparation. In particular, we now describe the literature about these two effects separately in the introduction (pg 2 ln 46-59), and we have added a new section in the discussion (“Foveal feedback during saccade preparation”) that more thoroughly explains why a passive fixation condition would have been unlikely to produce the same results we find (pg 8 ln 211-227). We also adapted the section about “Saccadic remapping or foveal prediction”, clearly delineating foveal prediction from feature remapping and predictive updating of attention pointers. As recommended by the reviewer, we conducted the parametric modulation analyses on the control condition, strengthening the claim that our findings are saccade-related. These results were added as Supplementary Figure 2 and are discussed in (pg 7 ln 190-191) and (pg 8 ln 224-227).</p><p>Lastly, we would like to apologize about a mistake we made with the pre-registration. We realized that the pre-registration had indeed not been submitted. We have now done so without changing the pre-registration itself, which can be seen from the recent activity of the preregistration (screenshot attached in the end). After consulting an open science expert at the University of Leipzig, we added a note of this mistake to the methods section of the revised manuscript (pg 10 ln 326-332). We could remove reference to this preregistration altogether, but would keep it at the discretion of the editor.</p><disp-quote content-type="editor-comment"><p>Specifics:</p><p>(1) In the eccentricity-dependent decoding results (Figure 2B), are there any statistical tests to support the results being a U-shaped curve? The dip isn't especially pronounced. Is 4 degrees lower than the further ones? Are there alternative methods of quantifying this (e.g., fitting it to a linear and quadratic function)?</p></disp-quote><p>We statistically tested the U-shaped relationship using a weighted quadratic regression, which showed significant positive curvature for decoding between fovea and periphery in all early visual areas (V1: t(27) = 3.98, p = 0.008, V2: t(27) = 3.03, p = 0.02, V3: t(27) = 2.776, p = 0.025, one-sided). We now report these results in the revised manuscript (pg 5 ln 137-138).</p><disp-quote content-type="editor-comment"><p>(2) In the parametric modulation analysis, the evidence for IPS being the only region showing stronger fovea vs peripheral beta values was weak, especially given the exploratory nature of this analysis. The raw beta value can reflect other things, such as global brain fluctuations or signal-to-noise ratio. I would also want to see the results of the same analysis performed on the control condition decoding results.</p></disp-quote><p>We appreciate the reviewer’s suggestion and repeated the same parametric modulation analysis on the control condition to assess the influence of potential confounds on the overall beta values (Supplementary Figure 2). The results show a negative association between foveal decoding and FEF and IPS (likely because eye movements in the control condition lead to less foveal presentation of the stimulus) and a positive association with LO. Peripheral decoding was not associated with significant changes in any of the ROIs, indicating that global brain fluctuations alone are not responsible for the effects reported in the experimental condition. The results of this analysis thus show a specific positive association of IPS activity with the experimental condition, not the control condition, which is in line with the idea that the foveal feedback effect reported in this study may be related to saccade preparation.</p><disp-quote content-type="editor-comment"><p>(3) Many of the claims feel overstated. There is an emphasis throughout the manuscript (including claims in the abstract) that these findings demonstrate foveal prediction, specifically that &quot;image-specific feedback during saccade preparation must underlie this effect.&quot; To my understanding, one of the key aspects of the foveal prediction phenomenon that ties it closely to trans-saccadic stability is its specificity to the saccade target but not to other objects in the environment. However, it is not clear to what degree the observed findings are specific to saccade preparation and the peripheral saccade target. Should the observers be asked to make a saccade to another fixation location, or simply maintain passive fixation, will foveal retinotopic cortex similarly contain the object's identity information? Without these control conditions, the results are consistent with foveal prediction, but do not definitively demonstrate that as the cause, so claims need to be toned down.</p></disp-quote><p>We fully agree with the reviewer and toned down claims about foveal prediction. We engage with the questions raised by the reviewer more thoroughly in the new discussion section “Foveal feedback during saccade preparation”.</p><p>In addition, we agree that another condition in which subjects make a saccade towards a different location would have been a great addition that we also considered, but due to concerns with statistical power did not add. While including such a condition exceeds the scope of the current study, we included this limitation in the Discussion section (pg 10 ln 316) and hope that future studies will address this question.</p><disp-quote content-type="editor-comment"><p>(4) Another critical aspect is the temporal locus of the feedback signal. In the paradigm, the authors ensured that the saccade target object was never foveated via the gaze-contingent procedure and a conservative data exclusion criterion, thus enabling the test of feedback signals to foveal retinotopic cortex. However, due to the temporal sluggishness of fMRI BOLD signals, it is unclear when the feedback signal arrives at the foveal retinotopic cortex. In other words, it is possible that the feedback signal arrives after the eyes land at the saccade target location. This possibility is also bolstered by Chambers et al. (2013)'s TMS study, where they found that TMS to the foveal cortex at 350-400 ms SOA interrupts the peripheral discrimination task. The authors should qualify their claims of the results occurring &quot;during saccade preparation&quot; (e.g., pg 1 ln 22) throughout the manuscript, and discuss the importance of temporal dynamics of the effect in supporting stability across saccades.</p></disp-quote><p>We fully agree that the sluggishness of the fMRI signal presents an important challenge in investigating foveal feedback. We have now included this limitation in the discussion (pg 10 ln 306-318). We also clarify that our argument connects to previous studies investigating the temporal dynamics of foveal feedback using similar tasks (pg 10 ln 313-316). Specifically, in their psychophysical work, Kroell and Rolfs (2022) and (2025) showed that foveal feedback occurs before saccade execution with a peak around 80 ms before the eye movement.</p><disp-quote content-type="editor-comment"><p>(5) Relatedly, the claims that result in this paradigm reflect &quot;activity exclusively related to predictive feedback&quot; and &quot;must originate from predictive rather than direct visual processes&quot; (e.g., lines 60-65 and throughout) need to be toned down. The experimental design nicely rules out direct visual foveal stimulation, but predictive feedback is not the only alternative to that. The activation could also reflect mental imagery, visual working memory, attention, etc. Importantly, the experiment uses a block design, where the same exact image is presented multiple times over the block, and the activation is taken for the block as a whole. Thus, while at no point was the image presented at the fovea, there could still be more going on than temporally-specific and saccade-specific predictive feedback.</p></disp-quote><p>We agree that those claims could have misled the reader. Our intention was to state that the activation originates from feedback rather than direct foveal stimulation because of the nature of the design. We have now clarified these statements (pg 2 ln 65) and also included a discussion of other effects including imagery and working memory in the limitations section (pg 10 ln 306-313).</p><disp-quote content-type="editor-comment"><p>(6) The authors should avoid using the terms foveal feedback and foveal prediction interchangeably. To me, foveal feedback refers to the findings of Williams et al. (2008), where participants maintained passive fixation and discriminated objects in the periphery (see also Fan et al., 2016), whereas foveal prediction refers to the neural mechanism hypothesized by Kroell &amp; Rolfs (2022), occurring before a saccade to the target object and contains task irrelevant feature information.</p></disp-quote><p>We agree, and we have now adopted a clearer distinction between these terms, referring to foveal prediction only when discussing the distinct predictive nature of the effect discovered by Kroell and Rolfs (2022). Otherwise we referred to this effect as foveal feedback.</p><disp-quote content-type="editor-comment"><p>(7) More broadly, the treatment of how foveal prediction relates to saccadic remapping is overly simplistic. The authors seem to be taking the perspective that remapping is an attentional phenomenon marked by remapping of only attentional/spatial pointers, but this is not the classic or widely accepted definition of remapping. Within the field of saccadic remapping, it is an ongoing debate whether (/how/where/when) information about stimulus content is remapped alongside spatial location (and also whether the attentional pointer concept is even neurophysiologically viable). This relationship between saccadic remapping and foveal prediction needs clarification and deeper treatment, in both the introduction and discussion.</p></disp-quote><p>We thank the reviewer for their remarks. We reformulated the discussion section on “Saccadic remapping or foveal prediction” to include the nuances about spatial and feature remapping laid out in the reviewer’s comment (pg 8-9 ln 241-269). We also put a stronger focus on the special role the fovea seems to be playing regarding the feedback of visual features (pg 8-9 ln 265-269).</p><disp-quote content-type="editor-comment"><p>(8) As part of this enhanced discussion, the findings should be better integrated with prior studies. E.g., there is some evidence for predictive remapping inducing integration of non-spatial features (some by the authors themselves; Harrison et al., 2013; Szinte et al., 2015). How do these findings relate to the observed results? Can the results simply be a special case of non-spatial feature integration between the currently attended and remapped location (fovea)? How are the results different from neurophysiological evidence for facilitation of the saccade target object's feature across the visual field (Burrow et al., 2014)? How might the results be reconciled with a prior fMRI study that failed to find decoding of stimulus content in remapped responses (Lescroart et al, 2016)? Might this reflect a difference between peripheral-to-peripheral vs peripheral-to-foveal remapping? A recent study by Chiu &amp; Golomb (2025) provided supporting evidence for peripheral-to-fovea remapping (but not peripheral-to-peripheral remapping) of object-location binding (though in the post-saccadic time window), and suggested foveal prediction as the underlying mechanism.</p></disp-quote><p>We thank the reviewer for raising these intriguing questions. We now address them in the revised discussion. We argue that the findings by Harrison et al., 2013 and Szinte et al., 2015 of presaccadic integration of features across two peripheral locations can be explained by presaccadic updating of spatial attention pointers rather than remapping of feature information (pg 8 ln 248-253). The lack of evidence for periphery-to-periphery remapping (Lescroart et al, 2016) and the recent study by Chiu &amp; Golomb (2025) showing object-location binding from periphery to fovea nicely align with our characterization of foveal processing as unique in predicting feature information of upcoming stimuli (pg 8-9 ln 265-269). Finally, we argue that the global (i.e., space-invariant) selection task-irrelevant saccadic target features (Burrows et al., 2014) is well-established at the neural level, but does not suffice to explain the spatially specific nature of foveal prediction (pg 8 ln 220-224). We now include these studies in the revised discussion section.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>Summary:</p><p>In this paper, the authors used fMRI to determine whether peripherally viewed objects could be decoded from the foveal cortex, even when the objects themselves were never viewed foveally. Specifically, they investigated whether pre-saccadic target attributes (shape, semantic category) could be decoded from the foveal cortex. They found that object shape, but not semantic category, could be decoded, providing evidence that foveal feedback relies on low-mid-level information. The authors claim that this provides evidence for a mechanism underlying visual stability and object recognition across saccades.</p><p>Strengths:</p><p>I think this is another nice demonstration that peripheral information can be decoded from / is processed in the foveal cortex - the methods seem appropriate, and the experiments and analyses are carefully conducted, and the main results seem convincing. The paper itself was very clear and well-written.</p></disp-quote><p>We thank the reviewer for this positive evaluation of our work. As discussed in our response to Reviewer 1, we now elaborate on the differences between previous work showing decoding of peripheral information from foveal cortex from the effect shown here. While there are important similarities between these findings, foveal prediction in our study occurs in a saccade condition and in the absence of a task that is specific to stimulus features.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>There are a couple of reasons why I think the main theoretical conclusions drawn from the study might not be supported, and why a more thorough investigation might be needed to draw these conclusions.</p><p>(1) The authors used a blocked design, with each object being shown repeatedly in the same block. This meant that the stimulus was entirely predictable on each block, which weakens the authors' claims about this being a predictive mechanism that facilitates object recognition - if the stimulus is 100% predictable, there is no aspect of recognition or discrimination actually being tested. I think to strengthen these claims, an experiment would need to have unpredictable stimuli, and potentially combine behavioural reports with decoding to see whether this mechanism can be linked to facilitating object recognition across saccades.</p></disp-quote><p>We appreciate the reviewer’s point and would like to highlight that it was not our intention to claim a behavioral effect on object recognition. We believe that an ambiguous formulation in the original abstract may have been interpreted this way, and we thus removed this reference. We also speculated in our Discussion that a potential reason for foveal prediction could be a headstart in peripheral object recognition and in the revised manuscript more clearly highlight that this is a potential future direction only.</p><disp-quote content-type="editor-comment"><p>(2) Given that foveal feedback has been found in previous studies that don't incorporate saccades, how is this a mechanism that might specifically contribute to stability across saccades, rather than just being a general mechanism that aids the processing/discrimination of peripherally-viewed stimuli? I don't think this paper addresses this point, which would seem to be crucial to differentiate the results from those of previous studies.</p></disp-quote><p>We fully agree that this point had not been sufficiently addressed in the previous version of the manuscript. As described in our responses to similar comments from reviewers 1 and 2, we included an additional section in the Discussion (“Foveal feedback during saccade preparation”) to more clearly delineate the present study from previous findings of foveal feedback. Previous studies (Williams et al., 2008) only found foveal feedback during narrow discrimination tasks related to spatial features of the target stimulus, not during color-discrimination or fixation-only tasks, concluding that the observed effect must be related to the discrimination behavior. In contrast, we found foveal feedback (as evidenced by decoding of target features) during a saccade condition that was independent of the target features, suggesting a different role of foveal feedback than hypothesized by Williams et al. (2008).</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>(A) Minor comments:</p><p>(1) The task should be clarified earlier in the manuscript.</p></disp-quote><p>We now characterise the task in the abstract and clarified its description in the third paragraph, right after introducing the main literature.</p><disp-quote content-type="editor-comment"><p>(2) Is there actually only 0.5 seconds between saccades? This feels very short/rushed.</p></disp-quote><p>The inter-trial-interval was 0.5 seconds, though effectively it varied because the target only appeared once participants fixated on the fixation dot. Note that this pacing is slower than the rate of saccades in natural vision (about 3 to 4 saccades per second).Participants did not report this paradigm as rushed.</p><disp-quote content-type="editor-comment"><p>(3) Typo on pg2 ln64 (whooe).</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>(4) Can the authors also show individual data points for Figures 3 and 4?</p></disp-quote><p>We added individual data points for Figures 4 and S2</p><disp-quote content-type="editor-comment"><p>(5) The MNI coordinates on Figure 4A seem to be incorrect.</p></disp-quote><p>We took out those coordinates.</p><disp-quote content-type="editor-comment"><p>(6) Pg4 ln126 and pg6 ln194, why cite Williams et al. (2008)?</p></disp-quote><p>We included this reference here to acknowledge that Williams et al. raised the same issues. We added a “cf.” before this reference to clarify this.</p><disp-quote content-type="editor-comment"><p>(7) Pg7 ln207 Fabius et al. (2020) showed slow post-saccadic feature remapping, rather than predictive remapping of spatial attention.</p></disp-quote><p>We have corrected this mistake.</p><disp-quote content-type="editor-comment"><p>(8) The OSF link is valid, but I couldn't find a pre-registration.</p></disp-quote><p>The issue with the OSF link has been resolved. The pre-registration had been set up but not published. We now published it without changing the original pre-registration (see the screenshot attached).</p><disp-quote content-type="editor-comment"><p>(9) I couldn't access the OpenNeuro repository.</p></disp-quote><p>The issue with the OpenNeuro link has been resolved.</p><disp-quote content-type="editor-comment"><p>(B) Additional references you may wish to include:</p><p>(1) Burrows, B. E., Zirnsak, M., Akhlaghpour, H., Wang, M., &amp; Moore, T. (2014). Global selection of saccadic target features by neurons in area v4. Journal of Neuroscience.</p><p>(2) Chambers, C. D., Allen, C. P., Maizey, L., &amp; Williams, M. A. (2013). Is delayed foveal feedback critical for extra-foveal perception?. Cortex.</p><p>(3) Chiu, T. Y., &amp; Golomb, J. D. (2025). The influence of saccade target status on the reference frame of object-location binding. Journal of Experimental Psychology. General.</p><p>(4) Harrison, W. J., Retell, J. D., Remington, R. W., &amp; Mattingley, J. B. (2013). Visual crowding at a distance during predictive remapping. Current Biology.</p><p>(5) Lescroart, M. D., Kanwisher, N., &amp; Golomb, J. D. (2016). No evidence for automatic remapping of stimulus features or location found with fMRI. Frontiers in Systems Neuroscience.</p><p>(6) Moran, C., Johnson, P. A., Hogendoorn, H., &amp; Landau, A. N. (2025). The representation of stimulus features during stable fixation and active vision. Journal of Neuroscience.</p><p>(7) Szinte, M., Jonikaitis, D., Rolfs, M., Cavanagh, P., &amp; Deubel, H. (2016). Presaccadic motion integration between current and future retinotopic locations of attended objects. Journal of Neurophysiology.</p></disp-quote><p>We thank the reviewer for pointing out these references. We have included them in the revised version of the manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>I just have a few minor points where I think some clarifications could be made.</p><p>(1) Line 64 - &quot;whooe&quot; should be &quot;whoose&quot; I think.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>(2) Around line 53 - you might consider citing this review on foveal feedback - <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/jov.20.12.2">https://doi.org/10.1167/jov.20.12.2</ext-link></p></disp-quote><p>We included the reference (pg 2 ln 55).</p><disp-quote content-type="editor-comment"><p>(3) Line 129 - you mention a u-shaped relationship for decoding - I wasn't quite sure of the significance/relevance of this relationship - it would be helpful to expand on this / clarify what this means.</p></disp-quote><p>We have expanded this section and added statistical tests of the u-shaped relationship in decoding using a weighted quadratic regression. We found significant positive curvature in all early visual areas between fovea and periphery (V1: t(27) = 3.98, p = 0.008, V2: t(27) = 3.03, p = 0.02, V3: t(27) = 2.776, p = 0.025). These findings support a u-shaped relationship. We now report these results in the revised manuscript (pg 5 ln 137-138).</p><disp-quote content-type="editor-comment"><p>(4) Figure 1 - it would be helpful to indicate how long the target was viewed in the &quot;stim on&quot; panels - I assume it was for the saccade latency, but it would be good to include those values in the main text.</p></disp-quote><p>We included that detail in the text (pg 3 ln 96-97).</p></body></sub-article></article>