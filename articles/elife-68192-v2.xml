<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">68192</article-id><article-id pub-id-type="doi">10.7554/eLife.68192</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Unsupervised Bayesian Ising Approximation for decoding neural activity and other biological dictionaries</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-229356"><name><surname>Hernández</surname><given-names>Damián G</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8995-7495</contrib-id><email>damian.g.h.l@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-9067"><name><surname>Sober</surname><given-names>Samuel J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1140-7469</contrib-id><email>samuel.j.sober@emory.edu</email><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-7348"><name><surname>Nemenman</surname><given-names>Ilya</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3024-4244</contrib-id><email>ilya.nemenman@emory.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05m802881</institution-id><institution>Department of Medical Physics, Centro Atómico Bariloche and Instituto Balseiro</institution></institution-wrap><addr-line><named-content content-type="city">Bariloche</named-content></addr-line><country>Argentina</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Department of Physics, Emory University</institution></institution-wrap><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Department of Biology, Emory University</institution></institution-wrap><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Initiative in Theory and Modeling of Living Systems</institution><addr-line><named-content content-type="city">Atlanta</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a26mh11</institution-id><institution>CNRS LPENS</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>22</day><month>03</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e68192</elocation-id><history><date date-type="received" iso-8601-date="2021-03-09"><day>09</day><month>03</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-03-19"><day>19</day><month>03</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2019-11-20"><day>20</day><month>11</month><year>2019</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/849034"/></event></pub-history><permissions><copyright-statement>© 2022, Hernández et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Hernández et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-68192-v2.pdf"/><abstract><p>The problem of deciphering how low-level patterns (action potentials in the brain, amino acids in a protein, etc.) drive high-level biological features (sensorimotor behavior, enzymatic function) represents the central challenge of quantitative biology. The lack of general methods for doing so from the size of datasets that can be collected experimentally severely limits our understanding of the biological world. For example, in neuroscience, some sensory and motor codes have been shown to consist of precisely timed multi-spike patterns. However, the combinatorial complexity of such pattern codes have precluded development of methods for their comprehensive analysis. Thus, just as it is hard to predict a protein’s function based on its sequence, we still do not understand how to accurately predict an organism’s behavior based on neural activity. Here, we introduce the unsupervised Bayesian Ising Approximation (uBIA) for solving this class of problems. We demonstrate its utility in an application to neural data, detecting precisely timed spike patterns that code for specific motor behaviors in a songbird vocal system. In data recorded during singing from neurons in a vocal control region, our method detects such codewords with an arbitrary number of spikes, does so from small data sets, and accounts for dependencies in occurrences of codewords. Detecting such comprehensive motor control dictionaries can improve our understanding of skilled motor control and the neural bases of sensorimotor learning in animals. To further illustrate the utility of uBIA, we used it to identify the distinct sets of activity patterns that encode vocal motor exploration versus typical song production. Crucially, our method can be used not only for analysis of neural systems, but also for understanding the structure of correlations in other biological and nonbiological datasets.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>dictionary reconstruction</kwd><kwd>combinatorial patterns</kwd><kwd>pre-motor activity</kwd><kwd>Bengalese finch</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-EB022872</award-id><principal-award-recipient><name><surname>Hernández</surname><given-names>Damián G</given-names></name><name><surname>Sober</surname><given-names>Samuel J</given-names></name><name><surname>Nemenman</surname><given-names>Ilya</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-NS084844</award-id><principal-award-recipient><name><surname>Sober</surname><given-names>Samuel J</given-names></name><name><surname>Nemenman</surname><given-names>Ilya</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-NS099375</award-id><principal-award-recipient><name><surname>Hernández</surname><given-names>Damián G</given-names></name><name><surname>Sober</surname><given-names>Samuel J</given-names></name><name><surname>Nemenman</surname><given-names>Ilya</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>BCS-1822677 (CRCNS Program)</award-id><principal-award-recipient><name><surname>Sober</surname><given-names>Samuel J</given-names></name><name><surname>Nemenman</surname><given-names>Ilya</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>The Simons-Emory International Consortium on Motor Control</award-id><principal-award-recipient><name><surname>Sober</surname><given-names>Samuel J</given-names></name><name><surname>Nemenman</surname><given-names>Ilya</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>Simons Investigator in MPS</award-id><principal-award-recipient><name><surname>Nemenman</surname><given-names>Ilya</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The proposed method deciphers which low-level patterns (such as spikes) control high-level features (behavior) in relative small biological datasets, taking into account the statistical dependencies between such patterns.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>One of the goals of modern high-throughput biology is to generate predictive models of interaction networks, from interactions among individual biological molecules (<xref ref-type="bibr" rid="bib32">Marks et al., 2011</xref>) to the encoding of information by networks of neurons in the brain (<xref ref-type="bibr" rid="bib52">Schneidman et al., 2006</xref>). To make predictions about activity across networks requires one to accurately approximate—or build a <italic>model</italic> of—their joint probability distribution, such as the distribution of joint firing patterns in neural populations or the distribution of co-occurring mutations in proteins of the same family. To successfully generalize and to improve interpretability, models should contain as few as possible terms. Thus constructing a model requires detecting <italic>relevant</italic> features in the data: namely, the smallest possible set of spike patterns or nucleotide sequences that capture the most correlations among the network components. By analogy with human languages, where words are strongly correlated co-occurring combinations of letters, we refer to the problem of detecting features that succinctly describe correlations in a data set as the problem of <italic>dictionary reconstruction</italic>, as schematized in <xref ref-type="fig" rid="fig1">Figure 1</xref>. It is the first step towards building a model of the underlying data, but it is substantially different (and potentially simpler) than the latter: detecting which features are relevant is not the same as quantifying how they matter in detail.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The dictionary reconstruction problem.</title><p>In many biological systems, such as understanding the neural code, identifying protein-DNA binding sites, or predicting 3-D protein structures, we need to infer dictionaries — the sets of statistically over- or under-represented features in the datasets (relative to some null model), which we refer to as words in the dictionary. To do so, we represent the data as a matrix of binary activities of <inline-formula><mml:math id="inf1"><mml:mi>N</mml:mi></mml:math></inline-formula> biological units (spike/no spike, presence/absence of a mutation, etc.), and view <inline-formula><mml:math id="inf2"><mml:mi>M</mml:mi></mml:math></inline-formula> different experimental instantiations as samples from an underlying stationary probability distribution. We then use the uBIA method to identify the significant words in the data. Specifically, uBIA systematically searches for combinatorial activity patterns that are over- or under-represented compared to their expectation given the marginal activity of the individual units. If multiple similar patterns can (partially) explain the same statistical regularities in the data, they compete with each other for importance, resulting in an irreducible dictionary of significant codewords. In different biological problems, such dictionaries can represent neural control words, DNA binding motifs, or conserved patterns of amino acids that must be neighbors in the 3-D protein structure.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68192-fig1-v2.tif"/></fig><p>In recent years, the problem of dictionary reconstruction has been addressed under different names for a variety of biological contexts (<xref ref-type="bibr" rid="bib36">Natale et al., 2018</xref>) including gene expression networks (<xref ref-type="bibr" rid="bib30">Margolin et al., 2006</xref>; <xref ref-type="bibr" rid="bib27">Lezon et al., 2006</xref>), protein structure, protein-protein interactions (<xref ref-type="bibr" rid="bib32">Marks et al., 2011</xref>; <xref ref-type="bibr" rid="bib35">Morcos et al., 2011</xref>; <xref ref-type="bibr" rid="bib8">Bitbol et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">Halabi et al., 2009</xref>), the structure of regulatory DNA (<xref ref-type="bibr" rid="bib41">Otwinowski and Nemenman, 2013</xref>), distribution of antibodies and pathogenic sequences (<xref ref-type="bibr" rid="bib34">Mora et al., 2010</xref>; <xref ref-type="bibr" rid="bib13">Ferguson et al., 2013</xref>), species abundance (<xref ref-type="bibr" rid="bib63">Tikhonov et al., 2015</xref>), and collective behaviors (<xref ref-type="bibr" rid="bib7">Bialek et al., 2012</xref>; <xref ref-type="bibr" rid="bib9">Couzin and Krause, 2003</xref>; <xref ref-type="bibr" rid="bib28">Lukeman et al., 2010</xref>; <xref ref-type="bibr" rid="bib23">Kelley and Ouellette, 2013</xref>; <xref ref-type="bibr" rid="bib44">Pérez-Escudero and de Polavieja, 2011</xref>). The efforts to identify interactions in neural activity have been particularly plentiful (<xref ref-type="bibr" rid="bib59">Stevens and Zador, 1995</xref>; <xref ref-type="bibr" rid="bib52">Schneidman et al., 2006</xref>; <xref ref-type="bibr" rid="bib45">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib4">Bassett and Sporns, 2017</xref>; <xref ref-type="bibr" rid="bib65">Williams, 2019</xref>). The diversity of biological applications notwithstanding, most of these attempts have relied on similar mathematical constructs, and most have suffered from the same limitations. First, unlike in classical statistics and traditional quantitative model building, where the number of observations, <inline-formula><mml:math id="inf3"><mml:mi>M</mml:mi></mml:math></inline-formula>, usually vastly exceeds the number of unknowns to be estimated, <inline-formula><mml:math id="inf4"><mml:mi>K</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, modern biological data often has <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≫</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, but also <inline-formula><mml:math id="inf7"><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>≫</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Indeed, because of network features such as protein allostery, recurrent connections within neural populations, and coupling to global stimuli, biological systems are rarely limited to local interactions only (<xref ref-type="bibr" rid="bib54">Schwab et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Merchan and Nemenman, 2016</xref>; <xref ref-type="bibr" rid="bib38">Nemenman, 2017</xref>), so that the number of pairwise interactions among <inline-formula><mml:math id="inf8"><mml:mi>N</mml:mi></mml:math></inline-formula> variables is <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>K</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, and the number of all higher order interactions among them is (that is, interactions that involve more than two network variables at the same time) is <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>K</mml:mi><mml:mo>∼</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Put differently, words in biological dictionaries can be of an arbitrary length, and spelling rules may involve many letters simultaneously, some of which are far away from each other. Because of this, reconstruction of biological dictionaries from data sets of realistic sizes requires assumptions and simplifications about the structure of possible biological correlations, and will not be possible by brute force. The second problem is that, as in human languages, biological dictionaries have redundancies: there are synonyms and words that share roots. For example, a set of gene expressions may be correlated not because the genes interact directly, but because they are related to some other genes that do. Similarly, a certain pattern of neural activity may be statistically over- or under-represented (relative to a null model) not on its own, but because it is a subset or a superset of another, more important, pattern. Identifying <italic>irreducible words</italic>—the root forms of biological dictionaries—is therefore harder than detecting all correlations while also being crucial to fully understanding biological systems. Together, these complications make it impossible to use standard methods for reconstructing combinatorially complex dictionaries from datasets of realistic sizes.</p><p>In this work, we propose a new method for reconstructing complex biological dictionaries from relatively small datasets, as few as <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> samples of the joint activity and test it on neural data from songbirds. We focus on the regime <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≫</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, which means <inline-formula><mml:math id="inf13"><mml:mi>N</mml:mi></mml:math></inline-formula> of a few tens for our datasets. While small compared to some of the largest high throughput biological datasets, this regime is relevant in many biological contexts, and especially in studies of motor systems, where recording from multiple single motor units is hard. Crucially, the method imposes no limitation on the structure of the words that can enter the dictionary — neither their length nor their rules of spelling — beyond the obvious limitation that (i) words that do not happen in the data cannot be detected, and (ii) that data contain few samples of many words, rather than of just a few that repeat many times. Additionally, we address the problem of irreducibility, making the inferred dictionaries compact, non-redundant, and easier to comprehend. The main realization that allows this progress is that instead of building an explicit model of the entire joint probability distribution of a system’s states and hence answering <italic>how</italic> specific significant words matter, we can focus on a more restricted, and thus possibly simpler, question: <italic>which</italic> specific words contribute to the dictionary. In other words, unlike many other methods, we do not build an explicit model of the underlying probability distribution, which would allow us to ‘decode’ the meaning of the data, but only detect features that can be used in such models later. We do this using the language of Bayesian inference and statistical mechanics by developing an unsupervised version of the Bayesian Ising Approximation (<xref ref-type="bibr" rid="bib14">Fisher and Mehta, 2015</xref>) and by merging it with the <italic>reliable interactions</italic> model (<xref ref-type="bibr" rid="bib15">Ganmor et al., 2011</xref>).</p><p>We believe that the approach we develop is fully general and will allow analysis of diverse datasets with realistic size requirements, and with few assumptions. However, to illustrate the capabilities of the approach, we present it here in the context of a specific biological system: recordings from single neurons in brain area RA (the robust nucleus of the arcopallium) in the Bengalese finch, a songbird. Neurons communicate with each other using patterns of action potentials (spikes), which encode sensory information and motor commands, and hence behavior. Reconstructing the neural dictionary, and specifically detecting irreducible patterns of neural activity that correlate with (or ‘encode’) sensory stimuli or motor behaviors — which we hereafter call <italic>codewords</italic> — has been a key problem in computational neuroscience for decades (<xref ref-type="bibr" rid="bib59">Stevens and Zador, 1995</xref>). It is now known that in both sensory (<xref ref-type="bibr" rid="bib5">Berry et al., 1997</xref>; <xref ref-type="bibr" rid="bib60">Strong et al., 1998</xref>; <xref ref-type="bibr" rid="bib48">Reinagel and Reid, 2000</xref>; <xref ref-type="bibr" rid="bib2">Arabzadeh et al., 2006</xref>; <xref ref-type="bibr" rid="bib49">Rokem et al., 2006</xref>; <xref ref-type="bibr" rid="bib37">Nemenman et al., 2008</xref>; <xref ref-type="bibr" rid="bib26">Lawhern et al., 2011</xref>; <xref ref-type="bibr" rid="bib12">Fairhall et al., 2012</xref>) and motor systems (<xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>; <xref ref-type="bibr" rid="bib58">Srivastava et al., 2017</xref>; <xref ref-type="bibr" rid="bib57">Sober et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Putney et al., 2019</xref>) the timing of neural action potentials (spikes) in multispike patterns, down to millisecond resolution, can contribute to the encoding of sensory or motor information. Such dictionaries that involve long sequences of neural activities (or incorporate multiple neurons) at high temporal resolution are both more complex and more likely to be severely undersampled. Specifically, even though the songbird datasets considered here are large by neurophysiological standards, they are too small to build their statistical models, which is the goal of most existing analysis approaches. This motivates the general inference problem we address here.</p><p>The power of our approach is illustrated by discoveries in the analysis of the songbird vocal motor code. Specifically, while it is known that various features of the complex vocal behaviors are encoded by millisecond-scale firing patterns (<xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>), here we identify which specific patterns most strongly predict behavioral variations. Further, we show that dictionaries of individual neurons are rather large and quite variable, so that neurons speak different languages, which nonetheless share some universal features. Intriguingly, we detect that codewords that predict large, exploratory deviations in vocal acoustics are statistically different from those that predict typical behaviors. Collectively, these findings pave the way for development of future theories of the structure of these dictionaries, of how they are formed during development, how they adapt to different contexts, and how motor biophysics translates them into specific movements. More importantly, the development of this method and its successful application to neural and behavioral data from songbirds suggests its utility in other biological domains, where reconstruction of feature dictionaries is equally important, and where new discoveries are waiting to be made.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>The dictionary reconstruction problem</title><p>We formalize the dictionary reconstruction problem as follows. An experimental dataset consists of <inline-formula><mml:math id="inf14"><mml:mi>M</mml:mi></mml:math></inline-formula> samples of a vector of binary variables of length <inline-formula><mml:math id="inf15"><mml:mi>N</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf16"><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, which we call <italic>letters</italic> (or <italic>spins</italic>, by analogy with statistical physics). These samples are assumed to be taken from a stationary joint probability distribution <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but the distribution is unknown. From the frequency of occurrence of various combinations of σs in the dataset, we need to detect <italic>words</italic> in the model defined by <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, namely those patterns of σs that are significantly over- or under-represented (statistically anomalous) compared to some null model. While different null models are possible, a common choice is the product of marginal distributions <inline-formula><mml:math id="inf19"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>null</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In this case, words are defined as <italic>correlated</italic> patterns of binary letters. Additionally, to be a part of the dictionary, a word must be <italic>irreducible</italic>. That is, it must be statistically anomalous not only with respect to the null model, but also with respect to its own various parts. For example, a word <inline-formula><mml:math id="inf20"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> all equal to 1 is a word in a dictionary only if it is statistically anomalous with respect to its frequency expected from frequencies of <inline-formula><mml:math id="inf21"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf22"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and also from frequencies of <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf25"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf26"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and so on, eventually accounting for all other words that include any combination of letters <inline-formula><mml:math id="inf27"><mml:msub><mml:mi>σ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf28"><mml:msub><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf29"><mml:msub><mml:mi>σ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula>. In principle, such statistical over- or under-representation of a word has a precise mathematical definition, based on comparing the entropy of the maximum entropy distribution constrained by frequencies of all other words in the distribution to that constrained additionally by the word itself (<xref ref-type="bibr" rid="bib31">Margolin et al., 2010</xref>). For example, three anomalous overrepresented words are shown in the top right panel of <xref ref-type="fig" rid="fig1">Figure 1</xref>. Similarly, an example of anomalous underrepresentaion is shown in light blue in <xref ref-type="fig" rid="fig1">Figure 1</xref>, right middle panel, in which the word 1.1 is underrepresented relative to the frequency expected from the (marginal) frequencies of 1·· and ··1. In practice, performing these many comparisons is impossible for all but the simplest cases. Even approximate analyses, aiming to prove that a method results in irreducible dictionaries under some specific assumptions, have not been very successful to date. As a result, typical methods for dictionary reconstruction are assessed in their ability to build irreducible dictionaries based on heuristics, such as having a low probability of including overlapping words. We will follow the same route here.</p></sec><sec id="s2-2"><title>Songbird neural motor code as a dictionary reconstruction problem</title><p>Owing to their complex and tightly regulated vocal behavior and experimentally accessible nervous system, songbirds provide an ideal model system for investigating the neural dictionaries underlying complex motor behaviors (<xref ref-type="bibr" rid="bib10">Doupe and Kuhl, 1999</xref>; <xref ref-type="bibr" rid="bib25">Kuebrich and Sober, 2015</xref>). We recorded from individual neurons in the motor cortical area RA of Bengalese finches during spontaneous singing and quantified the acoustic ‘features’ of song, specifically the fundamental frequency (which we will refer to as ‘pitch’), amplitude, and spectral entropy of individual vocal gestures, or ‘syllables’, as described previously (<xref ref-type="bibr" rid="bib55">Sober et al., 2008</xref>; <xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>; <xref ref-type="bibr" rid="bib66">Wohlgemuth et al., 2010</xref>). The data sets are sufficiently large to be used as examples of dictionary reconstruction, allowing us to illustrate the type of biological insight that our approach can gain: we have 49 data sets — spanning 4 birds, 30 neurons and sometimes multiple song syllables for each neuron — for which we observed at least 200 instances of the behavior and the associated neural activity, which we estimate below to be the lower threshold for a sufficient statistical power.</p><p>To represent analysis of this motor code as a dictionary reconstruction problem, we binarize the recorded spiking time series so that <inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the absence or presence of a spike in a time slice of <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig2">Figure 2</xref>. Thus each time interval is represented by a binary variable, and interactions among these patterns are described by over-represented or under-represented sequences of zeros and ones in the data. Using a complementary information-theoretic analysis, <xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref> showed that the mutual information between the neural spike train and various features of song acoustics peaks at <inline-formula><mml:math id="inf32"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> ms. Thus, studying precise timing pattern codes means that we focus on <inline-formula><mml:math id="inf33"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> ms (our datasets are not large enough to explore smaller <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>) as discussed previously in <xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>. Detection of statistically significant codewords at this temporal resolution would both re-confirm that this neural motor code is timing based, consistent with previous analyses (<xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>), as well as for the first time reveal the specific patterns that most strongly predict behavior. We focus on neural time series of length <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math></inline-formula> ms duration preceding a certain acoustic syllable, which includes the approximate premotor delay with which neurons and muscles influence behavior (<xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>). Thus the index <inline-formula><mml:math id="inf36"><mml:mi>t</mml:mi></mml:math></inline-formula> runs between 1 and <inline-formula><mml:math id="inf37"><mml:mrow><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Quantification of the neural activity and the behavior.</title><p>A spectrogram of a single song syllable in top-left corner shows the acoustic power (color scale) at different frequencies as a function of time. Each tick mark (bottom-left) represents one spike and each row represents one instantiation of the syllable. We analyze spikes produced in a 40ms premotor window (red box) prior to the time when acoustic features were measured (red arrowhead). These spikes were binarized as 0 (no spike) or 1 (at least one spike) in 2ms bins, totaling 20 time bins. The different acoustic features (pitch, amplitude, spectral entropy) were also binarized. For different analyses in this paper, 0/1 can denote the behavioral feature that is below/above or above/below its median, or as not belonging/belonging to a specific percentile interval. The inset shows the area RA within the song pathway, two synapses away from the vocal muscles, from which these data were recorded.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68192-fig2-v2.tif"/></fig><p>Since we are interested in words that relate neural activity and behavior, we similarly binarize the motor output (<xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>), denoting by 0 or 1 different binary characteristics of the behavior, such as pitch being either below or above its median value, or outside or inside a certain range (<xref ref-type="fig" rid="fig2">Figure 2</xref>). We treat the behavioral variable as the 0’th component of the neuro-behavioral activity, which then has <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:math></inline-formula> binary variables, <inline-formula><mml:math id="inf39"><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. Building the neural-behavioral dictionary is then equivalent to detecting significantly over- or under-represented patterns in the probability distribution <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Focusing specifically on the statistically anomalous words that include the behavioral bit results in detection of <italic>codewords</italic>, for which the neural activity is correlated with (or is predictive of) the behavioral bit. Note that <inline-formula><mml:math id="inf41"><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mn>21</mml:mn></mml:msup><mml:mo>≈</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is much greater than <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mn>100</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> observations of the activity that we can record, illustrating the complexity of the problem. In fact, similar to the famous birthday problem (one gets coinciding birthdays with a lot fewer people than the number of days in a year), one expects a substantial number of repeating samples of the activity of the full length <inline-formula><mml:math id="inf43"><mml:mi>N</mml:mi></mml:math></inline-formula> — and hence the ability to detect statistically over- and under-represented binary words – only when <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∼</mml:mo><mml:msqrt><mml:msup><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:msup></mml:msqrt></mml:mrow></mml:math></inline-formula>, which is what limits the statistical power of any dictionary reconstruction method. Crucially, the approach presented here works by analyzing all patterns, not just patterns of the full length, allowing us to detect anomalous sub-words even in more severely undersampled regimes.</p></sec><sec id="s2-3"><title>The unsupervised BIA method (UBIA) for dictionary reconstruction</title><p>To reconstruct dictionaries in the neural motor code dataset and others with similar properties, we have developed the unsupervised Bayesian Ising Approximation (uBIA) method based on the Bayesian Ising Approximation for detection of significant features in linear Gaussian regression problems (<xref ref-type="bibr" rid="bib14">Fisher and Mehta, 2015</xref>). Specifically, we extend BIA to detect significant interaction terms in probability distributions, rather than in linear regression models. For this, we write the probability distribution <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> without any loss of generality as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mover><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>Z</mml:mi><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≥</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mi>j</mml:mi><mml:mo>≥</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>…</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mo>⋯</mml:mo><mml:mo>×</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>Z</mml:mi><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf46"><mml:mi>Z</mml:mi></mml:math></inline-formula> is the normalization coefficient (<xref ref-type="bibr" rid="bib1">Amari, 2001</xref>). We use the notation such that <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> is a nonempty subset of indexes <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the subset number. Then <inline-formula><mml:math id="inf50"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are coefficients in the log-linear model in front of the corresponding product of binary σs. In other words, <inline-formula><mml:math id="inf51"><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> denotes a specific combination of the behavior and / or times when the neuron is active. If <inline-formula><mml:math id="inf52"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> is nonzero for a term <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (the response variable) is in <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, then this specific spike word is correlated with the motor output, and is a significant codeword in the neural code, see <xref ref-type="fig" rid="fig1">Figure 1</xref>. Finding nonzero <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is then equivalent to identifying <italic>which</italic> codewords matter and should be included in the dictionary in <xref ref-type="fig" rid="fig1">Figure 1</xref>, and inferring the exact values of <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> tells <italic>how</italic> they matter. Notice that <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> makes precise the definition of the order of an interaction, which corresponds to the number of variables <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> in the interaction term.</p><p>A common alternative model of probability distributions uses <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> instead of <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. A third order term coupling, for example, <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents a combination of first, second, and third order terms in the corresponding xs, and vice versa. Thus which words are codewords may depend on the parameterization used, but the longest codewords and nonoverlapping groups of codewords remain the same in both parameterizations. Our choice of σ vs <inline-formula><mml:math id="inf62"><mml:mi>x</mml:mi></mml:math></inline-formula> is for a practical reason: a codeword in the σ basis does not contribute to <inline-formula><mml:math id="inf63"><mml:mi>P</mml:mi></mml:math></inline-formula> unless <italic>all</italic> of its constituent bins are nonzero. Thus since spikes are rare, we do not need to consider contributions of very long words to the code.</p><p>We would like to investigate the neural dictionary systematically and without arbitrarily truncating <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> at some order of interactions or making other strong assumptions about the structure of the words in the dictionary. In fact, this is possibly the biggest distinction of our approach from others in the literature (<xref ref-type="bibr" rid="bib6">Bialek et al., 1991</xref>; <xref ref-type="bibr" rid="bib45">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib52">Schneidman et al., 2006</xref>), which usually start with strong a priori assumptions. However, as discussed above, some assumptions <italic>must</italic> be made to solve the problem for typical data set sizes, and we would like to be very explicit about those we make. To achieve all of this, we define indicator variables <inline-formula><mml:math id="inf64"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which denote if a particular sequence of <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∉</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, ‘matters’ (is a putative word in the dictionary), that is, it is either statistically significantly over- or under-represented in the data set compared to a null model (which we define later). In other words, we rewrite <inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> without any assumptions as:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>μ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We then choose a prior on <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and on <inline-formula><mml:math id="inf72"><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>. We choose to focus on problems where there are many weak words in the dictionary; in other words, typically <inline-formula><mml:math id="inf73"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. We make this choice for two reasons. First, detecting words that are strongly anomalously represented is easy, and does not require a complex statistical apparatus. Second, having many contributing small effects is more realistic biologically. Specifically, for songbird vocal motor control, since many neurons control the muscles and hence the behavioral output, individual spikes in single neuron can only have a very weak effect on the motor behavior. We thus work in the <italic>strong regularization limit</italic> and impose priors<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">,</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that the prior distribution <inline-formula><mml:math id="inf74"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is irrelevant since, for <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> does not contribute to <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>At this point, we need to choose the null model for the occurrence of letter patterns. We do this by choosing <inline-formula><mml:math id="inf78"><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> in a way such that the a priori averages (calculated within the prior only) and the empirical averages (frequencies, observed in the data) of individual <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are equal, <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> (we always use <inline-formula><mml:math id="inf81"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf82"><mml:mover accent="true"><mml:mi mathvariant="normal">…</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> to denote a priori and a posteriori expectations, respectively). This is equivalent to saying that the null model reproduces the firing rate of neurons and the frequency of the behavior, <inline-formula><mml:math id="inf83"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>null</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This is possible to do since, in typical problems, marginal probabilities <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are, indeed, well-known, and it is the higher order interaction terms, the words in the dictionary, that make the reconstruction hard. Finally, we choose the least informative prior <inline-formula><mml:math id="inf85"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, so that a priori a particular word has a fifty-fifty chance of being included in the neural dictionary. If we have reasons to suspect that some words are more or less likely to be included a priori, this probability can be changed.</p><p>Since we are only interested in whether a word is anomalous enough to be in the dictionary, but not in the full model of the joint probability distribution, we integrate out all <inline-formula><mml:math id="inf86"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, after having observed <inline-formula><mml:math id="inf87"><mml:mi>M</mml:mi></mml:math></inline-formula> samples of the <inline-formula><mml:math id="inf88"><mml:mi>N</mml:mi></mml:math></inline-formula> dimensional vector <inline-formula><mml:math id="inf89"><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>. To perform this calculation, we start with the Bayes formula (notice that for the whole set of <inline-formula><mml:math id="inf90"><mml:mi>M</mml:mi></mml:math></inline-formula> samples of the vector <inline-formula><mml:math id="inf91"><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> we use the notation <inline-formula><mml:math id="inf92"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>)<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>Θ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Now we make two approximations. First, we evaluate the integral in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> using the saddle point approximation around the peak of the <italic>prior</italic>, <inline-formula><mml:math id="inf93"><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>. This is a low signal-to-noise limit, and it is different from most high signal-to-noise approaches that analyze the saddle around the peak of the <italic>posterior</italic>. This leads to<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>⊺</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf94"><mml:mi mathvariant="bold">H</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf95"><mml:mi mathvariant="bold">b</mml:mi></mml:math></inline-formula> have size <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:mi>S</mml:mi></mml:math></inline-formula> respectively, being <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>μ</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> being the total number of active variables. Their elements corresponds to<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo fence="true">|</mml:mo></mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd/></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mo>∂</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo fence="true">|</mml:mo></mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf99"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the log-likelihood. Second, we do all calculations as a Taylor series in the small parameter <inline-formula><mml:math id="inf100"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> (see below on the choice of <inline-formula><mml:math id="inf101"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>). Both approximations are facets of the same strong regularization assumption, which insists that most coupling constants <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> are small. Again, the logic here is that we may not have enough information to know what θ is a posteriori, but we should have enough to know if it is nonzero. Following <xref ref-type="bibr" rid="bib14">Fisher and Mehta, 2015</xref>, we obtain<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>Tr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>⊺</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>Tr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>⊺</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi>Hb</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Finally, after explicitly reintroducing We now explicitly reintroduce the indicator variables and by taking into account the both <inline-formula><mml:math id="inf103"><mml:mi mathvariant="bold">H</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf104"><mml:mi mathvariant="bold">b</mml:mi></mml:math></inline-formula> are restricted to the dimensions where <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. That is, for example, the term <inline-formula><mml:math id="inf106"><mml:mrow><mml:msup><mml:mi mathvariant="bold">b</mml:mi><mml:mo>⊺</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:math></inline-formula> corresponds to <inline-formula><mml:math id="inf107"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>μ</mml:mi></mml:msub><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, adding the normalization, we get<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">Z</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>μ</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the magnetic fields (biases) <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and the exchange interactions <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mo>∂</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo fence="true">|</mml:mo></mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd/></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="bold" stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mo>∂</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mo>∂</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo fence="true">|</mml:mo></mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>see ‘Geometric interpretation of uBIA Field’ in ‘Materials and Methods’ for a geometric interpretation of the field.</p><p>Notice that <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> has a familiar pairwise Ising form (<xref ref-type="bibr" rid="bib62">Thompson, 2015</xref>), with data-dependent magnetic fields <inline-formula><mml:math id="inf110"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and the couplings <inline-formula><mml:math id="inf111"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. This Ising model has <inline-formula><mml:math id="inf112"><mml:msup><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:msup></mml:math></inline-formula> spins, replacing the Ising model with <inline-formula><mml:math id="inf113"><mml:mi>N</mml:mi></mml:math></inline-formula> spins, but with higher order interactions in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. Naively, we created a harder problem, with many more variables! However, since most of the <inline-formula><mml:math id="inf114"><mml:msup><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:msup></mml:math></inline-formula> words do not appear in the actual data, and because of the <inline-formula><mml:math id="inf115"><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> in front of the pairwise coupling term, evaluating posterior expectations <inline-formula><mml:math id="inf116"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> for all word that actually occur is relatively easy, as we show now. Indeed, plugging in the model of the probability distribution, <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, we get for the fields and the exchange interactions<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mi>cov</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>ν</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here, to simplify the notation, we defined <inline-formula><mml:math id="inf117"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>≡</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and we remind the reader that <inline-formula><mml:math id="inf118"><mml:mi>M</mml:mi></mml:math></inline-formula> represents the number of samples. Further, angular brackets, <inline-formula><mml:math id="inf119"><mml:mi>cov</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf120"><mml:mi>var</mml:mi></mml:math></inline-formula> denote the a priori expectations, covariances, and variances of frequencies of words in the null model, which matches frequency of occurrence of each individual <inline-formula><mml:math id="inf121"><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (probability of firing in every time bin for the songbird data). Similarly, overlines denote the empirical counts or correlations between co-occurrences of words in the observed data. Specifically, denoting by <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>n</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> the marginal frequencies of the word <inline-formula><mml:math id="inf123"><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> in the data, these expectations and frequencies are defined as follows:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To derive these equations, note that <inline-formula><mml:math id="inf124"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Note also that <inline-formula><mml:math id="inf125"><mml:mrow><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> if the intersection of <inline-formula><mml:math id="inf126"><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:msub><mml:mi>V</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:math></inline-formula> is empty.</p><p><xref ref-type="disp-formula" rid="equ10">Equation 10</xref> has a straightforward interpretation. Specifically, if the difference between the a priori expected frequency and the empirical frequency of a word is statistically significantly nonzero (compared to the a priori standard error), then the corresponding word is anomalously represented. It does not matter whether the word is over- or under-represented: in either case, if the frequency deviates from the expectation, then the field <inline-formula><mml:math id="inf128"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> is positive, biasing the indicator <inline-formula><mml:math id="inf129"><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> toward 1, and hence toward inclusion of the word in the dictionary. If the frequency is as expected, then the field is negative, and the indicator is biased towards 0, excluding the word from the dictionary. Note that as <inline-formula><mml:math id="inf130"><mml:mi>M</mml:mi></mml:math></inline-formula> increases, the standard error goes down, and the field generally increases, allowing us to consider more words. The sign of <inline-formula><mml:math id="inf131"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> would reflect whether the word is over- or underrepresented. However, estimating the exact value of <inline-formula><mml:math id="inf132"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> from small datasets is often impossible and is not our goal, even though, in <xref ref-type="fig" rid="fig2">Figure 2</xref>, we denote words as under- or over-represented by whether their empirical frequency is smaller or larger than the a priori expectation. Thus in some aspects, our approach is similar to the previous work (<xref ref-type="bibr" rid="bib53">Schnitzer and Meister, 2003</xref>), where multi-neuronal patterns are found by comparing empirical firing probabilities to expectations. However, we do this comprehensively for <italic>all</italic> patterns that occur in data. Crucially, in addition, the exchange interactions <inline-formula><mml:math id="inf133"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> also allow us to account for reducibility of the dictionaries.</p><p>To see this, recall that correlations among words create a problem since a word can occur too frequently not in its own right, but either (a) because its sub-words are common, or (b) it is a sub-word of a larger common word, as illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>. In other approaches, resolving these overlaps requires imposing sparsity or other additional constraints. In contrast, the couplings <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> address this problem for uBIA naturally and computationally efficiently. Notice that because of the factor of 2 in the negative term in <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>, the exchange interactions are predominantly negative if one expects the two studied words to be correlated, and if they co-occur in the empirical data as much as they are expected to co-occur in the null model because of the overlaps in their composition, <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf136"><mml:msub><mml:mi>V</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:math></inline-formula>. Negative <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> implement a mechanism, where statistical anomalies in data that can be explained, in principle, by many different <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> are attributed predominantly to one such <inline-formula><mml:math id="inf139"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> that explains them the best, bringing the dictionary closer to the irreducible form. On the other hand, the exchange interactions are positive if one expects correlations between the words a priori, but does not observe them. Thus, in principle, a word can be included in the dictionary even at zero field <inline-formula><mml:math id="inf140"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>. Crucially, every word affects the probability of every other word to be included in the dictionary by means of their corresponding <inline-formula><mml:math id="inf141"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. In this way, while uBIA is not equivalent to the full maximum entropy definition of irreducibility (<xref ref-type="bibr" rid="bib31">Margolin et al., 2010</xref>), it comes close.</p><p>Knowing the coefficients <inline-formula><mml:math id="inf142"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf143"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, one can numerically estimate <inline-formula><mml:math id="inf144"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>, the posterior expectation for including a word <inline-formula><mml:math id="inf145"><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> in the dictionary. Generally, finding such marginal expectations from the joint distribution in disordered systems is a hard problem. However, here <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>∝</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, so that the fields and the interactions create small perturbations around the ‘total ignorance’ solution, <inline-formula><mml:math id="inf148"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (this is a manifestation of our general assumption that none of the words is very easy to detect). Therefore, we calculate the marginal expectation using fast mean field techniques (<xref ref-type="bibr" rid="bib40">Opper and Saad, 2001</xref>). We use the <italic>naive</italic> mean field approximation, which is given by self-consistent equations for the posterior expectations in terms of the magnetizations <inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:msup><mml:mi>tanh</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>ν</mml:mi></mml:munder></mml:mstyle><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>ν</mml:mi></mml:munder></mml:mstyle><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>μ</mml:mi><mml:mtext>eff</mml:mtext></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>so that interactions among spins are encapsulated in an effective field <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>μ</mml:mi><mml:mtext>eff</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>. We solve <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> iteratively (<xref ref-type="bibr" rid="bib14">Fisher and Mehta, 2015</xref>), by increasing <inline-formula><mml:math id="inf151"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> from 0 —that is, from the total ignorance <inline-formula><mml:math id="inf152"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf153"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> — and up to the limiting value <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>ϵ</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:math></inline-formula> in steps of <inline-formula><mml:math id="inf155"><mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This limiting value <inline-formula><mml:math id="inf156"><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>max</mml:mi></mml:msub></mml:math></inline-formula> is determined by the two approximations involved in the strong regularization assumption. First, the saddle point approximation around the peak of the prior in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> implies that the characteristic width of the prior should be smaller than that of the likelihood, <inline-formula><mml:math id="inf157"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Second, the Taylor series up to second order in <inline-formula><mml:math id="inf158"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> for the posterior of the indicator variables implies that the quadratic corrections should not be larger than the linear terms. Within the mean field approximation, this means that <inline-formula><mml:math id="inf159"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>μ</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>μ</mml:mi><mml:mtext>eff</mml:mtext></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϵ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which is saturated at some <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> (notice that, in contrast to our usual notation, the averages here are over the indices, and not the data). Thus, overall we take <inline-formula><mml:math id="inf161"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext>min</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In other words, we use the largest <inline-formula><mml:math id="inf162"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> (the weakest possible regularization), which is still consistent with the strong regularization limit. Additionally we have used the TAP equations (<xref ref-type="bibr" rid="bib40">Opper and Saad, 2001</xref>), instead of <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> to calculate magnetizations. These are more accurate since they account for how a spin affects itself through its couplings with the other spins. However, corrections due to this more complicated method were observed to be negligible in our strong regularized regime, since they were of higher order in <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Thus, all results that we report here, and the software implementation of uBIA on <ext-link ext-link-type="uri" xlink:href="https://github.com/dghernandez/decomotor">GitHub</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:72fe5ae0cf1ae9883ee2cf60b634fe81cc86eb0d;origin=https://github.com/dghernandez/decomotor;visit=swh:1:snp:c8680421bc03b5ff09d2d14d1823f09a546b3545;anchor=swh:1:rev:a374c0d478958aaf38415c7b616bbdebe83c6219">swh:1:rev:a374c0d478958aaf38415c7b616bbdebe83c6219</ext-link>) are based on the mean field estimation.</p><p>Note that the analysis above, and our GitHub code, only focused on words that appear in the data. However, most of the <inline-formula><mml:math id="inf164"><mml:msup><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:msup></mml:math></inline-formula> possible words must be absent from any realistic dataset. In the <italic>Supplementary Materials</italic>, we show that neglecting these words when calculating posterior probabilities for word inclusion does not lead to significant errors.</p></sec><sec id="s2-4"><title>Testing and fine-tuning uBIA on synthetic data</title><p>To verify that uBIA can, indeed, recover dictionaries and to set various adjustable parameters involved in the method, we tested the approach on synthetic data that are statistically similar to those that we expect to encounter in real-world applications, such as our neural recordings. We used the log-linear model, <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, as a generative model for binary correlated observables <inline-formula><mml:math id="inf165"><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> with <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>. While somewhat small compared to many state of the art experimental datasets, this choice of <inline-formula><mml:math id="inf167"><mml:mi>N</mml:mi></mml:math></inline-formula> is highly relevant to the motor control studies, which are our primary target in this work. We chose the individual biases in the generative model from a normal distribution, <inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.7</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mn>0.1</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which matched the observed probability of a spike in a bin in the bird data. That is, <inline-formula><mml:math id="inf169"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≃</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mi>q</mml:mi><mml:mo>∼</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>. Then we selected which binary variables interacted. We allowed interactions of 2nd, 3rd, and 4th order, with an equal number of interactions per order. For different tests, we chose the interaction strengths from (a) the sum of two Gaussian distributions, one with a positive mean and the other with a negative one, <inline-formula><mml:math id="inf170"><mml:mrow><mml:mrow><mml:mi>mean</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf171"><mml:mrow><mml:mrow><mml:mi>std</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, and (b) from one Gaussian distribution centered at zero with <inline-formula><mml:math id="inf172"><mml:mrow><mml:mrow><mml:mi>std</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. Both choices reflect our strong regularization assumption, so that effects of individual variables on each other are weak, and a state of one variable does not determine the state of the others, and hence does not ‘freeze’ the system. We were specifically interested in performance of the algorithm in the case where <inline-formula><mml:math id="inf173"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> are distributed as the sum of Gaussians. On the one hand, this tested how the algorithm deals with data that are atypical within its own assumptions. On the other hand, this choice ensured that there were fewer values of <inline-formula><mml:math id="inf174"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> that were statistically indistinguishable from zero, making it easier to quantify findings of the algorithm as either true or false. We have additionally tested other distributions of <inline-formula><mml:math id="inf175"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>, but no new qualitatively different behaviors were observed. Finally, for both types of distributions of <inline-formula><mml:math id="inf176"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>, we also varied the density of interactions α (number of interactions per spin), from <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, which spans the interaction densities of tree-like and 2D lattice-like networks. We generated <inline-formula><mml:math id="inf179"><mml:mi>M</mml:mi></mml:math></inline-formula> samples from these random probability distribution and we applied our pipeline to reconstruct the dictionary. We tested on 400 distributions from each family. As the first step, we discarded high-order words absent in the data using a threshold on the expected number of occurrences <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Next, we selected <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>N</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:math></inline-formula> words that have the highest (absolute) values in magnetic field <inline-formula><mml:math id="inf182"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> (we have tested <inline-formula><mml:math id="inf183"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>200</mml:mn><mml:mo>,</mml:mo><mml:mn> 500</mml:mn><mml:mo>,</mml:mo><mml:mn> 2000</mml:mn><mml:mo>,</mml:mo><mml:mn> 5000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and finally used 500 after not observing substantial differences). To decide which of these high-field words are to be included in the dictionary, we built the Ising model on the indicator variables, <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>, with its corresponding magnetizations <inline-formula><mml:math id="inf184"><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> given by the mean field equations. We started from an inverse regularization strength of <inline-formula><mml:math id="inf185"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and then decreased the regularization strength by increasing <inline-formula><mml:math id="inf186"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> in steps of <inline-formula><mml:math id="inf187"><mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>20</mml:mn><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, up to <inline-formula><mml:math id="inf188"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext>min</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, as detailed above.</p><p>Next we needed to identify the significance threshold for the magnetization <inline-formula><mml:math id="inf189"><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, or, equivalently, for the posterior probability of including a word into the dictionary <inline-formula><mml:math id="inf190"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. As is often the case, this is affected by two contradictory criteria. Setting the threshold high would miss a lot of weakly significant words (high false negatives), but the words remaining in the dictionary would be likelier to be true (low false positives). In contrast, setting the threshold low would admit weakly significant words (lowering false negatives) at the cost of also admitting words by chance (increasing false positives). To measure false positives and negatives, we used two metrics: precision and recall. The precision, ξ, is the fraction of the words included in the dictionary that are true, that is, have a nonzero <inline-formula><mml:math id="inf191"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> in the generated true model. The recall, η, is the fraction of the words in the generated model with <inline-formula><mml:math id="inf192"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> that were included in the dictionary. Fundamentally, there are no first principles arguments for the choice of the magnetization inclusion threshold, and thus we explored many different values of <inline-formula><mml:math id="inf193"><mml:mi>m</mml:mi></mml:math></inline-formula> and infer the functions <inline-formula><mml:math id="inf194"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for every data set explored. In <xref ref-type="fig" rid="fig3">Figure 3</xref>, we plot the precision vs. recall curves parametrically as a function of <inline-formula><mml:math id="inf196"><mml:mi>m</mml:mi></mml:math></inline-formula>. We see that, as the amount of data increases, the recall generally increases, though it remains small. However, since data set sizes are relatively small, we do not expect to detect all words, especially in the case where <inline-formula><mml:math id="inf197"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> are allowed to be close to 0 in the generative model (Gaussian distributed). Thus we emphasize precision over recall in setting parameters of the algorithm: we are willing to not include words in a dictionary, but those words that we include should have a high probability of being true words in the underlying model. It is thus encouraging that, in all tested cases, there was an underlying magnetization threshold that allowed for a high (e.g. 80%) precision to ensure that almost all of the words that we detected can be trusted to be true. Crucially, we see that the precision-recall curves are remarkably stable with the changing density of interactions. As a final point for interpreting these figures, we point out that η is smaller when interactions coefficients are taken from a Gaussian centered at zero. However, one could argue that missing words with very small <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> should not be considered a mistake: they are not significant words in the studied dictionary.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Results of the synthetic data analysis.</title><p>Performance on synthetic data as a function of the density of interactions α, the distributions <inline-formula><mml:math id="inf199"><mml:mrow><mml:mi>p</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the strength of interactions, and the number of samples <inline-formula><mml:math id="inf200"><mml:mrow><mml:mi>M</mml:mi><mml:mo mathvariant="normal">∈</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">{</mml:mo><mml:mn mathvariant="normal">200</mml:mn><mml:mo mathvariant="normal">,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mn mathvariant="normal">1600</mml:mn><mml:mo mathvariant="normal" stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (logarithmically spaced). The first and the second columns correspond to precision-recall curves for the different density of interactions (significant words) per variable, <inline-formula><mml:math id="inf201"><mml:mrow><mml:mi>α</mml:mi><mml:mo mathvariant="normal">∈</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">{</mml:mo><mml:mn mathvariant="normal">2</mml:mn><mml:mo mathvariant="normal">,</mml:mo><mml:mn mathvariant="normal">4</mml:mn><mml:mo mathvariant="normal" stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, within the true generative model. The top and the bottom rows corresponds to the interaction strengths <inline-formula><mml:math id="inf202"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo mathvariant="normal" stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> selected from the sum of two Gaussian distributions, or a single Gaussian, as described in the text. For the first two columns, we vary the significance threshold in marginal magnetization <inline-formula><mml:math id="inf203"><mml:mrow><mml:mi>m</mml:mi><mml:mo mathvariant="normal">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi mathvariant="normal">false</mml:mi></mml:msub><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, such that the full false discovery rate on the shuffled data <inline-formula><mml:math id="inf204"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi mathvariant="normal">false</mml:mi></mml:msub><mml:mo mathvariant="normal">∈</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">[</mml:mo><mml:mn mathvariant="normal">0.005</mml:mn><mml:mo mathvariant="normal">,</mml:mo><mml:mn mathvariant="normal">40</mml:mn><mml:mo mathvariant="normal" stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In the third column we show the value of <inline-formula><mml:math id="inf205"><mml:msub><mml:mi>n</mml:mi><mml:mi mathvariant="normal">false</mml:mi></mml:msub></mml:math></inline-formula> that corresponds to the precision of 80% as a function of <inline-formula><mml:math id="inf206"><mml:mi>M</mml:mi></mml:math></inline-formula> (the number of samples), so that the precision is larger than in the shaded region. This region is quite large and overlaps considerably for the four cases analyzed, illustrating robustness of the method.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68192-fig3-v2.tif"/></fig><p>An additional way of measuring the accuracy of our approach is by exploring the full false discovery rate <italic>n</italic><sub><italic>false</italic></sub> — the total number of dictionary words that are false positives, averaged over our usual 400 realizations of the training samples — produced by our algorithm on fully reshuffled data, where every identified word is false, by definition. We did this with reshuffling that kept the observed frequency of individual variables <italic>n</italic><sub><italic>i</italic></sub> constant. We mapped out computationally the relation <inline-formula><mml:math id="inf207"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>false</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which, together with <inline-formula><mml:math id="inf208"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> explored above, allowed to explore the dependence between ξ and <inline-formula><mml:math id="inf209"><mml:mi>m</mml:mi></mml:math></inline-formula>. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows that <inline-formula><mml:math id="inf210"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>80</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for every data set that we have tried. Specifically, by keeping <italic>n</italic><sub><italic>false</italic></sub> below 0.5 (only about half a word detected falsely, on average, in shuffled data), we can reach a precision as high as 80%, with the recall of 20% - 30% of the codewords depending on the number of samples, the distribution of <inline-formula><mml:math id="inf212"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>, and α. This shows that the findings of our approach are robust to variation of many parameters.</p><p>For the rest of this work, we set the magnetization threshold as a function of the false discovery rate, and we will admit words to the dictionary only when they have their marginal magnetizations <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. With that, we are confident that our method produces dictionaries, in which a substantial fraction of words correspond to true words in the data, though the details of how many may depend on various (often unknown) properties of the data themselves, including with respect to patterns that are possible yet were never observed empirically.</p><p>To quantify the effect of interactions in among words in shaping the final dictionary, we check how many words with a field larger than the smallest field of a putative codeword corresponding to the kept codewords were discarded. Crucially, of such words with large magnetic field were We observe that <inline-formula><mml:math id="inf214"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:mn>40</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of all such large-field words are removed from the final dictionary due to the word-word exchange interactions. This signifies that uBIA works as designed in identifying multiple words that can explain a statistical pattern and choosing the smallest subset of words able to explain it.</p><p>We finish this section with the following observation about the performance of uBIA in regimes that are even more severely undersampled than considered above, so that most of relatively long words only happen once or never in the data. In this regime, uBIA has two major strengths. First, uBIA analyzes putative codewords of arbitrary length, so that then it will detect short sub-patterns as codewords – and, in any reasonable dataset, at least some short sub-patterns will coincide – for example, there are only <inline-formula><mml:math id="inf215"><mml:mi>N</mml:mi></mml:math></inline-formula> first order words formed by <inline-formula><mml:math id="inf216"><mml:mi>N</mml:mi></mml:math></inline-formula> interacting units, and typically <inline-formula><mml:math id="inf217"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≫</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. Second, uBIA detects not just over-representation of patterns, but also their under-representation. Thus, if a complex spike word happens once or does not happen at all, it may enter the uBIA dictionary as well precisely because it happens so infrequently.</p></sec><sec id="s2-5"><title>Reconstructing songbird neural motor codes</title><p>Having fine-tuned the method on synthetic data, we can now test it on a biological dataset. We applied uBIA to the songbird data, confirming the precise timing pattern nature of the code in this dataset. We then demonstrate the generality of the algorithm by applying it to different parameterizations of the data, which allows us to make surprising observations about control of exploratory vs typical renditions of the song by the birds. Notice that, for all applications below, we have to binarize the behavior (pitch). This inevitably results in the loss of resolution and the corresponding loss of codewords. However, such binarization is meaningful in the context of songbirds (<xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>), and, crucially, it cannot lead to emergence of keywords where they would not exist otherwise.</p><sec id="s2-5-1"><title>Statistical properties of the motor codes</title><p>We start with <xref ref-type="fig" rid="fig4">Figure 4</xref>, which explores the occurence of just two specific codewords found by uBIA that encode high-pitch renditions of syllables. Note that these codewords are, indeed, overrepresented together with the high pitch vocalizations. Analyzing if a particular word is correlated with an acoustic feature is, of course, not hard. However, detecting words that should be tested, without a multiple hypothesis testing significance penalty is nontrivial. Thus the power of uBIA comes from being able to systematically analyze abundances of <italic>combinatorially many</italic> such spike patterns, and further to identify which of them are <italic>irreducibly</italic> over- or under-represented. <xref ref-type="fig" rid="fig5">Figure 5</xref> illustrates statistical properties of entire neural-behavior dictionaries discovered by uBIA for different songbird premotor neurons and for three features of the acoustic behavior. While we reconstruct the dictionaries that include all irreducible words, including those that have only anomalous firing patterns but a statistically normal behavioral bit, here we primarily focus on codewords, which, recall, are defined as statistically anomalous relations between the behavior and the neural activity. We do the analysis twice, first for behavior binarized as <inline-formula><mml:math id="inf218"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for the above-median acoustic features, and then for the below-median acoustic features. This way we detect words that predict either a behavior or its opposite. We do this because the same pattern of spikes should not be anomalous in the same way simultaneously when studying both the above and the below median codes, since the pattern cannot code for two mutually exclusive features. Detecting such patterns thus serves as a consistency check on our method. There were 0.7 such codewords on average per dictionary. This is consistent with the expected false discovery rate of <italic>lt</italic><sub>1</sub> codewords per neuron for data sets of our size and statistical properties, further increasing our confidence in the method.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Sample multispike codewords.</title><p>(<bold>A</bold>) Probability distribution an acoustic parameter (fundamental frequency, or pitch). For this analysis, we consider the output to be <inline-formula><mml:math id="inf219"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:msub><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:math></inline-formula> when the pitch is above median (blue), and zero otherwise (red). (<bold>B</bold>) Distribution of two sample codewords (a two-spike word in the left raster, and a three-spike word in the right raster) conditional on pitch. In each raster plot, a row represents 40ms of the spiking activity preceding the syllable, with a grey tick denoting a spike. Every time a particular pattern is observed, its ticks are plotted in black. Note that these two spike words are codewords since they are overrepresented for above-median pitch (blue box) compared both to the null model based on the marginal expectation of individual spikes, and to the presence of the patterns in the low pitch region. Labels (a) and (b) identify these patterns in <xref ref-type="fig" rid="fig5">Figure 5B</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68192-fig4-v2.tif"/></fig><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Statistical properties of neural dictionaries.</title><p>(<bold>A</bold>) Sample neural-behavioral dictionaries for two neurons from two different birds (columns) and for three different acoustic features of the song (rows: pitch, amplitude, and the spectral entropy). The light gray curve in the background and the vertical axis corresponds to the probability of neural firing in each 2ms bin (the firing rate). The rectangular tics represents the timing of spikes in neural words that predict the acoustic features. For example, a two spike word with tics at points <inline-formula><mml:math id="inf220"><mml:mrow><mml:mi>t</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to the probability that the word <inline-formula><mml:math id="inf221"><mml:mrow><mml:mi>μ</mml:mi><mml:mo mathvariant="normal">=</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo mathvariant="normal">,</mml:mo><mml:mi>j</mml:mi><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a codeword for the acoustic feature with a probability statistically significantly higher than 1⁄2. Codewords for high (low) output, that is, <inline-formula><mml:math id="inf222"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn mathvariant="normal">0</mml:mn></mml:msub><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow></mml:math></inline-formula> above (below) the median, are shown in blue (red). Full (empty) symbols correspond to over(under)-occurrence of the codeword-behavior combinations compared to the null model. Finally full (empty) black symbols represent words that over(under)-occur in the blue code and under(over)-occur in the red code. Words labeled (c)-(g) are also shown in (<bold>B</bold>). (<bold>B</bold>) Frequency of occurrence of statistically significant codewords for different acoustic features in different neurons. Only first 200 codewords shown for clarity. Plotting conventions same as in (<bold>A</bold>), and letters label the same codewords as in (<bold>A</bold>) and in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. (<bold>C</bold>) Proportion of <inline-formula><mml:math id="inf223"><mml:mi>m</mml:mi></mml:math></inline-formula>-spike codewords found in the dictionaries analyzed. An <inline-formula><mml:math id="inf224"><mml:mi>m</mml:mi></mml:math></inline-formula>-spike word corresponds to an <inline-formula><mml:math id="inf225"><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo mathvariant="normal">+</mml:mo><mml:mn mathvariant="normal">1</mml:mn></mml:mrow><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-dimensional word in the neural-behavioral dictionary. Most of the significant codewords have two or more spikes in them. (<bold>D</bold>) Mean number of significant codewords, averaged across all neurons and acoustic features. An average neuron has 5.6 codewords in our dataset, of which 3.1 code for the pitch, 2.5 for the amplitude, and 2.8 for the spectral entropy, with the number of words coding for pairs of features or for all three of them indicated by the overlap of rectangles in the Venn diagram. For comparison, our estimated false discovery rate is 0.3 words, so that only ∼0.3 spurious words are expected to be discovered in each individual dictionary. We note that about a third of all analyzed dictionaries are empty, so that those that have words in them typically have more than illustrated here. (<bold>E</bold>) Mean inter-spike interval (ISI) for the codewords (spike words that code for behavior) vs. all spike words that are significantly over- or under-represented, but do not code for behavior. Averages in each of the four analyzed birds are shown, illustrating that the ISI statistics of the coding and non-coding words are different, but the differences themselves vary across the birds. Star denotes 95% confidence. Other properties of the dictionaries (mean number of spikes in codewords, fraction of codewords shared by three vocal features, proportion of under/over-occurring codewords), do not differ statistically significantly across the birds.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68192-fig5-v2.tif"/></fig><p>The most salient observation is that the inferred codewords consist of present or absent spikes in specific 2ms time bins, (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). This is consistent with our previous analysis (<xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>), which identified the same timescale for this dataset by analyzing the dependence of the mutual information between the activity and the behavior on the temporal resolution, but was unable to detect the specific words that carry the information. The second crucial observation is that most of codewords are composed of multiple spikes, (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) representing an orthographically complex <italic>pattern</italic> timing code (<xref ref-type="bibr" rid="bib57">Sober et al., 2018</xref>), in contrast to single spike timing codes, such as in <xref ref-type="bibr" rid="bib6">Bialek et al., 1991</xref>. Large number of codewords of 2 or more spikes (and thus 3 or more features, including the behavior itself) suggests that analyzing these dictionaries with the widely used lower order MaxEnt or GLM methods that typically focus on lower-order statistics (see <italic>Online Methods</italic>) would miss their significant components. Our third crucial observation is that very few sub-words / super-words pairs occur in the dictionaries (<xref ref-type="fig" rid="fig5">Figure 5A</xref>; e.g. the second codeword coding for entropy in neuron 2 in the panel A is a subword of the others). Finally, similarly to our synthetic data analysis, 30% of words with large magnetic field were removed from the final dictionary due to word-word interactions. This indicates that uBIA fulfills its goal of rejecting multiple correlated explanations for the same data.</p><p>We quantify these observations as follows. In the 49 different datasets, the average size of a dictionary within one dataset is 14 words. Of these words, on average 5.6 include the behavioral feature and hence are <italic>codewords</italic> (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). That there are so many specific temporally precise codewords suggests that the behaviorally-relevant spike timing patterns are the rule, rather than the exception, in this dataset. We found that 66% of codewords are unique to one of the three analyzed acoustic features. This further quantifies the observation that some neurons in RA are <italic>selective</italic> for specific acoustic features, as noted previously (<xref ref-type="bibr" rid="bib55">Sober et al., 2008</xref>). Across all neurons and all acoustic features, only 15% of codewords consist of a single spike (or absence of spike), while 58%, 23%, and 4% consist of two, three, and four spikes respectively, (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) (we are likely missing many long codewords, especially with small θ’s due to undersampling). This observation is consistent across all neurons and acoustic features, again indicating that coding by temporally precise spike patterns is a rule and not an exception.</p><p>At the same time, the observed dictionaries are quite variable across neurons and the production of particular song syllables. Codewords are built by stitching together multiple spikes or spike absences, and individual spikes occur at certain time points in the (-40,0) ms window with different probabilities in different neurons and syllables (i.e. the firing rate is both time and neuron dependent, <xref ref-type="fig" rid="fig5">Figure 5A</xref>, grey lines). Codewords are likely to occur where the probability of seeing a spike in a bin is ∼50%, since these are the times that have more capacity to transmit information. Thus variability in firing rates as a function of time across neurons necessarily creates variability in the dictionaries across these neurons. Beyond this, we observe additional variability among the dictionaries that is <italic>not</italic> explained by the rate fluctuations. For example, we can differentiate one of the four birds from two of the others just by looking at the proportions of high-order codewords (an average of 0.21 bits in Jensen-Shannon divergence between the target bird and the rest, which means that we need around five independent samples/codewords to distinguish this bird from the others). This is further corroborated by the fact that the mean inter-spike interval (ISI) for codewords is different from that of other words in the dictionaries, and this difference is also bird-dependent, see <xref ref-type="fig" rid="fig5">Figure 5E</xref>.</p></sec><sec id="s2-5-2"><title>Verification validation of the inferred dictionaries</title><p>To show that the dictionaries we decoded are biologically (and not just statistically) significant, we verify whether Statistical significance is not a substitute for biological significance. The only way to interpret findings of any statistical method, including ours, is through perturbation experiments. For example, one could try to see if a stimulation of a neuron with a specific codeword-like patterns would <italic>cause</italic> (rather than merely correlate with) a certain behavior (<xref ref-type="bibr" rid="bib58">Srivastava et al., 2017</xref>). Unable to do this, we do a weaker validation and check if the codewords can, in fact, be used to predict the behavioral features. For this, we built two logistic regression models that relate the neural activity to behavior. The first one uses the presence / absence of spikes in individual time bins and the second the presence / absence of the uBIA detected codewords as predictor variables (see <italic>Online Methods</italic>). Note that the individual spikes model is still a precise-timing model, which has 20 predictors (20 time bins, each 2 ms long), and hence one may expect it to predict better than the codewords model, which typically has many fewer predictors. To account for the possibility of overfitting, in all comparisons we test the predictive power of models using cross-validation. We emphasize that we do not expect either of the two models to capture an especially large fraction of the behavioral variation. Indeed, <xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref> have shown that, at 2ms resolution, on average, there is only about 0.12 bits of information between the activity of an individual neuron and the behavioral features, and the assumption behind our entire approach is that none of individual predictors have strong effects. Further, a specific model, such as logistic regression, will likely recover even less predictive power from the data. With this, Figure 9 compares prediction between the two models, obtaining a significantly higher accuracy and a lower mean cross-entropy between the model and the data for the models that use codewords as predictors. In other words, the higher order, but smaller, dictionaries decoded by uBIA outperform larger, non-specific dictionaries in predicting behavior. This is especially significant since uBIA codewords are detected for their statistical anomaly and irreducibility, and not directly for how accurately they predict behavior.</p></sec><sec id="s2-5-3"><title>Dictionaries for exploratory vs. typical behaviors</title><p>Bengalese finches retain the ability to learn through their lifetimes, updating their vocalizations based on sensorimotor feedback (<xref ref-type="bibr" rid="bib25">Kuebrich and Sober, 2015</xref>; <xref ref-type="bibr" rid="bib24">Kelly and Sober, 2014</xref>; <xref ref-type="bibr" rid="bib56">Sober and Brainard, 2009</xref>; <xref ref-type="bibr" rid="bib50">Saravanan et al., 2019</xref>). A key element of this lifelong learning capacity is the precise regulation of vocal variability, which songbirds use to explore the space of possible motor outputs, (<xref ref-type="fig" rid="fig6">Figure 6A and B</xref>). For example, male songbirds minimize variability when singing to females during courtship, but dramatically increase the range of variability in acoustic features such as pitch when singing alone (<xref ref-type="bibr" rid="bib21">Hessler and Doupe, 1999</xref>; <xref ref-type="bibr" rid="bib67">Woolley et al., 2014</xref>). The variability is controlled by the activity of nucleus LMAN. Silencing or lesioning LMAN reduces the acoustic variance of undirected song (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) to a level approximately equal to that of female-directed song (<xref ref-type="bibr" rid="bib22">Kao et al., 2005</xref>; <xref ref-type="bibr" rid="bib39">Olveczky et al., 2005</xref>). Using uBIA, we can ask for the first time whether the statistics of codewords controlling the exploratory vs. the baseline range of motor variability are different. To do this, we analyze the statistics of codewords representing different parts of the pitch distribution. First, we define the output as <inline-formula><mml:math id="inf226"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if the behavior belongs to a specific 20-percentile interval ([0 -20], [10 - 30], …, [80 -100] ) and compare the dictionaries that code for behavior in each of the intervals. We find that there are significantly more codewords for exploratory behaviors (percentile intervals farthest from the median, <xref ref-type="fig" rid="fig6">Figure 6C</xref>). This holds true for different features of the vocal output, although the results are only statistically significant if pooled over all features. To improve statistical power by increasing the number of trails in each acoustic interval, we also consider a division of the output into three equal intervals: low, medium, and high. In this case, there are still more codewords for the high exploratory pitch, and the dictionaries for each of the intervals are still multispike (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). We further observe that the codewords themselves are different for the three percentile groups: the mean ISI of high pitch, amplitude, and spectral entropy codewords is higher, with the largest effect coming from the pitch and the spectral entropy (<xref ref-type="fig" rid="fig6">Figure 6E</xref>). Examples of typical and exploratory dictionaries are illustrated in <xref ref-type="fig" rid="fig6">Figure 6F</xref>. Note that this analysis partially addresses the concern about losing resolution due to discretization of behavior by exploring effects of different discretizations on the reconstructed dictionaries.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Codes for vocal motor exploration.</title><p>(<bold>A</bold>) Distribution of syllable pitch relative to the mean for exploratory and performance behaviors (blue, intact birds, vs. grey, LMAN-lesioned animals, see main text). (<bold>B</bold>) Ratio of the histograms in (<bold>A</bold>) evaluated in the quintiles of the exploratory (blue) distribution centered around [10%,20,...90%] points. (<bold>C</bold>) Total number of codewords when considering the vocal output as 1 if it belongs to a specific 20-percentile interval of the output distribution, and 0 otherwise. We observe that there are significantly more codewords for the exploratory behavior (tails of the distribution compared to the middle intervals). Notice that the shape of the curves parallels that in (<bold>B</bold>), suggesting that exploration drives the diversity of the codewords. (<bold>D</bold>) Number of codewords when considering the vocal output as 1 if it belongs to a 33-percentile (non-overlapping) interval of the output distribution, and 0 otherwise. Here there are significantly more codewords when coding for high pitch. Further, the codewords found for each of the three intervals are mostly multi-spike (histograms show the distribution of the order of the codewords for each percentile interval). (<bold>E</bold>) For codewords for the 33-percentile intervals, we compare the mean inter-spike intervals (ISIs). Codewords for high outputs (especially for pitch and spectral entropy) have a significantly larger mean ISI. (<bold>F</bold>) We illustrate dictionaries of two neurons for the medium and the high spectral entropy ranges. Notice that the high entropy range has significantly more codewords.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68192-fig6-v2.tif"/></fig><p>These findings challenge common accounts of motor variability, in songbirds and other systems, that motor exploration is induced by adding random spiking variations to a baseline motor program. Rather, the over-abundance of codewords in the exploratory flanks of the acoustic distributions indicates that the mapping between the neural activity and the behavior is more reliable than in the bulk of the behavioral activity: multiple renditions of the same neural command result in the same behaviors more frequently, making it easier to detect the codewords. One possibility is that the motor system is less biomechanically noisy for large behavioral deviations. This is unlikely due to the tremendous variation in the acoustic structure (pitch, etc.) of different song syllables within and across animals (<xref ref-type="bibr" rid="bib56">Sober and Brainard, 2009</xref>; <xref ref-type="bibr" rid="bib11">Elemans et al., 2015</xref>), which indicates that songbirds can produce a wide range of sounds and that particular pitches (i.e. those at at one syllable’s exploratory tail) are not intrinsically different or harder for an animal to produce. Similarly, songbirds can dramatically modify syllable pitch in response to manipulations of auditory feedback (<xref ref-type="bibr" rid="bib56">Sober and Brainard, 2009</xref>; <xref ref-type="bibr" rid="bib25">Kuebrich and Sober, 2015</xref>). A more likely explanation for the greater prevalence of codewords in the exploratory tails is that the nervous system drives motor exploration by selectively introducing particular patterns into motor commands that are specifically chosen for their reliable neural-to-motor mapping. This would result in a more accurate deliberate exploration and evaluation of the sensory feedback signal, which, in turn, is likely to be useful during sensorimotor learning (<xref ref-type="bibr" rid="bib68">Zhou et al., 2018</xref>).</p><p>Finally, although dissecting the role of different neural structures to generating code words would require additional (perturbation) experiments, we can speculate about the contributions of LMAN inputs and local RA circuitry to shaping the statistics of activity in RA. One possibility is that the greater prevalence of code words in exploratory behaviors reflects the interaction of unstructured variability (from LMAN) with the dynamics determined by local circuitry within RA. Alternatively, inputs patterns from LMAN during exploration might be highly structured across LMAN neurons such that tightly-regulated multi-spike patterns in LMAN, rather than the interaction of uncoordinated LMAN activity with intrinsic RA dynamics, is responsible for generating exploratory deviations in behavior. Future studies, including both perturbations of neural activity and recording ensembles of LMAN neurons, will shed light on these questions.</p></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this work, we developed the unsupervised Bayesian Ising Approximation as a new method for reconstructing biological dictionaries — the sets of anomalously represented joint activities of multiple components of biological systems. Inferring these dictionaries directly from data is a key problem in many fields of modern data-rich biological and complex systems research including systems biology, immunology, collective animal behavior, and population genetics. Our approach addresses crucial shortcomings that so far have limited the applicability of other methods. First, uBIA does not limit the possible dictionaries, either by considering words of only limited length or of a pre-defined structure, instead performing a systematic analysis through all possible words that occur in the data sample. Second, it promotes construction of irreducible dictionaries, de-emphasizing related, co-occurring words. Finally, uBIA does not make assumptions about the linear structure of dependencies unlike various linear methods.</p><p>To illustrate capabilities of the method, we applied it first to simulated data sets that are similar to those we expect in experiments. The method was able to reconstruct the dictionaries with a very low false discovery rate for a wide range of parameters and statistical properties of the data (<xref ref-type="fig" rid="fig3">Figure 3</xref>), which made us hopeful that uBIA’s findings will be similarly meaningful in real-life applications. In this analysis, we explored the range <inline-formula><mml:math id="inf227"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>N</mml:mi><mml:mo>∼</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>, which is highly relevant to the neurobiological data we focus on here. Crucially, this <inline-formula><mml:math id="inf229"><mml:mi>N</mml:mi></mml:math></inline-formula> is smaller than in many modern high-throughput experiments. Indeed, there is a necessary trade-off among the system size, the amount of data, and the ability to explore the interactions systematically, to all orders. Since there are many methods able to analyze data at much larger <inline-formula><mml:math id="inf230"><mml:mi>N</mml:mi></mml:math></inline-formula>, but with making assumptions about, in particular, pairwise structure of words in the dictionary (see <italic>Overview of prior related methods in the literature</italic> in <italic>Online Methods</italic>), we decided to focus uBIA on systematic exploration of somewhat smaller systems, <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>N</mml:mi><mml:mo>∼</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>To show that the methoduBIA, indeed, can work with real data, we applied it to analysis of motor activity in cortical area RA in a songbird. We were able to infer statistically significant codewords from large-dimensional probability distributions (<inline-formula><mml:math id="inf232"><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>21</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>097</mml:mn><mml:mo>,</mml:mo><mml:mn>152</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> possible different words) with relatively small data sets (<inline-formula><mml:math id="inf233"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> samples). We verified that the codewords are meaningful, in the sense that they predict behavioral features better than alternative approaches. Importantly, most of words in hundreds of dictionaries that we reconstructed were more complex than is usually considered, involving multiple spikes in precisely timed patterns. The multi-spike, precisely timed nature of the codes was universal across individuals, neurons, and acoustic features, while details of the codes (e.g. specific codewords and their number) showed tremendous variability.</p><p>Further, we identified codewords that correlate with three different acoustic features of the behavior (pitch, amplitude, and spectral entropy), and different percentile ranges for each of these acoustic features. Across many of these analyses, various statistics of codewords predicting exploratory vs. typical behaviors were different. Specifically, the exploratory dictionaries contained more codewords than the dictionaries for typical behavior, suggesting that the exploratory spiking patterns are more consistently able to evoke particular behaviors. This is surprising since the exploratory behavior is usually viewed as being noisier than the typical one. Crucially, exploration is a fundamental aspect of sensorimotor learning (<xref ref-type="bibr" rid="bib64">Tumer and Brainard, 2007</xref>; <xref ref-type="bibr" rid="bib25">Kuebrich and Sober, 2015</xref>; <xref ref-type="bibr" rid="bib24">Kelly and Sober, 2014</xref>), and it has been argued that large deviations in behaviors are crucial to explaining the observed learning phenomenology (<xref ref-type="bibr" rid="bib68">Zhou et al., 2018</xref>). However, the neural basis for controlling exploration vs. typical performance is not well understood. Intriguingly, vocal motor exploration in songbirds is driven by the output of a cortical-basal ganglia-thalamo-cortical circuit, and lesions of the output nucleus of this circuit (area LMAN) abolishes the exploratory (larger) pitch deviations (<xref ref-type="bibr" rid="bib22">Kao et al., 2005</xref>; <xref ref-type="bibr" rid="bib39">Olveczky et al., 2005</xref>). Our findings therefore suggest that the careful selection of the spike patterns most consistently able to drive behavior may be a key function of basal ganglia circuits.</p><p>While the identified codewords are statistically significant, and, for the songbird data, we show that they can predict the behavior better than larger, but non-specific features of the neural activity, a crucial future test of our findings will be in establishing <italic>biological</italic> significance of uBIA findings. In the context of the neural motor control, biological significance may be in establishing the <italic>causal</italic> rather than merely correlative nature of the codewords, which can be done by stimulating neurons with patterns of pulses mimicking the codewords (<xref ref-type="bibr" rid="bib58">Srivastava et al., 2017</xref>). Such verification will be facilitated by the speed of our method, which can reconstruct dictionaries in real time on a laptop computer. The speed and the relatively modest data requirements by uBIA will also allow us to explore how population-level dictionaries are built from the activity of individual neurons, how control of complex behaviors differs from control of their constituent features, how the dictionaries develop and are modified in development, and whether the structure of dictionaries as a whole can be predicted from various biomechanical and information-theoretic optimization principles.</p><p>In building uBIA, we have made a lot of simplifying assumptions. For example, in applications to neural populations, uBIA would explore only symmetric statistical correlations, while the physical neural connectivity is certainly asymmetric. Similarly, in the analysis of activity of a neuron over time bins in the current work, we did not account for causality. Further, we assumed that all data are stationary for the duration of the experiment, which will break down for longer experiments. Making useful biological interpretation of uBIA findings and designing better perturbation experiments may depend on our ability to lift some of these restrictions. The interpretation may be aided by extending uBIA to different null models. The current null model assumes independence between the units—a common assumption in the field. Detecting words that are represented anomalously compared to more complex null models, such as those preserving pairwise correlations, may focus the analysis on the most surprising, and hence maybe easier to interpret, codewords. This is a feasible future extension of uBIA. However, one would have to be careful, since, for datasets with <inline-formula><mml:math id="inf234"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, pairwise correlations are not known well themselves, which may introduce additional uncertainty into the interpretation.</p><p>We tested uBIA on a small number of biological and synthetic datasets. However, for our method to be broadly applicable, users will need to adjust our the hyperparameters, for example, the significance threshold <inline-formula><mml:math id="inf235"><mml:mi>m</mml:mi></mml:math></inline-formula> and the inverse regularization strength <inline-formula><mml:math id="inf236"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>, depending on the statistical structure of their data. Different values may be required depending on the dataset size and the prevalence and strength of higher-order interactions. Although it is hard to say a priori what these adjustments might be, we offer a few suggestions. First, we note that false negatives (failure to identify a significant pattern) should not be considered a failure point major problem in the undersampled regime, since it is manifestly impossible to observe all patterns with relatively small datasetspossible coding patterns with little data. More worrying would be false positive errors, in which statistically insignificant patterns are identified as code wordscodewords. However, our algorithms algorithm offers a self-consistency check: a single pattern should not be identified as predicting both the presence and the absence of a behavior. One can search for such errors by relabeling the presence as 0 or 1 and repeating the analysis, see <xref ref-type="fig" rid="fig5">Figure 5A</xref>. Hyperparameters should therefore be adjusted to keep the number of such cases, and with them the rate of all false positives, below an acceptable threshold More generally, one can do similar tests by relabeling individuals variables 0→1 —words and their partial negations should not be both anomalously represented. The fraction of such incorrectly identified words is a good measure of the false positives rate. Then one should adjust the detection threshold <inline-formula><mml:math id="inf237"><mml:mi>m</mml:mi></mml:math></inline-formula> to the smallest value that keeps the false discovery rate acceptable to the user.</p><p>Another important parameter is the inverse regularization strength <inline-formula><mml:math id="inf238"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>. We would like to keep it as large as possible, so that the regularization is the weakest. At the same time, our perturbative analysis depends crucially on <inline-formula><mml:math id="inf239"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> being small. This suggests a trade-off for choosing <inline-formula><mml:math id="inf240"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>: make it as large as possible, but such that the perturbative constraints, discussed after <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> are satisfied. This value will also depend on a specific dataset and cannot be predicted a priori.</p><p>Finally, additional evidence of biological significance of the method will need to come from its application to other types of data, such as, in particular, molecular sequences, or species abundances in ecology. Crucial for this will be the match between the assumptions of our method (e.g. no dominant words) and the actual data in specific applications: there is no way to say a priori when this will happen, and one will simply need to try. Crucially, in all cases, if the method works, we expect it to be fast and to work well even for problems with large <inline-formula><mml:math id="inf241"><mml:mi>N</mml:mi></mml:math></inline-formula>. In part, this is because the accuracy of the method does not collapse for undersampled problems (large <inline-formula><mml:math id="inf242"><mml:mi>N</mml:mi></mml:math></inline-formula> and not too large <inline-formula><mml:math id="inf243"><mml:mi>M</mml:mi></mml:math></inline-formula>, <xref ref-type="fig" rid="fig3">Figure 3</xref>), and its computational complexity is limited not by <inline-formula><mml:math id="inf244"><mml:mi>N</mml:mi></mml:math></inline-formula>, but by the number of distinct words that occur in data.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Overview of prior related methods in the lterature</title><p>As we pointed out in the main text, a number of different methods have been developed for reconstructing various biological dictionaries, or for the related problem of building the model of the underlying probability distribution. It is important to compare and contrast uBIA to these methods in order to highlight when it should be used. Since these prior methods have been especially common in neuroscience, and since our main biological application throughout this article is also in neuroscience, this is where we will focus our comparisons.</p><p>For many different experimental systems, it has been possible to measure the information content of spike trains (<xref ref-type="bibr" rid="bib12">Fairhall et al., 2012</xref>; <xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>; <xref ref-type="bibr" rid="bib58">Srivastava et al., 2017</xref>), but the question of decoding – which spike patterns carry this information and how? – has turned out to be a much harder one. Most of the effort has been expended on decoding per se: building the model of the activity distribution, rather than deciding which specific spike patterns should belong to the model. Multiple approaches have been used, whether in the context of sensory or motor systems, starting with linear decoding methods (<xref ref-type="bibr" rid="bib6">Bialek et al., 1991</xref>). All have fallen a bit short, especially in the context of motor codes in natural settings, where an animal is free to perform any one of many behaviors it wishes, and hence statistics are usually poor, with only a few samples per behavior. A leading method is Generalized Linear Models (GLMs) (<xref ref-type="bibr" rid="bib43">Paninski, 2004</xref>; <xref ref-type="bibr" rid="bib45">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib17">Gerwinn et al., 2010</xref>), which encode the rate of spike generation from a certain neuron at a certain time as a nonlinear function of a linear combination of past stimuli (sensory systems) or of future motor behavior (motor systems) and the past spiking activity of a neuron and its presynaptic partners. GLM approaches can detect the importance of the timing of individual spikes and sometimes interspike intervals for information encoding, but generalizations to detect importance of higher order spiking patters are not yet well established. Another common approach is based on maximum entropy (MaxEnt) models (<xref ref-type="bibr" rid="bib52">Schneidman et al., 2006</xref>; <xref ref-type="bibr" rid="bib19">Granot-Atedgi et al., 2013</xref>; <xref ref-type="bibr" rid="bib51">Savin and Tkačik, 2017</xref>). These replace the true distribution of the data with the least constrained (i. e., maximum entropy) approximation consistent with low-order, well-sampled correlation functions of the distribution. The In some versions of the method, one then searches for the constraints that affect the distributions the most, and only focuses on those, thus avoiding overfitting (<xref ref-type="bibr" rid="bib3">Barrat-Charlaix et al., 2021</xref>). The MaxEnt approach is computationally intensive, especially when higher order correlations are constrained by data. As a result, almost all of the applications focus on, at most, constraining pairwise activities in the data. At the same time, to approximate empirical distributions well, a large number of such constraints is constraints—even just pairwise ones—is often required. This requires very large datasets, especially if one is interested in relating the neural activity to the external (behavioral or sensory) signals. Such large datasets are hard to obtain in the motor control setting. More recently, feed-forward and recurrent artificial neural network approaches have been used to decode large-scale neural activity (<xref ref-type="bibr" rid="bib42">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="bib18">Glaser et al., 2017</xref>), but these have focused primarily on neural firing rates over large (tens of milliseconds) temporal windows, and typically require larger datasets than considered here.</p><p>As a result of the large data set size requirement and of the focus on building the model of the neural activity rather than finding statistically anomalous features in it, to date, there have not been successful attempts to reconstruct neural dictionaries from data. A success method must (i) resolve spike timing in words of the dictionary to a high temporal resolution, (ii) be comprehensive and orthographically complex, not limiting the words to just single spikes or pairs of spikes, and (iii) discount correlations among spiking words to produce irreducible dictionaries that only detect those codewords that cannot be explained away by correlations with other words in the dictionary.</p><p>There are methods (<xref ref-type="bibr" rid="bib16">Ganmor et al., 2015</xref>; <xref ref-type="bibr" rid="bib46">Prentice et al., 2016</xref>) that are more closely related to our approach, and which can be used within the same pipeline as uBIA, potentially for even better results —specifically in an undersampled regime. When modeling high-dimensional data using too few samples, all such methods tend to decrease complexity of their fitted models (<xref ref-type="bibr" rid="bib29">MacKay, 1992</xref>), such as using fewer parameters or sparser dictionaries. In this regime, such methods can be improved if there is some a priori information regarding which elements of the model must be fitted first. Here, uBIA could be invaluable: it can be used first to choose the features to be included in models, and then a complementary algorithm can be used to actually construct a model on these feature, not unlike we did with the logistic regression in this work. For example, <xref ref-type="bibr" rid="bib16">Ganmor et al., 2015</xref> proposed to understand ganglion cell population activity in relation to the stimuli they encode, and they used a pairwise MaxEnt approach to model the (undersampled) probability distributions of neural activities conditional on specific stimuli. Alternatively, uBIA can be used first to identify conditional neural dictionaries (which will include only a subset of neural pairs, but also potentially higher order combinations of neurons). Then the conditional MaxEnt models can be build based on such detected conditional dictionaries, potentially alleviating the undersampling problem. Crucially, the same approach can applied to other models, not just the log-linear model, <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, or the MaxEnt model. Indeed, as long as the log-likelihood function is specified within a model, and the derivatives of the log-likelihood with respect to the model parameters can be evaluated, similar to <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, then we can use uBIA to calculate the fields and the exchange interactions, and eventually the probability of inclusion of any term in the model.</p><p>On the other hand, as the number of samples increases, we might want to increase the complexity of the involved models by adding additional explanatory terms. Choosing which terms to add is a combinatorially complex problem, and one generally wants to add terms that are non-redundant. Here, uBIA can be useful as well by providing a broad picture of how various terms in the more complex models interact, and hence biasing us towards growing the model complexity in complementary, rather than competing directions. For example <xref ref-type="bibr" rid="bib46">Prentice et al., 2016</xref> considered a Hidden Markov model to describe ganglion cell activity, where different hidden modes activated at different time points. For a fixed mode, the probability distribution of the neural population activity was modeled as belonging to an exponential family. Before adding a new mode, one can calculate the uBIA exchange interactions between it and the already existing modes, as in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>. If the exchange interactions are negative, the new mode is (partially) redundant with the existing one, and should probably be skipped in favor of another, more independent mode. Again, as long as the likelihood function can be written analytically and is differentiable with respect to the model parameters, uBIA can be applied in this way.</p></sec><sec id="s4-2"><title>Direct application of MaxEnt methods to synthetic and experimental data</title><p>To illustrate that traditional MaxEnt methods for creating generative models do not work in our undersampled data regime, we apply the methods described in <xref ref-type="bibr" rid="bib16">Ganmor et al., 2015</xref> to both our synthetic dataset and to our data collected from songbirds. First, note that the Ganmor method assumes that activity is dominated by a single (silent) state and then detects words in a hierarchical fashion. Specifically, higher order patterns (i.e. deviations from the silent state) cannot be detected unless all constituent lower order patterns have already been shown to be statistically significant. This requires datasets much larger than our approach, which can identify higher-order patterns directly, as illustrated in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Comparison of Ganmor et al.method to uBIA.</title><p>(<bold>A, B</bold>) For the synthetic data that we consider in the paper (<xref ref-type="fig" rid="fig3">Figure 3</xref>, interactions arising from the sum of two Gaussians), we obtained precision vs recall curves for the Ganmor et al. method (green and red) using a sweep over the absolute value of the inferred interaction threshold and comparing the detected interactions to the true ones. We also show the corresponding uBIA curves (blue) from <xref ref-type="fig" rid="fig3">Figure 3</xref> for <inline-formula><mml:math id="inf245"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>800</mml:mn><mml:mo>,</mml:mo><mml:mn>1600</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. As illustrated, the Ganmor approach requires two orders of magnitude more data to begin discovering interactions and still does not reach the performance of uBIA for datasets with realistic sizes. (<bold>C</bold>) For the songbird data, the Ganmor et al. approach did not detect any interactions for most datasets. Of the 82 interactions that were detected, most corresponded to pairwise interaction between the behavior and the time bin. (<bold>D</bold>) Words identified by the Ganmor et al. were largely detected based on high marginal probability, consistent with an inability to detect higher order patterns directly. (<bold>E</bold>) The most significant detected interactions (largest interaction coefficients) generally overlap with words detected by uBIA.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68192-fig7-v2.tif"/></fig></sec><sec id="s4-3"><title>Geometric interpretation of uBIA field</title><p>The geometric interpretation of the <inline-formula><mml:math id="inf246"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> fields in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> is illustrated in <xref ref-type="fig" rid="fig8">Figure 8</xref>, and it showcases a part of how uBIA weights the addition of new parameters in terms of the improvement of the fitting, but without the need of building a explicit model. In relation to a null model located at <inline-formula><mml:math id="inf247"><mml:msup><mml:mi>θ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula>, the inclusion of a new parameter <inline-formula><mml:math id="inf248"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> in general will improve the value of the log-likelihood <inline-formula><mml:math id="inf249"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi></mml:math></inline-formula>. This improvement would have an approximated value of <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and it would require to move from <inline-formula><mml:math id="inf251"><mml:msup><mml:mi>θ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> a distance <inline-formula><mml:math id="inf252"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The sign of the field <inline-formula><mml:math id="inf253"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, which indicates the presence or absence of the parameter <inline-formula><mml:math id="inf254"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, is determined by how big is the improvement, and the magnitude of the field by how far you need to move to fit such parameter. Then a rather small parameter that provides an acceptable improvement to the log-likelihood will have a high field.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Geometric interpretation of the fields <inline-formula><mml:math id="inf255"><mml:msub><mml:mi>h</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> in the uBIA method, in relation to the log-likelihood function <inline-formula><mml:math id="inf256"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mrow><mml:mo mathvariant="normal" stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo mathvariant="normal" stretchy="false">{</mml:mo><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo mathvariant="normal" stretchy="false">→</mml:mo></mml:mover><mml:mo mathvariant="normal" stretchy="false">}</mml:mo></mml:mrow><mml:mo lspace="2.5pt" mathvariant="normal" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo mathvariant="normal" stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</title><p>The uBIA method makes an approximate guess (dashed line in left panel) of how much in log-likelihood <inline-formula><mml:math id="inf257"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> we would win be fitting a parameter <inline-formula><mml:math id="inf258"><mml:msub><mml:mi>θ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula>, and how far in parameter space we would need to go, <inline-formula><mml:math id="inf259"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (see left panel). The sign of the field only depends on the improvement in log-likelihood, being positive beyond a threshold (inclusion of a word). This complexity penalty comes from the Bayesian approach in this strong regularized regime. On the other hand, the farther we go in parameter space, the smaller in absolute value the field becomes (see right panel).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68192-fig8-v2.tif"/></fig></sec><sec id="s4-4"><title>Effect of absent words</title><p>To calculate posterior expectations of inclusion of words, we focus only on words that appear in a specific dataset. There are many more words that do not, and the effect of these absent words on uBIA results must be analyzed.</p><p>We start with noticing that, of the exponentially many possible words, majority do not happen in a realistic data set. In particular, this includes most of long words. At the same time, a priori expectations for the frequency of such words, <xref ref-type="disp-formula" rid="equ13">Equation 13</xref>, decrease exponentially fast with the word length. Thus the fields, <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>, for the words that do not occur are small, and the posterior expectation for including these words in the dictionary is <inline-formula><mml:math id="inf260"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, so that we do not need to analyze them explicitly. A bit more complicated is the fact that all words affect each other’s probability to be included in the dictionary through the exchange couplings <inline-formula><mml:math id="inf261"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, so that, in principle, the sum in the mean field equations, <xref ref-type="disp-formula" rid="equ17">Equation 17</xref>, is over exponentially many terms. Thus it is possible for the absent words collectively to have a significant effect on the probability of inclusion of more common words into the dictionary. Here, we show that this collective effect on the interaction terms is exponentially small in <inline-formula><mml:math id="inf262"><mml:mi>N</mml:mi></mml:math></inline-formula>, as long as the empirical averages <inline-formula><mml:math id="inf263"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>To illustrate this, we start with the probabilities <inline-formula><mml:math id="inf264"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of a single variable <inline-formula><mml:math id="inf265"><mml:mi>i</mml:mi></mml:math></inline-formula> being active. We then define the average such probability <inline-formula><mml:math id="inf266"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Without the loss of generality, we assume <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and otherwise we rename <inline-formula><mml:math id="inf268"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Denoting a long word of a high order <inline-formula><mml:math id="inf269"><mml:mi>k</mml:mi></mml:math></inline-formula> that does not occur in the data as <inline-formula><mml:math id="inf270"><mml:msub><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mi>ω</mml:mi></mml:msub></mml:math></inline-formula>, we have <inline-formula><mml:math id="inf271"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Then the corresponding field is<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:msub><mml:mi>h</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>M</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>M</mml:mi><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here, we consider as <italic>high order</italic> words those, for which <inline-formula><mml:math id="inf272"><mml:mrow><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (in general, <inline-formula><mml:math id="inf273"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, which happens for <inline-formula><mml:math id="inf274"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> for our datasets). Then the magnetization is<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi/><mml:mo>≃</mml:mo><mml:mrow><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ω</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mn>8</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This illustrates our first assertion that none of these non-occurring words will be included in the dictionary. However, as a group, they may still have an effect on words of lower orders. To estimate this effect, for a word <inline-formula><mml:math id="inf275"><mml:msub><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> of a low order <italic>k</italic><sub>0</sub>, we calculate the effective field <inline-formula><mml:math id="inf276"><mml:msubsup><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>μ</mml:mi><mml:mtext>eff</mml:mtext></mml:msubsup></mml:math></inline-formula>, which all of the non-occurring words <inline-formula><mml:math id="inf277"><mml:msub><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mi>ω</mml:mi></mml:msub></mml:math></inline-formula> have on it. First we notice that, if <inline-formula><mml:math id="inf278"><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf279"><mml:msub><mml:mi>V</mml:mi><mml:mi>ω</mml:mi></mml:msub></mml:math></inline-formula> do not overlap, then their covariance is zero, and <inline-formula><mml:math id="inf280"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. That is, only high-order words that overlap with <inline-formula><mml:math id="inf281"><mml:msub><mml:mi>V</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> can contribute to <inline-formula><mml:math id="inf282"><mml:msubsup><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>μ</mml:mi><mml:mtext>eff</mml:mtext></mml:msubsup></mml:math></inline-formula>. Since <inline-formula><mml:math id="inf283"><mml:mrow><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the couplings are<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:msup><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mi class="ltx_font_mathcaligraphic">O</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Using <xref ref-type="disp-formula" rid="equ16">Equation 16</xref>, this gives for the typical effective field that absent words have on the word μ<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>μ</mml:mi><mml:mtext>eff</mml:mtext></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi/><mml:mo>≃</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⪆</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where the number of words of order <inline-formula><mml:math id="inf284"><mml:mi>k</mml:mi></mml:math></inline-formula> that overlap with <inline-formula><mml:math id="inf285"><mml:msub><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mi>μ</mml:mi></mml:msub></mml:math></inline-formula> and can affect it is given by the combinatorial coefficient <inline-formula><mml:math id="inf286"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≃</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac linethickness="0pt"><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This has a very sharp peak at <inline-formula><mml:math id="inf287"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf288"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">N</mml:mi><mml:mo>≃</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We can approximate the sum in <xref ref-type="disp-formula" rid="equ22">Equation 22</xref> as the argument of the sum evaluated at this peak <inline-formula><mml:math id="inf289"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, obtaining an effective field coming from high order words<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>μ</mml:mi><mml:mtext>eff</mml:mtext></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mn> 2</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:msup><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mn>16</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:msup></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>ϵ</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>q</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>q</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:mi/><mml:mo>∝</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>q</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In other words, even the combined effect of all higher order absent words is small if the average frequency of individual letters is smaller than 1/2. We thus can disregard all non-occurring words in the mean field equations.</p><p>We stress that, for this to hold, the average of the binary variables <inline-formula><mml:math id="inf290"><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> must be small, <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. In our songbird dataset, this condition was fulfilled with <inline-formula><mml:math id="inf292"><mml:mrow><mml:mi>q</mml:mi><mml:mo>∼</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>. However, in 4% of cases the probability to have a spike in a certain time bin was <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus to stay on the safe side, we performed additional analyses by redefining variables as <inline-formula><mml:math id="inf294"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> if the presence of a spike in a bin was &gt;50 %. In other words, in such cases, we defined the absence of the spike as 1 and the presence as 0. For our datasets, the findings did not change with this redefinition.</p><p>This previous analysis does not imply that absent words of high order are irrelevant — it only says that they cannot be detected with the available datasets. In the numerical implementation of the method, we filter out long absent words ω such that <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, with this cutoff determined by <xref ref-type="disp-formula" rid="equ18 equ19 equ20">Equation 18-20</xref>, so that, for these words, <inline-formula><mml:math id="inf296"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. These words get assigned 1/2 as the posterior probability of inclusion in the dictionary, and their contribution to the mean field equations is neglected. In contrast, if a word ω is absent but <inline-formula><mml:math id="inf297"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>ω</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></inline-formula>, we include them in the analysis, <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>. Such words may turn out to be relevant code words, especially if they happen a lot less frequently than expected a priori.</p></sec><sec id="s4-5"><title>Testing the predictive power of the uBIA dictionaries</title><p>In this section, we test whether the codewords found in data from songbird premotor neurons can be used to predict the subsequent behavior. We compare two logistic regression models: one that uses the activity in the 20 time bins to predict the behavior and another that only uses as features the activity of the few relevant codewords, usually far fewer than 20. The features corresponding to the codewords are binary, and they are only active when all the time bins of such words are active. This means that the model using the time bins is more complex, as it already has all the information that the codewords model has and more, though it does not account for combinatorial effects of combining spikes into patterns. In order to properly test the predictive power between these two models with different complexity we perform twofold cross-validation, using a log-likelihood loss function. As is common in these cases, an L2 penalty is included to help the convergence to a solution (the models were implemented with the Classify function from Mathematica, whose optimization is done by the LBFGS algorithm). As shown by <xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref>, not all neurons in our dataset are timing neurons, or even code for the behavior at all. Thus we restrict the comparison to those cases that have at least 4 codewords (27 case in total, with 10 codewords on average). Both of the logistic regression models have the following structure<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>ॆ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf298"><mml:mi>y</mml:mi></mml:math></inline-formula> corresponds to the behavior, and the features correspond to the time bins in one case (<inline-formula><mml:math id="inf299"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) and to the codewords in the other (<inline-formula><mml:math id="inf300"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), while <inline-formula><mml:math id="inf301"><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are the coefficients of the model. The loss function used is the log-likelihood with the L2 penalty,<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>ॆ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>β</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf302"><mml:mi>M</mml:mi></mml:math></inline-formula> is the number of samples, and λ is the regularization strength. In our analysis, as different datasets have different number of samples, we show the results for the mean cross-entropy over the test data, which correspond to the normalized log-likelihood.</p><p><xref ref-type="bibr" rid="bib61">Tang et al., 2014</xref> showed that individual neurons on average carry around 0.12 bits at a 2ms scale. So for both models, we expect the prediction accuracy to be barely above chance, especially since we are focusing on a particular prediction model (a logistic regression), and may be missing predictive features not easily incorporated in it. <xref ref-type="fig" rid="fig9">Figure 9a</xref> shows the scatter plot of accuracy in the 27 analyzed datasets, plotting the prediction using the time bins activity on the horizontal axis versus prediction using only the codewords activity on the vertical one. We observe that the models based on codewords are consistently better than the ones using all the 20 time bins, and the difference is significant (inset). We additionally evaluate the quality of prediction using the mean cross-entropy between the model and the data. <xref ref-type="fig" rid="fig9">Figure 9b</xref> shows that the models with the codewords have lower mean cross-entropies and thus generalize better (see Inset).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Prediction accuracy with uBIA dictionaries.</title><p>We compare prediction of the behavior using logistic regression models that have as features (i) neural activity in all the time bins at 2ms resolution versus (ii) only the detected relevant codewords. (<bold>A</bold>) Scatter plot of accuracy of models of both types, evaluated using twofold cross-validation. Inset shows that the different between the prediction is significant with <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> according to the paired t-test. (<bold>B</bold>) Scatter plots of the mean cross-entropy between the data and the models for the two model classes. Inset: Even though the models that use the codewords are simpler (have fewer terms), they are able to predict better (with lower cross-entropy) according to the paired t-test.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68192-fig9-v2.tif"/></fig></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Software, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-68192-transrepform1-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The software implementation of uBIA is available from <ext-link ext-link-type="uri" xlink:href="https://github.com/dghernandez/decomotor">GitHub</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:72fe5ae0cf1ae9883ee2cf60b634fe81cc86eb0d;origin=https://github.com/dghernandez/decomotor;visit=swh:1:snp:c8680421bc03b5ff09d2d14d1823f09a546b3545;anchor=swh:1:rev:a374c0d478958aaf38415c7b616bbdebe83c6219">swh:1:rev:a374c0d478958aaf38415c7b616bbdebe83c6219</ext-link>). The data used in this work is available from <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/Songbird_premotor_dictionaries/10315844">figshare</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Hernandez</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Songbird premotor dictionaries</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.10315844.v1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank David Hoffman and Pankaj Mehta for valuable discussions. This work was supported in part by NIH Grants R01-EB022872, R01-NS084844, and R01-NS099375, NSF grant BCS-1822677 (CRCNS Program), and a grant from the Simons Foundation as part of the Simons-Emory International Consortium on Motor Control. IN acknowledges hospitality of the Kavli Institute for Theoretical Physics, supported in part by NSF Grant PHY-1748958, NIH Grant R25GM067110, and the Gordon and Betty Moore Foundation Grant 2919.01. IN and SJS further acknowledge hospitality of the Aspen Center for Physics, which is supported by NSF grant PHY-1607611.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amari</surname><given-names>SI</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Information geometry on hierarchy of probability distributions</article-title><source>Information Theory, IEEE Transactions On</source><volume>47</volume><fpage>1701</fpage><lpage>1711</lpage><pub-id pub-id-type="doi">10.1109/18.930911</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arabzadeh</surname><given-names>E</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Deciphering the spike train of a sensory neuron: counts and temporal patterns in the rat whisker pathway</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>9216</fpage><lpage>9226</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1491-06.2006</pub-id><pub-id pub-id-type="pmid">16957078</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrat-Charlaix</surname><given-names>P</given-names></name><name><surname>Muntoni</surname><given-names>AP</given-names></name><name><surname>Shimagaki</surname><given-names>K</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name><name><surname>Zamponi</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sparse generative modeling via parameter reduction of Boltzmann machines: Application to protein-sequence families</article-title><source>Physical Review. E</source><volume>104</volume><elocation-id>024407</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.104.024407</pub-id><pub-id pub-id-type="pmid">34525554</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassett</surname><given-names>DS</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Network neuroscience</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>353</fpage><lpage>364</lpage><pub-id pub-id-type="doi">10.1038/nn.4502</pub-id><pub-id pub-id-type="pmid">28230844</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Warland</surname><given-names>DK</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The structure and precision of retinal spike trains</article-title><source>PNAS</source><volume>94</volume><fpage>5411</fpage><lpage>5416</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.10.5411</pub-id><pub-id pub-id-type="pmid">9144251</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Rieke</surname><given-names>F</given-names></name><name><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name><name><surname>Warland</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Reading a neural code</article-title><source>Science (New York, N.Y.)</source><volume>252</volume><fpage>1854</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1126/science.2063199</pub-id><pub-id pub-id-type="pmid">2063199</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Cavagna</surname><given-names>A</given-names></name><name><surname>Giardina</surname><given-names>I</given-names></name><name><surname>Mora</surname><given-names>T</given-names></name><name><surname>Silvestri</surname><given-names>E</given-names></name><name><surname>Viale</surname><given-names>M</given-names></name><name><surname>Walczak</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Statistical mechanics for natural flocks of birds</article-title><source>PNAS</source><volume>109</volume><fpage>4786</fpage><lpage>4791</lpage><pub-id pub-id-type="doi">10.1073/pnas.1118633109</pub-id><pub-id pub-id-type="pmid">22427355</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bitbol</surname><given-names>AF</given-names></name><name><surname>Dwyer</surname><given-names>RS</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name><name><surname>Wingreen</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inferring interaction partners from protein sequences</article-title><source>PNAS</source><volume>113</volume><fpage>12180</fpage><lpage>12185</lpage><pub-id pub-id-type="doi">10.1073/pnas.1606762113</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Couzin</surname><given-names>ID</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Self-Organization and Collective Behavior in Vertebrates</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doupe</surname><given-names>AJ</given-names></name><name><surname>Kuhl</surname><given-names>PK</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Birdsong and human speech: common themes and mechanisms</article-title><source>Annual Review of Neuroscience</source><volume>22</volume><fpage>567</fpage><lpage>631</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.22.1.567</pub-id><pub-id pub-id-type="pmid">10202549</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elemans</surname><given-names>CPH</given-names></name><name><surname>Rasmussen</surname><given-names>JH</given-names></name><name><surname>Herbst</surname><given-names>CT</given-names></name><name><surname>Düring</surname><given-names>DN</given-names></name><name><surname>Zollinger</surname><given-names>SA</given-names></name><name><surname>Brumm</surname><given-names>H</given-names></name><name><surname>Srivastava</surname><given-names>K</given-names></name><name><surname>Svane</surname><given-names>N</given-names></name><name><surname>Ding</surname><given-names>M</given-names></name><name><surname>Larsen</surname><given-names>ON</given-names></name><name><surname>Sober</surname><given-names>SJ</given-names></name><name><surname>Švec</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Universal mechanisms of sound production and control in birds and mammals</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8978</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9978</pub-id><pub-id pub-id-type="pmid">26612008</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname><given-names>A</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Barreiro</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Information theoretic approaches to understanding circuit function</article-title><source>Current Opinion in Neurobiology</source><volume>22</volume><fpage>653</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2012.06.005</pub-id><pub-id pub-id-type="pmid">22795220</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferguson</surname><given-names>AL</given-names></name><name><surname>Falkowska</surname><given-names>E</given-names></name><name><surname>Walker</surname><given-names>LM</given-names></name><name><surname>Seaman</surname><given-names>MS</given-names></name><name><surname>Burton</surname><given-names>DR</given-names></name><name><surname>Chakraborty</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Computational prediction of broadly neutralizing HIV-1 antibody epitopes from neutralization activity data</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e80562</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0080562</pub-id><pub-id pub-id-type="pmid">24312481</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>CK</given-names></name><name><surname>Mehta</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bayesian feature selection for high-dimensional linear regression via the Ising approximation with applications to genomics</article-title><source>Bioinformatics (Oxford, England)</source><volume>31</volume><fpage>1754</fpage><lpage>1761</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btv037</pub-id><pub-id pub-id-type="pmid">25619995</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganmor</surname><given-names>E</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sparse low-order interaction network underlies a highly correlated and learnable neural population code</article-title><source>PNAS</source><volume>108</volume><fpage>9679</fpage><lpage>9684</lpage><pub-id pub-id-type="doi">10.1073/pnas.1019641108</pub-id><pub-id pub-id-type="pmid">21602497</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganmor</surname><given-names>E</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A thesaurus for A neural population code</article-title><source>eLife</source><volume>4</volume><elocation-id>e06134</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06134</pub-id><pub-id pub-id-type="pmid">26347983</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerwinn</surname><given-names>S</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Bayesian inference for generalized linear models for spiking neurons</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2010.00012</pub-id><pub-id pub-id-type="pmid">20577627</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Glaser</surname><given-names>J</given-names></name><name><surname>Chowdhury</surname><given-names>R</given-names></name><name><surname>Perich</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>L</given-names></name><name><surname>Körding</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Machine Learning for Neural Decoding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1708.00909">https://arxiv.org/abs/1708.00909</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granot-Atedgi</surname><given-names>E</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Schneidman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Stimulus-dependent maximum entropy models of neural population codes</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1002922</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002922</pub-id><pub-id pub-id-type="pmid">23516339</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halabi</surname><given-names>N</given-names></name><name><surname>Rivoire</surname><given-names>O</given-names></name><name><surname>Leibler</surname><given-names>S</given-names></name><name><surname>Ranganathan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Protein sectors: evolutionary units of three-dimensional structure</article-title><source>Cell</source><volume>138</volume><fpage>774</fpage><lpage>786</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2009.07.038</pub-id><pub-id pub-id-type="pmid">19703402</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hessler</surname><given-names>NA</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Social context modulates singing-related neural activity in the songbird forebrain</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>209</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1038/6306</pub-id><pub-id pub-id-type="pmid">10195211</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>MH</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Contributions of an avian basal ganglia-forebrain circuit to real-time modulation of song</article-title><source>Nature</source><volume>433</volume><fpage>638</fpage><lpage>643</lpage><pub-id pub-id-type="doi">10.1038/nature03127</pub-id><pub-id pub-id-type="pmid">15703748</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname><given-names>DH</given-names></name><name><surname>Ouellette</surname><given-names>NT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Emergent dynamics of laboratory insect swarms</article-title><source>Scientific Reports</source><volume>3</volume><elocation-id>1073</elocation-id><pub-id pub-id-type="doi">10.1038/srep01073</pub-id><pub-id pub-id-type="pmid">23323215</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>CW</given-names></name><name><surname>Sober</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A simple computational principle predicts vocal adaptation dynamics across age and error size</article-title><source>Frontiers in Integrative Neuroscience</source><volume>8</volume><elocation-id>75</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2014.00075</pub-id><pub-id pub-id-type="pmid">25324740</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuebrich</surname><given-names>BD</given-names></name><name><surname>Sober</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Variations on a theme: Songbirds, variability, and sensorimotor error correction</article-title><source>Neuroscience</source><volume>296</volume><fpage>48</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2014.09.068</pub-id><pub-id pub-id-type="pmid">25305664</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawhern</surname><given-names>V</given-names></name><name><surname>Nikonov</surname><given-names>AA</given-names></name><name><surname>Wu</surname><given-names>W</given-names></name><name><surname>Contreras</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spike rate and spike timing contributions to coding taste quality information in rat periphery</article-title><source>Frontiers in Integrative Neuroscience</source><volume>5</volume><elocation-id>18</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2011.00018</pub-id><pub-id pub-id-type="pmid">21617730</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lezon</surname><given-names>TR</given-names></name><name><surname>Banavar</surname><given-names>JR</given-names></name><name><surname>Cieplak</surname><given-names>M</given-names></name><name><surname>Maritan</surname><given-names>A</given-names></name><name><surname>Fedoroff</surname><given-names>NV</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Using the principle of entropy maximization to infer genetic interaction networks from gene expression patterns</article-title><source>PNAS</source><volume>103</volume><fpage>19033</fpage><lpage>19038</lpage><pub-id pub-id-type="doi">10.1073/pnas.0609152103</pub-id><pub-id pub-id-type="pmid">17138668</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lukeman</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>YX</given-names></name><name><surname>Edelstein-Keshet</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Inferring individual rules from collective behavior</article-title><source>PNAS</source><volume>107</volume><fpage>12576</fpage><lpage>12580</lpage><pub-id pub-id-type="doi">10.1073/pnas.1001763107</pub-id><pub-id pub-id-type="pmid">20616032</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>DJC</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Bayesian Interpolation</article-title><source>Neural Computation</source><volume>4</volume><fpage>415</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1162/neco.1992.4.3.415</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margolin</surname><given-names>AA</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name><name><surname>Basso</surname><given-names>K</given-names></name><name><surname>Wiggins</surname><given-names>C</given-names></name><name><surname>Stolovitzky</surname><given-names>G</given-names></name><name><surname>Dalla Favera</surname><given-names>R</given-names></name><name><surname>Califano</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>ARACNE: An algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context</article-title><source>BMC Bioinformatics</source><volume>7 Suppl 1</volume><elocation-id>S7</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2105-7-S1-S7</pub-id><pub-id pub-id-type="pmid">16723010</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margolin</surname><given-names>AA</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Califano</surname><given-names>A</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Multivariate dependence and genetic networks inference</article-title><source>IET Systems Biology</source><volume>4</volume><fpage>428</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1049/iet-syb.2010.0009</pub-id><pub-id pub-id-type="pmid">21073241</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marks</surname><given-names>DS</given-names></name><name><surname>Colwell</surname><given-names>LJ</given-names></name><name><surname>Sheridan</surname><given-names>R</given-names></name><name><surname>Hopf</surname><given-names>TA</given-names></name><name><surname>Pagnani</surname><given-names>A</given-names></name><name><surname>Zecchina</surname><given-names>R</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Protein 3D structure computed from evolutionary sequence variation</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e28766</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0028766</pub-id><pub-id pub-id-type="pmid">22163331</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merchan</surname><given-names>L</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>On the Sufficiency of Pairwise Interactions in Maximum Entropy Models of Networks</article-title><source>Journal of Statistical Physics</source><volume>162</volume><fpage>1294</fpage><lpage>1308</lpage><pub-id pub-id-type="doi">10.1007/s10955-016-1456-5</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mora</surname><given-names>T</given-names></name><name><surname>Walczak</surname><given-names>AM</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Callan</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Maximum entropy models for antibody diversity</article-title><source>PNAS</source><volume>107</volume><fpage>5405</fpage><lpage>5410</lpage><pub-id pub-id-type="doi">10.1073/pnas.1001705107</pub-id><pub-id pub-id-type="pmid">20212159</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morcos</surname><given-names>F</given-names></name><name><surname>Pagnani</surname><given-names>A</given-names></name><name><surname>Lunt</surname><given-names>B</given-names></name><name><surname>Bertolino</surname><given-names>A</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Zecchina</surname><given-names>R</given-names></name><name><surname>Onuchic</surname><given-names>JN</given-names></name><name><surname>Hwa</surname><given-names>T</given-names></name><name><surname>Weigt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Direct-coupling analysis of residue coevolution captures native contacts across many protein families</article-title><source>PNAS</source><volume>108</volume><fpage>E1293</fpage><lpage>E1301</lpage><pub-id pub-id-type="doi">10.1073/pnas.1111471108</pub-id><pub-id pub-id-type="pmid">22106262</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Natale</surname><given-names>JL</given-names></name><name><surname>Hofmann</surname><given-names>D</given-names></name><name><surname>Hernández</surname><given-names>DG</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Reverse-engineering biological networks from large data sets</chapter-title><person-group person-group-type="editor"><name><surname>Munsky</surname><given-names>B</given-names></name><name><surname>Hlavacek</surname><given-names>WS</given-names></name><name><surname>Tsimring</surname><given-names>L</given-names></name></person-group><source>Quantitative Biology: Theory, Computational Methods, and Models</source><publisher-loc>Cambridge, MA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>213</fpage><lpage>246</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nemenman</surname><given-names>I</given-names></name><name><surname>Lewen</surname><given-names>GD</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural coding of natural stimuli: information at sub-millisecond resolution</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000025</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000025</pub-id><pub-id pub-id-type="pmid">18369423</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nemenman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Renormalizing complex models: It is hard without landau</article-title><source>Journal Club Condensed Matter Physics</source><volume>941</volume><fpage>868</fpage><lpage>899</lpage><pub-id pub-id-type="doi">10.1016/j.nuclphysb.2018.07.004</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olveczky</surname><given-names>BP</given-names></name><name><surname>Andalman</surname><given-names>AS</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Vocal experimentation in the juvenile songbird requires a basal ganglia circuit</article-title><source>PLOS</source><volume>3</volume><elocation-id>e153</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030153</pub-id><pub-id pub-id-type="pmid">15826219</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Opper</surname><given-names>M</given-names></name><name><surname>Saad</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Advanced Mean Field Methods: Theory and Practice</source><publisher-name>MIT press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/1100.001.0001</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otwinowski</surname><given-names>J</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Genotype to phenotype mapping and the fitness landscape of the <italic>E. coli</italic> lac promoter</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e61570</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0061570</pub-id><pub-id pub-id-type="pmid">23650500</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname><given-names>C</given-names></name><name><surname>O’Shea</surname><given-names>DJ</given-names></name><name><surname>Collins</surname><given-names>J</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title><source>Nature Methods</source><volume>15</volume><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id><pub-id pub-id-type="pmid">30224673</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Maximum likelihood estimation of cascade point-process neural encoding models</article-title><source>Network (Bristol, England)</source><volume>15</volume><fpage>243</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">15600233</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pérez-Escudero</surname><given-names>A</given-names></name><name><surname>de Polavieja</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Collective animal behavior from Bayesian estimation and probability matching</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002282</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002282</pub-id><pub-id pub-id-type="pmid">22125487</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title><source>Nature</source><volume>454</volume><fpage>995</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nature07140</pub-id><pub-id pub-id-type="pmid">18650810</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prentice</surname><given-names>JS</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Ioffe</surname><given-names>ML</given-names></name><name><surname>Loback</surname><given-names>AR</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Error-Robust Modes of the Retinal Population Code</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005148</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005148</pub-id><pub-id pub-id-type="pmid">27855154</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Putney</surname><given-names>J</given-names></name><name><surname>Conn</surname><given-names>R</given-names></name><name><surname>Sponberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Timing Is (Almost) Everything in a Comprehensive, Spike-Resolved Flight Motor Program</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/602961</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinagel</surname><given-names>P</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Temporal coding of visual information in the thalamus</article-title><source>The Journal of Neuroscience</source><volume>20</volume><fpage>5392</fpage><lpage>5400</lpage><pub-id pub-id-type="pmid">10884324</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Watzl</surname><given-names>S</given-names></name><name><surname>Gollisch</surname><given-names>T</given-names></name><name><surname>Stemmler</surname><given-names>M</given-names></name><name><surname>Herz</surname><given-names>AVM</given-names></name><name><surname>Samengo</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spike-timing precision underlies the coding efficiency of auditory receptor neurons</article-title><source>Journal of Neurophysiology</source><volume>95</volume><fpage>2541</fpage><lpage>2552</lpage><pub-id pub-id-type="doi">10.1152/jn.00891.2005</pub-id><pub-id pub-id-type="pmid">16354733</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saravanan</surname><given-names>V</given-names></name><name><surname>Hoffmann</surname><given-names>LA</given-names></name><name><surname>Jacob</surname><given-names>AL</given-names></name><name><surname>Berman</surname><given-names>GJ</given-names></name><name><surname>Sober</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dopamine Depletion Affects Vocal Acoustics and Disrupts Sensorimotor Adaptation in Songbirds</article-title><source>ENeuro</source><volume>6</volume><elocation-id>ENEURO.0190-19.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0190-19.2019</pub-id><pub-id pub-id-type="pmid">31126913</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savin</surname><given-names>C</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Maximum entropy models as a tool for building precise neural controls</article-title><source>Current Opinion in Neurobiology</source><volume>46</volume><fpage>120</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.08.001</pub-id><pub-id pub-id-type="pmid">28869818</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname><given-names>E</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Segev</surname><given-names>R</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title><source>Nature</source><volume>440</volume><fpage>1007</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1038/nature04701</pub-id><pub-id pub-id-type="pmid">16625187</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Multineuronal firing patterns in the signal from eye to brain</article-title><source>Neuron</source><volume>37</volume><fpage>499</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00004-7</pub-id><pub-id pub-id-type="pmid">12575956</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwab</surname><given-names>DJ</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name><name><surname>Mehta</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Zipf’s law and criticality in multivariate data without fine-tuning</article-title><source>Physical Review Letters</source><volume>113</volume><elocation-id>068102</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.113.068102</pub-id><pub-id pub-id-type="pmid">25148352</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sober</surname><given-names>SJ</given-names></name><name><surname>Wohlgemuth</surname><given-names>MJ</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Central contributions to acoustic variation in birdsong</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>10370</fpage><lpage>10379</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2448-08.2008</pub-id><pub-id pub-id-type="pmid">18842896</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sober</surname><given-names>SJ</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Adult birdsong is actively maintained by error correction</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>927</fpage><lpage>931</lpage><pub-id pub-id-type="doi">10.1038/nn.2336</pub-id><pub-id pub-id-type="pmid">19525945</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sober</surname><given-names>SJ</given-names></name><name><surname>Sponberg</surname><given-names>S</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name><name><surname>Ting</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Millisecond Spike Timing Codes for Motor Control</article-title><source>Trends in Neurosciences</source><volume>41</volume><fpage>644</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2018.08.010</pub-id><pub-id pub-id-type="pmid">30274598</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>KH</given-names></name><name><surname>Holmes</surname><given-names>CM</given-names></name><name><surname>Vellema</surname><given-names>M</given-names></name><name><surname>Pack</surname><given-names>AR</given-names></name><name><surname>Elemans</surname><given-names>CPH</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name><name><surname>Sober</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Motor control by precisely timed spike patterns</article-title><source>PNAS</source><volume>114</volume><fpage>1171</fpage><lpage>1176</lpage><pub-id pub-id-type="doi">10.1073/pnas.1611734114</pub-id><pub-id pub-id-type="pmid">28100491</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>CF</given-names></name><name><surname>Zador</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Neural coding: The enigma of the brain</article-title><source>Current Biology</source><volume>5</volume><fpage>1370</fpage><lpage>1371</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(95)00273-9</pub-id><pub-id pub-id-type="pmid">8749388</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strong</surname><given-names>SP</given-names></name><name><surname>Koberle</surname><given-names>R</given-names></name><name><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Entropy and Information in Neural Spike Trains</article-title><source>Physical Review Letters</source><volume>80</volume><fpage>197</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.80.197</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>C</given-names></name><name><surname>Chehayeb</surname><given-names>D</given-names></name><name><surname>Srivastava</surname><given-names>K</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name><name><surname>Sober</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Millisecond-scale motor encoding in a cortical vocal area</article-title><source>PLOS Biology</source><volume>12</volume><elocation-id>e1002018</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002018</pub-id><pub-id pub-id-type="pmid">25490022</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Mathematical Statistical Mechanics</source><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tikhonov</surname><given-names>M</given-names></name><name><surname>Leach</surname><given-names>RW</given-names></name><name><surname>Wingreen</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Interpreting 16S metagenomic data without clustering to achieve sub-OTU resolution</article-title><source>The ISME Journal</source><volume>9</volume><fpage>68</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1038/ismej.2014.117</pub-id><pub-id pub-id-type="pmid">25012900</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tumer</surname><given-names>EC</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Performance variability enables adaptive plasticity of “crystallized” adult birdsong</article-title><source>Nature</source><volume>450</volume><fpage>1240</fpage><lpage>1244</lpage><pub-id pub-id-type="doi">10.1038/nature06390</pub-id><pub-id pub-id-type="pmid">18097411</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>AH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Discovering Precise Temporal Patterns in Large-Scale Neural Recordings through Robust and Interpretable Time Warping</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/661165</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wohlgemuth</surname><given-names>MJ</given-names></name><name><surname>Sober</surname><given-names>SJ</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Linked control of syllable sequence and phonology in birdsong</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>12936</fpage><lpage>12949</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2690-10.2010</pub-id><pub-id pub-id-type="pmid">20881112</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolley</surname><given-names>SC</given-names></name><name><surname>Rajan</surname><given-names>R</given-names></name><name><surname>Joshua</surname><given-names>M</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Emergence of context-dependent variability across a basal ganglia network</article-title><source>Neuron</source><volume>82</volume><fpage>208</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.039</pub-id><pub-id pub-id-type="pmid">24698276</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Hofmann</surname><given-names>D</given-names></name><name><surname>Pinkoviezky</surname><given-names>I</given-names></name><name><surname>Sober</surname><given-names>S</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Chance, long tails, and inference: a non-gaussian, bayesian theory of vocal learning in songbirds</article-title><source>PNAS</source><volume>115</volume><fpage>E8538</fpage><lpage>E8546</lpage><pub-id pub-id-type="doi">10.1073/pnas.1713020115</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68192.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/849034" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/849034"/></front-stub><body><p>This work introduces a new unsupervised Bayesian method for identifying important patterns in neural population responses. The method offers improvements relative methods that do not assume correlations in neural responses, and is likely to also outperform methods that take into account pairwise correlations in neural responses.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68192.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xez1567</institution-id><institution>Salk Institute for Biological Studies</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/849034">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/849034v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your work entitled &quot;Unsupervised Bayesian Ising Approximation for revealing the neural dictionary in songbirds&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The reviewers have opted to remain anonymous.</p><p>Our decision has been reached after consultation between the reviewers and your reviewing editor. It took an unusually long time to reach this decision due to both personal and global health emergencies. We are sorry for the delay. Based on these discussions and the individual reviews below, we regret to inform you that this manuscript will not be considered further for publication in <italic>eLife</italic> as a Research Article.</p><p>However, your reviewing editor and reviewers have agreed that they would be willing to consider a revised version of the paper, reformatted as a &quot;Tools and Resources&quot; paper, if you choose to resubmit it as such to <italic>eLife</italic>. That revised paper should address fully the methodological and technical questions raised by the reviewers and your reviewing editor in this letter. As it stands, the reviewing team did not feel that they had enough information to properly judge the merits of the method, and that would be reassessed if a new and revised Tools and Resources manuscript is submitted.</p><p>The individual reviewer comments are appended below and can be used either as directives for that new submission, or as helpful pointers for a submission elsewhere. Summary thoughts are included here, with the detailed reviews from the 2 reviewers included at the end.</p><p>The paper details a new approach to learning the input/output relationships in complex biological systems that drive things like motor behavior or (perhaps) protein folding and other processes. The work focuses on birdsong as an example of a complex motor task potentially modulated by precise temporal patterns of spiking in the bird motor cortex (RA). The method claims to be able to use very few jointly recorded samples of the input and output to learn both a model for the distribution of the spiking patterns observed as well as how they drive behavior. The method uses strong priors in a Bayesian Ising Approximation to identify the neural code words that are expressed at unusually high or low frequency with respect to the behavioral output. The authors find a large number of spike patterns correlated with behavioral variations in syllable pitch. The method has potential applications beyond birdsong; the ability to fit input/output relationships with very few samples is appealing.</p><p>The reviewing team, however, had some major reservations about the results and felt that the current draft did not give them enough details to properly evaluate the method. A summary of those major points is given here:</p><p>1) The method:</p><p>The reviewers felt that there wasn't enough detail presented to clearly analyze the results of the model. Much of the detail was in the SI and Methods, where the exposition wasn't clear or detailed enough for the reader to properly assess the results.</p><p>Questions raised included:</p><p>– What's the null model?</p><p>– How does this method scale to larger neural populations?</p><p>– Is the clustering used here justified? Can it be compared to other, simpler clustering methods?</p><p>– How do you justify the choice of prior and how are the unknowns fit from the data?</p><p>– Other extant methods for finding reliable interactions in exponential models were not referenced or compared.</p><p>– Artificial data used in the SI are based on properties of the recorded neurons, and as the authors explain the cells seem to be quite weakly coupled, so the relevance of the model for other data is not clear.</p><p>2) The behavioral results:</p><p>The reviewers were not convinced by the pitch results, primarily because of the coarse-graining of pitch into just two categories. Again, much of the explanation was relegated to the Methods part of the paper and could have been presented more clearly. If this method works, this is clearly interesting, but the results presented seemed rather weak.</p><p>3) The format of the paper:</p><p>The paper read a bit like 3 manuscripts glued together, each of which has merits, but none of which seemed to satisfactorily stand on its own. The paper starts with a grand introduction that talks about solving a major challenge in quantitative biology. Then it quickly dives into something that reads more like a methods paper, but with insufficient exposition and clarity. Finally, the paper shifts to the application of the method to behavior and neural data, the results of which seemed somewhat weak and preliminary.</p><p>Taking these critiques together, the reviewing team felt that the paper could not be reasonably revised for <italic>eLife</italic> as a Research Article. However, the team did find the central method potentially interesting if more details are provided that allow for proper assessment of it. The team felt that it could be a good &quot;Tools and Resources&quot; advance, if the method holds up to this more detailed scrutiny.</p><p><italic>Reviewer 2:</italic></p><p>The paper is a potentially interesting application of a recent statistical method, the Bayesian Ising Approximation (Fisher and Mehta), aimed at addressing the problem of estimating a complex distribution, in particular the relationship between a code and an output, from very few samples. This is accomplished by heavily weighting by the prior. The authors have applied this method to support the idea that there are precisely timed spike patterns that are especially significant in the generation of behavior, in this case in the output of syllables of birdsong. They use the method to identify &quot;code words&quot;, patterns that are claimed to appear with notably high or low frequency in correspondence with a behavioral variable. They apply this method to find a larger number of precise spike patterns correlated with behavioral variations in the tails of the behavioral distribution. They suggest an interpretation that perhaps larger exploratory motor gestures require a more precise spiking code; although it might also be argued that this shows that there are more ways to achieve these larger fluctuations? While the claim is counterintuitive, it rests on the power of the method, whose exposition left a number of gaps.</p><p>1. It is unclear to what extent this goal is meaningful in the case that the paper analyses: premotor drive of muscles in the birdsong pathway. It seems evident that timing of RA spikes necessarily sculpt motor output; and the authors have previously analyzed this in detail to reveal timing precision (Tang et al.). It was not clear, however, that specific patterns meaningfully emerge from a sea of similar spike trains to encode exact outputs. The authors ask whether precise patterns in 2ms bins have a significant relationship with binarized song characteristics in 40ms bins-a very low fidelity relationship on the &quot;behavior&quot; side compared with a high level of precision on the neural side. This seems a dramatic discrepancy between the dimensionality of neural response vs behavior that deserves more justification. How general a space of coding relationships (nonmonotonic, etc) does this method explore given this binarization?</p><p>2. Further, it was unclear whether a temporally structured firing rate would be equally convincing as a representation of behavioral variation. While the concept of &quot;irreducible dictionaries&quot; is appealing, does this method fairly compare a timing code to a rate code? For example, doesn't the competition component of the model suppress the identification of firing rate-like contributions? Looking at Figure 1, the cartoon of eliminating overlapping but similar code words seem equivalent to ignoring a less temporally precise method of coding for behavior. That is to say, if you are forcing your model to choose one of several similar code words in a winner-take all inclusion choice, isn't the timing precision you see partially a factor of not looking at the other similar code words which didn't win out? The comparison of this method to GLM methods of relating the spiking to behavior is unclear. One alternative model was analyzed in the online methods, but is not clearly explained and such comparisons deserve to be treated in the main text. It should also be easy to report the simple 0th order analysis of how well you can predict the binary version of behavior simply with a spike count within the 40 ms window, or with some intermediate level of temporal variation. Seeing these comparisons as a control of the assertion that precise timing matters in this system would enhance intuition for the necessity of this method and better justify the claim &quot;that they predict behavioral features better than other approaches&quot; (line 256).</p><p>3. In general, the exposition of the method leaves many questions and could be improved considerably in clarity. I have tried to summarize the logic of the paper's core argument below and underscored throughout where more help could be given to understand the meaning and generality of assumptions made.</p><p>Logic of the calculation:</p><p>The paper sets up the problem of writing down a model for the joint probability of a behavioral event s0 and the firing of a spike in each, i, of N bins, σi. The authors write this in terms of an expansion over every possible combination of σi,</p><p>Log P(σ|θμ) = -log Z + Σ θμ Π _{i in Vμ} σi.</p><p>Q3.1: please expand a little on the exponential form. In maximum entropy approaches, this form of distribution arises from the maximum entropy solution, but if we are avoiding MaxEnt approaches, can you briefly justify it? In this approach, a firing pattern with spikes in bins (1,2,3,4) is considered to be independent of the events (1,2) and (3,4); whereas in maxent, if one is only expanding up to order 2, one hypothesizes that the probability of the word (1,2,3,4) will be absorbed into the pairwise probabilities, and one can ask how different the data is from that guess. It seems that here one is parametrizing with vastly more parameters-one for every possible spike/behavior combination-- without a systematic method of cutting them down (or at least one that was clear to me; it is addressed at the end of the Online Methods calculations once we have moved into the s variables but it would be helpful to get some intuition from the start).</p><p>One then defines &quot;indicator variables&quot; sμ which tell us which patterns appear significantly less or more often than one would expect under some null model. It would be good to specify this null model right away, and in straightforward terms; I don't see it defined in the main text at all.</p><p>Q3.2: Please explain at this point the null model for the frequency of appearance of any possible pattern of spikes and its correlation with behavior? Should it not depend on the specific behavior?</p><p>It is unclear why one is now allowed to write that the original definition of the model is EQUAL to the model now written including only terms defined by a criterion defined by our subjective interest in them:</p><p>Log P(σ|θ) = -log Z + Σ θμ sμ Π _{i in Vμ} σi</p><p>Q3.3: Why does one not here need to define a new and different probability distribution, something that makes some claim about belonging to the codeword dictionary, in which these terms are the only ones that appear?</p><p>Now the authors turn to the task of estimating the 2N parameters θμ. Instead of doing this estimation directly from the data (i.e. the a posteriori expectation), they use a prior on these values: they choose to assume that each of the values are narrowly distributed about some mean θ∗μ:</p><p>P(θ∗μ : sμ=1) ~ exp(-(θμ −θ∗μ)2).</p><p>This distribution is taken only to hold if the word is indeed a code word, ie statistically associated with a behavior. If it is not a code word, the prior is not defined.</p><p>Q3.4: Please justify this choice in more detail. Why are the epsilons not dependent on μ?</p><p>Q3.5: The argument for where these 2N values of θ∗μ come from is very unclear. Please spell out more clearly how constraining the means of the individual σis provides sufficient constraints for all 2N values, or why this is not needed.</p><p>Q3.6: Why does a code word have an a priori likelihood of 0.5 of being in the dictionary?</p><p>Now rather than estimating or computing the values of the θμs, the next step is to flip the variables in the distribution we seek, to P(s|theta), and then integrate out the thetas, based (it seems?) only on their prior distributions, Equation (5).</p><p>Q3.7: Now that s can again take value of either 0 or 1, why did we not need the distribution over θs in the case that s = 0? How can a distribution over s on the LHS in Equation (5) limit itself to the case of s = 1 on the RHS?</p><p>The calculations in the online methods may resolve some of these issues? But these all appear to start from Equation (5) and at least for me at this stage, the route to this equation is unclear.</p><p><italic>Reviewer #3:</italic></p><p>The manuscript suggests a new approach to learning coding dictionaries. The method is applied to neural population data recorded in singing birds. While the idea of the approach is interesting, it was hard to understand the details or how well it works. It seems that too much has been pushed to the SI, and more controls and examples over synthetic data sets would be useful. Moreover, and importantly – it was hard to interpret the results. Also, the current approach was not compared against other methods presented in recent years for learning metrics on neural population patterns, or even more simple clustering approaches.</p><p>Briefly, the authors use a log-linear model for the population words of N neurons and two behavioral states – using 2^(N+1) terms that span all potential orders of interactions among cells (theta's). To avoid arbitrary cutoff on the order of interactions that they consider, and use the important ones, they use a constructed prior distribution over theta's to learn a posterior model over the population words after integrating out the theta's. This gives an Ising model over 2^N interacting 'spins' where each sping now stands for a word that may or may not have appeared in the sampled data. The local fields acting on each of these spins is related to the prob of appearance of the words the spins stand for, and the pairwise interaction terms stand for an overlap between the words. These interactions now imply a 'competition' between words in terms of the explanatory power they carry (and are therefore typically negative)</p><p>Thus, if I understand this correctly, the constructed model gives a form of compression of the population 'codebook' by picking words that are dominant to be included in the dictionary and omitting similar words with less explanatory power and frequency.</p><p>Critically, it was unclear to me how one should evaluate the results. It is hard to interpret the structure of the dictionary in Figures4-5, and in particular over the pooling of conditions in Figure 5. In addition, there have been different approaches towards learning neural dictionaries in recent years, which are not mentioned here or compared against the current approach. Maybe this is too naïve, but it seems natural to ask how well would naïve clustering or Edit-distance based methods work here? Then, how does the current approach compare to more structured metrics such as those presented in Ganmor et al. <italic>eLife</italic> 2015, Prentice et al. Plos Comp Biol 2016, Rubin et al. Nature Comm, 2019, or Chaudhuri et al. Nature Neuro, 2019 ?</p><p>Finally, how would approach work for larger populations, where all observed patterns would appear just once?</p><p>It was not clear how should one interpret the nature of the organization of patterns in Figure 4B (the gap between lines, their very linear dependency?)</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Unsupervised Bayesian Ising Approximation for decoding neural activity and other biological dictionaries&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Aleksandra Walczak as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) The numerical comparison to other existing methods is minimal and should be expanded. The analysis of the irreducibility of the codewords has insufficient support based on the numerical simulations. Moreover, the generality of the tool and comparison to other methods are discussed in almost entirely theoretical terms, which makes the claim on immediate utility for other datasets less convincing, especially outside the neuroscience community. In particular, the identified codewords are relatively short (3rd, 4th order stats) and N=20 is easy to fit an Ising model; so one would think that it would be relatively straightforward to fit the Ganmor model jointly. At least on artificial data one should be able to explore a larger N regime where there is a more immediate gain relative to existing models.</p><p>2) Although the paper is written as a methods paper, emphasizing the technical contributions and promising wide applicability to a range of different types of datasets, the numerical validation of the method is very much restricted to the statistical regime of the songbird dataset. From the perspective of a potential future user of the tool it's less clear how the method would behave on different datasets, and what needs to happen in practice for adopting the tool to data with different statistics. Based on current content, it would be better to focus the abstract and introduction on applications that are actually studied here.</p><p>3) The songbird analysis already reveals some challenges with respect to interpretability: in particular it is not clear how much information about the underlying neural processes can be revealed by summary statistics generated by the method, such as the number of codewords and their length distribution.</p><p>4) Limiting the analysis to N=20 patterns with somewhat random code structure makes it hard for a potential user of the tool to work out what needs to change when switching to a novel dataset (with respect to N, M, codeword complexity, sparsity of neural responses). Some work could be invested in spelling out the considerations of applicability with respect to quantities that an experimentalist would know about the data.</p><p>5) Is it possible to include multiple binary quantifications of behavior, similarly to how words are constructed from neural spike trains? For example, one can envision describing a particular song segment with respect to multiple binary features simultaneously.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Line 168: I think the notation here should P(s_mu=0) not s_mu=-1.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I am overall very excited about the framework and I see significant technical potential in the approach. Nonetheless, there are several missed opportunities in the numerical analysis, and things that could and should probably be improved to maximize the impact of the work on the broader community.</p><p>– Limiting the analysis to N=20 patterns with somewhat random code structure makes it hard for a potential user of the tool to work out what needs to change when switching to a novel dataset (with respect to N, M, codeword complexity, sparsity of neural responses). Some work could be invested in spelling out the considerations of applicability with respect to quantities that an experimentalist would know about the data.</p><p>– The codeword irreducibility could be analyzed in more detail, again in terms of experimentally relevant quantities.</p><p>– The comparison to other methods is minimal. The identified codewords are relatively short (third, fourth order stats) and N=20 is easy to fit an Ising model; so I'd think that it would be relatively straightforward to fit the Ganmor model jointly. At least on artificial data one should be able to explore a larger N regime where there is a more immediate gain relative to existing models.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Unsupervised Bayesian Ising Approximation for decoding neural activity and other biological dictionaries&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Aleksandra Walczak (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>Essential revisions:</p><p>Thank you for the revised manuscript. The reviewers found the manuscript much improved and close to acceptance. Nevertheless, reviewers are suggesting a few additional analyses to demonstrate the advantages of the method and increase its impact.</p><p>1) Please consider the null model that includes pairwise correlations.</p><p>2) Please test the method to a larger dataset.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I generally find that the revisions have improved the overall clarity of the text, and addressed most of my questions.</p><p>Nonetheless, I found the answer to two of my concerns underwhelming:</p><p>– Discussing the practical considerations about translating the approach to other datasets (new discussion paragraph) is quite vague, essentially saying that the hyperparameters will more or less have to be figured out de novo. I was expecting a quantitative description of how different statistical properties of the data affect the process, which would translate into something closer to a concrete recipe for hyperparameters adjustment. The fact that this is not straightforward to do makes a broad adoption of the tool less likely.</p><p>– The concern about the overall interpretability of the extracted statistics remains unaddressed. I don't agree with the position is that &quot;it only matters to estimate these statistics well, interpretation can come later.&quot; Being able to estimate a quantity does not necessarily make it useful to do. In the context of songbird specifically, the paper make the case for the results being difficult to interpret even when the statistics are well estimated. This seems like a pretty big problem, at least big enough to warrant a minimum amount of thought and a couple of sentences in the discussion.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The authors already made a great effort to improve their manuscript. However, from my side, I provide the following concerns about the significance and broad impact of the current revised manuscript.</p><p>Strength of the manuscript</p><p>The traditional model of maximum entropy type requires a large amount of data to predict effective interactions among elements (e.g., neurons), and fails to identify statistically over- or under-represented (relative to a null model) codewords, and thus the neural-behavior prediction remains elusive. This manuscript provides a novel approach that can systematically search for the significant codewords, yet requiring fewer data samples (compared to the number of higher-order model parameters).</p><p>The method first writes all orders of interactions in a probability distribution form; finding non-zero higher order interaction parameters could tell which codewords should be included in the neural dictionary and how they matter. To make the method algorithmically solvable, the authors turn the optimization into an Ising model of interacting indicator variables. This indicator variable is responsible for identification of key codewords. The inferred value of the external fields signals the importance of the codeword, while the sign of the high-order interaction parameter reflects whether the word is over or under-represented. To explain the neural-behavior correlates, the authors also include the behavior variable into the components of the activity. This framework works consistently in the songbird motor experiments.</p><p>Weakness of the manuscript</p><p>To relate the neural activity and behavior, the authors test their method in both synthetic data and songbird real data. There are two weakness, considering the impact of the method in a broad context. First, the variable size is limited to N~20, which is severely below the number of neurons the neural data scientists get interested in (e.g., for modeling retinal ganglion cells, and even cortical cells). This may be a tradeoff between all order of interactions to be considered and the number of model parameters. Second, a null model is chosen to be an independent model that neglects correlations in the data, which may affect the characterization of the redundancy of the uBIA model. Even if an irreducible ensemble of codewords is obtained, what is the predictive power of this ensemble with respect to control the macroscopic behavior (e.g., skilled motor control or sensorimotor learning), rather than showing statistics of neural-behavior activity, remains obscure in the current manuscript.</p><p>Considering the strength and weakness of the manuscript, I recommend the following issues for the authors to improve their idea.</p><p>1. The proposed model is limited to symmetric couplings and i.i.d data samples. First, in a neural circuit, the asymmetric coupling is common among neurons; Second, the temporal order of each spiking activity may play a key role in encoding information. For example, a paper &quot;Blindfold learning of an accurate neural metric&quot; (PNAS, 2018) takes the temporal information into account. When putting in the context of neural-behavior relationship, these two factors would become important. But they seem neglected in the current manuscript.</p><p>2. To achieve a higher-order-interaction-included and non-redundant model is really important problem in the field. For example, the reliable interaction model proposed in ref. 26 is not fully compared in the current manuscript, i.e., what is the new advance? This is better to clearly demonstrated in a plot in the main text. Second, a recent paper (Phys Rev E, 104, 024407, 2021) used an information-based criterion to identify statistically significant couplings, which may be applicable to the context the authors consider here. In this <italic>eLife</italic> submission, the method relies on the magnetization inclusion threshold, which may be expensive for real data analysis.</p><p>3. On the equation 1, the authors seem to use the local model parameter to detect the global significance of the collective codeword, which I could not understand very well, because a codeword is determined by all order of interactions considered in the log-linear model.</p><p>4. Technically, the logic going from Equation 7 to Equation 8 is broken, since the s-dependence in Equation 8 does not naturally arise from Equation7.</p><p>5. I could not understand well how the sign of reflects whether the word is over- or under-represented, and even, how the parameter account for the reducibility of the dictionaries. This part is better to be expanded in the manuscript, although these parameters have highly non-linear function relationship.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68192.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>The individual reviewer comments are appended below and can be used either as directives for that new submission, or as helpful pointers for a submission elsewhere. Summary thoughts are included here, with the detailed reviews from the 2 reviewers included at the end.</p><p>The paper details a new approach to learning the input/output relationships in complex biological systems that drive things like motor behavior or (perhaps) protein folding and other processes. The work focuses on birdsong as an example of a complex motor task potentially modulated by precise temporal patterns of spiking in the bird motor cortex (RA). The method claims to be able to use very few jointly recorded samples of the input and output to learn both a model for the distribution of the spiking patterns observed as well as how they drive behavior. The method uses strong priors in a Bayesian Ising Approximation to identify the neural code words that are expressed at unusually high or low frequency with respect to the behavioral output. The authors find a large number of spike patterns correlated with behavioral variations in syllable pitch. The method has potential applications beyond birdsong; the ability to fit input/output relationships with very few samples is appealing.</p></disp-quote><p>This is an excellent overall summary of the paper. However, there is one crucial aspect of our approach which is summarized incorrectly and which we clarify briey here as well as in detail in the revised manuscript: uBIA does not fit or reconstruct the input/output relationship from data. Rather, it detects which keywords contribute statistically significantly to the input/output dictionary, but unlike many other methods, it does not build a model of the input/output relationship. This is crucial, since it is one of the reasons why comparing uBIA to other approaches is hard, and it is also one of the main reasons why uBIA can work with fewer assumptions and smaller datasets compared to other methods. We have emphasized that uBIA does not build an explicit model of the data in the new revision.</p><disp-quote content-type="editor-comment"><p>The reviewing team, however, had some major reservations about the results and felt that the current draft did not give them enough details to properly evaluate the method. A summary of those major points is given here:</p><p>1) The method:</p><p>The reviewers felt that there wasn't enough detail presented to clearly analyze the results of the model. Much of the detail was in the SI and Methods, where the exposition wasn't clear or detailed enough for the reader to properly assess the results.</p></disp-quote><p>Following these concerns, we have completely restructured the paper, moved some of the calculations from the Methods into the main text, de-emphasized biological applications and re-emphasized and explained in more detail the algorithmic details, as described below.</p><disp-quote content-type="editor-comment"><p>Questions raised included:</p><p>– What's the null model?</p></disp-quote><p>In the revised manuscript, we now explicitly define the null model after Equation 3.</p><disp-quote content-type="editor-comment"><p>– How does this method scale to larger neural populations?</p></disp-quote><p>It is hard to be precise answering this. The size of the population is not the most crucial variable since, as is, the method analyzes 2<italic><sup>N</sup></italic> possible terms in a model with just <italic>N</italic> variables. What is more important is the quality and quantity of sampling, the strength of the contribution of terms to the likelihood, and so on. We discussed this explicitly in the Discussion section, but also throughout the manuscript, wherever relevant, such as in the first paragraph of Page 4 and in the last paragraph of the “Testing and fine-tuning uBIA on synthetic data” section.</p><disp-quote content-type="editor-comment"><p>– Is the clustering used here justified? Can it be compared to other, simpler clustering methods?</p></disp-quote><p>We apologize, but we do not understand the question here, since there is no clustering used in our method. We hope that the other revisions and clarifications described here are sufficient to clarify our overall approach.</p><disp-quote content-type="editor-comment"><p>– How do you justify the choice of prior and how are the unknowns fit from the data?</p></disp-quote><p>We have completely reworked the presentation of the method, and we hope that the new section “The unsupervised BIA method (uBIA) for dictionary reconstruction” answers these questions. Briefly, as explained in this section, we choose the priors we work with because they are standard (e.g., Gaussian), and allow for the strongly regularized regime, which is a key for our work. And then we chose the weakest of such strongly regularizing priors (largest), as detailed in the manuscript.</p><disp-quote content-type="editor-comment"><p>– Other extant methods for finding reliable interactions in exponential models were not referenced or compared.</p></disp-quote><p>We now introduce the necessary references and comparisons, largely in the first section of the supplementary materials. Briefly, as we discussed in the preamble to this response letter, the goal of our method is very different from many others. Thus, as explained in the revised Supplemental Information, our method is complimentary to, rather than a competitor of, the other methods we review, which complicates the task of comparing them.</p><disp-quote content-type="editor-comment"><p>– Artificial data used in the SI are based on properties of the recorded neurons, and as the authors explain the cells seem to be quite weakly coupled, so the relevance of the model for other data is not clear.</p></disp-quote><p>Just to make sure there is no confusion: the data is from single cell recordings, and the interactions are between time points of neural activity, and not within a population of neurons. However, our response to the substance of the comment is that there is no way a priori to know if a method would work for other data, without trying it. We explain this in the revised Discussion section, where we also comment on what the determinants of the success will be. We only have access to the neural dataset, and analyzing it was a lot of work – we simply cannot do more in one article. To check if the method works in other fields, we must first publish it, so that other researchers can use it on their data. This is precisely why we are submitting the article to <italic>eLife</italic>.</p><disp-quote content-type="editor-comment"><p>2) The behavioral results:</p><p>The reviewers were not convinced by the pitch results, primarily because of the coarse-graining of pitch into just two categories. Again, much of the explanation was relegated to the Methods part of the paper and could have been presented more clearly. If this method works, this is clearly interesting, but the results presented seemed rather weak.</p></disp-quote><p>We thank the reviewers for directing our attention to this. As the reviewers rightly point out, uBIA uses discretized measures of motor output, and in some cases such discretization will discard relevant behavioral data, as when a (continuous) measure of vocal pitch is discretized into two categories. However, as explained below this coarse-graining does not limit the usefulness of the uBIA approach, either in the specific case of the songbird vocal motor system or in its usefulness to biologists more generally. In the case of the songbird data, Dr. Nemenman and Sober’s prior work (Tang et al., 2014), which analyzes the same dataset, has demonstrated significant mutual information between behavior and millisecond-scale spike timing in neurons of the songbird motor cortex, even when the continuous pitch variable was discretized into just two categories. Moreover, that same paper demonstrated similar statistical dependencies when pitch was discretized into 3, 5, or 8 groups, showing that information between millisecond-scale (cortical) spike patterns and a coarse-grained representation of behavior does not depend critically on the details of the discretization.</p><p>More generally, the loss of resolution due to the (artificial) discretization is inevitable. However, it would be hard for the discretization to create new codewords – by discretizing, we largely lose codewords, for which the behavior is mapped into a single bin. We believe that this is acceptable – the method does not claim to discover all codewords (this would require infinitely large datasets!), and so, as long as we keep making the same error again and again (deliberately missing some possible keywords), and still come out with relatively large dictionaries reconstructed, the approach remains useful. In addition, one can partially address this concern by trying different binarizations. Specifically, in the last part of the paper, we did just that – binarizing the data into different groups, and hence exploring the exploration-exploitation tradeoff. We now emphasize this in the text, in the header of the “Reconstructing songbird neural motor codes” section and at the end of the “Dictionaries for exploratory vs. typical behaviors” section.</p><disp-quote content-type="editor-comment"><p>3) The format of the paper:</p><p>The paper read a bit like 3 manuscripts glued together, each of which has merits, but none of which seemed to satisfactorily stand on its own. The paper starts with a grand introduction that talks about solving a major challenge in quantitative biology. Then it quickly dives into something that reads more like a methods paper, but with insufficient exposition and clarity. Finally, the paper shifts to the application of the method to behavior and neural data, the results of which seemed somewhat weak and preliminary.</p></disp-quote><p>We agree with these concerns, and we took them seriously. To respond to them, we have completely changed the paper structure, and the paper is now a methods paper. We think the exposition has improved because of the change, and we hope that the reviewers agree.</p><disp-quote content-type="editor-comment"><p>Taking these critiques together, the reviewing team felt that the paper could not be reasonably revised for eLife as a Research Article. However, the team did find the central method potentially interesting if more details are provided that allow for proper assessment of it. The team felt that it could be a good &quot;Tools and Resources&quot; advance, if the method holds up to this more detailed scrutiny.</p></disp-quote><p>Thank you. This is a great suggestion, and we are resubmitting the paper as a “Tools and Resources” article here.</p><p>Finally, we also change Figure 4 to make it easier to read.</p><disp-quote content-type="editor-comment"><p>Reviewer 2:</p><p>The paper is a potentially interesting application of a recent statistical method, the Bayesian Ising Approximation (Fisher and Mehta), aimed at addressing the problem of estimating a complex distribution, in particular the relationship between a code and an output, from very few samples. This is accomplished by heavily weighting by the prior. The authors have applied this method to support the idea that there are precisely timed spike patterns that are especially significant in the generation of behavior, in this case in the output of syllables of birdsong. They use the method to identify &quot;code words&quot;, patterns that are claimed to appear with notably high or low frequency in correspondence with a behavioral variable. They apply this method to find a larger number of precise spike patterns correlated with behavioral variations in the tails of the behavioral distribution. They suggest an interpretation that perhaps larger exploratory motor gestures require a more precise spiking code; although it might also be argued that this shows that there are more ways to achieve these larger fluctuations? While the claim is counterintuitive, it rests on the power of the method, whose exposition left a number of gaps.</p><p>1. It is unclear to what extent this goal is meaningful in the case that the paper analyses: premotor drive of muscles in the birdsong pathway. It seems evident that timing of RA spikes necessarily sculpt motor output; and the authors have previously analyzed this in detail to reveal timing precision (Tang et al.). It was not clear, however, that specific patterns meaningfully emerge from a sea of similar spike trains to encode exact outputs. The authors ask whether precise patterns in 2ms bins have a significant relationship with binarized song characteristics in 40ms bins-a very low fidelity relationship on the &quot;behavior&quot; side compared with a high level of precision on the neural side. This seems a dramatic discrepancy between the dimensionality of neural response vs behavior that deserves more justification. How general a space of coding relationships (nonmonotonic, etc) does this method explore given this binarization?</p></disp-quote><p>We answered this question in detail above (see “(2) The behavioral results” in our reply to the Editors above). Briefly, by discretizing, we certainly lose a lot of possible coding relationships – but we also lose many of them simply due to the finite size of the data sets. These are the errors that everyone working in the field has to agree to make: it is ok to lose codewords, as long as we do not introduce many spurious ones. Finally, exploring different discretizations certainly increases the diversity of the codes we can detect – and we do, indeed, detect nonmonotonic codes in the section on exploratory vs. typical behavior.</p><disp-quote content-type="editor-comment"><p>2. Further, it was unclear whether a temporally structured firing rate would be equally convincing as a representation of behavioral variation. While the concept of &quot;irreducible dictionaries&quot; is appealing, does this method fairly compare a timing code to a rate code? For example, doesn't the competition component of the model suppress the identification of firing rate-like contributions? Looking at Figure 1, the cartoon of eliminating overlapping but similar code words seem equivalent to ignoring a less temporally precise method of coding for behavior. That is to say, if you are forcing your model to choose one of several similar code words in a winner-take all inclusion choice, isn't the timing precision you see partially a factor of not looking at the other similar code words which didn't win out?</p></disp-quote><p>Thank you for this important question. Parenthetically, our method actually allows for partially overlapping words, as the dictionaries shown by tic marks in Figure 5 illustrate. However, our main response to this comment is that the framing of the time-rate debate by the reviewer is inconsistent with our view of the matter. Of course, both the rate and the precise timing contribute to encoding information about the ensuing behavior – the question is how much is contributed by each, and how. As we tried to elucidate in our recent review paper (Sober et al., 2018), we view neural codes on a two-dimensional continuum, where temporal precision is on one axis, and the order of the code is on the other (see Figure 2d of Sober et al., 2018 for a reproduction of the argument from the above mentioned review). While allowing the rate code to change the rate over orders of magnitude on a millisecond scale would introduce the millisecond-level precision, it would still be a first order code, not requiring to know the neural activity at two instances to predict the behavior. To the extent that most of the codewords that we observe involve multiple spikes, cf Figure 5c, the codes are not simple first order rate codes – they are pattern codes, with temporal resolution of a few ms. We have checked and correct the entire manuscript so that nowhere in the manuscript we contrast timing to rate, but instead we focus on the difference between rate and timed patterns code.</p><disp-quote content-type="editor-comment"><p>The comparison of this method to GLM methods of relating the spiking to behavior is unclear. One alternative model was analyzed in the online methods, but is not clearly explained and such comparisons deserve to be treated in the main text. It should also be easy to report the simple 0th order analysis of how well you can predict the binary version of behavior simply with a spike count within the 40 ms window, or with some intermediate level of temporal variation. Seeing these comparisons as a control of the assertion that precise timing matters in this system would enhance intuition for the necessity of this method and better justify the claim &quot;that they predict behavioral features better than other approaches&quot; (line 256).</p></disp-quote><p>Thank you for this important question. As we showed in Tang et al. (and mentioned in the manuscript), there is essentially no information between the spike count at 20+ ms resolution and the pitch, which makes making such comparisons not illustrative. Crucially, as we now stress throughout the manuscript, and in particular in “Overview of prior related methods in the literature” (see also our response to the introduction of the Editorial Decision letter), uBIA does not build a generative model of the underlying distribution; it merely detects which features need to be part of the model, and after that one can build the model using any number of other methods, including GLMs. In other words, uBIA doesn’t compete with (and should not be compared to) other methods, and should be viewed as a complementary technique.</p><disp-quote content-type="editor-comment"><p>3. In general, the exposition of the method leaves many questions and could be improved considerably in clarity. I have tried to summarize the logic of the paper's core argument below and underscored throughout where more help could be given to understand the meaning and generality of assumptions made. Logic of the calculation:</p></disp-quote><p>We agree that the original manuscript has left a lot to be desired, and we hope that the extensive changes we have made to the manuscript have solved these problems.</p><disp-quote content-type="editor-comment"><p>The paper sets up the problem of writing down a model for the joint probability of a behavioral event s0 and the firing of a spike in each, i, of N bins, σi. The authors write this in terms of an expansion over every possible combination of σi, Log P(σ|θμ) = -log Z + Σ θμ Π _{i in Vμ} σi.</p><p>Q3.1: Please expand a little on the exponential form. In maximum entropy approaches, this form of distribution arises from the maximum entropy solution, but if we are avoiding MaxEnt approaches, can you briefly justify it?</p></disp-quote><p>This is the most general form of the underlying probability distribution, and it does not lead to any loss of generality. We now emphasized this in the text before Equation (1).</p><disp-quote content-type="editor-comment"><p>In this approach, a firing pattern with spikes in bins (1,2,3,4) is considered to be independent of the events (1,2) and (3,4); whereas in maxent, if one is only expanding up to order 2, one hypothesizes that the probability of the word (1,2,3,4) will be absorbed into the pairwise probabilities, and one can ask how different the data is from that guess. It seems that here one is parametrizing with vastly more parameters-one for every possible spike/behavior combination-- without a systematic method of cutting them down (or at least one that was clear to me; it is addressed at the end of the Online Methods calculations once we have moved into the s variables but it would be helpful to get some intuition from the start).</p></disp-quote><p>This is not quite correct – the whole point of the method is that one should (and does) consider all possible combinations, but then one systematically evaluates the evidence for needing to include a certain combination into the probability distribution. We hope that the current revision, which has moved the appendix into the main text, has resolved this ambiguity. Specifically, the paragraph before Equation (2) has been revised to address this question as well.</p><disp-quote content-type="editor-comment"><p>One then defines &quot;indicator variables&quot; sμ which tell us which patterns appear significantly less or more often than one would expect under some null model. It would be good to specify this null model right away, and in straightforward terms; I don't see it defined in the main text at all.</p><p>Q3.2: Please explain at this point the null model for the frequency of appearance of any possible pattern of spikes and its correlation with behavior? Should it not depend on the specific behavior?</p></disp-quote><p>In the revised manuscript, we now explicitly define the null model after Equation 3. Briefly, the null model matches the firing rates in all bins to experimental observations.</p><disp-quote content-type="editor-comment"><p>It is unclear why one is now allowed to write that the original definition of the model is EQUAL to the model now written including only terms defined by a criterion defined by our subjective interest in them: Log P(σ|θ) = -log Z + Σ θμ sμ Π _{i in Vμ} σi</p><p>Q3.3: Why does one not here need to define a new and different probability distribution, something that makes some claim about belonging to the codeword dictionary, in which these terms are the only ones that appear?</p></disp-quote><p>Thank you very much for this critique. Indeed, what’s written in Equation (2) is the same distribution as in Equation (1), following a series of algebraic transformations. We now stressed it in the text before Equation (2) that no new quantities are being defined.</p><disp-quote content-type="editor-comment"><p>Now the authors turn to the task of estimating the 2^N parameters θμ. Instead of doing this estimation directly from the data (i.e. the a posteriori expectation), they use a prior on these values: they choose to assume that each of the values are narrowly distributed about some mean θ*μ: P(θ*μ : sμ=1) ~ exp(-(θμ −θ*μ)2). This distribution is taken only to hold if the word is indeed a code word, ie statistically associated with a behavior. If it is not a code word, the prior is not defined.</p><p>Q3.4: Please justify this choice in more detail. Why are the epsilons not dependent on μ?</p></disp-quote><p>It would be impossible to obtain a simple a posteriori expectation here – with 2<italic><sup>N</sup></italic> equal to about 2<italic>,</italic>000<italic>,</italic>000 in our example, and with only a few hundred samples, the uncertainties would be too large. This is where Bayesian methods are useful and needed. We explained in the revision why we made this specific choice of the prior. We would like to point out here that we do not define the prior for non-codewords simply because it doesn’t matter – those words won’t contribute to the distribution at all.</p><disp-quote content-type="editor-comment"><p>Q3.5: The argument for where these 2^N values of θ*μ come from is very unclear. Please spell out more clearly how constraining the means of the individual σis provides sufficient constraints for all 2^N values, or why this is not needed.</p></disp-quote><p>There are 2<italic><sup>N</sup></italic> values of <italic>θ</italic>s because there are 2<italic><sup>N</sup></italic> different inclusion/ exclusion combinations of <italic>N</italic> spins. The constraining on the rates (means) does not provide sufficient information for all 2<italic><sup>N</sup></italic>. This is, in fact, the point of the algorithm: there’s no way to determine 2<italic><sup>N</sup></italic> values from small data sets, and we thus need to do something else. Specifically, we only ask a question of which <italic>θ</italic>s are statistically significantly nonzero, and hence should be included as codewords, but we never try to determine their values directly. We hope that the current revision makes this point clear.</p><disp-quote content-type="editor-comment"><p>Q3.6: Why does a code word have an a priori likelihood of 0.5 of being in the dictionary?</p></disp-quote><p>As in any Bayesian analysis, we need to define a prior, and we choose the least informative 50/50 prior in our case – a priori we do not know whether a particular codeword is used or not used by the animal. If we had a reason to believe a priori that a certain word should or should not be included in the dictionary with a different probability, then that different prior could be used. We now explain this in the text in the paragraph following Equation (3).</p><disp-quote content-type="editor-comment"><p>Now rather than estimating or computing the values of the θμs, the next step is to flip the variables in the distribution we seek, to P(s|theta), and then integrate out the thetas, based (it seems?) only on their prior distributions, Equation (5).</p></disp-quote><p>This is not entirely correct. We are interested in the posterior distribution of <italic>s</italic> given the observed data, and integrating out <italic>θ</italic>. We choose to integrate <italic>θ</italic> out since, as we argued before this equation, the posterior expectation of <italic>θ</italic> cannot be determined with a reasonable accuracy from datasets of realistic sizes. Here, crucially, the a priori expectations an<italic>d</italic> data get combined, and the integration is not controlled just by the prior distributions.</p><disp-quote content-type="editor-comment"><p>Q3.7: Now that s can again take value of either 0 or 1, why did we not need the distribution over θs in the case that s = 0? How can a distribution over s on the LHS in Equation (5) limit itself to the case of s = 1 on the RHS?</p></disp-quote><p>It’s not that we do not need the distribution of <italic>θ</italic>s for <italic>s</italic> = 0, it’s that the distribution does not matter: if <italic>s<sub>µ</sub></italic> = 0, the corresponding <italic>θ<sub>µ</sub></italic> has no effect on the distributions of data. We hope that this has become clearer in the current revision.</p><disp-quote content-type="editor-comment"><p>The calculations in the online methods may resolve some of these issues? But these all appear to start from Equation (5) and at least for me at this stage, the route to this equation is unclear.</p></disp-quote><p>We hope that this has become clearer in the current revision, which included a major restructuring of the paper.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The manuscript suggests a new approach to learning coding dictionaries. The method is applied to neural population data recorded in singing birds. While the idea of the approach is interesting, it was hard to understand the details or how well it works. It seems that too much has been pushed to the SI, and more controls and examples over synthetic data sets would be useful. Moreover, and importantly – it was hard to interpret the results. Also, the current approach was not compared against other methods presented in recent years for learning metrics on neural population patterns, or even more simple clustering approaches.</p></disp-quote><p>We have substantially revised this manuscript to address this concern, and discuss this in the “Overview of prior related methods in the literature” section, as well as in the reply to the editorial decision above. In particular, we added a description emphasizing that our method does not compete with most others in the literature, and cannot really be compared to them, as the goals are different. Indeed, uBIA learns dictionaries – in other words, it detects which words are codewords. Most other methods, in contrast, take a predefined set of putative codewords, and then build a generative model based on them. While usually the set of putative codewords is defined as words with some number of spikes, nothing prevents to use these other methods in conjunction with uBIA, so that uBIA first detects the codewords, and other methods are then used to build a model based on them.</p><disp-quote content-type="editor-comment"><p>Briefly, the authors use a log-linear model for the population words of N neurons and two behavioral states – using 2^(N+1) terms that span all potential orders of interactions among cells (theta's). To avoid arbitrary cutoff on the order of interactions that they consider, and use the important ones, they use a constructed prior distribution over theta's to learn a posterior model over the population words after integrating out the theta's. This gives an Ising model over 2^N interacting 'spins' where each sping now stands for a word that may or may not have appeared in the sampled data. The local fields acting on each of these spins is related to the prob of appearance of the words the spins stand for, and the pairwise interaction terms stand for an overlap between the words. These interactions now imply a 'competition' between words in terms of the explanatory power they carry (and are therefore typically negative).</p><p>Thus, if I understand this correctly, the constructed model gives a form of compression of the population 'codebook' by picking words that are dominant to be included in the dictionary and omitting similar words with less explanatory power and frequency.</p></disp-quote><p>This is, indeed, a reasonable summary of our paper. We would like to point out, however, that compression is not the primary goal here, and it emerges naturally, not at the expense of the quality of the fit to the data. In other words, if there is sufficient evidence for even overlapping words to be included in the dictionary, then both will be. Finally, some words – especially non-overlapping ones – may have positive interactions, promoting rather than inhibiting each other.</p><disp-quote content-type="editor-comment"><p>Critically, it was unclear to me how one should evaluate the results. It is hard to interpret the structure of the dictionary in Figures4-5, and in particular over the pooling of conditions in Figure 5.</p></disp-quote><p>We thank Reviewer for these comments. Some of them were answered above in our response to the editorial summary questions, and we refer the Reviewer there, to our response to “The behavioral results” editorial comments. In addition, here we point out a few more things. First, consistent with our resubmitting this paper as a “Tools and Resources” contribution rather than a research article, we have de-emphasized the interpretation of the birdsong data and focused on the mechanics the method itself. Second, it is simply not possible to do give a full interpretation requested by the Reviewer, without pooling, since we lack tools to collect the amounts and the types of data we would need. In particular, pooling is done, in part, to overcome the statistical power problems caused by small datasets. Further, the ultimate test of any method for dictionary reconstruction must be causal stimulation: one needs to build a model based on the detected codewords, predict responses to possible experimental perturbations, and then apply the perturbation and compare the results to the predictions. We do not do this here, but neither do the absolute majority of other publications in the field (our previous publication on bird breathing is one of the few exceptions, and the goal is to do something similar in the current system as well, but this requires development of new experimental methods). We already did what the best publications in the field do – we build the dictionaries, and then used them, in the “Verification of the inferred dictionaries”, section to build a simple statistical model of the code, and we showed that this model outperforms at least some competitors. Beyond this, we do not know how to provide a more useful biological interpretation. However, again, the same concern is applicable to essentially any other publication in the field.</p><p>As argued in the previous paragraph, pooling of conditions in Figure 5 (now Figure 6), indeed, adds an additional complexity to interpretation of individual dictionaries, as using a pooled dictionary to predicted pooled data would certainly be pointless (this would be like trying to translate from a corpus of books written in different languages using a mixture of dictionaries that translate between different pairs of languages). However, what would make more sense in such pooled books analogy – and is precisely what we do for the birdsong data – is to ask questions of the type: how are Romance languages different from Germanic ones? Are the words of similar length? Are roots similar? etc. We hope that this analogy is illustrative for the Reviewer.</p><disp-quote content-type="editor-comment"><p>In addition, there have been different approaches towards learning neural dictionaries in recent years, which are not mentioned here or compared against the current approach. Maybe this is too naïve, but it seems natural to ask how well would naïve clustering or Edit-distance based methods work here? Then, how does the current approach compare to more structured metrics such as those presented in Ganmor et al. eLife 2015, Prentice et al. Plos Comp Biol 2016, Rubin et al. Nature Comm, 2019, or Chaudhuri et al. Nature Neuro, 2019?</p></disp-quote><p>We now review the relation of all of the methods mentioned by the reviewer to uBIA in “Overview of prior related methods in the literature”. Here we also add that, as we now emphasize in the manuscript, our method is complementary to the ones mentioned by the reviewer, and does not compete with them. These methods can be used to reconstruct models of the neural code <italic>after</italic> uBIA detects which keywords should enter the model.</p><disp-quote content-type="editor-comment"><p>Finally, how would approach work for larger populations, where all observed patterns would appear just once?</p></disp-quote><p>Performance of uBIA will certainly degrade in this situation. However, no method will work well then, so that this cannot be viewed as a something particularly bad about uBIA. That said, uBIA has two major strengths in this regime, which are now emphasized in the manuscript. First, uBIA analyzes not just patterns, but all sub-patterns. In other words, as long as there are even partial coincidences among the spiking patterns, they will be detected, and these coinciding sub-patterns will become keywords in the dictionary. It is clear that at least some short sub-patterns must coincide in any reasonable dataset – e.g., there are only <italic>N</italic> first order words formed by <italic>N</italic> interacting units – and any realistic dataset will have, at least, Μ»Ν, ensuring that, at least, such simple one-spike (and maybe even many two-spikes) words have good statistics behind them. Second, uBIA detects not just over-representation of patterns, but also their under-representation. Thus, if a complex spike word happens once (or does not happen at all), while it should happen many times based on its composition from small sub-patterns, such word could enter the uBIA dictionary as well, with its absence potentially encoding the behavior.</p><disp-quote content-type="editor-comment"><p>It was not clear how should one interpret the nature of the organization of patterns in Figure 4B (the gap between lines, their very linear dependency?)</p></disp-quote><p>In Figure 4B (now 5B), the reason that there are no points near <italic>x</italic> = <italic>y</italic> (in other words, there is a gap there) is because <italic>x</italic> = <italic>y</italic> means that the observed frequency of a spike codeword is roughly the same as its frequency expected from the observation of its various subparts. Then, by definition, it isn’t significantly under/overrepresented. The width of the gap around the diagonal is determined by various significance threshold, which we set as explained in the text.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The numerical comparison to other existing methods is minimal and should be expanded.</p></disp-quote><p>We have argued in our previous submission that there really are no other methods to compare to, designed to work in the regime similar to uBIA. It seemed to us that it would be unfair to run other methods on our datasets, see them not work well (as expected – because they make assumptions that are invalid in our regime), and then claim success. However, since the concern has been raised again, we really have to address it. To do this, we added a section in the <italic>Online Methods</italic> “Direct application of MaxEnt methods to synthetic and experimental data”, in which we compare uBIA to the relevant interactions model of Ganmor et al., with which uBIA has the highest similarity. The results are as expected – a method not designed for our data regime fails. We emphasize here again that the relative superiority of uBIA on these data should not be taken as a slight directed at other methods, but rather as an indication than, to cover different data regimes, multiple methods should be combined. We emphasized this in the “Overview of prior related methods in the literature” supplemental section.</p><disp-quote content-type="editor-comment"><p>The analysis of the irreducibility of the codewords has insufficient support based on the numerical simulations. Moreover, the generality of the tool and comparison to other methods are discussed in almost entirely theoretical terms, which makes the claim on immediate utility for other datasets less convincing, especially outside the neuroscience community. In particular, the identified codewords are relatively short (3rd, 4th order stats) and N=20 is easy to fit an Ising model; so one would think that it would be relatively straightforward to fit the Ganmor model jointly.</p></disp-quote><p>We hope that the addition of the new comparison figure (described above) partially alleviates these concerns. Additionally, we point out that 3rd and 4th order words are long, as most others deal with just pairs, as illustrated in the new Figure 7. Indeed, it is not easy to fit an <italic>N</italic> = 20 Ising model with 4th order terms, because there are 20 ∗ 19 ∗ 18 ∗ 17<italic>/</italic>(4 ∗ 3 ∗ 2 ∗ 1) = 4845 terms in this model, which cannot be fit from just a few hundred samples, which is precisely why the Ganmor model fails in this case (Figure 7).</p><disp-quote content-type="editor-comment"><p>At least on artificial data one should be able to explore a larger N regime where there is a more immediate gain relative to existing models.</p></disp-quote><p>Indeed, in the newly-added Figure 7 we explored dataset sizes up to <italic>M</italic> = 10<sup>5</sup>, showing that uBIA begins detecting interactions with datasets orders of magnitude smaller than those required by alternative methods.</p><disp-quote content-type="editor-comment"><p>2) Although the paper is written as a methods paper, emphasizing the technical contributions and promising wide applicability to a range of different types of datasets, the numerical validation of the method is very much restricted to the statistical regime of the songbird dataset. From the perspective of a potential future user of the tool it's less clear how the method would behave on different datasets, and what needs to happen in practice for adopting the tool to data with different statistics. Based on current content, it would be better to focus the abstract and introduction on applications that are actually studied here.</p></disp-quote><p>We have edited second half of the abstract and a few sentences in the Introduction (see latexdiff file) to make it clear that our main applications to date have been to songbird data.</p><disp-quote content-type="editor-comment"><p>3) The songbird analysis already reveals some challenges with respect to interpretability: in particular it is not clear how much information about the underlying neural processes can be revealed by summary statistics generated by the method, such as the number of codewords and their length distribution.</p></disp-quote><p>The reviewer is correct that our analysis of the songbird data raises a number of important questions for future studies. Although these remain to be answered, we emphasize that before the biological interpretation of over/underrepresented neural patterns can be attempted, such patterns must first be identified. uBIA therefore represents a crucial advance in our ability to address these questions.</p><disp-quote content-type="editor-comment"><p>4) Limiting the analysis to N=20 patterns with somewhat random code structure makes it hard for a potential user of the tool to work out what needs to change when switching to a novel dataset (with respect to N, M, codeword complexity, sparsity of neural responses). Some work could be invested in spelling out the considerations of applicability with respect to quantities that an experimentalist would know about the data.</p></disp-quote><p>We agree with the reviewer and have added a discussion of how to set hyperparameters (second-to-last paragraph of the revised Discussion).</p><disp-quote content-type="editor-comment"><p>5) Is it possible to include multiple binary quantifications of behavior, similarly to how words are constructed from neural spike trains? For example, one can envision describing a particular song segment with respect to multiple binary features simultaneously.</p></disp-quote><p>We explicitly examine this question in “Dictionaries for exploratory vs. typical behaviors” and the corresponding Figure 6, which repeats our analysis for different binary discretizations of our behavioral data.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Line 168: I think the notation here should P(s_mu=0) not s_mu=-1.</p></disp-quote><p>Thank you, this has been corrected</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I am overall very excited about the framework and I see significant technical potential in the approach. Nonetheless, there are several missed opportunities in the numerical analysis, and things that could and should probably be improved to maximize the impact of the work on the broader community.</p></disp-quote><p>We thank the Reviewer for their generally positive comments.</p><disp-quote content-type="editor-comment"><p>– Limiting the analysis to N=20 patterns with somewhat random code structure makes it hard for a potential user of the tool to work out what needs to change when switching to a novel dataset (with respect to N, M, codeword complexity, sparsity of neural responses). Some work could be invested in spelling out the considerations of applicability with respect to quantities that an experimentalist would know about the data.</p></disp-quote><p>Please see our response to this issue above under “Essential revisions.”</p><disp-quote content-type="editor-comment"><p>– The codeword irreducibility could be analyzed in more detail, again in terms of experimentally relevant quantities.</p></disp-quote><p>Please see our response to this issue above under “Essential revisions.”</p><disp-quote content-type="editor-comment"><p>– The comparison to other methods is minimal. The identified codewords are relatively short (third, fourth order stats) and N=20 is easy to fit an Ising model; so I'd think that it would be relatively straightforward to fit the Ganmor model jointly. At least on artificial data one should be able to explore a larger N regime where there is a more immediate gain relative to existing models.</p></disp-quote><p>Please see our response to this issue above under “Essential revisions.”</p><p>[Editors’ note: what follows is the authors’ response to the third round of review.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Thank you for the revised manuscript. The reviewers found the manuscript much improved and close to acceptance. Nevertheless, reviewers are suggesting a few additional analyses to demonstrate the advantages of the method and increase its impact.</p><p>1) Please consider the null model that includes pairwise correlations.</p></disp-quote><p>This is not a simple request. For starters, most methods that aim to reconstruct features of multivariate probability distributions from data use the product of marginal distributions as the null model. In this context, our approach is consistent with the best practices. Further, if we were to follow this request, we think that this would create more issues than it would solve. In the regime we operate in, where the number of pairwise correlations is only a few times smaller than the dataset size, any method for constructing a null model that respects pairwise correlations in data would result in a null model that itself has substantial error bars. In our subsequent estimate of deviation of frequencies of various patterns from the null model, we would then be hampered by these large error bars, and whether some words should be or shouldn’t be included into the dictionary would depend on these fluctuations. To be sure, we agree that extending our method to a different null model is a worthy research project – but this is a research project that will take months of work to do right, and we would prefer to publish the current paper first, before studying such complicated extensions.</p><p>In summary, consistent with the second decision letter, we chose not to perform these additional analyses. However, in the revised Discussion, we now suggest that this expansion of the algorithm would be a worthy goal, but also argue briefly that it is not an easy task.</p><disp-quote content-type="editor-comment"><p>2) Please test the method to a larger dataset.</p></disp-quote><p>Again, while we appreciate where the comment is coming from, we would like to push back on this. Our understanding is that by “larger dataset”, the Reviewers mean larger <italic>N</italic>, the number of units, rather than <italic>M</italic>, the number of samples, and they are requesting us to list which specific changes to the algorithm’s parameters we need to adopt in this case. Our concern is that we already provided a recipe for parameter adjustment: count the number of patterns that are identified as predictive of both presence and absence of behaviors. Those are obviously wrong, and parameters should be set to avoid them. Any more specific suggestions will depend on not just the size of the dataset, but also on how many patterns really contribute to the underlying distribution, and what is the distribution of the coupling constants associated with these patterns. While doing some runs over the dataset size is relatively easy, we do not see this providing a substantial insight, since all of our findings would be specific to the statistical properties of the data we analyze. In other words, we would like to request the permission to publish the manuscript with no additional simulations of larger datasets. When published, we will then focus on adapting the methods to specific datasets of interest to us and others. For example, one such interesting dataset comes from recording multiple single motor units in marmosets. There <italic>N</italic> is naturally larger, so that we will be further developing our methods for a concrete application, rather than for solving toy models.</p><p>Nonetheless, we took this request very seriously. Now there are two additional paragraphs in the Discussion, which focus on how one would set the detection threshold and the inverse regularization strength, the two main hyperparameters of our algorithm.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I generally find that the revisions have improved the overall clarity of the text, and addressed most of my questions.</p></disp-quote><p>Thank you! We are glad to hear this.</p><disp-quote content-type="editor-comment"><p>Nonetheless, I found the answer to two of my concerns underwhelming:</p><p>– Discussing the practical considerations about translating the approach to other datasets (new discussion paragraph) is quite vague, essentially saying that the hyperparameters will more or less have to be figured out de novo. I was expecting a quantitative description of how different statistical properties of the data affect the process, which would translate into something closer to a concrete recipe for hyperparameters adjustment. The fact that this is not straightforward to do makes a broad adoption of the tool less likely.</p></disp-quote><p>As we tried to emphasize in the response to the Editors above, we do not expect there to be a practical guide at this point. We have analyzed many datasets in our careers, and we firmly believe that interesting discoveries require one to operate at the edge of applicability of any method, so that no standard, simple prescription would work. In our specific case, there are simply too many parameters to explore: the number of measurements <italic>N</italic>, the number of units <italic>M</italic>, the number and the order of the true words contributing to the dictionary, the statistics of the coupling constants associated with these words, and so on. We do not expect there to be simple heuristics, and the Reviewer is absolutely correct that one will have to work hard to adjust the algorithm to each new qualitatively different dataset.</p><p>The Reviewer is rightly worried that this may decrease the adoption of the algorithm, but we believe that this is not going to be the case. Indeed, as a part of the Simons-Emory International Consortium on Motor Control, we ran training workshops to disseminate the method in the community. The workshops were attended, cumulatively, by many dozens of trainees from within and outside the Consortium. That is, the method is being adopted by a number of labs. This has created a weird dynamics where these labs, in their own publications, want to refer to an article with the original description of the method, and this article (the current one!) is not yet published.</p><p>In other words, we would prefer to publish the manuscript now with minimal changes and explore different data regimes in future work. That said, we added a shortened version of the above arguments, and a succinct description of how one should set the hyperparameters of the algorithm to the Discussion section of the manuscript. Specifically, there are now two new paragraphs where we focus on setting correct values for the detection threshold <italic>m</italic> and the inverse regularization strength <italic>ϵ</italic>, the two most important hyperparameters of uBIA.</p><disp-quote content-type="editor-comment"><p>– The concern about the overall interpretability of the extracted statistics remains unaddressed. I don't agree with the position is that &quot;it only matters to estimate these statistics well, interpretation can come later.&quot; Being able to estimate a quantity does not necessarily make it useful to do. In the context of songbird specifically, the paper make the case for the results being difficult to interpret even when the statistics are well estimated. This seems like a pretty big problem, at least big enough to warrant a minimum amount of thought and a couple of sentences in the discussion.</p></disp-quote><p>We agree with the general sentiment and address the Reviewer’s comment in two ways. First, we would like to point out that every statistical analysis approach will suffer from the same problem: statistical significance is not the same as biological significance. The best way to interpret statistical significance is through perturbation experiments, as we now explain in the revised “Validation of the inferred dictionaries” section. However, doing such experiments is beyond both the scope of the paper and the limit of current experimental technology, since precisely stimulating individual neurons in behaving songbirds is not yet achievable. Therefore, the field is unable to fully validate uBIA (or, really, any correlation-based analysis) using perturbations.</p><p>Second, we have added a paragraph to our discussion of the interpretability of the spiking patterns identified by uBIA. The revised “Dictionaries for exploratory vs. typical behaviors” now ends with a paragraph that includes an expanded discussion of the role that brain nucleus LMAN might play in motor exploration (per the suggestion of Reviewer 2 in their recent comments).</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The authors already made a great effort to improve their manuscript. However, from my side, I provide the following concerns about the significance and broad impact of the current revised manuscript.</p></disp-quote><p>We thank the Reviewer for these helpful suggestions.</p><disp-quote content-type="editor-comment"><p>Strength of the manuscript</p><p>The traditional model of maximum entropy type requires a large amount of data to predict effective interactions among elements (e.g., neurons), and fails to identify statistically over- or under-represented (relative to a null model) codewords, and thus the neural-behavior prediction remains elusive. This manuscript provides a novel approach that can systematically search for the significant codewords, yet requiring fewer data samples (compared to the number of higher-order model parameters).</p></disp-quote><p>Thank you, we agree with this assessment.</p><disp-quote content-type="editor-comment"><p>The method first writes all orders of interactions in a probability distribution form; finding non-zero higher order interaction parameters could tell which codewords should be included in the neural dictionary and how they matter. To make the method algorithmically solvable, the authors turn the optimization into an Ising model of interacting indicator variables. This indicator variable is responsible for identification of key codewords. The inferred value of the external fields signals the importance of the codeword, while the sign of the high-order interaction parameter reflects whether the word is over or under-represented. To explain the neural-behavior correlates, the authors also include the behavior variable into the components of the activity. This framework works consistently in the songbird motor experiments.</p></disp-quote><p>Thank you for this correct summary.</p><disp-quote content-type="editor-comment"><p>Weakness of the manuscript</p><p>To relate the neural activity and behavior, the authors test their method in both synthetic data and songbird real data. There are two weakness, considering the impact of the method in a broad context. First, the variable size is limited to N~20, which is severely below the number of neurons the neural data scientists get interested in (e.g., for modeling retinal ganglion cells, and even cortical cells). This may be a tradeoff between all order of interactions to be considered and the number of model parameters.</p></disp-quote><p>Indeed, this is a tradeoff between being systematic in detecting codewords of all orders, and the dataset sizes. Systematic analysis of codewords to all orders will necessarily require smaller <italic>N</italic> at the same number of recordings, <italic>M</italic>, compared to methods that focus on linear or, at most, pairwise codes. That said, we also point out that in many applications, specifically in recording from motor units, <italic>N</italic> of just a few is the state of the art – recording even from a handful of units at the same time is already hard. This and similar contexts is what our methods are devised for, as not all of biological data have become as high dimensional rapidly as, say, neuropixels. We have introduced the relevant discussion in the Discussion section of the manuscript, as well as in the synthetic data sections, where we explain why we chose to focus on datasets with specific properties.</p><disp-quote content-type="editor-comment"><p>Second, a null model is chosen to be an independent model that neglects correlations in the data, which may affect the characterization of the redundancy of the uBIA model. Even if an irreducible ensemble of codewords is obtained, what is the predictive power of this ensemble with respect to control the macroscopic behavior (e.g., skilled motor control or sensorimotor learning), rather than showing statistics of neural-behavior activity, remains obscure in the current manuscript.</p></disp-quote><p>These are absolutely valid concerns. However, as we pointed out above, the same questions can be asked of essentially all neural motor control papers. We are eager to publish this manuscript precisely for the reason that it’s time to leave the development of statistical methods behind us, and to focus on the biology that can be discovered by applying them to learn new biology. As we explained above, we introduced a few sentences to discuss the interpretability questions in the sections on experimental data.</p><disp-quote content-type="editor-comment"><p>Considering the strength and weakness of the manuscript, I recommend the following issues for the authors to improve their idea.</p><p>1. The proposed model is limited to symmetric couplings and i.i.d data samples. First, in a neural circuit, the asymmetric coupling is common among neurons; Second, the temporal order of each spiking activity may play a key role in encoding information. For example, a paper &quot;Blindfold learning of an accurate neural metric&quot; (PNAS, 2018) takes the temporal information into account. When putting in the context of neural-behavior relationship, these two factors would become important. But they seem neglected in the current manuscript.</p></disp-quote><p>The Reviewer is absolutely correct that temporal correlations matter, that (symmetric) statistical correlations are not the same as (asymmetric) neural interactions, and so on. However, any statistical analysis method must have a well delineated domain of applicability. There are certainly a lot of scenarios where symmetric correlations without time ordering are sufficient to answer questions. Our approach focuses on those. We added these caveats to the Discussion, when we summarize pros and cons of the method. However, we chose not to reference the above mentioned PNAS article, as it is only tangentially related to our work in our opinion.</p><disp-quote content-type="editor-comment"><p>2. To achieve a higher-order-interaction-included and non-redundant model is really important problem in the field. For example, the reliable interaction model proposed in ref. 26 is not fully compared in the current manuscript, i.e., what is the new advance?</p></disp-quote><p>Please see section “Direct application of MaxEnt methods to synthetic and experimental data”, which deals with comparisons to the Ganmor et al. work.</p><disp-quote content-type="editor-comment"><p>This is better to clearly demonstrated in a plot in the main text.</p></disp-quote><p>As we have discussed above in the response to the Reviewer 2 minor comment about the structure of the paper, we strongly believe that this comparison should be kept out of the main paper text. The main reason is that, as we have argued consistently, the Ganmor approach and uBIA are not competing, but are complementary: uBIA identifies the dictionary, and the Ganmor approach can build a model on that dictionary. Since, unaided by the constraints of uBIA, the latter makes assumptions incompatible with the statistics of the data we have, it cannot work well on our data, and the above referenced section in the Methods demonstrates this beyond doubt. However, taking this as an evidence of success of uBIA over other methods would be disingenuous. As an analogy, if a Jeep gets through the mud where a Corvette gets stuck, it’s only an indication that these cars are designed for different tasks, and people would not consider a comparison of the two newsworthy. We similarly think that the comparison of uBIA and the Ganmor’s approach is not newsworthy enough to be in the main text.</p><disp-quote content-type="editor-comment"><p>Second, a recent paper (Phys Rev E, 104, 024407, 2021) used an information-based criterion to identify statistically significant couplings, which may be applicable to the context the authors consider here. In this eLife submission, the method relies on the magnetization inclusion threshold, which may be expensive for real data analysis.</p></disp-quote><p>This is an interesting paper (we will refer to it as the PRE method hereafter). Thank you for alerting us to it. There are substantial differences between this method and ours. Most importantly, the PRE method aims to build a generative model of the data, not just to identify which features of must be accounted for in the model. Thus, other things being equal, on very general grounds we know that it would require more data. However, other things are not equal. The PRE method is a <italic>pairwise</italic> methods (though the decimation discussed there may introduce some effective higher order couplings, but only of a very specific structure), and thus it cannot account for higher order structures, which uBIA notices. Finally, the PRE method detects important interactions by starting with a fully connected model, and decimating the weakest interactions turn by turn. Such approach will not work when the number of interactions, which scales as <italic>N</italic><sup>2</sup> increases; in contrast, we expect uBIA to be limited by the number of patterns actually seen in the data, and hence depend largely on <italic>M</italic> and not <italic>N</italic> in its complexity. We decided not to repeat this long discussion in the paper text, but we now mention this PRE article in the “Overview of prior related methods in the literature” section, and discuss the reduction in the number of interaction terms that it provides.</p><disp-quote content-type="editor-comment"><p>3. On the equation 1, the authors seem to use the local model parameter to detect the global significance of the collective codeword, which I could not understand very well, because a codeword is determined by all order of interactions considered in the log-linear model.</p></disp-quote><p>Equation 1 does not define the significance. It is just the all-orders model of the underlying probability distribution, and the significance is not defined till we introduce <italic>s<sub>µ</sub></italic> and calculate their postrior expectations much later in the paper, up to Equation 16.</p><disp-quote content-type="editor-comment"><p>4. Technically, the logic going from Equation 7 to Equation 8 is broken, since the s-dependence in Equation 8 does not naturally arise from Equation7.</p></disp-quote><p>Thank you! This has been fixed now by re-introducing the indicator variables between Equations 7 and 8 explicitly.</p><disp-quote content-type="editor-comment"><p>5. I could not understand well how the sign of reflects whether the word is over- or under-represented, and even, how the parameter account for the reducibility of the dictionaries. This part is better to be expanded in the manuscript, although these parameters have highly non-linear function relationship.</p></disp-quote><p>Equation 10 shows that, to include a word or not is determined largely by the deviation of its frequency from the null model. By comparing the expected and the empirical frequencies (overbarred and angle-bracketed quantities), we know if the word is over- or underrepresented. This is described in the paragraph that starts with “Equation (10) has a straightforward interpretation.”</p></body></sub-article></article>