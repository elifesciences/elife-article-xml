<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">94961</article-id><article-id pub-id-type="doi">10.7554/eLife.94961</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.94961.2</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Effects of noise and metabolic cost on cortical task representations</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Stroud</surname><given-names>Jake Patrick</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4263-5755</contrib-id><email>j.stroud@eng.cam.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Wojcik</surname><given-names>Michal</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Jensen</surname><given-names>Kristopher Torp</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9242-5572</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Kusunoki</surname><given-names>Makoto</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5381-8506</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Kadohisa</surname><given-names>Mikiko</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Buckley</surname><given-names>Mark J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7455-8486</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Duncan</surname><given-names>John</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Stokes</surname><given-names>Mark G</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Lengyel</surname><given-names>Mate</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>Computational and Biological Learning Lab, Department of Engineering, University of Cambridge</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Department of Experimental Psychology, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>MRC Cognition and Brain Sciences Unit, University of Cambridge</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Oxford Centre for Human Brain Activity, Wellcome Centre for Integrative Neuroimaging, Department of Psychiatry, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s3xwp33</institution-id><institution>Center for Cognitive Computation, Department of Cognitive Science, Central European University</institution></institution-wrap><addr-line><named-content content-type="city">Budapest</named-content></addr-line><country>Hungary</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Salinas</surname><given-names>Emilio</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0207ad724</institution-id><institution>Wake Forest University School of Medicine</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>21</day><month>01</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP94961</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-12-13"><day>13</day><month>12</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-12-08"><day>08</day><month>12</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.07.11.548492"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-27"><day>27</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.94961.1"/></event></pub-history><permissions><copyright-statement>© 2024, Stroud et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Stroud et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-94961-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-94961-figures-v1.pdf"/><abstract><p>Cognitive flexibility requires both the encoding of task-relevant and the ignoring of task-irrelevant stimuli. While the neural coding of task-relevant stimuli is increasingly well understood, the mechanisms for ignoring task-irrelevant stimuli remain poorly understood. Here, we study how task performance and biological constraints jointly determine the coding of relevant and irrelevant stimuli in neural circuits. Using mathematical analyses and task-optimized recurrent neural networks, we show that neural circuits can exhibit a range of representational geometries depending on the strength of neural noise and metabolic cost. By comparing these results with recordings from primate prefrontal cortex (PFC) over the course of learning, we show that neural activity in PFC changes in line with a minimal representational strategy. Specifically, our analyses reveal that the suppression of dynamically irrelevant stimuli is achieved by activity-silent, sub-threshold dynamics. Our results provide a normative explanation as to why PFC implements an adaptive, minimal representational strategy.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>recurrent neural networks</kwd><kwd>dynamical systems</kwd><kwd>cognition</kwd><kwd>prefrontal cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/215909</award-id><principal-award-recipient><name><surname>Stroud</surname><given-names>Jake Patrick</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/212262</award-id><principal-award-recipient><name><surname>Lengyel</surname><given-names>Mate</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/101092</award-id><principal-award-recipient><name><surname>Kusunoki</surname><given-names>Makoto</given-names></name><name><surname>Kadohisa</surname><given-names>Mikiko</given-names></name><name><surname>Duncan</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGP0044/2018</award-id><principal-award-recipient><name><surname>Lengyel</surname><given-names>Mate</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MC_UU_00030/7</award-id><principal-award-recipient><name><surname>Kusunoki</surname><given-names>Makoto</given-names></name><name><surname>Kadohisa</surname><given-names>Mikiko</given-names></name><name><surname>Duncan</surname><given-names>John</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/M010732/1</award-id><principal-award-recipient><name><surname>Stokes</surname><given-names>Mark G</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100005370</institution-id><institution>Gates Cambridge Trust</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Jensen</surname><given-names>Kristopher Torp</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100014748</institution-id><institution>Clarendon Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Wojcik</surname><given-names>Michal</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.37717/220020405</award-id><principal-award-recipient><name><surname>Stokes</surname><given-names>Mark G</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The dynamical solutions exhibited by task-optimized recurrent neural networks, and their similarity to prefrontal cortex dynamics, depends strongly on the strength of neural noise and metabolic cost imposed during training.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>How systems solve complex cognitive tasks is a fundamental question in neuroscience and artificial intelligence (<xref ref-type="bibr" rid="bib54">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib58">Silver et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Jensen et al., 2023</xref>). A key aspect of complex tasks is that they often involve multiple types of stimuli, some of which can even be <italic>irrelevant</italic> for performing the correct behavioral response (<xref ref-type="bibr" rid="bib23">Freedman et al., 2001</xref>; <xref ref-type="bibr" rid="bib40">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib49">Parthasarathy et al., 2017</xref>) or predicting reward (<xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>; <xref ref-type="bibr" rid="bib6">Chadwick et al., 2023</xref>). Over the course of task exposure, subjects must typically identify which stimuli are relevant and which are irrelevant. Examples of irrelevant stimuli include those that are irrelevant at all times in a task (<xref ref-type="bibr" rid="bib23">Freedman et al., 2001</xref>; <xref ref-type="bibr" rid="bib6">Chadwick et al., 2023</xref>; <xref ref-type="bibr" rid="bib15">Duncan, 2001</xref>; <xref ref-type="bibr" rid="bib50">Rainer et al., 1998</xref>; <xref ref-type="bibr" rid="bib61">Stokes et al., 2013</xref>) – which we refer to as <italic>static</italic> irrelevance – and stimuli that are relevant at some time points but are irrelevant at other times in a trial – which we refer to as <italic>dynamic</italic> irrelevance (e.g. as is often the case for context-dependent decision-making tasks <xref ref-type="bibr" rid="bib40">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib46">Monsell, 2003</xref>; <xref ref-type="bibr" rid="bib5">Braver, 2012</xref>). Although tasks involving irrelevant stimuli have been widely used, it remains an open question as to how different types of irrelevant stimuli should be represented, in combination with relevant stimuli, to enable optimal task performance.</p><p>One may naively think that statically irrelevant stimuli should always be suppressed. However, stimuli that are currently irrelevant may be relevant in a future task. Furthermore, it is unclear whether dynamically irrelevant stimuli should be suppressed at all since the information is ultimately needed by the circuit. It may therefore be beneficial for a neural circuit to represent irrelevant information as long as no unnecessary costs are incurred and task performance remains high. Several factors could have a strong impact on whether irrelevant stimuli affect task performance. For example, levels of neural noise in the circuit as well as energy constraints and the metabolic costs of overall neural activity (<xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib67">Whittington et al., 2022</xref>; <xref ref-type="bibr" rid="bib64">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib36">Löwe, 2023</xref>; <xref ref-type="bibr" rid="bib9">Cueva and Wei, 2018</xref>; <xref ref-type="bibr" rid="bib37">Luo et al., 2023</xref>; <xref ref-type="bibr" rid="bib31">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="bib11">Deneve et al., 2001</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>) can affect how stimuli are represented in a neural circuit. Indeed, both noise and metabolic costs are factors that biological circuits must contend with (<xref ref-type="bibr" rid="bib65">Tomko and Crapper, 1974</xref>; <xref ref-type="bibr" rid="bib34">Laughlin, 2001</xref>; <xref ref-type="bibr" rid="bib7">Churchland et al., 2006</xref>; <xref ref-type="bibr" rid="bib27">Hasenstaub et al., 2010</xref>). Despite these considerations, models of neural population codes, including hand-crafted models and optimized artificial neural networks, typically use only a very limited range of the values of such factors (<xref ref-type="bibr" rid="bib66">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Cueva et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Driscoll et al., 2022</xref>; <xref ref-type="bibr" rid="bib60">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib17">Echeveste et al., 2020</xref>; <xref ref-type="bibr" rid="bib62">Stroud et al., 2021</xref>) (but also see <xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Orhan and Ma, 2019</xref>). Therefore, despite the success of recent comparisons between neural network models and experimental recordings (<xref ref-type="bibr" rid="bib66">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Cueva and Wei, 2018</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Cueva et al., 2020</xref>; <xref ref-type="bibr" rid="bib17">Echeveste et al., 2020</xref>; <xref ref-type="bibr" rid="bib62">Stroud et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Lindsay, 2021</xref>), we may only be recovering very few out of a potentially large range of different representational strategies that neural networks could exhibit (<xref ref-type="bibr" rid="bib55">Schaeffer et al., 2022</xref>).</p><p>One challenge for distinguishing between different representational strategies, particularly when analyzing experimental recordings, is that some stimuli may simply be represented more strongly than others. In particular, we might expect stimuli to be strongly represented in cortex a priori if they have previously been important to the animal. Indeed, being able to represent a given stimulus when learning a new task is likely a prerequisite for learning whether it is relevant or irrelevant in that particular context (<xref ref-type="bibr" rid="bib54">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>). Previously, it has been difficult to distinguish between whether a given representation existed a priori or emerged as a consequence of learning because neural activity is typically only recorded <italic>after</italic> a task has already been learned. A more relevant question is how the representation <italic>changes</italic> over learning (<xref ref-type="bibr" rid="bib6">Chadwick et al., 2023</xref>; <xref ref-type="bibr" rid="bib52">Reinert et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">Durstewitz et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Schuessler et al., 2020</xref>; <xref ref-type="bibr" rid="bib8">Costa et al., 2017</xref>), which provides insights into how the specific task of interest affects the representational strategy used by an animal or artificial network (<xref ref-type="bibr" rid="bib53">Richards et al., 2019</xref>).</p><p>To resolve these questions, we optimized recurrent neural networks on a task that involved two types of irrelevant stimuli. One feature of the stimulus was statically irrelevant, and another feature of the stimulus was dynamically irrelevant. We found that, depending on the neural noise level and metabolic cost that was imposed on the networks during training, a range of representational strategies emerged in the optimized networks, from maximal (representing all stimuli) to minimal (representing only relevant stimuli). We then compared the strategies of our optimized networks with <italic>learning-resolved</italic> recordings from the prefrontal cortex (PFC) of monkeys exposed to the same task. We found that the representational geometry of the neural recordings changed in line with the minimal strategy. Using a simplified model, we derived mathematically how the strength of relevant and irrelevant coding depends on the noise level and metabolic cost. We then confirmed our theoretical predictions in both our task-optimized networks and neural recordings. By reverse-engineering our task-optimized networks, we also found that activity-silent, sub-threshold dynamics led to the suppression of dynamically irrelevant stimuli, and we confirmed predictions of this mechanism in our neural recordings.</p><p>In summary, we provide a mechanistic understanding of how different representational strategies can emerge in both biological and artificial neural circuits over the course of learning in response to salient biological factors such as noise and metabolic costs. These results in turn explain why PFC appears to employ a minimal representational strategy by filtering out task-irrelevant information.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A task involving relevant and irrelevant stimuli</title><p>We study a task used in previous experimental work that uses a combination of multiple, relevant and irrelevant stimuli (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>; <xref ref-type="fig" rid="fig1">Figure 1a</xref>). The task consists of an initial ‘fixation’ period, followed by a ‘color’ period, in which one of two colors are presented. After this, in the ‘shape’ period, either a square or diamond shape is presented (while the color stimulus stays on), such that the width of the shape can be either thick or thin. After this, the stimuli disappear and reward is delivered according to an XOR structure between color and shape (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Note that the width of the shape is not predictive of reward, and it is therefore an <italic>irrelevant</italic> stimulus dimension (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). As width information is always irrelevant when it is shown, its irrelevance is <italic>static</italic>. In contrast, color is relevant during the shape period but could be ignored during the color period without loss of performance. Hence its irrelevance is <italic>dynamic</italic>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task design and irrelevant stimulus representations.</title><p>(<bold>a</bold>) Illustration of the timeline of task events in a trial with the corresponding displays and names of task periods. Red dot shows fixation ring, blue (or green) circles appear during the color and shape periods, gray squares (or diamonds) appear during the shape period, and a juice reward is given during the reward period for rewarded combinations of color and shape stimuli (see panel b). No behavioral response was required for the monkeys as it was a passive object–association task (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>). (<bold>b</bold>) Schematic showing that rewarded conditions of color and shape stimuli follows an XOR structure. In addition, the width of the shape was not predictive of reward and was thus an irrelevant stimulus dimension. (<bold>c</bold>) Schematic of four possible representational strategies, as indicated by linear decoding of population activity, for the task shown in panels a and b. Turquoise lines with shading show early color decoding and black lines with shading show width decoding. Strategies are split according to whether early color decoding is low (left column) or high (right column), and whether width decoding is low (bottom row) or high (top row).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig1-v1.tif"/></fig><p>Due to the existence of multiple different forms of irrelevant stimuli, there exist multiple different representational strategies for a neural circuit solving the task in <xref ref-type="fig" rid="fig1">Figure 1a</xref>. These representational strategies can be characterized by assessing the extent to which different stimuli are linearly decodable from neural population activity (<xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Stokes et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Meyers et al., 2008</xref>; <xref ref-type="bibr" rid="bib32">King and Dehaene, 2014</xref>). We use linear decodability because it only requires the computation of simple weighted sums of neural responses, and as such, it is a widely accepted criterion for the usefulness of a neural representation (<xref ref-type="bibr" rid="bib12">DiCarlo and Cox, 2007</xref>). Moreover, while representational strategies can differ along several dimensions in this task (e.g. the decodability of color or shape during the shape period – both of which are task-relevant <xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>), our main focus here is on the two dimensions that specifically control the representation of irrelevant stimuli. Therefore, depending on whether each of the irrelevant stimuli are linearly decodable during their respective period of irrelevance, we distinguish four different (extreme) strategies, ranging from a ‘minimal strategy’, in which the irrelevant stimuli are only weakly represented (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, bottom left; pink shading), to a ‘maximal strategy’, in which both irrelevant stimuli are strongly represented (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, top right; blue shading).</p></sec><sec id="s2-2"><title>Stimulus representations in task-optimized recurrent neural networks</title><p>To understand the factors determining which representational strategy neural circuits employ to solve this task, we optimized recurrent neural networks to perform the task (<xref ref-type="fig" rid="fig2">Figure 2a</xref>; see also Neural network models). The neural activities in these stochastic recurrent networks evolved according to<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mtext>with </mml:mtext><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>​</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>​</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Stronger levels of noise and firing rate regularization lead to suppression of task-irrelevant stimuli in optimized recurrent networks.</title><p>(<bold>a</bold>) Top: illustration of a recurrent neural network model where each neuron receives independent white noise input with strength <italic>σ</italic> (middle). Color, shape, and width inputs are delivered to the network via three input channels (left). Firing rate activity is read out into two readout channels (either rewarded or not rewarded; right). All recurrent weights in the network, as well as weights associated with the input and readout channels, were optimized (Neural network models). Bottom: cost function used for training the recurrent neural networks (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) and timeline of task events within a trial for the recurrent neural networks (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Yellow shading on time axis shows the time period in which the task performance term enters the cost function (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>). (<bold>b</bold>) Top: neural firing rate trajectories in the top two PCs for an example network over the course of a trial (from color stimulus onset) for a particular noise (<italic>σ</italic>) and regularization (<italic>λ</italic>) regime. Open gray circles indicate color onset, filled gray squares indicate shape onset, filled gray circles indicate offset of both stimuli, and colored thick and thin squares and diamonds indicate the end of the trial at 1.5 s for all stimulus conditions. Pale and brightly colored trajectories indicate the two width conditions. We show results for networks exposed to low noise and low regularization (<inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>; left, pale blue shading), high noise and low regularization (<inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>; middle, purple shading), and medium noise and medium regularization (<inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>; right, pink shading). Bottom: performance of a linear decoder (mean over 10 networks) trained at each time point within the trial to decode color (turquoise) or width (black) from neural firing rate activity for each noise and regularization regime. Dotted gray lines and shading show mean ± 2 s.d. of chance level decoding based on shuffling trial labels 100 times. (<bold>c</bold>) Left: performance of optimized networks determined as the mean performance over all trained networks during the reward period (a, bottom; yellow shading) for all noise (<italic>σ</italic>, horizontal axis) and regularization levels (<italic>λ</italic>, vertical axis) used during training. Pale blue, pink, and purple dots indicate parameter values that correspond to the dynamical regimes shown in panel b and <xref ref-type="fig" rid="fig1">Figure 1c</xref> with the same background coloring. For parameter values above the white line, networks achieved a mean performance of less than 0.95. Middle: early color decoding determined as mean color decoding over all trained networks at the end of the color period (b, bottom left, turquoise arrow) using the same plotting scheme as the left panel. Right: width decoding determined as mean width decoding over all trained networks at the end of the shape period (b, bottom left, black arrow) using the same plotting scheme as the left panel. (<bold>d</bold>) Width decoding plotted against early color decoding for all noise and regularization levels and colored according to performance. Pale blue, pink, and purple highlights indicate the parameter values shown with the same colors in panel c.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Temporal decoding for networks with different noise and regularization levels.</title><p>(<bold>a</bold>) Performance of a linear decoder trained at each time point to predict either XOR (far left), shape (middle left), color (middle right), or width (far right) from neural population activity from optimized recurrent neural networks trained in the <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. We show decoding results separately from the first half of sessions (gray, ‘early learning’) and the second half of sessions (black, ‘late learning’). Dotted gray lines and shading show mean ± 2 s.d. of chance level decoding based on shuffling trial labels 100 times. Horizontal black bars show significant differences between early and late decoding using a two-sided cluster-based permutation test and a significance threshold of 0.05 (Statistics). (<bold>b</bold>) Same as panel a, but for networks optimized in the <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>c</bold>) Same as panel a, but for networks optimized in the <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>d</bold>) Same as panel a, but for networks optimized in a very high regularization regime (<inline-formula><mml:math id="inf7"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>) while still being able to perform the task to a high accuracy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig2-figsupp1-v1.tif"/></fig></fig-group><p>where <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>​</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>​</mml:mo><mml:mi mathvariant="normal">⊤</mml:mi><mml:mo>​</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to the vector of ‘sub-threshold’ activities of the <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons in the network, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is their momentary firing rates which is a rectified linear function (ReLU) of the sub-threshold activity, <italic>τ</italic>=50 ms is the effective time constant, and <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the inputs to the network encoding the three stimulus features as they become available over the course of the trial (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, bottom). The optimized parameters of the network were <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the recurrent weight matrix describing connection strengths between neurons in the network (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, top; middle), <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the feedforward weight matrix describing connections from the stimulus inputs to the network (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, top; left; see also Neural network models), and <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, a stimulus-independent bias. Importantly, <italic>σ</italic> is the standard deviation of the neural noise process (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, top; pale gray arrows; with <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mo>​</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> being a sample from a Gaussian white noise process with mean 0 and variance 1), and as such represents a fundamental constraint on the operation of the network. The output of the network was given by<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Softmax</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with optimized parameters <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the readout weights (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, top; right), and <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, a readout bias.</p><p>We optimized networks for a canonical cost function (<xref ref-type="bibr" rid="bib48">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib13">Driscoll et al., 2022</xref>; <xref ref-type="bibr" rid="bib60">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Stroud et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Masse et al., 2019</xref>; <xref ref-type="fig" rid="fig2">Figure 2a</xref>, bottom; Network optimization):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">H</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>The first term in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> is a task performance term measuring the cross-entropy loss <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> between the one-hot encoded target choice, <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the network’s output probabilities, <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, during the reward period, <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, bottom; yellow shading). The second term in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> is a firing rate regularization term. This regularization term can be interpreted as a form of energy or metabolic cost (<xref ref-type="bibr" rid="bib67">Whittington et al., 2022</xref>; <xref ref-type="bibr" rid="bib31">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="bib10">Cueva et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Masse et al., 2019</xref>; <xref ref-type="bibr" rid="bib56">Schimel et al., 2023</xref>) measured across the whole trial, <italic>T</italic>, because it penalizes large overall firing rates. Therefore, optimizing this cost function encourages networks to not only solve the task, but to do so using low overall firing rates. How important it is for the network to keep firing rates low is controlled by the ‘regularization’ parameter <italic>λ</italic>. Critically, we used different noise levels <italic>σ</italic> and regularization strengths <italic>λ</italic> (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, highlighted in red) to examine how these two constraints affected the dynamical strategies employed by the optimized networks to solve the task and compared them to our four hypothesized representational strategies (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). We focused on these two parameters because they are both critical factors for real neural circuits and a priori can be expected to have important effects on the resulting circuit dynamics. For example, metabolic costs will constrain the overall level of firing rates that can be used to solve the task while noise levels directly affect how reliably stimuli can be encoded in the network dynamics. For the remainder of this section, we analyze representational strategies utilized by networks after training. In subsequent sections, we analyze learning-related changes in both our optimized networks and neural recordings.</p><p>We found that networks trained in a low noise–low firing rate regularization setting (which we denote by <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) employed a maximal representational strategy (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, left column; pale blue shading). Trajectories in neural firing rate space diverged for the two different colors as soon as the color stimulus was presented (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, top left; blue and green trajectories from open gray circle to gray squares), which resulted in high color decoding during the color period (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom left; turquoise line; Linear decoding). During the shape period, the trajectories corresponding to each color diverged again (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, top left; trajectories from gray squares to filled gray circles), such that all stimuli were highly decodable, including width (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom left; black line, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref>). After removal of the stimuli during the reward period, trajectories converged to one of two parts of state space according to the XOR task rule – which is required for high performance (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, top left; trajectories from filled gray circles to colored squares and diamonds). Because early color and width were highly decodable in these networks trained with a low noise and low firing rate regularization, the dynamical strategy they employed corresponds to the maximal representational regime (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, blue shading).</p><p>We next considered the setting of networks that solve this task while being exposed to a high level of neural noise (which we denote by <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2">Figure 2b</xref>, middle column; purple shading). In this setting, we also observed neural trajectories that diverged during the color period (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, top middle; gray squares are separated), which yielded high color decoding during the color period (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom middle; turquoise line). However, in contrast to networks with a low level of neural noise (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, left), width was poorly decodable during the shape period (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom middle; black line). Therefore, for networks challenged with higher levels of neural noise, the irrelevant stimulus dimension of width is represented more weakly (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, purple shading). A similar representational strategy was also observed in networks that were exposed to a low level of noise but a high level of firing rate regularization (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1c</xref>, black lines).</p><p>Finally, we considered the setting of networks that solve this task while being exposed to medium levels of both neural noise and firing rate regularization (which we denote by <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2">Figure 2b</xref>, right column; pink shading). In this setting, neural trajectories diverged only weakly during the color period (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, top middle; gray squares on-top of one another), which yielded relatively poor (non-ceiling) color decoding during the color period (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom right; turquoise line). Nevertheless, color decoding was still above chance despite the neural trajectories strongly overlapping in the two-dimensional state space plot in <xref ref-type="fig" rid="fig2">Figure 2b</xref>, top right, because these trajectories became separable in the full-dimensional state space of these networks. Additionally, width decoding was also poor during the shape period (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom right; black line). Therefore, networks that were challenged with increased levels of both neural noise and firing rate regularization employed dynamics in line with a minimal representational strategy by only weakly representing irrelevant stimuli (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, pink shading).</p><p>To gain a more comprehensive picture of the full range of dynamical solutions that networks can exhibit, we performed a grid search over multiple different levels of noise and firing rate regularization. Nearly all parameter values allowed networks to achieve high performance (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, left), except when both the noise and regularization levels were high (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, left; parameter values above white line). Importantly, all of the dynamical regimes that we showed in <xref ref-type="fig" rid="fig2">Figure 2b</xref> achieved similarly high performances (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, left; pale blue, purple, and pink dots).</p><p>When looking at early color decoding (defined as color decoding at the end of the color period; <xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom left, turquoise arrow) and width decoding (defined as width decoding at the end of the shape period; <xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom left, black arrow), we saw a consistent pattern of results. Early color decoding was high only when either the lowest noise level was used (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, middle; <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> column) or when the lowest regularization level was used (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, middle; <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0005</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> row). In contrast, width decoding was high only when both the level of noise and regularization were small (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, right; bottom left corner). Otherwise, width decoding became progressively worse as either the noise or regularization level was increased and achieved values that were typically lower than the corresponding early color decoding (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, compare right with middle). This pattern becomes clearer when we plot width decoding against early color decoding (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). No set of parameters yielded higher width decodability compared to early color decodability (<xref ref-type="fig" rid="fig2">Figure 2d</xref>, no data point above the identity line). This means that we never observed the fourth dynamical regime we hypothesized a priori, in which width decoding would be high and early color decoding would be low (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, top left). Therefore, information with static irrelevance (width) was more strongly suppressed compared to information whose irrelevance was dynamic (color). We also note that we never observed pure chance levels of decoding of color or width during stimulus presentation. This is likely because it is challenging for recurrent neural networks to strongly suppress their inputs and it may also be the case that other hyperparameter regimes more naturally lead to stronger suppression of inputs (we discuss this second possibility later; e.g. Figure 5).</p></sec><sec id="s2-3"><title>Comparing learning-related changes in stimulus representations in neural networks and primate lateral PFC</title><p>To understand the dynamical regime employed by PFC, we analyzed a dataset (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>) of multi-channel recordings from lateral prefrontal cortex (lPFC) in two monkeys exposed to the task shown in <xref ref-type="fig" rid="fig1">Figure 1a</xref>. These recordings yielded 376 neurons in total across all recording sessions and both animals (Experimental materials and methods). Importantly, for understanding the direction in which neural geometries changed over learning, recordings commenced in the first session in which the animals were exposed to the task – i.e., the recordings spanned the entirety of the learning process. For our analyses, we distinguished between the first half of recording sessions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, gray; ‘early learning’) and the second half of recording sessions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, black; ‘late learning’). A previous analysis of this dataset showed that, over the course of learning, the XOR representation of the task comes to dominate the dynamics during the late shape period of the task (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>). Here, however, we focus on relevant and irrelevant task variable coding during the stimulus periods and compare the recordings to the dynamics of task-optimized recurrent neural networks. Also, in line with the previous study (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>), we do not examine the reward period of the task because the one-to-one relationship between XOR and reward in the data (which is not present in the models) will likely lead to trivial differences in XOR representations in the reward period between the data and models.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Stimulus representations in primate lateral prefrontal cortex (lPFC) correspond to a minimal representational strategy.</title><p>(<bold>a</bold>) Performance of a linear decoder trained at each time point to predict either XOR (far left), shape (middle left), color (middle right), or width (far right) from neural population activity in lPFC (Experimental materials and methods and Linear decoding). We show decoding results separately from the first half of sessions (gray, ‘early learning’) and the second half of sessions (black, ‘late learning’). Dotted gray lines and shading show mean ± 2 s.d. of chance level decoding based on shuffling trial labels 100 times. Horizontal black bars show significant differences between early and late decoding using a two-sided cluster-based permutation test and a significance threshold of 0.05 (Statistics). Open gray circles, filled gray squares, and filled gray circles on the horizontal indicate color onset, shape onset, and offset of both stimuli, respectively. (<bold>b</bold>) Same as panel a but for decoders trained on neural activity from optimized recurrent neural networks in the <inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (<xref ref-type="fig" rid="fig2">Figure 2b–d</xref>, pink). (<bold>c</bold>) Black horizontal lines show the mean change between early and late decoding during time periods when there were significant differences between early and late decoding in the data (horizontal black bars in panel a) for XOR (top row), color (middle row), and width (bottom row). (No significant differences in shape decoding were observed in the data; cf. panel a.) Violin plots show chance differences between early and late decoding based on shuffling trial labels 100 times. We show results for the data (far left column), <inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> networks (middle left column, pale blue shading), <inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> networks (middle right column, purple shading), and <inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> networks (far right column, pink shading). (<bold>d</bold>) Same as panel c but we show results using cross-generalized decoding (<xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>) during the same time periods as those used in panel c (Linear decoding).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Cross-generalized temporal decoding for lateral prefrontal cortex (lPFC) recordings and networks with different noise and regularization levels.</title><p>(<bold>a</bold>) Performance of a cross-generalized decoder (Linear decoding) trained at each time point to predict either XOR (far left), shape (middle left), color (middle right), or width (far right) from neural population activity from our lPFC recordings. We show decoding results separately from the first half of sessions (gray, ‘early learning’) and the second half of sessions (black, ‘late learning’). Dotted gray lines and shading show mean ± 2 s.d. of chance level decoding based on shuffling trial labels 100 times. Horizontal black bars show significant differences between early and late decoding using a two-sided cluster-based permutation test and a significance threshold of 0.05 (Statistics). (<bold>b</bold>) Same as panel a, but for recurrent neural networks optimized in the <inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>c</bold>) Same as panel a, but for recurrent neural networks optimized in the <inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>d</bold>) Same as panel a, but for recurrent neural networks optimized in the <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Temporal decoding for networks with input weights initialized to 0 prior to optimization.</title><p>(<bold>a</bold>) Performance of a linear decoder trained at each time point to predict either XOR (far left), shape (middle left), color (middle right), or width (far right) from neural population activity from optimized recurrent neural networks trained in the <inline-formula><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime with input weights initialized to 0 prior to optimization (<xref ref-type="fig" rid="fig3">Figure 3a and b</xref>; Network optimization). We show decoding results separately from the first half of sessions (gray, ‘early learning’) and the second half of sessions (black, ‘late learning’). Dotted gray lines and shading show mean ± 2 s.d. of chance level decoding based on shuffling trial labels 100 times. Horizontal black bars show significant differences between early and late decoding using a two-sided cluster-based permutation test and a significance threshold of 0.05 (Statistics). (<bold>b</bold>) Same as panel a, but for networks optimized in the <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>c</bold>) Same as panel a, but for networks optimized in the <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>d</bold>) Same as panel a, but for networks optimized in the <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Temporal decoding for networks with input weights initialized to large random values prior to optimization.</title><p>(<bold>a</bold>) Performance of a linear decoder trained at each time point to predict either XOR (far left), shape (middle left), color (middle right), or width (far right) from neural population activity from optimized recurrent neural networks trained in the <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime with input weights initialized to large random values prior to optimization (<xref ref-type="fig" rid="fig3">Figure 3a and b</xref>; Network optimization). We show decoding results separately from the first half of sessions (gray, ‘early learning’) and the second half of sessions (black, ‘late learning’). Dotted gray lines and shading show mean ± 2 s.d. of chance level decoding based on shuffling trial labels 100 times. Horizontal black bars show significant differences between early and late decoding using a two-sided cluster-based permutation test and a significance threshold of 0.05 (Statistics). (<bold>b</bold>) Same as panel a, but for networks optimized in the <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>c</bold>) Same as panel a, but for networks optimized in the <inline-formula><mml:math id="inf40"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>d</bold>) Same as panel a, but for networks optimized in the <inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig3-figsupp3-v1.tif"/></fig></fig-group><p>Similar to our analyses of our recurrent network models (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom), we performed population decoding of the key task variables in the experimental data (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, see also Linear decoding). We found that the decodability of the XOR relationship between task-relevant stimuli that determined task performance (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) significantly increased during the shape period over the course of learning, consistent with the animals becoming more familiar with the task structure (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, far left; compare gray and black lines). We also found that shape decodability during the shape period decreased slightly, but not significantly, over learning (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, middle left; compare gray and black lines from gray square to gray circle), while color decodability during the shape period increased slightly (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, middle right; compare gray and black lines from gray square to gray circle). Importantly, however, color decodability significantly <italic>decreased</italic> during the color period (when it is ‘irrelevant’; <xref ref-type="fig" rid="fig3">Figure 3a</xref>, middle right; compare gray and black lines), and width decodability significantly decreased during the shape period (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, far right; compare gray and black lines). Neural activities in lPFC thus appear to change in line with the minimal representational strategy over the course of learning, consistent with recurrent networks trained in the <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> regime (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, pink and <xref ref-type="fig" rid="fig2">Figure 2b</xref>, bottom, pink).</p><p>We then directly compared these learning-related changes in stimulus decodability from lPFC with those that we observed in our task-optimized recurrent neural networks (<xref ref-type="fig" rid="fig2">Figure 2</xref>). We found that the temporal decoding dynamics of networks trained with medium noise and firing rate regularization (<inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2">Figure 2b–d</xref>, pink shading and pink dots) exhibited decoding dynamics most similar to those that we observed in lPFC. Specifically, XOR decodability significantly increased after shape onset, consistent with the networks learning the task (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, far left; compare gray and black lines). We also found that shape and color decodability did not significantly change during the shape period (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, middle left and middle right; compare gray and black lines from gray square to gray circle). Importantly, however, color decodability significantly <italic>decreased</italic> during the color period (when it is ‘irrelevant’; <xref ref-type="fig" rid="fig3">Figure 3b</xref>, middle right; compare gray and black lines from open gray circle to gray square), and width decodability significantly decreased during the shape period (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, far right; compare gray and black lines). Other noise and regularization settings yielded temporal decoding that displayed a poorer resemblance to the data (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). For example, <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks exhibited almost no changes in decodability during the color and shape periods (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a and c</xref>) and <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks exhibited increased XOR, shape, and color decodability at all times after stimulus onset while width decodability decreased during the shape period (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>). We also found that if regularization is driven to very high levels, color and shape decoding becomes weak during the shape period while XOR decoding remains high (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1d</xref>). Therefore, such networks effectively perform a pure XOR computation during the shape period.</p><p>We also note that the <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> model does not perfectly match the decoding dynamics seen in the data. For example, although not significant, we observed a decrease in shape decoding and an increase in color decoding in the data during the shape period whereas the model only displayed a slight (non-significant) increase in decodability of both shape and color during the same time period. These differences may be due to fundamentally different ways that brains encode sensory information upstream of PFC, compared to the more simplistic abstract sensory inputs used in models (see Discussion).</p><p>To systematically compare learning-related changes in the data and models, we analyzed time periods when there were significant changes in decoding in the data over the course of learning (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, horizontal black bars). This yielded a substantial increase in XOR decoding during the shape period, and substantial decreases in color and width decoding during the color and shape periods, respectively, in the data (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, far left column). During the same time periods in the models, networks in the <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> regime exhibited no changes in decoding (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, middle left column; blue shading). In contrast, networks in the <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> regime exhibited substantial increases in XOR and color decoding, and a substantial decrease in width decoding (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, middle right column; purple shading). Finally, in line with the data, networks in the <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> regime exhibited a substantial increase in XOR decoding, and substantial decreases in both color and width decoding (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, middle right column; pink shading).</p><p>In addition to studying changes in traditional decoding, we also studied learning-related changes in ‘cross-generalized decoding’ which provides a measure of how factorized the representational geometry is across stimulus conditions (<xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). (For example, for evaluating cross-generalized decoding for color, a color decoder trained on square trials would be tested on diamond trials, and vice versa <xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>; see also Linear decoding.) Using this measure, we found that changes in decoding were generally more extreme over learning and that models and data bore a stronger resemblance to one another compared with traditional decoding. Specifically, all models and the data showed a strong increase in XOR decoding (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, top row, ‘XOR’) and a strong decrease in width decoding (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, bottom row, ‘width’). However, only the data and <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks showed a decrease in color decoding (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, compare middle far left and middle far right), whereas <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> networks showed no change in color decoding (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, middle left; blue shading) and <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> networks showed an increase in color decoding (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, middle right; purple shading).</p><p>Beside studying the effects of input noise and firing rate regularization, we also examined the effects of different strengths of the initial stimulus input connections prior to training (<xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>). In line with previous studies (<xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib62">Stroud et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Masse et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Dubreuil et al., 2022</xref>), for all of our previous results, the initial input weights were set to small random values prior to training (Network optimization). We found that changing these weights had similar effects to changing neural noise (with the opposite sign). Specifically, when input weights were set to 0 before training, initial decoding was at chance levels and only increased with learning for XOR, shape, and color (whereas width decoding hardly changed with learning and remained close to chance levels; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>) – analogously to the <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> regime (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>). In contrast, when input weights were set to large random values prior to training, initial decoding was at ceiling levels and did not significantly change over learning during the color and shape periods for any of the task variables (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>) – similar to what we found in the <inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref>). Thus, neither extremely small nor extremely large initial input weights were consistent with the data that exhibited both increases and decreases in decodability of task variables over learning (<xref ref-type="fig" rid="fig3">Figure 3a</xref>).</p></sec><sec id="s2-4"><title>Theoretical predictions for strength of relevant and irrelevant stimulus coding</title><p>To gain a theoretical understanding of how irrelevant stimuli should be processed in a neural circuit, we performed a mathematical analysis of a minimal linear model that only included a single relevant stimulus and a single statically irrelevant stimulus (see also Appendix 1, Mathematical analysis of relevant and irrelevant stimulus coding in a linear network). Although this analysis applies to a simpler task compared with that faced by our neural networks and animals, crucially it still allows us to understand how relevant and irrelevant coding depend on noise and metabolic constraints. Our mathematical analysis suggested that the effects of noise and firing rate regularization on the performance and metabolic cost of a network can be understood via three key aspects of its representation: the strength of neural responses to the relevant stimulus (‘relevant coding’), the strength of responses to the irrelevant stimulus (‘irrelevant coding’), and the overlap between the population responses to relevant and irrelevant stimuli (‘overlap’; <xref ref-type="fig" rid="fig4">Figure 4a</xref>). In particular, maximizing task performance (i.e. the decodability of the relevant stimulus) required relevant coding to be strong, irrelevant coding to be weak, and the overlap between the two to be small (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, top; see also Appendix 1, The performance of the optimal linear decoder). This ensures that the irrelevant stimulus interferes minimally with the relevant stimulus. In contrast, to reduce a metabolic cost (such as we considered previously in our optimized recurrent networks, see <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> and <xref ref-type="fig" rid="fig2">Figure 2</xref>), both relevant and irrelevant coding should be weak (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, bottom; see also Appendix 1, Metabolic cost).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Theoretical predictions for strength of relevant and irrelevant stimulus coding and comparison to lateral prefrontal cortex (lPFC) recordings.</title><p>(<bold>a</bold>) Schematic of activity in neural state space for two neurons for a task involving two relevant (black drops vs. red crosses) and two irrelevant stimuli (thick vs. thin squares). Strengths of relevant and irrelevant stimulus coding are shown with red and black arrows, respectively, and the overlap between relevant and irrelevant coding is also shown (‘overlap’). (<bold>b</bold>) Schematic of our theoretical predictions for the optimal setting of relevant (red arrows) and irrelevant (black arrows) coding strengths when either maximizing performance (top) or minimizing a metabolic cost with strength <italic>λ</italic> (bottom; see <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>). (<bold>c</bold>) Schematic of our theoretical predictions for the strength of relevant (<inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; black drops vs. red crosses; red arrows) and irrelevant (<inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; thick vs. thin squares; black arrows) coding when jointly optimizing for both performance and a metabolic cost (cf. panel b). In the low noise regime (left), relevant conditions are highly distinguishable and irrelevant conditions are poorly distinguishable as well as strongly orthogonal to the relevant conditions. In the high noise regime (right), all conditions are poorly distinguishable. (<bold>d</bold>) Our theoretical predictions (<xref ref-type="disp-formula" rid="equ44">Equation S34</xref>) for the strength of relevant coding (<inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, see panel c) as a function of the noise level <italic>σ</italic> (horizontal axis) and firing rate regularization strength <italic>λ</italic> (colorbar). (<bold>e</bold>) Same as panel d but for our optimized recurrent neural networks (<xref ref-type="fig" rid="fig2">Figure 2</xref>) where we show the strength of relevant (XOR) coding (Measuring stimulus coding strength). Pale blue, purple, and pink highlights correspond to the noise and regularization strengths shown in <xref ref-type="fig" rid="fig2">Figure 2c and d</xref>. Gray dotted line and shading shows mean ±2 s.d. (over 250 networks; 10 networks for each of the 25 different noise and regularization levels) of <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> prior to training. (<bold>f</bold>) Same as panel e but for the strength of irrelevant (width) coding (<inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>). The black arrow indicates the theoretical prediction of 0 irrelevant coding. (<bold>g</bold>) The absolute value of the normalized dot product (overlap) between relevant and irrelevant representations (<inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e. 0 implies perfect orthogonality and 1 implies perfect overlap) for our optimized recurrent neural networks. The black arrow indicates the theoretical prediction of 0 overlap. (<bold>h</bold>) Left: coding strength (length of arrows in panel a; Measuring stimulus coding strength) for relevant (XOR; red) and irrelevant (width; black) stimuli during early and late learning for our lPFC recordings. Right: the absolute value of the overlap between relevant and irrelevant representations for our lPFC recordings (0 implies perfect orthogonality and 1 implies perfect overlap). Error bars show the mean ± 2 s.d. over 10 non-overlapping splits of the data. (<bold>i</bold>) Same as panel h but for the optimized recurrent neural networks in the <inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (see pink dots in <xref ref-type="fig" rid="fig2">Figure 2</xref>). Error bars show the mean ± 2 s.d. over 10 different networks. p-Values resulted from a two-sided Mann–Whitney U test (*, p&lt;0.05; **, p&lt;0.01; n.s., not significant; see Statistics).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig4-v1.tif"/></fig><p>In combination, when decoding performance and metabolic cost are <italic>jointly</italic> optimized, as in our task-optimized recurrent networks (<xref ref-type="fig" rid="fig2">Figure 2</xref>), our theoretical analyses suggested that performance should decrease with both the noise level <italic>σ</italic> and the strength of firing rate regularization <italic>λ</italic> in an approximately interchangeable way, and metabolic cost should increase with <italic>σ</italic> but decrease with <italic>λ</italic> (Appendix 1, Qualitative predictions about optimal parameters). We also found that the strength of relevant coding should decrease with <italic>λ</italic>, but its dependence on <italic>σ</italic> was more nuanced. For small <italic>σ</italic>, the performance term effectively dominates the metabolic cost (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, top) and the strength of relevant coding should increase with <italic>σ</italic>. However, if <italic>σ</italic> is too high, the strength of relevant coding starts decreasing otherwise a disproportionately high metabolic cost must be incurred to achieve high performance (<xref ref-type="fig" rid="fig4">Figure 4c,d</xref>). Our mathematical analysis also suggested that irrelevant coding and relevant–irrelevant overlap could in principle depend on the noise and metabolic cost strength – particularly if performing noisy optimization where the curvature of the cost function can be relatively shallow around the minimum (Appendix 1, Qualitative predictions about optimal parameters). Therefore, practically, we also expect that irrelevant coding should be mostly dependent (inversely) on <italic>λ</italic>, but relevant–irrelevant overlap should mostly depend on <italic>σ</italic> (Appendix 1, Curvature of the loss function around the optimum). These theoretical predictions were confirmed by our recurrent neural network simulations (<xref ref-type="fig" rid="fig4">Figure 4e–g</xref>).</p><p>We next measured, in both recorded and simulated population responses, the three aspects of population responses that our theory identified as being key in determining the performance and metabolic cost of a network (<xref ref-type="fig" rid="fig4">Figure 4a</xref>; Measuring stimulus coding strength). We found a close correspondence in the learning-related changes of these measures between our lPFC recordings (<xref ref-type="fig" rid="fig4">Figure 4h</xref>) and optimized recurrent networks (<xref ref-type="fig" rid="fig4">Figure 4i</xref>). In particular, we found that the strength of relevant (XOR) coding increased significantly over the course of learning (<xref ref-type="fig" rid="fig4">Figure 4h and i</xref>, left; red). The strength of irrelevant (width) coding decreased significantly over the course of learning (<xref ref-type="fig" rid="fig4">Figure 4h and i</xref>, left; black), such that it became significantly smaller than the strength of relevant coding (<xref ref-type="fig" rid="fig4">Figure 4h and i</xref>, left; compare red and black at ‘late learning’). Finally, relevant and irrelevant directions were always strongly orthogonal in neural state space, and the level of orthogonality did not significantly change with learning (<xref ref-type="fig" rid="fig4">Figure 4h and i</xref>, right). Although we observed no learning-related changes in overlap, it may be that for stimuli that are more similar than the relevant and irrelevant features we studied here (XOR and width), the overlap between these features may decrease over learning rather than simply remaining small.</p></sec><sec id="s2-5"><title>Activity-silent, sub-threshold dynamics lead to the suppression of dynamically irrelevant stimuli</title><p>While the representation of statically irrelevant stimuli can be suppressed by simply weakening the input connections conveying information about it to the network, the suppression of dynamically irrelevant stimuli requires a mechanism that alters the dynamics of the network (since this information ultimately needs to be used by the network to achieve high performance). To gain some intuition about this mechanism, we first analyzed two-neuron networks trained on the task. To demonstrate the basic mechanism of suppression of dynamically irrelevant stimuli (i.e., weak early color coding), we compared the <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) and <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5b</xref>) regimes for these networks, as these corresponded to the minimal and maximal amount of suppression of dynamically irrelevant stimuli (<xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Activity-silent, sub-threshold dynamics lead to the suppression of dynamically irrelevant stimuli.</title><p>(<bold>a</bold>) Sub-threshold neural activity (<inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) in the full state space of an example two-neuron network over the course of a trial (from color onset) trained in the <inline-formula><mml:math id="inf66"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. Pale blue arrows show flow field dynamics (direction and magnitude of movement in the state space as a function of the momentary state). Open gray circles indicate color onset, gray squares indicate shape onset, filled gray circles indicate offset of both stimuli, and colored squares and diamonds indicate the end of the trial at 1.5 s. We plot activity separately for the three periods of the task (color period, left; shape period, middle; reward period, right). We plot dynamics without noise for visual clarity. (<bold>b</bold>) Same as panel a but for a network trained in the <inline-formula><mml:math id="inf67"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>c</bold>) Momentary magnitude of firing rates (<inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>; i.e. momentary metabolic cost, see <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) for the two-neuron networks from panels a (blue line) and b (pink line). (<bold>d</bold>) Mean ± 2 s.d. (over 10 two-neuron networks) proportion of color inputs that have a negative sign for the two noise and regularization regimes shown in panels a–c. Gray dotted line shows chance level proportion of negative color input. (<bold>e</bold>) Mean (over 10 fifty-neuron networks) proportion of color inputs that have a negative sign for all noise (horizontal axis) and regularization (colorbar) strengths shown in <xref ref-type="fig" rid="fig2">Figure 2c and d</xref>. Pale blue, purple, and pink highlights correspond to the noise and regularization strengths shown in <xref ref-type="fig" rid="fig2">Figure 2c and d</xref>. Gray dotted line and shading shows mean ± 2 s.d. (over 250 networks; 10 networks for each of the 25 different noise and regularization levels) of the proportion of negative color input prior to training (i.e. the proportion of negative color input expected when inputs are drawn randomly from a Gaussian distribution; Network optimization). (<bold>f</bold>) Momentary magnitude of firing rates (<inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>) for our lateral prefrontal cortex (lPFC) recordings (far left) and 50-neuron networks in the <inline-formula><mml:math id="inf70"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (middle left, blue shading), <inline-formula><mml:math id="inf71"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (middle right, purple shading), and <inline-formula><mml:math id="inf72"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (far right, pink shading). Error bars show the mean ± 2 s.d. over either 10 non-overlapping data splits for the data or over 10 different networks for the models. p-Values resulted from a two-sided Mann–Whitney U test (*, p&lt;0.05; **, p&lt;0.01; ***, p&lt;0.001; n.s., not significant; see Statistics).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Temporal decoding for two-neuron networks and supplemental analysis of suppression of dynamically irrelevant stimuli.</title><p>(<bold>a</bold>) Performance of a linear decoder trained at each time point to predict either XOR (far left), shape (middle left), color (middle right), or width (far right) from neural population activity from optimized two-neuron networks (<xref ref-type="fig" rid="fig5">Figure 5a–d</xref>) trained in the <inline-formula><mml:math id="inf73"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. Dotted gray lines and shading show mean ± 2 s.d. of chance level decoding based on shuffling trial labels 100 times. Horizontal black bars show significant differences between early and late decoding using a two-sided cluster-based permutation test and a significance threshold of 0.05 (Statistics). (<bold>b</bold>) Same as panel a but for two-neuron networks trained in the <inline-formula><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>c</bold>) Mean (over 10 fifty-neuron networks) proportion of color inputs that have a negative sign for all noise and regularization strengths (colorbar) plotted against early color decoding (see <xref ref-type="fig" rid="fig2">Figure 2c</xref>, middle). Pale blue, purple, and pink highlights correspond to the noise and regularization strengths shown in <xref ref-type="fig" rid="fig2">Figure 2c</xref>. Gray dotted line shows the negative identity line shifted to intercept the vertical axis at 1. (<bold>d</bold>) Momentary magnitude of firing rates (<inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>; i.e. momentary metabolic cost, see <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) for 50-neuron networks in the <inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (left, blue shading), <inline-formula><mml:math id="inf77"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (purple shading), and <inline-formula><mml:math id="inf78"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (right, pink shading). We show results after learning (black lines, ‘late learning’; repeated from <xref ref-type="fig" rid="fig5">Figure 5f</xref>) and when shuffling color, shape, and width input weights (blue lines, ‘shuffle inputs’; Measuring the magnitude of neural firing rates). Error bars show the mean ± 2 s.d. over 10 different networks. p-Values resulted from a two-sided Mann–Whitney U test (*, p&lt;0.05; **, p&lt;0.01; ***, p&lt;0.001; n.s., not significant; see Statistics).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Activity-silent, sub-threshold dynamics lead to the suppression of dynamically irrelevant stimuli in networks with a sigmoid nonlinearity.</title><p>(<bold>a</bold>) Sub-threshold neural activity (<inline-formula><mml:math id="inf79"><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>​</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) in the full state space of an example two-neuron network with a sigmoid (<inline-formula><mml:math id="inf80"><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi></mml:math></inline-formula>) nonlinearity over the course of a trial (from color onset) trained in the <inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime (Neural network models). Pale blue arrows show flow field dynamics (direction and magnitude of movement in the state space as a function of the momentary state). Open gray circles indicate color onset, gray squares indicate shape onset, filled gray circles indicate offset of both stimuli, and colored squares and diamonds indicate the end of the trial at 1.5 s. We plot activity separately for the three periods of the task (color period, left; shape period, middle; reward period, right). We plot dynamics without noise for visual clarity. (<bold>b</bold>) Same as panel a but for a network trained in the <inline-formula><mml:math id="inf82"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> regime. (<bold>c</bold>) Momentary magnitude of firing rates (<inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>; i.e. momentary metabolic cost, see <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>) for the two-neuron networks from panels a (blue line) and b (pink line). (<bold>d</bold>) Mean ± 2 s.d. (over 10 two-neuron <inline-formula><mml:math id="inf84"><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi></mml:math></inline-formula> networks) proportion of color inputs that have a negative sign for the two noise and regularization regimes shown in panels a–c. Gray dotted line shows chance level proportion of negative color input.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-fig5-figsupp2-v1.tif"/></fig></fig-group><p>We examined trajectories of sub-threshold neural activity <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>​</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) in the full two-neuron state space (<xref ref-type="fig" rid="fig5">Figure 5a and b</xref>, blue and green curves). We distinguished between the negative quadrant of state space, which corresponds to the rectified part of the firing rate nonlinearity (<xref ref-type="fig" rid="fig5">Figure 5a and b</xref>, bottom left gray quadrant; <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), and the rest of state space. Importantly, if sub-threshold activity lies within the negative quadrant at some time point, both neurons in the network have zero firing rate and consequently a decoder cannot decode any information from the firing rate activity and the network exhibits no metabolic cost (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>). Therefore, we reasoned that color inputs may drive sub-threshold activity so that it lies purely in the negative quadrant of state space so that no metabolic cost is incurred during the color period (akin to nonlinear gating <xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib45">Miller and Cohen, 2001</xref>). Later in the trial, when these color inputs are combined with the shape inputs, activity may then leave the negative quadrant of state space so that the network can perform the task.</p><p>We found that for the <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> regime, there was typically at least one set of stimulus conditions for which sub-threshold neural activity evolved outside the negative quadrant of state space during any task period (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). Consequently, color was decodable to a relatively high level during the color period (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a</xref>, middle right) and this network produced a relatively high metabolic cost throughout the task (<xref ref-type="fig" rid="fig5">Figure 5c</xref>, blue line). In contrast, for networks in the <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> regime, the two color inputs typically drove neural activity into the negative quadrant of state space during the color period (<xref ref-type="fig" rid="fig5">Figure 5b</xref>, left). Therefore, during the color period, the network produced zero firing rate activity (<xref ref-type="fig" rid="fig5">Figure 5c</xref>, pink line from open gray circle to gray square). Consequently, color was poorly decodable (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1b</xref>, middle right; black line) and the network incurred no metabolic cost during the color period. Thus, color information was represented in a sub-threshold, ‘activity-silent’ (<xref ref-type="bibr" rid="bib19">Epsztein et al., 2011</xref>) state during the color period. However, during the shape and reward periods later in the trial, the color inputs, now in combination with the shape inputs, affected the firing rate dynamics and the neural trajectories explored the full state space in a similar manner to the <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> network (<xref ref-type="fig" rid="fig5">Figure 5b</xref>, middle and right panels). Indeed, we also found that color decodability increased substantially during the shape period in the <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> network (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1b</xref>, middle right; black line). This demonstrates how color inputs can cause no change in firing rates during the color period when they are irrelevant, and yet these same inputs can be utilized later in the trial to enable high task performance (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a and b</xref>, far left). While we considered individual example networks in <xref ref-type="fig" rid="fig5">Figure 5a–c</xref>, color inputs consistently drove neural activity into the negative quadrant of state space across repeated training of <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks but not for <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks (<xref ref-type="fig" rid="fig5">Figure 5d</xref>).</p><p>Next, we performed the same analysis as <xref ref-type="fig" rid="fig5">Figure 5d</xref> on the large (50-neuron) networks that we studied previously (<xref ref-type="fig" rid="fig2">Figures 2</xref>—<xref ref-type="fig" rid="fig4">4</xref>). Similar to the two-neuron networks, we found that large networks in the <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> regime did not exhibit more negative color inputs than would be expected by chance (<xref ref-type="fig" rid="fig5">Figure 5e</xref>, pale blue highlighted point) – consistent with the high early color decoding we found previously in these networks (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, middle, pale blue highlighted point). However, when increasing either the noise or regularization level, the proportion of negative color inputs increased above chance such that for the highest noise and regularization level, nearly all color inputs were negative (<xref ref-type="fig" rid="fig5">Figure 5e</xref>). We also found a strong negative correlation between the proportion of negative color input and the level of early color decoding that we found previously in <xref ref-type="fig" rid="fig2">Figure 2c</xref> (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1c</xref>, <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.9</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). This suggests that color inputs that drive neural activity into the rectified part of the firing rate nonlinearity, and thus generate purely sub-threshold activity-silent dynamics, is the mechanism that generates weak early color coding during the color period in these networks. We also examined whether these results generalized to networks that use a sigmoid (as opposed to a ReLU) nonlinearity. To do this, we trained networks with a <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> nonlinearity (shifted for a meaningful comparison with ReLU, so that the lower bound on firing rates was 0, rather than –1) and found qualitatively similar results to the ReLU networks. In particular, color inputs drove neural activity toward 0 firing rate during the color period in <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks but not in <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>, compare a and b), which resulted in a lower metabolic cost during the color period for <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks compared to <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2c</xref>). This was reflected in color inputs being more strongly negative in <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks compared to <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks which only showed chance levels of negative color inputs (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d</xref>).</p><p>We next sought to test whether this mechanism could also explain the decrease in color decodability over learning that we observed in the lPFC data. To do this, we measured the magnitude of firing rates in the fixation, color, and shape periods for both early and late learning (note that the magnitude of firing rates coincides with our definition of the metabolic cost; <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> and <xref ref-type="fig" rid="fig5">Figure 5c</xref>). To decrease metabolic cost over learning, we would expect two changes: firing rates should decrease with learning and firing rates should not significantly increase from the fixation to the color period after learning (<xref ref-type="fig" rid="fig5">Figure 5c</xref>, pink line). Indeed, we found that firing rates decreased significantly over the course of learning in all task periods (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, far left; compare gray and black lines), and this decrease was most substantial during the fixation and color periods (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, far left). We also found that after learning, firing rates during the color period were not significantly higher than during the fixation period (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, far left; compare black error bars during fixation and color periods). During the shape period however, firing rates increased significantly compared to those during the fixation period (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, far left; compare black error bars during fixation and shape periods). Therefore, the late learning dynamics of the data are in line with what we saw for the optimized two-neuron <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> network (<xref ref-type="fig" rid="fig5">Figure 5c</xref>, pink line).</p><p>We then compared these results from our neural recordings with the results from our large networks trained in different noise and regularization regimes. We found that for <inline-formula><mml:math id="inf102"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> networks, over the course of learning, firing rates decreased slightly during the fixation and color periods but actually increased during the shape period (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, middle left pale blue shading; compare gray and black lines). Additionally, after learning, firing rates increased between fixation and color periods and between color and shape periods (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, middle left pale blue shading; compare black error bars). For <inline-formula><mml:math id="inf103"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> networks, over the course of learning, we found that firing rates did not significantly change during the fixation and color periods but increased significantly during the shape period (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, middle right purple shading; compare gray and black lines). Furthermore, after learning, firing rates did not change significantly between fixation and color periods but did increase significantly between color and shape periods (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, middle right purple shading; compare black error bars). Finally, for the <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks, we found a pattern of results that was most consistent with the data. Firing rates decreased over the course of learning in all task periods (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, far right pink shading; compare gray and black lines) and the decrease in firing rates was most substantial during the fixation and color periods. After learning, firing rates did not change significantly from fixation to color periods but did increase significantly during the shape period (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, far right; compare black error bars).</p><p>To investigate whether these findings depended on our random network initialization prior to training, we also compared late learning firing rates to firing rates that resulted from randomly shuffling the color, shape, and width inputs (which emulates alternative tasks where different combinations of color, shape, and width are relevant). For example, the relative strengths of the three inputs to the network prior to training on this task may affect how the firing rates change over learning. Under this control, we also found that <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> networks bore a close resemblance to the data (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref>, compare black lines to blue lines).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Comparing the neural representations of task-optimized networks with those observed in experimental data has been particularly fruitful in recent years (<xref ref-type="bibr" rid="bib66">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Cueva et al., 2020</xref>; <xref ref-type="bibr" rid="bib17">Echeveste et al., 2020</xref>; <xref ref-type="bibr" rid="bib62">Stroud et al., 2021</xref>; <xref ref-type="bibr" rid="bib35">Lindsay, 2021</xref>). However, networks are typically optimized using only a very limited range of hyperparameter values (<xref ref-type="bibr" rid="bib66">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">Cueva et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Driscoll et al., 2022</xref>; <xref ref-type="bibr" rid="bib60">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib17">Echeveste et al., 2020</xref>; <xref ref-type="bibr" rid="bib62">Stroud et al., 2021</xref>). Instead, here we showed that different settings of key, biologically relevant hyperparameters such as noise and metabolic costs, can yield a variety of qualitatively different dynamical regimes that bear varying degrees of similarity with experimental data. In general, we found that increasing levels of noise and firing rate regularization led to increasing amounts of irrelevant information being filtered out in the networks. Indeed, filtering out of task-irrelevant information is a well-known property of the PFC and has been observed in a variety of tasks (<xref ref-type="bibr" rid="bib23">Freedman et al., 2001</xref>; <xref ref-type="bibr" rid="bib15">Duncan, 2001</xref>; <xref ref-type="bibr" rid="bib10">Cueva et al., 2020</xref>; <xref ref-type="bibr" rid="bib52">Reinert et al., 2021</xref>; <xref ref-type="bibr" rid="bib45">Miller and Cohen, 2001</xref>; <xref ref-type="bibr" rid="bib51">Rainer and Miller, 2002</xref>; <xref ref-type="bibr" rid="bib1">Asaad et al., 2000</xref>; <xref ref-type="bibr" rid="bib20">Everling et al., 2002</xref>). We provide a mechanistic understanding of the specific conditions that lead to stronger filtering of task-irrelevant information. We predict that these results should also generalize to richer, more complex cognitive tasks that may, for example, require context switching (<xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib52">Reinert et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Asaad et al., 2000</xref>) or invoke working memory (<xref ref-type="bibr" rid="bib23">Freedman et al., 2001</xref>; <xref ref-type="bibr" rid="bib50">Rainer et al., 1998</xref>; <xref ref-type="bibr" rid="bib1">Asaad et al., 2000</xref>). Indeed, filtering out of task-irrelevant information in the PFC has been observed in such tasks (<xref ref-type="bibr" rid="bib23">Freedman et al., 2001</xref>; <xref ref-type="bibr" rid="bib50">Rainer et al., 1998</xref>; <xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib52">Reinert et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Asaad et al., 2000</xref>; <xref ref-type="bibr" rid="bib39">Mack et al., 2020</xref>).</p><p>Our results are also likely a more general finding of neural circuits that extend beyond the PFC. In line with this, it has previously been shown that strongly regularized neural network models trained to reproduce monkey muscle activities during reaching bore a stronger resemblance to neural recordings from primary motor cortex compared to unregularized models (<xref ref-type="bibr" rid="bib64">Sussillo et al., 2015</xref>). In related work on motor control, recurrent networks controlled by an optimal feedback controller recapitulated key aspects of experimental recordings from primary motor cortex (such as orthogonality between preparatory and movement neural activities) when the control input was regularized (<xref ref-type="bibr" rid="bib31">Kao et al., 2021</xref>). Additionally, regularization of neural firing rates, and its natural biological interpretation as a metabolic cost, has recently been shown to be a key ingredient for the formation of grid cell-like response profiles in artificial networks (<xref ref-type="bibr" rid="bib67">Whittington et al., 2022</xref>; <xref ref-type="bibr" rid="bib9">Cueva and Wei, 2018</xref>).</p><p>By showing that PFC representations changed in line with a minimal representational strategy, our results are in line with various studies showing low-dimensional representations under a variety of tasks in the PFC and other brain regions (<xref ref-type="bibr" rid="bib50">Rainer et al., 1998</xref>; <xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib10">Cueva et al., 2020</xref>; <xref ref-type="bibr" rid="bib25">Ganguli et al., 2008</xref>; <xref ref-type="bibr" rid="bib59">Sohn et al., 2019</xref>). This is in contrast to several previous observations of high-dimensional neural activity in PFC (<xref ref-type="bibr" rid="bib54">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>). Both high- and low-dimensional regimes confer distinct yet useful benefits: high-dimensional representations allow many behavioral readouts to be generated, thereby enabling highly flexible behavior (<xref ref-type="bibr" rid="bib54">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib18">Enel et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Maass et al., 2002</xref>; <xref ref-type="bibr" rid="bib24">Fusi et al., 2016</xref>), whereas low-dimensional representations are more robust to noise and allow for generalization across different stimuli (<xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib24">Fusi et al., 2016</xref>). These two different representational strategies have previously been studied in models by setting the initial network weights to either small values (to generate low-dimensional ‘rich’ representations) or large values (<xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>) (to generate high-dimensional ‘lazy’ representations). However, in contrast to previous approaches, we studied the more biologically plausible effects of firing rate regularization (i.e. a metabolic cost; see also the supplement of <xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>) on the network activities over the course of learning and compared them to learning-related changes in PFC neural activities. Firing rate regularization will cause neural activities to wax and wane as networks are exposed to new tasks depending upon the stimuli that are currently relevant. In line with this, it is conceivable that prolonged exposure to a task that has a limited number of stimulus conditions, some of which can even be generalized over (as was the case for our task), encourages more low-dimensional dynamics to form (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>; <xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib14">Dubreuil et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Fusi et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Musslick, 2017</xref>; <xref ref-type="bibr" rid="bib43">Mastrogiuseppe and Ostojic, 2018</xref>). In contrast, tasks that use a rich variety of stimuli (that may even dynamically change over the task <xref ref-type="bibr" rid="bib66">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib29">Jensen et al., 2023</xref>; <xref ref-type="bibr" rid="bib28">Heald et al., 2021</xref>), and which do not involve generalization across stimulus conditions, may more naturally lead to high-dimensional representations (<xref ref-type="bibr" rid="bib54">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Dubreuil et al., 2022</xref>; <xref ref-type="bibr" rid="bib24">Fusi et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Mastrogiuseppe and Ostojic, 2018</xref>; <xref ref-type="bibr" rid="bib3">Bartolo et al., 2020</xref>). It would therefore be an important future question to understand how our results also depend on the task being studied as some tasks may more naturally lead to the ‘maximal’ representational regime (<xref ref-type="bibr" rid="bib2">Barak et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Dubreuil et al., 2022</xref>; <xref ref-type="bibr" rid="bib43">Mastrogiuseppe and Ostojic, 2018</xref>; <xref ref-type="fig" rid="fig1">Figures 1</xref>—<xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig5">5</xref>, blue shading).</p><p>A previous analysis of the same dataset that we studied here focused on the late parts of the trial (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>). In particular, they found that the final result of the computation needed for the task, the XOR operation between color and shape, emerges and eventually comes to dominate lPFC representations over the course of learning in the late shape period (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>). Our analysis goes beyond this by studying mechanisms of suppression of both static and dynamically irrelevant stimuli across all task periods and how different levels of neural noise and metabolic cost can lead to qualitatively different representations of irrelevant stimuli in task-optimized recurrent networks. Other previous studies focused on characterizing the representation of several task-relevant (<xref ref-type="bibr" rid="bib54">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Stokes et al., 2013</xref>) – and, in some cases, -irrelevant (<xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>) – variables over the course of individual trials. Characterizing how key aspects of neural representations change over the course of learning, as we did here, offers unique opportunities for studying the functional objectives shaping neural representations (<xref ref-type="bibr" rid="bib53">Richards et al., 2019</xref>).</p><p>There were several aspects of the data that were not well captured by our models. For example, during the shape period, decodability of shape decreased while decodability of color increased (although not significantly) in our neural recordings (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). These differences in changes in decoding may be due to fundamentally different ways that brains encode sensory information upstream of PFC, compared to our models. For example, shape and width are both geometric features of the stimulus, whose encoding is differentiated from that of color already at early stages of visual processing (<xref ref-type="bibr" rid="bib30">Kandel, 2000</xref>). Such a hierarchical representation of inputs may automatically lead to the (un)learning about the relevance of width (which the <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> model reproduced) generalizing to shape, but not to color. In contrast, inputs in our model used a non-hierarchical one-hot encoding (<xref ref-type="fig" rid="fig2">Figure 2</xref>), which did not allow for such generalization. Moreover, in the data, we may expect width to be a priori more strongly represented than color or shape because it is a much more potent sensory feature. In line with this, in our neural recordings, we found that width was very strongly represented in early learning compared to the other stimulus features (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, far right) and width always yielded high cross-generalized decoding – even after learning (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>, far right). Nevertheless, studying <italic>changes</italic> in decoding over learning, rather than absolute decoding levels, allowed us to focus on features of learning that do not depend on the details of upstream sensory representations of stimuli. Future studies could incorporate aspects of sensory representations that we ignored here by using stimulus inputs with which the model more faithfully reproduces the experimentally observed initial decodability of stimulus features.</p><p>In line with previous studies (<xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Whittington et al., 2022</xref>; <xref ref-type="bibr" rid="bib64">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib31">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="bib10">Cueva et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Driscoll et al., 2022</xref>; <xref ref-type="bibr" rid="bib60">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Stroud et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Masse et al., 2019</xref>; <xref ref-type="bibr" rid="bib56">Schimel et al., 2023</xref>), we operationalized metabolic cost in our models through <italic>L</italic><sub>2</sub> firing rate regularization. This cost penalizes high overall firing rates. There are however alternative conceivable ways to operationalize a metabolic cost; e.g., <italic>L</italic><sub>1</sub> firing rate regularization has been used previously when optimizing neural networks and promotes more sparse neural firing (<xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>). Interestingly, although our <italic>L</italic><sub>2</sub> is generally conceived to be weaker than <italic>L</italic><sub>1</sub> regularization, we still found that it encouraged the network to use purely sub-threshold activity in our task. The regularization of synaptic weights may also be biologically relevant (<xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>) because synaptic transmission uses the most energy in the brain compared to other processes (<xref ref-type="bibr" rid="bib21">Faria-Pereira and Morais, 2022</xref>; <xref ref-type="bibr" rid="bib26">Harris et al., 2012</xref>). Additionally, even sub-threshold activity could be regularized as it also consumes energy (although orders of magnitude less than spiking <xref ref-type="bibr" rid="bib70">Zhu et al., 2019</xref>). Therefore, future work will be needed to examine how different metabolic costs affect the dynamics of task-optimized networks.</p><p>We build on several previous studies that have also analyzed learning-related changes in PFC activity (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>; <xref ref-type="bibr" rid="bib52">Reinert et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">Durstewitz et al., 2010</xref>; <xref ref-type="bibr" rid="bib3">Bartolo et al., 2020</xref>) – although these studies typically used reversal-learning paradigms in which animals are already highly task proficient and the effects of learning and task switching are inseparable. For example, in a rule-based categorization task in which the categorization rule changed after learning an initial rule, neurons in mouse PFC adjusted their selectivity depending on the rule such that currently irrelevant information was not represented (<xref ref-type="bibr" rid="bib52">Reinert et al., 2021</xref>). Similarly, neurons in rat PFC transition rapidly from representing a familiar rule to representing a completely novel rule through trial-and-error learning (<xref ref-type="bibr" rid="bib16">Durstewitz et al., 2010</xref>). Additionally, the dimensionality of PFC representations was found to increase as monkeys learned the value of novel stimuli (<xref ref-type="bibr" rid="bib3">Bartolo et al., 2020</xref>). Importantly, however, PFC representations did not distinguish between novel stimuli when they were first chosen. It was only once the value of the stimuli were learned that their representations in PFC were distinguishable (<xref ref-type="bibr" rid="bib3">Bartolo et al., 2020</xref>). These results are consistent with our results where we found poor XOR decoding during the early stages of learning which then increased over learning as the monkeys learned the rewarded conditions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, far left). However, we also observed high decoding of width during early learning which was not predictive of reward (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, far right). One key distinction between our study and that of <xref ref-type="bibr" rid="bib3">Bartolo et al., 2020</xref>, is that our recordings commenced from the first trial the monkeys were exposed to the task. In contrast, in <xref ref-type="bibr" rid="bib3">Bartolo et al., 2020</xref>, the monkeys were already highly proficient at the task and so the neural representation of their task was already likely strongly task specific by the time recordings were taken.</p><p>In line with our approach here, several recent studies have also examined the effects of different hyperparameter settings on the solution that optimized networks exhibit. One study found that decreasing regularization on network weights led to more sequential dynamics in networks optimized to perform working memory tasks (<xref ref-type="bibr" rid="bib48">Orhan and Ma, 2019</xref>). Another study found that the number of functional clusters that a network exhibits does not depend strongly on the strength of (<italic>L</italic><sub>1</sub> rate or weight) regularization, but did depend upon whether the single neuron nonlinearity saturates at high firing rates (<xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>). It has also been shown that networks optimized to perform path integration can exhibit a range of different properties, from grid cell-like receptive fields to distinctly non grid cell-like receptive fields, depending upon biologically relevant hyperparameters – including noise and regularization (<xref ref-type="bibr" rid="bib67">Whittington et al., 2022</xref>; <xref ref-type="bibr" rid="bib9">Cueva and Wei, 2018</xref>; <xref ref-type="bibr" rid="bib55">Schaeffer et al., 2022</xref>). Indeed, in addition to noise and regularization, various other hyperparameters have also been shown to affect the representational strategy used by a circuit, such as the firing rate nonlinearity (<xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Whittington et al., 2022</xref>; <xref ref-type="bibr" rid="bib55">Schaeffer et al., 2022</xref>) and network weight initialization (<xref ref-type="bibr" rid="bib22">Flesch et al., 2022</xref>; <xref ref-type="bibr" rid="bib55">Schaeffer et al., 2022</xref>). It is therefore becoming increasingly clear that analyzing the interplay between learning and biological constraints will be key for understanding the computations that various brain regions perform.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Experimental materials and methods</title><p>Experimental methods have been described previously (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>). The experiments were conducted in line with the Animals (Scientific Procedures) Act 1986 of the UK and licensed by a Home Office Project License obtained after review by Oxford University’s Animal Care and Ethical Review committee. The procedures followed the standards set out in the European Community for the care and use of laboratory animals (EUVD, European Union directive 86/609/EEC). Briefly, two adult male rhesus macaques (monkey 1 and monkey 2) performed a passive object–association task (<xref ref-type="fig" rid="fig1">Figure 1a and b</xref>; see the main text ‘A task involving relevant and irrelevant stimuli’ for a description of the task). Neural recordings commenced from the first session the animals were exposed to the task. All trials with fixation errors were excluded. The dataset contained on average 237.9 (s.d. = 23.9) and 104.8 (s.d. = 2.3) trials for each of the eight conditions for monkeys 1 and 2, respectively. Data were recorded from the ventral and dorsal lPFC over a total of 27 daily sessions across both monkeys which yielded 146 and 230 neurons for monkey 1 and monkey 2, respectively. To compute neural firing rates, we convolved binary spike trains with a Gaussian kernel with a standard deviation of 50 ms. In order to characterize changes in neural dynamics over learning, analyses were performed separately on the first half of sessions (‘early learning’; 9 and 5 sessions from monkey 1 and monkey 2, respectively) and the second half of sessions (‘late learning’; 8 and 5 sessions from monkey 1 and monkey 2, respectively; <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>). This experiment was only performed once in these two animals.</p></sec><sec id="s4-2"><title>Neural network models</title><p>The dynamics of our simulated networks evolved according to <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref> and are repeated here:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mtext>with</mml:mtext><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mtext>and</mml:mtext><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to the vector of ‘sub-threshold’ activities of the <inline-formula><mml:math id="inf108"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons in the network, <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is their momentary firing rates, obtained as a rectified linear function (ReLU) of their sub-threshold activities (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>; except for the networks of <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref> in which we used a <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> nonlinearity to examine the generalizability of our results), <italic>τ</italic>=50 ms is the effective time constant, <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the recurrent weight matrix, <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the total stimulus input, <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a stimulus-independent bias, <italic>σ</italic> is the standard deviation of the neural noise process, <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mo>​</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a sample from a Gaussian white noise process with mean 0 and variance 1, <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are color, shape, and width input weights, respectively, and <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are one-hot encodings of color, shape, and width inputs, respectively.</p><p>All simulations started at <inline-formula><mml:math id="inf119"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula> s and lasted until <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mo movablelimits="true" form="prefix">max</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> s, and consisted of a fixation (–0.5 to 0 s), color (0–0.5 s), shape (0.5–1 s), and reward (1–1.5 s) period (<xref ref-type="fig" rid="fig1">Figures 1a</xref> and <xref ref-type="fig" rid="fig2">2a</xref>). The initial condition of neural activity was set to <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. In line with the task, elements of <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> were set to 0 outside the color and shape periods, and elements of both <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> were set to 0 outside the shape period. All networks used <italic>N</italic>=50 neurons (except for <xref ref-type="fig" rid="fig5">Figure 5a–d</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a and b</xref> which used <italic>N</italic>=2 neurons). We solved the dynamics of <xref ref-type="disp-formula" rid="equ5 equ6 equ7">Equations 5–7</xref> using a first-order Euler–Maruyama approximation with a discretization time step of 1 ms.</p><sec id="s4-2-1"><title>Network optimization</title><p>Choice probabilities were computed through a linear readout of network activities:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the readout weights and <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a readout bias. To measure network performance, we used a canonical cost function (<xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib13">Driscoll et al., 2022</xref>; <xref ref-type="bibr" rid="bib60">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Stroud et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Masse et al., 2019</xref>; <xref ref-type="disp-formula" rid="equ3 equ4">Equations 3 and 4</xref>). We repeat the cost function from the main text here:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">H</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>where the first term is a task performance term which consists of the cross-entropy loss <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">H</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="italic">l</mml:mi><mml:mi mathvariant="italic">n</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> between the one-hot encoded target choice, <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula> (based on the stimuli of the given trial, as defined by the task rules, <xref ref-type="fig" rid="fig1">Figure 1b</xref>), and the network’s readout probabilities, <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>). Note that we measure total classification performance (cross-entropy loss) during the reward period (integral in the first term runs from <italic>t</italic>=1.0 to <italic>t</italic>=1.5; <xref ref-type="fig" rid="fig2">Figure 2a</xref>, bottom; yellow shading), as appropriate for the task. The second term in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> is a widely used (<xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib67">Whittington et al., 2022</xref>; <xref ref-type="bibr" rid="bib64">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Orhan and Ma, 2019</xref>; <xref ref-type="bibr" rid="bib31">Kao et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Driscoll et al., 2022</xref>; <xref ref-type="bibr" rid="bib60">Song et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Stroud et al., 2021</xref>; <xref ref-type="bibr" rid="bib42">Masse et al., 2019</xref>) <italic>L</italic><sub>2</sub> regularization term (with strength <italic>λ</italic>) applied to the neural firing rates throughout the trial (integral in the second term runs from <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>t</italic>=1.5).</p><p>We initialized the free parameters of the network (the elements of <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) by sampling (independently) from a normal distribution of mean 0 and standard deviation <inline-formula><mml:math id="inf133"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. There were two exceptions to this: we also investigated the effects of initializing the elements of the input weights (<inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>) to either 0 (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>) or sampling their elements from a normal distribution of mean 0 and standard deviation <inline-formula><mml:math id="inf135"><mml:mrow><mml:mn>10</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). After initialization, we optimized these parameters using gradient descent with Adam (<xref ref-type="bibr" rid="bib33">Kingma and Ba, 2014</xref>), where gradients were obtained from backpropagation through time. We used a learning rate of 0.001 and trained networks for 1000 iterations using a batch size of 10. For each noise <italic>σ</italic> and regularization level <italic>λ</italic> (see <xref ref-type="table" rid="table1">Table 1</xref>), we optimized 10 networks with different random initializations of the network parameters.</p></sec></sec><sec id="s4-3"><title>Analysis methods</title><p>Here, we describe methods that we used to analyze neural activities. No data were excluded from our analyses. Whenever applicable, the same processing and analysis steps were applied to both experimentally recorded and model simulated data. All neural firing rates were sub-sampled at a 10 ms resolution and, unless stated otherwise, we did not trial-average firing rates before performing analyses. Analyses were either performed at every time point in the trial (<xref ref-type="fig" rid="fig2">Figures 2b</xref> and <xref ref-type="fig" rid="fig3">3a, b</xref>, <xref ref-type="fig" rid="fig5">Figure 5a–c</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a and b</xref>), at the end of either the color (<xref ref-type="fig" rid="fig2">Figure 2c and d</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1c</xref>; ‘early color decoding’) or shape periods (<xref ref-type="fig" rid="fig2">Figure 2c and d</xref>, ‘width decoding’), during time periods of significant changes in decoding over learning in the data (<xref ref-type="fig" rid="fig3">Figure 3c and d</xref>), during the final 100 ms of the shape period (<xref ref-type="fig" rid="fig4">Figure 4e–i</xref>), or during the final 100 ms of the fixation, color, and shape periods (<xref ref-type="fig" rid="fig5">Figure 5f</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref>).</p><sec id="s4-3-1"><title>Linear decoding</title><p>For decoding analyses (<xref ref-type="fig" rid="fig2">Figures 2b–d</xref>–<xref ref-type="fig" rid="fig3">3</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a–c</xref>), we fitted decoders using linear support vector machines to decode the main task variables: color, shape, width, and the XOR between color and shape. We measured decoding performance in a cross-validated way, using separate sets of trials to train and test the decoders, and we show results averaged over 10 random 1:1 train:test splits. For firing rates resulting from simulated neural networks, we used 10 trials for both the train and test splits. Chance level decoding was always 0.5 as all stimuli were binary.</p><p>For showing changes in decoding over learning (<xref ref-type="fig" rid="fig3">Figure 3c and d</xref>), we identified time periods of significant changes in decoding during the color and shape periods in the data (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, horizontal black bars; see Statistics), and show the mean change in decoding during these time periods for both the data and models (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, horizontal black lines). We used the same time periods when showing changes in cross-generalized decoding over learning (<xref ref-type="fig" rid="fig3">Figure 3d</xref>, see below).</p><p>For cross-generalized decoding (<xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>; <xref ref-type="fig" rid="fig3">Figure 3d</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), we used the same decoding approach as described above, except that cross-validation was performed across trials corresponding to different stimulus conditions. Specifically, following the approach outlined in <xref ref-type="bibr" rid="bib4">Bernardi et al., 2020</xref>, because there were three binary stimuli in our task (color, shape, and width), there were eight different trial conditions (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Therefore, for each task variable (we focused on color, shape, width, and the XOR between color and shape), there were six different ways of choosing two of the four conditions corresponding to each of the two possible stimuli for that task variable (e.g. color 1 vs. color 2). For example, when training a decoder to decode color, there were six different ways of choosing two conditions corresponding to color 1, and six different ways of choosing two conditions corresponding to color 2 (the remaining four conditions were then used for testing the decoder). Therefore, for each task variable, there were 6×6 = 36 different ways of creating training and testing sets that corresponded to different stimulus conditions. We then took the mean decoding accuracy we obtained across all 36 different training and testing sets.</p></sec><sec id="s4-3-2"><title>Measuring stimulus coding strength</title><p>In line with our mathematical analysis (Mathematical analysis of relevant and irrelevant stimulus coding in a linear network, and in particular <xref ref-type="disp-formula" rid="equ11">Equation S1</xref>) and in line with previous studies (<xref ref-type="bibr" rid="bib40">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib14">Dubreuil et al., 2022</xref>), to measure the strength of stimulus coding for relevant and irrelevant stimuli, we fitted the following linear regression model<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ϵ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is size <italic>K</italic>×<italic>N</italic> (where <italic>K</italic> is the number of trials) and corresponds to the neural firing rates, <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is size <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> where the first column is all 1 (thus encoding the mean firing rate of each neuron across all conditions) and elements of the final two columns are either –0.5 or 0.5 depending upon whether trials correspond to XOR 1 or XOR 2 (relevant) conditions (column 2) or whether trials correspond to width 1 or width 2 (irrelevant) conditions (column 3). The coefficients to be fitted (<inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) is size <inline-formula><mml:math id="inf140"><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula> and has the following structure <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <italic>µ</italic> is the mean across all conditions for each neuron, <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the coefficients corresponding to relevant (XOR) conditions, and <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the coefficients corresponding to irrelevant (width) conditions. Finally, <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">ϵ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is size <italic>K</italic>×<italic>N</italic> and contains the residuals. Note that calculating the mean difference in firing rate between the two relevant conditions and between the two irrelevant conditions would yield identical estimations of <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> because our stimuli are binary. (We also fitted decoders to decode either relevant or irrelevant conditions and extracted their coefficients to instead obtain <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and obtained near-identical results.)</p><p>We then calculated the Euclidean norm of both <inline-formula><mml:math id="inf149"><mml:mrow><mml:mi>Δ</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>Δ</mml:mi><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>r</mml:mi></mml:mstyle><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> normalized by the number of neurons (<inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the absolute value of the normalized dot product (overlap) between them (<inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig4">Figure 4e–i</xref>). For our neural recordings (<xref ref-type="fig" rid="fig4">Figure 4h</xref>), we calculated these quantities separately for 10 non-overlapping splits of the data. For our neural networks (<xref ref-type="fig" rid="fig4">Figure 4e–g, i</xref>), we calculated these quantities separately for 10 different networks.</p></sec><sec id="s4-3-3"><title>Measuring the magnitude of neural firing rates</title><p>We first calculated the firing rate for each condition and time point and then calculated the mean (across conditions and time points) Euclidean norm of firing rates appropriately normalized by the number of neurons: <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig5">Figure 5f</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref>). For our neural recordings (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, far left), we calculated this separately for 10 non-overlapping splits of the data. For our neural networks (<xref ref-type="fig" rid="fig5">Figure 5f</xref>, middle left, middle right, and far right, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref>), we calculated this separately for 10 different networks. For our optimized neural networks, to emulate alternative tasks where different combinations of color, shape, and width are relevant, we also performed the same analysis when shuffling all 6 input weights (after learning) in all 14 possible ways (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref>, blue lines, ‘shuffle inputs’). These 14 possible shuffles excluded the original setup of input weights, any re-ordering of inputs within a single input channel (since any re-ordering within a single input channel would be identical up to a re-labeling), and any re-ordering between the shape and width input channels (since any re-ordering within the shape and width input channels would also be identical up to a re-labeling).</p></sec></sec><sec id="s4-4"><title>Statistics</title><p>For decoding analyses, we used non-parametric permutation tests to calculate statistical significance. We used 100 different random shuffles of condition labels across trials to generate a null distribution for decoding accuracy. We plotted chance level decoding (<xref ref-type="fig" rid="fig3">Figure 3a and b</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a and b</xref>) by combining both early and late learning null distributions.</p><p>To calculate significant differences in decoding accuracy over learning (<xref ref-type="fig" rid="fig3">Figure 3a and b</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a and b</xref>), our test statistic was the difference in decoding accuracy between early and late learning, and our null distribution was the difference in decoding accuracy between early and late learning for the 100 different shuffles of condition labels (see above). We calculated two-tailed p-values for all tests. Additionally, to control for time-related cluster-based errors, we also added a cluster-based permutation correction (<xref ref-type="bibr" rid="bib41">Maris and Oostenveld, 2007</xref>).</p><p>For all other tests, we used a two-sided Mann–Whitney U test (<xref ref-type="fig" rid="fig4">Figures 4h, i</xref>, <xref ref-type="fig" rid="fig5">5f</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref>).</p></sec><sec id="s4-5"><title>Randomization</title><p>No new experimental data was gathered for this paper. Previously collected experimental data contained no groups. Trial types were randomly determined by a computer program.</p></sec><sec id="s4-6"><title>Blinding</title><p>As data collection had been performed well before the development of our models and our corresponding analyses were performed, it was effectively blind to the purposes of our study. Data analysis was not performed blind to the conditions of the experiments. For previously collected experimental data, there was no blinding as there was no group allocation.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Parameters used in the simulations of our models.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Symbol</th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3b-d</xref><xref ref-type="fig" rid="fig4">Figure 4e-g,i</xref>, <xref ref-type="fig" rid="fig5">Figure 5e,f</xref> <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a-c</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplements 2</xref> and <xref ref-type="fig" rid="fig3s3">3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1b,c</xref>, and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1c,d</xref></th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig5">Figure 5a-d</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a,b</xref></th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref></th><th align="left" valign="bottom"><xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1d</xref></th><th align="left" valign="bottom">Units</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>N</italic></td><td align="char" char="." valign="bottom">50</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">2</td><td align="char" char="." valign="bottom">50</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">number of neurons</td></tr><tr><td align="left" valign="bottom"><italic>t</italic><sub>0</sub></td><td align="char" char="." valign="bottom">–0.5</td><td align="left" valign="bottom">–0.5</td><td align="left" valign="bottom">–0.5</td><td align="char" char="." valign="bottom">–0.5</td><td align="left" valign="bottom">s</td><td align="left" valign="bottom">simulation start time</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf155"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="char" char="." valign="bottom">1.5</td><td align="left" valign="bottom">1.5</td><td align="left" valign="bottom">1.5</td><td align="char" char="." valign="bottom">1.5</td><td align="left" valign="bottom">s</td><td align="left" valign="bottom">simulation end time</td></tr><tr><td align="left" valign="bottom"><italic>τ</italic></td><td align="char" char="." valign="bottom">0.05</td><td align="left" valign="bottom">0.05</td><td align="left" valign="bottom">0.05</td><td align="char" char="." valign="bottom">0.05</td><td align="left" valign="bottom">s</td><td align="left" valign="bottom">effective time constant</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf156"><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula></td><td align="left" valign="bottom">ReLU</td><td align="left" valign="bottom">ReLU</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf157"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">tan</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula></td><td align="left" valign="bottom">ReLU</td><td align="left" valign="bottom">Hz</td><td align="left" valign="bottom">nonlinearity</td></tr><tr><td align="left" valign="bottom"><italic>σ</italic></td><td align="left" valign="bottom">variable<italic><sup><xref ref-type="table-fn" rid="table1fn1">*</xref></sup></italic></td><td align="left" valign="bottom">[0.01, 0.255]</td><td align="left" valign="bottom">[0.01, 0.255]</td><td align="char" char="." valign="bottom">0.01</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">noise standard deviation</td></tr><tr><td align="left" valign="bottom"><italic>λ</italic></td><td align="left" valign="bottom">variable<italic><sup><xref ref-type="table-fn" rid="table1fn2">†</xref></sup></italic></td><td align="left" valign="bottom">[0.0005, 0.02525]</td><td align="left" valign="bottom">[0.0005, 0.02525]</td><td align="char" char="." valign="bottom">0.5</td><td align="left" valign="bottom">s</td><td align="left" valign="bottom">regularization strength</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">s</td><td align="left" valign="bottom">weight matrix</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">color input weights</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">shape input weights</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">width input weights</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">bias</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">s</td><td align="left" valign="bottom">readout weights</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">optimized <italic><sup><xref ref-type="table-fn" rid="table1fn3">‡</xref></sup></italic></td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">readout bias</td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p> The noise standard deviation <italic>σ</italic> took one of 5 evenly spaced values between 0.01 and 0.5 (see <xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p></fn><fn id="table1fn2"><label>†</label><p>The firing rate regularization strength <italic>λ</italic> took one of 5 evenly spaced values between 0.0005 and 0.05 (see <xref ref-type="fig" rid="fig2">Figure 2c</xref>).</p></fn><fn id="table1fn3"><label>‡</label><p> See Neural network models section for details.</p></fn></table-wrap-foot></table-wrap></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Data curation, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Data curation, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Supervision, Funding acquisition, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Data curation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Resources, Formal analysis, Supervision, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-94961-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code was custom written in Python using NumPy, SciPy, Matplotlib, Scikit-learn, and Tensorflow. All code is available in <ext-link ext-link-type="uri" xlink:href="https://github.com/jakepstroud/optimizing_RNNs_XOR_task">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib63">Stroud, 2023</xref>). All neural recordings have been deposited at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.c2fqz61kb">Dryad</ext-link>.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Wojic</surname><given-names>M</given-names></name><name><surname>Stroud</surname><given-names>J</given-names></name><name><surname>Wasmuht</surname><given-names>D</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Kadohisa</surname><given-names>M</given-names></name><name><surname>Buckley</surname><given-names>M</given-names></name><name><surname>Myers</surname><given-names>N</given-names></name><name><surname>Hunt</surname><given-names>L</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Stokes</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Electrophysiological recordings of prefrontal activity over learning in non-human primates</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.c2fqz61kb</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was funded by the Wellcome Trust (Investigator Award in Science 212262/Z/18/Z to ML, Sir Henry Wellcome Postdoctoral Fellowship 215909/Z/19/Z to JS, and award 101092/Z/13/Z to MK, MK, and JD), the Human Frontier Science Programme (Research Grant RGP0044/2018 to ML), the Medical Research Council UK Program (MC_UU_00030/7 to MK, MK, and JD), the Biotechnology and Biological Sciences Research Council (award BB/M010732/1 to MGS), a Gates Cambridge scholarship (to KTJ), a Clarenden scholarship and Saven European scholarship (to MW), and the James S McDonnell Foundation (award 220020405 to MGS). For the purpose of open access, the authors have applied a CC-BY public copyright licence to any author accepted manuscript version arising from this submission. We thank Christopher Summerfield for useful feedback and detailed comments on the manuscript</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaad</surname><given-names>WF</given-names></name><name><surname>Rainer</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Task-specific neural activity in the primate prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>84</volume><fpage>451</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1152/jn.2000.84.1.451</pub-id><pub-id pub-id-type="pmid">10899218</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>From fixed points to chaos: three models of delayed discrimination</article-title><source>Progress in Neurobiology</source><volume>103</volume><fpage>214</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2013.02.002</pub-id><pub-id pub-id-type="pmid">23438479</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartolo</surname><given-names>R</given-names></name><name><surname>Saunders</surname><given-names>RC</given-names></name><name><surname>Mitz</surname><given-names>AR</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dimensionality, information and learning in prefrontal cortex</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007514</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007514</pub-id><pub-id pub-id-type="pmid">32330126</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernardi</surname><given-names>S</given-names></name><name><surname>Benna</surname><given-names>MK</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Munuera</surname><given-names>J</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The geometry of abstraction in the hippocampus and prefrontal cortex</article-title><source>Cell</source><volume>183</volume><fpage>954</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.09.031</pub-id><pub-id pub-id-type="pmid">33058757</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The variable nature of cognitive control: A dual mechanisms framework</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>106</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.12.010</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chadwick</surname><given-names>A</given-names></name><name><surname>Khan</surname><given-names>AG</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>Blot</surname><given-names>A</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Learning shapes cortical dynamics to enhance integration of relevant sensory input</article-title><source>Neuron</source><volume>111</volume><fpage>106</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.10.001</pub-id><pub-id pub-id-type="pmid">36283408</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural variability in premotor cortex provides a signature of motor preparation</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>3697</fpage><lpage>3712</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3762-05.2006</pub-id><pub-id pub-id-type="pmid">16597724</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>Padamsey</surname><given-names>Z</given-names></name><name><surname>D’Amour</surname><given-names>JA</given-names></name><name><surname>Emptage</surname><given-names>NJ</given-names></name><name><surname>Froemke</surname><given-names>RC</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Synaptic transmission optimization predicts expression loci of long-term plasticity</article-title><source>Neuron</source><volume>96</volume><fpage>177</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.021</pub-id><pub-id pub-id-type="pmid">28957667</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</article-title><conf-name>6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings</conf-name><fpage>1</fpage><lpage>19</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Saez</surname><given-names>A</given-names></name><name><surname>Marcos</surname><given-names>E</given-names></name><name><surname>Genovesio</surname><given-names>A</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Low-dimensional dynamics for working memory and time encoding</article-title><source>PNAS</source><volume>117</volume><fpage>23021</fpage><lpage>23032</lpage><pub-id pub-id-type="doi">10.1073/pnas.1915984117</pub-id><pub-id pub-id-type="pmid">32859756</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficient computation and cue integration with noisy population codes</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>826</fpage><lpage>831</lpage><pub-id pub-id-type="doi">10.1038/90541</pub-id><pub-id pub-id-type="pmid">11477429</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Driscoll</surname><given-names>L</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Flexible Multitask Computation in Recurrent Networks Utilizes Shared Dynamical Motifs</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.08.15.503870</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Valente</surname><given-names>A</given-names></name><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The role of population structure in computations through neural dynamics</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>783</fpage><lpage>794</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01088-4</pub-id><pub-id pub-id-type="pmid">35668174</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An adaptive coding model of neural function in prefrontal cortex</article-title><source>Nature Reviews. Neuroscience</source><volume>2</volume><fpage>820</fpage><lpage>829</lpage><pub-id pub-id-type="doi">10.1038/35097575</pub-id><pub-id pub-id-type="pmid">11715058</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Vittoz</surname><given-names>NM</given-names></name><name><surname>Floresco</surname><given-names>SB</given-names></name><name><surname>Seamans</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Abrupt transitions between prefrontal neural ensemble states accompany behavioral transitions during rule learning</article-title><source>Neuron</source><volume>66</volume><fpage>438</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.03.029</pub-id><pub-id pub-id-type="pmid">20471356</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Echeveste</surname><given-names>R</given-names></name><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical-like dynamics in recurrent circuits optimized for sampling-based probabilistic inference</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1138</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0671-1</pub-id><pub-id pub-id-type="pmid">32778794</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enel</surname><given-names>P</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name><name><surname>Quilodran</surname><given-names>R</given-names></name><name><surname>Dominey</surname><given-names>PF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reservoir computing properties of neural dynamics in prefrontal cortex</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004967</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004967</pub-id><pub-id pub-id-type="pmid">27286251</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epsztein</surname><given-names>J</given-names></name><name><surname>Brecht</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Intracellular determinants of hippocampal CA1 place and silent cell activity in a novel environment</article-title><source>Neuron</source><volume>70</volume><fpage>109</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.03.006</pub-id><pub-id pub-id-type="pmid">21482360</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everling</surname><given-names>S</given-names></name><name><surname>Tinsley</surname><given-names>CJ</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Filtering of neural signals by focused attention in the monkey prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>671</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.1038/nn874</pub-id><pub-id pub-id-type="pmid">12068302</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faria-Pereira</surname><given-names>A</given-names></name><name><surname>Morais</surname><given-names>VA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Synapses: the brain’s energy-demanding sites</article-title><source>International Journal of Molecular Sciences</source><volume>23</volume><elocation-id>3627</elocation-id><pub-id pub-id-type="doi">10.3390/ijms23073627</pub-id><pub-id pub-id-type="pmid">35408993</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesch</surname><given-names>T</given-names></name><name><surname>Juechems</surname><given-names>K</given-names></name><name><surname>Dumbalska</surname><given-names>T</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Orthogonal representations for robust context-dependent task performance in brains and neural networks</article-title><source>Neuron</source><volume>110</volume><fpage>1258</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.01.005</pub-id><pub-id pub-id-type="pmid">35085492</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Categorical representation of visual stimuli in the primate prefrontal cortex</article-title><source>Science</source><volume>291</volume><fpage>312</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1126/science.291.5502.312</pub-id><pub-id pub-id-type="pmid">11209083</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Why neurons mix: high dimensionality for higher cognition</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>66</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.01.010</pub-id><pub-id pub-id-type="pmid">26851755</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Bisley</surname><given-names>JW</given-names></name><name><surname>Roitman</surname><given-names>JD</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>One-dimensional dynamics of attention and decision making in LIP</article-title><source>Neuron</source><volume>58</volume><fpage>15</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.01.038</pub-id><pub-id pub-id-type="pmid">18400159</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>JJ</given-names></name><name><surname>Jolivet</surname><given-names>R</given-names></name><name><surname>Attwell</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Synaptic energy use and supply</article-title><source>Neuron</source><volume>75</volume><fpage>762</fpage><lpage>777</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.019</pub-id><pub-id pub-id-type="pmid">22958818</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasenstaub</surname><given-names>A</given-names></name><name><surname>Otte</surname><given-names>S</given-names></name><name><surname>Callaway</surname><given-names>E</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Metabolic cost as a unifying principle governing neuronal biophysics</article-title><source>PNAS</source><volume>107</volume><fpage>12329</fpage><lpage>12334</lpage><pub-id pub-id-type="doi">10.1073/pnas.0914886107</pub-id><pub-id pub-id-type="pmid">20616090</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heald</surname><given-names>JB</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Contextual inference underlies the learning of sensorimotor repertoires</article-title><source>Nature</source><volume>600</volume><fpage>489</fpage><lpage>493</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04129-3</pub-id><pub-id pub-id-type="pmid">34819674</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>KT</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Mattar</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A Recurrent Network Model of Planning Explains Hippocampal Replay and Human Behavior</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.16.523429</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kandel</surname><given-names>ER</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Principles of Neural Science</source><publisher-name>McGraw-hill</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>TC</given-names></name><name><surname>Sadabadi</surname><given-names>MS</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Optimal anticipatory control as A theory of motor preparation: A thalamo-cortical circuit model</article-title><source>Neuron</source><volume>109</volume><fpage>1567</fpage><lpage>1581</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.009</pub-id><pub-id pub-id-type="pmid">33789082</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>JR</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Energy as a constraint on the coding and processing of sensory information</article-title><source>Current Opinion in Neurobiology</source><volume>11</volume><fpage>475</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(00)00237-3</pub-id><pub-id pub-id-type="pmid">11502395</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsay</surname><given-names>GW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Convolutional neural networks as a model of the visual system: past, present, and future</article-title><source>Journal of Cognitive Neuroscience</source><volume>33</volume><fpage>2017</fpage><lpage>2031</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01544</pub-id><pub-id pub-id-type="pmid">32027584</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Löwe</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Regularised Neural Networks Mimic Human Insight</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2302.11351">https://arxiv.org/abs/2302.11351</ext-link></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>X</given-names></name><name><surname>Mok</surname><given-names>RM</given-names></name><name><surname>Roads</surname><given-names>BD</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A Controller-Peripheral Architecture and Costly Energy Principle for Learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.16.524194</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschläger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Real-time computing without stable states: A new framework for neural computation based on perturbations</article-title><source>Neural Computation</source><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>ML</given-names></name><name><surname>Preston</surname><given-names>AR</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Ventromedial prefrontal cortex compression during concept learning</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>46</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13930-8</pub-id><pub-id pub-id-type="pmid">31911628</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masse</surname><given-names>NY</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Circuit mechanisms for the maintenance and manipulation of information in working memory</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1159</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0414-3</pub-id><pub-id pub-id-type="pmid">31182866</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Linking connectivity, dynamics, and computations in low-rank recurrent neural networks</article-title><source>Neuron</source><volume>99</volume><fpage>609</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.003</pub-id><pub-id pub-id-type="pmid">30057201</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic population coding of category information in inferior temporal and prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>1407</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1152/jn.90248.2008</pub-id><pub-id pub-id-type="pmid">18562555</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrative theory of prefrontal cortex function</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id><pub-id pub-id-type="pmid">11283309</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monsell</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Task switching</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>134</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(03)00028-7</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Musslick</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multitasking Capability Versus Learning Efficiency in Neural Network Architectures</article-title><conf-name>CogSci 2017 - Proceedings of the 39th Annual Meeting of the Cognitive Science Society</conf-name><fpage>829</fpage><lpage>834</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orhan</surname><given-names>AE</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A diverse range of factors affect the nature of neural representations underlying short-term memory</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>275</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0314-y</pub-id><pub-id pub-id-type="pmid">30664767</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname><given-names>A</given-names></name><name><surname>Herikstad</surname><given-names>R</given-names></name><name><surname>Bong</surname><given-names>JH</given-names></name><name><surname>Medina</surname><given-names>FS</given-names></name><name><surname>Libedinsky</surname><given-names>C</given-names></name><name><surname>Yen</surname><given-names>S-C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mixed selectivity morphs population codes in prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1770</fpage><lpage>1779</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0003-2</pub-id><pub-id pub-id-type="pmid">29184197</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rainer</surname><given-names>G</given-names></name><name><surname>Asaad</surname><given-names>WF</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Selective representation of relevant information by neurons in the primate prefrontal cortex</article-title><source>Nature</source><volume>393</volume><fpage>577</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1038/31235</pub-id><pub-id pub-id-type="pmid">9634233</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rainer</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Timecourse of object-related neural activity in the primate prefrontal cortex during a short-term memory task</article-title><source>The European Journal of Neuroscience</source><volume>15</volume><fpage>1244</fpage><lpage>1254</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2002.01958.x</pub-id><pub-id pub-id-type="pmid">11982635</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinert</surname><given-names>S</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Goltstein</surname><given-names>PM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Mouse prefrontal cortex represents learned rules for categorization</article-title><source>Nature</source><volume>593</volume><fpage>411</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03452-z</pub-id><pub-id pub-id-type="pmid">33883745</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>de Berker</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Therien</surname><given-names>D</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep learning framework for neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1761</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id><pub-id pub-id-type="pmid">31659335</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schaeffer</surname><given-names>R</given-names></name><name><surname>Khona</surname><given-names>M</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>No free lunch from deep learning in neuroscience: a case study through models of the entorhinal-hippocampal circuit</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.08.07.503109</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schimel</surname><given-names>M</given-names></name><name><surname>Kao</surname><given-names>TC</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>When and Why Does Motor Preparation Arise in Recurrent Neural Network Models of Motor Control?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.04.03.535429</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schuessler</surname><given-names>F</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>F</given-names></name><name><surname>Dubreuil</surname><given-names>A</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The interplay between randomness and structure during learning in RNNs</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>13352</fpage><lpage>13362</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Huang</surname><given-names>A</given-names></name><name><surname>Maddison</surname><given-names>CJ</given-names></name><name><surname>Guez</surname><given-names>A</given-names></name><name><surname>Sifre</surname><given-names>L</given-names></name><name><surname>van den Driessche</surname><given-names>G</given-names></name><name><surname>Schrittwieser</surname><given-names>J</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Panneershelvam</surname><given-names>V</given-names></name><name><surname>Lanctot</surname><given-names>M</given-names></name><name><surname>Dieleman</surname><given-names>S</given-names></name><name><surname>Grewe</surname><given-names>D</given-names></name><name><surname>Nham</surname><given-names>J</given-names></name><name><surname>Kalchbrenner</surname><given-names>N</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Leach</surname><given-names>M</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Graepel</surname><given-names>T</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mastering the game of Go with deep neural networks and tree search</article-title><source>Nature</source><volume>529</volume><fpage>484</fpage><lpage>489</lpage><pub-id pub-id-type="doi">10.1038/nature16961</pub-id><pub-id pub-id-type="pmid">26819042</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Meirhaeghe</surname><given-names>N</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian computation through cortical latent dynamics</article-title><source>Neuron</source><volume>103</volume><fpage>934</fpage><lpage>947</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.012</pub-id><pub-id pub-id-type="pmid">31320220</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Training excitatory-inhibitory recurrent neural networks for cognitive tasks: a simple and flexible framework</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004792</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004792</pub-id><pub-id pub-id-type="pmid">26928718</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Kusunoki</surname><given-names>M</given-names></name><name><surname>Sigala</surname><given-names>N</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Stroud</surname><given-names>JP</given-names></name><name><surname>Watanabe</surname><given-names>K</given-names></name><name><surname>Suzuki</surname><given-names>T</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Optimal information loading into working memory in prefrontal cortex explains dynamic coding</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.11.16.468360</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Stroud</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Optimizing_RNNs_XOR_task</data-title><version designator="swh:1:rev:f24ffb82d6bb813a0a059339dd538adea54c9c8d">swh:1:rev:f24ffb82d6bb813a0a059339dd538adea54c9c8d</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:72f5ef312797fc3afefb9f0bc5fc60f6e7697adc;origin=https://github.com/jakepstroud/optimizing_RNNs_XOR_task;visit=swh:1:snp:27503d476c06c68d1dcd4b092817a04a0bcc9399;anchor=swh:1:rev:f24ffb82d6bb813a0a059339dd538adea54c9c8d">https://archive.softwareheritage.org/swh:1:dir:72f5ef312797fc3afefb9f0bc5fc60f6e7697adc;origin=https://github.com/jakepstroud/optimizing_RNNs_XOR_task;visit=swh:1:snp:27503d476c06c68d1dcd4b092817a04a0bcc9399;anchor=swh:1:rev:f24ffb82d6bb813a0a059339dd538adea54c9c8d</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds A naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomko</surname><given-names>GJ</given-names></name><name><surname>Crapper</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Neuronal variability: non-stationary responses to identical visual stimuli</article-title><source>Brain Research</source><volume>79</volume><fpage>405</fpage><lpage>418</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(74)90438-7</pub-id><pub-id pub-id-type="pmid">4422918</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible timing by temporal scaling of cortical responses</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>102</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0028-6</pub-id><pub-id pub-id-type="pmid">29203897</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Dorrell</surname><given-names>W</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Disentangling with Biological Constraints: A Theory of Functional Cell Types</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2210.01768">https://arxiv.org/abs/2210.01768</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wójcik</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Learning Shapes Neural Dimensionality in the Prefrontal Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.04.24.538054</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Joglekar</surname><given-names>MR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>297</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0310-2</pub-id><pub-id pub-id-type="pmid">30643294</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Pan</surname><given-names>X</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Energy expenditure computation of a single bursting neuron</article-title><source>Cognitive Neurodynamics</source><volume>13</volume><fpage>75</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1007/s11571-018-9503-3</pub-id><pub-id pub-id-type="pmid">30728872</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><p>In this supplementary appendix, we derive the optimal linear decoder for a linear network that receives both relevant and irrelevant inputs (Problem definition and The optimal linear decoder). We derive how both the performance of the optimal decoder (The performance of the optimal linear decoder) and the metabolic cost of the network (Metabolic cost) depend on noise and the strength of relevant and irrelevant inputs. We also derive how the optimal setting of the relevant and irrelevant inputs, when jointly optimizing for both performance and a metabolic cost, depend on noise and the strength of metabolic cost (Qualitative predictions about optimal parameters). Finally, we also study how noise and the strength of metabolic cost affect the curvature of the loss function around the optimum (Curvature of the loss function around the optimum).</p><sec sec-type="appendix" id="s8"><title>Mathematical analysis of relevant and irrelevant stimulus coding in a linear network</title><p>Even though in the main text we simulate and numerically analyze neural networks with nonlinear temporal dynamics that solve an XOR task, for analytical tractability, our mathematical analyses below are for a model in which responses to different inputs combine linearly. Our analysis is agnostic as to whether responses come about by simple, instantaneous feedforward or temporally extended recurrent interactions, and is only concerned with the phenomenological mapping between stimuli and the resulting response distributions. As such, we cannot distinguish between dynamically and statically irrelevant stimuli, and so we only include a single relevant and single (statically) irrelevant stimulus in our analysis. Nevertheless, as we show in the main text (<xref ref-type="fig" rid="fig4">Figure 4</xref>), some of the key insights from our analysis of such simplified systems generalize to the original setting.</p><sec sec-type="appendix" id="s8-1"><title>Problem definition</title><p>We consider the responses, <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, of a neural population of <inline-formula><mml:math id="inf166"><mml:mi>N</mml:mi></mml:math></inline-formula> neurons to a stimulus that has a ‘relevant’ feature, <italic>c</italic>, which we assume to be binary with values <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> (occurring with uniform frequency), and to a (scalar) irrelevant feature, <inline-formula><mml:math id="inf167"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> that is continuous (for convenience). We further assume that the responses are determined by a linear interaction of the relevant and irrelevant features and also subject to unspecific neural noise:<disp-formula id="equ11"><label>(S1)</label><mml:math id="m11"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">η</mml:mi></mml:mrow></mml:math></disp-formula></p><p>where <italic>µ</italic> is the grand average response, averaging across both the relevant and the irrelevant feature, <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the relevant tuning of the population, <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the irrelevant tuning, <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">ϵ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is (without loss of generality) zero mean and unit variance variability in the irrelevant feature, and <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">η</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is other neural noise.</p><p>In this note, we study how this neural population can optimize the decodability of <italic>c</italic> while also balancing metabolic cost (defined below). Specifically, we will regard <italic><bold>µ</bold></italic> and <inline-formula><mml:math id="inf172"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> as givens (constraints; one could also consider <italic><bold>µ</bold></italic> as an optimizable parameter, and its optimal value would simply be <bold>0</bold>) and ask how the population should ‘choose’ <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for this trade-off (or some summary statistics thereof, see below).</p></sec><sec sec-type="appendix" id="s8-2"><title>The optimal linear decoder</title><p>We first study in general how information can be decoded from the population. The statistically optimal decoder, decoding <italic>c</italic> from <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, is the maximum likelihood decoder (note that we have assumed <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> to occur with equal probabilities, see above). To make the derivations tractable, from here on we assume that <inline-formula><mml:math id="inf176"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> is specifically Gaussian distributed, i.e., <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, unless otherwise noted. In this case, the distribution of responses conditioned on the relevant stimulus feature becomes (equivariate) Gaussian, with some effective noise covariance <inline-formula><mml:math id="inf178"><mml:mi mathvariant="bold-italic">Σ</mml:mi></mml:math></inline-formula> (to be determined later, <xref ref-type="disp-formula" rid="equ23">Equation S13</xref>):<disp-formula id="equ12"><label>(S2)</label><mml:math id="m12"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ13"><label>(S3)</label><mml:math id="m13"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Thus, the log odds ratio is<disp-formula id="equ14"><label>(S4)</label><mml:math id="m14"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="italic">l</mml:mi><mml:mi mathvariant="italic">n</mml:mi></mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold">r</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>and the optimal decoder responds ‘1’ if<disp-formula id="equ15"><label>(S5)</label><mml:math id="m15"><mml:mrow><mml:mi>z</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>Thus, in this case, the optimal decoder is linear in <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, or conversely, a linear decoder (with the correct coefficients) is optimal.</p></sec><sec sec-type="appendix" id="s8-3"><title>The performance of the optimal linear decoder</title><p>We now turn to the question of how the parameters of the neural population affect its decodability, i.e., the performance of the optimal linear decoder. (We still assume that <inline-formula><mml:math id="inf180"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>, and thus <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, is Gaussian distributed.)</p><p>As we saw, the optimal decoder can be described by a simple thresholding of the log odds (<xref ref-type="disp-formula" rid="equ15">Equation S5</xref>). The log odds, <italic>z</italic>, itself is also Gaussian distributed conditioned on the relevant feature, because <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is Gaussian distributed (<xref ref-type="disp-formula" rid="equ12 equ13">Equations S2 and S3)</xref> and <italic>z</italic> is a linear function of it (<xref ref-type="disp-formula" rid="equ14">Equation S4</xref>):<disp-formula id="equ16"><label>(S6)</label><mml:math id="m16"><mml:mrow><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and, due to symmetry,<disp-formula id="equ17"><label>(S7)</label><mml:math id="m17"><mml:mrow><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with<disp-formula id="equ18"><label>(S8)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ19"><label>(S9)</label><mml:math id="m19"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>The probability of correct decoding for <italic>c</italic><sub>1</sub> (and by symmetry, also for <italic>c</italic><sub>2</sub>, and thus also after averaging over <italic>c</italic>) is given by<disp-formula id="equ20"><label>(S10)</label><mml:math id="m20"><mml:mrow><mml:mi mathvariant="normal">Π</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>respond</mml:mtext><mml:mspace width="thinmathspace"/><mml:mo>“</mml:mo><mml:mn>1</mml:mn><mml:mo>”</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>respond</mml:mtext><mml:mspace width="thinmathspace"/><mml:mo>“</mml:mo><mml:mn>2</mml:mn><mml:mo>”</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ21"><label>(S11)</label><mml:math id="m21"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mfrac><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:mfrac></mml:msqrt><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore, the performance of the optimal linear decoder scales monotonically with μ<sub>z</sub>. To gain further insight into what this means in our particular setting, let us now express the effective noise covariance matrix <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with the parameters of the problem definition:<disp-formula id="equ22"><label>(S12)</label><mml:math id="m22"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">C</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">C</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ23"><label>(S13)</label><mml:math id="m23"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold">I</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>and its inverse (expressed using the Sherman–Morrison formula):<disp-formula id="equ24"><label>(S14)</label><mml:math id="m24"><mml:mrow><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">I</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Substituting <xref ref-type="disp-formula" rid="equ24">Equation S14</xref> into the formula for <italic>μ<sub>z</sub></italic> (<xref ref-type="disp-formula" rid="equ18">Equation S8</xref>), we obtain:<disp-formula id="equ25"><label>(S15)</label><mml:math id="m25"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ26"><label>(S16)</label><mml:math id="m26"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Thus, <italic>μ<sub>z</sub></italic> can be simply expressed as<disp-formula id="equ27"><label>(S17)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ28"><label>(S18)</label><mml:math id="m28"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mtext>is the magnitude of tuning to the relevant feature</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="equ29"><label>(S19)</label><mml:math id="m29"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mtext>is the magnitude of tuning to the irrelevant feature</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="equ30"><label>(S20)</label><mml:math id="m30"><mml:mrow><mml:mtext>and</mml:mtext><mml:mtext> </mml:mtext><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:mtext> </mml:mtext><mml:mtext>is the overlap of irrelevant with relevant tuning</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>which reveals that the effects of noise, relevant tuning, and irrelevant tuning factorize (corresponding to the first, second, and third terms, respectively), and that – intuitively – μ<sub>z</sub> and thus performance increases with <italic>γ</italic> and decreases with <italic>α</italic>, <inline-formula><mml:math id="inf184"><mml:mi>β</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf185"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> (where we always consider the latter a constraint and thus fixed, see the problem definition).</p><p>Note that in the small noise limit, <inline-formula><mml:math id="inf186"><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>≪</mml:mo><mml:msup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>:<disp-formula id="equ31"><label>(S21)</label><mml:math id="m31"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>​</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>​</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>showing that performance in this case can only be increased by decreasing <inline-formula><mml:math id="inf187"><mml:mi>β</mml:mi></mml:math></inline-formula> (or, trivially, by increasing <italic>γ</italic>), but not by decreasing <italic>α</italic>. In contrast, when the small noise limit does not hold, the original <xref ref-type="disp-formula" rid="equ27">Equation S17</xref> applies, and so performance can be increased by decreasing either <italic>α</italic> or <inline-formula><mml:math id="inf188"><mml:mi>β</mml:mi></mml:math></inline-formula> (or, again, by increasing <italic>γ</italic>).</p></sec><sec sec-type="appendix" id="s8-4"><title>Metabolic cost</title><p>Following previous work (<xref ref-type="bibr" rid="bib68">Wójcik, 2023</xref>; <xref ref-type="bibr" rid="bib54">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib69">Yang et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Wang et al., 2018</xref>; <xref ref-type="bibr" rid="bib58">Silver et al., 2016</xref>; <xref ref-type="bibr" rid="bib29">Jensen et al., 2023</xref>), we define the metabolic cost to be the average sum of squared neural responses (where the averaging is over realizations of the relevant and irrelevant features as well as neural noise):<disp-formula id="equ32"><label>(S22)</label><mml:math id="m32"><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which, by making the averaging over relevant features explicit, can be rewritten as<disp-formula id="equ33"><label>(S23)</label><mml:math id="m33"><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where the metabolic cost for <italic>c</italic><sub>1</sub> is<disp-formula id="equ34"><label>(S24)</label><mml:math id="m34"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">C</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ35"><label>(S25)</label><mml:math id="m35"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ36"><label>(S26)</label><mml:math id="m36"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>and, again by symmetry,<disp-formula id="equ37"><label>(S27)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold">r</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>and so, by substituting <xref ref-type="disp-formula" rid="equ36 equ37">Equations S26 and S27</xref> into <xref ref-type="disp-formula" rid="equ33">Equation S23</xref>, we get:<disp-formula id="equ38"><label>(S28)</label><mml:math id="m38"><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>​</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>This result is also interesting, because it shows that the metabolic cost factorizes into four additive terms, each of which scales monotonically with a separate parameter: the average magnitude of responses, neural noise, the magnitude of relevant tuning, and the magnitude of irrelevant coding. It is also interesting to note what the metabolic cost does <italic>not</italic> depend on: the overlap between relevant and irrelevant tuning, <inline-formula><mml:math id="inf189"><mml:mi>β</mml:mi></mml:math></inline-formula>.</p><p>The first two terms in <xref ref-type="disp-formula" rid="equ38">Equation S28</xref> are assumed to be fixed (see problem definition, Problem definition), so we will not consider them further. The last two terms in <xref ref-type="disp-formula" rid="equ38">Equation S28</xref> increase with <italic>γ</italic> and <italic>α</italic>, respectively. As we want the metabolic cost to be small, this prefers small <italic>γ</italic> and <italic>α</italic>.</p><p>Note that, unlike for deriving the optimal linear decoder (The optimal linear decoder) and its performance (The performance of the optimal linear decoder), for which we needed to assume that <inline-formula><mml:math id="inf190"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> is normally distributed, no such assumption needed to be made for computing the metabolic cost (<xref ref-type="disp-formula" rid="equ38">Equation S28</xref>). Thus, for the metabolic cost, the same result is obtained for example when the irrelevant feature is binary, such that <inline-formula><mml:math id="inf191"><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> with equal probability (without loss of generality; note that, in this case, it is still true that <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is all that we assumed in the derivation above (see <xref ref-type="disp-formula" rid="equ23">Equation S13</xref>)), i.e., when variability in the irrelevant feature simply changes the responses by <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>±</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s8-5"><title>Qualitative predictions about optimal parameters</title><p>In general, the optimal setting of optimizable parameters, <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (Problem definition), depends on the overall objective function, which will usually be a sum of performance (which we want to be high) and (negative) metabolic cost (which we want to be low), with some suitable Lagrange multiplier, <italic>λ</italic>, controlling the trade-off between the two terms:<disp-formula id="equ39"><label>(S29)</label><mml:math id="m39"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mi mathvariant="normal">Π</mml:mi><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>Eqs. S11 and S17</mml:mtext></mml:mrow></mml:munder><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:munder><mml:mrow><mml:munder><mml:msup><mml:mi>ω</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>Eq. S28</mml:mtext></mml:mrow></mml:munder></mml:mrow></mml:math></disp-formula></p><p>We note that the objective function also depends on the constraint parameters, <italic><bold>µ</bold></italic> and <italic>σ</italic> (Problem definition). The effect of <italic>µ</italic> is trivial: it only affects the metabolic cost, not performance, and it does so as a simple additive term (<xref ref-type="disp-formula" rid="equ38">Equation S28</xref>), so it does not affect the optimal values of the other parameters. (This remains true even if <italic><bold>µ</bold></italic> is optimizable, in which case it is also obvious from <xref ref-type="disp-formula" rid="equ38">Equation S28</xref> that its optimal value is <bold>0</bold>.) The effect of the other constraint, <italic>σ,</italic> is more nuanced, so we will separately consider two different regimes for it: small noise (<inline-formula><mml:math id="inf197"><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>σ</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>crit</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>) and large noise (<inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>crit</mml:mtext></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) with the constant <inline-formula><mml:math id="inf199"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>crit</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> defined later.</p><p>We also note that both performance (<xref ref-type="disp-formula" rid="equ27">Equation S17</xref>) and metabolic cost (<xref ref-type="disp-formula" rid="equ38">Equation S28</xref>) only depend on the optimizable parameters, <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, through their summary statistics, <italic>γ</italic>, <italic>α</italic>, and (for performance) <inline-formula><mml:math id="inf202"><mml:mi>β</mml:mi></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ28 equ29 equ30">Equations S18–S20</xref>). In general, the dependence on the latter two parameters is straightforward: performance decreases in both <italic>α</italic> and <inline-formula><mml:math id="inf203"><mml:mi>β</mml:mi></mml:math></inline-formula> (so that the last term in <xref ref-type="disp-formula" rid="equ27">Equation S17</xref> achieves its maximal possible value 1 when either parameter is 0), while the metabolic cost increases with <italic>α</italic>. This means that their optimal values will be at 0. However, we recall that in the small noise limit, only decreasing <inline-formula><mml:math id="inf204"><mml:mi>β</mml:mi></mml:math></inline-formula> can improve performance (<xref ref-type="disp-formula" rid="equ31">Equation S21</xref>), while otherwise, decreasing <italic>α</italic> can also make a contribution to it. At the same time, the metabolic cost only depends on <italic>α</italic> but not on <inline-formula><mml:math id="inf205"><mml:mi>β</mml:mi></mml:math></inline-formula>. Thus to summarize, in the small noise limit, there is ‘pressure’ on both <inline-formula><mml:math id="inf206"><mml:mi>β</mml:mi></mml:math></inline-formula> (from performance) and <italic>α</italic> (from the metabolic cost) to be small. In other cases, the pressure on <italic>α</italic> comes from both performance and metabolic cost, while <inline-formula><mml:math id="inf207"><mml:mi>β</mml:mi></mml:math></inline-formula> only matters for performance, so we expect <italic>α</italic> to be more aggressively minimized by optimization, and perhaps <inline-formula><mml:math id="inf208"><mml:mi>β</mml:mi></mml:math></inline-formula> not so much (which we find to be the case for our optimized recurrent neural networks; <xref ref-type="fig" rid="fig4">Figure 4f and g</xref>; see also Curvature of the loss function around the optimum).</p><p>The overall effect of <italic>γ</italic> is slightly less trivial. <xref ref-type="disp-formula" rid="equ38">Equation S28</xref> shows that the metabolic cost depends on it quadratically, i.e., it should be small. However, as we saw earlier (<xref ref-type="disp-formula" rid="equ27">Equation S17</xref>), decoding performance monotonically grows with it. Nevertheless, this dependence of decoding performance on <italic>γ</italic> is nonlinear (through the standard normal c.d.f., <xref ref-type="disp-formula" rid="equ21">Equation S11</xref>), such that it is effectively linear in <italic>γ</italic> for small values, but saturates for large values of <italic>γ</italic>. (This is because <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is linear in the <italic>x</italic>→0 limit, and the argument of <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for computing the performance is linear in the square root of <italic>μ<sub>z</sub></italic> (<xref ref-type="disp-formula" rid="equ21">Equation S11</xref>), which in turn is quadratic in <italic>γ</italic> (<xref ref-type="disp-formula" rid="equ27">Equation S17</xref>), so the argument of <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is linear in <italic>γ.</italic>) This means that for small values of <italic>γ</italic> the linear performance will dominate over the quadratic metabolic cost, but eventually, for large values, the quadratic metabolic term is guaranteed to dominate over the saturating performance, thus effectively limiting the magnitude of relevant tuning to some finite value. Note, however, that this argument does not yet reveal what happens to <italic>γ</italic> depending on the noise regime, and where the transition between these two regimes happens.</p><p>Fortunately, it is possible to analytically derive <inline-formula><mml:math id="inf212"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the optimal value of <italic>γ</italic>, as a function of <italic>σ</italic>. For this, we assume that <italic>α</italic> and <inline-formula><mml:math id="inf213"><mml:mi>β</mml:mi></mml:math></inline-formula> already take their optimal values (without loss of generality, as we are interested in jointly optimizing all relevant parameters), such that the last term in <xref ref-type="disp-formula" rid="equ27">Equation S17</xref> is simply 1 (see above). In this case, the terms in the overall objective function (<xref ref-type="disp-formula" rid="equ39">Equation S29</xref>) that depend on the two remaining parameters of interest (<italic>γ</italic> and <italic>σ</italic>) are simply:<disp-formula id="equ40"><label>(S30)</label><mml:math id="m40"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>​</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>4</mml:mn></mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mi>N</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>…</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The optimal <italic>γ</italic> can be simply defined implicitly as a function of <italic>σ</italic> as the solution to the following equation:<disp-formula id="equ41"><label>(S31)</label><mml:math id="m41"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We now substitute (the partial derivative of) <xref ref-type="disp-formula" rid="equ40">Equation S30</xref> into <xref ref-type="disp-formula" rid="equ41">Equation S31</xref> to obtain:<disp-formula id="equ42"><label>(S32)</label><mml:math id="m42"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Note that this yields a solution for any <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> since the line <inline-formula><mml:math id="inf215"><mml:mfrac><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> intersects the pdf of the Normal distribution for some <inline-formula><mml:math id="inf216"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. Re-arranging <xref ref-type="disp-formula" rid="equ42">Equation S32</xref> gives us:<disp-formula id="equ43"><label>(S33)</label><mml:math id="m43"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>γ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mn>8</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:msqrt><mml:mi>σ</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>This equation has a solution in terms of the Lambert <italic>W</italic> function (defined by its inverse as <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>)<disp-formula id="equ44"><label>(S34)</label><mml:math id="m44"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>σ</mml:mi><mml:msqrt><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>8</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>where we only take the positive solution since <italic>γ</italic> can only be positive.</p><p>We can find the maximum of this function by taking the derivative of <xref ref-type="disp-formula" rid="equ44">Equation S34</xref> and setting it equal to 0. This gives:<disp-formula id="equ45"><label>(S35)</label><mml:math id="m45"><mml:mi>W</mml:mi><mml:mi>​</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mi>​</mml:mi><mml:mi>π</mml:mi><mml:mi>​</mml:mi><mml:msup><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>​</mml:mi><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>crit</mml:mtext></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></disp-formula></p><p>Using the fact that <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, we obtain:<disp-formula id="equ46"><label>(S36)</label><mml:math id="m46"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>crit</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>8</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Substituting this into <xref ref-type="disp-formula" rid="equ44">Equation S34</xref> gives:<disp-formula id="equ47"><label>(S37)</label><mml:math id="m47"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mtext>crit</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>crit</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>These results are shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, together with a numerical confirmation (see also <xref ref-type="fig" rid="fig4">Figure 4d</xref>).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Plot of <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of <italic>σ</italic> obtained by numerically optimizing <xref ref-type="disp-formula" rid="equ40">Equation S30</xref> (black), or using the analytical expression in <xref ref-type="disp-formula" rid="equ44">Equation S34</xref> (red).</title><p>Blue dot shows (analytically computed) critical values where <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> has a maximum (<xref ref-type="disp-formula" rid="equ46 equ47">Equations S36 and S37</xref>). We used <italic>λ</italic>=1 for these results.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-app1-fig1-v1.tif"/></fig><p>The qualitative dependence of <inline-formula><mml:math id="inf221"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> on <italic>σ</italic> is straightforward to interpret and intuitive. In the <italic>small</italic> noise regime, <inline-formula><mml:math id="inf222"><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>σ</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>crit</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, <italic>γ</italic> needs to grow with <italic>σ</italic> so that the separation of the response distributions conditioned on <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> remains large enough to guarantee high performance. This growth starts at zero because for zero noise any infinitesimal separation between the mean responses for <italic>c</italic> = <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> makes for perfect performance. Thus, <italic>γ</italic> remains small, and so – as we saw above – the performance term in the objective function dominates over the metabolic cost term. This means that the value of <italic>γ</italic> that optimizes the full objective function can be understood from just this performance-based perspective. However, in the <italic>large</italic> noise regime, <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>crit</mml:mtext></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <italic>γ</italic> would need to be so large to guarantee high performance that it would reach a regime in which – as we saw above – the metabolic cost dominates. Thus the optimal <italic>γ</italic> is increasingly influenced by the metabolic cost, and thus decreases with <italic>σ</italic>.</p><p>Finally, we derive classification performance with optimized parameters as a function of <italic>σ</italic> by substituting <xref ref-type="disp-formula" rid="equ44">Equation S34</xref> into <xref ref-type="disp-formula" rid="equ21 equ27">Equations S11 and S17</xref> and obtain:<disp-formula id="equ48"><label>(S38)</label><mml:math id="m48"><mml:mrow><mml:mi mathvariant="normal">Π</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>8</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:msqrt><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which shows the intuitive result that performance monotonically decreases with <italic>σ</italic> and drops to chance level for <inline-formula><mml:math id="inf224"><mml:mi>σ</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:math></inline-formula> (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>, red). This is because the argument of <italic>W</italic> monotonically decreases with <italic>σ</italic> from infinity to zero, while both <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf226"><mml:msqrt><mml:mi>x</mml:mi></mml:msqrt></mml:math></inline-formula> monotonically increase (without bounds) with <italic>x</italic> from zero at <italic>x</italic>=0, and finally <inline-formula><mml:math id="inf227"><mml:mi>Ψ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> also monotonically increases with <italic>x</italic> but from <inline-formula><mml:math id="inf228"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> at <italic>x</italic>=0 and to an asymptotic bound of 1.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Plot of <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>correct</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of <italic>σ</italic> when using <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> that is optimized as in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> (<xref ref-type="disp-formula" rid="equ48">Equation S38</xref>).</title><p>We used <italic>λ</italic>=1 for these results. All other parameters were optimized in all cases.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-94961-app1-fig2-v1.tif"/></fig></sec><sec sec-type="appendix" id="s8-6"><title>Curvature of the loss function around the optimum</title><p>The curvature of the loss landscape around the optimum is important for determining how tightly constrained the parameters will be in the presence of noisy gradient updates due to the finite batch sizes used for stochastic gradient descent. Before we proceed, we first repeat here the main equations regarding the loss <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ49"><label>(S39)</label><mml:math id="m49"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Π</mml:mi><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ50"><label>(S40)</label><mml:math id="m50"><mml:mi>Π</mml:mi><mml:mo>=</mml:mo><mml:mi>Ψ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:msqrt></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="equ51"><label>(S41)</label><mml:math id="m51"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ52"><label>(S42)</label><mml:math id="m52"><mml:mrow><mml:msup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><sec sec-type="appendix" id="s8-6-1"><title>Curvature of the loss function with respect to <italic>α</italic></title><p>In this section, we are interested in the first non-zero term of the Taylor expansion of the loss function <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with respect to <italic>α</italic> – the magnitude of the irrelevant input – around the optimum at <inline-formula><mml:math id="inf233"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. It turns out that the first non-zero term results from the second derivative of the metabolic cost term in the loss function since the other terms always contain at least an <italic>α</italic> or <inline-formula><mml:math id="inf234"><mml:mi>β</mml:mi></mml:math></inline-formula> (which are 0 at the optimum). We therefore find that:<disp-formula id="equ53"><label>(S43)</label><mml:math id="m53"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ54"><label>(S44)</label><mml:math id="m54"><mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mfrac><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ55"><label>(S45)</label><mml:math id="m55"><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mi>λ</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>From this result, we see that the magnitude of the curvature of the loss decreases as a function of <italic>λ</italic>. In other words, the loss landscape as a function of the irrelevant input <italic>α</italic> becomes steeper with increasing regularization. This is why we observed larger values of <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ28">Equation S18</xref>) in <xref ref-type="fig" rid="fig4">Figure 4f</xref> with decreasing <italic>λ</italic>.</p></sec><sec sec-type="appendix" id="s8-6-2"><title>Curvature of the loss function with respect to <italic>β</italic></title><p>We now turn our attention to the first non-zero term of the Taylor expansion of the loss function <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with respect to <inline-formula><mml:math id="inf237"><mml:mi>β</mml:mi></mml:math></inline-formula> – the overlap of irrelevant with relevant tuning – around the optimum at <inline-formula><mml:math id="inf238"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. It turns out that the first non-zero term results from a fourth derivative of the performance term Π with respect to both <italic>α</italic> and <inline-formula><mml:math id="inf239"><mml:mi>β</mml:mi></mml:math></inline-formula>:<disp-formula id="equ56"><label>(S46)</label><mml:math id="m56"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ57"><label>(S47)</label><mml:math id="m57"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mfrac><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mn>4</mml:mn></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ58"><label>(S48)</label><mml:math id="m58"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:math></disp-formula><disp-formula id="equ59"><label>(S49)</label><mml:math id="m59"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>To evaluate this term we note that:<disp-formula id="equ60"><label>(S50)</label><mml:math id="m60"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>​</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ61"><label>(S51)</label><mml:math id="m61"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore,<disp-formula id="equ62"><label>(S52)</label><mml:math id="m62"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ63"><label>(S53)</label><mml:math id="m63"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>β</mml:mi><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ64"><label>(S54)</label><mml:math id="m64"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Now, noting that <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, we obtain:<disp-formula id="equ65"><label>(S55)</label><mml:math id="m65"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ66"><label>(S56)</label><mml:math id="m66"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ67"><label>(S57)</label><mml:math id="m67"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ68"><label>(S58)</label><mml:math id="m68"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>4</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>8</mml:mn><mml:mi>α</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>12</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>16</mml:mn><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ69"><label>(S59)</label><mml:math id="m69"><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>4</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi mathvariant="normal">Π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ70"><label>(S60)</label><mml:math id="m70"><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>4</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:msqrt><mml:mfrac><mml:mn>2</mml:mn><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:msqrt><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mfrac><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mn>2</mml:mn></mml:mfrac></mml:msqrt><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ71"><label>(S61)</label><mml:math id="m71"><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow><mml:mi>γ</mml:mi></mml:mfrac><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ72"><label>(S62)</label><mml:math id="m72"><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Therefore, from <xref ref-type="disp-formula" rid="equ64">Equation S54</xref> we obtain:<disp-formula id="equ73"><label>(S63)</label><mml:math id="m73"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>γ</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We therefore find that the magnitude of this term becomes smaller with increasing noise. This is why we observed larger values of <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>rel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi mathvariant="bold">r</mml:mi><mml:mrow><mml:mtext>irrel</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ30">Equation S20</xref>) in <xref ref-type="fig" rid="fig4">Figure 4g</xref> with increasing <italic>σ</italic>.</p></sec></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94961.2.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Salinas</surname><given-names>Emilio</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Wake Forest University School of Medicine</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Incomplete</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This work provides a <bold>valuable</bold> analysis of the effect of two commonly used hyperparameters, noise amplitude and firing rate regularization, on the representations of relevant and irrelevant stimuli in trained recurrent neural networks (RNNs). The results suggest an interesting interpretation of prefrontal cortex (PFC) dynamics, based on comparisons to previously published data from the same lab, in terms of decreasing metabolic cost during learning. The evidence indicating that the mechanisms identified in the RNNs are the same ones operating in PFC was considered <bold>incomplete</bold>, but could potentially be bolstered by additional analyses and appropriate revisions.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94961.2.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study compares experimental data recorded from the PFC of monkeys to the activity of recurrent neural networks trained to perform the same `task' as the monkeys, namely, to predict the delivery of reward following the presentation of visual stimuli. The visual information varied along 3 dimensions, color, shape, and width. Shape was always relevant for reward prediction, width was always irrelevant, and color was irrelevant at the beginning of the trial but became relevant later on, once it could be assessed together with shape. The neural data showed systematic changes in the representations of these features and of the expected reward as the learning progressed, and the objective of this study was to try to understand what principles could underlie these changes. The simulations and theoretical calculations indicated that the changes in PFC activity (throughout learning and throughout a trial) can be understood as an attempt by the circuitry to use an efficient representational strategy, i.e., one that uses as few spikes as possible, given that the resulting representation should be accurate enough for task performance.</p><p>Strengths:</p><p>- The paper is concise and clearly written.</p><p>- The paper shows that, in a neural circuit, the information that is decodable and the information that is task-relevant may relate in very different ways. Decodable information may be very relevant or very irrelevant. This fact is critical for interpreting the results of pure decoding studies, which often assume an equivalence. This take-home message is not emphasized by the authors, but I think is quite important.</p><p>- The results provide insight as to how neural representations may be transformed as a task is learned, which often results in subtle changes in selectivity and overall activity levels whose impact or reason is not entirely clear just by looking at the data.</p><p>Weaknesses:</p><p>The match between the real PFC and the model networks is highly qualitative, and as noted by the authors, comparisons only make sense in terms of *changes* between early and late learning. The time scales, activity levels, and decoding accuracies involved are all different between the model and recording data. This is not to disregard what the authors have done, but simply to point out an important limitation.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94961.2.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The study investigates the representation of irrelevant stimuli in neural circuits using neural recordings from the primate prefrontal cortex during a passive object association task. They find a significant decrease in the linear decodability of irrelevant stimuli over the course of learning (in the time window in which the stimuli are irrelevant). They then compare these trends to RNNs trained with varying levels of noise and firing rate regularization and find agreement when these levels are at an intermediate value. In a complementary analysis, they found (in both RNNs and PFC) that the magnitude of relevant and irrelevant stimuli increased and decreased, respectively, during learning. These findings were interpreted in terms of a minimization of metabolic cost in the cortex.</p><p>To understand how stimuli can be dynamically suppressed at times when they are irrelevant, the authors constructed and analyzed a reduced two-neuron model of the task. They found a mechanism in which firing rate regularization increased the probability of negative weights in the input, pushing the neural activities below the threshold. A similar mechanism was observed in RNNs.</p><p>Strengths:</p><p>The article is well-written and the figures are easily understood. The analyses are well explained and motivated. The article provides a valuable analysis of the effect of two parameters on representations of irrelevant stimuli in trained RNNs.</p><p>Weaknesses:</p><p>(1) The mechanism for suppressing dynamically relevant stimuli appears to be incomplete and does not explain clearly enough how representations of 'color' which are suppressed through negative input weights become un-suppressed in the presence of the second variable 'shape'.</p><p>(2) Interpretation of results in terms of the effect of metabolic cost on cortical dynamics is not backed up by the presented data/analyses. The change in dynamics of 'color' representations in the prefrontal cortex only qualitatively matches RNN dynamics and may arise from other causes.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94961.2.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In order to study the factors and neural dynamics that lead to the suppression of irrelevant information in the brain, the authors trained artificial neural networks in the execution of a task that involved the discrimination of complex stimuli with three main features: color, shape, and width. Specific combinations of color and shape led to a reward, but the temporal structure made color dynamically irrelevant at the beginning of the trial, and then it became relevant once the shape was presented. On the other hand, the width of the stimulus was always irrelevant. Importantly, non-human primates were also trained to execute this task (in a previous study by the authors) and the activity from neural populations from the dorsolateral Prefrontal Cortex (dlPFC) was recorded, allowing to compare the coding of information by the artificial neural network model with what happens in biological neural populations.</p><p>The authors changed systematically the amount of noise present in the neural network model, as well as limiting the firing rate of the artificial neurons to simulate the limitations imposed by high metabolic costs in biological neurons. They found that models with medium and low noise, as well as medium and low metabolic cost, developed information encoding patterns that resembled the patterns observed throughout learning in the dlPFC, as follows: early in the learning process, color information was strongly represented during the whole trial, as well as shape and width, whereas the color/shape combination significance (XOR operation) was weakly encoded. Late in learning, color information was initially suppressed (while it was deemed irrelevant) and became more prominent during the shape presentation. Width information coding decreased, and the XOR operation result became more strongly encoded.</p><p>Subthreshold activity dynamics were studied by training artificial networks consisting of 2 neurons, with the aim of understanding how dynamically irrelevant information is suppressed and then encoded more strongly at a different time during the trial. Under medium noise and medium metabolic cost, color information is suppressed by the divergence of the activity away from the level that triggers spikes. The authors claim that this subthreshold dynamic explains the suppression of irrelevant information in biological neural networks.</p><p>Strengths:</p><p>The study leverages the power of computational models to simulate biological networks and do manipulations that are difficult (if not impossible) to perform in vivo. The analyses of the activity of the network model are neat and thorough and provide a clear demonstration of how noise and metabolic costs may affect the information coding in the brain. The mathematical analyses are rigorous and nicely documented.</p><p>Weaknesses:</p><p>The study does not leverage the fact that they have access to the activity of individual neurons both on a neural network model and in neural recordings. The model/brain comparison results are limited to the decodability of different pieces of information during the execution of the task at different stages of learning. It would have been useful if the authors had shown response profiles of individual neurons, both biological and artificial, to strengthen the claim that the activity patterns are similar. Perhaps showing that the firing rates vary in a similar way in the large models (like they do for the 2-neuron model) would have been informative. For instance, it is possible that suppression is not occurring in the dlPFC, but that the PFC receives input with this information already suppressed. If suppression indeed happens in the PFC, response profiles associated with this process may be observed.</p><p>There is no way to say that the 2-neuron models are in any way informative of what happens in brain neurons, or even larger artificial networks since the sources of sensory input, noise, and inhibition will differ between biological and artificial networks. And because the firing patterns are not shown for large networks, it is not clear if some non-coding artificial neurons will become broadly inhibitory but maintain a relatively high firing rate (to mention only one possibility).</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.94961.2.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Stroud</surname><given-names>Jake Patrick</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Wojcik</surname><given-names>Michal</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Jensen</surname><given-names>Kristopher Torp</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Kusunoki</surname><given-names>Makoto</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Kadohisa</surname><given-names>Mikiko</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Buckley</surname><given-names>Mark</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Duncan</surname><given-names>John</given-names></name><role specific-use="author">Author</role><aff><institution>Medical Research Council Cognition and Brain Sciences Unit</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Stokes</surname><given-names>Mark</given-names></name><role specific-use="author">Author</role><aff><institution>Oxford University</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Lengyel</surname><given-names>Mate</given-names></name><role specific-use="author">Author</role><aff><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p><bold>Reviewer 1:</bold></p><p>Many thanks for your positive review and clear overview of our paper. We also agree with your interpretation of our results that ‘the information that is decodable and the information that is task-relevant may relate in very different ways’ and we could have emphasised this point more in the paper.</p><p>With regards to the qualitative similarities between our models and our data, we agree that due to the fact that one can achieve any desired level of activity, decoding accuracy, performance, etc in a model, we focussed on <italic>changes</italic> over learning of key metrics that are commonly used in the field. Although this can appear qualitative at times because the raw values can differ between the data and our models, our main results are ultimately strongly quantitative (e.g., Fig. 3c,d, Fig. 4h,i, and Fig. 5f). We note that we could have fine tuned the models to have similar activity levels, decoding accuracies etc to our data, and on the face of it this may have made the results appear more convincing, but we felt that such trivial fine tuning does not change any of our key results in any fundamental way and is not the aim of computational modelling. The model one chooses to analyse will always be abstracted from biology in some way, by definition.</p><p><bold>Reviewer 2:</bold></p><p>Thank you very much for your kind comments and clear overview of our paper. We also hope that our paper ‘provides a valuable analysis of the effect of two parameters on representations of irrelevant stimuli in trained RNNs.’</p><p>With regards to our suggested mechanism of suppressing dynamically irrelevant stimuli, we are sorry that we did not provide a sufficient enough explanation of suppressing color representations when they are irrelevant. We hopefully provide a longer explanation here. Our mechanism of suppression of dynamically irrelevant stimuli does not suggest that it becomes un-suppressed later, only the behaviourally relevant variable should be decodable when it is needed (i.e., XOR). Although color decodability did increase slightly in the data and some of the models from the color period to the shape period, it was typically not significant and was therefore not a result that we emphasise in the paper (although this could be analysed further to see if additional mechanisms might explain it). We emphasise throughout that color decoding is typically similar between color and shape periods (either high or low) and either decreases or increases over time in both periods. We also focus on whether color decodability increases or decreases over learning during the color period when it is irrelevant (which we call ‘early color decoding’). Importantly, decoding of color or shape is not needed to perform the task, only decoding of XOR is needed to perform the task. For example, in our two-neuron networks, we observe perfect XOR decoding and only 75% decoding of color and shape, and decoding during the shape period is the same as the network at initialisation before any training. The mechanism we suggest of suppressing dynamically irrelevant stimuli does not predict that that stimulus should be un-suppressed later, only the behaviourally relevant variable should be decodable (i.e., XOR). Instead, what we try to explain is that color inputs can generate zero firing rate during the color period, when that input does not need to be used and is therefore irrelevant (and color decoding decreases during the color period over learning), but these inputs can be combined with shape inputs later to create a perfectly decodable XOR response.</p><p>With regards to interpretation of our results based on metabolic cost constraints, we feel that this is an unnecessarily strong criticism to say that it ‘is not backed up by the presented data/analyses.’ All of our models were trained with only a metabolic cost constraint, a noise strength, and a task performance term. Therefore, the results of the models are directly attributable to the strength of metabolic cost that we use. Additionally, although one could in principle pick any of infinitely many different parameters to change and measure the response in an optimized network, varying metabolic cost and noise are two of the most fundamental phenomena that neural circuits must contend with, and many studies have analysed the impact they have on neural circuit dynamics. Furthermore, in line with previous studies (Yang et al., 2019, Whittington et al., 2022, Sussillo et al., 2015, Orhan et al., 2019, Kao et al., 2021, Cueva et al., 2020, Driscoll et al., 2022, Song et al., 2016, Masse et al., 2019, Schimel et al., 2023), we operationalized metabolic cost in our models through L2 firing rate regularization. This cost penalizes high overall firing rates. (Such an operationalization of metabolic cost also makes sense for our models because network performance is based on firing rates rather than subthreshold activities.) There are however alternative conceivable ways to operationalize a metabolic cost; for example L1 firing rate regularization has been used previously when optimizing neural networks and promotes more sparse neural firing. Interestingly, although our L2 is generally conceived to be weaker than L1 regularization, we still found that it encouraged the network to use purely sub-threshold activity in our task. The regularization of synaptic weights may also be biologically relevant because synaptic transmission uses the most energy in the brain compared to other processes (Faria-Pereira et al., 2022, Harris et al., 2012). Additionally, even subthreshold activity could be regularized as it also consumes energy (although orders of magnitude less than spiking (Zhu et al., 2019)). Therefore, future work will be needed to examine how different metabolic costs affect the dynamics of task-optimized networks. Finally, with regards to our data analysis, we show that firing rates indeed decrease significantly over the course of learning (Fig. 5f), which is a direct prediction of neural circuits contending with metabolic cost constraints and is reflected in our networks trained under an L2 firing rate metabolic cost.</p><p>With regards to color representations in PFC only qualitatively matching those in our models, in line with the comment from Reviewer 1, we agree that due to the fact that one can achieve any desired level of activity, decoding accuracy, performance, etc in a model, we focussed on changes over learning of key metrics that are commonly used in the field. Although this can appear qualitative at times because the raw values can differ between the data and our models, our main results are ultimately strongly quantitative (e.g., Fig. 3c,d, Fig. 4h,i, and Fig. 5f). We note that we could have fine tuned the models to have similar activity levels, decoding accuracies etc to our data, and on the face of it this may have made the results appear more convincing, but we felt that such trivial fine tuning does not change any of our key results in any fundamental way and is not the aim of computational modelling. The model one chooses to analyse will always be abstracted from biology in some way, by definition. Finally, of course we note that changes in color decoding could result from other causes, but we focussed on two key phenomena that neural circuits must contend with: noise and metabolic costs. Therefore, it is likely that these two variables play a strong role in stimulus representations in neural circuits</p><p><bold>Reviewer 3:</bold></p><p>Thank you very much for your thorough and clear overview of our paper and we agree that it is important to investigate phenomena and manipulations in computational models that are almost impossible to do in vivo and we are pleased you found our mathematical analyses rigorous and nicely documented.</p><p>Although we agree that it can be useful to study the responses of individual neurons, we focussed on population analyses of all available neurons without omitting or specifically selecting neurons based on their dynamics. We are also not suggesting that the activities of individual ‘neurons’ in the models and data should be similar since our models are highly abstract firing rate models. But rather, the overall computational strategy, which one can access through population decoding and cross-generalised decoding, was what we were interested in comparing between the models and the data and is arguably the correct level of analysis of such models (an data) given our key questions (Vyas et al., 2020, Churchland et al., 2012, Mante et al., 2013, Ebitz et al., 2021).</p><p>We also certainly agree and are more than open to the fact that suppression of irrelevant stimuli may already be happening on the inputs arriving in PFC. Indeed, we actually suggest this as the mechanism in Fig. 5 (together with recurrent circuit dynamics that make use of these inputs).</p><p>With regards to the dynamics of the two-neuron networks not being ‘informative of what happens in brain networks’, we agree that these models are very simplified and may only contain very fundamental similarities with biological neurons. However, we only used them to illustrate the fundamental mechanism of generating zero firing rate during the color epoch so that it is more easily understandable for readers as they can see the entire 2-dimensional state space and the entire computational strategy can be seen (Fig. 5a-d). We also note that we did this for both rectified linear and tanh networks, thus showing that such a mechanism is preserved across fundamentally different firing rate nonlinearities. Additionally, after illustrating this fundamental mechanism of networks receiving color information but generating zero firing rate, we show that the exact same mechanism is at play in the large networks we use throughout the paper (Fig. 5e). We also only compare the large networks to our neural recordings. We do agree though that it would be interesting to further compare fundamental similarities and differences between our models and our neural recordings (always at the right level of analysis that makes sense for our chosen models) to show that the mechanisms we uncover in our models are also strongly relevant for our data.</p></body></sub-article></article>