<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91398</article-id><article-id pub-id-type="doi">10.7554/eLife.91398</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91398.4</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Machine learning of dissection photographs and surface scanning for quantitative 3D neuropathology</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-329170"><name><surname>Gazula</surname><given-names>Harshvardhan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329171"><name><surname>Tregidgo</surname><given-names>Henry FJ</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3509-8154</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329172"><name><surname>Billot</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329190"><name><surname>Balbastre</surname><given-names>Yael</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329173"><name><surname>Williams-Ramirez</surname><given-names>Jonathan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329174"><name><surname>Herisse</surname><given-names>Rogeny</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329184"><name><surname>Deden-Binder</surname><given-names>Lucas J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329175"><name><surname>Casamitjana</surname><given-names>Adria</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund13"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329176"><name><surname>Melief</surname><given-names>Erica J</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329177"><name><surname>Latimer</surname><given-names>Caitlin S</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329185"><name><surname>Kilgore</surname><given-names>Mitchell D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1101-6924</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329186"><name><surname>Montine</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329178"><name><surname>Robinson</surname><given-names>Eleanor</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329179"><name><surname>Blackburn</surname><given-names>Emily</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329180"><name><surname>Marshall</surname><given-names>Michael S</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con15"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329187"><name><surname>Connors</surname><given-names>Theresa R</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con16"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329188"><name><surname>Oakley</surname><given-names>Derek H</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con17"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-87778"><name><surname>Frosch</surname><given-names>Matthew P</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con18"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329189"><name><surname>Young</surname><given-names>Sean I</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con19"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329181"><name><surname>Van Leemput</surname><given-names>Koen</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con20"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-329182"><name><surname>Dalca</surname><given-names>Adrian V</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con21"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-270532"><name><surname>Fischl</surname><given-names>Bruce</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund14"/><xref ref-type="other" rid="fund15"/><xref ref-type="other" rid="fund16"/><xref ref-type="other" rid="fund17"/><xref ref-type="other" rid="fund18"/><xref ref-type="other" rid="fund19"/><xref ref-type="other" rid="fund20"/><xref ref-type="other" rid="fund21"/><xref ref-type="other" rid="fund23"/><xref ref-type="other" rid="fund22"/><xref ref-type="other" rid="fund27"/><xref ref-type="other" rid="fund29"/><xref ref-type="other" rid="fund30"/><xref ref-type="other" rid="fund31"/><xref ref-type="other" rid="fund32"/><xref ref-type="other" rid="fund33"/><xref ref-type="other" rid="fund34"/><xref ref-type="other" rid="fund35"/><xref ref-type="other" rid="fund36"/><xref ref-type="other" rid="fund37"/><xref ref-type="other" rid="fund38"/><xref ref-type="other" rid="fund39"/><xref ref-type="fn" rid="con22"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-329183"><name><surname>MacDonald</surname><given-names>Christine L</given-names></name><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con23"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-85349"><name><surname>Keene</surname><given-names>C Dirk</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5291-1469</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con24"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-269069"><name><surname>Hyman</surname><given-names>Bradley T</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7959-9401</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con25"/><xref ref-type="fn" rid="conf3"/></contrib><contrib contrib-type="author" corresp="yes" id="author-329135"><name><surname>Iglesias</surname><given-names>Juan E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7569-173X</contrib-id><email>jiglesiasgonzalez@mgh.harvard.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund11"/><xref ref-type="other" rid="fund12"/><xref ref-type="other" rid="fund24"/><xref ref-type="other" rid="fund25"/><xref ref-type="other" rid="fund26"/><xref ref-type="other" rid="fund28"/><xref ref-type="fn" rid="con26"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/002pd6e78</institution-id><institution>Martinos Center for Biomedical Imaging, MGH and Harvard Medical School</institution></institution-wrap><addr-line><named-content content-type="city">Charlestown</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Centre for Medical Image Computing, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042nb2s44</institution-id><institution>Computer Science and Artificial Intelligence Laboratory, MIT</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03mb6wj31</institution-id><institution>Biomedical Imaging Group, Universitat Politècnica de Catalunya</institution></institution-wrap><addr-line><named-content content-type="city">Barcelona</named-content></addr-line><country>Spain</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>BioRepository and Integrated Neuropathology (BRaIN) Laboratory and Precision Neuropathology Core, UW School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/002pd6e78</institution-id><institution>Massachusetts Alzheimer Disease Research Center, MGH and Harvard Medical School</institution></institution-wrap><addr-line><named-content content-type="city">Charlestown</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/020hwjq30</institution-id><institution>Neuroscience and Biomedical Engineering, Aalto University</institution></institution-wrap><addr-line><named-content content-type="city">Espoo</named-content></addr-line><country>Finland</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>Department of Neurological Surgery, UW School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Zhou</surname><given-names>Juan Helen</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tgyzw49</institution-id><institution>National University of Singapore</institution></institution-wrap><country>Singapore</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>19</day><month>06</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP91398</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-08-16"><day>16</day><month>08</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-07-24"><day>24</day><month>07</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.06.08.544050"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-12-12"><day>12</day><month>12</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91398.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-13"><day>13</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91398.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-21"><day>21</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91398.3"/></event></pub-history><permissions><copyright-statement>© 2023, Gazula et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Gazula et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91398-v1.pdf"/><abstract><p>We present open-source tools for three-dimensional (3D) analysis of photographs of dissected slices of human brains, which are routinely acquired in brain banks but seldom used for quantitative analysis. Our tools can: (1) 3D reconstruct a volume from the photographs and, optionally, a surface scan; and (2) produce a high-resolution 3D segmentation into 11 brain regions per hemisphere (22 in total), independently of the slice thickness. Our tools can be used as a substitute for ex vivo magnetic resonance imaging (MRI), which requires access to an MRI scanner, ex vivo scanning expertise, and considerable ﬁnancial resources. We tested our tools on synthetic and real data from two NIH Alzheimer’s Disease Research Centers. The results show that our methodology yields accurate 3D reconstructions, segmentations, and volumetric measurements that are highly correlated to those from MRI. Our method also detects expected diﬀerences between <italic>post mortem</italic> conﬁrmed Alzheimer’s disease cases and controls. The tools are available in our widespread neuroimaging suite ‘FreeSurfer’ (<ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools">https://surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools</ext-link>).</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Every year, thousands of human brains are donated to science. These brains are used to study normal aging, as well as neurological diseases like Alzheimer’s or Parkinson’s. Donated brains usually go to ‘brain banks’, institutions where the brains are dissected to extract tissues relevant to different diseases. During this process, it is routine to take photographs of brain slices for archiving purposes.</p><p>Often, studies of dead brains rely on qualitative observations, such as ‘the hippocampus displays some atrophy’, rather than concrete ‘numerical’ measurements. This is because the gold standard to take three-dimensional measurements of the brain is magnetic resonance imaging (MRI), which is an expensive technique that requires high expertise – especially with dead brains. The lack of quantitative data means it is not always straightforward to study certain conditions.</p><p>To bridge this gap, Gazula et al. have developed an openly available software that can build three-dimensional reconstructions of dead brains based on photographs of brain slices. The software can also use machine learning methods to automatically extract different brain regions from the three-dimensional reconstructions and measure their size. These data can be used to take precise quantitative measurements that can be used to better describe how different conditions lead to changes in the brain, such as atrophy (reduced volume of one or more brain regions).</p><p>The researchers assessed the accuracy of the method in two ways. First, they digitally sliced MRI-scanned brains and used the software to compute the sizes of different structures based on these synthetic data, comparing the results to the known sizes. Second, they used brains for which both MRI data and dissection photographs existed and compared the measurements taken by the software to the measurements obtained with MRI images. Gazula et al. show that, as long as the photographs satisfy some basic conditions, they can provide good estimates of the sizes of many brain structures.</p><p>The tools developed by Gazula et al. are publicly available as part of FreeSurfer, a widespread neuroimaging software that can be used by any researcher working at a brain bank. This will allow brain banks to obtain accurate measurements of dead brains, allowing them to cheaply perform quantitative studies of brain structures, which could lead to new findings relating to neurodegenerative diseases.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>dissection photography</kwd><kwd>machine learning</kwd><kwd>surface scanning</kwd><kwd>volumetry</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000049</institution-id><institution>National Institute on Aging</institution></institution-wrap></funding-source><award-id>R01AG070988</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000049</institution-id><institution>National Institute on Aging</institution></institution-wrap></funding-source><award-id>P30AG062421</award-id><principal-award-recipient><name><surname>Hyman</surname><given-names>Bradley T</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>RF1MH123195</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EB031114</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>UM1MH130981</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P30AG066509 (UW ADRC)</award-id><principal-award-recipient><name><surname>Keene</surname><given-names>C Dirk</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U19AG066567</award-id><principal-award-recipient><name><surname>Keene</surname><given-names>C Dirk</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U19AG060909</award-id><principal-award-recipient><name><surname>Keene</surname><given-names>C Dirk</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>K08AG065426</award-id><principal-award-recipient><name><surname>Latimer</surname><given-names>Caitlin S</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS112161</award-id><principal-award-recipient><name><surname>Van Leemput</surname><given-names>Koen</given-names></name></principal-award-recipient></award-group><award-group id="fund11"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Union</institution></institution-wrap></funding-source><award-id>ERC Starting Grant 677697</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund12"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002283</institution-id><institution>Alzheimer’s Research UK</institution></institution-wrap></funding-source><award-id>ARUK-IRG2019A-003</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund13"><funding-source><institution-wrap><institution>Politècnica de Catalunya</institution></institution-wrap></funding-source><award-id>Grant Ref 2021UPC-MS-67573</award-id><principal-award-recipient><name><surname>Casamitjana</surname><given-names>Adria</given-names></name></principal-award-recipient></award-group><award-group id="fund14"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U01MH117023</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund15"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R01EB023281</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund16"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EB006758</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund17"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21EB018907</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund18"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EB019956</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund19"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P41EB030006</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund20"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R56AG064027</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund21"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R01AG064027</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund22"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>5R01AG008122</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund23"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01AG016495</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund24"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R01AG070988</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund25"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>UM1MH130981</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund26"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 MH123195</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund27"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 MH121885</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund28"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1RF1MH123195</award-id><principal-award-recipient><name><surname>Iglesias</surname><given-names>Juan E</given-names></name></principal-award-recipient></award-group><award-group id="fund29"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS0525851</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund30"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21NS072652</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund31"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS070963</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund32"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS083534</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund33"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>5U01NS086625</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund34"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>5U24NS10059103</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund35"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01NS105820</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund36"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1S10RR023401</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund37"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1S10RR019307</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund38"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1S10RR023043</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><award-group id="fund39"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>5U01MH093765</award-id><principal-award-recipient><name><surname>Fischl</surname><given-names>Bruce</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>New open-source software enables, for the first time, extraction of quantitative information from brain dissection photographs that are routinely taken at brain banks for archiving purposes but otherwise not exploited.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Morphometric measurements such as cortical thickness and subcortical volumetry can be used as surrogate biomarkers of aging (<xref ref-type="bibr" rid="bib40">Salat et al., 2004</xref>; <xref ref-type="bibr" rid="bib49">Walhovd et al., 2005</xref>; <xref ref-type="bibr" rid="bib11">Coupé et al., 2017</xref>) and disease (<xref ref-type="bibr" rid="bib27">Lerch et al., 2005</xref>; <xref ref-type="bibr" rid="bib13">Desikan et al., 2009</xref>; <xref ref-type="bibr" rid="bib16">Dickerson et al., 2009</xref>; <xref ref-type="bibr" rid="bib7">Blanken et al., 2017</xref>) via conﬁrmed histopathological changes. Integrating neuroimaging tools with neuropathological assessments can enable neuropathological–neuroimaging correlations for studying neurodegenerative diseases, that is, connecting macroscopic imaging with histological ground truth derived from microscopic imaging (<xref ref-type="bibr" rid="bib50">Webster et al., 2021</xref>). While <italic>ante mortem</italic> magnetic resonance imaging (MRI) studies provide accurate and reliable morphometric data, they are often unavailable or occur too long before death, impeding reliable histopathological correlation. Cadaveric MRI can circumvent these challenges, but logistic and legal issues often complicate this procedure.</p><p>An alternative to cadaveric imaging is ex vivo MRI, which enables high-resolution image acquisition free of subject motion and other physiological noise (<xref ref-type="bibr" rid="bib17">Edlow et al., 2019</xref>). However, ex vivo MRI also has disadvantages, including: tissue degradation due to bacteria and autolysis from death to initiation of tissue ﬁxation; cross-linking of proteins due to ﬁxation that dramatically changes MRI properties of the tissue; and image artifacts caused by magnetic susceptibility interfaces that do not occur in vivo (<xref ref-type="bibr" rid="bib42">Shatil et al., 2016</xref>).</p><p>While ex vivo MRI is relatively uncommon in brain banks that seek to establish neuropathological–neuroimaging correlations (<xref ref-type="bibr" rid="bib38">Ravid, 2009</xref>; <xref ref-type="bibr" rid="bib28">Love, 2005</xref>), dissection photography is routine in nearly every brain bank. Collected specimens are typically dissected into coronal slices and photographed before further blocking and histological analysis. These photographs, often underutilized, are an invaluable information resource that, if leveraged appropriately, can play a vital role in advancing our understanding of various brain functions and disorders — mainly when ex vivo MRI is unavailable. To this end, we propose a novel software suite that, for the ﬁrst time, enables three-dimensional (3D) reconstruction and quantitative 3D morphometry of dissection photographs, powered by modern machine learning techniques (<xref ref-type="bibr" rid="bib22">Goodfellow et al., 2016</xref>) and 3D surface scanning (<xref ref-type="bibr" rid="bib41">Salvi et al., 2004</xref>). Provided that the photographs satisfy some basic requirements (presence of a ruler or ﬁducial markers to estimate pixel sizes), our tools enable volumetric analysis of brain structures from the photographs, computationally guided dissection via automated segmentation, and accurate spatial mapping for neuropathological–neuroimaging correlation.</p><p>Our suite is freely available and includes three modules. The ﬁrst module is a set of preprocessing routines for the photographs that enables the correction of perspective and calibration of pixel sizes. The second module is a joint image registration algorithm that allows 3D reconstruction of the photographs using a 3D surface scan of the brain as a reference. This module can also use a probabilistic atlas as a reference for the reconstruction, thus circumventing the need for a surface scanner. This scan-free mode enables retrospective analysis of photographs without corresponding 3D scans, albeit with lower accuracy than if surface scanning was available. The third and ﬁnal modules use machine learning to provide a high-resolution 3D image segmentation of the reconstructed stack. This module combines a state-of-the-art deep segmentation neural network (a U-Net, <xref ref-type="bibr" rid="bib39">Ronneberger et al., 2015</xref>) with a domain randomization approach (<xref ref-type="bibr" rid="bib4">Billot et al., 2023a</xref>), thus enabling analysis of photographs with diﬀerent intensity proﬁles, for example, acquired under various illumination conditions, with diﬀerent cameras or camera settings, or from ﬁxed or fresh tissue. Moreover, the machine learning method enables the estimation of ‘smooth’, isotropic segmentations that accurately interpolate across the gaps between the coronal planes captured in the photographs.</p><p>We note that this article extends our previous conference paper (<xref ref-type="bibr" rid="bib47">Tregidgo et al., 2020</xref>) by (1) improving upon the 3D reconstruction methods; (2) using machine learning (rather than Bayesian methods) to produce segmentations that are more accurate and also isotropic (i.e., provide labels in between slices); and (3) providing extensive experiments on diﬀerent synthetic and real datasets. The rest of this article is organized as follows. First, we present results on three diﬀerent datasets, both synthetic (which enable ﬁne-grained analysis with known ground truth) and real (which enables evaluation in real-world scenarios). Next, we discuss these results and their impact on quantitative <italic>post mortem</italic> neuroimaging without MRI. Finally, the Methods section elaborates on the technicalities of the preprocessing steps, the reconstruction algorithm, and the deep learning segmentation method.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Volumetric group study of <italic>post mortem</italic> conﬁrmed Alzheimer’s disease</title><p>One of the main use cases of our tools is the volumetric analysis of diﬀerent cerebral regions of interest (ROIs) from dissection photographs, without requiring cadaveric or ex vivo MRI. We used our tools to analyze 21 <italic>post mortem</italic> conﬁrmed Alzheimer’s disease (AD) cases from the Massachusetts Alzheimer’s Disease Research Center (MADRC), as well as 12 age-matched controls (<italic>N</italic> = 33 in total). We note that these cases comprise thick (∼10 mm) slabs, sliced by hand, without cutting guides – as cutting on anatomic landmarks was prioritized over consistent slice thickness. Therefore, this dataset is representative of a challenging, real-world scenario.</p><p>Examples of the inputs and outputs of the pipeline can be found in <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="video" rid="video1">Video 1</xref> in the supplementary material (also available at <ext-link ext-link-type="uri" xlink:href="https://youtu.be/wo5meYRaGUY">https://youtu.be/wo5meYRaGUY</ext-link>). The 3D surface scan (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) reveals the coarse shape of the specimen, in this case a left hemisphere. Our preprocessing tools correct for the perspective and pixel size of routine dissection photographs (b). Then, the 3D reconstruction tool uses information from the surface scan to produce a 3D reconstruction of the photographs into an imaging volume (c). This volume consists of highly anisotropic voxels, since the slice thickness is much larger than the pixel size of the photograph. The 3D reconstruction is fed to our machine learning segmentation method (Photo-SynthSeg), which produces a high-resolution, isotropic segmentation (d, e), independently of the slice thickness. The segmentations are then used to compare the volumes of brain ROIs between the two groups (e.g., the hippocampus, as in <xref ref-type="fig" rid="fig1">Figure 1f</xref>).</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-91398-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Overview of the proposed method.</title></caption></media><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Examples of inputs and outputs from the MADRC dataset.</title><p>(<bold>a</bold>) Three-dimensional (3D) surface scan of left human hemisphere, acquired prior to dissection. (<bold>b</bold>) Routine dissection photography of coronal slabs, after pixel calibration, with digital rulers overlaid. (<bold>c</bold>) 3D reconstruction of the photographs into an imaging volume. (<bold>d</bold>) Sagittal cross-section of the volume in (<bold>c</bold>) with the machine learning segmentation overlaid. The color code follows the FreeSurfer convention. Also, note that the input has low, anisotropic resolution due to the large thickness of the slices (i.e., rectangular pixels in sagittal view), whereas the 3D segmentation has high, isotropic resolution (squared pixels in any view). (<bold>e</bold>) 3D rendering of the 3D segmentation into the diﬀerent brain regions, including hippocampus (yellow), amygdala (light blue), thalamus (green), putamen (pink), caudate (darker blue), lateral ventricle (purple), white matter (white, transparent), and cortex (red, transparent). (<bold>f</bold>) Distribution of hippocampal volumes in <italic>post mortem</italic> conﬁrmed Alzheimer’s disease vs controls in the MADRC dataset, corrected for age and gender.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91398-fig1-v1.tif"/></fig><p><xref ref-type="table" rid="table1">Table 1</xref> shows the area under the receiver operating characteristic curve (AUROC) and the p-value for non-parametric Wilcoxon rank sum tests (<xref ref-type="bibr" rid="bib32">Mann and Whitney, 1947</xref>) comparing the ROI volumes of AD vs controls; we leave the accumbens area and ventral diencephalon out of the analysis as their segmentations are not reliable due to poor contrast (<xref ref-type="bibr" rid="bib18">Fischl et al., 2002</xref>). We note that the AUROC is the non-parametric equivalent of the eﬀect size; a value of 0.5 represents chance, while 1.0 represents perfect separation. Age and gender were corrected with a general linear model, whereas volumes of contralateral ROIs were averaged when full brains (rather than hemispheres) were available. Our method successfully captures well-known atrophy patterns of AD, such as hippocampal atrophy and ventricle enlargement.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Area under the receiver operating characteristic curve (AUROC) and p-value of a non-parametric Wilcoxon rank sum test comparing the volumes of brain regions for Alzheimer’s cases vs controls.</title><p>The volumes were corrected by age and sex using a general linear model. We note that the AUROC is bounded between 0 and 1 (0.5 is chance) and is the non-parametric equivalent of the eﬀect size (higher AUROC corresponds to larger diﬀerences). The sample size is <inline-formula><mml:math id="inf1"><mml:semantics><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>33</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Region</th><th align="left" valign="top">Wh matter</th><th align="left" valign="top">Cortex</th><th align="left" valign="top">Vent</th><th align="left" valign="top">Thal</th><th align="left" valign="top">Caud</th><th align="left" valign="top">Putamen</th><th align="left" valign="top">Pallidum</th><th align="left" valign="top">Hippoc</th><th align="left" valign="top">Amyg</th></tr></thead><tbody><tr><td align="left" valign="top">AUROC</td><td align="char" char="." valign="top">0.45</td><td align="char" char="." valign="top">0.52</td><td align="char" char="." valign="top">0.73</td><td align="char" char="." valign="top">0.48</td><td align="char" char="." valign="top">0.65</td><td align="char" char="." valign="top">0.64</td><td align="char" char="." valign="top">0.77</td><td align="char" char="." valign="top">0.75</td><td align="char" char="." valign="top">0.77</td></tr><tr><td align="left" valign="top">p-value</td><td align="char" char="." valign="top">0.666</td><td align="char" char="." valign="top">0.418</td><td align="char" char="." valign="top">0.016</td><td align="char" char="." valign="top">0.596</td><td align="char" char="." valign="top">0.086</td><td align="char" char="." valign="top">0.092</td><td align="char" char="." valign="top">0.005</td><td align="char" char="." valign="top">0.009</td><td align="char" char="." valign="top">0.007</td></tr></tbody></table></table-wrap></sec><sec id="s2-2"><title>Quantitative evaluation of segmentation with Photo-SynthSeg</title><p>While the AD experiment illustrates the ability of our method to detect diﬀerences in real-world data, it is crucial to speciﬁcally assess the accuracy of our segmentation and 3D reconstruction methods. To evaluate Photo-SynthSeg, we 3D reconstructed the brain volumes from the photographs of 24 cases from the AD Research Center at the University of Washington (UW-ADRC), which were cut into uniform 4 mm thick slices. Crucially, isotropic FLAIR MRI scans were acquired for these specimens ex vivo prior to dissection, which enables the use of MRI as gold standard.</p><p>We compare Photo-SynthSeg against ‘SAMSEG’, which is (to the best of our knowledge) the only available competing method. SAMSEG is a segmentation algorithm which we originally conceived for brain MRI (<xref ref-type="bibr" rid="bib37">Puonti et al., 2016</xref>), and which employs Bayesian techniques to segment MRI scans irrespective of their contrast and pulse sequence. As we showed in <xref ref-type="bibr" rid="bib47">Tregidgo et al., 2020</xref>, this technique can be adapted to 3D reconstructed photographs. <xref ref-type="fig" rid="fig2">Figure 2</xref> shows the segmentation of an UW-ADRC case using both methods. As opposed to SAMSEG, Photo-SynthSeg eﬀectively ‘interpolates’ the segmentation in between slices, independently of their thickness, and is more robust against uneven intensities than the Bayesian method. Photo-SynthSeg also includes a volumetric parcellation of the cortex based on the original SynthSeg pipeline (<xref ref-type="bibr" rid="bib4">Billot et al., 2023a</xref>), which relies on the Desikan–Killiany atlas (<xref ref-type="bibr" rid="bib12">Desikan et al., 2006</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Qualitative comparison of SAMSEG vs Photo-SynthSeg: coronal (top) and sagittal (bottom) views of the reconstruction and automated segmentation of a sample whole brain from the UW-ADRC dataset.</title><p>Note that Photo-SynthSeg supports subdivision of the cortex with tools of the SynthSeg pipeline.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91398-fig2-v1.tif"/></fig><p>To evaluate Photo-SynthSeg and SAMSEG quantitatively, we computed Dice scores against manual segmentations made on a single selected slice per subject. This slice is visually chosen to be close to the mid-coronal plane, while maximizing visibility of subcortical structures; an example of such slice, along with the manual and automated segmentations, is shown in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>. The Dice scores are displayed in <xref ref-type="fig" rid="fig3">Figure 3</xref>; as in the previous analysis, the accumbens area and ventral diencephalon are left out of the analysis. The ﬁgure also shows two ablations: using a probabilistic atlas instead of the case-speciﬁc reference; and using a version of Photo-SynthSeg dedicated to 4 mm slice thickness (i.e., the thickness of the UW-ADRC dataset, rather than using the general, thickness-agnostic model). The former assesses the impact of having to rely on a generic atlas when surface scans are not available, whereas the latter evaluates the ability of our neural network to adapt to thicknesses that are not known a priori (thanks to our domain randomization approach). The results show that Photo-SynthSeg generally outperforms SAMSEG, producing Dice scores over 0.8 for all structures except the amygdala – even though SAMSEG is better at segmenting the ventricles, thanks to their large size and strong contrast. The plots also show that using the probabilistic atlas produces realistic enough reconstructions, such that the two-dimensional (2D) Dice scores remain high. Finally, the results also show the superiority of the thickness randomization strategy over the ﬁxed spacing strategy, even if the latter had access to the ground truth spacing of the photographs. Even though the reader may initially ﬁnd this result counterintuitive, it is consistent with our previous ﬁndings in MRI segmentation, where domain randomization strategies also outperformed targeted simulations (<xref ref-type="bibr" rid="bib3">Billot et al., 2020</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Dice scores of automated vs manual segmentations on select slices.</title><p>Box plots are shown for SAMSEG, Photo-SynthSeg, and two ablations: use of probabilistic atlas and targeted simulation with 4 mm slice spacing. Dice is computed in two-dimensional (2D), using manual segmentations on select slices. We also note that the absence of extracerebral tissue in the images contributes to high Dice for the cortex.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91398-fig3-v1.tif"/></fig><p>This evaluation with Dice scores above is direct, but: (1) is based on a relatively small number of slices, and (2) disregards the ultimate purpose of segmentation, which is downstream analysis in 3D (e.g., volumetry). For this purpose, we also indirectly evaluated the methods by analyzing the volumes of brain ROIs derived from the segmentations of the whole stack. Speciﬁcally, we correlated these volumes with silver standard values derived from the isotropic FLAIR MRI scans using our ‘standard’ SynthSeg for MRI (<xref ref-type="bibr" rid="bib4">Billot et al., 2023a</xref>). <xref ref-type="table" rid="table2">Table 2</xref> shows the correlations and p-values for Steiger tests comparing the correlation coeﬃcients achieved by SAMSEG and Photo-SynthSeg – while considering their dependency due to the common ground truth sample. The results show once more that Photo-SynthSeg outperforms SAMSEG for nearly every structure – and in the few cases in which it does not, the Steiger test does not yield statistically signiﬁcant diﬀerences. We note that the correlations are above 0.8 for most brain structures, indicating that the 3D reconstructed photographs yield usable volumes in volumetric analysis. We also note that the correlations are slightly but consistently lower for the reconstructions with the probabilistic atlas, yielding correlations close to 0.8 for most brain regions.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Correlations of volumes of brains regions estimated by SAMSEG and Photo-SynthSeg from the photographs against the ground truth values derived from the magnetic resonance imaging (MRI).</title><p>The p-values are for Steiger tests comparing the correlations achieved by the two methods (accounting for the common sample).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top" colspan="3">Mask from MRI as reference</th><th align="left" valign="top" colspan="3">Probabilistic atlas as reference</th></tr></thead><tbody><tr><td align="left" valign="top"/><td align="left" valign="top"><bold>SAMSEG</bold></td><td align="left" valign="top"><bold>Photo-SynthSeg</bold></td><td align="left" valign="top"><bold>p-value</bold></td><td align="left" valign="top"><bold>SAMSEG</bold></td><td align="left" valign="top"><bold>Photo-SynthSeg</bold></td><td align="left" valign="top"><bold>p-value</bold></td></tr><tr><td align="left" valign="top">White matter</td><td align="char" char="." valign="top">0.935</td><td align="char" char="." valign="top">0.981</td><td align="char" char="." valign="top">0.0011</td><td align="char" char="." valign="top">0.886</td><td align="char" char="." valign="top">0.935</td><td align="char" char="." valign="top">0.0117</td></tr><tr><td align="left" valign="top">Cortex</td><td align="char" char="." valign="top">0.930</td><td align="char" char="." valign="top">0.979</td><td align="char" char="." valign="top">0.0001</td><td align="char" char="." valign="top">0.889</td><td align="char" char="." valign="top">0.920</td><td align="char" char="." valign="top">0.0366</td></tr><tr><td align="left" valign="top">Ventricle</td><td align="char" char="." valign="top">0.968</td><td align="char" char="." valign="top">0.988</td><td align="char" char="." valign="top">0.0004</td><td align="char" char="." valign="top">0.980</td><td align="char" char="." valign="top">0.993</td><td align="char" char="." valign="top">0.0006</td></tr><tr><td align="left" valign="top">Thalamus</td><td align="char" char="." valign="top">0.812</td><td align="char" char="." valign="top">0.824</td><td align="char" char="." valign="top">0.4350</td><td align="char" char="." valign="top">0.812</td><td align="char" char="." valign="top">0.824</td><td align="char" char="." valign="top">0.4252</td></tr><tr><td align="left" valign="top">Caudate</td><td align="char" char="." valign="top">0.719</td><td align="char" char="." valign="top">0.779</td><td align="char" char="." valign="top">0.2525</td><td align="char" char="." valign="top">0.733</td><td align="char" char="." valign="top">0.792</td><td align="char" char="." valign="top">0.2062</td></tr><tr><td align="left" valign="top">Putamen</td><td align="char" char="." valign="top">0.904</td><td align="char" char="." valign="top">0.779</td><td align="char" char="." valign="top">0.9923</td><td align="char" char="." valign="top">0.872</td><td align="char" char="." valign="top">0.792</td><td align="char" char="." valign="top">0.9598</td></tr><tr><td align="left" valign="top">Pallidum</td><td align="char" char="." valign="top">0.727</td><td align="char" char="." valign="top">0.694</td><td align="char" char="." valign="top">0.6171</td><td align="char" char="." valign="top">0.676</td><td align="char" char="." valign="top">0.658</td><td align="char" char="." valign="top">0.5698</td></tr><tr><td align="left" valign="top">Hippocampus</td><td align="char" char="." valign="top">0.830</td><td align="char" char="." valign="top">0.757</td><td align="char" char="." valign="top">0.8873</td><td align="char" char="." valign="top">0.764</td><td align="char" char="." valign="top">0.776</td><td align="char" char="." valign="top">0.4293</td></tr><tr><td align="left" valign="top">Amygdala</td><td align="char" char="." valign="top">0.598</td><td align="char" char="." valign="top">0.703</td><td align="char" char="." valign="top">0.1663</td><td align="char" char="." valign="top">0.576</td><td align="char" char="." valign="top">0.763</td><td align="char" char="." valign="top">0.0221</td></tr></tbody></table></table-wrap></sec><sec id="s2-3"><title>Quantitative evaluation of reconstruction with digitally sliced MRI data</title><p>In addition to the segmentation, it is also desirable to evaluate the 3D reconstruction algorithm with the registration error (in mm). Measuring such error with real data would require manual annotation of pairs of matching landmarks, which is labor-intensive, error-prone, and not reproducible. Instead, we use simulated (digitally sliced) data created from MRI scans. While errors estimated this way may be optimistic compared with real sliced tissue, this approach enables us to analyze the error as a continuous function of slice thickness without manual annotation eﬀort.</p><p>For this purpose, we used 500 subjects from the Human Connectome Project (HCP) dataset, which includes T1- and T2-weighted MRI scans acquired at 0.7 mm isotropic resolution. After skull stripping with FreeSurfer (<xref ref-type="bibr" rid="bib19">Fischl, 2012</xref>), we simulated dissection photographs and matching surface scans by: (1) digitally slicing the T2 scans and (2) using the surface of the T1 as a 3D reference to reconstruct the T2 slices. Speciﬁcally, we simulated slices with <inline-formula><mml:math id="inf2"><mml:semantics><mml:mrow><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> mm thickness (<inline-formula><mml:math id="inf3"><mml:semantics><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>), including random aﬃne transforms and illumination ﬁelds for every simulated slice (see example in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). While we could use a nonlinear model, the results would depend heavily on the strength of the simulated deformation. Instead, we keep the warps linear as we believe that the value of this experiment lies in the trends that the errors reﬂect, that is, their relative rather than absolute value.</p><p>After digitally distorting the images, we used our method to 3D reconstruct the slices into their original shape. The registration error was calculated as the mean voxel displacement in mm between the reconstructed and ground truth T2 slices. Additionally, to test the robustness of the reconstruction algorithm to uneven slice spacing during dissection, we also analyze the error when a random variation of the nominal thickness (thickness jitter) is introduced for every slice.</p><p><xref ref-type="fig" rid="fig4">Figure 4</xref> shows the box plot for the mean reconstruction error as a function of the slice spacing and thickness jitter. The results show that the reconstruction error is reasonably robust to increased slice spacing. On the other hand, thickness jitter yields greater increases in reconstruction error, particularly at larger slice spacings. This result highlights the importance of keeping the slice thickness as constant as possible during acquisition.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Reconstruction error (in mm) in synthetically sliced HCP data.</title><p>The ﬁgure shows box plots for the mean reconstruction error as a function of spacing and thickness jitter. A jitter of <inline-formula><mml:math id="inf4"><mml:semantics><mml:mi>j</mml:mi></mml:semantics></mml:math></inline-formula> means that the n<sup>th</sup> slice is randomly extracted from the interval <inline-formula><mml:math id="inf5"><mml:semantics><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> (rather than exactly <inline-formula><mml:math id="inf6"><mml:semantics><mml:mi>n</mml:mi></mml:semantics></mml:math></inline-formula>). The center of each box represents the median; the edges of the box represent the ﬁrst and third quartiles; and the whiskers extend to the most extreme data points not considered outliers (not shown, in order not to clutter the plot).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91398-fig4-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Neuroimaging to neuropathology correlation studies explore the relationship between gold-standard pathological diagnoses and imaging phenotypes by transferring the microscopic signatures of pathology to in vivo MRI. A signiﬁcant impediment to this eﬀort is the lack of quantitative tools for <italic>post mortem</italic> tissue analysis. For example, quantitative measurements such as cortical thickness and speciﬁc regional atrophy are often estimated qualitatively from 2D coronal slices during gross examination. A solution to this problem is leveraging dissection photography of these coronal slices routinely acquired before histology. By designing algorithms to reconstruct 3D volumes from 2D photographs and subsequently segment them, we have enabled a cost-eﬀective and time-saving link between morphometric phenotypes and neuropathological diagnosis.</p><p>Our new tools are publicly available and utilize modern deep learning techniques and surface scanning. Being available as part of FreeSurfer makes the tools easy to use by anyone with little or no training. Furthermore, the absence of tunable parameters in Photo-SynthSeg makes the results produced by our methods highly reproducible, and the robustness of the tools enables widespread application to heterogeneous datasets. This robustness has been demonstrated by applying the tools to images from two diﬀerent biobanks, with diﬀerent slice thickness, tissue processing, and photographic setup. On the UW-ADRC dataset, for which MRI scans were available, we achieved correlations above 0.8 between the volumes derived from the photographs and the ground truth obtained from the MRI.</p><p>Retrospective reconstruction of datasets with no accompanying surface scan available can be achieved with a probabilistic atlas. However, such a reconstruction is laden with ambiguities because the probabilistic atlas does not have access to the true shape of the tissue – which the surface scan directly measures. Whether the increased reconstruction error is tolerable depends on the downstream task, for example, shape analysis vs volumetry.</p><p>While deep learning segmentation tools are increasingly common in medical imaging research, their practical applicability in modalities with highly varying appearance (like dissection photography) has been hindered by their limited generalization ability. Photo-SynthSeg circumvents this problem by building on our recent work on domain randomization (<xref ref-type="bibr" rid="bib4">Billot et al., 2023a</xref>), and can segment 3D reconstructed stacks of photographs irrespective of the thickness of the slices and of the contrast properties of the tissue. Compared with SAMSEG, Photo-Synthseg also has the advantage of estimating the segmentation in between slices (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Moreover, Photo-Synthseg inherits the computational eﬃciency of neural networks, and can segment a whole case in a few seconds (or tens of seconds if no graphics processing unit [GPU] is available) without any special requirements on machine learning expertise – thus enabling broad applicability.</p><p>While registration and volumetric segmentation enable morphometry and neuropathology–neuroimaging correlation, precise white matter and pial surface placement on 3D reconstructed photographs are crucial for accurate cortical analyses – for example, producing topologically correct segmentations, as opposed to volumetric segmentations that can, for example, leak across gyri. In the future, we will extend Photo-SynthSeg to enable surface analysis for cortical placement and parcellation. While the convoluted nature of cortical surfaces makes this task diﬃcult for photographic volumes, integrating the triangular mesh provided by the surface scanner could enable accurate surface placement.</p><p>Another direction of future work will be extending the tools to axial and sagittal slices of the cerebellum and brainstem. While adapting the 3D reconstruction (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) is straightforward, the U-Net will need additional image synthesis and manual labeling eﬀorts – particularly if one wishes to include new regions, such as brainstem nuclei. Additional future analyses will include: correlating the segmentation-derived volumes with clinical scores, disease subtypes, and disease duration; using techniques like SynthSR (<xref ref-type="bibr" rid="bib23">Iglesias et al., 2023</xref>) to improve the resolution of the reconstructed volumes; exploring nonlinear deformation models for the 3D reconstruction; fully automatizing tissue segmentation from the background using neural networks; and extending the tools to 3D analysis of histological sections.</p><p>Leveraging the vast amounts of dissection photographs available at brain banks worldwide to perform morphometry is a promising avenue for enhancing our understanding of various neurodegenerative diseases. Our new tools will allow extraction of quantitative phenotypical information from these photographs – and thus augmentation of histopathological analysis. We expect this new methodology to play a crucial role in the discovery of new imaging markers to study neurodegenerative diseases.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Datasets</title><sec id="s4-1-1"><title>MADRC</title><p>Dissection photography of ﬁxed tissue and companion surface scans for 76 cases from the Massachusetts Alzheimer’s Research Center (18 whole cerebrums and 58 hemispheres). Ruling out cases with frontotemporal dementia and other comorbidities, as well as subjects that did not pass manual quality control (by JWR, RH, and LJD), led to a ﬁnal sample size of <inline-formula><mml:math id="inf7"><mml:semantics><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>33</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> (21 <italic>post mortem</italic> conﬁrmed Alzheimer’s and 12 controls). The surface scans were acquired using a turntable with an Einscan Pro HD scanner (Shining 3D, Hangzhou, China, 0.05 mm point accuracy). Slices with variable thickness were cut with a dissecting knife on a predeﬁned set of landmarks and photographed with a 15.1 MP Canon EOS 50D Digital SLR camera. Further details on the dissection and processing of the specimens can be found in Appendix 1.</p></sec><sec id="s4-1-2"><title>UW-ADRC</title><p>Dissection photography of ﬁxed tissue and companion ex vivo MRI scans for 24 cases (all of them with both hemispheres) from the Alzheimer’s Disease Research Center at the University of Washington. The MRI scans were acquired at 0.8 mm isotropic resolution using a FLAIR sequence. Coronal slices were cut with 4 mm thickness using a modiﬁed deli slicer and photographed with a 35 Megapixel (MP) camera. While no 3D surface scanning was available for this dataset, we obtained surfaces by skull stripping the MRI scans and meshing the brain surface. This dataset enables us to compute volumetric measurements from the 3D reconstructed photographs and compare them with reference values obtained from the corresponding MRI scans. Furthermore, two experienced labelers manually traced the contour of nine brain regions (white matter, cortex, lateral ventricle, thalamus, caudate, putamen, pallidum, hippocampus, and amygdala) in one slice per case, which enables computation of 2D Dice scores. Further details on the dissection and processing of the specimens can be found in Appendix 1 and <xref ref-type="bibr" rid="bib26">Latimer et al., 2023</xref>.</p></sec><sec id="s4-1-3"><title>HCP</title><p>T1- and T2-weighted MRI scans of 500 subjects from the Human Connectome Project, acquired at 0.7 mm isotropic resolution. The scans were skull stripped using FreeSurfer (<xref ref-type="bibr" rid="bib19">Fischl, 2012</xref>). We use these scans to simulate dissection photographs and matching surface scans by digitally slicing the T2 scans and using the T1 as a 3D reference to reconstruct the T2 slices. Further details on the MRI acquisition can be found in <xref ref-type="bibr" rid="bib48">Van Essen et al., 2012</xref>.</p></sec><sec id="s4-1-4"><title>T1-39</title><p>39 T1-weighted MRI scans at 1 mm isotropic resolution, with manual volumetric segmentations for 36 brain structures, which we used to train Photo-SynthSeg. We note that this is the labeled dataset that was used to build the probabilistic atlas in FreeSurfer (<xref ref-type="bibr" rid="bib18">Fischl et al., 2002</xref>). The 36 structures include 22 that are segmented by Photo-Synthseg: left and right white matter, cortex, ventricle, thalamus, caudate, putamen, pallidum, hippocampus, amygdala, accumbens area, and ventral diencephalon. The remaining 14 labels include: four labels for the cerebellum (left and right cortex and white matter); the brainstem; ﬁve labels for cerebrospinal ﬂuid regions that we do not consider; the left and right choroid plexus; and two labels for white matter hypointensities in the left and right hemispheres.</p></sec></sec><sec id="s4-2"><title>Surface scanning</title><p>Surface scanning (also known as ‘proﬁlometry’) is a technology that is becoming increasingly inexpensive (a few thousand dollars), mainly via structured light technology (<xref ref-type="bibr" rid="bib41">Salvi et al., 2004</xref>). The preferred version of our proposed pipeline relies on a surface scan of the specimen (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) acquired before slicing. This surface scan, represented by a triangular mesh, is used as an external reference to guide the 3D reconstruction of the photographs (details below). While there are no speciﬁc technical requirements for the surface scan (e.g., minimum resolution), minimizing geometric distortion (i.e., deformation) between scanning and subsequent slicing is crucial. The surface scan may be acquired with a handheld scanner with the specimen placed on a table, or with the scanner on a tripod and the sample on a turntable. As mentioned earlier, our pipeline does not strictly require the surface scan, as it can be replaced with a probabilistic atlas with a slight loss of accuracy (as shown in the Results section).</p></sec><sec id="s4-3"><title>Tissue slicing and photography</title><p>In preparation for the downstream 3D reconstruction, some requirements exist for the dissection and photography of the brain slabs. Speciﬁcally, our pipeline assumes that approximately parallel slices are cut in coronal direction – which is the standard practice in most brain banks. We further assume that the thickness of these slices is approximately constant, as ‘thickness jitter’ has a detrimental eﬀect on the results (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>Moreover, we require the presence of ﬁducials to enable pixel size calibration and perspective correction. Ideally, four ﬁducials are placed on the corners of a rectangle of known dimensions (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). In the absence of such ﬁducials, we require the presence of at least a ruler or two orthogonal rulers; the former enables pixel size calibration via image scaling, whereas the latter enables approximate perspective correction by ﬁtting an aﬃne transform.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Steps of proposed processing pipeline.</title><p>(<bold>a</bold>) Dissection photograph with brain slices on black board with ﬁducials. (<bold>b</bold>) Scale-invariant feature transform (SIFT) features for ﬁducial detection. (<bold>c</bold>) Photograph from (<bold>a</bold>) corrected for pixel size and perspective, with digital ruler overlaid. (<bold>d</bold>) Segmentation against the background, grouping pieces of tissue from the same slice. (<bold>e</bold>) Sagittal slice of the initialization of a three-dimensional (3D) reconstruction. (<bold>f</bold>) Corresponding slice of the ﬁnal 3D reconstruction, obtained with a surface as reference (overlaid in yellow). (<bold>g</bold>) Corresponding slice of the 3D reconstruction provided by a probabilistic atlas (overlaid as a heat map); the real surface is overlaid in light blue for comparison.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91398-fig5-v1.tif"/></fig><p>Our pipeline allows for multiple slices to be present in one photograph but requires that all photographs are of the same side of the slices (either anterior or posterior) and that the inferior–superior direction of the anatomy is approximately aligned with the vertical axis of the image (as in <xref ref-type="fig" rid="fig5">Figure 5a</xref>). Ideally, the slices should be photographed on a ﬂat board with a solid background color that stands out from the brain tissue, as stronger contrast between tissue and background greatly facilitates image segmentation when preprocessing the photographs (further details below).</p></sec><sec id="s4-4"><title>Preprocessing of photographs</title><p>Image preprocessing starts with geometric correction, that is, pixel size calibration and perspective correction. In the ideal scenario with four ﬁducials, the widespread widespread scale-invariant feature transform (SIFT, <xref ref-type="bibr" rid="bib29">Lowe, 1999</xref>) is used to detect such ﬁducials (<xref ref-type="fig" rid="fig5">Figure 5b</xref>) and compute a spatial transform that corrects for perspective distortion and calibrates the pixel size – see <xref ref-type="fig" rid="fig5">Figure 5c</xref>, where the pixel size calibration enables superimposition of a digital ruler.</p><p>Our software also supports a manual mode where the user clicks on two, three, or four landmarks: two points with a known distance in between (enables approximate pixel size calibration); three points at the ends of two rulers plus their intersection (enables approximate perspective correction with an aﬃne transform); or four points on the corners of a rectangle of known dimensions (enables full perspective correction). This manual mode is useful when no ﬁducials are present, but the user can still identify features with known dimensions in the photograph, for example, on an imprinted grid pattern, or along rulers.</p><p>After geometric correction, our methods require a binary segmentation of the brain tissue, separating it from the background. While our tools do not have any speciﬁc requisites in terms of background, using a solid background with a distinct color (e.g., a ‘green screen’) can greatly facilitate segmentation; otherwise, more extensive manual intervention may be needed. In our experiments, using a ﬂat black background enabled us to automatically segment the tissue with a combination of thresholding and morphological operations, keeping manual edits to a minimum (a couple of minutes per photograph, mostly to erase bits of cortical surface that are sometimes visible around the edge of the face of the slice).</p><p>Given this binary mask, the ﬁnal preprocessing step requires the user to specify the order of the slices within the photograph – which may be anterior to posterior or vice versa, but must be consistent within and across photographs of the same case. Moreover, the user also needs to specify whether two connected components belong to the same slice, which often happens around the temporal pole. This can be quickly accomplished with a dedicated graphical user interface that we distribute with our tools (<xref ref-type="fig" rid="fig5">Figure 5d</xref>).</p></sec><sec id="s4-5"><title>3D volumetric reconstruction from photographs</title><p>Recovering a 3D volume from a stack of 2D images entails a consistent 3D reconstruction of the stack via joint image alignment – known as ‘registration’ (<xref ref-type="bibr" rid="bib30">Maintz and Viergever, 1998</xref>; <xref ref-type="bibr" rid="bib36">Pluim et al., 2003</xref>; <xref ref-type="bibr" rid="bib51">Zitová and Flusser, 2003</xref>; <xref ref-type="bibr" rid="bib45">Sotiras et al., 2013</xref>). We pose 3D reconstruction as a joint optimization problem (<xref ref-type="bibr" rid="bib35">Pichat et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Mancini et al., 2019</xref>) and use a 3D reference volume for the registration. This volume is ideally a binary 3D mask obtained by rasterizing (ﬁlling) the triangular mesh from the surface scan. If a surface scan is not available, one can instead use a probabilistic atlas (<xref ref-type="bibr" rid="bib43">Shattuck et al., 2008</xref>) of brain shape, which provides a rough reference for the reconstruction. However, this reference cannot drive the reconstruction toward the actual shape of the specimen nor correct deviations from the user-provided slice thickness.</p><p>Given <inline-formula><mml:math id="inf8"><mml:semantics><mml:mi>N</mml:mi></mml:semantics></mml:math></inline-formula> photographed slices and their corresponding masks, the goal of this optimization framework is to simultaneously identify: (1) a set of <inline-formula><mml:math id="inf9"><mml:semantics><mml:mi>N</mml:mi></mml:semantics></mml:math></inline-formula> 2D aﬃne geometric transforms <inline-formula><mml:math id="inf10"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> (rotation, translation, shear, and scaling for each slice); (2) a scaling factor <inline-formula><mml:math id="inf11"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula> in the anterior–posterior direction shared by all slices; and (3) a rigid 3D transform <inline-formula><mml:math id="inf12"><mml:semantics><mml:mi>Ψ</mml:mi></mml:semantics></mml:math></inline-formula> for the reference (rotation and translation). These transforms seek to align the slices with each other and with the reference volume.</p><p>We note that aﬃne transforms (rather than rigid) are required for the photographs due to imperfections in the image preprocessing of the previous section; nevertheless, we expect the shear and scaling of these transforms to be small. Furthermore, we use aﬃne rather than nonlinear transforms because the latter compromise the robustness of the registration, as they introduce huge ambiguity in the space of solutions (e.g., one could add an identical small nonlinear deformation to every slice almost without changing the feature of the 3D reconstruction). This aﬃne model makes it particularly important to place connected components of the same slice in a correct relative position when there is more than one, for example, in the temporal pole. We further note that the scaling <inline-formula><mml:math id="inf13"><mml:semantics><mml:mi>s</mml:mi></mml:semantics></mml:math></inline-formula> in the anterior–posterior direction is required to correct deviations from the slice thickness speciﬁed by the user, which in practice is never completely exact.</p><p>The optimal set of transforms is obtained by maximizing the objective function <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. This objective encodes four desired attributes of the reconstructed data, with relative weights α, β, γ, and υ:</p><list list-type="order"><list-item><p>The α term encourages a high overlap between the stack of <inline-formula><mml:math id="inf15"><mml:semantics><mml:mi>N</mml:mi></mml:semantics></mml:math></inline-formula> 3D reconstructed slice masks <inline-formula><mml:math id="inf16"><mml:semantics><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msub><mml:mi>Φ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow> <mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> and the aligned reference volume <inline-formula><mml:math id="inf17"><mml:semantics><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>Ψ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula>; we note that images are a function of spatial location <inline-formula><mml:math id="inf18"><mml:semantics><mml:mi>x</mml:mi></mml:semantics></mml:math></inline-formula>.</p></list-item><list-item><p>The β term promotes a high similarity between the image intensities of successive (reconstructed) slices <inline-formula><mml:math id="inf19"><mml:semantics><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>Φ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:semantics><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula>, for <inline-formula><mml:math id="inf21"><mml:semantics><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>.</p></list-item><list-item><p>The γ term encourages a high overlap between successive (reconstructed) slice masks <inline-formula><mml:math id="inf22"><mml:semantics><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>Φ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:semantics><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula>, for <inline-formula><mml:math id="inf24"><mml:semantics><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>.</p></list-item><list-item><p>The υ term promotes minimal scaling and shear in the 2D aﬃne transforms; the function <inline-formula><mml:math id="inf25"><mml:semantics><mml:mi>f</mml:mi></mml:semantics></mml:math></inline-formula> is a regularizer that prevents excessive deformation of the slices – particularly those showing little tissue, for example, the ﬁrst and last slices of the stack.</p></list-item></list><p><disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="normal">Ψ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Mathematically, overlap in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is measured with the soft Dice coeﬃcient <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib15">Dice, 1945</xref>; <xref ref-type="bibr" rid="bib44">Sorensen, 1948</xref>; <xref ref-type="bibr" rid="bib33">Milletari et al., 2016</xref>); image similarity is measured with the normalized cross-correlation <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>; and the scaling and shearing are measured with the absolute value of the logarithm of the determinant of the aﬃne matrices, that is, <inline-formula><mml:math id="inf28"><mml:semantics><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Φ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:semantics></mml:math></inline-formula> is the absolute log-determinant of the 3 × 3 matrix encoding <inline-formula><mml:math id="inf29"><mml:semantics><mml:mrow><mml:msub><mml:mi>Φ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula>. The relative weights <inline-formula><mml:math id="inf30"><mml:semantics><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>ν</mml:mi></mml:mrow></mml:semantics></mml:math></inline-formula> are set via visual inspection of the output on a small pilot dataset. For the surface reference, we used: <inline-formula><mml:math id="inf31"><mml:semantics><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>.95</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>.025</mml:mn><mml:mo>,</mml:mo><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mo>/</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>. For the probabilistic atlas, we trust the reference less and also use more regularization to prevent excessive deformation of slices: <inline-formula><mml:math id="inf32"><mml:semantics><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>.8</mml:mn><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mn>.1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>. Either way, the 3D reconstruction is in practice not very sensitive to the exact value of these parameters. We also note that, as opposed to the preprocessing described in the previous section, SIFT is not a good candidate for matching consecutive slices: while it is resilient against changes in pose (e.g., object rotation), perspective, and lightning, it is not robust against changes in the object itself – such as changes between one slice to the next.</p><p>The objective function <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is minimized with standard numerical methods – speciﬁcally the limited-memory Broyden–Fletcher–Goldfarb–Shanno (LBFGS) algorithm (<xref ref-type="bibr" rid="bib20">Fletcher, 1987</xref>). The LBFGS optimizer is initialized by stacking the photographs with their centers of gravity on coordinate (0,0), and then matching the center of gravity of the whole stack with the center of gravity of the 3D reference (as illustrated in <xref ref-type="video" rid="video1">Video 1</xref> in the supplement, see also <ext-link ext-link-type="uri" xlink:href="https://youtu.be/wo5meYRaGUY">https://youtu.be/wo5meYRaGUY</ext-link>).</p><p>If a probabilistic atlas is used instead of the surface scan, the same objective function is used but with a slightly diﬀerent search space. Since we can no longer trust the external reference to correct for ﬁne shape correction: (1) we keep the scaling factor of the anterior–posterior direction ﬁxed to <inline-formula><mml:math id="inf34"><mml:semantics><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> (i.e., we trust the slice thickness speciﬁed by the user); (2) we use rigid rather than aﬃne transforms <inline-formula><mml:math id="inf35"><mml:semantics><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> for the slices (which also has the eﬀect of making the regularizer equal to zero); and (3) we use a full 3D aﬃne transform <inline-formula><mml:math id="inf36"><mml:semantics><mml:mi>Ψ</mml:mi></mml:semantics></mml:math></inline-formula> for the reference. Sample reconstructions of a case with a 3D surface and a probabilistic atlas are shown in <xref ref-type="fig" rid="fig5">Figure 5e–g</xref>. The probabilistic atlas produces a plausible reconstruction, which is however far from the real shape of the specimen given by the surface (<xref ref-type="fig" rid="fig5">Figure 5g</xref>). An additional example from the MADRC dataset is shown in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>. We note that 3D reconstruction is implemented in PyTorch, and runs eﬃciently on a GPU – less than 5 min on a Nvidia Quadro RTX 6000.</p></sec><sec id="s4-6"><title>Segmentation</title><p>The 3D reconstructed photographs have various applications (e.g., volumetry, computationally guided dissection) that require image segmentation, that is, assigning neuroanatomical labels to every spatial location (<xref ref-type="bibr" rid="bib34">Pham et al., 2000</xref>; <xref ref-type="bibr" rid="bib14">Despotović et al., 2015</xref>; <xref ref-type="bibr" rid="bib2">Akkus et al., 2017</xref>). There are two main challenges when segmenting the 3D reconstructed photographs. First, the appearance of the images varies widely across cases due to diﬀerences in camera hardware (sensor, lens), camera settings, illumination conditions, and tissue preparation (e.g., ﬁxed vs fresh). And second, accurate volumetry requires estimating the segmentation not only on the planes of the photographs but also in between slices.</p><p>In our previous work (<xref ref-type="bibr" rid="bib47">Tregidgo et al., 2020</xref>), we adopted a Bayesian segmentation strategy that handled the ﬁrst issue but not the second. Here, we extend a machine learning approach based on domain randomization (<xref ref-type="bibr" rid="bib46">Tobin et al., 2017</xref>) that we have successfully applied to segment clinical brain MRI scans with large slice spacing (<xref ref-type="bibr" rid="bib3">Billot et al., 2020</xref>; <xref ref-type="bibr" rid="bib4">Billot et al., 2023a</xref>; <xref ref-type="bibr" rid="bib5">Billot et al., 2023b</xref>). Speciﬁcally, our newly proposed approach ‘Photo-SynthSeg’ trains a convolutional neural network for image segmentation (a 3D U-Net, <xref ref-type="bibr" rid="bib39">Ronneberger et al., 2015</xref>; <xref ref-type="bibr" rid="bib9">Çiçek et al., 2019</xref>) with synthetic data as follows.</p><p>‘Photo-SynthSeg’ starts from a training dataset (the T1-39 dataset) that comprises a pool of 3D segmentations of brain images at isotropic 3D resolution (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). At every iteration during training, one of these 3D segmentations is randomly selected and geometrically deformed with a random nonlinear transform, which simulates imperfect 3D reconstruction by including a ‘deformation jitter’ in the coronal direction, that is, small but abrupt deformations from one coronal slice to the next (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). This deformed isotropic segmentation is used to generate a synthetic 3D image (also at isotropic resolution) using a Gaussian mixture model conditioned on the segmentation (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). Crucially, we randomize the Gaussian parameters (means and variances) to make the neural network robust against variations in image appearance. This synthetic isotropic image is then ‘digitally sliced’ into a 3D stack of synthetic coronal photographs, each of which is independently corrupted by a random 2D smooth illumination ﬁeld that simulates inhomogeneities in the illumination of the photographs (<xref ref-type="fig" rid="fig6">Figure 6d</xref>). Importantly, we also randomize the thickness of the simulated slices in every iteration and introduce small stochastic variations in thickness around the central value within each simulation. This strategy makes the network agnostic to the slice thickness of the reconstructed 3D stack at test time. Finally, the simulated stack, which has anisotropic resolution (e.g., 1 mm in-plane and several mm in slice thickness), is resampled to the original isotropic resolution of the (deformed) 3D segmentation (<xref ref-type="fig" rid="fig6">Figure 6e</xref>) – typically 1 mm isotropic.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Intermediate steps in the generative process.</title><p>(<bold>a</bold>) Randomly sampled input label map from the training set. (<bold>b</bold>) Spatially augmented input label map; imperfect 3D reconstruction is simulated with a deformation jitter across the coronal plane. (<bold>c</bold>) Synthetic image obtained by sampling from a Gaussian mixture model conditioned on the segmentation, with randomized means and variances. (<bold>d</bold>) Slice spacing is simulated by downsampling to low resolution. This imaging volume is further augmented with a bias ﬁeld and intensity transforms (brightness, contrast, gamma). (<bold>e</bold>) The ﬁnal training image is obtained by resampling (<bold>d</bold>) to high resolution. The neural network is trained with pairs of images like (<bold>e</bold>) (input) and (<bold>b</bold>) (target).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91398-fig6-v1.tif"/></fig><p>This generative process mimics the image formation process in the real world and has two crucial aspects. First, the super-resolution component: Photo-SynthSeg is a U-Net that will produce a high-resolution (1 mm) isotropic 3D segmentation for every input at test time, independently of the slice spacing of the 3D reconstructed stack of photographs (<xref ref-type="fig" rid="fig2">Figure 2</xref>). And second, domain randomization: sampling diﬀerent Gaussian parameters, illumination ﬁelds, and slice thicknesses at every iteration beyond realistic limits (as in <xref ref-type="fig" rid="fig6">Figure 6</xref>, where even contralateral regions have diﬀerent appearance) forces the U-Net to learn features that are independent of the intensity proﬁles of the photographs, as well as of the spacing between slices. This process makes the U-Net robust against changes in the acquisition. We note that the Gaussian distributions are univariate rather than trivariate, that is, they model grayscale rather than red–green–blue triplets; we tried training a U-Net with the latter, but the performance on color images was worse than when converting them to grayscale and using the U-Net trained with univariate Gaussians.</p><p>During training, resampled stacks and corresponding segmentations (<xref ref-type="fig" rid="fig6">Figure 6b–e</xref>) are fed to a U-Net. The U-Net architecture is the same as in our previous works with synthetic scans (<xref ref-type="bibr" rid="bib3">Billot et al., 2020</xref>; <xref ref-type="bibr" rid="bib4">Billot et al., 2023a</xref>): it consists of ﬁve levels, each separated by a batch normalization layer (<xref ref-type="bibr" rid="bib24">Ioﬀe and Szegedy, 2015</xref>) along with a max-pooling (contracting path) or an upsampling operation (expanding path). All levels comprise two convolution layers with 3 × 3 × 3 kernels. Every convolutional layer is associated with an Exponential Linear Unit activation (<xref ref-type="bibr" rid="bib10">Clevert et al., 2016</xref>), except for the last one, which uses a softmax. While the ﬁrst layer comprises 24 feature maps, this number is doubled after each max-pooling, and halved after each upsampling. Following the original U-Net architecture, skip connections are used across the contracting and expanding paths. The network is trained with a soft Dice loss (<xref ref-type="bibr" rid="bib33">Milletari et al., 2016</xref>) and the Adam optimizer (<xref ref-type="bibr" rid="bib25">Kingma and Ba, 2014</xref>). The deep learning model is implemented in Keras (<xref ref-type="bibr" rid="bib8">Chollet, 2015</xref>) with a Tensorﬂow backend (<xref ref-type="bibr" rid="bib1">Abadi et al., 2016</xref>) and runs on a few seconds on a Nvidia Quadro RTX 6000 GPU. Training takes around 7 days on the same GPU.</p><p>Finally, we note that there are two diﬀerent versions of Photo-SynthSeg: one for full cerebra and one for single hemispheres. The later is trained with left hemispheres and ﬂipped right hemispheres, and can thus be used as is for left hemispheres. To process a right hemisphere, we simply left–right ﬂip it, segment it, and ﬂip the results back. While the default Photo-SynthSeg pipeline segments the cerebral cortex as a whole (i.e., as in <xref ref-type="fig" rid="fig6">Figure 6a</xref>), our tool also oﬀers the option of subdividing it into parcels as deﬁned by the Desikan–Killiany atlas (<xref ref-type="bibr" rid="bib13">Desikan et al., 2009</xref>), like in <xref ref-type="fig" rid="fig2">Figure 2</xref>. This is achieved with the cortical parcellation module (Segmenter S3) of our tool ‘SynthSeg’ (<xref ref-type="bibr" rid="bib4">Billot et al., 2023a</xref>), which is also distributed with FreeSurfer.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>BF has a financial interest in CorticoMetrics, a company developing brain MRI measurementtechnology; his interests are reviewed and managed by Massachusetts General Hospital</p></fn><fn fn-type="COI-statement" id="conf3"><p>Reviewing editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Validation, Investigation, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Investigation, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Software, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Software, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Resources, Data curation, Investigation</p></fn><fn fn-type="con" id="con6"><p>Resources, Data curation, Investigation</p></fn><fn fn-type="con" id="con7"><p>Resources, Data curation, Investigation</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Software, Investigation, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con11"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con12"><p>Resources, Data curation, Writing – review and editing</p></fn><fn fn-type="con" id="con13"><p>Data curation, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con14"><p>Resources, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con15"><p>Resources, Data curation</p></fn><fn fn-type="con" id="con16"><p>Data curation, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con17"><p>Conceptualization, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con18"><p>Conceptualization, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con19"><p>Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con20"><p>Conceptualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con21"><p>Conceptualization, Writing – review and editing</p></fn><fn fn-type="con" id="con22"><p>Conceptualization, Writing – review and editing</p></fn><fn fn-type="con" id="con23"><p>Conceptualization, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con24"><p>Conceptualization, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con25"><p>Conceptualization, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con26"><p>Conceptualization, Software, Funding acquisition, Investigation, Visualization, Writing - original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>MADRC data: The Institutional Review Board of Massachusetts General Hospital approved the study, and consent was obtained from the patient's authorized representative at the time of death. UW-ADRC data: In 2016, Institutional Review Board of the University of Washington (UW) issued an official determination that our repository work does not meet the metric of human subject's research as we are working solely with deceased individuals. Our practices are now informed by the US Revised Uniform Anatomical Gift ACT 2006 (Last Revised or Amended in 2009) and Washington Statute Chapter 68.64 RCW. Furthermore, we work closely with the UW School of Medicine Compliance Office on our consent forms and HIPAA compliance. All materials are indeed collected under informed consent.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91398-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>HCP is publicly available at <ext-link ext-link-type="uri" xlink:href="https://www.humanconnectome.org/study/hcp-young-adult/data-releases">https://www.humanconnectome.org/study/hcp-young-adult/data-releases</ext-link>. The MADRC and UW-ADRC datasets are available on Datalad. Instructions for retrieving them can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/MGH-LEMoN/elife-data">https://github.com/MGH-LEMoN/elife-data</ext-link> (copy archived at <xref ref-type="bibr" rid="bib21">Gazula, 2024</xref>). We note that the T1-39 dataset cannot be released because it was collected under a set of Institutional Review Boards that did not include a provision approving public distribution. The code used for training segmentation models using the T1-39 dataset can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/BBillot/SynthSeg">https://github.com/BBillot/SynthSeg</ext-link> (copy archived at <xref ref-type="bibr" rid="bib6">Billot, 2024</xref>).</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>Human Connectome Project</data-title><source>Humanconnectome</source><pub-id pub-id-type="accession" xlink:href="https://www.humanconnectome.org/study/hcp-young-adult/document/900-subjects-data-release">900-subjects-data-release</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>First and foremost, we thank the research participants who donated their brains to science and their families, without whom this work would be impossible. This research was supported by primarily supported by the National Institute of Aging (R01AG070988 and P30AG062421). Other support was provided by NIH grants RF1MH123195, R01EB031114, UM1MH130981, P30AG066509 (UW ADRC), U19AG066567, U19AG060909, K08AG065426, and R01NS112161; the European Union (ERC Starting Grant 677697); Alzheimer’s Research UK (ARUK-IRG2019A-003); and the Massachusetts Life Sciences Center. AC was further funded by the European Union, the Ministry of Universities and Recovery, and the Transformation and Resilience Plan, through a call from Universitat Politècnica de Catalunya (Grant Ref 2021UPC-MS-67573). Additional support was provided by NIH grants U01MH117023, 1R01EB023281, R01EB006758, R21EB018907, R01EB019956, P41EB030006, 1R56AG064027, 1R01AG064027, 5R01AG008122, R01AG016495, 1R01AG070988, UM1MH130981, R01 MH123195, R01 MH121885, 1RF1MH123195, R01NS0525851, R21NS072652, R01NS070963, R01NS083534, 5U01NS086625,5U24NS10059103, R01NS105820, 1S10RR023401, 1S10RR019307, 1S10RR023043, and 5U01MH093765, and by the Buster Alvord Endowment. BF has a ﬁnancial interest in CorticoMetrics, a company developing brain MRI measurement technology. His interests are reviewed and managed by Massachusetts General Hospital.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tensorﬂow: A system for large-scale machine learning</article-title><conf-name>Symposium on Operating Systems Design and Implementation</conf-name><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akkus</surname><given-names>Z</given-names></name><name><surname>Galimzianova</surname><given-names>A</given-names></name><name><surname>Hoogi</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DL</given-names></name><name><surname>Erickson</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Deep Learning for brain mri segmentation: State of the art and future directions</article-title><source>Journal of Digital Imaging</source><volume>30</volume><fpage>449</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1007/s10278-017-9983-4</pub-id><pub-id pub-id-type="pmid">28577131</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Billot</surname><given-names>B</given-names></name><name><surname>Greve</surname><given-names>D</given-names></name><name><surname>Van Leemput</surname><given-names>K</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Iglesias</surname><given-names>JE</given-names></name><name><surname>Dalca</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Learning Strategy for Contrast-agnostic MRI Segmentation</article-title><conf-name>Medical Imaging with Deep Learning</conf-name><fpage>75</fpage><lpage>93</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billot</surname><given-names>B</given-names></name><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Puonti</surname><given-names>O</given-names></name><name><surname>Thielscher</surname><given-names>A</given-names></name><name><surname>Van Leemput</surname><given-names>K</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Dalca</surname><given-names>AV</given-names></name><name><surname>Iglesias</surname><given-names>JE</given-names></name><collab>ADNI</collab></person-group><year iso-8601-date="2023">2023a</year><article-title>SynthSeg: Segmentation of brain MRI scans of any contrast and resolution without retraining</article-title><source>Medical Image Analysis</source><volume>86</volume><elocation-id>102789</elocation-id><pub-id pub-id-type="doi">10.1016/j.media.2023.102789</pub-id><pub-id pub-id-type="pmid">36857946</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billot</surname><given-names>B</given-names></name><name><surname>Magdamo</surname><given-names>C</given-names></name><name><surname>Cheng</surname><given-names>Y</given-names></name><name><surname>Arnold</surname><given-names>SE</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Iglesias</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2023">2023b</year><article-title>Robust machine learning segmentation for large-scale analysis of heterogeneous clinical brain MRI datasets</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2216399120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2216399120</pub-id><pub-id pub-id-type="pmid">36802420</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Billot</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Synthseg</data-title><version designator="swh:1:rev:66342c1f05e520f3729a6d7ca1042eb7f01b6266">swh:1:rev:66342c1f05e520f3729a6d7ca1042eb7f01b6266</version><publisher-name>Software Heritage</publisher-name><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:64d291d8bcc9cb7f3116c94fd64a3f41f9de2bae;origin=https://github.com/BBillot/SynthSeg;visit=swh:1:snp:0b7a74fb1614664222c0156f6f9d85b302c91900;anchor=swh:1:rev:66342c1f05e520f3729a6d7ca1042eb7f01b6266">https://archive.softwareheritage.org/swh:1:dir:64d291d8bcc9cb7f3116c94fd64a3f41f9de2bae;origin=https://github.com/BBillot/SynthSeg;visit=swh:1:snp:0b7a74fb1614664222c0156f6f9d85b302c91900;anchor=swh:1:rev:66342c1f05e520f3729a6d7ca1042eb7f01b6266</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanken</surname><given-names>AE</given-names></name><name><surname>Hurtz</surname><given-names>S</given-names></name><name><surname>Zarow</surname><given-names>C</given-names></name><name><surname>Biado</surname><given-names>K</given-names></name><name><surname>Honarpisheh</surname><given-names>H</given-names></name><name><surname>Somme</surname><given-names>J</given-names></name><name><surname>Brook</surname><given-names>J</given-names></name><name><surname>Tung</surname><given-names>S</given-names></name><name><surname>Kraft</surname><given-names>E</given-names></name><name><surname>Lo</surname><given-names>D</given-names></name><name><surname>Ng</surname><given-names>DW</given-names></name><name><surname>Vinters</surname><given-names>HV</given-names></name><name><surname>Apostolova</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Associations between hippocampal morphometry and neuropathologic markers of Alzheimer’s disease using 7T MRI</article-title><source>NeuroImage. Clinical</source><volume>15</volume><fpage>56</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.nicl.2017.04.020</pub-id><pub-id pub-id-type="pmid">28491492</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Keras</data-title><version designator="3.0">3.0</version><source>Keras</source><ext-link ext-link-type="uri" xlink:href="https://keras.io/">https://keras.io/</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Çiçek</surname><given-names>Ö</given-names></name><name><surname>Abdulkadir</surname><given-names>A</given-names></name><name><surname>Lienkamp</surname><given-names>SS</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>3D U-Net: learning dense volumetric segmentation from sparse annotation</article-title><conf-name>Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th International Conference</conf-name><fpage>424</fpage><lpage>432</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-46723-8</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Clevert</surname><given-names>DA</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Hochreiter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast and Accurate deep network learning by exponential linear units (ELUs)</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.07289">https://arxiv.org/abs/1511.07289</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coupé</surname><given-names>P</given-names></name><name><surname>Catheline</surname><given-names>G</given-names></name><name><surname>Lanuza</surname><given-names>E</given-names></name><name><surname>Manjón</surname><given-names>JV</given-names></name><name><surname>Initiative</surname><given-names>ADN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Towards A unified analysis of brain maturation and aging across the entire lifespan: A MRI analysis</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>5501</fpage><lpage>5518</lpage><pub-id pub-id-type="doi">10.1002/hbm.23743</pub-id><pub-id pub-id-type="pmid">28737295</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Blacker</surname><given-names>D</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Maguire</surname><given-names>RP</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name><name><surname>Albert</surname><given-names>MS</given-names></name><name><surname>Killiany</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Cabral</surname><given-names>HJ</given-names></name><name><surname>Hess</surname><given-names>CP</given-names></name><name><surname>Dillon</surname><given-names>WP</given-names></name><name><surname>Glastonbury</surname><given-names>CM</given-names></name><name><surname>Weiner</surname><given-names>MW</given-names></name><name><surname>Schmansky</surname><given-names>NJ</given-names></name><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Salat</surname><given-names>DH</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Automated MRI measures identify individuals with mild cognitive impairment and Alzheimer’s disease</article-title><source>Brain</source><volume>132</volume><fpage>2048</fpage><lpage>2057</lpage><pub-id pub-id-type="doi">10.1093/brain/awp123</pub-id><pub-id pub-id-type="pmid">19460794</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Despotović</surname><given-names>I</given-names></name><name><surname>Goossens</surname><given-names>B</given-names></name><name><surname>Philips</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>MRI segmentation of the human brain: challenges, methods, and applications</article-title><source>Computational and Mathematical Methods in Medicine</source><volume>2015</volume><elocation-id>450341</elocation-id><pub-id pub-id-type="doi">10.1155/2015/450341</pub-id><pub-id pub-id-type="pmid">25945121</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dice</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="1945">1945</year><article-title>Measures of the amount of ecologic association between species</article-title><source>Ecology</source><volume>26</volume><fpage>297</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.2307/1932409</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Feczko</surname><given-names>E</given-names></name><name><surname>Augustinack</surname><given-names>JC</given-names></name><name><surname>Pacheco</surname><given-names>J</given-names></name><name><surname>Morris</surname><given-names>JC</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Differential effects of aging and Alzheimer’s disease on medial temporal lobe cortical thickness and surface area</article-title><source>Neurobiology of Aging</source><volume>30</volume><fpage>432</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2007.07.022</pub-id><pub-id pub-id-type="pmid">17869384</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edlow</surname><given-names>BL</given-names></name><name><surname>Mareyam</surname><given-names>A</given-names></name><name><surname>Horn</surname><given-names>A</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Witzel</surname><given-names>T</given-names></name><name><surname>Tisdall</surname><given-names>MD</given-names></name><name><surname>Augustinack</surname><given-names>JC</given-names></name><name><surname>Stockmann</surname><given-names>JP</given-names></name><name><surname>Diamond</surname><given-names>BR</given-names></name><name><surname>Stevens</surname><given-names>A</given-names></name><name><surname>Tirrell</surname><given-names>LS</given-names></name><name><surname>Folkerth</surname><given-names>RD</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>van der Kouwe</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>7 Tesla MRI of the ex vivo human brain at 100 micron resolution</article-title><source>Scientific Data</source><volume>6</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41597-019-0254-8</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Salat</surname><given-names>DH</given-names></name><name><surname>Busa</surname><given-names>E</given-names></name><name><surname>Albert</surname><given-names>M</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Haselgrove</surname><given-names>C</given-names></name><name><surname>van der Kouwe</surname><given-names>A</given-names></name><name><surname>Killiany</surname><given-names>R</given-names></name><name><surname>Kennedy</surname><given-names>D</given-names></name><name><surname>Klaveness</surname><given-names>S</given-names></name><name><surname>Montillo</surname><given-names>A</given-names></name><name><surname>Makris</surname><given-names>N</given-names></name><name><surname>Rosen</surname><given-names>B</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Whole brain segmentation: automated labeling of neuroanatomical structures in the human brain</article-title><source>Neuron</source><volume>33</volume><fpage>341</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00569-x</pub-id><pub-id pub-id-type="pmid">11832223</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fletcher</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1987">1987</year><source>Practical Methods of Optimization</source><publisher-name>John Wiley &amp; Sons</publisher-name><pub-id pub-id-type="doi">10.1002/9781118723203</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gazula</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>MGH-lemon/Elife-data</data-title><version designator="swh:1:rev:ea7acdca0b6b61bf37e5d555d47ec41f81e37993">swh:1:rev:ea7acdca0b6b61bf37e5d555d47ec41f81e37993</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:9e017b55b13136eff5ffc7456cdd1e527988fb6f;origin=https://github.com/MGH-LEMoN/elife-data;visit=swh:1:snp:c00f2c5a3968f93ed2492611c185016fc1a3a4aa;anchor=swh:1:rev:ea7acdca0b6b61bf37e5d555d47ec41f81e37993">https://archive.softwareheritage.org/swh:1:dir:9e017b55b13136eff5ffc7456cdd1e527988fb6f;origin=https://github.com/MGH-LEMoN/elife-data;visit=swh:1:snp:c00f2c5a3968f93ed2492611c185016fc1a3a4aa;anchor=swh:1:rev:ea7acdca0b6b61bf37e5d555d47ec41f81e37993</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Deep Learning</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iglesias</surname><given-names>JE</given-names></name><name><surname>Billot</surname><given-names>B</given-names></name><name><surname>Balbastre</surname><given-names>Y</given-names></name><name><surname>Magdamo</surname><given-names>C</given-names></name><name><surname>Arnold</surname><given-names>SE</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Edlow</surname><given-names>BL</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Golland</surname><given-names>P</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>SynthSR: A public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry</article-title><source>Science Advances</source><volume>9</volume><elocation-id>eadd3607</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.add3607</pub-id><pub-id pub-id-type="pmid">36724222</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ioﬀe</surname><given-names>S</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Batch Normalization: Accelerating deep network training by reducing internal covariate shift</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>448</fpage><lpage>456</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Latimer</surname><given-names>CS</given-names></name><name><surname>Melief</surname><given-names>EJ</given-names></name><name><surname>Ariza-Torres</surname><given-names>J</given-names></name><name><surname>Howard</surname><given-names>K</given-names></name><name><surname>Keen</surname><given-names>AR</given-names></name><name><surname>Keene</surname><given-names>LM</given-names></name><name><surname>Schantz</surname><given-names>AM</given-names></name><name><surname>Sytsma</surname><given-names>TM</given-names></name><name><surname>Wilson</surname><given-names>AM</given-names></name><name><surname>Grabowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><chapter-title>Protocol for the systematic fixation, circuit-based sampling, and qualitative and quantitative Neuropathological analysis of human brain tissue</chapter-title><person-group person-group-type="editor"><name><surname>Latimer</surname><given-names>CS</given-names></name></person-group><source>Alzheimer’s Disease</source><publisher-name>Springer</publisher-name><fpage>3</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1007/978-1-0716-2655-9</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerch</surname><given-names>JP</given-names></name><name><surname>Pruessner</surname><given-names>JC</given-names></name><name><surname>Zijdenbos</surname><given-names>A</given-names></name><name><surname>Hampel</surname><given-names>H</given-names></name><name><surname>Teipel</surname><given-names>SJ</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Focal decline of cortical thickness in Alzheimer’s disease identified by computational neuroanatomy</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>995</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh200</pub-id><pub-id pub-id-type="pmid">15537673</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Love</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neuropathological investigation of dementia: a guide for neurologists</article-title><source>Journal of Neurology, Neurosurgery &amp; Psychiatry</source><volume>76</volume><fpage>v8</fpage><lpage>v14</lpage><pub-id pub-id-type="doi">10.1136/jnnp.2005.080754</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lowe</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Object recognition from local scale-invariant features</article-title><conf-name>Proceedings of the Seventh IEEE International Conference on Computer Vision</conf-name><fpage>1150</fpage><lpage>1157</lpage><pub-id pub-id-type="doi">10.1109/ICCV.1999.790410</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maintz</surname><given-names>JB</given-names></name><name><surname>Viergever</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A survey of medical image registration</article-title><source>Medical Image Analysis</source><volume>2</volume><fpage>1</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/s1361-8415(01)80026-8</pub-id><pub-id pub-id-type="pmid">10638851</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mancini</surname><given-names>M</given-names></name><name><surname>Crampsie</surname><given-names>S</given-names></name><name><surname>Thomas</surname><given-names>DL</given-names></name><name><surname>Jaunmuktane</surname><given-names>Z</given-names></name><name><surname>Holton</surname><given-names>JL</given-names></name><name><surname>Iglesias</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Hierarchical joint registration of tissue blocks with soft shape constraints for large-scale histology of the human brain</article-title><conf-name>2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI</conf-name><fpage>666</fpage><lpage>669</lpage><pub-id pub-id-type="doi">10.1109/ISBI.2019.8759396</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>HB</given-names></name><name><surname>Whitney</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="1947">1947</year><article-title>On a test of whether one of two random variables is stochastically larger than the other</article-title><source>The Annals of Mathematical Statistics</source><volume>18</volume><fpage>50</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177730491</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Milletari</surname><given-names>F</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Ahmadi</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>V-Net: Fully convolutional neural networks for volumetric medical image segmentation</article-title><conf-name>2016 Fourth International Conference on 3D Vision (3DV</conf-name><fpage>565</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1109/3DV.2016.79</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pham</surname><given-names>DL</given-names></name><name><surname>Xu</surname><given-names>C</given-names></name><name><surname>Prince</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Current methods in medical image segmentation</article-title><source>Annual Review of Biomedical Engineering</source><volume>2</volume><fpage>315</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1146/annurev.bioeng.2.1.315</pub-id><pub-id pub-id-type="pmid">11701515</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichat</surname><given-names>J</given-names></name><name><surname>Iglesias</surname><given-names>JE</given-names></name><name><surname>Yousry</surname><given-names>T</given-names></name><name><surname>Ourselin</surname><given-names>S</given-names></name><name><surname>Modat</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A survey of methods for 3D histology reconstruction</article-title><source>Medical Image Analysis</source><volume>46</volume><fpage>73</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.media.2018.02.004</pub-id><pub-id pub-id-type="pmid">29502034</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pluim</surname><given-names>JPW</given-names></name><name><surname>Maintz</surname><given-names>JBA</given-names></name><name><surname>Viergever</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Mutual-information-based registration of medical images: a survey</article-title><source>IEEE Transactions on Medical Imaging</source><volume>22</volume><fpage>986</fpage><lpage>1004</lpage><pub-id pub-id-type="doi">10.1109/TMI.2003.815867</pub-id><pub-id pub-id-type="pmid">12906253</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puonti</surname><given-names>O</given-names></name><name><surname>Iglesias</surname><given-names>JE</given-names></name><name><surname>Van Leemput</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast and sequence-adaptive whole-brain segmentation using parametric Bayesian modeling</article-title><source>NeuroImage</source><volume>143</volume><fpage>235</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.09.011</pub-id><pub-id pub-id-type="pmid">27612647</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravid</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Biobanks for biomarkers in neurological disorders: the Da Vinci bridge for optimal clinico-pathological connection</article-title><source>Journal of the Neurological Sciences</source><volume>283</volume><fpage>119</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.jns.2009.02.364</pub-id><pub-id pub-id-type="pmid">19324376</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>U-Net: Convolutional networks for biomedical image Segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Hornegger</surname><given-names>J</given-names></name><name><surname>Wells</surname><given-names>WM</given-names></name><name><surname>Frangi</surname><given-names>AF</given-names></name></person-group><source>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 Lecture Notes in Computer Science</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-24574-4</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salat</surname><given-names>DH</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Desikan</surname><given-names>RSR</given-names></name><name><surname>Busa</surname><given-names>E</given-names></name><name><surname>Morris</surname><given-names>JC</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Thinning of the cerebral cortex in aging</article-title><source>Cerebral Cortex</source><volume>14</volume><fpage>721</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhh032</pub-id><pub-id pub-id-type="pmid">15054051</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salvi</surname><given-names>J</given-names></name><name><surname>Pagès</surname><given-names>J</given-names></name><name><surname>Batlle</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Pattern codification strategies in structured light systems</article-title><source>Pattern Recognition</source><volume>37</volume><fpage>827</fpage><lpage>849</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2003.10.002</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shatil</surname><given-names>AS</given-names></name><name><surname>Matsuda</surname><given-names>KM</given-names></name><name><surname>Figley</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A method for whole brain <italic>Ex Vivo</italic> magnetic resonance imaging with minimal susceptibility artifacts</article-title><source>Frontiers in Neurology</source><volume>7</volume><elocation-id>208</elocation-id><pub-id pub-id-type="doi">10.3389/fneur.2016.00208</pub-id><pub-id pub-id-type="pmid">27965620</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shattuck</surname><given-names>DW</given-names></name><name><surname>Mirza</surname><given-names>M</given-names></name><name><surname>Adisetiyo</surname><given-names>V</given-names></name><name><surname>Hojatkashani</surname><given-names>C</given-names></name><name><surname>Salamon</surname><given-names>G</given-names></name><name><surname>Narr</surname><given-names>KL</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Bilder</surname><given-names>RM</given-names></name><name><surname>Toga</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Construction of a 3D probabilistic atlas of human cortical structures</article-title><source>NeuroImage</source><volume>39</volume><fpage>1064</fpage><lpage>1080</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.031</pub-id><pub-id pub-id-type="pmid">18037310</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorensen</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on Danish commons</article-title><source>Biologiske Skrifter</source><volume>5</volume><fpage>1</fpage><lpage>34</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sotiras</surname><given-names>A</given-names></name><name><surname>Davatzikos</surname><given-names>C</given-names></name><name><surname>Paragios</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Deformable medical image registration: A survey</article-title><source>IEEE Transactions on Medical Imaging</source><volume>32</volume><fpage>1153</fpage><lpage>1190</lpage><pub-id pub-id-type="doi">10.1109/TMI.2013.2265603</pub-id><pub-id pub-id-type="pmid">23739795</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tobin</surname><given-names>J</given-names></name><name><surname>Fong</surname><given-names>R</given-names></name><name><surname>Ray</surname><given-names>A</given-names></name><name><surname>Schneider</surname><given-names>J</given-names></name><name><surname>Zaremba</surname><given-names>W</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Domain randomization for transferring deep neural networks from simulation to the real world</article-title><conf-name>2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS</conf-name><fpage>23</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1109/IROS.2017.8202133</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tregidgo</surname><given-names>HF</given-names></name><name><surname>Casamitjana</surname><given-names>A</given-names></name><name><surname>Latimer</surname><given-names>CS</given-names></name><name><surname>Kilgore</surname><given-names>MD</given-names></name><name><surname>Robinson</surname><given-names>E</given-names></name><name><surname>Blackburn</surname><given-names>E</given-names></name><name><surname>Van Leemput</surname><given-names>K</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Dalca</surname><given-names>AV</given-names></name><name><surname>Donald</surname><given-names>CLM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>3D reconstruction and segmentation of dissection photographs for MRI-free neuropathology</article-title><conf-name>In: Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference</conf-name><fpage>204</fpage><lpage>214</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Auerbach</surname><given-names>E</given-names></name><name><surname>Barch</surname><given-names>D</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Bucholz</surname><given-names>R</given-names></name><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Curtiss</surname><given-names>SW</given-names></name><name><surname>Della Penna</surname><given-names>S</given-names></name><name><surname>Feinberg</surname><given-names>D</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Heath</surname><given-names>AC</given-names></name><name><surname>Larson-Prior</surname><given-names>L</given-names></name><name><surname>Marcus</surname><given-names>D</given-names></name><name><surname>Michalareas</surname><given-names>G</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Prior</surname><given-names>F</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2012">2012</year><article-title>The Human Connectome Project: a data acquisition perspective</article-title><source>NeuroImage</source><volume>62</volume><fpage>2222</fpage><lpage>2231</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.018</pub-id><pub-id pub-id-type="pmid">22366334</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walhovd</surname><given-names>KB</given-names></name><name><surname>Fjell</surname><given-names>AM</given-names></name><name><surname>Reinvang</surname><given-names>I</given-names></name><name><surname>Lundervold</surname><given-names>A</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Eilertsen</surname><given-names>DE</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Salat</surname><given-names>D</given-names></name><name><surname>Makris</surname><given-names>N</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Effects of age on volumes of cortex, white matter and subcortical structures</article-title><source>Neurobiology of Aging</source><volume>26</volume><fpage>1261</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2005.05.020</pub-id><pub-id pub-id-type="pmid">16005549</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webster</surname><given-names>JM</given-names></name><name><surname>Grabowski</surname><given-names>TJ</given-names></name><name><surname>Madhyastha</surname><given-names>TM</given-names></name><name><surname>Gibbons</surname><given-names>LE</given-names></name><name><surname>Keene</surname><given-names>CD</given-names></name><name><surname>Latimer</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Leveraging neuroimaging tools to assess precision and accuracy in an alzheimer’s disease neuropathologic sampling protocol</article-title><source>Frontiers in Neuroscience</source><volume>15</volume><elocation-id>693242</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2021.693242</pub-id><pub-id pub-id-type="pmid">34483821</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zitová</surname><given-names>B</given-names></name><name><surname>Flusser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Image registration methods: a survey</article-title><source>Image and Vision Computing</source><volume>21</volume><fpage>977</fpage><lpage>1000</lpage><pub-id pub-id-type="doi">10.1016/S0262-8856(03)00137-9</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Data acquisition at the Massachusetts Alzheimer’s Disease Research Center (MADRC)</title><p>The whole brain is removed using standard autopsy procedures. After removal of the dura matter, the brain is weighed, photo-documented, and then ﬁxed in 10% neutral-buﬀered formalin (NBF). Some specimens are cut along the midline sagittal plane with one hemibrain frozen for future biochemical studies. After at least 1 week of ﬁxation, the specimens are photo-documented again. The posterior fossa is then dissected oﬀ the ﬁxed full or hemibrain through the midbrain at the level of cranial nerve III, and the supratentorial portion of the brain is 3D surface scanned.</p><p>After scanning, the brain or hemibrain is sectioned at deﬁned anatomical landmarks (anterior temporal tips, optic chiasm, infundibulum, mamillary bodies, cerebral peduncles, red nuclei, and colliculi) with the frontal and occipital lobes additionally sectioned at approximately 10 mm intervals. The front and back of all sections are then photographed with a metric ruler included for size reference. The photographs are then deidentiﬁed for further analysis.</p></sec><sec sec-type="appendix" id="s9"><title>Data acquisition at the Alzheimer’s Disease Research Center of the University of Washington (UW-ADRC)</title><p>The whole brain is removed using standard autopsy procedures. The brain is weighed and photo-documented, and then the meninges are removed. For donors with a <italic>post mortem</italic> interval (PMI: the time interval between death and procurement) longer than 12 hr, the whole brain is ﬁxed in 10% NBF with no tissue frozen. A rapid slicing protocol is performed for donors with a <italic>post mortem</italic> interval of fewer than 12 hr. For a rapid protocol, the brain is bisected along the midline, and one hemibrain is placed in a bucket with 10% NBF for 2 weeks. The other hemibrain is 3D surface scanned, then placed in a scaﬀolding box with the vermis of the cerebellum ﬂush against the posterior wall. The hemibrain is embedded in freshly mixed dental alginate (540 g powder blended in 4 l of water) and submerged until the alginate is set. The alginate block containing the hemibrain is placed in a slicing sled with the frontal pole toward the front. The hemibrain is sliced anterior to posterior in 4 mm slices. All slices are set on Teﬂon-coated aluminum plates and photographed. Alternating slices are set to be ﬂash-frozen, with the remaining slices ﬁxed in 10% NBF. The hemibrain (rapid) or whole (non-rapid) brain is ﬁxed in 10% NBF for 2 weeks, embedded in agarose, and scanned in a 3T MRI scanner.</p><p>Following the MRI, the agarose-embedded brain is sliced on a modiﬁed deli slicer for precise 4 mm tissue slabs aligned with the ex vivo MRI images for image-guided tissue sampling to standard tissue sampling following the current National Institute of Aging-Alzheimer’s Association (NIA-AA) consensus guidelines. After sampling, all blocks are processed and embedded in paraﬃn according to standard techniques. Following current NIA-AA guidelines, the resulting Fast-Frozen Paraﬃn-Embedded blocks are cut and stained for diagnostic analysis. Histologically stained slides are scanned into the HALO (<ext-link ext-link-type="uri" xlink:href="https://indicalab.com/halo/">https://indicalab.com/halo/</ext-link>) imaging workstation using an Aperio AT2 slide scanner and are analyzed by a trained neuropathologist.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Simulation and reconstruction of synthetic data.</title><p>Top row: skull stripped T1 scan and (randomly translated and rotated) binary mask of the cerebrum, in yellow. Second row: original T2 scan. Third row: randomly sliced and linearly deformed T2 images. Bottom row: output of the 3D reconstruction algorithm, that is, reconstructed T2 slices and registered reference mask overlaid in yellow.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91398-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Reconstruction with surface scan vs probabilistic atlas.</title><p>(<bold>a</bold>) Initialization, with contour of 3D surface scan superimposed. (<bold>b</bold>) Reconstruction with 3D surface scan. (<bold>c</bold>) Reconstruction with probabilistic atlas (overlaid as heat map with transparency); the contour of the surface scan is overlaid in light blue, for comparison. Even though the shape of the reconstruction in (<bold>c</bold>) is plausible, it is clearly inaccurate in light of the surface scan.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91398-app1-fig2-v1.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Example of mid-coronal slice selected for manual segmentation and computation of Dice scores.</title><p>Compared with the FreeSurFer protocol, we merge the ventral diencephalon (which has almost no visible contrast in the photographs) with the cerebral white matter in our manual delineations. We also merged this structures in the automated segmentations from SAMSEG and Photo-SynthSeg in this ﬁgure, for a more consistent comparison.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91398-app1-fig3-v1.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91398.4.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Juan Helen</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>National University of Singapore</institution><country>Singapore</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>The authors of this study implemented an <bold>important</bold> toolset for 3D reconstruction and segmentation of dissection photographs, which could serve as an alternative for cadaveric and ex vivo MRIs. The tools were tested on synthetic and real data with <bold>compelling</bold> performance. This toolset could further contribute to the study of neuroimaging-neuropathological correlations.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91398.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Gazula and co-workers presented in this paper a software tool for 3D structural analysis of human brains, using slabs of fixed or fresh brains. This tool will be included in Freesurfer, a well-known neuroimaging processing software. It is possible to reconstruct a 3D surface from photographs of coronal sliced brains, optionally using a surface scan as model. A high-resolution segmentation of 11 brain regions is produced, independent of the thickness of the slices, interpolating information when needed. Using this method, the researcher can use the sliced brain to segment all regions, without the need of ex vivo MRI scanning.</p><p>The software suite is freely available and includes 3 modules. The first accomplishes preprocessing steps, for correction of pixel sizes and perspective. The second module is a registration algorithm that registers a 3D surface scan obtained prior to sectioning (reference) to the multiple 2D slices. It is not mandatory to scan the surface, -a probabilistic atlas can also be used as reference- however the accuracy is lower. The third module uses machine learning to perform the segmentation of 11 brain structures in the 3D reconstructed volume. This module is robust, dealing with different illumination conditions, cameras, lens and camera settings. This algorithm (&quot;Photo-SynthSeg&quot;) produces isotropic smooth reconstructions, even in high anisotropic datasets (when the in-plane resolution of the photograph is much higher than the thickness), interpolating the information between slices.</p><p>To verify the accuracy and reliability of the toolbox, the authors reconstructed 3 datasets, using real and synthetic data. Real data of 21 postmortem confirmed Alzheimer's disease cases from the Massachusetts Alzheimer's Disease Research Center (MADRC)and 24 cases from the AD Research at the University of Washington(who were MRI scanned prior to processing)were employed for testing. These cases represent a challenging real-world scenario. Additionally, 500 subjects of the Human Connectome project were used for testing error as a continuous function of slice thickness. The segmentations were performed with the proposed deep-learning new algorithm (&quot;Photo-SynthSeg&quot;) and compared against MRI segmentations performed to &quot;SAMSEG&quot; (an MRI segmentation algorithm, computing Dice scores for the segmentations. The methods are sound and statistically showed correlations above 0.8, which is good enough to allow volumetric analysis. The main strengths of the methods are the datasets used (real-world challenging and synthetic) and the statistical treatment, which showed that the pipeline is robust and can facilitate volumetric analysis derived from brain sections and conclude which factors can influence in the accuracy of the method (such as using or not 3D scan and using constant thickness)).</p><p>Although very robust and capable of handling several situations, the researcher has to keep in mind that processing has to follow some basic rules in order for this pipeline to work properly. For instance, fiducials and scales need to be included in the photograph, and the slabs should be photographed against a contrasting background. Also, only coronal slices can be used, which can be limiting for certain situations.</p><p>The authors achieved their aims, and the statistical analysis confirms that the machine learning algorithm performs segmentations comparable to the state-of-the-art of automated MRI segmentations.</p><p>Those methods will be particularly interesting to researchers who deal with post-mortem tissue analysis and do not have access to ex vivo MRI. Quantitative measurements of specific brain areas can be performed in different pathologies and even in the normal aging process. The method is highly reproducible, and cost-effective since allows the pipeline to be applied by any researcher with small pre-processing steps.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91398.4.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary</p><p>The authors proposed a toolset Photo-SynthSeg to the software FreeSurfer which performs 3D reconstruction and high-resolution 3D segmentation on a stack of coronal dissection photographs of brain tissues. To prove the performance of the toolset, three experiments were conducted, including volumetric comparison of brain tissues on AD and HC groups from MADRC, quantitative evaluation of segmentation on UW-ADRC and quantitative evaluation of 3D reconstruction on HCP digitally sliced MRI data.</p><p>Strengths</p><p>To guarantee the successful workflow of the toolset, the authors clearly mentioned the prerequisites of dissection photograph acquisition, such as fiducials or rulers in the photos and tissue placement of brain slices with more than one connected component. The quantitative evaluation of segmentation and reconstruction on synthetic and real data demonstrates the accuracy of the methodology. Also, the successful application of this toolset on two brain banks with different slice thicknesses, tissue processing and photograph settings demonstrates its robustness. By working with tools of the SynthSeg pipeline, Photo-SynthSeg could further support volumetric cortex parcellation. The toolset also benefits from its adaptability of different 3D references, such as surface scan, ex vivo MRI and even probabilistic atlas, suiting the needs for different brain banks.</p><p>Weaknesses</p><p>Certain weaknesses are already covered in the manuscript. Cortical tissue segmentation could be further improved. The quantitative evaluation of 3D reconstruction is quite optimistic due to random affine transformations. Manual edits of slice segmentation task are still required and take a couple of minutes per photograph. Finally, the current toolset only accepts coronal brain slices and should adapt to axial or sagittal slices in future work.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91398.4.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gazula</surname><given-names>Harshvardhan</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Tregidgo</surname><given-names>Henry FJ</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Billot</surname><given-names>Benjamin</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts Institute of Technology</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Balbastre</surname><given-names>Yael</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Williams-Ramirez</surname><given-names>Jonathan</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Herisse</surname><given-names>Rogeny</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Deden-Binder</surname><given-names>Lucas J</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Casamitjana</surname><given-names>Adria</given-names></name><role specific-use="author">Author</role><aff><institution>Universitat Politecnica de Catalunya</institution><addr-line><named-content content-type="city">Barcelona</named-content></addr-line><country>Spain</country></aff></contrib><contrib contrib-type="author"><name><surname>Melief</surname><given-names>Erica J</given-names></name><role specific-use="author">Author</role><aff><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Latimer</surname><given-names>Caitlin S</given-names></name><role specific-use="author">Author</role><aff><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Kilgore</surname><given-names>Mitchell D</given-names></name><role specific-use="author">Author</role><aff><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Montine</surname><given-names>Mark</given-names></name><role specific-use="author">Author</role><aff><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Robinson</surname><given-names>Eleanor</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Blackburn</surname><given-names>Emily</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Marshall</surname><given-names>Miachael S</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Connors</surname><given-names>Theresa R</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Oakley</surname><given-names>Derek H</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Frosch</surname><given-names>Matthew P</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital and Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Young</surname><given-names>Sean I</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Van Leemput</surname><given-names>Koen</given-names></name><role specific-use="author">Author</role><aff><institution>Aalto University</institution><addr-line><named-content content-type="city">Helsinki</named-content></addr-line><country>Finland</country></aff></contrib><contrib contrib-type="author"><name><surname>Dalca</surname><given-names>Adrian V</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Fischl</surname><given-names>Bruce</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>MacDonald</surname><given-names>Christine L</given-names></name><role specific-use="author">Author</role><aff><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Keene</surname><given-names>C Dirk</given-names></name><role specific-use="author">Author</role><aff><institution>University of Washington</institution><addr-line><named-content content-type="city">Seattle</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Hyman</surname><given-names>Bradley T</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Iglesias</surname><given-names>Juan E</given-names></name><role specific-use="author">Author</role><aff><institution>Massachusetts General Hospital</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Is the coronal slice in Figure 2 the corresponding mid-coronal plane to compute Dice scores? If so, the authors could mention it so that readers have an idea where the selected slice is.</p></list-item></list></disp-quote><p>This is indeed a good point. The coronal slice in Figure 2 is not part of the set of slices that we used to compute Dice scores. Showing such a slice is important, so we have added a small figure to the appendix with one of these slices, along with the corresponding automated segmentations.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>SIFT descriptors were adopted to detect fiducials only. Maybe it could also be applied to align stacked photographs of brain slices.</p></list-item></list></disp-quote><p>While SIFT is robust against changes in pose (e.g., object rotation), perspective, and lightning, it is not robust against changes in the object itself – such as changes between one slice to the next, as is the case in our work. We have added a sentence to the methods section clarifying this issue.</p></body></sub-article></article>