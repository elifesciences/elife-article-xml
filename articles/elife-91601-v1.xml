<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91601</article-id><article-id pub-id-type="doi">10.7554/eLife.91601</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91601.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A previously undescribed scene-selective site is the key to encoding ego-motion in naturalistic environments</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-330155"><name><surname>Kennedy</surname><given-names>Bryan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-330156"><name><surname>Malladi</surname><given-names>Sarala N</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-330157"><name><surname>Tootell</surname><given-names>Roger BH</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-207752"><name><surname>Nasr</surname><given-names>Shahin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7546-9976</contrib-id><email>shahin.nasr@mgh.harvard.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/032q5ym94</institution-id><institution>Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital</institution></institution-wrap><addr-line><named-content content-type="city">Charlestown</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Radiology, Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schlichting</surname><given-names>Margaret L</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gt1vc06</institution-id><institution>Howard Hughes Medical Institute, Stanford University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>20</day><month>03</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP91601</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-08-28"><day>28</day><month>08</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-09-25"><day>25</day><month>09</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.21203/rs.3.rs-3378081/v1"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-01-16"><day>16</day><month>01</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91601.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-07"><day>07</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91601.2"/></event></pub-history><permissions><copyright-statement>© 2024, Kennedy et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Kennedy et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91601-v1.pdf"/><abstract><p>Current models of scene processing in the human brain include three scene-selective areas: the parahippocampal place area (or the temporal place areas), the restrosplenial cortex (or the medial place area), and the transverse occipital sulcus (or the occipital place area). Here, we challenged this model by showing that at least one other scene-selective site can also be detected within the human posterior intraparietal gyrus. Despite the smaller size of this site compared to the other scene-selective areas, the posterior intraparietal gyrus scene-selective (PIGS) site was detected consistently in a large pool of subjects (n <italic>=</italic> 59; 33 females). The reproducibility of this finding was tested based on multiple criteria, including comparing the results across sessions, utilizing different scanners (3T and 7T) and stimulus sets. Furthermore, we found that this site (but not the other three scene-selective areas) is significantly sensitive to ego-motion in scenes, thus distinguishing the role of PIGS in scene perception relative to other scene-selective areas. These results highlight the importance of including finer scale scene-selective sites in models of scene processing – a crucial step toward a more comprehensive understanding of how scenes are encoded under dynamic conditions.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>scene perception</kwd><kwd>egomotion</kwd><kwd>functional MRI</kwd><kwd>intraparietal cortex</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EY017081</award-id><principal-award-recipient><name><surname>Tootell</surname><given-names>Roger BH</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01EY030434</award-id><principal-award-recipient><name><surname>Nasr</surname><given-names>Shahin</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>There are many fine-scale scene-selective areas within the visual system, beyond the occipital, temporal, and medial place areas, whose function may be key to performing complex tasks in dynamic environments.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In human and non-human primates (NHPs), fMRI has been used for many decades to localize the cortical regions that are preferentially involved in scene perception (<xref ref-type="bibr" rid="bib14">Epstein and Kanwisher, 1998</xref>; <xref ref-type="bibr" rid="bib66">Tsao et al., 2008</xref>; <xref ref-type="bibr" rid="bib54">Rajimehr et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>). Early studies focused mainly on larger activity sites that were more easily reproducible across sessions and individuals, ignoring smaller sites that were not detectable in all subjects and/or were not reproducible across scan sessions, based on the techniques available at that time. This led to relatively simple models of neuronal processing solely based on larger visual areas.</p><p>These models suggested three scene-selective areas within the human visual cortex, with possible homologs in NHPs (<xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>; <xref ref-type="bibr" rid="bib32">Kornblith et al., 2013</xref>; <xref ref-type="bibr" rid="bib35">Li et al., 2022</xref>). The human cortical areas were originally named parahippocampal place area (PPA) (<xref ref-type="bibr" rid="bib14">Epstein and Kanwisher, 1998</xref>), retrosplenial cortex (RSC) (<xref ref-type="bibr" rid="bib36">Maguire, 2001</xref>), and transverse occipital sulcus (TOS) (<xref ref-type="bibr" rid="bib20">Grill-Spector, 2003</xref>), based on the local anatomical landmarks. However, subsequent studies noticed the discrepancy between the location of these functionally defined areas and the anatomical landmarked, and instead named those regions temporal place area (TPA), medial place area (MPA), and occipital place area (OPA) (<xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>; <xref ref-type="bibr" rid="bib12">Dilks et al., 2013</xref>; <xref ref-type="bibr" rid="bib56">Silson et al., 2016</xref>).</p><p>The idea that scene-selective areas are limited to these three regions is based largely on group-averaged activity maps, generated after applying large surface/volume-based smoothing to the data from individual subjects. In such group-averaged data, originally based on fixed- rather than random-effects, thresholds tended to be high to reduce the impact of nuisance artifacts (<xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>). Thus, though well founded, this approach conceivably may not have identified smaller scene-selective areas (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Distribution of scene-selective areas within the human visual cortex.</title><p>Panel (<bold>A</bold>) shows the group-averaged (n = 14) response to ‘scenes &gt; faces’ contrast (Experiment 1). Areas parahippocampal place area/temporal place area (PPA/TPA), restrosplenial cortex/medial place area (RSC/MPA), and transverse occipital sulcus/occipital place area (TOS/OPA) are localized within the temporal, medial and posterior-lateral brain surfaces, respectively. To show consistency with our previous reports (<xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>), data from individual subjects was largely smoothed (FWHM = 5 mm) and the group-averaged maps were generated based on fixed- rather than random-effects (see also <xref ref-type="fig" rid="fig3">Figure 3</xref>). The resultant map was thresholded at p&lt;10<sup>–25</sup> and overlaid on the common brain template (fsaverage). Panel (<bold>B</bold>) shows the activity map in one randomly selected subject (see also <xref ref-type="fig" rid="fig2">Figure 2</xref>), evoked in response to the same stimulus contrast as in panel (<bold>A</bold>) Here, the activity map was only minimally smoothed (FWHM = 2 mm). Consequently, multiple smaller scene-selective sites could be detected across the cortex, including posterior intraparietal gyrus scene-selective site (PIGS) (black arrow), located within the posterior intraparietal gyrus. Traditionally, these smaller activity patches are treated as noise in measurements and discarded. For ease in comparing the two panels, the individual’s data was also overlaid on the fsaverage.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig1-v1.tif"/></fig><p>However, at the single-subject level, multiple smaller scene-selective sites can be detected outside these scene-selective areas, especially when drastic spatial smoothing is avoided (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). This phenomenon is highlighted in a recent neuroimaging study in NHPs (<xref ref-type="bibr" rid="bib35">Li et al., 2022</xref>) in which the authors took advantage of high-resolution neuroimaging techniques using implanted head coils. Their findings suggested that scene-selective areas are likely not limited to the three expected sites, and that other, smaller, scene-selective areas may also be detected across the brain. Still, the reliability in the detection of these smaller sites, their spatial consistency across large populations, and their specific role in scene perception that distinguishes them from the other scene-selective areas remain unclear.</p><p>Here, we used conventional (based on a 3T scanner) and high-resolution (based on a 7T scanner) fMRI to localize and study additional scene-selective site(s) that were detected outside PPA/TPA, RSC/MPA, and TOS/OPA. We focused our efforts on the posterior portion of the intraparietal cortex mainly because multiple previous studies reported indirect evidence for scene and/or scene-related information processing within this region (<xref ref-type="bibr" rid="bib33">Lescroart and Gallant, 2019</xref>; <xref ref-type="bibr" rid="bib49">Pitzalis et al., 2020</xref>; <xref ref-type="bibr" rid="bib60">Sulpizio et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Park et al., 2022</xref>). Consistent with these studies, we found at least one additional scene-selective area within the posterior intraparietal gyrus, adjacent to the motion-selective area V6 (<xref ref-type="bibr" rid="bib47">Pitzalis et al., 2010</xref>). This site was termed PIGS, reflecting its location (posterior intraparietal gyrus) and function (scene-selectivity). PIGS was detected consistently across individual subjects and populations and localized reliably across scan sessions. Besides its distinct location relative to two major anatomical landmarks (i.e., intraparietal sulcus [IPS] and parieto-occipital sulci [POS]) and the retinotopic visual areas (IPS0-4) that distinguishes it from other scene-selective areas (e.g., TOS/OPA and RSC/MPA), PIGS showed sensitivity to ego-motion within naturalistic visual scenes, a phenomenon not detectable in other scene-selective areas.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>This study consists of seven experiments. Experiment 1 focused on localizing the scene-selective site (PIGS) within the posterior intraparietal region. Experiment 2 showed consistency in the spatial location of PIGS across sessions. Experiment 3 examined PIGS location relative to V6, an area involved in motion coherency and optic flow encoding, and also relative to the retinotopic visual areas IPS0-4. Experiment 4 showed that, despite its small size, PIGS is detectable in group-averaged maps in large populations. Experiment 5 showed that scenes and non-scene objects are differentiable from each other based on the evoked response evoked within PIGS. Experiment 6 tested the response in PIGS to ego-motion in scenes, yielding a result that differentiated PIGS from the other scene-selective regions. Finally, Experiment 7 showed that PIGS does not respond selectively to biological motion.</p><sec id="s2-1"><title>Experiment 1: Small scene-selective sites are detectable within the posterior intraparietal gyrus</title><p>When the level of spatial smoothing is relatively low, scene-selective sites (other than PPA/TPA, TOS/OPA, and RSC/MPA) are detectable across the brain, especially within the posterior intraparietal gyrus (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). To test the consistency in location of these scene-selective sites across individuals, 14 subjects were presented with scene and face stimuli while we collected their fMRI activity. Considering the expected small size of the scene-selective sites within the intraparietal region, we used limited signal smoothing in our analysis (FWHM = 2 mm; see ‘Methods’) to increase the chance of detecting these sites.</p><p><xref ref-type="fig" rid="fig2">Figure 2</xref> shows the activity maps evoked by the ‘scenes &gt; faces’ contrast in seven exemplar subjects. All activity maps were overlaid on a common brain template to clarify the consistency in the location of scene-selective sites across individuals. In all tested individuals, besides areas RSC/MPA and TOS/OPA, we detected at least one scene-selective site within the posterior portion of the intraparietal gyrus, close to (but outside) the POS. Accordingly, we named this site the posterior interparietal gyrus scene-selective site (PIGS).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Activity evoked by ‘scene &gt; face’ contrast in seven individual subjects, other than the one shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</title><p>Panel (<bold>A</bold>) shows the significance of evoked activity in the left hemisphere (LH) of one individual subject. The inset shows the enlarged activity map within the intraparietal region. The three scene-selective areas, along with area posterior intraparietal gyrus scene-selective site (PIGS), are indicated in the map with arrows. The location of the parieto-occipital sulcus (POS), the intraparietal sulcus (IPS), and the calcarine sulcus (CS) is also indicated in the inset. Panel (<bold>B</bold>) shows the result from six other individuals. In this panel, the first two columns show the activity within the LH, while the next two columns show the activity within the right hemisphere (RH) of the same subjects. In all subjects, PIGS (black arrow) is detectable bilaterally within the posterior portion of the intraparietal gyrus, near (but outside) the POS. All activity maps were overlaid on the fsaverage to highlight the consistency in PIGS location across the subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig2-v1.tif"/></fig><p>When measured at the same threshold levels (p&lt;10<sup>–2</sup>), the relative size of PIGS was 73.86% ± 49.01% (mean ± SD) of RSC/MPA, 28.26% ± 15.67% of TOS/OPA, and 19.45% ± 8.43% of PPA/TPA. Considering the proximity of PIGS to the skull and head coil surface (<xref ref-type="fig" rid="fig1">Figure 1</xref>), the relatively small size of PIGS could not be ascribed to the lower signal-/contrast-to-noise ratio in that region.</p><p>To better clarify the consistency of PIGS localization across subjects, we also generated group-averaged activity maps based on random-effects, and after correction for multiple comparisons. As demonstrated in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, PIGS was also detectable in the group-averaged activity maps, in almost the same location as in the individual subject maps. Overall, these results suggest that, despite the relatively small size of this scene-selective site, PIGS is consistently detectable across subjects in the same cortical location.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Posterior intraparietal gyrus scene-selective site (PIGS) was detected in group-averaged activity maps across two non-overlapping populations.</title><p>Panel (<bold>A</bold>) shows the group-averaged activity, evoked within the intraparietal region of 14 subjects who participated in Experiment 1. Panel (<bold>B</bold>) shows the group-averaged activity, evoked within the intraparietal region of 31 subjects who participated in Experiment 4. Importantly, PIGS was evident in both groups bilaterally in the corresponding location (black arrows). Thus, despite its small size, this area was detectable even in the group-averaged activity maps based on large populations. Notably, in both panels, maps were generated based on random-effects, after correction for multiple comparisons. In both maps, the location of restrosplenial cortex/medial place area (RSC/MPA) and transverse occipital sulcus/occipital place area (TOS/OPA) are respectively indicated with white and green arrows.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig3-v1.tif"/></fig></sec><sec id="s2-2"><title>Experiment 2: PIGS reproducibility across scan sessions</title><p>To test the reproducibility of our results, four subjects were selected randomly from those who participated in Experiment 1. These subjects were scanned again (on a different day) using a 7T (rather than a 3T) scanner and a different set of scenes and faces (<xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Posterior intraparietal gyrus scene-selective site (PIGS) was detected consistently across sessions.</title><p>Panel (<bold>A</bold>) shows the stimuli used for localizing PIGS during 7T scans. Stimuli including indoor, manmade outdoor, and natural outdoor scenes and faces other than those used in Experiment 1. Panels (<bold>B</bold>) and (<bold>C</bold>) show the significance (p&lt;10<sup>–2</sup>) of activity evoked by ‘scene &gt; face’ contrast in the 3T scans (Experiment 1), overlaid on subjects own reconstructed brain. Panel (<bold>D</bold>) shows the significance (p&lt;0.05) of activity evoked by ‘scene &gt; face’ contrast during 7T scans (Experiment 2). Despite the difference in scanners (3T vs. 7T) and stimuli, the location of PIGS remained mostly unchanged. Panel (<bold>E</bold>) shows the location of PIGS, measured in 3T (black dashed lines) and 7T (green dashed lines) relative to the location of area V6 (white arrow), localized functionally based on the response to ‘optic-flow &gt; random motion’ (Experiment 3 a). In all subjects, the center of scene- and optic-flow-selective responses was adjacent, but not overlapping.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig4-v1.tif"/></fig><p>As demonstrated in <xref ref-type="fig" rid="fig4">Figure 4</xref>, despite utilizing a different scanner and a different set of stimuli, PIGS was still detectable in the same location (<xref ref-type="fig" rid="fig4">Figure 4B–D</xref>). Here again, PIGS was localized within the posterior portion of the intraparietal gyrus and close to the posterior lip of POS. Considering the higher contrast/signal-to-noise ratio of 7T (compared to 3T) scans, this result strongly suggested that the PIGS evidence was not simply a nuisance artifact in fMRI measurements.</p></sec><sec id="s2-3"><title>Experiment 3: Localization of areas PIGS vs. V6 and retinotopic visual areas</title><p>Posterior intraparietal cortex accommodates area V6, which is involved in motion coherency (optic-flow) encoding (<xref ref-type="bibr" rid="bib47">Pitzalis et al., 2010</xref>). Recent studies have suggested that scene stimuli evoke a strong response within V6 (<xref ref-type="bibr" rid="bib60">Sulpizio et al., 2020</xref>). Moreover, the intraparietal cortex accommodates multiple retinotopically organized visual areas (<xref ref-type="bibr" rid="bib61">Swisher et al., 2007</xref>), including IPS0-4 that are believed to be involved in spatial attention control and higher-level object information processing (<xref ref-type="bibr" rid="bib57">Silver et al., 2005</xref>; <xref ref-type="bibr" rid="bib28">Konen and Kastner, 2008</xref>). Previous studies have suggested that the area TOS/OPA overlaps with the retinotopic visual areas V3A/B and IPS0 (V7) (<xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Silson et al., 2016</xref>). In Experiment 3, we clarified the location of PIGS relative to these regions.</p><p>In Experiment 3a, we localized V6 in all subjects who participated in Experiment 1, based on visual presentation of random vs. radially moving dots (see ‘Methods’). <xref ref-type="fig" rid="fig4">Figure 4D</xref> shows the co-localization of V6 and PIGS in four individual subjects. Consistent with previous studies (<xref ref-type="bibr" rid="bib47">Pitzalis et al., 2010</xref>; <xref ref-type="bibr" rid="bib48">Pitzalis et al., 2015</xref>), V6 was localized <italic>within</italic> the posterior portion of the POS without any overlap between its center and PIGS. To test the relative localization of these two regions at the group level, we generated probabilistic labels for PIGS and V6 (see ‘Methods’). As demonstrated in <xref ref-type="fig" rid="fig5">Figure 5</xref>, the probabilistic label for PIGS was localized within the intraparietal gyrus and outside the POS (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), while V6 was located within the POS (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). We also did not find any overlap between area V6 and areas RSC/MPA and TOS/OPA (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Thus, despite the low threshold level used to generate these labels (probability &gt;20%), the areas PIGS and V6 were located side-by-side (<xref ref-type="fig" rid="fig5">Figure 5D</xref>), without any overlapping between their centers.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Area posterior intraparietal gyrus scene-selective site (PIGS) is located outside the parieto-occipital sulci (POS) and adjacent to the functionally localized area V6.</title><p>Panels (<bold>A</bold>) and (<bold>B</bold>) show the probabilistic localization of areas PIGS and V6, respectively (see ‘Methods’). Panel (<bold>C</bold>) shows the probabilistic localization of areas restrosplenial cortex/medial place area (RSC/MPA) and transverse occipital sulcus/occipital place area (TOS/OPA). All probability maps are thresholded at 20–50% (red-to-yellow) and overlaid on the fsaverage. Panel (<bold>D</bold>) shows the relative location of these sites. Consistent with the results from the individual maps (<xref ref-type="fig" rid="fig4">Figure 4E</xref>), PIGS and V6 were located adjacent to each other, such that V6 was located within the POS and PIGS was located outside the POS (within the intraparietal gyrus) with minimal overlap between the two regions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig5-v1.tif"/></fig><p>In Experiment 3b, we scanned two subjects, randomly selected from those who had participated in Experiment 2, using a 7T scanner to map the borders of retinotopic visual areas (see ‘Methods’). As demonstrated in <xref ref-type="fig" rid="fig6">Figure 6</xref>, in both subjects, PIGS was located adjacent to IPS3 and IPS4. In comparison, TOS/OPA was located more ventrally relative to PIGS, overlapping with areas V3A/B and IPS0 (V7). Considering these differences in the localization of PIGS vs. TOS/OPA, relative to the anatomical and functionally defined landmarks, our results further suggest that PIGS and TOS/OPA are two distinct visual areas.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Localization of posterior intraparietal gyrus scene-selective site (PIGS) and transverse occipital sulcus/occipital place area (TOS/OPA) relative to the retinotopic visual areas in the right hemisphere of two subjects.</title><p>The right and left columns show respectively the polar angle and scene &gt; face response mapping, collected in a 7T scanner on two different days. In both subjects, PIGS was located close to areas IPS3-4. Area TOS/OPA overlapped with areas V3A/B and IPS0 (V7). The borders of visual areas (defined based on the polar angle mapping) are indicated by dashed black lines. Notably, for both subjects, maps were overlaid on their own reconstructed flattened cortex. No activity smoothing was applied to the collected data (i.e., FWHM = 0; see ‘Methods’). Similar results were also found in the opposite hemispheres (not shown here). On the right column, the scale bars indicate 1 cm.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig6-v1.tif"/></fig></sec><sec id="s2-4"><title>Experiment 4: PIGS localization in a larger population</title><p>The results of Experiments 1–3 suggest that PIGS can be localized consistently across individual subjects, and this area appears to be distinguishable from the adjacent area V6. However, considering the small size of this area, it appears necessary to test whether this area was detectable based on group averaging in a larger population. Accordingly, in Experiment 4 we scanned 31 individuals (other than those who participated in Experiments 1–3) while they were presented with the same stimuli as in Experiment 1 (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p><p>As demonstrated in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, PIGS was also detectable in this new population in almost the same location as in Experiment 1. Specifically, PIGS was detected bilaterally within the posterior portion of the intraparietal gyrus, adjacent to the POS. We did not find a significant difference between the two populations in the size of PIGS when normalized either relative to the size of RSC/MPA (t(43) = 0.98, p=0.33), or TOS/OPA (t(43) = 0.26, p=0.80) or PPA/TPA (t(43) = 0.52, p=0.61). Thus, the location and relative size of PIGS appeared to remain unchanged across populations.</p><p>These results suggest that one may rely on the probabilistically generated labels to examine the evoked activity within PIGS. To test this hypothesis, we measured the level of scene-selective activity in PIGS, along with the areas TOS/OPA, RSC/MPA, and V6, using the probabilistic labels generated based on the results of Experiments 1 and 3a (see ‘Methods’ and <xref ref-type="fig" rid="fig5">Figure 5</xref>). As demonstrated in <xref ref-type="fig" rid="fig7">Figure 7A and B</xref>, results of this region of interest (ROI) analysis showed a significant scene-selective activity in PIGS (t(31) = 8.11, p&lt;10<sup>–8</sup>), TOS/OPA (t(31) = 7.91, p&lt;10<sup>–7</sup>), and RSC/MPA (t(31) = 9.11, p&lt;10<sup>–8</sup>). Importantly, despite the proximity of PIGS and V6, the level of scene-selective activity in PIGS was significantly higher than that in V6 (t(11) = 5.03, p&lt;10<sup>–4</sup>). Thus, it appears that the probabilistically generated ROIs can be used to examine PIGS response and differentiate it from adjacent areas such as V6 (see also Experiment 5).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Probabilistically generated labels can be used to measure posterior intraparietal gyrus scene-selective site (PIGS) response in different experimental conditions.</title><p>Panel (<bold>A</bold>) shows the activity evoked by scenes and faces, across PIGS, V6, restrosplenial cortex/medial place area (RSC/MPA), and transverse occipital sulcus/occipital place area (TOS/OPA), all of them localized based on probabilistically generated labels based on a different group of subjects. Panel (<bold>B</bold>) shows the level of scene-selective activity, measured as ‘scene – face’, within these regions. Despite the small size of PIGS, the probabilistic label could detect the scene-selective activity within this area and the level of this activity was significantly higher than the adjacent area V6. In all panels, each dot represents the activity measured in one subject.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig7-v1.tif"/></fig></sec><sec id="s2-5"><title>Experiment 5: Selective response to scenes compared to non-scene objects in PIGS</title><p>Thus far, we localized PIGs in multiple experiments by contrasting the response evoked by scenes vs. faces. In Experiments 5a and 5b, we examined whether PIGS also showed a selective response to scenes compared to objects (not just faces). In Experiment 5a, 12 individuals, other than those who participated in Experiments 1–3, were scanned while viewing pictures of scenes (other than those used to localize PIGS) and everyday objects (<xref ref-type="fig" rid="fig8">Figure 8A</xref>; see Methods).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Posterior intraparietal gyrus scene-selective site (PIGS) could also be detected based on the ‘scene &gt; object’ contrast.</title><p>Panels (<bold>A</bold>) and (<bold>D</bold>) show the stimuli used in Experiments 5a and 5b respectively. Panels (<bold>B</bold>) and (<bold>E</bold>) show the activity maps evoked by ‘scene &gt; object’ contrast in two different individuals who participated in Experiments 5a and 5b. Panels (<bold>C</bold>) and (<bold>F</bold>) show the activity maps evoked by a different set of scenes and faces (used in Experiments 1 and 4) in the same individuals. The location of PIGS, as indicated by the blacks, remained unchanged between the two maps.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig8-v1.tif"/></fig><p>As demonstrated in <xref ref-type="fig" rid="fig8">Figure 8B and C</xref> for one individual subject, ‘scenes vs. objects’ and ‘scenes vs. faces’ (Experiment 4) contrasts generated similar activity maps. Importantly, in both maps, PIGS was detectable in a consistent location adjacent to (but outside) the POS. Moreover, results of an ROI analysis, using the probabilistically generated labels based on the results of Experiments 1 and 3a, yielded significant scene-selective activity within PIGS (t(11) = 6.57, p&lt;10<sup>–4</sup>), RSC/MPA (t(12) = 11.00, p&lt;10<sup>–6</sup>), and TOS/OPA (t(12) = 6.26, p&lt;10<sup>–3</sup>) (<xref ref-type="fig" rid="fig9">Figure 9A and B</xref>). We also found that the level of scene-selective activity within PIGS is significantly higher than that in the adjacent area V6 (t(11) = 2.42, p=0.03). Thus, scenes and (non-face) objects are differentiable from each other, based on the activity evoked within PIGS.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>The application of probabilistically generated labels to measure the posterior intraparietal gyrus scene-selective site (PIGS) response to ‘scene vs. object’ stimuli.</title><p>Panels (<bold>A</bold>) and (<bold>C</bold>) show the activity evoked by scenes and objects in Experiments 5a and 5b, respectively. Panels (<bold>B</bold>) and (<bold>D</bold>) show the level of scene-selective activity within the regions of interest. As in Experiment 4, the probabilistic label detected the scene-selective activity within PIGS and the level of this activity was significantly higher than the adjacent area V6. Other details are similar to <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig9-v1.tif"/></fig><p>In Experiment 5b, 15 individuals (other than those who participated in Experiments 1 and 5a), were scanned while viewing a new set of stimuli that included pictures of scenes, faces, everyday objects, and scrambled objects (<xref ref-type="fig" rid="fig8">Figure 8D</xref>). In contrast to Experiment 5a in which the number of objects within each image could vary, here, each image contained only one object (see ‘Methods’). Despite this change, contrasting the response to scene vs. non-scene images (averaged over objects, scrambled objects, and faces) evoked a similar activity pattern, as scene vs. faces (<xref ref-type="fig" rid="fig8">Figure 8E and F</xref>). Moreover, the ROI analysis yielded a significant scene-selective activity within PIGS (t(14) = 2.37, p=0.03), RSC/MPA (t(14) = 10.33, p&lt;10<sup>–7</sup>), and TOS/OPA (t(14) = 4.79, p&lt;10<sup>–3</sup>) (<xref ref-type="fig" rid="fig9">Figure 9</xref>). Here again, the level of scene-selective activity within PIGS was higher than V6 (t(14) = 2.27, p=0.04). Together, results of Experiments 1–5 suggest that PIGS responds selectively to a wide range of scenes compared to non-scene objects, and that the level of this activity is higher than in the adjacent area V6.</p></sec><sec id="s2-6"><title>Experiment 6: PIGS response to ego-motion</title><p>Experiments 1–5 clarified the location of PIGS and its general functional selectivity for scenes. However, a more specific role of this area in scene perception remains undefined. Experiment 6 tested the hypothesis that area PIGS is involved in encoding ego-motion within scenes. This hypothesis was motivated by the fact that PIGS is located adjacent to V6 (<xref ref-type="fig" rid="fig5">Figure 5D</xref>), an area involved in encoding optic flow. Other studies have also suggested that ego-motion may influence the scene-selective activity within this region, without clarifying whether this activity was centered either within or outside V6 (<xref ref-type="bibr" rid="bib49">Pitzalis et al., 2020</xref>; <xref ref-type="bibr" rid="bib60">Sulpizio et al., 2020</xref>).</p><p>Twelve individuals, from those who participated in Experiment 1, took part in this experiment (see ‘Methods’). These subjects were presented with coherently changing scene stimuli that implied ego-motion across different outdoor trails (<xref ref-type="fig" rid="fig10">Figure 10</xref>). In separate blocks, they were also presented with incoherently changing scenes and faces. <xref ref-type="fig" rid="fig9">Figure 9</xref> shows the group-averaged scene-selective activity, evoked by coherently (<xref ref-type="fig" rid="fig11">Figure 11A</xref>) and incoherently changing scene stimuli (<xref ref-type="fig" rid="fig11">Figure 11B</xref>). Consistent with our hypothesis, PIGS showed a significantly stronger response (bilaterally) to coherently (compared to incoherently) changing scenes that implied ego-motion (<xref ref-type="fig" rid="fig11">Figure 11C</xref>). However, the level of activity within RSC/MPA and TOS/OPA did not change significantly between these two conditions.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Example of stimuli used in Experiment 6.</title><p>Coherently changing scenes implied ego-motion as if the observer was jogging through a trail. Incoherently changing scenes consisted of the same scene images as the coherently changing scenes but presented in a pseudo-random order. Face stimuli consisted of a mosaic of faces. These stimuli were different than those used in the previous experiments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig10-v1.tif"/></fig><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Scene-selective response to coherently vs. incoherently changing scenes within the intraparietal region (Experiment 6).</title><p>Panels (<bold>A</bold>) and (<bold>B</bold>) show respectively the group-averaged activity evoked by coherently and incoherently changing scenes relative to faces. Panel (<bold>C</bold>) shows the group-averaged response evoked by the ‘coherently &gt; incoherently changing scenes’ contrast. Among scene-selective areas, only posterior intraparietal gyrus scene-selective site (PIGS) showed significant sensitivity to the observer ego-motion. Besides PIGS, this contrast also evoked activity within area MT (cyan arrows), also within more dorsal portions of the parietal cortex. Panel (<bold>D</bold>) shows the location of scene-selective areas in the same group of subjects based on an independent set of scene and face stimuli (Experiment 1). In all panels, the location of PIGS outside the parieto-occipital sulci (POS) defined based on panel (<bold>D</bold>) is indicated by black dashed lines. All maps were generated based on random-effects, after correction for multiple comparisons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig11-v1.tif"/></fig><p>Consistent with the group-averaged activity maps, results of an ROI analysis (<xref ref-type="fig" rid="fig12">Figure 12</xref>) yielded a significantly stronger response to coherently (vs. incoherently) changing scenes in PIGS (t(11) = 5.97, p&lt;10<sup>–4</sup>) but not in RSC/MPA (t(11) = 0.12, p=0.90) and TOS/OPA (t(11) = 0.48, p=0.64). Interestingly, area PPA/TPA showed a stronger response to incoherently (compared to coherently) changing scenes (t(11) = 3.48, p&lt;0.01). To better clarify the difference between scene-selective areas, we repeated this test by applying a one-way repeated measures ANOVA to the differential response to ‘coherently vs. incoherently changing scenes’, measured across these four scene-selective areas. This test yielded a significant effect of area on the evoked differential activity (F(3, 11) = 53.89, p&lt;10<sup>–10</sup>). Post hoc analysis, with Bonferroni correction, showed that the level of differential activity evoked by ‘coherently vs. incoherently changing scenes’ was significantly higher within PIGS than all other scene-selective areas (p&lt;10<sup>–6</sup>). These results suggest a distinctive role for area PIGS in ego-motion encoding, which differentiates it from the other scene-selective areas. The absence of activity modulation in the other scene-selective areas also ruled out the possibility that the activity increase in PIGS was simply due to attentional modulation during coherently vs. incoherently changing scenes (see ‘Discussion’).</p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>The scene-selective activity evoked within posterior intraparietal gyrus scene-selective site (PIGS) is influenced by the ego-motion.</title><p>Panel (<bold>A</bold>) shows the scene-selective activity evoked by the coherently (red) and incoherently changing scenes (blue), measured relative to the response to the faces, across areas PIGS, V6, restrosplenial cortex/medial place area (RSC/MPA), transverse occipital sulcus/occipital place area (TOS/OPA), and parahippocampal place area/temporal place area (PPA/TPA). Panel (<bold>B</bold>) shows the level of difference between the response evoked by ‘coherently – incoherently changing scenes’ across the regions of interest. While all regions showed a significantly stronger response to scenes compared to faces, PIGS showed the strongest impact of ego-motion on the scene-selective response. Other details are similar to <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig12-v1.tif"/></fig><p>In addition to PIGS, we also found a significantly stronger response to coherently (rather than incoherently) changing scenes in area V6 (t(11) = 3.57, p&lt;0.01). However, the level of this selectivity was significantly weaker in V6 compared to that in PIGS (t(11) = 2.63, p=0.02). Moreover, in the group-averaged activity maps, the contrast between coherently vs. incoherently changing scenes yielded a stronger response outside (rather than inside) the POS and also in area MT, located at the tip of medial temporal sulcus (<xref ref-type="fig" rid="fig11">Figure 11C</xref>). Together, these results suggest that the impact of ego-motion on scene processing is stronger in PIGS than that in V6.</p><p>In the same session (but different runs), we also tested the selectivity of the PIGS response for simpler forms of motion. In different blocks, subjects were presented with radially moving vs. stationary concentric rings (see ‘Methods’). Consistent with the previous studies of motion perception (<xref ref-type="bibr" rid="bib47">Pitzalis et al., 2010</xref>; <xref ref-type="bibr" rid="bib31">Korkmaz Hacialihafiz and Bartels, 2015</xref>), the results of an ROI analysis here did not yield any strong (significant) motion-selective activity within PIGS (t(11) = 1.84, p=0.10), RSC/MPA (t(11) = 1.97, p=0.08), PPA/TPA (t(11) = 1.93, p=0.08), and V6 (t(11) = 2.03, p=0.07). In contrast, we found strong motion selectivity within area TOS/OPA (t(11) = 4.57, p&lt;10<sup>–3</sup>), likely due to its overlap with the motion-selective area V3A/B (<xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>). Thus, in contrast to optic flow and ego-motion, simpler forms of motion only evoke weak-to-no selective activity within PIGS and V6.</p></sec><sec id="s2-7"><title>Experiment 7: PIGS response to biological motion</title><p>The results of Experiment 6 showed that PIGS responds selectively to ego-motion in scenes, but not strongly to radially moving rings. However, it could be argued that PIGS may also respond to the other types of complex motion, for example, biological motion. To test this hypothesis, we measured the PIGS response to biological vs. translational motion in 12 subjects (see ‘Methods’). As illustrated in <xref ref-type="fig" rid="fig13">Figure 13</xref>, and consistent with the previous studies of biological motion (<xref ref-type="bibr" rid="bib52">Puce et al., 1998</xref>; <xref ref-type="bibr" rid="bib5">Beauchamp et al., 2003</xref>; <xref ref-type="bibr" rid="bib53">Puce and Perrett, 2003</xref>; <xref ref-type="bibr" rid="bib46">Pelphrey et al., 2005</xref>; <xref ref-type="bibr" rid="bib23">Jastorff and Orban, 2009</xref>; <xref ref-type="bibr" rid="bib25">Kamps et al., 2016</xref>), biological motion evoked a stronger response bilaterally within area MT and superior temporal sulcus but not within the posterior intraparietal gyrus. Consistent with the maps, an ROI analysis (based on the functionally defined labels) showed no significant difference between the response to biological vs. translational motion within PIGS (t(11) = 1.27, p=0.23), TOS/OPA (t(11) = 1.63, p=0.13), RSC/MPA (t(11) = 1.40, p=0.18), and PPA/TPA (t(11) = 0.41, p=0.69). These results indicated that PIGS does not respond to all types of complex motion.</p><fig id="fig13" position="float"><label>Figure 13.</label><caption><title>The group-averaged activity map evoked by the ‘biological &gt; translational motion’ contrast.</title><p>Despite the low threshold used to generate these maps, we did not detect any significant activity evoked by the ‘biological &gt; translational motion’ contrast within the posterior intraparietal gyrus scene-selective site (PIGS) and/or the other scene-selective areas. Rather, this contrast evoked a significant activity mainly within the inferior temporal sulcus (ITS), medial temporal sulcus (MTS), and superior temporal sulcus (STS). In both hemispheres, the location of PIGS, detected based on an independent set of scene and face stimuli (as in Experiment 1), is indicated by black dashed lines. The location of other scene-selective areas is indicated by white dashed lines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91601-fig13-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>These data suggest that selective scene processing is not limited to areas PPA/TPA, RSC/MPA, and TOS/OPA, and that additional smaller scene-selective sites can also be found across the visual system. By focusing on one small scene-selective site, we showed that this site (PIGS) was consistently identifiable across individuals and groups. We also showed that inclusion of this site in the models of scene processing may clarify how ego-motion influences scene perception.</p><sec id="s3-1"><title>FMRI and all that ‘noise, noise, noise’!</title><p>The early fMRI studies dealt with a considerable amount of noise in measurements, partly due to using lower magnetic field scanners and imperfect hardware and software. This noise in measurements affected the reliability of the findings. Consequently, those early studies focused on larger activity sites that were more reliably detectable across subjects/sessions. The smaller sites were either ignored or eliminated by excessive signal smoothing, applied to enhance the level of contrast-to-noise ratio.</p><p>However, advances in neuroimaging techniques have now made it possible to detect and distinguish fMRI activity at the spatial scale of cortical columns (<xref ref-type="bibr" rid="bib68">Yacoub et al., 2007</xref>; <xref ref-type="bibr" rid="bib71">Zimmermann et al., 2011</xref>; <xref ref-type="bibr" rid="bib41">Nasr et al., 2016a</xref>). Although the reliability of the fMRI signal still depends on the number of trial repetitions, a spatially confined, but extensively repeated, evoked response can be detected reliably across different sessions (<xref ref-type="bibr" rid="bib41">Nasr et al., 2016a</xref>; <xref ref-type="bibr" rid="bib27">Kennedy et al., 2023</xref>).</p><p>The present data shows that PIGS could be localized consistently across multiple subjects and across different sessions and scanners. Furthermore, our results indicated that the probabilistic labels, generated based on one population, can be used to localize PIGS and distinguish its function from the adjacent regions (e.g., V6) in a second population. Together, these results highlight the reliability of current fMRI techniques in detecting smaller cortical regions in the level of individual subjects.</p></sec><sec id="s3-2"><title>PIGS responds selectively to a variety of scene stimuli</title><p>To establish a true category-selective response, the stimulus set should sample enough variety to reflect the range and variability among the category members. Consistent with this are the many (and continuing) studies seeking to define the range and fundamental aspects of ‘place selective’ (<xref ref-type="bibr" rid="bib14">Epstein and Kanwisher, 1998</xref>; <xref ref-type="bibr" rid="bib65">Troiani et al., 2014</xref>) and ‘face selective’ (<xref ref-type="bibr" rid="bib26">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="bib69">Yue et al., 2011</xref>) stimuli in extrastriate visual cortex, decades after their first discovery.</p><p>Accordingly, here we tested five different scene stimulus sets across our experiments, including a wide variety of indoor/outdoor and natural/manmade scenes. In all cases, we were able to evoke a selective response within PIGS, and the level of this response was comparable to that in the adjacent scene-selective areas RSC/MPA and TOS/OPA. Thus, the scene-selective response in PIGS appeared not to be limited to a single subset of scenes. However, it remains unclear whether scene stimuli are differentiable from each other based on the pattern of evoked response in this region. More experiments are necessary to test this hypothesis (see also the Limitations).</p></sec><sec id="s3-3"><title>PIGS and TOS/OPA are two different areas</title><p>Our results clearly showed that PIGS and TOS/OPA are two distinct scene-selective areas based on multiple criteria: first, anatomically, TOS/OPA is located mostly anterior to the IPS, whereas PIGS is located more dorsally and posterior to the IPS. Second, TOS/OPA overlaps with areas V3A/B and IPS0, whereas PIGS was located adjacent to IPS3-4. Third, these two areas respond distinctly to moving stimuli. Specifically, while TOS/OPA responds selectively to moving concentric rings and less selectively to ego-motion, PIGS shows the opposite pattern and responds selectively to ego-motion within the naturalistic scenes but not to moving rings (see below). Considering these anatomical and functional differences, these two areas appear to be two distinct hubs within the scene processing networks.</p><p>Also notably, PIGS is located relatively far from the lateral place memory area (LPMA), which is located anterior to the IPS and close to the tip of the superior temporal sulcus (<xref ref-type="bibr" rid="bib58">Steel et al., 2021</xref>; <xref ref-type="bibr" rid="bib59">Steel et al., 2023</xref>). Considering this, and the fact that there was no memory demand in our paradigms, PIGS and LPMA also appear to be two distinct visual areas.</p></sec><sec id="s3-4"><title>PIGS is not just another scene-selective area</title><p>Our results (Experiment 6) suggest that ego-motion can significantly influence the activity evoked within PIGS. This phenomenon distinguishes the role of PIGS in scene perception, relative to other scene-selective regions. Specifically, previous studies have shown that PPA/TPA and RSC/MPA show weak-to-no sensitivity to motion per se (<xref ref-type="bibr" rid="bib31">Korkmaz Hacialihafiz and Bartels, 2015</xref>). In comparison, area TOS/OPA shows a stronger motion-selective response, presumably related to its (partial) overlap with area V3A/B (<xref ref-type="bibr" rid="bib63">Tootell et al., 1997</xref>; <xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>). Instead, the current data show that the ego-motion-related activity within PIGS is stronger than in TOS/OPA.</p><p>This finding is consistent with the fact that PIGS is located adjacent to area V6 (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>), an area that contributes to encoding optic flow (<xref ref-type="bibr" rid="bib47">Pitzalis et al., 2010</xref>). Considering PIGS and V6 proximity, hypothetical inputs from V6 may contribute to the strong ego-motion selective response in PIGS. This said, the current data also suggests that the role of PIGS differs from that in V6 in terms of ego-motion encoding. Compared to V6, PIGS showed a stronger impact of ego-motion on scene processing, while V6 shows a stronger response to optic flow induced by random dot arrays. Thus, PIGS contributes to scene encoding and ego-motion within scenes, while V6 is likely involved in detecting optic flow caused by ego-motion.</p></sec><sec id="s3-5"><title>Ego-motion encoding in PIGS vs. TOS/OPA</title><p>We showed that PIGS and TOS/OPA are located on two different sides of the IPS with TOS/OPA located more ventrally compared to PIGS. We also showed a stronger impact of ego-motion on activity within PIGS compared to TOS/OPA. In contrast, TOS/OPA (but not PIGS) responded selectively to simpler forms of motion. These results suggest that PIGS and TOS/OPA are likely two different visual areas, with PIGS being involved in encoding higher-level ego-motion cues.</p><p>However, at least two previous studies suggested that area TOS/OPA may also contribute to ego-motion encoding in scenes. Specifically, Kamps and colleagues have shown increased response in TOS/OPA during ego-motion vs. static scene presentation (<xref ref-type="bibr" rid="bib25">Kamps et al., 2016</xref>). <xref ref-type="bibr" rid="bib24">Jones et al., 2023</xref> have also shown that ego-motion (and not other types of movements) enhances TOS/OPA activity when compared to scrambled scenes. In contrast to these findings, our tests showed weak-to-no ego-motion-related activity enhancement in area TOS/OPA.</p><p>This difference may well reflect methodological discrepancies. Specifically, in the study by Kamps et al., the static and ego-motion stimuli were presented with two different refresh rates. While in our study, the coherently and incoherently changing stimuli were refreshed with the same temporal frequency (see ‘Methods’). In the study by <xref ref-type="bibr" rid="bib24">Jones et al., 2023</xref>, the response to scrambled scenes was used as a control condition, whereas our stimuli were more equivalent, differing only in the sequence of image presentation. Moreover, these studies used higher levels of spatial smoothing (FWHM = 5 mm) compared to the values we used here during preprocessing. Also, for understandable reasons, they limited their analysis to previously known scene-selective areas. These technical differences make it difficult to directly compare the two sets of results.</p></sec><sec id="s3-6"><title>Ego-motion but not attention and/or visual context</title><p>Experiment 6 showed stronger scene-selective activity within PIGS when subjects were presented with coherently (compared to incoherently) changing scenes. It could be argued that coherently changing scenes attract more attention compared to incoherently changing scenes. On the face of it, this hypothesis appears to be consistent with the expected contribution of the intraparietal cortex in controlling spatial attention (<xref ref-type="bibr" rid="bib6">Behrmann et al., 2004</xref>; <xref ref-type="bibr" rid="bib62">Szczepanski et al., 2010</xref>). But if true, attention to scenes should also increase the level of activity within the scene-selective areas (<xref ref-type="bibr" rid="bib43">O’Craven et al., 1999</xref>; <xref ref-type="bibr" rid="bib39">Nasr and Tootell, 2012b</xref>; <xref ref-type="bibr" rid="bib3">Baldauf and Desimone, 2014</xref>). While here, we did not find any significant activity increases in response to coherently (vs. incoherently) changing scenes in PPA/TPA, RSC/MPA, and TOS/OPA. Thus, modulation of attention, per se, could not be responsible for the enhanced activity within PIGS in response to coherently (compared to incoherently) changing scenes.</p><p>Furthermore, the stimuli used in coherently vs. incoherently changing block conditions represented the same scenes, and the only difference between the two conditions was the sequence of images within the blocks. In that sense, the two experimental conditions may be considered to have the same visuospatial context. However, it could be also argued that the coherently changing scenes provide more information about the environmental layout. In that case, considering the previous reports that PPA/TPA and RSC/MPA are also involved in layout encoding (<xref ref-type="bibr" rid="bib14">Epstein and Kanwisher, 1998</xref>; <xref ref-type="bibr" rid="bib67">Wolbers et al., 2011</xref>), we expected to see more activity within those regions in response to coherently compared incoherently changing scenes. In the absence of such an activity modulation, a change in the visual context could not be responsible for the enhanced PIGS activity.</p></sec><sec id="s3-7"><title>Direction-selective response within the intraparietal cortex</title><p>Motion-selective sites are expected to show at least some level of sensitivity to motion direction (<xref ref-type="bibr" rid="bib2">Albright et al., 1984</xref>; <xref ref-type="bibr" rid="bib71">Zimmermann et al., 2011</xref>). We did not test the sensitivity of PIGS to the direction of ego-motion. However, <xref ref-type="bibr" rid="bib49">Pitzalis et al., 2020</xref> have shown evidence for motion direction encoding within the V6+ region (<xref ref-type="bibr" rid="bib49">Pitzalis et al., 2020</xref>). Furthermore, Tootell et al. reported evidence for motion direction (approaching vs. withdrawing) encoding within posterior intraparietal cortex (<xref ref-type="bibr" rid="bib64">Tootell et al., 2022</xref>). Although none of these studies showed any evidence for a new scene-selective area, they raised the possibility that PIGS may also contribute toward encoding ego-motion direction, and even higher-level cognitive concepts such as detecting an intrusion to personal space (<xref ref-type="bibr" rid="bib22">Holt et al., 2014</xref>).</p></sec><sec id="s3-8"><title>Limitations</title><p>In the past, many studies have scrutinized the response function of scene-selective areas to numerous stimulus contrasts. According to these studies, scene-selective areas can differentiate many object categories based on their low-, mid-, and/or higher-level visual features such as their natural size (<xref ref-type="bibr" rid="bib29">Konkle and Oliva, 2012</xref>), (non-)animacy (<xref ref-type="bibr" rid="bib70">Yue et al., 2020</xref>; <xref ref-type="bibr" rid="bib9">Coggan and Tong, 2023</xref>), rectilinearity (<xref ref-type="bibr" rid="bib40">Nasr et al., 2014</xref>), spatial layout (<xref ref-type="bibr" rid="bib21">Harel et al., 2013</xref>), orientation (<xref ref-type="bibr" rid="bib38">Nasr and Tootell, 2012a</xref>), spikiness (<xref ref-type="bibr" rid="bib9">Coggan and Tong, 2023</xref>), location within the visual field (<xref ref-type="bibr" rid="bib34">Levy et al., 2001</xref>), and spatial content (<xref ref-type="bibr" rid="bib4">Bar et al., 2008</xref>). Our findings are only a <italic>first</italic> step toward characterizing PIGS in greater detail. More tests are required to reach the current (yet incomplete) knowledge about the response function of PIGS.</p></sec><sec id="s3-9"><title>Conclusion</title><p>Neuroimaging studies of scene perception have typically focused on linking scene perception to the evoked activity within PPA/TPA, TOS/OPA, and RSC/MPA. Although other scene-selective sites are detectable across the visual cortex, they are largely ignored because of their relatively small size. Our data suggests that the future inclusion of these small sites in the models of scene perception may help clarify current models of scene processing in dynamic environments.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Participants</title><p>Fifty-nine human subjects (33 females), aged 22–68 y, participated in this study. All subjects had normal or corrected-to-normal vision and radiologically normal brains, without any history of neuropsychological disorder. All experimental procedures conformed to NIH guidelines and were approved by the institutional review board of the Massachusetts General Hospital (2018P001557). Written informed consent was obtained from all subjects before the experiments.</p></sec><sec id="s4-2"><title>General procedure</title><p>This study consists of seven experiments during which we used fMRI to localize and study the evoked scene-selective responses. During these experiments, stimuli were presented via a projector (1024 × 768 pixel resolution, 60 Hz refresh rate) onto a rear-projection screen. Subjects viewed the stimuli through a mirror mounted on the receive coil array. Details of these stimuli are described in the following sections.</p><p>During all experiments, to ensure that subjects were attending to the screen, they were instructed to report color changes (red to blue and vice versa) for a centrally presented fixation object (0.1° × 0.1°) by pressing a key on the keypad. Subject detection accuracy remained above 75% and showed no significant difference across experimental conditions (p&gt;0.10). MATLAB (MathWorks, Natick, MA) and the Psychophysics Toolbox (<xref ref-type="bibr" rid="bib8">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib45">Pelli, 1997</xref>) were used to control stimulus presentation.</p><sec id="s4-2-1"><title>Experiment 1: Localization of scene-selective areas</title><p>In 14 subjects (six females), we localized scene-selective areas PPA/TPA, RSC/MPA, and TOS/OPA by measuring their evoked brain activity using a 3T fMRI scanner as they were presented with eight colorful images of real-world (indoor) scenes vs. (group) faces (<xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>). Scene and face images were retinotopically centered and subtended 20° × 26° of visual field without any significant differences between their root mean square (RMS) contrast (t(14) = 1.10, p=0.29). Scene and face stimuli were presented in different blocks (16 s per block and 1 s per image). Each subject participated in four runs and each run consisted of 10 blocks plus 32 s of blank presentation at the beginning and at the end of each block. Within each run, the sequence of blocks and the sequence of images within them was randomized.</p></sec><sec id="s4-2-2"><title>Experiment 2: Reproducibility of PIGS across scan sessions (3T vs. 7T)</title><p>To localize PIGS with higher spatial resolution and enhance the signal-/contrast-to-noise ratio (relative to Experiment 1), four subjects were randomly selected from those who participated in Experiment 1 and were scanned using a 7T scanner. These individuals were presented with 300 grayscale images of scenes and 48 grayscale images of (single) faces other than those used in Experiment 1. Here, scene images included pictures of indoor (100 images), manmade outdoor (100 images) and natural outdoor (100 images) scenes, selected from the Southampton-York Natural Scenes dataset (<xref ref-type="bibr" rid="bib1">Adams et al., 2016</xref>).</p><p>As in Experiment 1, all images were retinotopically centered and subtended 20° × 26° of visual field, and there was no significant difference between the RMS contrast across the two categories (t(346) = 0.75, p=0.38). Scene and face images were presented across different blocks. Each block contained 24 stimuli (1 s per stimuli), with no blank presentation between the stimuli. The sequence of stimuli was randomized within the blocks. Each subject participated in 12 runs (11 blocks per run; 24 s per block; 1 s per stimulus), beginning and ending with an additional block (12 s) of uniform black presentation. In each run, the sequence of blocks and the sequence of images within them were randomized.</p></sec><sec id="s4-2-3"><title>Experiment 3: PIGS localization relative to area V6 and retinotopic visual areas</title><p>Experiment 3a was designed to clarify the relative localization of PIGS vs. area V6 (<xref ref-type="bibr" rid="bib47">Pitzalis et al., 2010</xref>). All 14 subjects who participated in Experiment 1 were examined again in a separate scan session using a 3T scanner. During this scan session, we localized area V6 by contrasting the response evoked by coherent radially moving (optic flow) vs. randomly moving white dots (20° × 26°), presented against a black background. The experiment was block-designed, and each block took 16 s. Each subject participated in five runs (14 blocks per run), beginning and ending with an additional block of 16 s uniform black presentation.</p><p>Experiment 3b was designed to compare the localization of PIGS relative to the border of retinotopic visual areas such as V3A/B and IPS0-4. Two subjects who had participated in Experiment 2 were randomly selected and scanned again in a 7T scanner, during which we defined the border of retinotopic visual areas using a phase encoding approach (<xref ref-type="bibr" rid="bib55">Sereno et al., 1995</xref>; <xref ref-type="bibr" rid="bib13">Engel et al., 1997</xref>). Specifically, subjects were presented with rotating (CW and CCW) wedge-shaped (45°) apertures that revolved over 28 s, followed by a 4 s blank presentation. Instead of using a flashing checkerboard, we used naturalistic stimuli consisting of color objects presented against a pink-noise background, updated at 15 Hz (<xref ref-type="bibr" rid="bib7">Benson et al., 2018</xref>). Each subject participated in 10 runs (four blocks per run).</p></sec><sec id="s4-2-4"><title>Experiment 4: Localization of PIGS in a larger population</title><p>Considering the small size of PIGS, it was important to show that this area could survive group-averaging over larger populations, compared to Experiment 1. Accordingly, Experiment 4 localized this area in a large pool of subjects, consisting of 31 individuals (19 females) other than those who participated in Experiment 1. The stimuli and procedure were identical to Experiment 1.</p></sec><sec id="s4-2-5"><title>Experiment 5: Response to two independent sets of scenes and non-scene objects</title><p>Experiments 1–4 used the response evoked by scenes vs. faces to localize PIGS. However, it remained unknown whether PIGS also showed a selective response to the ‘scenes vs. objects’ contrast. Accordingly, in two independent groups of subjects (no overlap), Experiment 5 tested the response evoked by scenes vs. non-scene objects in PIGS and the adjacent areas (i.e., V6, TOS/OPA and RSC/MPA).</p><p>Specifically, in Experiment 5a, 13 subjects (seven females), other than those who participated in Experiment 1, were scanned using a 3T scanner. They were presented with 22 grayscale images of indoor/outdoor scenes, other than those presented in Experiments 1–4, and 88 grayscale images that included either a single or multiple everyday non-animate (non-face) objects (<xref ref-type="bibr" rid="bib38">Nasr and Tootell, 2012a</xref>; <xref ref-type="bibr" rid="bib40">Nasr et al., 2014</xref>). All stimuli were retinotopically centered and presented within a circular aperture (diameter = 20°). The RMS contrast of the objects was significantly higher than the scenes (t(108) = 3.72, p&lt;10<sup>–3</sup>). Scene and object images were presented in different blocks according to their category (22 s per block and 1 s per image). Each subject participated in 12 runs and each run consisted of 9 blocks, plus 16 s of blank presentation at the beginning and the end of each block. As in other experiments, the sequence of blocks and the sequence of images within them were randomized.</p><p>In Experiment 5b, 14 subjects (eight females), other than those who participated in Experiments 1 and 5a, were scanned using a 3T scanner. Each subject was presented with 32 grayscales images of indoor/outdoor scenes, 32 images of everyday (non-face) objects plus also their scrambled versions, and 32 images of single faces (<xref ref-type="bibr" rid="bib42">Nasr and Rosas, 2016b</xref>). Scene and non-scene stimuli were different than those used in Experiments 1–4 and 5a. In contrast to Experiment 5a, all non-scene images included only one single object and there was no significant difference between the RMS contrasts of scenes and the three object categories (F(3, 111) = 0.42, p=0.74). Other details were similar to those in Experiment 5a.</p></sec><sec id="s4-2-6"><title>Experiment 6: Coherently vs. incoherently changing scenes</title><p>This experiment was designed to differentiate the role of PIGS in scene perception from TOS/OPA, RSC/MPA, and PPA/TPA. In total, 12 subjects, from the 14 subjects who participated in Experiment 1, participated in this experiment. The excluded two subjects could not participate further in our tests for personal reasons. Subjects were scanned using a 3T scanner on a different day relative to Experiments 1–3. During this scan, they were presented with rapidly ‘coherently vs. incoherently changing scenes’ (100 ms per image), across different blocks (16 s per block).</p><p>Coherently changing scenes implied ego-motion (fast walking) along three different outdoor natural trails. Stimuli (20° × 26°) were generated as one of the experimenters walked through the trails while carrying a camera mounted on his forehead, taking pictures every 2 m. Incoherently changing scenes consisted of the same images as the coherently changing blocks, but with randomized order. In other words, the only difference between the coherently vs. incoherently changing scenes was the sequence of stimuli within the block. For both coherently and incoherently changing scenes, images from different trails were presented across different blocks.</p><p>In separate blocks, subjects were also presented with 80 images that included multiple faces (20° × 26°) with the same timing as the scene images (i.e., 100 ms per image; 16 s per block). All stimuli were grayscaled. Each subject participated in six runs and each run consisted of nine blocks, plus 8 s of blank presentation at the beginning and the end of each block and 4 s of blank presentation between blocks.</p><p>On different runs (within the same session), subjects were also presented with concentric rings, extending 20° × 26° (height × width) in the visual field, presented against a light gray background (40 cd/m<sup>2</sup>). In half of the blocks (16 s per block), rings moved radially (centrifugally vs. centripetally; 4°/s) and the direction of motion changed every 4 s to reduce the impact of motion aftereffects. In the remaining half of the blocks, rings remained stationary throughout the whole block. Each subject participated in two runs and each run consisted of eight blocks, plus 16 s of uniform gray presentation at the beginning and the end of each run. The sequence of moving and stationary blocks was pseudo-randomized across runs.</p></sec><sec id="s4-2-7"><title>Experiment 7: Response to biological motion</title><p>To test whether PIGS also responds selectively to biological motion, 12 individuals were selected randomly and scanned using a 3T scanner while they were presented with the moving point-lights that represented complex biological movements such as crawling, cycling, jumping, paddling, and walking (<xref ref-type="bibr" rid="bib23">Jastorff and Orban, 2009</xref>). Each action was presented for 2 s and the sequence of actions was randomized across the blocks (20 s per block). As a control, in different blocks, the subjects were shown the same stimuli when all of the point-lights moved in the same direction (i.e., translation motion). Each subject participated in 11 runs and each run consisted of 12 blocks, plus 10 s of blank presentation at the beginning and the end of each run.</p></sec></sec><sec id="s4-3"><title>Imaging</title><sec id="s4-3-1"><title>3T scans</title><p>In Experiments 1, 3a, and 4–6, subjects were scanned using a horizontal 3T scanner (Tim Trio, Siemens Healthcare, Erlangen, Germany). Gradient echo EPI sequences were used for functional imaging. Functional data were acquired using single-shot gradient echo EPI with nominally 3.0 mm isotropic voxels (TR = 2000 ms; TE = 30 ms; flip angle = 90°; band width [BW] = 2298 Hz/pix; echo-spacing = 0.5 ms; no partial Fourier; 33 axial slices covering the entire brain; and no acceleration). During the first 3T scan (see ‘Methods’), structural (anatomical) data were acquired for each subject using a 3D T1-weighted MPRAGE sequence (TR = 2530 ms; TE = 3.39 ms; TI = 1100 ms; flip angle = 7°; BW = 200 Hz/pix; echo-spacing = 8.2 ms; voxel size = 1.0 × 1.0 × 1.33 mm).</p></sec><sec id="s4-3-2"><title>7T scans</title><p>In Experiments 2 and 3b, subjects were scanned using a 7T Siemens whole-body scanner (Siemens Healthcare) equipped with SC72 body gradients (maximum gradient strength, 70 mT/m; maximum slew rate, 200 T/m/s) using a custom-built 32-channel helmet receive coil array and a birdcage volume transmit coil. Voxel dimensions were nominally 1.0 mm, isotropic. Single-shot gradient-echo EPI was used to acquire functional images with the following protocol parameter values: TR = 3000 ms; TE = 28 ms; flip angle = 78°; BW = 1184 Hz/pix; echo-spacing = 1 ms; 7/8 phase partial Fourier; 44 oblique-coronal slices; and acceleration factor <italic>r</italic> = 4 with GRAPPA reconstruction and FLEET-ACS data (<xref ref-type="bibr" rid="bib51">Polimeni et al., 2016</xref>) with 10° flip angle. The field of view included the occipital-parietal brain areas to cover PIGS, RSC/MPA, and TOS/OPA (but not PPA/TPA).</p></sec></sec><sec id="s4-4"><title>Data analysis</title><sec id="s4-4-1"><title>Structural data analysis</title><p>For each subject, inflated and flattened cortical surfaces were reconstructed based on the high-resolution anatomical data (<xref ref-type="bibr" rid="bib10">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="bib15">Fischl et al., 1999</xref>; <xref ref-type="bibr" rid="bib16">Fischl et al., 2002</xref>), during which the standard pial surface was generated as the gray matter border with the surrounding cerebrospinal fluid or CSF (i.e., the GM–CSF interface). The white matter surface was also generated as the interface between white and gray matter (i.e., WM–GM interface). In addition, an extra surface was generated at 50% of the depth of the local gray matter (<xref ref-type="bibr" rid="bib10">Dale et al., 1999</xref>).</p></sec><sec id="s4-4-2"><title>Individual-level functional data analysis</title><p>All functional data were rigidly aligned (6 df) relative to subject’s own structural scan using rigid Boundary-Based Registration (<xref ref-type="bibr" rid="bib19">Greve and Fischl, 2009</xref>), and then motion-corrected. Data collected in the 3T (but not 7T) scanner was spatially smoothed using a 3D Gaussian kernel (2 mm FWHM). To preserve the spatial resolution, data collected within the 7T scanner was not spatially smoothed.</p><p>Subsequently, a standard hemodynamic model based on a gamma function was fit to the fMRI signal to estimate the amplitude of the BOLD response. For each individual subject, the average BOLD response maps were calculated for each condition (<xref ref-type="bibr" rid="bib18">Friston et al., 1999</xref>). Finally, voxel-wise statistical tests were conducted by computing contrasts based on a univariate general linear model.</p><p>The resultant significance maps based on 3T scans were sampled from the middle of cortical gray matter defined for each subject based on their structural scan (see ‘Methods’). For 7T scans, the resultant significance maps were sampled from deep cortical layers at the gray–white matter interface. This procedure reduced the spatial blurring caused by superficial veins (<xref ref-type="bibr" rid="bib30">Koopmans et al., 2010</xref>; <xref ref-type="bibr" rid="bib50">Polimeni et al., 2010</xref>; <xref ref-type="bibr" rid="bib11">De Martino et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Nasr et al., 2016a</xref>). For presentation, the resultant maps were projected either onto the subject’s reconstructed cortical surfaces or onto a common template (fsaverage; FreeSurfer; <xref ref-type="bibr" rid="bib17">Fischl, 2012</xref>).</p></sec><sec id="s4-4-3"><title>Group-level functional data analysis</title><p>To generate group-averaged maps, functional maps were spatially normalized across subjects, then averaged using weighted least square random-effects models (using the contrast effect size and the variance of contrast effect size as the input parameters) and corrected for multiple comparisons (<xref ref-type="bibr" rid="bib18">Friston et al., 1999</xref>). For <xref ref-type="fig" rid="fig1">Figure 1A</xref> and to replicate our original finding (<xref ref-type="bibr" rid="bib37">Nasr et al., 2011</xref>), the group-average maps were generated using fixed-effects. The resultant significance maps were projected onto a common human brain template (fsaverage).</p></sec><sec id="s4-4-4"><title>ROI analysis</title><p>The main ROIs included area PIGS, the two neighboring scene-selective areas (RSC/MPA, TOS/OPA), and area V6. In Experiment 6, we also included area PPA/TPA in our analysis. These ROIs were localized in two different ways: (1) functionally, for each subject based on their own evoked activity (see below), and (2) probabilistically, based on activity measured in a different group of subjects.</p></sec><sec id="s4-4-5"><title>Functionally localized ROIs</title><p>For those subjects who participated in Experiments 6 and 7, we localized scene-selective areas PIGS, TOS/OPA, RSC/MPA, and PPA/TPA based on their stronger response to scenes compared to faces at a threshold level of p&lt;10<sup>–2</sup> using the method described in Experiment 1. For subjects in Experiment 6, we also localized area V6 based on the expected selective response in this region to coherent radially vs. incoherently moving random dots (see Experiment 3). In those subjects in which PIGS and V6 showed partial overlap, the overlapping parts were excluded for the analysis.</p></sec><sec id="s4-4-6"><title>Probabilistically localized ROIs</title><p>For those subjects who participated in Experiments 4 and 5, we tested the consistency of PIGS locations across populations using probabilistic labels for areas PIGS, TOS/OPA, RSC/MPA, and V6. These labels were generated based on the results of Experiment 1 (for PIGS, TOS/OPA, and RSC/MPA) and Experiment 3a (for V6). Specifically, we localized the ROIs separately for the individual subjects who participated in Experiments 1 and 3a. Then the labels were overlaid on a common brain template (fsaverage). We computed the probability that each vortex within the cortical surface belonged to one of the ROIs. The labels for PIGS, TOS/OPA, RSC/MPA, and V6 were generated based on those vertices that showed a probability higher than 20%. This method assured us that our measurements were not biased by those subjects who showed stronger scene-selective responses. Moreover, by selecting a relatively low threshold (i.e., 20%), we avoided confining our ROIs to the center of activity sites.</p></sec><sec id="s4-4-7"><title>Statistical tests</title><p>To test the effect of independent parameters, we applied paired <italic>t</italic>-tests and/or a repeated-measures ANOVA, with Greenhouse–Geisser correction whenever the sphericity assumption was violated.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Formal analysis, Visualization, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All experimental procedures conformed to NIH guidelines and were approved by Massachusetts General Hospital protocols. Written informed consent was obtained from all subjects before the experiments. (2018P001557).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91601-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The stimuli, significance maps, generated probabilistic labels, and the measured activity across ROIs, as presented in Figures 1-13, are already available to the public (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.xwdbrv1mj">https://doi.org/10.5061/dryad.xwdbrv1mj</ext-link>; see also <ext-link ext-link-type="uri" xlink:href="https://mesovision.martinos.org/datasets">https://mesovision.martinos.org/datasets</ext-link>). MATLAB (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001622">SCR_001622</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com">https://www.mathworks.com</ext-link>); FreeSurfer (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001847">SCR_001847</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/FsFast">https://surfer.nmr.mgh.harvard.edu/fswiki/FsFast</ext-link>); and Psychophysics Toolbox (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002881">SCR_002881</ext-link>; <ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/docs/Psychtoolbox">http://psychtoolbox.org/docs/Psychtoolbox</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>A previously undescribed scene-selective site is the key to encoding ego-motion in naturalistic environments</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.xwdbrv1mj</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by NIH NEI (grants R01 EY017081 and R01 EY030434) and the MGH/HST Athinoula A Martinos Center for Biomedical Imaging. Crucial resources were made available by a NIH Shared Instrumentation Grant S10-RR019371. We thank Ms. Azma Mareyam for help with hardware maintenance during this study. We also thank Dr. Claudio Galletti for his helpful comments.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>WJ</given-names></name><name><surname>Elder</surname><given-names>JH</given-names></name><name><surname>Graf</surname><given-names>EW</given-names></name><name><surname>Leyland</surname><given-names>J</given-names></name><name><surname>Lugtigheid</surname><given-names>AJ</given-names></name><name><surname>Muryy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The southampton-york natural scenes (syns) dataset: Statistics of surface attitude</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>35805</elocation-id><pub-id pub-id-type="doi">10.1038/srep35805</pub-id><pub-id pub-id-type="pmid">27782103</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albright</surname><given-names>TD</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Columnar organization of directionally selective cells in visual area MT of the macaque</article-title><source>Journal of Neurophysiology</source><volume>51</volume><fpage>16</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1152/jn.1984.51.1.16</pub-id><pub-id pub-id-type="pmid">6693933</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldauf</surname><given-names>D</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of object-based attention</article-title><source>Science</source><volume>344</volume><fpage>424</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1126/science.1247003</pub-id><pub-id pub-id-type="pmid">24763592</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name><name><surname>Aminoff</surname><given-names>E</given-names></name><name><surname>Schacter</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Scenes unseen: the parahippocampal cortex intrinsically subserves contextual associations, not scenes or places per se</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>8539</fpage><lpage>8544</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0987-08.2008</pub-id><pub-id pub-id-type="pmid">18716212</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Lee</surname><given-names>KE</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>FMRI responses to video and point-light displays of moving humans and manipulable objects</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>991</fpage><lpage>1001</lpage><pub-id pub-id-type="doi">10.1162/089892903770007380</pub-id><pub-id pub-id-type="pmid">14614810</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrmann</surname><given-names>M</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name><name><surname>Shomstein</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Parietal cortex and attention</article-title><source>Current Opinion in Neurobiology</source><volume>14</volume><fpage>212</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2004.03.012</pub-id><pub-id pub-id-type="pmid">15082327</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Jamison</surname><given-names>KW</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The human connectome project 7 tesla retinotopy dataset: Description and population receptive field analysis</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1167/18.13.23</pub-id><pub-id pub-id-type="pmid">30593068</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coggan</surname><given-names>DD</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Spikiness and animacy as potential organizing principles of human ventral visual cortex</article-title><source>Cerebral Cortex</source><volume>33</volume><fpage>8194</fpage><lpage>8217</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhad108</pub-id><pub-id pub-id-type="pmid">36958809</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cortical depth dependent functional responses in humans at 7T: improved specificity with 3D GRASE</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e60514</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0060514</pub-id><pub-id pub-id-type="pmid">23533682</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Julian</surname><given-names>JB</given-names></name><name><surname>Paunov</surname><given-names>AM</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The occipital place area is causally and selectively involved in scene perception</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>1331</fpage><lpage>6a</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4081-12.2013</pub-id><pub-id pub-id-type="pmid">23345209</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>SA</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Retinotopic organization in human visual cortex and the spatial precision of functional MRI</article-title><source>Cerebral Cortex</source><volume>7</volume><fpage>181</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1093/cercor/7.2.181</pub-id><pub-id pub-id-type="pmid">9087826</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A cortical representation of the local visual environment</article-title><source>Nature</source><volume>392</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/33402</pub-id><pub-id pub-id-type="pmid">9560155</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis II: inflation, flattening, and a surface-based coordinate system</article-title><source>NeuroImage</source><volume>9</volume><fpage>195</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0396</pub-id><pub-id pub-id-type="pmid">9931269</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Salat</surname><given-names>DH</given-names></name><name><surname>Busa</surname><given-names>E</given-names></name><name><surname>Albert</surname><given-names>M</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Haselgrove</surname><given-names>C</given-names></name><name><surname>van der Kouwe</surname><given-names>A</given-names></name><name><surname>Killiany</surname><given-names>R</given-names></name><name><surname>Kennedy</surname><given-names>D</given-names></name><name><surname>Klaveness</surname><given-names>S</given-names></name><name><surname>Montillo</surname><given-names>A</given-names></name><name><surname>Makris</surname><given-names>N</given-names></name><name><surname>Rosen</surname><given-names>B</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Whole brain segmentation: automated labeling of neuroanatomical structures in the human brain</article-title><source>Neuron</source><volume>33</volume><fpage>341</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00569-x</pub-id><pub-id pub-id-type="pmid">11832223</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name><name><surname>Worsley</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Multisubject fMRI studies and conjunction analyses</article-title><source>NeuroImage</source><volume>10</volume><fpage>385</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1006/nimg.1999.0484</pub-id><pub-id pub-id-type="pmid">10493897</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id><pub-id pub-id-type="pmid">19573611</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The neural basis of object perception</article-title><source>Current Opinion in Neurobiology</source><volume>13</volume><fpage>159</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(03)00040-0</pub-id><pub-id pub-id-type="pmid">12744968</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harel</surname><given-names>A</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Deconstructing visual scenes in cortex: gradients of object and spatial layout information</article-title><source>Cerebral Cortex</source><volume>23</volume><fpage>947</fpage><lpage>957</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs091</pub-id><pub-id pub-id-type="pmid">22473894</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holt</surname><given-names>DJ</given-names></name><name><surname>Cassidy</surname><given-names>BS</given-names></name><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>Rauch</surname><given-names>SL</given-names></name><name><surname>Boeke</surname><given-names>EA</given-names></name><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name><name><surname>Coombs</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural correlates of personal space intrusion</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4123</fpage><lpage>4134</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0686-13.2014</pub-id><pub-id pub-id-type="pmid">24647934</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jastorff</surname><given-names>J</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Human functional magnetic resonance imaging reveals separation and integration of shape and motion cues in biological motion processing</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>7315</fpage><lpage>7329</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4870-08.2009</pub-id><pub-id pub-id-type="pmid">19494153</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>CM</given-names></name><name><surname>Byland</surname><given-names>J</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The occipital place area represents visual information about walking, not crawling</article-title><source>Cerebral Cortex</source><volume>33</volume><fpage>7500</fpage><lpage>7505</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhad055</pub-id><pub-id pub-id-type="pmid">36918999</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamps</surname><given-names>FS</given-names></name><name><surname>Lall</surname><given-names>V</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The occipital place area represents first-person perspective motion information through scenes</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>83</volume><fpage>17</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2016.06.022</pub-id><pub-id pub-id-type="pmid">27474914</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennedy</surname><given-names>B</given-names></name><name><surname>Bex</surname><given-names>P</given-names></name><name><surname>Hunter</surname><given-names>DG</given-names></name><name><surname>Nasr</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Two fine-scale channels for encoding motion and stereopsis within the human magnocellular stream</article-title><source>Progress in Neurobiology</source><volume>220</volume><elocation-id>102374</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2022.102374</pub-id><pub-id pub-id-type="pmid">36403864</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konen</surname><given-names>CS</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Two hierarchically organized neural systems for object information in human visual cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>224</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1038/nn2036</pub-id><pub-id pub-id-type="pmid">18193041</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A real-world size organization of object responses in occipitotemporal cortex</article-title><source>Neuron</source><volume>74</volume><fpage>1114</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.036</pub-id><pub-id pub-id-type="pmid">22726840</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koopmans</surname><given-names>PJ</given-names></name><name><surname>Barth</surname><given-names>M</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Layer-specific BOLD activation in human V1</article-title><source>Human Brain Mapping</source><volume>31</volume><fpage>1297</fpage><lpage>1304</lpage><pub-id pub-id-type="doi">10.1002/hbm.20936</pub-id><pub-id pub-id-type="pmid">20082333</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korkmaz Hacialihafiz</surname><given-names>D</given-names></name><name><surname>Bartels</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Motion responses in scene-selective regions</article-title><source>NeuroImage</source><volume>118</volume><fpage>438</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.031</pub-id><pub-id pub-id-type="pmid">26091852</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Cheng</surname><given-names>X</given-names></name><name><surname>Ohayon</surname><given-names>S</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A network for scene processing in the macaque temporal lobe</article-title><source>Neuron</source><volume>79</volume><fpage>766</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.06.015</pub-id><pub-id pub-id-type="pmid">23891401</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lescroart</surname><given-names>MD</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human scene-selective areas represent 3d configurations of surfaces</article-title><source>Neuron</source><volume>101</volume><fpage>178</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.11.004</pub-id><pub-id pub-id-type="pmid">30497771</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Avidan</surname><given-names>G</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Center-periphery organization of human object areas</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>533</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1038/87490</pub-id><pub-id pub-id-type="pmid">11319563</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Zhu</surname><given-names>Q</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Submillimeter fMRI reveals an extensive, fine-grained and functionally-relevant scene-processing network in monkeys</article-title><source>Progress in Neurobiology</source><volume>211</volume><elocation-id>102230</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2022.102230</pub-id><pub-id pub-id-type="pmid">35101543</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The retrosplenial contribution to human navigation: a review of lesion and neuroimaging findings</article-title><source>Scandinavian Journal of Psychology</source><volume>42</volume><fpage>225</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1111/1467-9450.00233</pub-id><pub-id pub-id-type="pmid">11501737</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>N</given-names></name><name><surname>Devaney</surname><given-names>KJ</given-names></name><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>Rajimehr</surname><given-names>R</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scene-selective cortical regions in human and nonhuman primates</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>13771</fpage><lpage>13785</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2792-11.2011</pub-id><pub-id pub-id-type="pmid">21957240</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>A cardinal orientation bias in scene-selective visual cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>14921</fpage><lpage>14926</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2036-12.2012</pub-id><pub-id pub-id-type="pmid">23100415</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Role of fusiform and anterior temporal cortical areas in facial recognition</article-title><source>NeuroImage</source><volume>63</volume><fpage>1743</fpage><lpage>1753</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.08.031</pub-id><pub-id pub-id-type="pmid">23034518</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Echavarria</surname><given-names>CE</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Thinking outside the box: rectilinear shapes selectively activate scene-selective cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>6721</fpage><lpage>6735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4802-13.2014</pub-id><pub-id pub-id-type="pmid">24828628</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Interdigitated color- and disparity-selective columns within human visual cortical areas v2 and v3</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1841</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3518-15.2016</pub-id><pub-id pub-id-type="pmid">26865609</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Rosas</surname><given-names>HD</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Impact of visual corticostriatal loop disruption on neural processing within the parahippocampal place area</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>10456</fpage><lpage>10471</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0741-16.2016</pub-id><pub-id pub-id-type="pmid">27707978</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Craven</surname><given-names>KM</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>fMRI evidence for objects as the units of attentional selection</article-title><source>Nature</source><volume>401</volume><fpage>584</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1038/44134</pub-id><pub-id pub-id-type="pmid">10524624</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Josephs</surname><given-names>E</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Ramp-shaped neural tuning supports graded population-level representation of the object-to-scene continuum</article-title><source>Scientific Reports</source><volume>12</volume><elocation-id>18081</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-21768-2</pub-id><pub-id pub-id-type="pmid">36302932</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The videotoolbox software for visual psychophysics: Transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelphrey</surname><given-names>KA</given-names></name><name><surname>Morris</surname><given-names>JP</given-names></name><name><surname>Michelich</surname><given-names>CR</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional anatomy of biological motion perception in posterior temporal cortex: an FMRI study of eye, mouth and hand movements</article-title><source>Cerebral Cortex</source><volume>15</volume><fpage>1866</fpage><lpage>1876</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhi064</pub-id><pub-id pub-id-type="pmid">15746001</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitzalis</surname><given-names>S</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Committeri</surname><given-names>G</given-names></name><name><surname>Fattori</surname><given-names>P</given-names></name><name><surname>Galati</surname><given-names>G</given-names></name><name><surname>Patria</surname><given-names>F</given-names></name><name><surname>Galletti</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Human v6: the medial motion area</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>411</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp112</pub-id><pub-id pub-id-type="pmid">19502476</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitzalis</surname><given-names>S</given-names></name><name><surname>Fattori</surname><given-names>P</given-names></name><name><surname>Galletti</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The human cortical areas V6 and V6A</article-title><source>Visual Neuroscience</source><volume>32</volume><elocation-id>E007</elocation-id><pub-id pub-id-type="doi">10.1017/S0952523815000048</pub-id><pub-id pub-id-type="pmid">26241369</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitzalis</surname><given-names>S</given-names></name><name><surname>Serra</surname><given-names>C</given-names></name><name><surname>Sulpizio</surname><given-names>V</given-names></name><name><surname>Committeri</surname><given-names>G</given-names></name><name><surname>de Pasquale</surname><given-names>F</given-names></name><name><surname>Fattori</surname><given-names>P</given-names></name><name><surname>Galletti</surname><given-names>C</given-names></name><name><surname>Sepe</surname><given-names>R</given-names></name><name><surname>Galati</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural bases of self- and object-motion in a naturalistic vision</article-title><source>Human Brain Mapping</source><volume>41</volume><fpage>1084</fpage><lpage>1111</lpage><pub-id pub-id-type="doi">10.1002/hbm.24862</pub-id><pub-id pub-id-type="pmid">31713304</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Laminar analysis of 7T BOLD using an imposed spatial activation pattern in human V1</article-title><source>NeuroImage</source><volume>52</volume><fpage>1334</fpage><lpage>1346</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.005</pub-id><pub-id pub-id-type="pmid">20460157</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Bhat</surname><given-names>H</given-names></name><name><surname>Witzel</surname><given-names>T</given-names></name><name><surname>Benner</surname><given-names>T</given-names></name><name><surname>Feiweier</surname><given-names>T</given-names></name><name><surname>Inati</surname><given-names>SJ</given-names></name><name><surname>Renvall</surname><given-names>V</given-names></name><name><surname>Heberlein</surname><given-names>K</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reducing sensitivity losses due to respiration and motion in accelerated echo planar imaging by reordering the autocalibration data acquisition</article-title><source>Magnetic Resonance in Medicine</source><volume>75</volume><fpage>665</fpage><lpage>679</lpage><pub-id pub-id-type="doi">10.1002/mrm.25628</pub-id><pub-id pub-id-type="pmid">25809559</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>Bentin</surname><given-names>S</given-names></name><name><surname>Gore</surname><given-names>JC</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Temporal cortex activation in humans viewing eye and mouth movements</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>2188</fpage><lpage>2199</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-06-02188.1998</pub-id><pub-id pub-id-type="pmid">9482803</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Perrett</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Electrophysiology and brain imaging of biological motion</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>358</volume><fpage>435</fpage><lpage>445</lpage><pub-id pub-id-type="doi">10.1098/rstb.2002.1221</pub-id><pub-id pub-id-type="pmid">12689371</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajimehr</surname><given-names>R</given-names></name><name><surname>Young</surname><given-names>JC</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>An anterior temporal face patch in human cortex, predicted by macaque maps</article-title><source>PNAS</source><volume>106</volume><fpage>1995</fpage><lpage>2000</lpage><pub-id pub-id-type="doi">10.1073/pnas.0807304106</pub-id><pub-id pub-id-type="pmid">19179278</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title><source>Science</source><volume>268</volume><fpage>889</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1126/science.7754376</pub-id><pub-id pub-id-type="pmid">7754376</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silson</surname><given-names>EH</given-names></name><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evaluating the correspondence between face-, scene-, and object-selectivity and retinotopic organization within lateral occipitotemporal cortex</article-title><source>Journal of Vision</source><volume>16</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/16.6.14</pub-id><pub-id pub-id-type="pmid">27105060</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>MA</given-names></name><name><surname>Ress</surname><given-names>D</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Topographic maps of visual spatial attention in human parietal cortex</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>1358</fpage><lpage>1371</lpage><pub-id pub-id-type="doi">10.1152/jn.01316.2004</pub-id><pub-id pub-id-type="pmid">15817643</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steel</surname><given-names>A</given-names></name><name><surname>Billings</surname><given-names>MM</given-names></name><name><surname>Silson</surname><given-names>EH</given-names></name><name><surname>Robertson</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A network linking scene perception and spatial memory systems in posterior cerebral cortex</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>2632</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22848-z</pub-id><pub-id pub-id-type="pmid">33976141</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steel</surname><given-names>A</given-names></name><name><surname>Garcia</surname><given-names>BD</given-names></name><name><surname>Goyal</surname><given-names>K</given-names></name><name><surname>Mynick</surname><given-names>A</given-names></name><name><surname>Robertson</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Scene perception and visuospatial memory converge at the anterior edge of visually responsive cortex</article-title><source>The Journal of Neuroscience</source><volume>43</volume><fpage>5723</fpage><lpage>5737</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2043-22.2023</pub-id><pub-id pub-id-type="pmid">37474310</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sulpizio</surname><given-names>V</given-names></name><name><surname>Galati</surname><given-names>G</given-names></name><name><surname>Fattori</surname><given-names>P</given-names></name><name><surname>Galletti</surname><given-names>C</given-names></name><name><surname>Pitzalis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A common neural substrate for processing scenes and egomotion-compatible visual motion</article-title><source>Brain Structure &amp; Function</source><volume>225</volume><fpage>2091</fpage><lpage>2110</lpage><pub-id pub-id-type="doi">10.1007/s00429-020-02112-8</pub-id><pub-id pub-id-type="pmid">32647918</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swisher</surname><given-names>JD</given-names></name><name><surname>Halko</surname><given-names>MA</given-names></name><name><surname>Merabet</surname><given-names>LB</given-names></name><name><surname>McMains</surname><given-names>SA</given-names></name><name><surname>Somers</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visual topography of human intraparietal sulcus</article-title><source>The Journal of Neuroscience</source><volume>27</volume><fpage>5326</fpage><lpage>5337</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0991-07.2007</pub-id><pub-id pub-id-type="pmid">17507555</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szczepanski</surname><given-names>SM</given-names></name><name><surname>Konen</surname><given-names>CS</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Mechanisms of spatial attention control in frontal and parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>148</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3862-09.2010</pub-id><pub-id pub-id-type="pmid">20053897</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tootell</surname><given-names>RB</given-names></name><name><surname>Mendola</surname><given-names>JD</given-names></name><name><surname>Hadjikhani</surname><given-names>NK</given-names></name><name><surname>Ledden</surname><given-names>PJ</given-names></name><name><surname>Liu</surname><given-names>AK</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Functional analysis of V3A and related areas in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>7060</fpage><lpage>7078</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-18-07060.1997</pub-id><pub-id pub-id-type="pmid">9278542</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tootell</surname><given-names>RBH</given-names></name><name><surname>Nasiriavanaki</surname><given-names>Z</given-names></name><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Holt</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Interdigitated columnar representation of personal space and visual space in human parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>9011</fpage><lpage>9029</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0516-22.2022</pub-id><pub-id pub-id-type="pmid">36198501</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troiani</surname><given-names>V</given-names></name><name><surname>Stigliani</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>ME</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multiple object properties drive scene-selective regions</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>883</fpage><lpage>897</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs364</pub-id><pub-id pub-id-type="pmid">23211209</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Comparing face patch systems in macaques and humans</article-title><source>PNAS</source><volume>105</volume><fpage>19514</fpage><lpage>19519</lpage><pub-id pub-id-type="doi">10.1073/pnas.0809662105</pub-id><pub-id pub-id-type="pmid">19033466</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolbers</surname><given-names>T</given-names></name><name><surname>Klatzky</surname><given-names>RL</given-names></name><name><surname>Loomis</surname><given-names>JM</given-names></name><name><surname>Wutte</surname><given-names>MG</given-names></name><name><surname>Giudice</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Modality-independent coding of spatial layout in the human brain</article-title><source>Current Biology</source><volume>21</volume><fpage>984</fpage><lpage>989</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.04.038</pub-id><pub-id pub-id-type="pmid">21620708</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Shmuel</surname><given-names>A</given-names></name><name><surname>Logothetis</surname><given-names>N</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Robust detection of ocular dominance columns in humans using hahn spin echo bold functional mri at 7 tesla</article-title><source>NeuroImage</source><volume>37</volume><fpage>1161</fpage><lpage>1177</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.05.020</pub-id><pub-id pub-id-type="pmid">17702606</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>Cassidy</surname><given-names>BS</given-names></name><name><surname>Devaney</surname><given-names>KJ</given-names></name><name><surname>Holt</surname><given-names>DJ</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Lower-level stimulus features strongly influence responses in the fusiform face area</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>35</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq050</pub-id><pub-id pub-id-type="pmid">20375074</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>X</given-names></name><name><surname>Robert</surname><given-names>S</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Curvature processing in human visual cortical areas</article-title><source>NeuroImage</source><volume>222</volume><elocation-id>117295</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117295</pub-id><pub-id pub-id-type="pmid">32835823</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>van de Moortele</surname><given-names>PF</given-names></name><name><surname>Feinberg</surname><given-names>D</given-names></name><name><surname>Adriany</surname><given-names>G</given-names></name><name><surname>Chaimow</surname><given-names>D</given-names></name><name><surname>Shmuel</surname><given-names>A</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mapping the organization of axis of motion selective features in human area MT using high-field fMRI</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e28716</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0028716</pub-id><pub-id pub-id-type="pmid">22163328</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91601.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schlichting</surname><given-names>Margaret L</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03dbr7087</institution-id><institution>University of Toronto</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>In this article, the authors present a wealth of fMRI data at both 3T and 7T to identify a scene-selective region of the intraparietal gyrus (‘PIGS’) that appears to have some responsivity to characteristics of ego-motion. In a series of experiments, they delineate the anatomical location of PIGS and functionally differentiate it from nearby V6 and OPA. Evidence for these <bold>important</bold> findings is <bold>solid</bold>, but further investigations as to the role of this region in processing ego-motion will be needed to confirm this conclusion.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91601.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary</p><p>The authors report an extensive series of neuroimaging experiments (at both 3T and 7T) to provide evidence for a scene-selective visual area in human posterior parietal cortex (PIGS) that is distinct from the main three (parahippocampal place area, PPA; occipital place area, OPA; medial place area, MPA) typically reported in the literature. Further, they argue that in comparison with the other three, this region may specifically be involved in representing ego-motion in natural contexts. The characterization of this scene-selective region provides a useful reference point for studies of scene processing in humans.</p><p>Strengths</p><p>One of the major strengths of the work is the extensive series of experiments reported, showing clear reproducibility of the main finding and providing functional insight into the region studied. The results are clearly presented and convincing with careful comparison to retinotopic and scene-selective regions described in prior studies.</p><p>Weaknesses</p><p>While the results are strong and clear, the claim in the title (&quot;A previously undescribed scene-selective site is the key to encoding ego-motion in naturalistic environments&quot;) is not fully supported. The results show that this scene-selective region is sensitive to visual cues that reflect ego-motion but not that it is &quot;key&quot; to encoding ego-motion. Further, there are many differences between the two types of stimuli used to test ego-motion and greater characterization of this scene-selective region will be needed to confirm this conclusion.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91601.3.sa2</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors report a scene-selective areas in the posterior intraparietal gyrus (PIGS). This area lies outside the classical three scene-selective regions (PPA/TPA, RSC/MPA, TOS/OPA), and is selective for ego motion.</p><p>Strengths:</p><p>The authors firmly establish the location and selectivity of the new area through a series of well-crafted controlled experiments. They show that the area can be missed with too much smoothing, thus providing a case for why it has not been previously described. They show that it appears in much the same location in different subjects, with different magnetic field strengths, and with different stimulus sets. Finally, they show that it is selective for ego motion - defined as series of sequential photographs of an egocentric trajectory along a path. They further clarify that the area is not generically motion selective by showing that it does not respond to biological motion without an egomotion component to it. All statistics are standard and sound; the evidence presented is strong.</p><p>Weaknesses:</p><p>There are a few weaknesses in this work. If pressed, I might say that the stimuli depicting ego motion do not, strictly speaking, depict motion, but only apparent motion between 2s apart photographs. However, this choice was made to equate frame rates and motion contrast between the 'ego motion' and a control condition, which is a useful and valid approach to the problem.</p><p>This is a very strong paper.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91601.3.sa3</article-id><title-group><article-title>Author Response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kennedy</surname><given-names>Bryan</given-names></name><role specific-use="author">Author</role><aff><institution>Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Malladi</surname><given-names>Sarala N</given-names></name><role specific-use="author">Author</role><aff><institution>Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Tootell</surname><given-names>Roger BH</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/002pd6e78</institution-id><institution>Massachusetts General Hospital</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Nasr</surname><given-names>Shahin</given-names></name><role specific-use="author">Author</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/002pd6e78</institution-id><institution>Massachusetts General Hospital</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 1:</bold></p><p>Comment 1.1: The distinction of PIGS from nearby OPA, which has also been implied in navigation and ego-motion, is not as clear as it could be.</p></disp-quote><p>Response1.1: The main “functional” distinction between TOS/OPA and PIGS is that TOS/OPA responds preferentially to moving vs. stationary stimuli (even concentric rings), likely due to its overlap with the retinotopic motion-selective visual area V3A, for which this is a defining functional property (e.g. Tootell et al., 1997, J Neurosci). In comparison, PIGS does not show such a motion-selectivity. Instead, PIGS responds preferentially to more complex forms of motion within scenes.</p><p>Moreover, PIGS and TOS/OPA are located in differently relative to the retinotopic visual areas. Briefly, PIGS is located adjacent to areas IPS3-4 while TOS/OPA overlaps with areas V3A/B and IPS0 (V7). This point is now highlighted in the new experiment 3b and the new Figure 6.In this revision, we also tried to better highlight these point in sections 4.3, 4.4 and 4.5. (see also the response to the first comment from Reviewer #2).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 2:</bold></p><p>Comment 2.1: First, the scene-selective region identified appears to overlap with regions that have previously been identified in terms of their retinotopic properties. In particular, it is unclear whether this region overlaps with V7/IPS0 and/or IPS1. This is particularly important since prior work has shown that OPA often overlaps with v7/IPS0 (Silson et al, 2016, Journal of Vision). The findings would be much stronger if the authors could show how the location of PIGS relates to retinotopic areas (other than V6, which they do currently consider). I wonder if the authors have retinotopic mapping data for any of the participants included in this study. If not, the authors could always show atlas-based definitions of these areas (e.g. Wang et al, 2015, Cerebral Cortex).</p></disp-quote><p>Response 2.1: We thank the reviewers for reminding us to more clearly delineate this issue of possible overlap, including the information provided by Silson et al, 2016. The issue of possible overlap between area TOS/OPA and the retinotopic visual areas, both in humans and non-human primates, was also clarified by our team in 2011 (Nasr et al., 2011). As you can see in Figure 6 (newly generated), and consistent with those previous studies, TOS/OPA overlaps with visual areas V3A/B and V7. Whereas PIGS is located more dorsally close to IPS3-4. As shown here, there is no overlap between PIGS and TOS/OPA and there is no overlap between PIGS and areas V3A/B and V7.</p><p>To more directly address the reviewer’s concern, in this revision, we have added a new experiment (Experiment 3b) in which we have shown the relative position of PIGS and the retinotopic areas in two individual subjects (Figure 6). All the relevant points are also discussed in section 4.3.</p><disp-quote content-type="editor-comment"><p>Comment 2.2: Second, recent studies have reported a region anterior to OPA that seems to be involved in scene memory (Steel et al, 2021, Nature Communications; Steel et al, 2023, The Journal of Neuroscience; Steel et al, 2023, biorXiv). Is this region distinct from PIGS? Based on the figures in those papers, the scene memory-related region is inferior to V7/IPS0, so characterizing the location of PIGS to V7/IPS0 as suggested above would be very helpful here as well. If PIGS overlaps with either of V7/IPS0 or the scene memory-related area described by Steel and colleagues, then arguably it is not a newly defined region (although the characterization provided here still provides new information).</p></disp-quote><p>Response 2.2: The lateral-place memory area (LPMA) is located on the lateral brain surface, anterior relative to the IPS (see Figure 1 from Steel et al., 2021 and Figure 3 from Steel et al., 2023). In contrast, PIGS is located on the posterior brain surface, also posterior relative to the IPS. In other words, they are located on two different sides of a major brain sulcus. In this revision we have clarified this point, including the citations by Steel and colleagues in section 4.3.</p><disp-quote content-type="editor-comment"><p>Comments 2.3: Another reason that it would be helpful to relate PIGS to this scene memory area is that this scene memory area has been shown to have activity related to the amount of visuospatial context (Steel et al, 2023, The Journal of Neuroscience). The conditions used to show the sensitivity of PIGS to ego-motion also differ in the visuospatial context that can be accessed from the stimuli. Even if PIGS appears distinct from the scene memory area, the degree of visuospatial context is an alternative account of what might be represented in PIGS.</p></disp-quote><p>Response 2.3: The reviewer raises an interesting point. One minor confusion is that we may be inadvertently referring to two slightly different types of “visuospatial context”. Specifically, the stimuli used in the ego-motion experiment here (i.e. coherently vs. incoherently changing scenes) represent the same scenes, and the only difference between the two conditions is the sequence of images across the experimental blocks. In that sense, the two experimental conditions may be considered to have the same visuospatial “context”. However, it could be also argued that the coherently changing scenes provide more information about the environmental layout. In that case, considering the previous reports that PPA/TPA and RSC/MPA may also be involved in layout encoding (Epstein and Kanwisher 1998; Wolbers et al. 2011), we expected to see more activity within those regions in response to coherently compared incoherently changing scenes. These issues are now more explicitly discussed in the revised article (section 4.6).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer 3:</bold></p><p>Comment 3.1: There are few weaknesses in this work. If pressed, I might say that the stimuli depicting ego-motion do not, strictly speaking, depict motion, but only apparent motion between 2s apart photographs. However, this choice was made to equate frame rates and motion contrast between the 'ego-motion' and a control condition, which is a useful and valid approach to the problem. Some choices for visualization of the results might be made differently; for example, outlines of the regions might be shown in more plots for easier comparison of activation locations, but this is a minor issue.</p></disp-quote><p>Response 3.1: We thank the reviewer for these constructive suggestions, and we agree with their comment that the ego-motion stimuli are not smooth, even though they were refreshed every 100 ms. However, the stimuli were nevertheless coherent enough to activate areas V6 and MT, two major areas known to respond preferentially to coherent compared to incoherent motion.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>I enjoyed reading this article. I have a few suggestions for improvement:</p><p>(1) Delineation from OPA: The OPA has been described in quite similar terms as PIGS, with its involvement in ego-motion (e.g., crawling, walking) and navigation in general (e.g., Dilks' recent work; Bonner and Epstein). The authors address the distinction in section 4.4. Unlike Kamps et al. (2016) and Jones et al. (2023), the authors found weak or no evidence for ego-motion in OPA. They explain this discrepancy with differences in refresh rates and different levels of spatial smoothing of the fMRI data. It is not clear why these fairly small methodological differences would lead to different findings of ego-motion in the OPA. Arguably, the OPA is the closest of the &quot;established&quot; scene areas to PIGS, both in anatomical location and in function. I would therefore appreciate a more detailed discussion of the differences between these two areas.</p></disp-quote><p>Response: Jones et al. have also shown that ego-motion TOS/OPA activity when compared to scrambled scenes. This is fundamentally different than what we have shown here, which coherently vs. incoherently changing scenes (i.e. not a small difference). Also, Kamps et al. used static scenes as a control which, considering TOS/OPA motion-selectivity, have a large impact on TOS/OPA response.</p><disp-quote content-type="editor-comment"><p>(2) Random effects analysis: The authors mention using a &quot;random effects analysis&quot; for several of their experiments. I would ask them to provide more details on what statistical models were used here. Were they purely random-effects models or actually mixed-effects models? What were the factors that entered into the analysis? Providing more detail would make the analysis techniques more transparent.</p></disp-quote><p>Response: This point is now clarified in the Methods section.</p><disp-quote content-type="editor-comment"><p>(3) Data and code availability: The authors write that data and code &quot;are ready to be shared upon request.&quot; (section 2.5) In the spirit of transparency and openness, I strongly encourage the authors to make the data publicly available, e.g., on OSF or OpenNeuro. In particular, having probabilistic maps of PIGS available will allow other researchers to include PIGS in their analysis pipelines, making the current work more impactful.</p></disp-quote><p>Response: We have made the probabilistic labels available to the public. This point is now highlighted in section 2.5.</p><disp-quote content-type="editor-comment"><p>(4) Minor comments on the writing that caught my eye while reading the article:</p><list list-type="bullet"><list-item><p>Line 27: &quot;in the human brain&quot;.</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><p>-Line 30: I don't agree with the characterization of the previous model of scene perception as &quot;simplistic.&quot; Adding one additional ROI makes it no less simplistic. Perhaps the authors can rephrase to make this slightly less antagonistic?</p></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 71: it is not clear why NHPs are relevant here.</p></list-item></list></disp-quote><p>Response: We decided to keep the text intact.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 138&quot; &quot;were randomized&quot;.</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 152: &quot;consisting&quot;.</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 155: &quot;sets&quot; (plural).</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Lines 253-255: Why were the 3T spatially smoothed but not the 7T data? This seems odd.</p></list-item></list></disp-quote><p>Response: We kept the text intact.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 481: &quot;we found strong motion selectivity&quot; (remove &quot;a&quot;).</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 564: a word is missing, probably: &quot;a stronger effect of ego-motion&quot;.</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 591: &quot;controlling spatial attention&quot; (remove &quot;the&quot;).</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 591 and 594: Both sentences start with &quot;However&quot;. I think the first of these should not because it is setting up the contrast for the second sentence.</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Line 607: &quot;higher-level&quot; (hyphen).</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Throughout the manuscript: adverbial phrases such as &quot;(in)coherently changing&quot; or &quot;probabilistically localized&quot; do not get a hyphen.</p></list-item></list></disp-quote><p>Response: Done.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>The authors state that &quot;All data, codes and stimuli are ready to be shared upon request&quot;. Ideally, these materials should be deposited in appropriate repositories (e.g. OpenMRI, GitHub) and not require readers to contact the authors to obtain such materials.</p><p>Other Comments:</p><p>(a) The title (&quot;A previously undescribed scene-selective site is the key to encoding ego-motion in natural environments&quot;) is potentially misleading - the work was not conducted in a natural environment. At best, you could say they are 'naturalistic stimuli'. Also, in what sense is PIGS &quot;key&quot; to encoding ego-motion - the study just shows sensitivity to this factor.</p></disp-quote><p>Response: We changed the title to “naturalistic environments”.</p><disp-quote content-type="editor-comment"><p>(b) Figure 1 - I'm not sure what point the authors are trying to make with Figure 1. The comparison is between a highly smoothed, group fixed-effects analysis and a less-smoothed individual subject analysis. The differences between the two could reflect group vs. individual, highly-smoothed (5 mm) versus less-smoothed (2 mm), or differences in thresholding. If the thresholding were lower for the group analysis, it would probably start to look more similar to the individual subject. As it stands, this figure isn't particularly informative, it seems redundant with Figure 2, and Figure 1A is not even referenced in the main text. Further, fixed effects analyses are relatively uncommon in the recent literature, so their inclusion is unusual.</p></disp-quote><p>Response: Figure 1A is a replication of the data/method used in Nasr et al., 2011 and it will help the readers see the difference between the “traditional” scene-selectivity maps generated based on group-averaging” vs. data from individual subjects. In this case, we decided not to change the Figure.</p><disp-quote content-type="editor-comment"><p>(c) Figure 3 - why are the two sets of maps shown at different thresholds? For 3B given the larger sample size, it is expected that the extent of the significant activations will increase. Currently the higher threshold for 3B and the smaller range for 3A is making the sets of maps look more comparable.</p></disp-quote><p>Response: As the reviewer noticed, the number of subjects is larger in Figure 3B compared to 3A. The main point of this figure is to show that the PIGS activity center does not vary across populations. Considering this point, we decided not to change this figure.</p><disp-quote content-type="editor-comment"><p>(d) Figure 10 - why is the threshold lower than used for other figures? It would be helpful if there was consistent thresholding across figures.</p></disp-quote><p>Response: Experiment 6 and Experiment 1 are based on different stimuli (see Methods). Also, among those subjects who participated in Experiment 1, two subjects did not participate in Experiment 6. These points are already highlighted in the text.</p><disp-quote content-type="editor-comment"><p>(e) Figures - how about the AFNI approach of thresholding and showing sub-threshold data at the same time? (Taylor et al, 2023, Neuroimage).</p></disp-quote><p>Response: We highly appreciate the methodology suggested by Taylor and colleagues. However, our main point here is to show the center of PIGS activity. In this condition, showing an unthresholded activity map doesn’t have any advantage over the current maps. Considering these points, we decided not to change the figures.</p><disp-quote content-type="editor-comment"><p>(f) Coherent versus incoherent scenes - there are many differences between the coherent and incoherent scenes. Arguing that it must be ego-motion seems a little premature without further investigation. Activity anterior to OPA has been associated with the construction of an internal representation of a spatial environment (Steel et al., 2023, The Journal of Neuroscience). Could it be that this is the key effect, not really the ego-motion?</p></disp-quote><p>Response: In this revision, we discussed the study by Steel et al., 2021 and 2023 in section 4.3.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>Overall, I think this is already an excellent contribution. The suggestions I have are minor and may help with the clarity of the results.</p><p>(1) My main request of the authors would be to provide more points of reference in some of the figures with cortical maps. In many cases, the authors use arrows to point to the locations of activations of interest. However, the arrows in adjacent figures are often not placed in exactly the same places on maps that are meant to be compared. It would very much help the viewer to compare activations if the arrows pointing to activations or regions of interest were placed in identical locations for the same brains appearing in different sub-panels (e.g. in panels A and B of Figure 1). The underlying folds of the cortical surface provide some points of reference, but these are often occluded to different extents by data in figures that are meant to be compared.</p></disp-quote><p>Response: To address the reviewer’s concern, we regenerated Figure 8 (Figure 7 in the previous submission) and we tried to put arrowheads in identical locations, as much as possible. Especially for PIGS, this point was also considered in Figures 2 and 3.</p><disp-quote content-type="editor-comment"><p>(2) Outlines (such as those in Figure 5) are also very useful, and I would encourage broader use of them in other figures (e.g. Figures 7, 10, and 12). Figures 10 and 12 are on the fsaverage surface, so the same outlines could be used for them as for Figure 5.</p><p>To be clear, it's possible to apprehend the results with the figures as they are, but I think a few small changes could help a lot.</p></disp-quote><p>Response: In this revision, we added outlines to Figures 11 and 13 (Figure 10 and 12 in the previous submission). We did not add the outline to Figure 8 because it made it hard to see PIGS. Rather we used arrows (see the previous comment).</p><disp-quote content-type="editor-comment"><p>Other minor points:</p><p>In the method for Experiment 4, the authors write: &quot;Other details of the experiment were similar to those in Experiment 1.&quot;. Similar or the same? The authors should clarify this statement, e.g. &quot;the number of images per block, the number of blocks, the number of runs were the same as Experiment 1&quot; - with any differences noted.</p></disp-quote><p>Response: This point is now addressed in the Methods section.</p><disp-quote content-type="editor-comment"><p>In Figure 8, it would be better to have the panel labels (A, B, C, D) in the upper left of each panel rather than the lower left.</p></disp-quote><p>Response: We tried to keep the panels arrangement consistent across the figures. That is why letters are positioned like this.</p><disp-quote content-type="editor-comment"><p>A final gentle suggestion: pycortex (<ext-link ext-link-type="uri" xlink:href="http://github.com/gallantlab/pycortex">http://github.com/gallantlab/pycortex</ext-link>) provides a means to visualize the flattened fsaveage surface with outlines for localized regions of interest and overlaid lines for major sulci. Though it is by no means necessary for publication, It would be lovely to see these results on that surface, which is freely available and downloadable via a pycortex command (surface here: <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/dataset/fsaverage_subject_for_pycortex/9916166">https://figshare.com/articles/dataset/fsaverage_subject_for_pycortex/9916166</ext-link>)</p></disp-quote><p>Response: We thank the reviewer for bringing pycortex to our attention. We will consider using it in our future studies.</p></body></sub-article></article>