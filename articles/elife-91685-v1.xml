<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91685</article-id><article-id pub-id-type="doi">10.7554/eLife.91685</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91685.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Factorized visual representations in the primate visual system and deep neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-204760"><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0930-7327</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-105215"><name><surname>Issa</surname><given-names>Elias B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5387-7207</contrib-id><email>elias.issa@columbia.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Neuroscience, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>École Normale Supérieure - PSL</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>05</day><month>07</month><year>2024</year></pub-date><volume>13</volume><elocation-id>RP91685</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-08-22"><day>22</day><month>08</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-08-04"><day>04</day><month>08</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.22.537916"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-02"><day>02</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91685.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-06-05"><day>05</day><month>06</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91685.2"/></event></pub-history><permissions><copyright-statement>© 2024, Lindsey and Issa</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Lindsey and Issa</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91685-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-91685-figures-v1.pdf"/><abstract><p>Object classification has been proposed as a principal objective of the primate ventral visual stream and has been used as an optimization target for deep neural network models (DNNs) of the visual system. However, visual brain areas represent many different types of information, and optimizing for classification of object identity alone does not constrain how other information may be encoded in visual representations. Information about different scene parameters may be discarded altogether (‘invariance’), represented in non-interfering subspaces of population activity (‘factorization’) or encoded in an entangled fashion. In this work, we provide evidence that factorization is a normative principle of biological visual representations. In the monkey ventral visual hierarchy, we found that factorization of object pose and background information from object identity increased in higher-level regions and strongly contributed to improving object identity decoding performance. We then conducted a large-scale analysis of factorization of individual scene parameters – lighting, background, camera viewpoint, and object pose – in a diverse library of DNN models of the visual system. Models which best matched neural, fMRI, and behavioral data from both monkeys and humans across 12 datasets tended to be those which factorized scene parameters most strongly. Notably, invariance to these parameters was not as consistently associated with matches to neural and behavioral data, suggesting that maintaining non-class information in factorized activity subspaces is often preferred to dropping it altogether. Thus, we propose that factorization of visual scene information is a widely used strategy in brains and DNN models thereof.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>When looking at a picture, we can quickly identify a recognizable object, such as an apple, applying a single word label to it. Although extensive neuroscience research has focused on how human and monkey brains achieve this recognition, our understanding of how the brain and brain-like computer models interpret other complex aspects of a visual scene – such as object position and environmental context – remains incomplete.</p><p>In particular, it was not clear to what extent object recognition comes at the expense of other important scene details. For example, various aspects of the scene might be processed simultaneously. On the other hand, general object recognition may interfere with processing of such details.</p><p>To investigate this, Lindsey and Issa analyzed 12 monkey and human brain datasets, as well as numerous computer models, to explore how different aspects of a scene are encoded in neurons and how these aspects are represented by computational models. The analysis revealed that preventing effective separation and retention of information about object pose and environmental context worsened object identification in monkey cortex neurons. In addition, the computer models that were the most brain-like could independently preserve the other scene details without interfering with object identification.</p><p>The findings suggest that human and monkey high level ventral visual processing systems are capable of representing the environment in a more complex way than previously appreciated. In the future, studying more brain activity data could help to identify how rich the encoded information is and how it might support other functions like spatial navigation. This knowledge could help to build computational models that process the information in the same way, potentially improving their understanding of real-world scenes.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual cortex</kwd><kwd>deep neural networks</kwd><kwd>neurophysiology</kwd><kwd>visual scenes</kwd><kwd>object recognition</kwd><kwd>fMRI</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>DOE CSGF</institution></institution-wrap></funding-source><award-id>DE-SC0020347</award-id><principal-award-recipient><name><surname>Lindsey</surname><given-names>Jack W</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Klingenstein-Simons Foundation</institution></institution-wrap></funding-source><award-id>Fellowship in Neuroscience</award-id><principal-award-recipient><name><surname>Issa</surname><given-names>Elias B</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000879</institution-id><institution>Sloan Foundation</institution></institution-wrap></funding-source><award-id>Fellowship</award-id><principal-award-recipient><name><surname>Issa</surname><given-names>Elias B</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution>Grossman-Kavli Center at Columbia</institution></institution-wrap></funding-source><award-id>Scholar Award</award-id><principal-award-recipient><name><surname>Issa</surname><given-names>Elias B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>High-level visual cortex and leading neural network models of the visual system retain information about multiple visual scene variables in independent, non-interfering dimensions of their population codes.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Artificial deep neural networks (DNNs) are the most predictive models of neural responses to images in the primate high-level visual cortex (<xref ref-type="bibr" rid="bib4">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="bib50">Schrimpf et al., 2020</xref>). Many studies have reported that DNNs trained to perform image classification produce internal feature representations broadly similar to those in areas V4 and IT of the primate cortex, and that this similarity tends to be greater in models with better classification performance (<xref ref-type="bibr" rid="bib57">Yamins et al., 2014</xref>). However, it remains opaque what aspects of the representations of these more performant models drive them to better match neural data. Moreover, beyond a certain threshold level of object classification performance, further improvement fails to produce a concomitant improvement in predicting primate neural responses (<xref ref-type="bibr" rid="bib50">Schrimpf et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Nonaka et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Linsley, 2023</xref>). This weakening trend motivates finding new normative principles, besides object classification ability, that push models to better match primate visual representations.</p><p>One strategy for achieving high object classification performance is to form neural representations that discard some (are tolerant to) or all (are invariant to) information besides object class. Invariance in neural representations is in some sense a zero-sum strategy: building invariance to some parameters improves the ability to decode others. We also note that our use of ‘invariance’ in this context refers to invariance in neural representations, rather than behavioral or perceptual invariance (<xref ref-type="bibr" rid="bib11">DiCarlo and Cox, 2007</xref>). However, high-level cortical neurons in the primate ventral visual stream are known to simultaneously encode many forms of information about visual input besides object identity, such as object pose (<xref ref-type="bibr" rid="bib19">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib27">Hong et al., 2016</xref>; <xref ref-type="bibr" rid="bib33">Kravitz et al., 2013</xref>; <xref ref-type="bibr" rid="bib45">Peters and Kriegeskorte, 2021</xref>). In this work, we seek to characterize how the brain simultaneously represents different forms of information.</p><p>In particular, we introduce methods to quantify the relationships between different types of visual information in a population code (e.g., object pose vs. camera viewpoint), and specifically the degree to which different forms of information are ‘factorized’. Intuitively, if the variance driven by one parameter is encoded along orthogonal dimensions of population activity space compared to the variance driven by other scene parameters, we say that this representation is factorized. We note that our definition of factorization is closely related to the existing concept of manifold disentanglement (<xref ref-type="bibr" rid="bib11">DiCarlo and Cox, 2007</xref>; <xref ref-type="bibr" rid="bib9">Chung et al., 2018</xref>) and can be seen as a generalization of disentanglement to high-dimensional visual scene parameters like object pose. Factorization can enable simultaneous decoding of many parameters at once, supporting diverse visually guided behaviors (e.g., spatial navigation, object manipulation, or object classification) (<xref ref-type="bibr" rid="bib29">Johnston and Fusi, 2023</xref>).</p><p>Using existing neural datasets, we found that both factorization of and invariance to object category and position information increase across the macaque ventral visual cortical hierarchy. Next, we leveraged the flexibility afforded by in silico models of visual representations to probe different forms of factorization and invariance in more detail, focusing on several scene parameters of interest: background content, lighting conditions, object pose, and camera viewpoint. Across a broad library of DNN models that varied in their architecture and training objectives, we found that factorization of all of the above scene parameters in DNN feature representations was positively correlated with models’ matches to neural and behavioral data. Interestingly, while neural invariance to some scene parameters (background scene and lighting conditions) predicted neural fits, invariance to others (object pose and camera viewpoint) did not. Our results generalized across both monkey and human datasets using different measures (neural spiking, fMRI, and behavior; 12 datasets total) and could not be accounted for by models’ classification performance. Thus, we suggest that factorized encoding of multiple behaviorally relevant scene variables is an important consideration, alongside other desiderata such as classification performance, in building more brain-like models of vision.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Disentangling object identity manifolds in neural population responses can be achieved by qualitatively different strategies. These include building invariance of responses to non-identity scene parameters (or, more realistically, partial invariance; <xref ref-type="bibr" rid="bib11">DiCarlo and Cox, 2007</xref>) and/or factorizing non-identity-driven response variance into isolated (factorized) subspaces (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, left vs. center panels, cylindrical/spherical-shaded regions represent object manifolds). Both strategies maintain an ‘identity subspace’ in which object manifolds are linearly separable. In a non-invariant, non-factorized representation, other variables like camera viewpoint also drive variance within the identity subspace, ‘entangling’ the representations of the two variables (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, right; viewpoint-driven variance is mainly in identity subspace, orange flat-shaded region).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Framework for quantifying factorization in neural and model representations.</title><p>(<bold>A</bold>) A subspace for encoding a variable, for example, object identity, in a linearly separable manner can be achieved by becoming invariant to non-class variables (compact spheres, middle column, where the volume of the sphere corresponds to the degree of neural invariance, or tolerance, for non-class variables; colored dots represent example images within each class) and/or by encoding variance induced by non-identity variables in orthogonal neural axes to the identity subspace (extended cylinders, left column). Only the factorization strategy simultaneously represents multiple variables in a disentangled fashion. A code that is sensitive to non-identity parameters within the identity subspace corrupts the ability to decode identity (right column) (identity subspace denoted by orange plane). (<bold>B</bold>) Variance across images within a class can be measured in two different linear subspaces: that containing the majority of variance for all other parameters (<italic>a, ‘other_param_subspace’</italic>) and that containing the majority of the variance for that parameter (<italic>b, ‘param_subspace’</italic>). Factorization is defined as the fraction of parameter-induced variance that avoids the other-parameter subspace (left). By contrast, invariance to the parameter of interest is computed by comparing the overall parameter-induced variance to the variance in response to other parameters (<italic>c, ‘var_other_param’</italic>) (right). (<bold>C</bold>) In a simulation of coding strategies for two binary variables out of 10 total dimensions that are varying (see ‘Methods<bold>’</bold>), a decrease in orthogonality of the relationship between the encoding of the two variables (alignment <italic>a</italic> &gt; 0, or going from a square to a parallelogram geometry), despite maintaining linear separability of variables, results in poor classifier performance in the few training-samples regime when i.i.d. Gaussian noise is present in the data samples (only 3 of 10 dimensions used in simulation are shown).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig1-v1.tif"/></fig><p>To formalize these different representational strategies, we introduced measures of factorization and invariance to scene parameters in neural population responses (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; see <xref ref-type="disp-formula" rid="equ2 equ3 equ4">Equations 2–4</xref> in ‘Methods’). Concretely, invariance to a scene variable (e.g., object motion) is computed by measuring the degree to which varying that parameter alone changes neural responses, relative to the changes induced by varying other parameters (lower relative influence on neural activity corresponds to higher invariance, or tolerance, to that parameter). Factorization is computed by identifying the axes in neural population activity space that are influenced by varying the parameter of interest and assessing how much it overlaps the axes influenced by other parameters (‘<italic>a’</italic> in <xref ref-type="fig" rid="fig1">Figure 1B and C</xref>; lower overlap corresponds to higher factorization). We quantified this overlap in two different ways (‘principal components analysis (PCA)-based’ and ‘covariance-based’ factorization, corresponding to Equations 2 and 4 in ‘Methods’), which produced similar results when compared in subsequent analyses (unless otherwise noted, factorization scores will generally refer to the PCA-based method, and the covariance method is shown in Figures 5–7 for comparison). Intuitively, a neural population in which one neural subpopulation encodes object identity and another separate subpopulation encodes object position exhibits a high degree of factorization of those two parameters (however, note that factorization may also be achieved by neural populations with mixed selectivity in which the ‘subpopulations’ correspond to subspaces, or independent orthogonal linear projections, of neural activity space rather than physical subpopulations). Though the example presented in <xref ref-type="fig" rid="fig1">Figure 1</xref> focused on factorization of and invariance to object identity versus non-identity variables, we stress that our definitions can be applied to any scene variables of interest. Furthermore, we presented a simplified visual depiction of the geometry within each scene variable subspace in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We emphasize that our factorization metric does not require a particular geometry within a variable’s subspace, whether parallel linearly ordered coding of viewpoint as in the cylindrical class manifolds shown in <xref ref-type="fig" rid="fig1">Figure 1A and B</xref>, or a more complex geometry where there is a lack of parallelism and/or a more nonlinear layout.</p><p>While factorization and invariance are not mutually exclusive representational strategies, they are qualitatively different. Factorization, unlike invariance, has the potential to enable the simultaneous representation of multiple scene parameters in a decodable fashion. Intuitively, factorization increases with higher dimensionality as this decreases overlap, all other things being equal (in the limit, the angle between points will approach 90<sup>o</sup> or a fully orthogonal code in high dimensions), and for a given finite, fixed dimension, factorization is mainly driven by the angle between this dimension and the other variable subspaces which measures the degree of contamination (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; square vs. parallelogram). In a simulation, we found that the extent to which the variables of interest were represented in a factorized way (i.e., along orthogonal axes, rather than correlated axes) influenced the ability of a linear discriminator to successfully decode both variables in a generalizable fashion from a few training samples (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>Given the theoretically desirable properties of factorized representations, we next asked whether such representations are observed in neural data, and how much factorization contributes empirically to downstream decoding performance in real data. Specifically, we took advantage of an existing dataset in which the tested images independently varied object identity versus object pose plus background context (<xref ref-type="bibr" rid="bib41">Majaj et al., 2015</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/brain-score/vision/blob/master/examples/data_metrics_benchmarks.ipynb">https://github.com/brain-score/vision/blob/master/examples/data_metrics_benchmarks.ipynb</ext-link>). We found that both V4 and IT responses exhibited more significant factorization of object identity information from non-identity information than a shuffle control (which accounts for effects on factorization due to dimensionality of these regions) (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; see ’Methods’). Furthermore, the degree of factorization increased from V4 to IT (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Consistent with prior studies, we also found that invariance to non-identity information increased from V4 to IT in our analysis (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right, solid lines; <xref ref-type="bibr" rid="bib48">Rust and DiCarlo, 2010</xref>). Invariance to non-identity information was even more pronounced when measured in the subspace of population activity capturing the bulk (90%) of identity-driven variance as a consequence of increased factorization of identity from non-identity information (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, right, dashed lines).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Benefit of factorization to neural decoding in macaque V4 and IT.</title><p>(<bold>A</bold>) Factorization of object identity and position increased from macaque V4 to IT (PCA-based factorization, see ‘Methods’; dataset E1 – multiunit activity in macaque visual cortex) (left). Like factorization, invariance also increased from V4 to IT (note, ‘identity’ refers to invariance to all non-identity position factors, solid black line) (right). Combined with increased factorization of the remaining variance, this led to higher invariance within the variable’s subspace (orange lines), representing a neural subspace for identity information with invariance to nuisance parameters which decoders can target for read-out. (<bold>B</bold>) An experiment to test the importance of factorization for supporting object class decoding performance in neural responses. We applied a transformation to the neural data (linear basis rotation) that rotated the relative positions of mean responses to object classes without changing the relative proportion of within- vs. between-class variance (Equation 1 in ’Methods’). This transformation preserved invariance to non-class factors (leftmost pair of bars in each plot), while decreasing factorization of class information from non-class factors (center pair of bars in each plot). Concurrently, it had the effect of significantly reducing object class decoding performance (light vs. dark red bars in each plot, chance = 1/64; n = 128 multi-unit sites in V4 and 128 in IT).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Factorization and invariance in V4 and IT neural data.</title><p>Normalized factorization and invariance as in <xref ref-type="fig" rid="fig2">Figure 2A</xref> but after subtracting shuffle control for V4 and IT neural datasets. Shuffling the image identities of each population vector accounts for increases in factorization driven purely by changes in the covariance statistics of population responses between V4 and IT. However, the normalized factorization scores remained significantly above zero for both brain areas.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig2-figsupp1-v1.tif"/></fig></fig-group><p>To illustrate the beneficial effect of factorization on decoding performance, we performed a statistical lesion experiment that precisely targeted this aspect of representational geometry. Specifically, we analyzed a transformed neural representation obtained by rotating the population data so that inter-class variance more strongly overlapped with the principal components (PCs) of the intra-class variance in the data (see Equation 1 in ’Methods’). Note that this transformation, designed to decrease factorization, acts on the angle between latent variable subspaces. The applied linear basis rotation leaves all other activity statistics completely intact (such as mean neural firing rates, covariance structure of the population, and its invariance to non-class variables) yet has the effect of strongly reducing object identity decoding performance in both V4 and IT (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Our analysis shows that maintaining invariance alone in the neural population code was insufficient to account for a large fraction of decoding performance in high-level visual cortex; factorization of non-identity variables is key to the decoding performance achieved by V4 and IT representations.</p><p>We next asked whether factorization is found in DNN model representations and whether this novel, heretofore unconsidered metric, is a strong indicator of more brainlike models. When working with computational models, we have the liberty to test an arbitrary number of stimuli; therefore, we could independently vary multiple scene parameters at sufficient scale to enable computing factorization and invariance for each, and we explored factorization in DNN model representations in more depth than previously measured in existing neural experiments. To gain insight back into neural representations, we also assessed the ability of each model to predict separately collected neural and behavioral data. In this fashion, we may indirectly assess the relative significance of geometric properties like factorization and invariance to biological visual representations – if, for instance, models with more factorized representations consistently match neural data more closely, we may infer that those neural representations likely exhibit factorization themselves (<xref ref-type="fig" rid="fig3">Figure 3</xref>). To measure factorization, invariance, and decoding properties of DNN models, we generated an augmented image set, based on the images used in the previous dataset (<xref ref-type="fig" rid="fig2">Figure 2</xref>), in which we independently varied the foreground object identity, foreground object pose, background identity, scene lighting, and 2D scene viewpoint. Specifically for each base image from the original dataset, we generated sets of images that varied exactly one of the above scene parameters while keeping the others constant, allowing us to measure the variance induced by each parameter relative to the variance across all scene parameters (<xref ref-type="fig" rid="fig3">Figure 3</xref>, top left; 100 base scenes and 10 transformed images for each source of variation). We presented this large image dataset to models (4000 images total) to assess the relative degree of representational factorization of and invariance to each scene parameter. We conducted this analysis across a broad range of DNNs varying in architecture and objective as well as other implementational choices to obtain the widest possible range of DNN representations for testing our hypothesis. These included models using supervised training for object classification (<xref ref-type="bibr" rid="bib35">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib20">He et al., 2016</xref>), contrastive self-supervised training (<xref ref-type="bibr" rid="bib22">He et al., 2020</xref>; <xref ref-type="bibr" rid="bib8">Chen et al., 2020</xref>), and self-supervised models trained using auxiliary objective functions (<xref ref-type="bibr" rid="bib53">Tian et al., 2019</xref>; <xref ref-type="bibr" rid="bib13">Doersch et al., 2015</xref>; <xref ref-type="bibr" rid="bib21">He et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Donahue and Simonyan, 2019</xref>; see ’Methods’ and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1b</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Measurement of factorization in deep neural network (DNN) models and comparison to brain data.</title><p>Schematic showing how meta-analysis on models and brain data was conducted by first computing various representational metrics on models and then measuring a model’s predictive power across a variety of datasets. For computing the representational metrics of factorization of and invariance to a scene parameter, variance in model responses was induced by individually varying each of four scene parameters (n = 10 parameter levels) for each base scene (n = 100 base scenes) (see images on the top left). The combination of model-layer metric and model-layer dataset predictivity for a choice of model, layer, metric, and dataset specifies the coordinates of a single dot on the scatter plots in <xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig7">7</xref>, and the across-model correlation coefficient between a particular representational metric and neural predictivity for a dataset summarizes the potential importance of the metric in producing more brainlike models (see <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig3-v1.tif"/></fig><p>First, we asked whether, in the course of training, DNN models develop factorized representations at all. We found that the final layers of trained networks exhibited consistent increases in factorization of all tested scene parameters relative to a randomly initialized (untrained) baseline with the same architecture (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, top row, rightward shift relative to black cross, a randomly initialized ResNet-50). By contrast, training DNNs produced mixed effects on invariance, typically increasing it for background and lighting but reducing it for object pose and camera viewpoint (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, bottom row, leftward shift relative to black cross for left two panels). Moreover, we found that the degree of factorization in models correlated with the degree to which they predicted neural activity for single-unit IT data (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, top row), which can be seen as correlative evidence that neural representations in IT exhibit factorization of all scene variables tested. Interestingly, we saw a different pattern for representational invariance to a scene parameter. Invariance showed mixed correlations with neural predictivity (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, bottom row), suggesting that IT neural representations build invariance to some scene information (background and lighting) but not to others (object pose and observer viewpoint). Similar effects were observed when we assessed correlations between these metrics and fits to human behavioral data rather than macaque neural data (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Neural and behavioral predictivity of models versus their factorization and invariance properties.</title><p>(<bold>A</bold>) Scatter plots, for example, neural dataset (IT single units, macaque E2 dataset) showing the correlation between a model’s predictive power as an encoding model for IT neural data versus a model’s ability to factorize or become invariant to different scene parameters (each dot is a different model, using each model’s penultimate layer). Note that factorization (PCA-based, see ‘Methods’) in trained models is consistently higher than that for an untrained, randomly initialized Resnet-50 DNN architecture (rightward shift relative to black cross). Invariance to background and lighting but not to object pose and viewpoint increased in trained models relative to the untrained control (rightward versus leftward shift relative to black cross). (<bold>B</bold>) Same as (<bold>A</bold>) except for human behavior performance patterns across images (human I2 dataset). Increasing scene parameter factorization in models generally correlated with better neural predictivity (top row). A noticeable drop in neural predictivity was seen for high levels of invariance to object pose (bottom row, second panel).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Scatter plots for all datasets for V4.</title><p>Scatter plots as in <xref ref-type="fig" rid="fig4">Figure 4A and B</xref> for all datasets. Brain metric (y-axes) is macaque neuron/human voxel fits in V4 cortex. The plots in the top half use deep neural network (DNN) factorization scores (PCA-based method) on the x-axis while the bottom half use DNN invariance scores.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Scatter plots for all datasets for ITC/HVC.</title><p>Scatter plots as in <xref ref-type="fig" rid="fig4">Figure 4A and B</xref> for all datasets. Brain metric (y-axes) is macaque neuron/human voxel fits in ITC/HVC. The plots in the top half use DNN factorization scores (PCA-based method) on the x-axis while the bottom half use DNN invariance scores.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Scatter plots for all datasets for behavior.</title><p>Scatter plots as in <xref ref-type="fig" rid="fig4">Figure 4A and B</xref> for all datasets. Brain metric (y-axes) is macaque/human per-image classification performance (I1) and image-by-distractor class performance (I2). The plots in the top half use deep neural network (DNN) factorization scores (PCA-based method) on the x-axis while the bottom half use DNN invariance scores.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig4-figsupp3-v1.tif"/></fig></fig-group><p>To assess the robustness of these findings to choice of images and brain regions used in an experiment, we conducted the same analyses across a large and diverse set of previously collected neural and behavioral datasets, from different primate species and visual regions (six macaque datasets [<xref ref-type="bibr" rid="bib41">Majaj et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Rust and DiCarlo, 2012</xref>; <xref ref-type="bibr" rid="bib46">Rajalingham et al., 2018</xref>]: two V4, two ITC (inferior temporal cortex), and two behavior; six human datasets [<xref ref-type="bibr" rid="bib46">Rajalingham et al., 2018</xref>; <xref ref-type="bibr" rid="bib30">Kay et al., 2008</xref>; <xref ref-type="bibr" rid="bib51">Shen et al., 2019</xref>]: two V4, two HVC (higher visual cortex), and two behavior; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref>). Consistently, increased factorization of scene parameters in model representations correlated with models being more predictive of neural spiking responses, voxel BOLD signal, and behavioral responses to images (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, black bars; see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s3">3</xref> for scatter plots across all datasets). Although invariance to appearance factors (background identity and scene lighting) correlated with more brainlike models, invariance for spatial transforms (object pose and camera viewpoint) consistently did not (zero or negative correlation values; <xref ref-type="fig" rid="fig5">Figure 5C</xref>, red and green open circles). Our results were preserved when we re-ran the analyses using only the subset of models with the identical ResNet-50 architecture (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>) or when we evaluated model predictivity using representational dissimilarity matrices of the population (RDMs) instead of linear regression (encoding) fits of individual neurons or voxels (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Furthermore, the main finding of a positive correlation between factorization and neural predictivity was robust to the particular choice of PCA threshold we used to quantify factorization (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). We found similar results using a covariance-based method for computing factorization that does not have any free parameters (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, faded filled circles; see Equations 4 in ‘Methods’).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Scene parameter factorization correlates with more brainlike deep neural network (DNN) models.</title><p>(<bold>A</bold>) Factorization of scene parameters in model representations computed using the PCA-based method consistently correlated with a model being more brainlike across multiple independent datasets measuring monkey neurons, human fMRI voxels, or behavioral performance in both macaques and humans (left vs. right column) (black bars). By contrast, increased invariance to camera viewpoint or object pose was not indicative of brainlike models (gray bars). In all cases, model representational metric and neural predictivity score were computed by averaging scores across the last 5 model layers. (<bold>B</bold>) Instead of computing factorization scores using our synthetic images (<xref ref-type="fig" rid="fig3">Figure 3</xref>, top left), recomputing camera viewpoint or object pose factorization from natural movie datasets that primarily contained camera or object motion, respectively, gave similar results for predicting which model representations would be more brainlike (right: example movie frames; also see ’Methods’). Error bars in (<bold>A and B</bold>) are standard deviations over bootstrapped resampling of the models. (<bold>C</bold>) Summary of the results from (<bold>A</bold>) across datasets (x-axis) for invariance (open symbols) versus factorization (closed symbols) (for reference, ‘<italic>x</italic>’ symbols indicate predictive power when using model classification performance). Results using a comparable, alternative method for computing factorization (covariance-based, Equation 4 in ’Methods’; light closed symbols) are shown adjacent to the original factorization metric (PCA-based, Equation 2 in ‘Methods’; dark closed symbols).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Predictivity of factorization and invariance restricting to ResNet-50 model architectures.</title><p>Same format as <xref ref-type="fig" rid="fig5">Figure 5C</xref> except with the analyses restricted to using only models with the Resnet-50 architecture. The main finding of factorization of scene parameters in deep neural networks (DNNs) being generally positively correlated with better predictions of brain data is replicated using this architecture-matched subset of models, controlling for potential confounds from model architecture.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Predictivity of factorization and invariance for representational dissimilarity matrices (RDMs).</title><p>Same format as <xref ref-type="fig" rid="fig5">Figure 5C</xref> except for predicting population RDMs of macaque neurophysiological and human fMRI data (in <xref ref-type="fig" rid="fig5">Figure 5C</xref>, linear encoding fits of each single neuron/voxel were used to measure brain predictivity of a model). The main finding of factorization of scene parameters in deep neural networks (DNNs) being positively correlated with better predictions of brain data is replicated using RDMs instead of neural/voxel goodness of encoding fit.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Effect on neural and behavioral predictivity of PCA threshold for computing PCA-based factorization, related to <xref ref-type="fig" rid="fig5">Figure 5</xref>.</title><p>The % variance threshold used in the main text for estimating a PCA linear subspace capturing the bulk of the variance induced by all other parameters besides the parameter of interest is somewhat arbitrary. Here, we show that the results of our main analysis change little if we vary this parameter from 50 to 99%. In the main text, a PCA threshold of 90% was used for computing PCA-based factorization scores.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig5-figsupp3-v1.tif"/></fig></fig-group><p>Finally, we tested whether our results generalized across the particular image set used for computing the model factorization scores in the first place. Here, instead of relying on our synthetically generated images, where each scene parameter was directly controlled, we re-computed factorization from two types of relatively unconstrained natural movies, one where the observer moves in an urban environment (approximates camera viewpoint changes) (<xref ref-type="bibr" rid="bib37">Lee et al., 2012</xref>) and another where objects move in front of a fairly stationary observer (approximates object pose changes) (<xref ref-type="bibr" rid="bib42">Monfort, 2019</xref>). Similar to the result found for factorization measured using augmentations of synthetic images, factorization of frame-by-frame variance (local in time, presumably dominated by either observer or camera motion; see ‘Methods’) from other sources of variance across natural movies (non-local in time) was correlated with improved neural predictivity in both macaque and human data while invariance to local frame-by-frame differences was not (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; black versus gray bars). Thus, we have shown that a main finding – the importance of object pose and camera viewpoint factorization for achieving brainlike representations – holds across types of brain signal (spiking vs. BOLD), species (monkey vs. human), cortical brain areas (V4 vs. IT), images for testing in experiments (synthetic, grayscale vs. natural, color), and image sets for computing the metric (synthetic images vs. natural movies).</p><p>Our analysis of DNN models provides strong evidence that greater factorization of a variety of scene variables is consistently associated with a stronger match to neural and behavioral data. Prior work had identified a similar correlation between object classification performance (measured fitting a decoder for object class using model representations) and fidelity to neural data (<xref ref-type="bibr" rid="bib57">Yamins et al., 2014</xref>). A priori, it is possible that the correlations we have demonstrated between scene parameter factorization and neural fit can be entirely captured by the known correlation between classification performance and neural fits (<xref ref-type="bibr" rid="bib50">Schrimpf et al., 2020</xref>; <xref ref-type="bibr" rid="bib57">Yamins et al., 2014</xref>) as factorization and classification may themselves be correlated. However, we found that factorization scores significantly boosted cross-validated predictive power of neural/behavioral fit performance compared to simply using object classification alone, and factorization boosted predictive power as much if not slightly more when using RDMs instead of linear regression fits to quantify the match to the brain/behavior (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Thus, considering factorization in addition to object classification performance improves upon our prior understanding of the properties of more brainlike models (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Scene parameter factorization combined with object identity classification improves correlations with neural predictivity.</title><p>Average across datasets of brain predictivity of classification (faded black bar), dimensionality (faded pink bar), and factorization (remaining faded colored bars) in a model representation. Linearly combining factorization with classification in a regression model (unfaded bars at right) produced significant improvements in predicting the most brainlike models (performance cross-validated across models and averaged across datasets, n = 4 datasets for each of V4, IT/HVC and behavior). The boost from factorization in predicting the most brainlike models was not observed for neural and fMRI data when combining classification with a model’s overall dimensionality (solid pink bars; compared to black dashed line for brain predictivity when using classification alone). Results are shown for both the PCA-based and covariance-based factorization metric (top versus bottom row). Error bars are standard deviations over bootstrapped resampling of the models.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig6-v1.tif"/></fig><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Combining classification performance with object pose factorization improves predictions of the most brainlike models on IT/HVC data.</title><p>Example scatter plots for neural and fMRI datasets (macaque E1 and E2, IT multi units and single units; human F1 and F2, fMRI voxels) showing a saturating and sometimes reversing trend in neural (voxel) predictivity for models that are increasingly good at classification (top row). This saturating/reversing trend is no longer present when adding object pose factorization to classification as a combined, predictive metric for brainlikeness of a model (middle and bottom rows). The x-axis of each plot indicates the predicted encoding fit or representational dissimilarity matrix (RDM) correlation after fitting a linear regression model with the indicated metrics as input (either classification or classification + factorization).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91685-fig7-v1.tif"/></fig></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Object classification, which has been proposed as a normative principle for the function of the ventral visual stream, can be supported by qualitatively different representational geometries (<xref ref-type="bibr" rid="bib57">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib43">Nayebi, 2021</xref>). These include representations that are completely invariant to non-class information (<xref ref-type="bibr" rid="bib6">Caron et al., 2019b</xref>; <xref ref-type="bibr" rid="bib5">Caron, 2019a</xref>) and representations that retain a high-dimensional but factorized encoding of non-class information, which disentangles the representation of multiple variables (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Here, we presented evidence that factorization of non-class information is an important strategy used, alongside invariance, by the high-level visual cortex (<xref ref-type="fig" rid="fig2">Figure 2</xref>) and by DNNs that are predictive of primate neural and behavioral data (<xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>).</p><p>Prior work has indicated that building representations that support object classification performance and representations that preserve high-dimensional information about natural images are both important principles of the primate visual system (<xref ref-type="bibr" rid="bib4">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="bib16">Elmoznino and Bonner, 2022</xref>; though see <xref ref-type="bibr" rid="bib10">Conwell et al., 2022</xref>). Critically, our results cannot be accounted for by classification performance or dimensionality alone (<xref ref-type="fig" rid="fig6">Figure 6</xref>, gray and pink bars); that is, the relationship between factorization and matches to neural data was not entirely mediated by classification or dimensionality. That said, we do not regard factorization and dimensionality, or factorization and object classification performance, as mutually exclusive hypotheses for useful principles of visual representations. Indeed, high-dimensional representations could be regarded as a means to facilitate factorization, and likewise factorized representations can better support classification (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>Our notion of factorization is related to, but distinct from, several other concepts in the literature. Many prior studies in machine learning have considered the notion of disentanglement, often defined as the problem of inferring independent factors responsible for generating the observed data (<xref ref-type="bibr" rid="bib31">Kim and Mnih, 2018</xref>; <xref ref-type="bibr" rid="bib15">Eastwood and Williams, 2018</xref>; <xref ref-type="bibr" rid="bib25">Higgins, 2018</xref>). One prior study notably found that machine learning models designed to infer disentangled representations of visual data displayed single-unit responses that resembled those of individual neurons in macaque IT (<xref ref-type="bibr" rid="bib26">Higgins et al., 2021</xref>). Our definition of factorization is more flexible, requiring only that independent factors be encoded in orthogonal subspaces, rather than by distinct individual neurons. Moreover, our definition applies to generative factors, such as camera viewpoint or object pose, that are multidimensional and context dependent. Factorization is also related to a measure of ‘abstraction’ in representational geometry introduced in a recent line of work (<xref ref-type="bibr" rid="bib2">Bernardi et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Boyle et al., 2024</xref>), which is observed to emerge in trained neural networks (<xref ref-type="bibr" rid="bib29">Johnston and Fusi, 2023</xref>; <xref ref-type="bibr" rid="bib1">Alleman et al., 2024</xref>). In these studies, an abstract representation is defined as one in which variables are encoded and can be decoded in a consistent fashion regardless of the values of other variables. A fully factorized representation should be highly abstract according to this definition, though factorization emphasizes the geometric properties of the population representation while these studies emphasize the consequences for decoding performance in training downstream linear read-outs. Relatedly, another recent study found that orthogonal encoding of class and non-class information is one of several factors that determines few-shot classification performance (<xref ref-type="bibr" rid="bib52">Sorscher et al., 2022</xref>). Our work can be seen as complementary to work on representational straightening of natural movie trajectories in the population space (<xref ref-type="bibr" rid="bib24">Hénaff et al., 2021</xref>). This work suggested that visual representations maintain a locally linear code of latent variables like camera viewpoint, while our work focused on the global arrangement of the linear subspaces affected by different variables (e.g., overall coding of camera viewpoint-driven variance versus sources of variance from other scene variables in a movie). Local straightening of natural movies was found to be important for early visual cortex neural responses but not necessarily for high-level visual cortex (<xref ref-type="bibr" rid="bib54">Toosi and Issa, 2022</xref>), where the present work suggests factorization may play a role.</p><p>Our work has several limitations. First, our analysis is primarily correlative. Going forward, we suggest that factorization could prove to be a useful objective function for optimizing neural network models that better resemble primate visual systems, or that factorization of latent variables should at least be a by-product of other objectives that lead to more brain-like models. An important direction for future work is finding ways to directly incentivize factorization in model objective functions so as to test its causal impact on the fidelity of learned representations to neural data. Second, our choice of scene variables to analyze in this study was heuristic and somewhat arbitrary. Future work could consider unsupervised methods (in the vein of independent components analysis) for uncovering the latent sources of variance that generate visual data, and assessing to what extent these latent factors are encoded in factorized form. Third, in our work we do not specify the details of how a particular scene parameter is encoded within its factorized subspace, including whether the code is linear (‘straightened’) or nonlinear (<xref ref-type="bibr" rid="bib24">Hénaff et al., 2021</xref>; <xref ref-type="bibr" rid="bib23">Hénaff et al., 2019</xref>). Neural codes could adopt different strategies, resulting in similar factorization scores at the population level, each with some support in visual cortex literature: (1) each neuron encodes a single latent variable (<xref ref-type="bibr" rid="bib17">Field, 1994</xref>; <xref ref-type="bibr" rid="bib7">Chang and Tsao, 2017</xref>), (2) separate brain subregions encode qualitatively different latent variables but using distributed representations within each region (<xref ref-type="bibr" rid="bib55">Tsao et al., 2006</xref>; <xref ref-type="bibr" rid="bib36">Lafer-Sousa and Conway, 2013</xref>; <xref ref-type="bibr" rid="bib56">Vaziri et al., 2014</xref>), and (3) each neuron encodes multiple variables in a distributed population code, such that the factorization of different variables is only apparent as independent directions when assessed in high-dimensional population activity space (<xref ref-type="bibr" rid="bib17">Field, 1994</xref>; <xref ref-type="bibr" rid="bib47">Rigotti et al., 2013</xref>). Future work can disambiguate among these possibilities by systematically examining ventral visual stream subregions (<xref ref-type="bibr" rid="bib33">Kravitz et al., 2013</xref>; <xref ref-type="bibr" rid="bib56">Vaziri et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Kravitz et al., 2011</xref>) and the single neuron tuning curves within them (<xref ref-type="bibr" rid="bib38">Leopold et al., 2006</xref>; <xref ref-type="bibr" rid="bib18">Freiwald et al., 2009</xref>).</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Monkey datasets</title><p>Macaque monkey datasets were of single-unit neural recordings (<xref ref-type="bibr" rid="bib49">Rust and DiCarlo, 2012</xref>), multi-unit neural recordings (<xref ref-type="bibr" rid="bib41">Majaj et al., 2015</xref>), and object recognition behavior (<xref ref-type="bibr" rid="bib46">Rajalingham et al., 2018</xref>). Single-unit spiking responses to natural images were measured in V4 and anterior ventral IT (<xref ref-type="bibr" rid="bib49">Rust and DiCarlo, 2012</xref>). The advantages of this dataset are that it contains well-isolated single neurons, the gold standard for electrophysiology. Furthermore, the IT recordings were obtained from penetrating electrodes targeting the anterior ventral portion of IT near the base of skull, reflecting the highest level of the IT hierarchy. On the other hand, the multi-unit dataset was obtained from across IT with a bias toward where multi-unit arrays are more easily placed such as CIT and PIT (<xref ref-type="bibr" rid="bib41">Majaj et al., 2015</xref>), complementing the recording locations of the single-unit dataset. An advantage of the multi-unit dataset using chronic recording arrays is that an order of magnitude more images were tested per recording site (see dataset comparisons in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1a</xref>). Finally, the monkey behavioral dataset came from a third study examining the image-by-image object classification performance of macaques and humans (<xref ref-type="bibr" rid="bib46">Rajalingham et al., 2018</xref>).</p></sec><sec id="s4-2"><title>Human datasets</title><p>Three datasets from humans were used, two fMRI datasets and one object recognition behavior dataset (<xref ref-type="bibr" rid="bib44">Nonaka et al., 2021</xref>; <xref ref-type="bibr" rid="bib46">Rajalingham et al., 2018</xref>; <xref ref-type="bibr" rid="bib30">Kay et al., 2008</xref>). The fMRI datasets used different images (color versus grayscale) but otherwise used a fairly similar number of images and voxel resolution in MR imaging. Human fMRI studies have found that different DNN layers tend to map to V4 and HVC human fMRI voxels (<xref ref-type="bibr" rid="bib44">Nonaka et al., 2021</xref>). The human behavioral dataset measured image-by-image classification performance and was collected in the same study as the monkey behavioral signatures (<xref ref-type="bibr" rid="bib46">Rajalingham et al., 2018</xref>).</p></sec><sec id="s4-3"><title>Computational models</title><p>In recent years, a variety of approaches to training DNN vision models have been developed that learn representations that can be used for downstream classification (and other) tasks. Models differ in a variety of implementational choices including in their architecture, objective function, and training dataset. In the models we sampled, objectives included supervised learning of object classification (AlexNet, ResNet), self-supervised contrastive learning (MoCo, SimCLR), and other unsupervised learning algorithms based on auxiliary tasks (e.g., reconstruction or colorization). A majority of the models that we considered relied on the widely used, performant ResNet-50 architecture, though some in our library utilized different architectures. The randomly initialized network control utilized ResNet-50 (see <xref ref-type="fig" rid="fig4">Figure 4A and B</xref>). The set of models we used is listed in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1b</xref>.</p></sec><sec id="s4-4"><title>Simulation of factorized versus non-factorized representational geometries</title><p>For the simulation in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, we generated data in the following way. First, we randomly sampled the values of N = 10 binary features. Feature values corresponded to positions in an N-dimensional vector space as follows: each feature was assigned an axis in N-dimensional space, and the value of each feature (+1 or –1) was treated as a coefficient indicating the position along that axis. All but two of the feature axes were orthogonal to the rest. The last two features, which served as targets for the trained linear decoders, were assigned axes whose alignment ranged from 0 (orthogonal) to 1 (identical). In the noiseless case, factorization of these two variables with respect to one another is given by subtracting the square of the cosine of the angle between the axes from 1. We added Gaussian noise to the positions of each data point and randomly sampled K positive and negative examples for each variable of interest to use as training data for the linear classifier (a support vector machine).</p></sec><sec id="s4-5"><title>Macaque neural data analyses</title><p>For the shuffle control used as a null model for factorization, we shuffled the object identity labels of the images (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). For the transformation used in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, we computed the PCs of the mean neural activity response to each object class (‘class centers,’ <bold><italic>x</italic></bold><italic><sup>c</sup></italic>), referred to as the inter-class PCs, <bold><italic>v</italic></bold><italic><sub>1</sub><sup>inter</sup></italic>, <bold><italic>v</italic></bold><italic><sub>2</sub><sup>inter</sup></italic>, …, <bold><italic>v</italic></bold><italic><sub>N</sub><sup>inter</sup></italic>. We also computed the PCs of the data with corresponding class centers subtracted (i.e., <bold><italic>x - x</italic></bold><italic><sup>c</sup></italic>) from each activity pattern, referred to as the intra-class PCs <bold><italic>v</italic></bold><italic><sub>1</sub><sup>intra</sup></italic>, <bold><italic>v</italic></bold><italic><sub>2</sub><sup>intra</sup></italic>, …, <bold><italic>v</italic></bold><italic><sub>N</sub><sup>intra</sup></italic>. We transformed the data by applying to the class centers a change of basis matrix <italic>W<sub>inter→intra</sub></italic> that rotated each inter-class PC into the corresponding intra-class PC: W<italic><sub>inter→intra</sub></italic>=<bold><italic>v</italic></bold><italic><sub>1</sub><sup>intra</sup></italic> (<bold><italic>v</italic></bold><italic><sub>1</sub><sup>inter</sup></italic>)<sup>T</sup> + …1<bold><italic>v</italic></bold><italic><sub>N</sub><sup>intra</sup></italic> (<bold><italic>v</italic></bold><italic><sub>N</sub><sup>inter</sup></italic>)<sup>T</sup>. That is, the class centers were transformed by this matrix, but the relative positions of activity patterns within a given class were fixed. For an activation vector <bold><italic>x</italic></bold> belonging to a class c for which the average activity vector over all images of class c is <bold><italic>x</italic></bold><italic><sup>c</sup></italic>, the transformed vector was<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This transformation has the effect of preserving intra-class variance statistics exactly from the original data and preserving everything about the statistics of inter-class variance except its orientation relative to intra-class variance. That is, the transformation is designed to affect (specifically decrease) factorization while controlling for all other statistics of the activity data that may be relevant to object classification performance (considering the simulation in <xref ref-type="fig" rid="fig1">Figure 1C</xref> of two binary variables, this basis change of the neural data in <xref ref-type="fig" rid="fig2">Figure 2B</xref> is equivalent to turning a square into the maximally flat parallelogram, the degenerate one where all the points are collinear).</p></sec><sec id="s4-6"><title>Scene parameter variation</title><p>Our generated scenes consisted of foreground objects imposed upon natural backgrounds. To measure variance associated with a particular parameter like the background identity, we randomly sampled 10 different backgrounds while holding the other variables (e.g., foreground object identity and pose constant). To measure variance associated with foreground object pose, we randomly varied object angle from [–90, 90] along all three axes independently, object position on the two in-plane axes, horizontal [–30%, 30%] and vertical [–60%, 60%], and object size [×1/1.6, ×1.6]. To measure variance associated with camera position, we took crops of the image with scale uniformly varying from 20 to 100% of the image size, and position uniformly distributed across the image. To measure variance associated with lighting conditions, we applied random jitters to the brightness, contrast, saturation, and hue of an image, with jitter value bounds of [–0.4, 0.4] for brightness, contrast, and saturation and [–0.1, 0.1] for hue. These parameter choices follow standard data augmentation practices for self-supervised neural network training, as used, for example, in the SimCLR and MoCo models tested here (<xref ref-type="bibr" rid="bib22">He et al., 2020</xref>; <xref ref-type="bibr" rid="bib8">Chen et al., 2020</xref>).</p></sec><sec id="s4-7"><title>Factorization and invariance metrics</title><p>Factorization and invariance were measured according to the following equations:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">r</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:msub><mml:mi mathvariant="normal">r</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Variance induced by a parameter (<italic>var<sub>param</sub></italic>) is computed by measuring the variance (summed across all dimensions of neural activity space) of neural responses to the 10 augmented versions of a base image where the augmentations are those obtained by varying the parameter of interest. This quantity is then averaged across the 100 base images. The variance induced by all parameters is simply the sum of the variances across all images and augmentations. To define the ‘other-parameter subspace,’ we averaged neural responses for a given base image over all augmentations using the parameter of interest, and ran PCA on the resulting set of averaged responses. The subspace was defined as the space spanned by top PCA components containing 90% of the variance of these responses. Intuitively, this space captures the bulk of the variance driven by all parameters other than the parameter of interest (due to the averaging step). The variance of the parameter of interest <italic>within</italic> this ‘other-parameter subspace,’ <italic>var<sub>param|other_param_subspace</sub></italic>, was computed the same way as <italic>var<sub>param</sub></italic> but using the projections of neural activity responses onto the other-parameter subspace. In the main text, we refer to this method of computing factorization as PCA-based factorization.</p><p>We also considered an alternative definition of factorization referred to as covariance-based factorization. In this alternative definition, we measured the covariance matrices <italic>cov<sub>param</sub></italic> and <italic>cov<sub>other_param</sub></italic> induced by varying (in the same fashion as above) the parameter of interest, and all other parameters. Factorization was measured by the following equation:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">v</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">v</mml:mi><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">v</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">v</mml:mi><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace linebreak="newline"/><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">v</mml:mi><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">v</mml:mi><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>This is equal to 1 minus the dot product between the normalized, flattened covariance matrices, and thus covariance-based factorization is a measure of the discrepancy of the covariance structure induced by the parameter of interest and other parameters. The main findings were unaffected by our choice of method for computing the factorization metric, whether PCA or covariance based (<xref ref-type="fig" rid="fig5">Figures 5</xref>—<xref ref-type="fig" rid="fig7">7</xref>). An advantage of the PCA-based method is that as an intermediate one recovers the linear subspaces containing parameter variance, but in so doing requires an arbitrary choice of the explained variance threshold used to choose the number of PCs. By contrast, the covariance-based method is more straightforward to compute and has no free parameters. Thus, these two metrics are complementary and somewhat analogous in methodology to two metrics commonly used for measuring dimensionality (the number of components needed to explain a certain fraction of the variance, analogous to our original PCA-based definition, and the participation ratio, analogous to our covariance-based definition) (<xref ref-type="bibr" rid="bib12">Ding and Glanzman, 2010</xref>; <xref ref-type="bibr" rid="bib40">Litwin-Kumar et al., 2017</xref>).</p></sec><sec id="s4-8"><title>Natural movie factorization metrics</title><p>For natural movies, variance is not induced by explicit control of a parameter as in our synthetic scenes but implicitly, by considering contiguous frames (separated by 200 ms in real time) as reflective of changes in one of two motion parameters (object versus observer motion) depending on how stationary the observer is (MIT Moments in Time movie set: stationary observer; UT-Austin Egocentric movie set: nonstationary) (<xref ref-type="bibr" rid="bib37">Lee et al., 2012</xref>; <xref ref-type="bibr" rid="bib42">Monfort, 2019</xref>). Here, the <italic>all parameters</italic> condition is simply the variance across all movie frames, which in the case of MIT Moments in Time dataset includes variance across thousands of video clips taken in many different settings and in the case of the UT-Austin Egocentric movie dataset includes variance across only four movies but over long durations of time during which an observer translates extensively in an environment (3–5 hr). Thus, movie clips in the MIT Moments in Time movie set contained new scenes with different object identities, backgrounds, and lightings and thus effectively captured variance induced by these non-spatial parameters (<xref ref-type="bibr" rid="bib42">Monfort, 2019</xref>). In the UT-Austin Egocentric movie set, new objects and backgrounds are encountered as the subject navigates around the urban landscape (<xref ref-type="bibr" rid="bib37">Lee et al., 2012</xref>).</p></sec><sec id="s4-9"><title>Model neural encoding fits</title><p>Linear mappings between model features and neuron (or voxel) responses were computed using ridge regression (with regularization coefficient selected by cross-validation) on a low-dimensional linear projection of model features (top 300 PCA components computed using images in each dataset). We also tested an alternative approach to measuring representational similarity between models and experimental data based on representational similarity analysis (<xref ref-type="bibr" rid="bib34">Kriegeskorte and Kievit, 2013</xref>), computing dot product similarities of the representations of all pairs of images and measuring the Spearman correlation coefficient between these pairwise similarity matrices obtained from a given model and neural dataset, respectively.</p></sec><sec id="s4-10"><title>Model behavioral signatures</title><p>We followed the approach of <xref ref-type="bibr" rid="bib46">Rajalingham et al., 2018</xref>. We took human and macaque behavioral data from the object classification task and used it to create signatures of image-level difficulty (the ‘I1’ vector) and image-by-distractor-object confusion rates (the ‘I2’ matrix). We did the same for the DNN models, extracting model ‘behavior’ by training logistic regression classifiers to classify object identity in the same image dataset used in the experiments of <xref ref-type="bibr" rid="bib46">Rajalingham et al., 2018</xref>, using model layer activations as inputs. Model behavioral accuracy rates on image by distractor object pairs were assessed using the classification probabilities output by the logistic regression model, and these were used to compute I1 and I2 metrics as was done for the true behavioral data. Behavioral similarity between models and data was assessed by measuring the correlation between the entries of the I1 vectors and I2 matrices (both I1 and I2 results are reported).</p></sec><sec id="s4-11"><title>Model layer choices</title><p>The scatter plots in <xref ref-type="fig" rid="fig4">Figure 4A and B</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s3">3</xref> use metrics (factorization, invariance, and goodness of neural fit) taken from the final representational layer of the network (the layer prior to the logits layer used for classification in supervised network, prior to the embedding head in contrastive learning models, or prior to any auxiliary task-specific layers in unsupervised models trained using auxiliary tasks). However, representational geometries of model activations, and their match to neural activity and behavior, vary across layers. This variability arises because different model layers correspond to different stages of processing in the model (convolutional layers in some cases, and pooling operations in others), and may even have different dimensionalities. To ensure that our results do not depend on idiosyncrasies of representations in one particular model layer and the particular network operations that precede it, summary correlation statistics in all other figures (<xref ref-type="fig" rid="fig5">Figures 5</xref>—<xref ref-type="fig" rid="fig7">7</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s3">3</xref>) show the results of the analysis in question averaged over the five final representational layers of the model. That is, the metrics of interest (factorization, invariance, neural encoding fits, RDM correlation, behavioral similarity scores) were computed independently for each of the five final representational layers of each model, and these five values were averaged prior to computing correlations between different metrics.</p></sec><sec id="s4-12"><title>Correlation of model predictions and experimental data</title><p>A Spearman linear correlation coefficient was calculated for each model layer by biological dataset combination (six monkey datasets and six human datasets). Here, we do not correct for noise in the biological data when computing the correlation coefficient as this would require trial repeats (for computing intertrial variability) that were limited or not available in the fMRI data used. In any event, normalizing by the data noise ceiling applies a uniform scaling to all model prediction scores and does not affect model comparison, which only depends on ranking models as being relatively better or worse in predicting brain data. Finally, we estimated the effectiveness of model factorization, invariance, or dimensionality in combination with model object classification performance for predicting model neural and behavioral fit by performing a linear regression on the particular dual metric combination (e.g., classification plus object pose factorization) and reporting the Spearman correlation coefficient of the linearly weighted metric combination. The correlation was assessed on held-out models (80% used for training, 20% for testing), and the results were averaged over 100 randomly sampled train/test splits.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Investigation, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Tables of datasets and models used.</title><p>(a) Table of datasets used for measuring similarity of models to the brain. Datasets from both macaque and human high-level visual cortex as well as high-level visual behavior were collated for testing the brainlikeness of computational models. For neural and fMRI datasets, the features in the model were used to predict the image-by-image response pattern of each neuron or voxel. For behavior datasets, the performance of linear decoders built atop model representations were compared to performance per image of macaques and humans. (b) Table of models tested. For each model, we measured representational factorization and invariance in each of the final five layers of the model as well as evaluating their brainlikeness using the datasets in (a).</p></caption><media xlink:href="elife-91685-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91685-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Publicly available datasets and models were used. Analysis code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/issalab/Lindsey-Issa-Factorization">https://github.com/issalab/Lindsey-Issa-Factorization</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib28">Issa, 2024</xref>).</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Gallant</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><data-title>fMRI of human visual areas in response to natural images</data-title><source>Collaborative Research in Computational Neuroscience</source><pub-id pub-id-type="doi">10.6080/K0QN64NG</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>G</given-names></name><name><surname>Horikawa</surname><given-names>T</given-names></name><name><surname>Majima</surname><given-names>K</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Deep Image Reconstruction</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds001506.v1.3.1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was performed on the Columbia Zuckerman Institute Axon GPU cluster and via generous access to Cloud TPUs from Google’s TPU Research Cloud (TRC). JWL was supported by the DOE CSGF (DE-SC0020347). EBI was supported by a Klingenstein-Simons fellowship, Sloan Foundation fellowship, and Grossman-Kavli Scholar Award. We thank Erica Shook for comments on a previous version of the manuscript. The authors declare no competing interests.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Alleman</surname><given-names>M</given-names></name><name><surname>Lindsey</surname><given-names>JW</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Task Structure and Nonlinearity Jointly Determine Learned Representational Geometry</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2401.13558">https://arxiv.org/abs/2401.13558</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernardi</surname><given-names>S</given-names></name><name><surname>Benna</surname><given-names>MK</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Munuera</surname><given-names>J</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Salzman</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The geometry of abstraction in the hippocampus and prefrontal cortex</article-title><source>Cell</source><volume>183</volume><fpage>954</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.09.031</pub-id><pub-id pub-id-type="pmid">33058757</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyle</surname><given-names>LM</given-names></name><name><surname>Posani</surname><given-names>L</given-names></name><name><surname>Irfan</surname><given-names>S</given-names></name><name><surname>Siegelbaum</surname><given-names>SA</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Tuned geometries of hippocampal representations meet the computational demands of social memory</article-title><source>Neuron</source><volume>112</volume><fpage>1358</fpage><lpage>1371</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2024.01.021</pub-id><pub-id pub-id-type="pmid">38382521</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Pinto</surname><given-names>N</given-names></name><name><surname>Ardila</surname><given-names>D</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003963</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003963</pub-id><pub-id pub-id-type="pmid">25521294</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Caron</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.09882">https://arxiv.org/abs/2006.09882</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Caron</surname><given-names>M</given-names></name><name><surname>Bojanowski</surname><given-names>P</given-names></name><name><surname>Joulin</surname><given-names>A</given-names></name><name><surname>Douze</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Deep Clustering for Unsupervised Learning of Visual Features</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.05520">https://arxiv.org/abs/1807.05520</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The code for facial identity in the primate brain</article-title><source>Cell</source><volume>169</volume><fpage>1013</fpage><lpage>1028</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.05.011</pub-id><pub-id pub-id-type="pmid">28575666</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Simple Framework for Contrastive Learning of Visual Representations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>DD</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Classification and geometry of general perceptual manifolds</article-title><source>Physical Review X</source><volume>8</volume><elocation-id>031003</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.8.031003</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Conwell</surname><given-names>C</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>What Can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.03.28.485868</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>M</given-names></name><name><surname>Glanzman</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>The Dynamic Brain Interactions between Intrinsic and Stimulus-Evoked Activity in Recurrent Neural Networks</source><publisher-name>Oxford Academic</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195393798.001.0001</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Doersch</surname><given-names>C</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Efros</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unsupervised Visual Representation Learning by Context Prediction</article-title><conf-name>IEEE International Conference on Computer Vision</conf-name><fpage>1422</fpage><lpage>1430</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2015.167</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Large scale adversarial representation learning</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Eastwood</surname><given-names>C</given-names></name><name><surname>Williams</surname><given-names>CKI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A framework for the quantitative evaluation of disentangled representations</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Elmoznino</surname><given-names>E</given-names></name><name><surname>Bonner</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>High-performing neural network models of visual cortex benefit from high latent dimensionality</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.07.13.499969</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>What is the goal of sensory coding?</article-title><source>Neural Computation</source><volume>6</volume><fpage>559</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1162/neco.1994.6.4.559</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A face feature space in the macaque temporal lobe</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1187</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1038/nn.2363</pub-id><pub-id pub-id-type="pmid">19668199</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id><pub-id pub-id-type="pmid">21051642</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Gkioxari</surname><given-names>G</given-names></name><name><surname>Dollar</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mask R-CNN</article-title><conf-name>2017 IEEE International Conference on Computer Vision</conf-name><fpage>2980</fpage><lpage>2988</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.322</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Fan</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Momentum contrast for unsupervised visual representation learning</article-title><conf-name>IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><fpage>9729</fpage><lpage>9738</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00975</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hénaff</surname><given-names>OJ</given-names></name><name><surname>Goris</surname><given-names>RLT</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Perceptual straightening of natural videos</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>984</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0377-4</pub-id><pub-id pub-id-type="pmid">31036946</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hénaff</surname><given-names>OJ</given-names></name><name><surname>Bai</surname><given-names>Y</given-names></name><name><surname>Charlton</surname><given-names>JA</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Goris</surname><given-names>RLT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Primary visual cortex straightens natural video trajectories</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>5982</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25939-z</pub-id><pub-id pub-id-type="pmid">34645787</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Towards a Definition of Disentangled Representations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1812.02230">https://arxiv.org/abs/1812.02230</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>I</given-names></name><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Langston</surname><given-names>V</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Tsao</surname><given-names>D</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unsupervised deep learning identifies semantic disentanglement in single inferotemporal face patch neurons</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6456</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26751-5</pub-id><pub-id pub-id-type="pmid">34753913</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Explicit information for category-orthogonal object properties increases along the ventral stream</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>613</fpage><lpage>622</lpage><pub-id pub-id-type="doi">10.1038/nn.4247</pub-id><pub-id pub-id-type="pmid">26900926</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Issa</surname><given-names>EB</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Lindsey-Issa-Factorization</data-title><version designator="swh:1:rev:0df67f8c65db6ab3c1fd2cafdf1505116a303c0d">swh:1:rev:0df67f8c65db6ab3c1fd2cafdf1505116a303c0d</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e248b4c30b948690ed90363530d767a9f2bd05cd;origin=https://github.com/issalab/Lindsey-Issa-Factorization;visit=swh:1:snp:0262a6231a9214d953af4a03e9e4eb2e067383fe;anchor=swh:1:rev:0df67f8c65db6ab3c1fd2cafdf1505116a303c0d">https://archive.softwareheritage.org/swh:1:dir:e248b4c30b948690ed90363530d767a9f2bd05cd;origin=https://github.com/issalab/Lindsey-Issa-Factorization;visit=swh:1:snp:0262a6231a9214d953af4a03e9e4eb2e067383fe;anchor=swh:1:rev:0df67f8c65db6ab3c1fd2cafdf1505116a303c0d</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnston</surname><given-names>WJ</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Abstract representations emerge naturally in neural networks trained to perform multiple tasks</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>1040</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-36583-0</pub-id><pub-id pub-id-type="pmid">36823136</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Identifying natural images from human brain activity</article-title><source>Nature</source><volume>452</volume><fpage>352</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nature06713</pub-id><pub-id pub-id-type="pmid">18322462</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Mnih</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Disentangling by Factorising</article-title><conf-name>Proceedings of the 35th International Conference on Machine Learning</conf-name><fpage>2649</fpage><lpage>2658</lpage></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A new neural framework for visuospatial processing</article-title><source>Nature Reviews. Neuroscience</source><volume>12</volume><fpage>217</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1038/nrn3008</pub-id><pub-id pub-id-type="pmid">21415848</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The ventral visual pathway: an expanded neural framework for the processing of object quality</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>26</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.10.011</pub-id><pub-id pub-id-type="pmid">23265839</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id><pub-id pub-id-type="pmid">23876494</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep Convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lafer-Sousa</surname><given-names>R</given-names></name><name><surname>Conway</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Parallel, multi-stage processing of colors, faces and shapes in macaque inferior temporal cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1870</fpage><lpage>1878</lpage><pub-id pub-id-type="doi">10.1038/nn.3555</pub-id><pub-id pub-id-type="pmid">24141314</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>YJ</given-names></name><name><surname>Ghosh</surname><given-names>J</given-names></name><name><surname>Grauman</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Discovering important people and objects for egocentric video summarization</article-title><conf-name>2012 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>1346</fpage><lpage>1353</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2012.6247820</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Bondar</surname><given-names>IV</given-names></name><name><surname>Giese</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title><source>Nature</source><volume>442</volume><fpage>572</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1038/nature04951</pub-id><pub-id pub-id-type="pmid">16862123</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Linsley</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Performance-Optimized Deep Neural Networks Are Evolving into Worse Models of Inferotemporal Visual Cortex</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2306.03779">https://arxiv.org/abs/2306.03779</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal degrees of synaptic connectivity</article-title><source>Neuron</source><volume>93</volume><fpage>1153</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.030</pub-id><pub-id pub-id-type="pmid">28215558</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Simple learned weighted sums of inferior temporal neuronal firing rates accurately predict human core object recognition performance</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>13402</fpage><lpage>13418</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5181-14.2015</pub-id><pub-id pub-id-type="pmid">26424887</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Monfort</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Moments in Time Dataset: One Million Videos for Event Understanding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1801.03150">https://arxiv.org/abs/1801.03150</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Nayebi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Goal-driven recurrent neural network models of the ventral visual stream</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.02.17.431717</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nonaka</surname><given-names>S</given-names></name><name><surname>Majima</surname><given-names>K</given-names></name><name><surname>Aoki</surname><given-names>SC</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Brain hierarchy score: Which deep neural networks are hierarchically brain-like?</article-title><source>iScience</source><volume>24</volume><elocation-id>103013</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2021.103013</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>B</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Capturing the objects of vision with neural networks</article-title><source>Nature Human Behaviour</source><volume>5</volume><fpage>1127</fpage><lpage>1144</lpage><pub-id pub-id-type="doi">10.1038/s41562-021-01194-6</pub-id><pub-id pub-id-type="pmid">34545237</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7255</fpage><lpage>7269</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0388-18.2018</pub-id><pub-id pub-id-type="pmid">30006365</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Both increase as visual information propagates from cortical area V4 to IT</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>12978</fpage><lpage>12995</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Balanced increases in selectivity and tolerance produce constant sparseness along the ventral visual stream</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>10170</fpage><lpage>10182</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6125-11.2012</pub-id><pub-id pub-id-type="pmid">22836252</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Geiger</surname><given-names>F</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Brain-Score: Which Artificial Neural Network for Object Recognition Is Most Brain-Like?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/407007</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>G</given-names></name><name><surname>Horikawa</surname><given-names>T</given-names></name><name><surname>Majima</surname><given-names>K</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep image reconstruction from human brain activity</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006633</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006633</pub-id><pub-id pub-id-type="pmid">30640910</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorscher</surname><given-names>B</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neural representational geometry underlies few-shot concept learning</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2200800119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2200800119</pub-id><pub-id pub-id-type="pmid">36251997</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Y</given-names></name><name><surname>Krishnan</surname><given-names>D</given-names></name><name><surname>Isola</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Contrastive Multiview Coding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.05849">https://arxiv.org/abs/1906.05849</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Toosi</surname><given-names>T</given-names></name><name><surname>Issa</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Brain-like representational straightening of natural movies in robust feedforward neural networks</article-title><conf-name>The Eleventh International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cortical region consisting entirely of face-selective cells</article-title><source>Science</source><volume>311</volume><fpage>670</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1126/science.1119983</pub-id><pub-id pub-id-type="pmid">16456083</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaziri</surname><given-names>S</given-names></name><name><surname>Carlson</surname><given-names>ET</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A channel for 3D environmental shape in anterior inferotemporal cortex</article-title><source>Neuron</source><volume>84</volume><fpage>55</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.043</pub-id><pub-id pub-id-type="pmid">25242216</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91685.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>École Normale Supérieure - PSL</institution><country>France</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>The study makes a <bold>valuable</bold> empirical contribution to our understanding of visual processing in primates and deep neural networks, with a specific focus on the concept of factorization. The analyses provide <bold>convincing</bold> evidence that high factorization scores are correlated with neural predictivity. This work will be of interest to systems neuroscientists studying vision and could inspire further research that ultimately may lead to better models of or a better understanding of the brain.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91685.3.sa1</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The dominant paradigm in the past decade for modeling the ventral visual stream's response to images has been to train deep neural networks on object classification tasks and regress neural responses from units of these networks. While object classification performance is correlated to variance explained in the neural data, this approach has recently hit a plateau of variance explained, beyond which increases in classification performance do not yield improvements in neural predictivity. This suggests that classification performance may not be a sufficient objective for building better models of the ventral stream. Lindsey &amp; Issa study the role of factorization in predicting neural responses to images, where factorization is the degree to which variables such as object pose and lighting are represented independently in orthogonal subspaces. They propose factorization as a candidate objective for breaking through the plateau suffered by models trained only on object classification. They show the degree of factorization in a model captures aspects of neural variance that classification accuracy alone does not capture, hence factorization may be an objective that could lead to better models of ventral stream. I think the most important figure for a reader to see is Fig. 6.</p><p>Strengths:</p><p>This paper challenges the dominant approach to modeling neural responses in the ventral stream, which itself is valuable for diversifying the space of ideas.</p><p>This paper uses a wide variety of datasets, spanning multiple brain areas and species. The results are consistent across the datasets, which is a great sign of robustness.</p><p>The paper uses a large set of models from many prior works. This is impressively thorough and rigorous.</p><p>The authors are very transparent, particularly in the supplementary material, showing results on all datasets. This is excellent practice.</p><p>Weaknesses:</p><p>The authors have addressed many of the weaknesses in the original review. The weaknesses that remain are limitations of the work that cannot be easily addressed. In addition to the limitations stated at the end of the discussion, I'll add two:</p><p>(1) This work shows that factorization is correlated with neural similarity, and notably explains some variance in neural similarity that classification accuracy does not explain. This suggests that factorization could be used as an objective (along with classification accuracy) to build better models of the brain. However, this paper does not do that - using factorization to build better models of the brain is left to future work.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91685.3.sa2</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Object classification serves as a vital normative principle in both the study of the primate ventral visual stream and deep learning. Different models exhibit varying classification performances and organize information differently. Consequently, a thriving research area in computational neuroscience involves identifying meaningful properties of neural representations that act as bridges connecting performance and neural implementation. In the work of Lindsey and Issa, the concept of factorization is explored, which has strong connections with emerging concepts like disentanglement [1,2,3] and abstraction [4,5]. Their primary contributions encompass two facets: (1) The proposition of a straightforward method for quantifying the degree of factorization in visual representations. (2) A comprehensive examination of this quantification through correlation analysis across deep learning models.</p><p>To elaborate, their methodology, inspired by prior studies [6], employs visual inputs featuring a foreground object superimposed onto natural backgrounds. Four types of scene variables, such as object pose, are manipulated to induce variations. To assess the level of factorization within a model, they systematically alter one of the scene variables of interest and estimate the proportion of encoding variances attributable to the parameter under consideration.</p><p>The central assertion of this research is that factorization represents a normative principle governing biological visual representation. The authors substantiate this claim by demonstrating an increase in factorization from macaque V4 to IT, supported by evidence from correlated analyses revealing a positive correlation between factorization and decoding performance. Furthermore, they advocate for the inclusion of factorization as part of the objective function for training artificial neural networks. To validate this proposal, the authors systematically conduct correlation analyses across a wide spectrum of deep neural networks and datasets sourced from human and monkey subjects. Specifically, their findings indicate that the degree of factorization in a deep model positively correlates with its predictability concerning neural data (i.e., goodness of fit).</p><p>Strengths:</p><p>The primary strength of this paper is the authors' efforts in systematically conducting analysis across different organisms and recording methods. Also, the definition of factorization is simple and intuitive to understand.</p><p>Weaknesses:</p><p>Comments on revised version:</p><p>I thank the authors for addressing the weaknesses I brought up regarding the manuscript.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91685.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lindsey</surname><given-names>Jack W</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Issa</surname><given-names>Elias B</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment</bold></p><p>The study makes a valuable empirical contribution to our understanding of visual processing in primates and deep neural networks, with a specific focus on the concept of factorization. The analyses provide solid evidence that high factorization scores are correlated with neural predictivity, yet more evidence would be needed to show that neural responses show factorization. Consequently, while several aspects require further clarification, in its current form this work is interesting to systems neuroscientists studying vision and could inspire further research that ultimately may lead to better models of or a better understanding of the brain.</p><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>The paper investigates visual processing in primates and deep neural networks (DNNs), focusing on factorization in the encoding of scene parameters. It challenges the conventional view that object classification is the primary function of the ventral visual stream, suggesting instead that the visual system employs a nuanced strategy involving both factorization and invariance. The study also presents empirical findings suggesting a correlation between high factorization scores and good neural predictivity.</p><p>Strengths:</p><p>(1) Novel Perspective: The paper introduces a fresh viewpoint on visual processing by emphasizing the factorization of non-class information.</p><p>(2) Methodology: The use of diverse datasets from primates and humans, alongside various computational models, strengthens the validity of the findings.</p><p>(3) Detailed Analysis: The paper suggests metrics for factorization and invariance, contributing to a future understanding &amp; measurements of these concepts.</p><p>Weaknesses:</p><p>(1) Vagueness (Perceptual or Neural Invariance?): The paper uses the term 'invariance', typically referring to perceptual stability despite stimulus variability [1], as the complete discarding of nuisance information in neural activity. This oversimplification overlooks the nuanced distinction between perceptual invariance (e.g., invariant object recognition) and neural invariance (e.g., no change in neural activity). It seems that by 'invariance' the authors mean 'neural' invariance (rather than 'perceptual' invariance) in this paper, which is vague. The paper could benefit from changing what is called 'invariance' in the paper to 'neural invariance' and distinguish it from 'perceptual invariance,' to avoid potential confusion for future readers. The assignment of 'compact' representation to 'invariance' in Figure 1A is misleading (although it can be addressed by the clarification on the term invariance). [1] DiCarlo JJ, Cox DD. Untangling invariant object recognition. Trends in cognitive sciences. 2007 Aug 1;11(8):333-41.</p></disp-quote><p>Thanks for pointing out this ambiguity. In our Introduction we now explicitly clarify that we use “invariance” to refer to neural, rather than perceptual invariance, and we point out that both factorization and (neural) invariance may be useful for obtaining behavioral/perceptual invariance.</p><disp-quote content-type="editor-comment"><p>(2) Details on Metrics: The paper's explanation of factorization as encoding variance independently or uncorrelatedly needs more justification and elaboration. The definition of 'factorization' in Figure 1B seems to be potentially misleading, as the metric for factorization in the paper seems to be defined regardless of class information (can be defined within a single class). Does the factorization metric as defined in the paper (orthogonality of different sources of variation) warrant that responses for different object classes are aligned/parallel like in 1B (middle)? More clarification around this point could make the paper much richer and more interesting.</p></disp-quote><p>Our factorization metric measures the degree to which two sets of scene variables are factorized from one another. In the example of Fig. 1B, we apply this definition to the case of factorization of class vs. non-class information. Elsewhere in the paper we measure factorization of several other quantities unrelated to class, specifically camera viewpoint, lighting conditions, background content, and object pose. In our revised manuscript we have clarified the exposition surrounding Fig. 1B to make it clear that factorization, as we define it, can be applied to other quantities as well and that responses do not need to be aligned/parallel but simply live in a different set of dimensions whether linearly or nonlinearly arranged. Thanks for raising the need to clarify this point.</p><disp-quote content-type="editor-comment"><p>(3) Factorization vs. Invariance: Is it fair to present invariance vs. factorization as mutually exclusive options in representational hypothesis space? Perhaps a more fair comparison would be factorization vs. object recognition, as it is possible to have different levels of neural variability (or neural invariance) underlying both factorization and object recognition tasks.</p></disp-quote><p>We do not mean to imply that factorization and invariance are mutually exclusive, or that they fully characterize the space of possible representations. However, they are qualitatively distinct strategies for achieving behavioral capabilities like object recognition. In the revised manuscript we also include a comparison to object classification performance (Figures 5C &amp; S4, black x’s) as a predictor of brain-like representations, alongside the results for factorization and invariance.</p><p>In our revised Introduction and beginning of the Results section, we make it more clear that factorization and invariance are not mutually exclusive – indeed, our results show that both factorization and invariance for some scene variables like lighting and background identity are signatures of brain-like representations. Our study focuses on factorization because we believe its importance has not been studied or highlighted to the degree that invariance to “nuisance” parameters has in concert with selectivity to object identity in individual neuron tuning functions. Moreover, the loss functions used for supervised training functions of neural networks for image classification would seem to encourage invariance as a representational strategy. Thus, the finding that factorization of scene parameters is an equally good if not better predictor of brain-like representations may motivate new objective functions for neural network training.</p><disp-quote content-type="editor-comment"><p>(4) Potential Confounding Factors in Empirical Findings: The correlation observed in Figure 3 between factorization and neural predictivity might be influenced by data dimensionality, rather than factorization per se [2]. Incorporating discussions around this recent finding could strengthen the paper.</p><p>[2] Elmoznino E, Bonner MF. High-performing neural network models of the visual cortex benefit from high latent dimensionality. bioRxiv. 2022 Jul 13:2022-07.</p></disp-quote><p>We thank the Reviewer for pointing out this important, potential confound and the need for a direct quantification. We have now included an analysis computing how well dimensionality (measured using the participation ratio metric for natural images, as was done in [2] Elmoznino&amp; Bonner bioRxiv. 2022) can account for model goodness-of-fit (additional pink bars in Figure 6). Factorization of scene parameters appears to add more predictive power than dimensionality on average (Figure 6, light shaded bars), and critically, factorization+classification jointly predict goodness-of-fit significantly better than dimensionality+classification for V4 and IT/HVC brain areas (Figure 6, dark shaded bars). Indeed, dimensionality+classification is only slightly more predictive than classification alone for V4 and IT/HVC indicating some redundancy in those measures with respect to neural predictivity of models (Figure 6, compare dark shaded pink bar to dashed line).</p><p>That said, high-dimensional representations can, in principle, better support factorization, and thus we do not regard these two representational strategies necessarily in competition. Rather, our results suggest (consistent with [2]) that dimensionality is predictive of brain-like representation to some degree, such that some (but not all) of factorization’s predictive power may indeed owe to a partial correlation with dimensionality. We elaborate in the Discussion where this point comes up and now refer to the updated Figure 6 that shows the control for dimensionality.</p><disp-quote content-type="editor-comment"><p>Conclusion:</p><p>The paper offers insightful empirical research with useful implications for understanding visual processing in primates and DNNs. The paper would benefit from a more nuanced discussion of perceptual and neural invariance, as well as a deeper discussion of the coexistence of factorization, recognition, and invariance in neural representation geometry. Additionally, addressing the potential confounding factors in the empirical findings on the correlation between factorization and neural predictivity would strengthen the paper's conclusions.</p></disp-quote><p>Taken together, we hope that the changes described above address the distinction between neural and perceptual invariance, provide a more balanced understanding of the contributions of factorization, invariance, and local representational geometry, and rule against dimensionality for natural images as contributing to the main finding of the benefits from factorization of scene parameters.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The dominant paradigm in the past decade for modeling the ventral visual stream's response to images has been to train deep neural networks on object classification tasks and regress neural responses from units of these networks. While object classification performance is correlated to the variance explained in the neural data, this approach has recently hit a plateau of variance explained, beyond which increases in classification performance do not yield improvements in neural predictivity. This suggests that classification performance may not be a sufficient objective for building better models of the ventral stream. Lindsey &amp; Issa study the role of factorization in predicting neural responses to images, where factorization is the degree to which variables such as object pose and lighting are represented independently in orthogonal subspaces. They propose factorization as a candidate objective for breaking through the plateau suffered by models trained only on object classification.</p><p>They claim that (i) maintaining these non-class variables in a factorized manner yields better neural predictivity than ignoring non-class information entirely, and (ii) factorization may be a representational strategy used by the brain.</p><p>The first of these claims is supported by their data. The second claim does not seem well-supported, and the usefulness of their observations is not entirely clear.</p><p>Strengths:</p><p>This paper challenges the dominant approach to modeling neural responses in the ventral stream, which itself is valuable for diversifying the space of ideas.</p><p>This paper uses a wide variety of datasets, spanning multiple brain areas and species. The results are consistent across the datasets, which is a great sign of robustness.</p><p>The paper uses a large set of models from many prior works. This is impressively thorough and rigorous.</p><p>The authors are very transparent, particularly in the supplementary material, showing results on all datasets. This is excellent practice.</p><p>Weaknesses:</p><p>(1) The primary weakness of this paper is a lack of clarity about what exactly is the contribution. I see two main interpretations: (1-A) As introducing a heuristic for predicting neural responses that improve over-classification accuracy, and (1-B) as a model of the brain's representational strategy. These two interpretations are distinct goals, each of which is valuable. However, I don't think the paper in its current form supports either of them very well:</p><p>(1-A) Heuristic for neural predictivity. The claim here is that by optimizing for factorization, we could improve models' neural predictivity to break through the current predictivity plateau. To frame the paper in this way, the key contribution should be a new heuristic that correlates with neural predictivity better than classification accuracy. The paper currently does not do this. The main piece of evidence that factorization may yield a more useful heuristic than classification accuracy alone comes from Figure 5. However, in Figure 5 it seems that factorization along some factors is more useful than others, and different linear combinations of factorization and classification may be best for different data. There is no single heuristic presented and defended. If the authors want to frame this paper as a new heuristic for neural predictivity, I recommend the authors present and defend a specific heuristic that others can use, e.g. [K * factorization_of_pose + classification] for some constant K, and show that (i) this correlates with neural predictivity better than classification alone, and (ii) this can be used to build models with higher neural predictivity. For (ii), they could fine-tune a state-of-the-art model to improve this heuristic and show that doing so achieves a new state-of-the-art neural predictivity. That would be convincing evidence that their contribution is useful.</p></disp-quote><p>Our paper does not make any strong claim regarding the Reviewer’s point 1-A (on heuristics for neural predictivity). In the Discussion, last paragraph, we better specify that our work is merely suggestive of claim 1-A about heuristics for more neurally predictive, more brainlike models. We believe that our paper supports the Reviewer’s point 1-B (on brain representation) as we discuss below.</p><p>We leave it to future work to determine if factorization could help optimize models to be more brainlike. This treatment may require exploration of novel model architectures and loss functions, and potentially also more thorough neural datasets that systematically vary many different forms of visual information for validating any new models.</p><p>(1-B) Model of representation in the brain. The claim here is that factorization is a general principle of representation in the brain. However, neural predictivity is not a suitable metric for this, because (i) neural predictivity allows arbitrary linear decoders, hence is invariant to the orthogonality requirement of factorization, and (ii) neural predictivity does not match the network representation to the brain representation. A better metric is representational dissimilarity matrices. However, the RDM results in Figure S4 actually seem to show that factorization does not do a very good job of predicting neural similarity (though the comparison to classification accuracy is not shown), which suggests that factorization may not be a general principle of the brain. If the authors want to frame the paper in terms of discovering a general principle of the brain, I suggest they use a metric (or suite of metrics) of brain similarity that is sensitive to the desiderata of factorization, e.g. doesn't apply arbitrary linear transformations, and compare to classification accuracy in addition to invariance.</p><p>We agree with the Reviewer about the shortcomings of neural predictivity for comparing representational geometries, and in our revised manuscript we have provided a more comprehensive set of results that includes RDM predictivity in new Figures 6 &amp; 7, alongside the results for neural fit predictivity. In addition, as suggested we added classification accuracy predictivity in Figures 5C &amp; S4 (black x’s) for visual comparison to factorization/invariance. In Figure S4 on RDMs, it is apparent how factorization is at least as good a predictor as classification on all V4 &amp; IT datasets from both monkeys and humans (compared x’s to filled circles in Figure S4; note that some of the points from the original Figure S4 changed as we discovered a bug in the code that specifically affected the RDM analysis for a few of the datasets).</p><p>We find that the newly included RDM analyses in Figures 6 &amp; 7 are consistent with the conclusions of the neural fit regression analyses: that the correlation of factorization metrics with RDM matches are strong, comparable in magnitude to that of classification accuracy (Figure 6, 3rd &amp; 4th columns, compare black dashed line to faded colored bars) and are not fully accounted for by the model’s classification accuracy alone (Figure 6, 3rd &amp; 4th columns, higher unfaded bars for classification combined with factorization, and see corresponding example scatters in Figure 7 middle/bottom rows).</p><p>It is encouraging that the added benefit of factorization for RDM predictivity accounting for classification performance is at least as good as the improvement seen for neural fit predictivity (Figure 6, 1st &amp; 2nd columns for encoding fits versus 3rd &amp; 4th columns for RDM correlations).</p><disp-quote content-type="editor-comment"><p>(2) I think the comparison to invariance, which is pervasive throughout the paper, is not very informative. First, it is not surprising that invariance is more weakly correlated with neural predictivity than factorization, because invariant representations lose information compared to factorized representations. Second, there has long been extensive evidence that responses throughout the ventral stream are not invariant to the factors the authors consider, so we already knew that invariance is not a good characterization of ventral stream data.</p></disp-quote><p>While we appreciate the Reviewer’s intuition that highly invariant representations are not strongly supported in the high-level visual cortex, we nevertheless thought it was valuable to put this intuition to a quantitative, detailed test. As a result, we uncovered effects that were not obvious a priori, at least to us – for example, that invariance for some scene parameters (camera view, object pose) is negatively correlated with neural predictions while invariance to others (background, lighting) is positively correlated. Thus, our work exercises the details of invariance for different types of information.</p><disp-quote content-type="editor-comment"><p>(3) The formalization of the factorization metric is not particularly elegant, because it relies on computing top K principal components for the other-parameter space, where K is arbitrarily chosen as 10. While the authors do show that in their datasets the results are not very sensitive to K (Figure S5), that is not guaranteed to be the case in general. I suggest the authors try to come up with a formalization that doesn't have arbitrary constants. For example, one possibility that comes to mind is E[delta_a x delta_b], where 'x' is the normalized cross product, delta_a, and delta_b are deltas in representation space induced by perturbations of factors a and b, and the expectation is taken over all base points and deltas. This is just the first thing that comes to mind, and I'm sure the authors can come up with something better. The literature on disentangling metrics in machine learning may be useful for ideas on measuring factorization.</p></disp-quote><p>Thanks to the Reviewer for raising this point. First, we wish to clarify a potential misunderstanding of the factorization metric: the number K of principal components we choose is not an arbitrary constant, but rather calibrated to capture a certain fraction of variance, set to 90% by default in our analyses. While this variance threshold is indeed an arbitrary hyperparameter, it has a more intuitive interpretation than the number of principal components.</p><p>Nonetheless, the Reviewer’s comment did inspire us to consider another metric for factorization that does not depend on any arbitrary parameters. In the revised version, we now include a covariance matrix based metric which simply measures the elementwise correlation of the covariance matrices induced by varying the scene parameter of interest and the covariance matrix induced by varying the other parameters (and then subtracts this quantity from 1).</p><p>Correspondingly, we now present results for both the new covariance based measure and the original PCA based one in Figures 5C, 6, and 7. The main findings remain largely the same when using the covariance based metric, and the covariance based metric (Figure 5C, compare light shaded to dark shaded filled circles; Figure 6, compare top row to bottom row; Figure 7, compare middle rows to bottom rows).</p><p>Ultimately, we believe these two metrics are complementary and somewhat analogous to two metrics commonly used for measuring dimensionality (the number of components needed to explain a certain fraction of the variance, analogous to our original PCA based definition; the participation ratio, analogous to our covariance based definition). We have added the formula for the covariance based factorization metric along with a brief description to the Methods.</p><disp-quote content-type="editor-comment"><p>(4) The authors defined the term &quot;factorization&quot; according to their metric. I think introducing this new term is not necessary and can be confusing because the term &quot;factorization&quot; is vague and used by different researchers in different ways. Perhaps a better term is &quot;orthogonality&quot;, because that is clear and seems to be what the authors' metric is measuring.</p></disp-quote><p>We agree with the Reviewer that factorization has become an overloaded term. At the same time, we think that in this context, the connotation of the term factorization effectively conveys the notion of separating out different latent sources of variance (factors) such that they can be encoded in orthogonal subspaces.</p><p>To aid clarity, we now mention in the Introduction that factorization defined here is meant to measure orthogonalization of scene factors. Additionally, in the Discussion section, we now go into more detail comparing our metric to others previously used in the literature, including orthogonality, to help put it in context.</p><disp-quote content-type="editor-comment"><p>(5) One general weakness of the factorization paradigm is the reliance on a choice of factors. This is a subjective choice and becomes an issue as you scale to more complex images where the choice of factors is not obvious. While this choice of factors cannot be avoided, I suggest the authors add two things: First, an analysis of how sensitive the results are to the choice of factors (e.g. transform the basis set of factors and re-run the metric); second, include some discussion about how factors may be chosen in general (e.g. based on temporal statistics of the world, independent components analysis, or something else).</p></disp-quote><p>The Reviewer raises a very reasonable point about the limitation of this work. While we limited our analysis to generative scene factors that we know about and that could be manipulated, there are many potential factors to consider. It is not clear to us exactly how to implement the Reviewer’s suggestion of transforming the basis set of factors, as the factors we consider are highly nonlinear in the input space. Ultimately, we believe that finding unsupervised methods to characterize the “true” set of factors that is most useful for understanding visual representations is an important subject for future work, but outside the scope of this particular study. We have added a comment to this effect in the Discussion.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>Object classification serves as a vital normative principle in both the study of the primate ventral visual stream and deep learning. Different models exhibit varying classification performances and organize information differently. Consequently, a thriving research area in computational neuroscience involves identifying meaningful properties of neural representations that act as bridges connecting performance and neural implementation. In the work of Lindsey and Issa, the concept of factorization is explored, which has strong connections with emerging concepts like disentanglement [1,2,3] and abstraction [4,5]. Their primary contributions encompass two facets: (1) The proposition of a straightforward method for quantifying the degree of factorization in visual representations. (2) A comprehensive examination of this quantification through correlation analysis across deep learning models.</p><p>To elaborate, their methodology, inspired by prior studies [6], employs visual inputs featuring a foreground object superimposed onto natural backgrounds. Four types of scene variables, such as object pose, are manipulated to induce variations. To assess the level of factorization within a model, they systematically alter one of the scene variables of interest and estimate the proportion of encoding variances attributable to the parameter under consideration.</p><p>The central assertion of this research is that factorization represents a normative principle governing biological visual representation. The authors substantiate this claim by demonstrating an increase in factorization from macaque V4 to IT, supported by evidence from correlated analyses revealing a positive correlation between factorization and decoding performance. Furthermore, they advocate for the inclusion of factorization as part of the objective function for training artificial neural networks. To validate this proposal, the authors systematically conduct correlation analyses across a wide spectrum of deep neural networks and datasets sourced from human and monkey subjects. Specifically, their findings indicate that the degree of factorization in a deep model positively correlates with its predictability concerning neural data (i.e., goodness of fit).</p><p>Strengths:</p><p>The primary strength of this paper is the authors' efforts in systematically conducting analysis across different organisms and recording methods. Also, the definition of factorization is simple and intuitive to understand.</p><p>Weaknesses:</p><p>This work exhibits two primary weaknesses that warrant attention: (i) the definition of factorization and its comparison to previous, relevant definitions, and (ii) the chosen analysis method.</p><p>Firstly, the definition of factorization presented in this paper is founded upon the variances of representations under different stimuli variations. However, this definition can be seen as a structural assumption rather than capturing the effective geometric properties pertinent to computation. More precisely, the definition here is primarily statistical in nature, whereas previous methodologies incorporate computational aspects such as deviation from ideal regressors [1], symmetry transformations [3], generalization [5], among others. It would greatly enhance the paper's depth and clarity if the authors devoted a section to comparing their approach with previous methodologies [1,2,3,4,5], elucidating any novel insights and advantages stemming from this new definition.</p><p>[1] Eastwood, Cian, and Christopher KI Williams. &quot;A framework for the quantitative evaluation of disentangled representations.&quot; International conference on learning representations. 2018.</p><p>[2] Kim, Hyunjik, and Andriy Mnih. &quot;Disentangling by factorising.&quot; International Conference on Machine Learning. PMLR, 2018.</p><p>[3] Higgins, Irina, et al. &quot;Towards a definition of disentangled representations.&quot; arXiv preprint arXiv:1812.02230 (2018).</p><p>[4] Bernardi, Silvia, et al. &quot;The geometry of abstraction in the hippocampus and prefrontal cortex.&quot; Cell 183.4 (2020): 954-967.</p><p>[5] Johnston, W. Jeffrey, and Stefano Fusi. &quot;Abstract representations emerge naturally in neural networks trained to perform multiple tasks.&quot; Nature Communications 14.1 (2023): 1040.</p></disp-quote><p>Thanks to the Reviewer for this suggestion. We agree that our initial submission did not sufficiently contextualize our definition of factorization with respect to other related notions in the literature. We have added additional discussion of these points to the Discussion section in the revised manuscript and have included therein the citations provided by the Reviewer (please see the third paragraph of Discussion).</p><disp-quote content-type="editor-comment"><p>Secondly, in order to establish a meaningful connection between factorization and computation, the authors rely on a straightforward synthetic model (Figure 1c) and employ multiple correlation analyses to investigate relationships between the degree of factorization, decoding performance, and goodness of fit. Nevertheless, the results derived from the synthetic model are limited to the low training-sample regime. It remains unclear whether the biological datasets under consideration fall within this low training-sample regime or not.</p></disp-quote><p>We agree that our model in Figure 1C is very simple and does not fully capture the complex interactions between task performance and features of representational geometry, like factorization. We intend it only as a proof of concept to illustrate how factorized representations can be beneficial for some downstream task use cases. While the benefits of factorized representations disappear for large numbers of samples in this simulation, we believe this is primarily a consequence of the simplicity and low dimensionality of the simulation. Real-world visual information is complex and high-dimensional, and as such the relevant sample size regime in which factorization offers tasks benefits may be much greater. As a first step toward this real-world setting, Figure 2 shows how decreasing the amount of factorization in neural population data in macaque V4/IT can have an effect on object identity decoding.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Missing citations: The paper could benefit from discussions &amp; references to related papers, such as:</p><p>Higgins I, Chang L, Langston V, Hassabis D, Summerfield C, Tsao D, Botvinick M. Unsupervised deep learning identifies semantic disentanglement in single inferotemporal face patch neurons. Nature communications. 2021 Nov 9;12(1):6456.</p></disp-quote><p>We have added additional discussion of related work, including the suggested reference and others on disentanglement, to the Discussion section in the revised manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Here are several small recommendations for the authors, all much more minor than those in the public review:</p><p>I suggest more use of equations in methods sections about Figure 1C and macaque neural data analysis.</p></disp-quote><p>Thanks for this suggestion. We have added new Equation 1 for the method transforming neural data to reduce factorization of a variable while preserving other firing rate statistics.</p><disp-quote content-type="editor-comment"><p>In Figure 1-C, the methods indicate that Gaussian noise was added. This is a very important detail, and complexifies the interpretation of the figure because it adds an assumption about the structure of noise. In other words, if I understand correctly, the correct interpretation of Figure 1C is &quot;assuming i.i.d. noise, decoding accuracy improves with factorization.&quot; The i.i.d. noise is a big assumption, and it is debated how well the brain satisfies this assumption. I suggest you either omit noise for this figure or clearly state in the main text (e.g. caption) that the figure must be interpreted under an i.i.d. noise assumption.</p></disp-quote><p>We have added an explicit statement of the i.i.d. noise assumption to the Figure 1C legend.</p><disp-quote content-type="editor-comment"><p>For Figure 2B, I suggest labeling the x-axis clearly below the axis on both panels. Currently, it is difficult to read, particularly in print.</p></disp-quote><p>We have made the x-axis labels more clear and included on both panels.</p><disp-quote content-type="editor-comment"><p>Figure 3A is difficult to read because of the very small task. I suggest avoiding such small fonts.</p></disp-quote><p>We agree that Figure 3A is difficult to read. We have broken out Figure 3 into two new Figures 3 &amp; 4 to increase clarity and sizing of text in Figure 3A.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>To strengthen this work, it is advisable to incorporate more comprehensive comparisons with previous research, particularly within the machine learning (ML) community. For instance, it would be beneficial to explore and reference works focusing on disentanglement [1,2,3]. This would provide valuable context and facilitate a more robust understanding of the contributions and novel insights presented in the current study.</p></disp-quote><p>We have added additional discussion of related work and other notions similar to factorization to the Discussion section in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Additionally, improving the quality of the figures is crucial to enhance the clarity of the findings:</p><list list-type="bullet"><list-item><p>Figure 2: The caption of subfigure B could be revised for greater clarity.</p></list-item></list></disp-quote><p>Thank you, we have substantially clarified this figure caption.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Figure 3: Consider a more equitable approach for computing the correlation coefficient, such as calculating it separately for different types of models. In the case of supervised models, it appears that the correlation between invariance and goodness of fit may not be negligible across various scene parameters.</p></list-item></list></disp-quote><p>We appreciate the suggestion, but we are not confident in our ability to conclude much from analyses restricted to particular model classes, given the relatively small N and the fact that the different model classes themselves are an important source of variance in our data.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Figure 4: To enhance the interpretability of subfigures A and B, it may be beneficial to include p-values (indicating confidence levels).</p></list-item></list></disp-quote><p>As we supply bootstrapped confidence intervals for our results, which provide at least as much information as p-values, and most of the effects of interest are fairly stark when comparing invariance to factorization, p-values were not needed to support our points. We added a sentence to the legend of new Figure 5 (previously Figure 4) indicating that error bars reflect standard deviations over bootstrap resampling of the models.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Figure 5: For subfigure B, it could be advantageous to plot the results solely for factorization, allowing for a clear assessment of whether the high correlation observed in Classification+Factorization arises from the combined effects of both factors or predominantly from factorization alone.</p></list-item></list></disp-quote><p>First, we clarify/note that the scatters solely for factorization that the Reviewer seeks are already presented earlier in the manuscript across all conditions in Figures 4A,B and Figure S2.</p><p>While we could also include these in new Figure 7 (previously Figure 5B) as the Reviewer suggests, we believe it would distract from the message of that figure at the end of the manuscript – which is that factorization is useful as a supplement to classification in predictive matches to neural data. Nonetheless, new Figure 6 (old Figure 5A) provides a summary quantification of the information that the reviewer requests (Fig. 6, faded colored bars reflect the contribution of factorization alone).</p></body></sub-article></article>