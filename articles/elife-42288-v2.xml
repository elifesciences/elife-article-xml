<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">42288</article-id><article-id pub-id-type="doi">10.7554/eLife.42288</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Cell Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>KymoButler, a deep learning software for automated kymograph analysis</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-85067"><name><surname>Jakobs</surname><given-names>Maximilian AH</given-names></name><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0879-7937</contrib-id><email>mj455@cam.ac.uk</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-122736"><name><surname>Dimitracopoulos</surname><given-names>Andrea</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-5685"><name><surname>Franze</surname><given-names>Kristian</given-names></name><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8425-7297</contrib-id><email>kf284@cam.ac.uk</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf3"/></contrib><aff id="aff1"><institution content-type="dept">Department of Physiology, Development and Neuroscience</institution><institution>University of Cambridge</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bassereau</surname><given-names>Patricia</given-names></name><role>Reviewing Editor</role><aff><institution>Institut Curie</institution><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Malhotra</surname><given-names>Vivek</given-names></name><role>Senior Editor</role><aff><institution>The Barcelona Institute of Science and Technology</institution><country>Spain</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>13</day><month>08</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e42288</elocation-id><history><date date-type="received" iso-8601-date="2018-09-24"><day>24</day><month>09</month><year>2018</year></date><date date-type="accepted" iso-8601-date="2019-07-11"><day>11</day><month>07</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Jakobs et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Jakobs et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-42288-v2.pdf"/><abstract><p>Kymographs are graphical representations of spatial position over time, which are often used in biology to visualise the motion of fluorescent particles, molecules, vesicles, or organelles moving along a predictable path. Although in kymographs tracks of individual particles are qualitatively easily distinguished, their automated quantitative analysis is much more challenging. Kymographs often exhibit low signal-to-noise-ratios (SNRs), and available tools that automate their analysis usually require manual supervision. Here we developed KymoButler, a Deep Learning-based software to automatically track dynamic processes in kymographs. We demonstrate that KymoButler performs as well as expert manual data analysis on kymographs with complex particle trajectories from a variety of different biological systems. The software was packaged in a web-based ‘one-click’ application for use by the wider scientific community (<ext-link ext-link-type="uri" xlink:href="https://deepmirror.ai/kymobutler">https://deepmirror.ai/kymobutler</ext-link>). Our approach significantly speeds up data analysis, avoids unconscious bias, and represents another step towards the widespread adaptation of Machine Learning techniques in biological data analysis.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Many molecules and structures within cells have to move about to do their job. Studying these movements is important to understand many biological processes, including the development of the brain or the spread of viruses.</p><p>Kymographs are images that represent the movement of particles in time and space. Unfortunately, tracing the lines that represent movement in kymographs of biological particles is hard to do automatically, so currently this analysis is done by hand. Manually annotating kymographs is tedious, time-consuming and prone to the researcher’s unconscious bias.</p><p>In an effort to simplify the analysis of kymographs, Jakobs et al. have developed KymoButler, a software tool that can do it automatically. KymoButler uses artificial intelligence to trace the lines in a kymograph and extract the information about particle movement. It speeds up analysis of kymographs by between 50 and 250 times, and comparisons show that it is as reliable as manual analysis. KymoButler is also significantly more effective than any previously existing automatic kymograph analysis programme. To make KymoButler accessible, Jakobs et al. have also created a website with a drag-and-drop facility that allows researchers to easily use the tool.</p><p>KymoButler has been tested in many areas of biological research, from quantifying the movement of molecules in neurons to analysing the dynamics of the scaffolds that help cells keep their shape. This variety of applications showcases KymoButler’s versatility, and its potential applications. Jakobs et al. are further contributing to the field of machine learning in biology with ‘deepmirror.ai’, an online hub with the goal of accelerating the adoption of artificial intelligence in biology.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>kymographs</kwd><kwd>kymograms</kwd><kwd>transport</kwd><kwd>neurons</kwd><kwd>artificial intelligence</kwd><kwd>machine learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>109145/Z/15/Z</award-id><principal-award-recipient><name><surname>Jakobs</surname><given-names>Maximilian AH</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Herchel Smith Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Dimitracopoulos</surname><given-names>Andrea</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004815</institution-id><institution>Isaac Newton Trust</institution></institution-wrap></funding-source><award-id>17.24(p)</award-id><principal-award-recipient><name><surname>Franze</surname><given-names>Kristian</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/N006402/1</award-id><principal-award-recipient><name><surname>Franze</surname><given-names>Kristian</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>772426</award-id><principal-award-recipient><name><surname>Franze</surname><given-names>Kristian</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>KymoButler is a machine learning approach towards the reliable, quick, and fully automated analysis of the dynamics of fluorescently labelled particles in living cells.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>Many processes in living cells are highly dynamic, and molecules, vesicles, and organelles diffuse or are transported along complex trajectories. Particle tracking algorithms represent powerful approaches to track the dynamics of such particles (<xref ref-type="bibr" rid="bib17">Jaqaman et al., 2008</xref>; <xref ref-type="bibr" rid="bib30">Sbalzarini and Koumoutsakos, 2005</xref>; <xref ref-type="bibr" rid="bib22">Lee and Park, 2018</xref>). However, in many scenarios particles follow a distinct pathway within cells and move much faster than the confounding cell. For example, molecules transported along neuronal axons, dendrites, or along cilia typically move along the structure’s long axis and do not show significant motion perpendicular to that path. Similarly, retrograde actin flow typically occurs along a single axis within the cell. Hence, when the cell is not moving significantly for the duration of imaging, one can define, for example manually draw, a so-called ‘stationary path’ (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) along which particles move either forwards or backwards. In these cases, kymographs provide an elegant solution to the visualisation and analysis of particle dynamics.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Kymograph generation and KymoButler.</title></caption><graphic xlink:href="elife-42288-fig1-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Example kymographs and software workflow.</title><p>(<bold>A</bold>) Three example kymographs from published manuscripts. Example 1: In vitro dynamics of single cytoplasmic dynein proteins adapted from <xref ref-type="bibr" rid="bib32">Tanenbaum et al. (2013)</xref>. Example 2: EB1-GFP labelled growing microtubule plus-ends in mouse dorsal root ganglion axons (<xref ref-type="bibr" rid="bib20">Lazarus et al., 2013</xref>). Example 3: Mitochondria dynamics in mouse retinal ganglion cell dendrites (<xref ref-type="bibr" rid="bib11">Faits et al., 2016</xref>). Each dilated coloured line depicts an identified track. (<bold>B</bold>) KymoButler software workflow. First, a classification module is applied to each kymograph to determine whether the kymograph is unidirectional or bidirectional. If the kymograph is deemed unidirectional the unidirectional segmentation module is applied to the image to generate two trackness maps that assign each pixel a score between 0–1, approximating the likelihood that this pixel is part of a track with negative slope (left image) or positive slope (right image). Subsequently, the trackness maps are binarized, skeletonised, and segmented into their respective connected components. Finally, those components are averaged over each row to generate individual tracks, and a dilated representation of each track is plotted in a random colour. If the kymograph is classified as bidirectional, another segmentation module is applied to the kymograph, which generates a trackness map that does not highlight any particular slope. This map is binarized with a user-defined threshold and subsequently skeletonised, resulting in a binary map that exhibits multiple track crossings. To resolve these crossings, we first apply a morphological operation that detects the starting points of tracks in the binary map (red dots). Then, the algorithm tracks each line from its starting point until a crossing is encountered. At each crossing, the decision module is called, whose inputs are (i) the raw kymograph in that region, (ii) the previous track skeleton, and (iii) all possible tracks in that region. The decision module then generates another trackness map that assigns high values to the most likely future path from the crossing. This map is then again binarized and thinned with a fixed threshold of 0.5. If the predicted path is longer than two pixels, the path tracking continues. Once all starting points have been tracked until an end (either no prediction or no further pixels available), the algorithm again looks for starting points in the skeletonised trackness map excluding the identified tracks, and repeats the steps outlined above until all pixels are occupied by a track. The resulting tracks are then drawn with each track in a random colour.</p></caption><graphic xlink:href="elife-42288-fig1-figsupp1-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>The software modules in detail.</title><p>(<bold>A</bold>) The class module. This module resizes any input kymograph to 64 × 64 pixels. It subsequently applies two convBlocks with no padding and 64 output feature maps to the image. ConvBlocks comprise a convolutional layer with 3 × 3 kernels followed by a BatchNormalisation Layer and a leaky <bold>Re</bold>ctified <bold>L</bold>inear <bold>U</bold>nit (ReLU) activation function (leak factor 0.1). The convBlocks are followed by 2 × 2 max pooling to halve the feature map sizes. This is repeated another two times while steadily increasing the number of feature maps until the last convBlock generates 256 feature maps of size 9 × 9. These maps are then pooled with a final 2 × 2 max pool operation followed by a 4 × 4 mean pool operation to generate a vector of 256 features. These features are then classified with a fully connected layer with output nodes followed by another leaky Ramp and finally another fully connected layer generates two output values that correspond to the probability of being a unidirectional/bidirectional kymograph. (<bold>B</bold>) The unidirectional segmentation module takes and an input kymograph of arbitrary size. Subsequently two convBlocks with 64 output feature maps are applied to the image followed by max pooling. This is repeated three times while doubling the number of feature maps with each pooling operation forming the ‘contracting path’. To obtain an image of the same size as the input image the small feature maps at the lowest level of the network have to be deconvolved four times each time halving the number of feature maps and applying further convBlocks. After each 2 × 2 deconvolution the resulting feature maps are catenated with the feature maps of the same size from the contracting path so that the network only learns residual alterations of the input image. The final 64 feature maps are linked to two independent convolutional layers that generate outputs that correspond to the trackness scores for positive and negative sloped lines. (<bold>C</bold>) The bidirectional segmentation module has the same architecture as the unidirectional one but only generates one output that corresponds to the trackness map for any lines in the image. (<bold>D</bold>) The decision module architecture is the same as the bidirectional segmentation module but takes three input images instead of one.</p></caption><graphic xlink:href="elife-42288-fig1-figsupp2-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Synthetic training data examples.</title><p>(<bold>A</bold>) Class module training data consisted of 64 × 64 pixel images that were either classified as unidirectional (example 1) or bidirectional (example 2). (<bold>B</bold>) Synthetic training data for the unidirectional segmentation module comprised 300 × 300 pixel kymographs with two binary ground truth maps, corresponding to particle motion with negative and positive slopes. (<bold>C</bold>) Synthetic bidirectional segmentation module training data comprises 300 × 300 pixel kymographs with only one ground truth image containing all ground truth tracks. (<bold>D</bold>) The decision module was trained with 48 × 48 pixel image crops of the raw kymograph, the previous skeletonised path, and all the skeletonised paths in the cropped region. The ground truth is simply the known future segment of the given path.</p></caption><graphic xlink:href="elife-42288-fig1-figsupp3-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Geometric mean of track recall and precision for different trackness thresholds.</title><p>(<bold>A</bold>) 10 synthetic unidirectional and bidirectional kymographs were analysed with varying trackness thresholds, and track recall and track precision were calculated. The geometric mean of recall and precision does not exhibit much variation between 0.1 and 0.3 but decreases at lower and higher values. Individual dots represent per kymograph values and the solid lines the binned mean.</p></caption><graphic xlink:href="elife-42288-fig1-figsupp4-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Geometric mean of track recall and precision for different signal to noise ratios and particle densities.</title><p>(<bold>A, B</bold>) The same synthetic (<bold>A</bold>) bidirectional and (<bold>B</bold>) unidirectional kymograph for three different SNR values (top). Note that some tracks become almost invisible at low SNRs. Bottom: Overlay of the tracks predicted by KymoButler (magenta, not post processed) with the ground truth (green). (<bold>C</bold>) A low SNR unidirectional/bidirectional kymograph analysed by KymoButler and manual annotation. Predicted tracks in magenta and ground truth in green. (<bold>D</bold>) The geometric mean of track recall and precision as a function of SNR. The same 10 kymographs were noised with different SNRs and the average score taken. Dots represent individual kymographs and the line the 0.1 bin average. Highlighted dots represent manually analysed kymographs. (<bold>E, F</bold>) Three example (<bold>E</bold>) bidirectional and (<bold>F</bold>) unidirectional kymographs for different particle densities (top). The percentage value gives the percentage of the image covered with signal. Bottom: Overlay of the tracks predicted by KymoButler (magenta) with the ground truth (green). (<bold>G</bold>) A high particle density unidirectional/bidirectional kymograph analysed with KymoButler and manual annotation. Predicted tracks in magenta and ground truth in green. (<bold>H</bold>) The geometric mean of track recall and precision as a function of coverage percentage. 20 kymographs were generated with varying numbers of particles. Tracks smaller than three pixels and shorter than three frames were discarded for unidirectional kymograph quantification while tracks smaller than 10 pixels and shorter than 25 frames were discarded for bidirectional kymograph quantification. Dots represent individual kymographs and the line the 5% bin average. Highlighted dots represent manually analysed kymographs.</p></caption><graphic xlink:href="elife-42288-fig1-figsupp5-v2.tif" mimetype="image" mime-subtype="tiff"/></fig></fig-group><p>To generate a kymograph, the intensity profile along the manually drawn stationary path (black dashed line in <xref ref-type="fig" rid="fig1">Figure 1A</xref>) is extracted for each frame of a time-lapse movie, and then these profiles are stacked into individual rows of an image (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). In the resulting space-time image, each (usually fluorescently) labelled particle is shown as a line, whose slope, for example, represents the velocity of that particle (<xref ref-type="fig" rid="fig1">Figure 1A</xref>).</p><p>In many biological processes, multiple particles move along the same stationary path with little to no deviations, making kymographs a very useful representation of their dynamics. Hence, kymographs have been widely employed to visualise biological processes across different length scales, ranging from diffusion and transport of single molecules to whole cell movements (<xref ref-type="bibr" rid="bib33">Twelvetrees et al., 2016</xref>; <xref ref-type="bibr" rid="bib4">Barry et al., 2015</xref>). The analysis of these kymographs only requires tracing lines in 2D images, a rather simple task compared to the more general approach of particle tracking, where one has to identify the centre of the particles in each frame, and then correctly assign these coordinates to corresponding particles across frames.</p><p>Publicly available kymograph analysis software simplifies the tedious and time-consuming task of tracing kymographs, but most of these solutions require manual supervision and/or high signal to noise ratios (<xref ref-type="bibr" rid="bib26">Neumann et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Mangeol et al., 2016</xref>; <xref ref-type="bibr" rid="bib7">Chenouard, 2010</xref>; <xref ref-type="bibr" rid="bib35">Zala et al., 2013</xref>). These tools perform reasonably well when applied to particles with unidirectional motion and uniform velocities as, for example, growing microtubule +ends (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, example 2) and F-actin dynamics in retrograde actin flow (<xref ref-type="bibr" rid="bib20">Lazarus et al., 2013</xref>; <xref ref-type="bibr" rid="bib10">del Castillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib1">Alexandrova et al., 2008</xref>; <xref ref-type="bibr" rid="bib3">Babich et al., 2012</xref>).</p><p>In many other biological contexts, however, particles can stop moving, change velocity, change direction, merge, cross each other’s path, or disappear for a few frames. The kymographs obtained from these processes exhibit ‘bidirectional’ motion (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, example 1); this category includes cellular transport processes, for example molecular or vesicle transport in neuronal axons and dendrites (<xref ref-type="bibr" rid="bib11">Faits et al., 2016</xref>; <xref ref-type="bibr" rid="bib32">Tanenbaum et al., 2013</xref>; <xref ref-type="bibr" rid="bib19">Koseki et al., 2017</xref>). Thus, the problem of automatically and reliably tracking dynamic processes in kymographs still leaves substantial room for improvement, and given the limitations of currently available kymograph analysis software, most kymographs are still analysed by hand, which is slow and prone to unconscious bias.</p><p>In recent years, Machine Learning (ML), and particularly Deep Neural Networks, have been very successfully introduced to data processing in biology and medicine (<xref ref-type="bibr" rid="bib24">Mathis, 2018</xref>; <xref ref-type="bibr" rid="bib34">Weigert, 2017</xref>; <xref ref-type="bibr" rid="bib13">Florian, 2017</xref>; <xref ref-type="bibr" rid="bib15">Guerrero-Pena, 2018</xref>; <xref ref-type="bibr" rid="bib12">Falk et al., 2019</xref>; <xref ref-type="bibr" rid="bib5">Bates, 2017</xref>). ML-based image analysis has several advantages over other approaches: it is less susceptible to bias than manual annotation, it takes a much shorter time to analyse large datasets, and, most importantly, it comes closer to human performance than conventional algorithms (<xref ref-type="bibr" rid="bib24">Mathis, 2018</xref>).</p><p>Most ML approaches to image analysis utilise <bold>F</bold>ully <bold>C</bold>onvolutional Deep Neural <bold>N</bold>etworks (FCNs) that were shown to excel at object detection in images (<xref ref-type="bibr" rid="bib9">Dai, 2016</xref>; <xref ref-type="bibr" rid="bib31">Szegedy, 2014</xref>; <xref ref-type="bibr" rid="bib21">LeCun et al., 1989</xref>; <xref ref-type="bibr" rid="bib12">Falk et al., 2019</xref>). Through several rounds of optimisation, FCNs select the best possible operations by exploiting a multitude of hidden layers. These layers apply image convolutions using kernels of different shapes and sizes, aiming to best match the output of the neural network to the provided training data labels, which were previously derived from manual annotation. This means that the network learns to interpret the images based on the available data, and not on a priori considerations. This approach has become possible due to the dramatic improvements in computation times of modern CPUs and the adoption of GPUs that can execute an enormous number of operations in parallel. Currently, the most successful architecture for biological and medical image analysis is the U-Net, which takes an input image to generate a binary map that highlights objects of interest based on the training data (<xref ref-type="bibr" rid="bib29">Ronneberger et al., 2015</xref>).</p><p>Here we present KymoButler, a new stand-alone FCN software based on the U-Net architecture, to automatically and reliably extract particle tracks from kymographs. The software is packaged into an easy-to-use web interface (<ext-link ext-link-type="uri" xlink:href="https://deepmirror.ai/kymobutler">https://deepmirror.ai/kymobutler</ext-link>) and a downloadable software package, and it was benchmarked against traditional software and manual annotation on synthetic (i.e., ground truth) and real (biological) data. We show that KymoButler performs as well as manual annotation on challenging bidirectional kymographs, where particles disappear, reappear, merge, cross each other’s path, move in any direction, change speed, immobilise, and reverse direction. KymoButler thus represents a substantial improvement in the automation of kymograph tracing, speeding up the experimental workflow, while preserving the accuracy of manual annotations.</p></sec><sec sec-type="results" id="s2"><title>Results</title><sec id="s2-1"><title>The KymoButler software package</title><p>For our FCN-based kymograph analysis software, we implemented a customised architecture based on U-Net (<xref ref-type="bibr" rid="bib29">Ronneberger et al., 2015</xref>). This architecture comprises two segmentation networks (‘modules’), one specialised on kymographs with exclusively unidirectional particle movements, the other one on bi-directional kymographs. These segmentation networks were trained to binarize the image into regions with particle tracks (foreground) and noise (background). They take an input kymograph to generate 2D maps that assign a ‘trackness’ value between 0 and 1 to each pixel of the input image, with higher values representing a higher likelihood of pixels being part of a track (see Materials and methods).</p><p>Our training (95%) and validation (5%) data consisted of manually annotated tracks in 487 unidirectional and 79 bidirectional kymographs (unpublished data from our group and other laboratories, see Materials and methods and Acknowledgements for details). Since no ground truth was available in the manually annotated kymographs, we also generated 221 synthetic unidirectional and 21 synthetic bidirectional kymographs that were used for training.</p><p>The unidirectional segmentation module generates separate trackness maps for tracks with negative and positive slopes (which could, for example, correspond to tracks of anterograde and retrograde transport processes, respectively), to remove line crossings from the output (see Materials and methods). Since particles have uniform speeds, individual tracks can be extracted via binarization of the trackness map.</p><p>In bidirectional kymographs, tracks show more complex morphologies, since they can change direction/speed and cross each other multiple times. The bidirectional segmentation module therefore generates a single trackness map, which needs to be further processed in order to obtain individual particle tracks. In particular, one has to resolve crossings between tracks. We did this by implementing a decision module which iterates through all crossings to find the most likely final segmentation (see Materials and methods for details).</p><p>We found the binarization thresholds for both modules to depend on the biological application and on the signal to noise ratio of the input image. However, we observed the best performance for both segmentation modules generally for values between 0.1–0.3 (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>).</p><p>Finally, our software has to decide whether to analyse a given kymograph with the unidirectional module or the bidirectional module. Therefore, we implemented a ‘classification module’ that classifies input kymographs into unidirectional or bidirectional ones. We linked the class module to the unidirectional and bidirectional segmentation modules as well as to the decision module and packaged them into KymoButler, an easy-to-use, drag and drop browser-based app for quick and fully automated analysis of individual kymographs (<ext-link ext-link-type="uri" xlink:href="https://deepmirror.ai/kymobutler">https://deepmirror.ai/kymobutler</ext-link>).</p><p>The only free parameter in KymoButler is the threshold for trackness map segmentation. The default threshold is set to 0.2, but users can freely adjust it for their specific application. After the computation, which takes 1–20 s per kymograph (depending on complexity), KymoButler generates several files including a dilated overlay image highlighting all the tracks found in different colours, a CSV file containing all track coordinates, a summary file with post processing data, such as average velocities and directionality, and preliminary plots of these quantities (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). KymoButler worked well on previously published kymographs from a variety of different biological data (<xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>) and on unpublished data from collaborators (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p></sec><sec id="s2-2"><title>Performance on unidirectional kymographs</title><p>We quantitatively evaluated the performance of KymoButler on unidirectional kymographs, that is particles that move with mostly uniform velocities and with no change in direction (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, <xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). The unidirectional module of KymoButler was compared to an existing kymograph analysis software, which is based on Fourier filters, and which provided the best performance among publicly available software in our hands (KymographDirect package; <xref ref-type="bibr" rid="bib23">Mangeol et al., 2016</xref>). Additionally, we traced kymographs by hand to obtain a control for the software packages.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Benchmark of KymoButler against unidirectional synthetic data.</title><p>(<bold>A</bold>) An example synthetic kymograph and its corresponding ground truth, manual control, the prediction by KymoButler, and the prediction by Fourier filtering. The top row depicts individual tracks in different colours and the bottom row shows the prediction overlay (magenta) with the ground truth (green) for all approaches. Discrepancies are thus highlighted in magenta (false positive) and green (false negative), while matching ground truth and prediction appears white. (<bold>B</bold>) Schematic explaining the concept of recall and precision. The top row depicts the possible deviations of the prediction from the ground truth. The middle and bottom rows show example overlays, again in green and magenta, from the synthetic data. In the left column, the prediction is larger than the ground truth (magenta is visible) leading to false positive pixels and low track precision, but a small number of false negatives and thus high track recall. An example prediction overlay of the Fourier filter approach is shown, which tends to elongate track ends. The right column shows a shorter prediction than the ground truth, leading to green segments in the overlay. While this prediction has high track precision (low number of false positive pixels), track recall is low due to the large number of false negatives. Again, a cut-out from the Fourier filter prediction is shown, where multiple gaps are introduced in tracks, thus severely diminishing track recall (see Material and methods for a detailed explanation of recall and precision). The middle column shows the same two cut outs analysed by KymoButler. No magenta or green segments are visible, thus leading to high recall and precision. (<bold>C</bold>) Synthetic kymograph region with four gaps highlighted (arrow heads): in one or more kymograph image rows the signal was artificially eliminated but kept in the ground truth to simulate real fluorescence data. While KymoButler efficiently connects tracks over gaps, the Fourier filter is unable to do so and breaks up those tracks into segments or incorrectly shortens these tracks (red arrow heads). Yellow arrow heads depict correct gap bridging events. (<bold>D</bold>) A synthetic kymograph with several line crossings. While KymoButler efficiently resolved all crossings, that is lines that cross other lines are not broken up into two segments, the Fourier filter correctly identifies the line crossing at the yellow arrow head but erroneously terminates the red and yellow tracks at the red arrow head. (<bold>E</bold>) The geometric means of recall and precision (‘track F1 score’) for KymoButler, the Fourier filter approach, and manual control. Each dot represents the average track F1 score of one synthetic kymograph (<inline-formula><mml:math id="inf1"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf2"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf3"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). (<bold>F</bold>) Quantification of gap bridging performance for KymoButler (89%), manual control (88%), and Fourier filter (72%); lines: medians of all 10 synthetic kymographs, <inline-formula><mml:math id="inf4"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf5"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf6"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. (<bold>G</bold>) The fraction of correctly identified crossings for KymoButler, manual annotation, and the Fourier filter (88% KymoButler, 86% manual, 60% Fourier filter; lines: medians of all 10 synthetic kymographs, <inline-formula><mml:math id="inf7"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf8"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf9"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). Tracks smaller than 3 pixels and shorter than 3 frames were discarded from the quantification.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Table of presented data.</title><p>A CSV file that contains: the average track F1 score, the average gap score, and the average crossing score for each unidirectional synthetic kymograph.</p></caption><media xlink:href="elife-42288-fig2-data1-v2.csv" mimetype="application" mime-subtype="octet-stream"/></supplementary-material></p><p><supplementary-material id="fig2sdata2"><label>Figure 2—source data 2.</label><caption><title>Synthetic kymographs and movies.</title><p>A ZIP file containing all analysed synthetic unidirectional movies, their kymographs, results from KymographClear based analysis and manually annotated ImageJ rois.</p></caption><media xlink:href="elife-42288-fig2-data2-v2.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></p></caption><graphic xlink:href="elife-42288-fig2-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Data quantities derived from unidirectional kymographs using manual annotation, KymoButler, and Fourier filtering for simulated and real data.</title><p>(<bold>A</bold>) Deviation from the average ground truth values per synthetic kymograph for all three different analysis approaches (velocity: <inline-formula><mml:math id="inf10"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf11"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf12"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math></inline-formula>; orientation: <inline-formula><mml:math id="inf13"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, manual vs KymoButler <inline-formula><mml:math id="inf14"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf15"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math></inline-formula>; travel time:<inline-formula><mml:math id="inf16"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, manual vs KymoButler <inline-formula><mml:math id="inf17"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf18"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>; travel distance: <inline-formula><mml:math id="inf19"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, manual vs KymoButler <inline-formula><mml:math id="inf20"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf21"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>; particle number: <inline-formula><mml:math id="inf22"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, manual vs KymoButler <inline-formula><mml:math id="inf23"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf24"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). (<bold>B</bold>) Deviation from manually obtained average values per real kymograph from our validation set (velocity: <inline-formula><mml:math id="inf25"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula>, Wilcoxon ranksum test; orientation: <inline-formula><mml:math id="inf26"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math></inline-formula>; travel time: <inline-formula><mml:math id="inf27"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>; travel distance: <inline-formula><mml:math id="inf28"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math></inline-formula>; particle number: <inline-formula><mml:math id="inf29"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math></inline-formula>).</p></caption><graphic xlink:href="elife-42288-fig2-figsupp1-v2.tif" mimetype="image" mime-subtype="tiff"/></fig></fig-group><p>First, we generated 10 synthetic movies depicting unidirectional particle dynamics with low signal-to-noise ratio (~1.2, see Materials and methods) and extracted kymographs from those movies using the KymographClear (<xref ref-type="bibr" rid="bib23">Mangeol et al., 2016</xref>) Fiji plugin. Each of the kymographs was then analysed by Fourier-filtering (KymographDirect), KymoButler, and by hand, and the identified trajectories overlaid with the ground truth (i.e., the known dynamics of the simulated data) (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). KymoButler typically took less than a minute to analyse the 10 kymographs while fourier filtering took about 10 min since thresholds had to be set individually for each image. Manual annotation by an expert took about 1.5 hr.</p><p>To quantify the quality of the predicted traces, we first determined the best predicted track for each ground truth track (in case several segments were predicted to cover the same track) and then calculated the fraction of the length of the ground truth track that was correctly identified by that predicted track (‘track recall’) (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Additionally, we determined the best overlapping ground truth track for each predicted track and then calculated the fraction of the length of the predicted track that was overlapping with the ground truth track (‘track precision’). Examples of low/high precision and low/high recall are shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. We then calculated the geometric mean of the average track recall and the average track precision (the ‘track F1 score’, see Materials and methods) for each kymograph (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). The median F1 score of the manual control was 0.90, KymoButler achieved 0.93, while Fourier filtering achieved a significantly lower F1 score of 0.63 (<inline-formula><mml:math id="inf30"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf31"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf32"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>).</p><p>Our synthetic data intentionally included gaps of exponentially distributed lengths (see Materials and methods), allowing us to quantify the ability of KymoButler to bridge gaps in kymograph tracks (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, F), which are frequently encountered in kymographs extracted from fluorescence data (<xref ref-type="bibr" rid="bib2">Applegate et al., 2011</xref>). Both KymoButler and manual annotation consistently bridged gaps that belonged to the same trajectory, while Fourier filtering was less accurate (89% of all gaps correctly bridged by KymoButler, 88% by manual, and 72% by Fourier filter analysis; median of all 10 synthetic kymographs, <inline-formula><mml:math id="inf33"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf34"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf35"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Figure 2F).</p><p>We also quantified the ability of KymoButler to resolve track crossings (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Again, both KymoButler and manual annotation performed significantly better than Fourier filtering (88% KymoButler, 86% manual, 60% Fourier filter; median percentage of correctly resolved crossings of all 10 synthetic kymographs, <inline-formula><mml:math id="inf36"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf37"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math></inline-formula>, manual vs Fourier Filtering <inline-formula><mml:math id="inf38"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, <xref ref-type="fig" rid="fig2">Figure 2G</xref>).</p><p>Remarkably, KymoButler was able to correctly pick up ~80% of all tracks at an SNR as low as 1.1, where tracks are barely visible by eye (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5B,D</xref>), and at high particle densities (~70% of the kymograph image covered with signal) (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5F,H</xref>). Manual annotation at such extremes performed similarly to KymoButler (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5C,D,G,H</xref>).</p><p>Finally, we compared KymoButler’s overall performance on kymographs containing unidirectional traces with that of alternative analysis approaches. KymoButler performed similar to or better than manual annotation of synthetic data when analysing particle velocities, directionality, travel time, travel distance, and particle numbers, while the Fourier filter frequently deviated by more than 50% from ground truth averaged values (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>). When testing KymoButler’s overall performance on real kymographs of our validation data set, we compared deviations from manual annotation as no ground truth was available. KymoButler deviated by less than 10% from most manual estimates (but found ~30% more particles), while the Fourier filter approach deviated by up to 50% from the manually calculated values (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>).</p><p>In summary, KymoButler was able to reliably track particle traces in kymographs at low SNR and high particle densities in both synthetic and real data, and it clearly outperformed currently existing automated software, while being as consistent as manual expert analysis while being ~100 x faster.</p></sec><sec id="s2-3"><title>KymoButler’s performance on bidirectional Kymographs</title><p>As in many kymographs obtained from biological samples trajectories are not unidirectional, we also tested the performance of KymoButler on complex bidirectional kymographs, that is of particles with wildly different sizes, velocities, and fluorescence intensities that frequently change direction, may become stationary and then resume motion again (see <xref ref-type="fig" rid="fig1">Figure 1B,C</xref>, <xref ref-type="fig" rid="fig3">Figure 3A</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref> for examples). Available fully automated software that relied on edge detection performed very poorly on our synthetic kymographs (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Therefore, we implemented a custom-written wavelet coefficient filtering algorithm to compare our FCN-based approach to a more traditional non-ML approach (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, Materials and methods). In short, the wavelet filtering algorithm generates a trackness map, similar to KymoButler, by applying a stationary wavelet transform to the kymograph to generate so-called ‘coefficient images’ that highlight horizontal or vertical lines. These coefficient images are then overlaid and binarized with a fixed value (0.3), skeletonised, and fed into the KymoButler algorithm without the decision module, that is crossings are resolved by linear regression prediction.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Benchmark of KymoButler against complex bidirectional synthetic data.</title><p>(<bold>A</bold>) Example synthetic kymograph and its corresponding ground truth, manual control, the prediction by KymoButler, and the prediction via wavelet coefficient filtering. The top row depicts individual tracks in different colours and the bottom row shows the prediction overlay (magenta) with the ground truth (green) for all approaches. Discrepancies are highlighted in magenta (false positive) and green (false negative), while the match of ground truth and prediction appears white. (<bold>B</bold>) Example recall and precision of KymoButler and wavelet filtering. While KymoButler shows high recall and high precision, the wavelet filter approach yields significant deviations from the ground truth (green and magenta pixels). (<bold>C</bold>) Synthetic kymograph region with three artificial gaps highlighted (arrow heads). While KymoButler efficiently connects tracks over gaps, the wavelet filter is unable to do so and breaks up those tracks into segments (red arrow heads). The yellow arrow heads depict correct gap bridging events. (<bold>D</bold>) A synthetic kymograph with several line crossings. While KymoButler efficiently resolved all crossings, that is lines that cross other lines are not broken up into segments, the wavelet filter only resolves one crossing correctly (yellow arrow head). (<bold>E</bold>) The geometric means of track recall and track precision (track F1 score) for KymoButler, manual control, and the wavelet filter. Each dot represents the average F1 score of one synthetic kymograph (<inline-formula><mml:math id="inf39"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>8</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf40"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:math></inline-formula>, manual vs wavelet filtering <inline-formula><mml:math id="inf41"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). (<bold>F</bold>) Quantification of gap performance for KymoButler, manual annotation, and wavelet filter (<inline-formula><mml:math id="inf42"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf43"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math></inline-formula>, manual vs wavelet filtering <inline-formula><mml:math id="inf44"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). (<bold>G</bold>) The fraction of resolved crossings for KymoButler, manual control, and the wavelet filter (<inline-formula><mml:math id="inf45"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf46"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math></inline-formula>, manual vs wavelet filtering <inline-formula><mml:math id="inf47"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). KymoButler identifies tracks in complex kymographs as precisely as manual annotation by an expert.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Table of presented data.</title><p>A CSV file that contains: the average track F1 score, the average gap score, and the average crossing score for each bidirectional synthetic kymograph.</p></caption><media xlink:href="elife-42288-fig3-data1-v2.csv" mimetype="application" mime-subtype="octet-stream"/></supplementary-material></p><p><supplementary-material id="fig3sdata2"><label>Figure 3—source data 2.</label><caption><title>Synthetic kymographs and movies.</title><p>A ZIP file containing all analysed synthetic bidirectional movies, their kymographs, and manually annotated ImageJ rois.</p></caption><media xlink:href="elife-42288-fig3-data2-v2.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></p></caption><graphic xlink:href="elife-42288-fig3-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Performance of different skeletisation techniques on a synthetic bidirectional kymograph.</title><p>(<bold>A</bold>) Example of a synthetic bidirectional kymograph and its corresponding ground truth, the predictions by manual annotation, KymoButler, wavelet coefficient filtering, and tracks detected through edge filtering. The top row depicts individual tracks in different colours and the bottom row shows the prediction overlay (magenta) with the ground truth (green) for both approaches. Discrepancies are highlighted in magenta (false positive) and green (false negative), while a match of ground truth and prediction appears white.</p></caption><graphic xlink:href="elife-42288-fig3-figsupp1-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Synthetic data quantities derived from bidirectional kymographs using manual annotation, KymoButler, and wavelet filtering for simulated and real data.</title><p>(<bold>A</bold>) Deviation from the average ground truth values per synthetic kymograph for all three different analysis approaches (velocity: <inline-formula><mml:math id="inf48"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf49"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, manual vs Wavelet Filtering <inline-formula><mml:math id="inf50"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula>; orientation: <inline-formula><mml:math id="inf51"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:math></inline-formula>; travel time: <inline-formula><mml:math id="inf52"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, manual vs KymoButler <inline-formula><mml:math id="inf53"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:math></inline-formula>, manual vs Wavelet Filtering <inline-formula><mml:math id="inf54"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>; travel distance: <inline-formula><mml:math id="inf55"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, manual vs KymoButler <inline-formula><mml:math id="inf56"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:math></inline-formula>, manual Wavelet Filtering <inline-formula><mml:math id="inf57"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>; pause time: <inline-formula><mml:math id="inf58"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, manual vs KymoButler <inline-formula><mml:math id="inf59"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, manual Wavelet Filtering <inline-formula><mml:math id="inf60"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>; percentage of tracks that change direction at least once: <inline-formula><mml:math id="inf61"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>; percentage of stationary tracks: <inline-formula><mml:math id="inf62"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>; particle number: <inline-formula><mml:math id="inf63"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:math></inline-formula>, manual vs KymoButler <inline-formula><mml:math id="inf64"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math></inline-formula>, manual vs Wavelet Filtering <inline-formula><mml:math id="inf65"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn></mml:math></inline-formula>). (<bold>B</bold>) Deviation from manually obtained average values per real kymograph from our validation set (velocity: <inline-formula><mml:math id="inf66"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn></mml:math></inline-formula>, Wilcoxon ranksum test; orientation: <inline-formula><mml:math id="inf67"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math></inline-formula>; travel time: <inline-formula><mml:math id="inf68"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula>; travel distance: <inline-formula><mml:math id="inf69"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:math></inline-formula>; pause times: <inline-formula><mml:math id="inf70"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn></mml:math></inline-formula>; percentage of tracks that change direction at least once: <inline-formula><mml:math id="inf71"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math></inline-formula>; percentage of stationary tracks per kymograph: <inline-formula><mml:math id="inf72"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.24</mml:mn></mml:math></inline-formula>; particle number per kymograph: <inline-formula><mml:math id="inf73"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.02</mml:mn></mml:math></inline-formula>).</p></caption><graphic xlink:href="elife-42288-fig3-figsupp2-v2.tif" mimetype="image" mime-subtype="tiff"/></fig></fig-group><p>We generated 10 kymographs from our synthetic movies with the KymographClear package (average signal-to-noise ratio was 1.4, since any lower signal generally obscured very faint and fast tracks). Each of the kymographs was then analysed by wavelet coefficient filtering, KymoButler, and manual annotation, and the predicted traces overlaid with the ground truth (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). While the wavelet approach and KymoButler were able to analyse the 10 kymographs in less than 1 min, manual annotation by an expert took about 1.5 hr. Moreover, whereas the manual annotation and KymoButler segmentation overlaid well with the ground truth, the wavelet approach yielded numerous small but important deviations.</p><p>Similarly to the unidirectional case, we quantified track precision and recall (<xref ref-type="fig" rid="fig3">Figure 3B, E</xref>) and calculated the resolved gap fraction (<xref ref-type="fig" rid="fig3">Figure 3C, F</xref>) and crossing fraction (<xref ref-type="fig" rid="fig3">Figure 3D, G</xref>). The median of the track F1 scores per kymograph for manual annotation (0.82) was similar to KymoButler (0.78), while the wavelet filter approach only gave 0.61 (<inline-formula><mml:math id="inf74"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>7</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf75"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:math></inline-formula>, manual vs wavelet filtering <inline-formula><mml:math id="inf76"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, <xref ref-type="fig" rid="fig3">Figure 3E</xref>). While gaps were resolved by KymoButler and manual annotation in 86% and 95% of cases, respectively, only 63% were resolved by the wavelet algorithm (median of all 10 synthetic kymographs, <inline-formula><mml:math id="inf77"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf78"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:math></inline-formula>, manual vs wavelet filtering <inline-formula><mml:math id="inf79"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, <xref ref-type="fig" rid="fig3">Figure 3F</xref>). Crossings were rarely resolved correctly by the wavelet algorithm (13%) but much more reliably by KymoButler (59%) and manual annotation (76%) (median of all 10 synthetic kymographs, <inline-formula><mml:math id="inf80"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, Kruskal-Wallis Test, Tukey post-hoc: manual vs KymoButler <inline-formula><mml:math id="inf81"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:math></inline-formula>, manual vs wavelet filtering <inline-formula><mml:math id="inf82"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, <xref ref-type="fig" rid="fig3">Figure 3G</xref>).</p><p>As bidirectional tracks varied in intensity, lower SNRs obscured faint tacks so that performance dropped slightly faster with decreasing SNR than for unidirectional tracks (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5A,D</xref>), and it decreased linearly for increasing particle densities (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5E,H</xref>). Manual annotation at high signal densities and low SNRs again yielded similar results as KymoButler (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5C,D,G,H</xref>). The analysis of quantity averages of 10 synthetic kymographs revealed that KymoButler was as accurate as manual annotation, while the wavelet filter deviated significantly more from the ground truth (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>). We also tested KymoButler on our validation dataset. Results were similar to the performance on synthetic data (~10% deviation from manual annotation), and the wavelet filter performed significantly worse than KymoButler (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>).</p><p>Overall, these results showed that KymoButler performs well on both unidirectional and bidirectional kymographs, clearly outperforms currently available automated analysis of kymographs, and it performs as well as manual tracing, while being much faster and not prone to unconscious bias.</p></sec></sec><sec sec-type="discussion" id="s3"><title>Discussion</title><p>In this work, we developed software based on Deep Learning techniques to automate the tracking of dynamic particles along a stationary path in a noisy cellular environment. Convolutional neural networks (CNNs) are nowadays widely applied for image recognition. Since tracking is a priori a visual problem, we built a modular software utilising CNNs for identifying tracks in kymographs. We deployed our networks as KymoButler, a software package that takes kymographs as inputs and outputs all tracks found in the image in a matter of seconds. The network outperforms standard image filtering techniques on synthetic data as well as on kymographs from a wide range of biological processes, while being as precise as expert manual annotation.</p><p>The KymoButler software has only one adjustable detection parameter that is left to the user: a sensitivity threshold that, if low, allows more ambiguous tracks to be recognised, and if high discards them. For our synthetic data, the best value for the threshold lay between 0.1 and 0.3 (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>), and we observed a similar range for a variety of kymographs from published data. However, the threshold depends on the SNR of the input images, so that the correct threshold has to be chosen based on each biological application and imaging conditions. Furthermore, the performance of both KymoButler and manual annotation decreased with decreasing SNR and increasing particle density (number of crossings in the image, <xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). Note that the particle density here also depends on the particle’s frequency of change in direction in dense kymographs as more bidirectional particles tend to cover larger proportions of the kymograph image. Hence, we strongly recommend to visually inspect the output of KymoButler for each new application, and to compare the output to manual annotation.</p><p>Most of the publicly available kymograph analysis software requires manual labelling to extract quantitative data (<xref ref-type="bibr" rid="bib7">Chenouard, 2010</xref>; <xref ref-type="bibr" rid="bib26">Neumann et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Zala et al., 2013</xref>). Some automated approaches have been published in the context of specific biological questions, but since these programs are currently not publicly available it is not clear how well they would perform on kymographs from other applications (<xref ref-type="bibr" rid="bib25">Mukherjee et al., 2011</xref>; <xref ref-type="bibr" rid="bib28">Reis et al., 2012</xref>). Other approaches do not extract individual tracks but only macroscopic quantities, as for example velocities (<xref ref-type="bibr" rid="bib6">Chan and Odde, 2008</xref>). As KymoButler is fully automated and able to reliably analyze kymographs from a wide range of biological applications, it fills an important gap. Here we showed that KymoButler is able to quantify mitochondria movement in neuronal dendrites, microtubule growth dynamics in axons, and in vitro dynamics of single cytoplasmic dynein proteins (<xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The training and validation data for KymoButler comprised kymographs depicting the dynamics of microtubule +ends, mitochondria movement, molecular motor movements, and vesicle transport in neuronal processes (example kymographs: <xref ref-type="bibr" rid="bib20">Lazarus et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Cioni et al., 2019</xref>; <xref ref-type="bibr" rid="bib16">Hangen et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Gerson-Gurwitz et al., 2011</xref>). Hence, KymoButler will perform best on similar data. However, we predict that it can furthermore be applied to most other kymographs obtained from time-lapse fluorescence microscopy without the need of any modifications.</p><p>KymoButler outperformed Fourier filtering, edge detection, and customised wavelet coefficient selection on synthetic kymographs. While Fourier filtering ‘only’ performed ~30% worse than KymoButler on synthetic unidirectional kymographs, edge detection on synthetic bidirectional kymographs suffered greatly from background fluctuations and low SNR to such an extent that the extracted data was unusable (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for one example). Therefore, we designed a filtering algorithm based on wavelet coefficient image selection to analyse complex bidirectional kymographs specifically for our synthetic data. KymoButler still performed 20% better than this approach (<xref ref-type="fig" rid="fig3">Figure 3</xref>). The main problem with either filtering approach compared to KymoButler was their inability to bridge track gaps and resolve line crossings, both of which occur frequently in biological data (<xref ref-type="fig" rid="fig2">Figures 2C,D</xref> and <xref ref-type="fig" rid="fig3">3C,D</xref>). These challenges were met by KymoButler, which performed as well as expert annotation, but within a much shorter time (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>). Consequently, KymoButler generated similar measures of averaged quantities (average velocities, displacements, etc.) as manual annotation (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>).</p><p>Synthetic kymographs, however, will reproduce the complexity of real kymographs only to some degree, as they exhibit homogenous background, no artefacts, no varying particle intensity in time, and individual tracks can appear rather similar. Hence, we also benchmarked KymoButler for both the unidirectional and bidirectional (real) validation datasets. KymoButler calculated similar average quantities as obtained from manual annotation, such as average particle velocities and pausing times, with some minor deviations (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>). However, since values obtained from manual annotation deviated by up to 20% from ground truth values on synthetic data, deviations from manual annotation should not automatically be interpreted as an erroneous deviation (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><p>Our results show that KymoButler is able to correctly identify individual full-length tracks in kymographs with an average track F1 score (geometric mean of track precision and recall) of 92% on unidirectional tracks and 78% on complex bidirectional tracks (similar to manual annotation), without suffering from inconsistency, bias, and laborious tracing, that plague manual tracking. While KymoButler is already performing very well, we aim to significantly improve it over future iterations. Every time someone uses our webform, s/he has the option to anonymously upload the kymograph to our cloud. Once a large number of diverse kymographs is uploaded, these kymographs will be annotated by us and added to our training data, improving KymoButler even further.</p><p>The ultimate challenge will be to expand our approach to 2D or even 3D tracking problems. Here, we defined a 1D region of interest in 2D time-lapse movies, extracted 2D (space and time) images (kymographs), and finally tracked 2D lines in those images. A similar, albeit computationally heavier, approach could stack the frames of a 2D/3D movie on top of each other to generate a 3D/4D kymogram (2D space and time, or 3D space and time). Previously generated kymograms have led to intriguing results on whole-cell particle tracking problems with high SNR (<xref ref-type="bibr" rid="bib27">Racine et al., 2007</xref>). The use of higher dimensional FCNs in the future has great potential to yield human-like performance on any biological and medical tracking problems.</p></sec><sec sec-type="materials|methods" id="s4"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th valign="top">Resource</th><th valign="top">Designation.</th><th valign="top">Source.</th><th valign="top">Identifiers.</th><th valign="top">Additional information</th></tr></thead><tbody><tr><td valign="top">Software, algorithm</td><td valign="top">MATLAB</td><td valign="top">MATLAB</td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link></td><td valign="top">Used for statistical analysis</td></tr><tr><td valign="top">Software, algorithm</td><td valign="top">Fiji</td><td valign="top">Fiji is Just ImageJ <break/>(<ext-link ext-link-type="uri" xlink:href="https://fiji.sc">https://fiji.sc</ext-link>)</td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002285">SCR_002285</ext-link></td><td valign="top">Used to generate and <break/>analyse kymographs with KymographClear/Direct <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/kymographanalysis/">https://sites.google.com/site/kymographanalysis/</ext-link></td></tr><tr><td valign="top">Software, algorithm</td><td valign="top">Wolfram Mathematica</td><td valign="top">Wolfram Mathematica</td><td valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_014448">SCR_014448</ext-link></td><td valign="top">Code available under <ext-link ext-link-type="uri" xlink:href="https://github.com/MaxJakobs/KymoButler">https://github.com/MaxJakobs/KymoButler</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f47b5bc2e657f0c1f85a6c9e622fbd608dfdc7fd;origin=https://github.com/MaxJakobs/KymoButler;visit=swh:1:snp:416bc50f1dfae8e87f58c448394935ed0c355c45;anchor=swh:1:rev:e35173e9051eb5395f9b13dcd8f487ffa4098592">swh:1:rev:e35173e9051eb5395f9b13dcd8f487ffa4098592</ext-link>)</td></tr></tbody></table></table-wrap><p>All code was written in the wolfram language in Mathematica <ext-link ext-link-type="uri" xlink:href="https://wolfram.com/mathematica">https://wolfram.com/mathematica</ext-link> and, if not stated otherwise, can be found online under: <ext-link ext-link-type="uri" xlink:href="https://github.com/MaxJakobs/KymoButler">https://github.com/MaxJakobs/KymoButler</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f47b5bc2e657f0c1f85a6c9e622fbd608dfdc7fd;origin=https://github.com/MaxJakobs/KymoButler;visit=swh:1:snp:416bc50f1dfae8e87f58c448394935ed0c355c45;anchor=swh:1:rev:e35173e9051eb5395f9b13dcd8f487ffa4098592">swh:1:rev:e35173e9051eb5395f9b13dcd8f487ffa4098592</ext-link>).</p><sec id="s4-1"><title>The KymoButler software package</title><p>The KymoButler software was implemented in Mathematica to take advantage of easy web form deployment and distribution. The workflow is shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>. Our approach was to first segment kymograph pixels that are part of particle tracks from pixels that were part of the background with our segmentation modules. From previous work we knew that kymographs that depict unidirectional movement only, can be filtered into tracks that have positive slope and those that have negative slope (<xref ref-type="bibr" rid="bib7">Chenouard, 2010</xref>), while no such assumptions can be made about bidirectional kymographs. Hence, we decided to take advantage of this simplification of unidirectional kymograph analysis by training two modules: one that is specialised to segment unidirectional kymographs and another one that segments bidirectional ones. Note that the bidirectional module is able to analyze any kymograph, including unidirectional ones, but since it is not specialised it performs slightly worse than the unidirectional module on unidirectional kymographs. To further simplify software usability, we prepended a class module that classifies input kymographs as bidirectional or unidirectional, and then applies the corresponding segmentation module and, if bidirectional, decision module. Our downloadable software package on GitHub allows the user to call either segmentation module (unidirectional/bidirectional) directly, if they wish to do so.</p><p>When the kymograph is classified as unidirectional by the class module, the unidirectional segmentation module generates two trackness score maps for particles with negative or positive slope (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). Since the particles move with roughly the same velocity, the resulting maps mostly do not exhibit any crossings. Thus, we binarize the maps with a threshold between 0.1–0.3 (see benchmarking section for more information about the threshold). The resulting binary maps are then thinned iteratively so that each trace is only one pixel wide at any point and pruned so that branches that are shorter than three pixels are deleted. Subsequently, each trace is segmented and selected only if they are at least three frames long and three pixels large (these values can be varied by the user if needed but were kept constant throughout this manuscript for unidirectional kymographs). In the final step, pixels that lie in the same row of the kymograph are averaged over so that the final track has only one entry per frame.</p><p>For bidirectional kymographs the software generates a trackness map, applies a binarization threshold (0.1-0.3, see benchmarking for more details), iterative thinning, and pruning (minimum length 3 pixels). Similar to the unidirectional case, our software allows the selection of tracks with a minimum number of pixels and/or frames. However, since the resulting skeletonised map had a substantial number of crossings and could not be easily segmented to yield individual tracks, we implemented a further module in the software. First, all lines in the skeletonised map are shortened so that each white pixel at a track end only has neighbouring pixels in different rows (time dimension). This was done so that we could detect track starting points (‘seeds’) with a Hit-Miss transformation with kernel: <inline-formula><mml:math id="inf83"><mml:mfenced separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></inline-formula>. Application of this kernel yielded a binary map with 0 everywhere except at track seeds (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>, red dots). These seeds were then used to start tracing individual tracks in the kymograph by always advancing to the next white pixel. Once more than one potential future pixel is encountered, the decision module is called. The module generates three 48x48 crops of (1) the input kymograph, (2) the skeletonised trackness map, and (3) the skeleton of the current track and predicts a trackness map that has high values on the skeleton segment of the most likely future track (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>). This map is binarized with threshold 0.5 and thinned. The precise threshold had little effect on the final output, so we fixed it at 0.5 for all applications. Users can vary this threshold as well in the source code on GitHub. Next, the largest connected component in the map is selected as the most likely future path and appended to the track if longer than 2 pixels. The average trackness value of this component (from the decision module prediction) is saved as a measure of decision ‘confidence’. This process is repeated until no further possible pixels are found or no future path is predicted which is when the track is terminated. Once all seeds are terminated, the software subtracts all the found paths from the skeletonised trackness map and again looks for new seeds which are then again tracked in the full skeletonised image. The process is repeated until no further seeds are found, and then all tracks are averaged over their timepoints (rows in the kymograph image). Subsequently the software deletes tracks that are shorter than 5 pixels or part of another track and assigns overlaps that are longer than 10 pixels to the track with the highest average decision confidence.</p><p>Both the unidirectional and the bidirectional module output a coloured overlay in which each track is drawn in a different randomly assigned colour and dilated with factor one for better visibility (see <xref ref-type="fig" rid="fig1">Figure 1B–C</xref> and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). Additionally, the software generates one CSV file that contains all the track coordinates, a summary CSV file that gives derived quantities, such as track direction and average speed, and plots depicting these quantities.</p><p>The software was deployed from Mathematica as a cloud based interface (<ext-link ext-link-type="uri" xlink:href="https://deepmirror.ai/kymobutler">https://deepmirror.ai/kymobutler</ext-link>) and a Mathematica package (<ext-link ext-link-type="uri" xlink:href="https://github.com/MaxJakobs/KymoButler">https://github.com/MaxJakobs/KymoButler</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f47b5bc2e657f0c1f85a6c9e622fbd608dfdc7fd;origin=https://github.com/MaxJakobs/KymoButler;visit=swh:1:snp:416bc50f1dfae8e87f58c448394935ed0c355c45;anchor=swh:1:rev:e35173e9051eb5395f9b13dcd8f487ffa4098592">swh:1:rev:e35173e9051eb5395f9b13dcd8f487ffa4098592</ext-link>). </p></sec><sec id="s4-2"><title>Network architectures</title><p>Our networks were built from convBlocks (a convolutional layer with 3 × 3 kernel size, padding, and arbitrary number of output channels followed by a batch normalisation layer and a ‘leaky’ ramp (leakyReLU) activation function (<inline-formula><mml:math id="inf84"><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mn>0.1</mml:mn> <mml:mi/><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>). Batch normalisation is useful to stabilise the training procedure as it rescales the inputs of the activation function (leakyReLu), so that they have zero mean and unit variance. The leakyReLu prevents the so-called ‘dead ReLu’s’ by applying a small gradient to values below 0. These building blocks were previously used for image recognition tasks in <italic>Google’s</italic> inception architecture and in the U-Net architecture (<xref ref-type="bibr" rid="bib31">Szegedy, 2014</xref>; <xref ref-type="bibr" rid="bib12">Falk et al., 2019</xref>).</p><p>The module architectures we settled on are shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplements 1</xref>–<xref ref-type="fig" rid="fig1s2">2</xref>. All modules used the same core building blocks while having different input and output ports. The classification module takes a resized kymograph of size 64 × 64 pixels and generates two output values that correspond to the class probabilities for unidirectional/bidirectional kymographs (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref>). The unidirectional segmentation module takes one input kymograph and generates two output images that correspond to the trackness scores of particles with positive or negative slopes (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>). The bidirectional segmentation module takes one input kymograph and generates one trackness score map highlighting any found particle tracks (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2C</xref>). Finally, the decision module takes three inputs of size 48 × 48 pixels to generate one trackness map (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2D</xref>). All modules share the same core network that is essentially a U-Net with padded convolutions and with 64 (in the top level) to 1024 (in the lowest level) feature maps. We experimented with more complex architectures (parallel convolution modules instead of blocks, different number of feature maps) but could only observe minor increase in accuracy at a large expense in computation time. Due to the U-Net architecture, each dimension of the inputs to the segmentation modules needs to be a multiple of 16. Thus, inputs were resized when they did not match the dimension requirements, and then the binarized output images from the segmentation modules were resized to the original input image size before proceeding further.</p></sec><sec id="s4-3"><title>Network training</title><p>To train the networks we quantified the difference between their output <inline-formula><mml:math id="inf85"><mml:mi>o</mml:mi></mml:math></inline-formula> and the desired target output <inline-formula><mml:math id="inf86"><mml:mi>t</mml:mi></mml:math></inline-formula> through a cross entropy loss layer (<inline-formula><mml:math id="inf87"><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>⋅</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>o</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>o</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>). The loss was averaged over all output entries (pixels and classes) of each network. While we tried other loss functions, specifically weighted cross entropy loss and neighbour dependent loss as described in <xref ref-type="bibr" rid="bib5">Bates (2017)</xref>, we persistently obtained higher track precision and track recall with the basic cross entropy loss above.</p><p>Our training data comprised a mixture of synthetic data and manually annotated unpublished kymographs, kindly provided by the research groups mentioned in the acknowledgements. Most of the manual annotation was done by M. A. H. J. and A. D. In total, we used 487 (+200 synthetic) unidirectional, and 79 (+21 synthetic) bidirectional kymographs, with 95% of the data used for network training, and ~5% of retained for network validation. All network training was performed on a workstation, using a nVidia 1080 Ti or a nVidia 1070 GPU.</p><p>The class module depicted in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2A</xref> was trained with batches of size 50 (with 25 unidirectional and 25 bidirectional kymographs to counter class imbalance) with random image transformations that included image reflections, rotations, resizing, colour negation, gaussian noise, random noise, and random background gradients. The final input image was randomly cropped to 64 × 64 pixels (see examples <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3A</xref>) and the class module was trained using stochastic gradient descent (ADAM optimiser, <xref ref-type="bibr" rid="bib18">Kingma, 2017</xref>, initial learning rate 0.001), until the validation set error rate was consistently 0%.</p><p>The unidirectional segmentation module (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2B</xref>) was trained with batches comprising 20 randomly selected kymographs from our training set (example in <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3B</xref>). We applied the following image transformations: Random reflections along either axis, random 180-degree rotations, random cropping to 128 × 80 pixels (approximately the size of our smallest kymograph), random gaussian and uniform noise, and random background gradients. Note that we did not apply any resizing to the raw kymograph since that generally decreased net performance. Additionally, we added Dropout Layers (10–20%) along the contracting path of our custom U-Net to improve regularisation. Each kymograph in this training set was generated by hand with KymographTracker (<xref ref-type="bibr" rid="bib7">Chenouard, 2010</xref>), but to increase dataset variability we took the line profiles from KymographTracker and generated kymographs with a custom Mathematica script that applied wavelet filtering to the plotted profiles. The resulting kymographs have a slightly different appearance than the ones created with KymographTracker and are thus useful to regularise our training process. Several modules were trained until convergence and the best performing one (according to the validation score) was selected (ADAM optimiser, initial learning rate of 0.001, learning rate schedule = <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>4000</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>.5</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>The bidirectional segmentation module (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2C</xref>, example data <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3C</xref>) was trained in the same way as the unidirectional segmentation module, with the exception of a slightly different learning rate schedule (<inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>3000</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>.5</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>). Additionally, since we did not have access to many of the original movies from which the kymographs were generated, we could not generate kymographs with different algorithms as done for the unidirectional module.</p><p>Training data for the decision module (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2D</xref>) was obtained from the bidirectional (synthetic +real) kymographs by first finding all the branch points in a given ground truth or manually annotated image. Then, each track was separated into multiple segments, that go from its start point to a branching point or its end point. For each branchpoint encountered while following a track, all segments that ended within 3 pixels of the branchpoint were selected. Then, (1) a 48 × 48 pixel crop of the raw kymograph around the branchpoint, (2) a binary map representing the track segment upstream of the branching point (centred with its end in pixel coordinates 25,25, with image padding applied if the end was close to an image corner), and (3) the corresponding 48 × 48 pixel region in the binary image representing all possible paths were used as inputs to the decision module. The binary image representing the ground truth or annotated future segment downstream of the branchpoint was used as the target image (see <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3D</xref> for an example training set). Thus, the training set comprised three input images and one output image which we used to train the decision module. To increase the module’s focus on the non-binary raw kymograph crop, we applied 50% dropout to the full skeletonised input and 5% dropout to the input segment. As explained above, we used random image augmentation steps like reflections, rotations, gaussian +uniform noise. Additionally, we employed random morphological thinning to the binary input/output images to simulate artefacts. Several networks were trained until convergence (pixel wise cross entropy loss, ADAM optimiser, initial learning rate 0.001, batch size 50, learning rate schedule <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>I</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>8000</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mn>.5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>), and the best performing one was selected.</p></sec><sec id="s4-4"><title>Synthetic data</title><p>Synthetic data was generated by simulating individual particles on a stationary path of length 300 pixels for 300 frames to generate 300 × 300 pixel kymographs. To obtain unidirectional particles we seeded 30 + 30 particles with negative or positive slope at random timepoints/positions. Next, a random velocity between 1–3 pixels/frame was chosen for all particles in the movie, with a random noise factor to allow slight changes in velocity, and a particle PSF between 3–6 pixels. Each particle was assigned a survival time drawn from an exponential distribution with scale 0.01, after which it would disappear. Gaps of random length (exponentially distributed) were subsequently assigned to each track individually. From these tracks we then generated a kymograph with gaussian noise, used for neural network training, and a 20 × 300 pixel movie with 300 frames for benchmarking. The resulting kymographs and movies had an average signal-to-noise ratio of 1.2 (calculated as the average intensity of the signal, divided by the average intensity of the background). Finally, we removed tracks that overlapped for the whole duration of their lifetime.</p><p>To obtain synthetic data of complex bidirectional particle movements, we generated datasets with either 15 tracks (for benchmarking) or 30 tracks (for training) per movie. The maximum velocity was set to three pixels/frame, as above this velocity it became hard to manually segment tracks from kymographs. Each movie was assigned a random velocity noise factor between 0 and 1.5 pixels/frame, a random switching probability between 0 and 0.1 (to switch between stationary and directed movement) and a random velocity flipping factor between 0 and 0.1 (to flip the direction of the velocity). Individual particles were simulated by first calculating their lifetime from an exponential distribution with scale 0.001. Then, a random initial state, moving or stationary, was selected as well as a random initial velocity and a particle size between 1–6 pixel. In the simulation, particles could randomly switch between different modes of movement (stationary/directed), flip velocities and were constantly subjected random velocity noise (movie specific). Finally, tracks that were occulted by other tracks were removed, and a movie (used for benchmarking) and a kymograph (used for training) were generated. The resulting kymographs and movies had an average signal-to-noise ratio of 1.4.</p></sec><sec id="s4-5"><title>Benchmarking</title><p>In order to benchmark the performance of software and manual predictions, we implemented a custom track F1 score which was calculated as the geometric mean of track recall and track precision. To calculate track recall, each ground truth track was first compared to its corresponding predicted track, and the fractional overlap between them was calculated. Since predicted tracks do not necessarily follow the exact same route through a kymograph, but frequently show small deviations from the ground truth (see <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>) we allowed for a 3.2-pixel deviation from the ground truth (two diagonal pixels). The maximum fractional overlap was then selected and stored as the track recall. The recall was thus one when the full length of a ground truth track was predicted, and 0 if the track was not found in the prediction. We would like to highlight that this criterion is very strict: if a ground truth track is predicted to be two tracks (for example, by failing to bridge a gap along the track), the recall fraction would decrease by up to 50%, even if most of the pixels are segmented correctly and belong to predicted tracks.</p><p>Track precision was calculated by finding the largest ground truth track that corresponded, that is had the largest overlap, to each prediction, and then calculating the fraction of the predicted track that overlapped to the ground truth track. Therefore, a track precision of 1 corresponded to a predicted track that was fully part of a ground truth track while a precision of 0 meant that the predicted track was not found in the ground truth. In general, increasing precision leads to a lower recall and vice versa, so that taking the track F1 score as the geometric mean between the two is a good measure of overall prediction performance.</p><p>To quantify gap performance, we searched for track segments within 3 pixels of the gap for each frame, to allow for predictions that deviated slightly from the ground truth. Once each frame of the gap was assigned to a corresponding predicted segment, the gap was deemed resolved. If one or more frames of the gap had no overlapping segment to the prediction, the gap was labelled unresolved. Our synthetic tracks had 954 gaps in the 10 kymographs of unidirectional data, and 840 gaps in the 10 kymographs of bidirectional data, and the largest gap size was six pixels. For each kymograph, we then calculated the fraction of gaps resolved.</p><p>To quantify KymoButler performance on crossings, we first generated binary images for each ground truth track and calculated overlaps with other ground truth tracks by multiplying those images with each other. The resulting images had white dots wherever two tracks crossed. Those dots were then dilated by a factor of 16 to generate circles and overlaid with the original single-track binary image to generate binary maps that contain segments of ground truth tracks that cross/merge with other tracks. Next, we generated dilated (factor 1) binary maps for each predicted track and multiplied them with each of those cross segments to obtain the largest overlapping track for each segment. We then visually inspected a few examples and determined that an overlap of 70% corresponds to a correctly resolved crossing and allowed for slight variations in predicted tracks when compared to ground truth. Finally, we calculated the fraction of crossings resolved per kymograph.</p><p>Derived quantities were calculated as follows. For average velocities, we first calculated the absolute frame to frame displacement and from there the average frame to frame velocity per track and the average frame to frame velocity per kymograph. The absolute displacement was calculated as a sum of all absolute frame to frame displacements and then averaged to yield a measure per kymograph. The travel time was calculated as the absolute time a particle was visible in a given kymograph and averaged for each kymograph. Directionality per particle was calculated as the sign of the end to end displacement for unidirectional kymographs. For bidirectional kymographs, we first calculated the directionality of up to five frame long segments, which was +1 when all displacements in that segment were positive, −1 when all displacements were negative, and 0 otherwise. The sign of the sum of all segment directionalities was taken as a measure for the bidirectional track directionality. The pause time for each bidirectional particle was calculated as the number of segments with 0 displacement and averaged per kymograph. Finally, the percentage of reversing tracks was calculated by dividing the number of tracks that exhibit segments in both directions by all tracks. In <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>, we only show relative deviations from the manual annotation because we cannot disclose any data from the real, unpublished kymographs obtained from collaborators.</p><p>All statistical analysis was carried out in MATLAB using either a Wilcoxon rank-sum test or a Kruskal Wallis test (<ext-link ext-link-type="uri" xlink:href="http://mathworks.com">http://mathworks.com</ext-link>).</p></sec><sec id="s4-6"><title>Module performance evaluation</title><p>To benchmark the unidirectional segmentation module of KymoButler, we generated 10 synthetic movies of the dynamics of particles that move with uniform speed and do not change direction as described in the section about synthetic data generation. We then imported these movies into ImageJ (<ext-link ext-link-type="uri" xlink:href="http://imagej.nih.gov">http://imagej.nih.gov</ext-link>) via the Kymograph Clear package (<xref ref-type="bibr" rid="bib23">Mangeol et al., 2016</xref>), drew a profile by hand and generated kymographs from them. These kymographs were then imported into the KymographDirect software package (also <xref ref-type="bibr" rid="bib23">Mangeol et al., 2016</xref>), Fourier filtered and thresholded to extract individual particle tracks. This approach required manual selection of the threshold for each individual kymograph. We additionally traced the same kymographs by hand in ImageJ to compare software performance to expert analysis. To find a suitable range of binarization thresholds for our unidirectional segmentation module we calculated the track wise F1 score on the 10 kymographs for thresholds between 0.05 and 0.5 (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>). We observed the highest scores between 0.1 and 0.3 for both our synthetic data and other unpublished kymographs and also deemed these thresholds best by visual inspection of predicted kymograph tracks. Hence, we chose 0.2 as the segmentation map threshold to benchmark our predictions at.</p><p>In order to benchmark the bidirectional segmentation module and the decision module we generated 10 synthetic movies of the dynamics of complex bidirectional particles. These movies were imported into ImageJ with the KymographClear package and kymographs extracted. We subsequently tried to use the edge detection option in KymographDirect to extract individual tracks but were unable to obtain meaningful tracks (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). We also tried other options in the package but could not get good results on our synthetic data without substantial manual labour for each kymograph, defeating the goal of a fully automated analysis. Therefore, we wrote a custom script to carry out automated bidirectional kymograph analysis. We experimented with a few different approaches (for example fourier-filtering and customised edge detection) and settled on wavelet coefficient filtering as it gave the highest F1 score on our test dataset. This algorithm applied a stationary wavelet transformation with Haar Wavelets (Mathematica wavelet package) to each kymograph to decompose the image into different coefficient images that highlight different details (for example vertical or horizontal lines). We then selected only those coefficient images that recapitulated particle traces in our synthetic kymographs. These images are overlaid and thresholded with an optimised threshold to generate binary maps that can be iteratively thinned to obtain a skeletonized ‘trackness’ map similar to the outputs of our segmentation modules. This map was then traced with the same algorithm as in our decision module. However, while the KymoButler decision module used a neural network to predict path crossings, the wavelet filtering algorithm performed simple linear prediction by taking the dilated (factor 1) binary segment of a track and rotating it by 180 degrees. Then the ‘prediction’ was multiplied with the skeletonized trackness map and the largest connected component selected as the future path. In contrast to the original decision module, this approach does not yield any information about decision ‘confidence’. Thus, to resolve track overlaps at the end of the algorithm, we randomly assigned each overlap to one track and deleted them from the others. Note that the wavelet approach was heavily optimised on our synthetic kymographs and performed poorly on generic real kymographs. We also traced the same 10 kymographs by hand in ImageJ. To find a suitable range of binarization thresholds for our bidirectional segmentation module we calculated the track wise F1 score for thresholds between 0.05 and 0.5 (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>) and observed the same optimal range as the unidirectional segmentation module (0.1–0.3) for both our synthetic data and other unpublished kymographs. Hence, we chose 0.2 as the threshold score to benchmark our predictions.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Eva Pillai for scientific input, proofreading, and logo design, Hannes Harbrecht for fruitful discussions about neural networks; Hendrik Schuermann, Ishaan Kapoor, and Kishen Chahwala for help with kymograph tracing; and the Mathematica stack exchange community (<ext-link ext-link-type="uri" xlink:href="https://mathematica.stackexchange.com">https://mathematica.stackexchange.com</ext-link>) without whom this project would have taken several decades longer. Unpublished kymographs to train KymoButler were provided by Caroline Bonnet (neurocampus, University of Bordeaux), Dr. Jean-Michel Cioni (San Raffaele Hospital, Milan), Dr. Julie Qiaojin Lin (University of Cambridge), Prof. Leah Gheber and Dr. Himanshu Pandey (Ben-Gurion University of the Negev), Dr. Carsten Janke and Satish Bodakuntla (Insitut Curie, Paris), and Dr. Timothy O’Leary and Adriano Bellotti (University of Cambridge). Additionally, we would like to thank the Bordeaux Imaging Center, part of the national infrastructure France BioImaging (ANR-10INBS-04–0), for valuable feedback on our software. We would also like to thank eLife and PLOS whose open access policy enabled us to show a variety of kymographs in this manuscript. The authors acknowledge funding by the Wellcome Trust (Research Grant 109145/Z/15/Z to MAHJ), the Herchel Smith Foundation (Fellowship to AD), Isaac Newton Trust (Research Grant 17.24(p) to KF), UK BBSRC (Research Project Grant BB/N006402/1 to KF), and the ERC (Consolidator Award 772426 to KF).</p></ack><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>We launched deepmirror.ai as a platform to promote the use of AI-based technologies for biological data analysis. We will be publishing tutorials and sample code to help people get started with developing their own machine learning software. We also intend to publish our work on KymoButler and future publications of our AI-based software on the website. All of this will be free of charge and available to all. Further in the future, we plan to also start offering paid professional services for customers that want to set up custom AI-based software for applications, in case they are not covered by our research. This software may or may not be made available on deepmirror.ai, depending on our clients' requests.</p></fn><fn fn-type="COI-statement" id="conf2"><p>We launched deepmirror.ai as a platform to promote the use of AI-based technologies for biological data analysis. We will be publishing tutorials and sample code to help people get started with developing their own machine learning software. We also intend to publish our work on KymoButler and future publications of our AI-based software on the website. All of this will be free of charge and available to all. Further in the future, we plan to also start offering paid professional services for customers that want to set up custom AI-based software for applications, in case they are not covered by our research. This software may or may not be made available on deepmirror.ai, depending on our clients' requests.</p></fn><fn fn-type="COI-statement" id="conf3"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Funding acquisition, Methodology, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-42288-transrepform-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code, if not stated otherwise, can be found online under: <ext-link ext-link-type="uri" xlink:href="https://github.com/MaxJakobs/KymoButler">https://github.com/MaxJakobs/KymoButler</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f47b5bc2e657f0c1f85a6c9e622fbd608dfdc7fd">https://archive.softwareheritage.org/swh:1:dir:f47b5bc2e657f0c1f85a6c9e622fbd608dfdc7fd</ext-link>). Source data for Figure 2 and 3 were uploaded as CSV files (Figure 2-source data 1 and Figure 3-source data 1) and the underlying datasets as zip files (Figure 2-source data 2 and Figure 3-source data 2).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexandrova</surname> <given-names>AY</given-names></name><name><surname>Arnold</surname> <given-names>K</given-names></name><name><surname>Schaub</surname> <given-names>S</given-names></name><name><surname>Vasiliev</surname> <given-names>JM</given-names></name><name><surname>Meister</surname> <given-names>JJ</given-names></name><name><surname>Bershadsky</surname> <given-names>AD</given-names></name><name><surname>Verkhovsky</surname> <given-names>AB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Comparative dynamics of retrograde actin flow and focal adhesions: formation of nascent adhesions triggers transition from fast to slow flow</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e3234</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0003234</pub-id><pub-id pub-id-type="pmid">18800171</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Applegate</surname> <given-names>KT</given-names></name><name><surname>Besson</surname> <given-names>S</given-names></name><name><surname>Matov</surname> <given-names>A</given-names></name><name><surname>Bagonis</surname> <given-names>MH</given-names></name><name><surname>Jaqaman</surname> <given-names>K</given-names></name><name><surname>Danuser</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>plusTipTracker: quantitative image analysis software for the measurement of microtubule dynamics</article-title><source>Journal of Structural Biology</source><volume>176</volume><fpage>168</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.jsb.2011.07.009</pub-id><pub-id pub-id-type="pmid">21821130</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babich</surname> <given-names>A</given-names></name><name><surname>Li</surname> <given-names>S</given-names></name><name><surname>O'Connor</surname> <given-names>RS</given-names></name><name><surname>Milone</surname> <given-names>MC</given-names></name><name><surname>Freedman</surname> <given-names>BD</given-names></name><name><surname>Burkhardt</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>F-actin polymerization and retrograde flow drive sustained plcγ1 signaling during T cell activation</article-title><source>The Journal of Cell Biology</source><volume>197</volume><fpage>775</fpage><lpage>787</lpage><pub-id pub-id-type="doi">10.1083/jcb.201201018</pub-id><pub-id pub-id-type="pmid">22665519</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barry</surname> <given-names>DJ</given-names></name><name><surname>Durkin</surname> <given-names>CH</given-names></name><name><surname>Abella</surname> <given-names>JV</given-names></name><name><surname>Way</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Open source software for quantification of cell migration, protrusions, and fluorescence intensities</article-title><source>The Journal of Cell Biology</source><volume>209</volume><fpage>163</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1083/jcb.201501081</pub-id><pub-id pub-id-type="pmid">25847537</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bates</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Extracting 3D vascular structures from microscopy images using convolutional recurrent networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.09597">https://arxiv.org/abs/1705.09597</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname> <given-names>CE</given-names></name><name><surname>Odde</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Traction dynamics of filopodia on compliant substrates</article-title><source>Science</source><volume>322</volume><fpage>1687</fpage><lpage>1691</lpage><pub-id pub-id-type="doi">10.1126/science.1163595</pub-id><pub-id pub-id-type="pmid">19074349</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chenouard</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>In 2010 17th IEEE international conference on image processing (ICIP 2010)</article-title><conf-name>Curvelet Analysis of Kymograph for Tracking Bi-Directional Particles in Fluorescence Microscopy Images</conf-name><fpage>3657</fpage><lpage>3660</lpage><pub-id pub-id-type="doi">10.1109/ICIP.2010.5652479</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cioni</surname> <given-names>JM</given-names></name><name><surname>Lin</surname> <given-names>JQ</given-names></name><name><surname>Holtermann</surname> <given-names>AV</given-names></name><name><surname>Koppers</surname> <given-names>M</given-names></name><name><surname>Jakobs</surname> <given-names>MAH</given-names></name><name><surname>Azizi</surname> <given-names>A</given-names></name><name><surname>Turner-Bridger</surname> <given-names>B</given-names></name><name><surname>Shigeoka</surname> <given-names>T</given-names></name><name><surname>Franze</surname> <given-names>K</given-names></name><name><surname>Harris</surname> <given-names>WA</given-names></name><name><surname>Holt</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Late endosomes act as mRNA translation platforms and sustain mitochondria in axons</article-title><source>Cell</source><volume>176</volume><fpage>56</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.11.030</pub-id><pub-id pub-id-type="pmid">30612743</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dai</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>R-FCN: object detection via Region-based fully convolutional networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1605.06409">https://arxiv.org/abs/1605.06409</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>del Castillo</surname> <given-names>U</given-names></name><name><surname>Winding</surname> <given-names>M</given-names></name><name><surname>Lu</surname> <given-names>W</given-names></name><name><surname>Gelfand</surname> <given-names>VI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Interplay between kinesin-1 and cortical dynein during axonal outgrowth and microtubule organization in <italic>Drosophila</italic> neurons</article-title><source>eLife</source><volume>4</volume><elocation-id>e10140</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.10140</pub-id><pub-id pub-id-type="pmid">26615019</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faits</surname> <given-names>MC</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name><name><surname>Soto</surname> <given-names>F</given-names></name><name><surname>Kerschensteiner</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dendritic mitochondria reach stable positions during circuit development</article-title><source>eLife</source><volume>5</volume><elocation-id>e11583</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.11583</pub-id><pub-id pub-id-type="pmid">26742087</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falk</surname> <given-names>T</given-names></name><name><surname>Mai</surname> <given-names>D</given-names></name><name><surname>Bensch</surname> <given-names>R</given-names></name><name><surname>Çiçek</surname> <given-names>Ö</given-names></name><name><surname>Abdulkadir</surname> <given-names>A</given-names></name><name><surname>Marrakchi</surname> <given-names>Y</given-names></name><name><surname>Böhm</surname> <given-names>A</given-names></name><name><surname>Deubner</surname> <given-names>J</given-names></name><name><surname>Jäckel</surname> <given-names>Z</given-names></name><name><surname>Seiwald</surname> <given-names>K</given-names></name><name><surname>Dovzhenko</surname> <given-names>A</given-names></name><name><surname>Tietz</surname> <given-names>O</given-names></name><name><surname>Dal Bosco</surname> <given-names>C</given-names></name><name><surname>Walsh</surname> <given-names>S</given-names></name><name><surname>Saltukoglu</surname> <given-names>D</given-names></name><name><surname>Tay</surname> <given-names>TL</given-names></name><name><surname>Prinz</surname> <given-names>M</given-names></name><name><surname>Palme</surname> <given-names>K</given-names></name><name><surname>Simons</surname> <given-names>M</given-names></name><name><surname>Diester</surname> <given-names>I</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name><name><surname>Ronneberger</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>U-Net: deep learning for cell counting, detection, and morphometry</article-title><source>Nature Methods</source><volume>16</volume><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id><pub-id pub-id-type="pmid">30559429</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Florian</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Gp-Unet: lesion detection from weak labels with a 3D regression network</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.07999">https://arxiv.org/abs/1705.07999</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerson-Gurwitz</surname> <given-names>A</given-names></name><name><surname>Thiede</surname> <given-names>C</given-names></name><name><surname>Movshovich</surname> <given-names>N</given-names></name><name><surname>Fridman</surname> <given-names>V</given-names></name><name><surname>Podolskaya</surname> <given-names>M</given-names></name><name><surname>Danieli</surname> <given-names>T</given-names></name><name><surname>Lakämper</surname> <given-names>S</given-names></name><name><surname>Klopfenstein</surname> <given-names>DR</given-names></name><name><surname>Schmidt</surname> <given-names>CF</given-names></name><name><surname>Gheber</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Directionality of individual kinesin-5 Cin8 motors is modulated by loop 8, ionic strength and microtubule geometry</article-title><source>The EMBO Journal</source><volume>30</volume><fpage>4942</fpage><lpage>4954</lpage><pub-id pub-id-type="doi">10.1038/emboj.2011.403</pub-id><pub-id pub-id-type="pmid">22101328</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Guerrero-Pena</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multiclass weighted loss for instance segmentation of cluttered cells</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.07465">https://arxiv.org/abs/1802.07465</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hangen</surname> <given-names>E</given-names></name><name><surname>Cordelières</surname> <given-names>FP</given-names></name><name><surname>Petersen</surname> <given-names>JD</given-names></name><name><surname>Choquet</surname> <given-names>D</given-names></name><name><surname>Coussen</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neuronal activity and intracellular calcium levels regulate intracellular transport of newly synthesized AMPAR</article-title><source>Cell Reports</source><volume>24</volume><fpage>1001</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.06.095</pub-id><pub-id pub-id-type="pmid">30044968</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaqaman</surname> <given-names>K</given-names></name><name><surname>Loerke</surname> <given-names>D</given-names></name><name><surname>Mettlen</surname> <given-names>M</given-names></name><name><surname>Kuwata</surname> <given-names>H</given-names></name><name><surname>Grinstein</surname> <given-names>S</given-names></name><name><surname>Schmid</surname> <given-names>SL</given-names></name><name><surname>Danuser</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Robust single-particle tracking in live-cell time-lapse sequences</article-title><source>Nature Methods</source><volume>5</volume><fpage>695</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1237</pub-id><pub-id pub-id-type="pmid">18641657</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title><italic>Adam: a method for stochastic optimization</italic></article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koseki</surname> <given-names>H</given-names></name><name><surname>Donegá</surname> <given-names>M</given-names></name><name><surname>Lam</surname> <given-names>BYH</given-names></name><name><surname>Petrova</surname> <given-names>V</given-names></name><name><surname>van Erp</surname> <given-names>S</given-names></name><name><surname>Yeo</surname> <given-names>GSH</given-names></name><name><surname>Kwok</surname> <given-names>JCF</given-names></name><name><surname>ffrench-Constant</surname> <given-names>C</given-names></name><name><surname>Eva</surname> <given-names>R</given-names></name><name><surname>Fawcett</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Selective rab11 transport and the intrinsic regenerative ability of CNS axons</article-title><source>eLife</source><volume>6</volume><elocation-id>e26956</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26956</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lazarus</surname> <given-names>JE</given-names></name><name><surname>Moughamian</surname> <given-names>AJ</given-names></name><name><surname>Tokito</surname> <given-names>MK</given-names></name><name><surname>Holzbaur</surname> <given-names>ELF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynactin subunit p150Glued is a Neuron-Specific Anti-Catastrophe factor</article-title><source>PLOS Biology</source><volume>11</volume><elocation-id>e1001611</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001611</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Boser</surname> <given-names>B</given-names></name><name><surname>Denker</surname> <given-names>JS</given-names></name><name><surname>Henderson</surname> <given-names>D</given-names></name><name><surname>Howard</surname> <given-names>RE</given-names></name><name><surname>Hubbard</surname> <given-names>W</given-names></name><name><surname>Jackel</surname> <given-names>LD</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Backpropagation applied to handwritten zip code recognition</article-title><source>Neural Computation</source><volume>1</volume><fpage>541</fpage><lpage>551</lpage><pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>BH</given-names></name><name><surname>Park</surname> <given-names>HY</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>HybTrack: a hybrid single particle tracking software using manual and automatic detection of dim signals</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>212</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-18569-3</pub-id><pub-id pub-id-type="pmid">29317715</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mangeol</surname> <given-names>P</given-names></name><name><surname>Prevo</surname> <given-names>B</given-names></name><name><surname>Peterman</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>KymographClear and KymographDirect: two tools for the automated quantitative analysis of molecular and cellular dynamics using kymographs</article-title><source>Molecular Biology of the Cell</source><volume>27</volume><fpage>1948</fpage><lpage>1957</lpage><pub-id pub-id-type="doi">10.1091/mbc.e15-06-0404</pub-id><pub-id pub-id-type="pmid">27099372</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Markerless tracking of user-defined features with deep learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1804.03142">https://arxiv.org/abs/1804.03142</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukherjee</surname> <given-names>A</given-names></name><name><surname>Jenkins</surname> <given-names>B</given-names></name><name><surname>Fang</surname> <given-names>C</given-names></name><name><surname>Radke</surname> <given-names>RJ</given-names></name><name><surname>Banker</surname> <given-names>G</given-names></name><name><surname>Roysam</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Automated kymograph analysis for profiling axonal transport of secretory granules</article-title><source>Medical Image Analysis</source><volume>15</volume><fpage>354</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1016/j.media.2010.12.005</pub-id><pub-id pub-id-type="pmid">21330183</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neumann</surname> <given-names>S</given-names></name><name><surname>Chassefeyre</surname> <given-names>R</given-names></name><name><surname>Campbell</surname> <given-names>GE</given-names></name><name><surname>Encalada</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>KymoAnalyzer: a software tool for the quantitative analysis of intracellular transport in neurons</article-title><source>Traffic</source><volume>18</volume><fpage>71</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1111/tra.12456</pub-id><pub-id pub-id-type="pmid">27770501</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Racine</surname> <given-names>V</given-names></name><name><surname>Sachse</surname> <given-names>M</given-names></name><name><surname>Salamero</surname> <given-names>J</given-names></name><name><surname>Fraisier</surname> <given-names>V</given-names></name><name><surname>Trubuil</surname> <given-names>A</given-names></name><name><surname>Sibarita</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Visualization and quantification of vesicle trafficking on a three-dimensional cytoskeleton network in living cells</article-title><source>Journal of Microscopy</source><volume>225</volume><fpage>214</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2818.2007.01723.x</pub-id><pub-id pub-id-type="pmid">17371444</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reis</surname> <given-names>GF</given-names></name><name><surname>Yang</surname> <given-names>G</given-names></name><name><surname>Szpankowski</surname> <given-names>L</given-names></name><name><surname>Weaver</surname> <given-names>C</given-names></name><name><surname>Shah</surname> <given-names>SB</given-names></name><name><surname>Robinson</surname> <given-names>JT</given-names></name><name><surname>Hays</surname> <given-names>TS</given-names></name><name><surname>Danuser</surname> <given-names>G</given-names></name><name><surname>Goldstein</surname> <given-names>LS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Molecular motor function in axonal transport in vivo probed by genetic and computational analysis in <italic>Drosophila</italic></article-title><source>Molecular Biology of the Cell</source><volume>23</volume><fpage>1700</fpage><lpage>1714</lpage><pub-id pub-id-type="doi">10.1091/mbc.e11-11-0938</pub-id><pub-id pub-id-type="pmid">22398725</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname> <given-names>O</given-names></name><name><surname>Fischer</surname> <given-names>P</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</chapter-title><source>Medical Image Computing and Computer-AssistedIntervention – MICCAI 2015</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer</publisher-name><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sbalzarini</surname> <given-names>IF</given-names></name><name><surname>Koumoutsakos</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Feature point tracking and trajectory analysis for video imaging in cell biology</article-title><source>Journal of Structural Biology</source><volume>151</volume><fpage>182</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1016/j.jsb.2005.06.002</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Szegedy</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Going deeper with convolutions</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.4842">https://arxiv.org/abs/1409.4842</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanenbaum</surname> <given-names>ME</given-names></name><name><surname>Vale</surname> <given-names>RD</given-names></name><name><surname>McKenney</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cytoplasmic dynein crosslinks and slides anti-parallel microtubules using its two motor domains</article-title><source>eLife</source><volume>2</volume><elocation-id>e00943</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.00943</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twelvetrees</surname> <given-names>AE</given-names></name><name><surname>Pernigo</surname> <given-names>S</given-names></name><name><surname>Sanger</surname> <given-names>A</given-names></name><name><surname>Guedes-Dias</surname> <given-names>P</given-names></name><name><surname>Schiavo</surname> <given-names>G</given-names></name><name><surname>Steiner</surname> <given-names>RA</given-names></name><name><surname>Dodding</surname> <given-names>MP</given-names></name><name><surname>Holzbaur</surname> <given-names>EL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The dynamic localization of cytoplasmic dynein in neurons is driven by Kinesin-1</article-title><source>Neuron</source><volume>90</volume><fpage>1000</fpage><lpage>1015</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.046</pub-id><pub-id pub-id-type="pmid">27210554</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weigert</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Content-Aware image restoration: pushing the limits of fluorescence microscopy</article-title><source>Nature</source><volume>15</volume><fpage>1090</fpage><lpage>1097</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0216-7</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zala</surname> <given-names>D</given-names></name><name><surname>Hinckelmann</surname> <given-names>MV</given-names></name><name><surname>Yu</surname> <given-names>H</given-names></name><name><surname>Lyra da Cunha</surname> <given-names>MM</given-names></name><name><surname>Liot</surname> <given-names>G</given-names></name><name><surname>Cordelières</surname> <given-names>FP</given-names></name><name><surname>Marco</surname> <given-names>S</given-names></name><name><surname>Saudou</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Vesicular glycolysis provides on-board energy for fast axonal transport</article-title><source>Cell</source><volume>152</volume><fpage>479</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2012.12.029</pub-id><pub-id pub-id-type="pmid">23374344</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.42288.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Bassereau</surname><given-names>Patricia</given-names></name><role>Reviewing Editor</role><aff><institution>Institut Curie</institution><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Urbach</surname><given-names>Jeff</given-names> </name><role>Reviewer</role><aff><institution>Georgetown University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Danuser</surname><given-names>Gaudenz</given-names> </name><role>Reviewer</role><aff><institution>UT Southwestern Medical Center</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>[Editors’ note: this article was originally rejected after discussions between the reviewers, but the authors were invited to resubmit after an appeal against the decision.]</p><p>Thank you for submitting your work entitled &quot;KymoButler: A deep learning software for automated kymograph analysis&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Jeff Urbach (Reviewer #1) and Gaudenz Danuser (Reviewer #2).</p><p>This manuscript describes KymoButler, a machine-learning software that has been developed for the automated analysis of kymographs. There is a big need in this area since kymograph analysis (or related linear tracking) is notoriously difficult to perform and to automate. Although the reviewers consider that KymoButler could potentially provide an open solution to automated kymograph tracking to the community, they eventually agree after discussion that in its current state, KymoButler has not been tested on challenging enough problems and so far, does not represent a general and new solution to complicated tracking problems. In particular, the reviewers were concerned about the performance of KymoButler for tracking elements on much more diverse traces that those considered in the manuscript: for instance, in the general case of axonal mobility tracking where cargoes or components can move in a direction, change speed, immobilize, reverse direction, and do any of this several times in a single trace. We feel that in its current form, this tool does not outperform some other existing tracking methods. For these reasons, we think that your manuscript does not meet the criteria of innovation of <italic>eLife</italic> and cannot be accepted for publication in the journal.</p><p><italic>Reviewer #1:</italic> </p><p>This manuscript describes a very valuable contribution to the field of quantitative biological imaging. It is well written and clearly motivated, and I have no substantial concerns. The results look solid, the approach is creative and appropriate, and it seems likely that the software will be widely utilized. As with all machine learning applications, it would be nice to have a deeper understanding of how the software is actually working, but that isn't a reason not to share the results in their current form.</p><p><italic>Reviewer #2:</italic> </p><p>This manuscript describes the use of a convolutional neural net (CNN) for the analysis of kymographs. While kymography is still used by the live cell imaging community to measure subcellular dynamics, and thus some tools to automate and reduce bias would be helpful, this work does not advance the existing repertoire of image analysis for this purpose.</p><p>First, the proposed method is not a kymograph analyzer, as claimed, but it relies on a CNN to learn a streak filter (the authors should have visualized the initial layers the trained net, and this would have become abundantly clear, I assume). The output of the CNN here is not a set of trajectories in the kymograph representation – the entity that eventually can be used to quantify particle velocities and, perhaps, lifetimes – but it offers merely a pixel-by-pixel map of a score that defines how probable the pixel is to be located on a streak. To get from the 'streakness' score map to the individual trajectories, the authors have to threshold the map, and then apply a morphological operator to thin the high score mask to single-pixel chains. Anyone who has written a program for line or edge detection knows, these two steps are much harder and are the decisive ones for the final results. These are the steps where the algorithm needs to account for streak crossings, junctions, and gaps. The authors largely wipe this under the rug. The results in Figures 2 and 3 reveal numerous places where exactly these two steps went wrong. And the authors probably intentionally pick data examples with relatively low particle density and/or fairly uniform particle motion. As the particle density and motion heterogeneity between and within trajectories increases, these limitations become overwhelming.</p><p>Second, the applicability of kymographs is very limited at large. Again, the authors wipe under the table that the task of selecting profiles is left to the user. When I was accepting to review this manuscript, I was hoping to see a creative solution to this very problem. The described software is far from providing an automated solution. In general, kymographs only work in scenarios where particles repeatedly follow a stationary path. As soon as trajectories deviate from the path, e.g. because of cell movement of deformation, the trajectory lifetime is unreliable and the velocity measurement, relying on streak integration in space and time, can get unstable/impossible. And even if sufficiently stationary paths exist, e.g. because the particle motion is much faster than the confounding cell movement, the paths are in all generality curvilinear and very difficult to define. For these reasons there is a sizeable community of labs developing single particle tracking methods. I do give the authors credit, however, for picking two scenarios, where the kymograph works. Axonal particle transport is one, Paul Foscher's glued down <italic>Aplysia</italic> is another. In both cases it is straightforward to manually identify a general particle path, and the required time scale separation between particle dynamics and path deformation holds up. The authors should have discussed at least the rare conditions under which kymograph analysis is valid. And, in this context, it would have been advantageous to present one full example where kymograph analysis would offer a new biological insight (like most high-profile methods paper do).</p><p>Third, the authors benchmark the performance of their kymograph analysis against a particle tracker. As the senior author of the plusTipTracker paper (published 2011) used as the benchmark reference, I get the shivers when I read that 'plusTipTracker is the gold standard for microtubule tracking'. Many advances have been made by us and other labs over the past 8 years. But, this is not the issue. The objection I raise here is that a kymograph analysis is tested against a particle tracking approach (whether it is plusTipTracker or any other software) in the one scenario the kymograph approach really has advantages over particle tracking. To repeat the statement above, the kymograph works well when particles follow a stationary path. And especially with bidirectional particle motion along this path, a more generic particle tracker that does not have the prior of directional stationarity, has a higher chance for confusion. This argument is trivial. My lab, for example, has taken the kymograph approach – also in an automated fashion – when we worked on bidirectional, axonal transport (PMID: 22398725). And, in my view, one of the most impressive examples of how the particle tracking and kymograph frameworks can be algorithmically merged to increase tracking robustness in cases of path stationarity is the work by Sibarita more than a decade ago (PMID:17371444). Neither one of these papers is discussed. If the authors want to benchmark their Deep Learning streak detector, then they should use automatic kymograph analyses as a reference (there are such tools in ImageJ, etc.). And in the spirit of my first comment, they should also benchmark against approaches that use steerable line filters or curvelets to generate the same score map. I am pretty sure that the Deep Learner will be widely outperformed by a filter that is designed to detect streaks a priori.</p><p>Even if the authors were to address these points, the innovation of this manuscript does not meet the bar of what I would expect to see in a journal like <italic>eLife</italic>. This tool may give the authors some useful results in their own research, but I do not think this is worth more than a subsection in a Materials and methods section.</p><p><italic>Reviewer #3:</italic> </p><p>In this manuscript, Jakobs et al. provide a machine-learning tool to analyze kymographs. This is a worthy addition to the state of the art, as kymograph analysis (or the related linear tracking) are notoriously difficult to perform and to automate. I think this tool could have a significant impact on a large community (researchers interested on quantifying subcellular mobility), if it can be made easily available, easy to use and broadly applicable. Related to this, I have a number of questions about the current manuscript I would like to discuss before its eventual publication in <italic>eLife</italic>.</p><p>The first point is about how this software is made available. Two authors of this manuscript (M. Jakobs and A. Dimitracopoulos) have launched a website/company, https://deepmirror.ai, for AI-based image processing, offering custom service and including the KymoButler software,. In addition, deepmirror.ai is providing the web-based KymoButler tool that is mentioned in the current manuscript (also, it looks like the web app now uses the deep network rather than the shallow one). This is a bit confusing, a clarification on what will be 1. open source 2. free to use (not necessarily open source) 3. commercial would be useful, as would be a more detailed &quot;Competing Interests&quot; section if needed.</p><p>The second point is the current limit to monodirectional tracking on kymographs. While this works for the few particular cases highlighted in the present manuscript (end-binding proteins on microtubules; actin speckles), this significantly limits the general usefulness of the tool. Kymograph analyses are generally done for bidirectional transport of cargoes (vesicles or organelles), usually along linear processes such as axons. Including in the present manuscript the extension of KymoButler to bidirectional tracking on kymographs (as is currently developed, as stated here: https://deepmirror.ai/2018/09/25/improvements-to-kymobutler/) and making it available in the open source/free tool would be a big leap in usefulness for the community. This would make a very strong case for publication in a high-profile, broad-readership journal such as <italic>eLife</italic>, and justify the general aspect of the current title &quot;A deep learning software for automated kymograph anaysis&quot;.</p><p>The third point is about network robustness. AI in image analysis is very useful, but the limiting factor for its adoption by biologists is the proper validation of deep-learning tools for different images, different situations etc. Due to a lack of knowledge from potential users, an improper use of deep-learning algorithms outside of their validated range is a real concern. In the current manuscript, the authors show that their trained algorithm can ba applied to a variety of cases (EBs in different cells, actin speckles) by adjusting the prediction probability threshold p. Do the authors think that the current tool could be generally used without the need of re-training? What advice/metric would help the interested researcher to validate results on one's own data? Would it be difficult (for a non-specialist) to re-train the open-source version of KymoButler for a particular application? Is this option considered as realistic by the authors? In my experience, this is usually the bottleneck of AI approaches in image analysis, because tools can be simple to use but the training part is usually beyond the skills of a non-specialist.</p><p>[Editors’ note: what now follows is the decision letter after the authors submitted for further consideration.]</p><p>Thank you for submitting your article &quot;KymoButler, a deep learning software for automated kymograph analysis&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Vivek Malhotra as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Jeff Urbach (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The manuscript by Jakobs et al. describes a fully automated approach to analyze kymographs based on deep learning. The software can run on both uni- and bidirectional kymographs. The code is freely available and the tool itself is also available via a web application. This is an important state of-the-art contribution as kymographs are notoriously difficult to obtain and to automate.</p><p>Essential revisions:</p><p>We are impressed with the substantial improvements to the analysis tool and the presentation in the revised manuscript. We all agree that KymoButler is a promising tool for the field. Nevertheless, some issues remain and should be addressed.</p><p>1) The authors have shown that the current version of the software can manage unidirectional and bidirectional movements. But, the capabilities of the software should be tested more thoroughly, both on synthetic and also on &quot;real&quot; data. For instance:</p><p>– Synthetic data: the limit of systems performances should be explored by varying density of tracings and signal/noise ratio. Moreover, the authors should also comment on the similarities and differences of synthetic data to real kymographs.</p><p>– Real data: the tool must be tested on several examples of different real situations presented (Rab11, mitochondria, microtubule plus-ends), distinct form the training data, and average performance should be quantified against manual expert tracing and best-performing available tool.</p><p>Since the authors already have a lot of manually annotated kymographs, they could remove some of them from the training set and get hard numbers on the performance of KymoButler on such data.</p><p>In addition, for both synthetic and real data, quantitative comparison should go as far as the end result that people seek to obtain from kymographs: average directionality,% of reversals, average speed, processivity, pause duration, etc. These quantitative parameters should be compared between Kymobutler, manual expert tracing and automatic alternatives in order to evaluate how the difference in tracing detection affects the final measurements.</p><p>2) The authors should state clearly in Introduction and Discussion what can and cannot be done with their software.</p><p>Considering that biological objects move in a stochastic 3D environment, the first paragraph of the Introduction should state explicitly which classes of motions are appropriate for kymographs and for KymoButler, respectively. It is not clearly described in the current version.</p><p>What a &quot; stationary path&quot; means should be explained.</p><p>In addition, it would be helpful to the user to know how KymoButler manages the stochastic versus directed motions as well as the constraints on the frequency of changes in direction. These points should be added in the Discussion.</p><p>3) It also appears that KymoButler's usability can be further improved and that some issues in the program should be fixed.</p><p>For instance, when an error occurs there is no possibility to try again, but instead one has to go back to previous screen, drag the input files over again, remember the old sensitivity threshold, submit again and hope for the best. Likewise, there is no possibility to simply change the sensitivity once you see the oversegmented results. One has to go back, drag all the data over, and repeat. Considering the allowed image size, simple thresholding should be fast enough to be interactive in the browser.</p><p>From a visual inspection, it is unclear how sensitivity parameter affects the outcome. It appears that the less sensitive setting (-1) does not pick some faint traces, and also breaks trajectories in pieces, resulting in a similar trace number as at the medium sensitivity setting.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.42288.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the author responses to the first round of peer review follow.]</p><disp-quote content-type="editor-comment"><p>This manuscript describes KymoButler, a machine-learning software that has been developed for the automated analysis of kymographs. There is a big need in this area since kymograph analysis (or related linear tracking) is notoriously difficult to perform and to automate. Although the reviewers consider that KymoButler could potentially provide an open solution to automated kymograph tracking to the community, they eventually agree after discussion that in its current state, KymoButler has not been tested on challenging enough problems and so far, does not represent a general and new solution to complicated tracking problems. In particular, the reviewers were concerned about the performance of KymoButler for tracking elements on much more diverse traces that those considered in the manuscript: for instance, in the general case of axonal mobility tracking where cargoes or components can move in a direction, change speed, immobilize, reverse direction, and do any of this several times in a single trace. We feel that in its current form, this tool does not outperform some other existing tracking methods. For these reasons, we think that your manuscript does not meet the criteria of innovation of eLife and cannot be accepted for publication in the journal.</p></disp-quote><p>We agree that the previously presented version of KymoButler did not apply to bidirectional cargo movement. To address this limitation, we have now developed an improved version of KymoButler, which is applicable to much more challenging tracking problems such as bidirectional cargo movement. We benchmarked the new software using complex synthetic (i.e., ground truth) data and tested it on bidirectional kymographs of a variety of biological problems including mitochondrial movement, in vitro dynamics of cytoplasmic dynein, Rab11 dynamics in axons, and microtubule plus end dynamics in dendrites. KymoButler is now able to accurately trace cargoes that move in a direction, change speed, immobilise, reverse direction, etc., even in very dense kymographs (new Figure 1 and Figure 1—figure supplement 1). It very reliably resolves crossings, junctions, and gaps, performs as well as manual data analysis by experts, and clearly outperforms current publicly available kymograph analysis tools. Our new software addresses all major concerns of the reviewers, and it will be made freely accessible to the community.</p><p>In our original submission, the presented data came from the same biological replicates as the training data. To properly challenge our software and to demonstrate that the new version of KymoButler can be applied to most if not all kymographs obtained from biological systems, we decided to replace these data in the revised manuscript with data previously published by other groups.</p><p>Reviewer #1:</p><disp-quote content-type="editor-comment"><p>This manuscript describes a very valuable contribution to the field of quantitative biological imaging. It is well written and clearly motivated, and I have no substantial concerns. The results look solid, the approach is creative and appropriate, and it seems likely that the software will be widely utilized. As with all machine learning applications, it would be nice to have a deeper understanding of how the software is actually working, but that isn't a reason not to share the results in their current form.</p></disp-quote><p>In the original manuscript, we have focused more on the process of extracting information from the kymographs than on how the software works. We agree that the user should have a deeper understanding of the actual workings of the software and therefore now provide a more detailed visualisation of the output of each module of the software (Figure 1—figure supplements 1 and 2).</p><p>Reviewer #2:</p><disp-quote content-type="editor-comment"><p>This manuscript describes the use of a convolutional neural net (CNN) for the analysis of kymographs. While kymography is still used by the live cell imaging community to measure subcellular dynamics, and thus some tools to automate and reduce bias would be helpful, this work does not advance the existing repertoire of image analysis for this purpose.</p><p>First, the proposed method is not a kymograph analyzer, as claimed, but it relies on a CNN to learn a streak filter (the authors should have visualized the initial layers the trained net, and this would have become abundantly clear, I assume).</p></disp-quote><p>KymoButler does not rely on learning a streak filter, and indeed it does not use any streak filters. A streak filter is decided a priori to extract potential linear traces from an image, but does not learn the characteristic signal and background from the images. In fact, if the background has streak-like properties, the filter will classify it as signal independently of the available data. Hence, in the context of kymographs derived from live cell imaging, a streak filter is unlikely to recover many meaningful tracks, due to the excessive background fluctuation which has to be taken into account (by the user) during the design of the filter (see also our new Figure 3).</p><p>KymoButler implements a different approach. Instead of assuming that a streak filter will be the best operation to extract traces from kymographs, our software selects the best possible operations through several rounds of optimisation, which aim to best match the output of the neural network to the provided training data labels derived from manual annotation. This means that the network learns which pixels are part of a trace and which ones are not based on the available data, and not on a priori considerations. This is possible due to the incredible improvements in computation times of modern CPUs and to the adoption of GPUs that can execute an enormous number of operations in parallel. This gives us a great advantage, with the main downside being that it becomes more difficult to gain a deeper understanding of precisely how the software is working (as highlighted by reviewers #1 and #3).</p><p>To make our point clearer, we would also like to highlight the difference between the 5x5 kernels of our original neural network (<xref ref-type="fig" rid="respfig1">Author response image 1A</xref>), the 3x3 kernels of our new segmentation modules (<xref ref-type="fig" rid="respfig1">Author response image 1B</xref>), and 5x5 streak filters (<xref ref-type="fig" rid="respfig1">Author response image 1C</xref>). There are no obvious streak kernels in our approach.</p><fig id="respfig1"><label>Author response image 1.</label><caption><title>Normalised kernel entries of our fully convolutional neural network.</title><p>(<bold>A</bold>) Normalised 5x5 kernels from the first layer of the network from our original manuscript. (<bold>B</bold>) Normalised 3x3 kernels from the first layer of our unidirectional/bidirectional module. (<bold>C</bold>) Example 5x5 streak filters. The values run from 0 (black) to 1 (white). No obvious line filter structure is visible in our kernels.</p></caption><graphic xlink:href="elife-42288-resp-fig1-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><disp-quote content-type="editor-comment"><p>The output of the CNN here is not a set of trajectories in the kymograph representation – the entity that eventually can be used to quantify particle velocities and, perhaps, lifetimes – but it offers merely a pixel-by-pixel map of a score that defines how probable the pixel is to be located on a streak.</p></disp-quote><p>This is correct and also consistent with our claims in the text. The power of CNNs is precisely this ability to identify such probability maps. However, we would like to emphasize that the probability map also takes into account the immediate neighbourhood of each pixel, to fill gaps in the images and ignore background noise.</p><disp-quote content-type="editor-comment"><p>To get from the 'streakness' score map to the individual trajectories, the authors have to threshold the map, and then apply a morphological operator to thin the high score mask to single-pixel chains. Anyone who has written a program for line or edge detection knows, these two steps are much harder and are the decisive ones for the final results.</p></disp-quote><p>As mentioned in the manuscript, we indeed apply morphological binarization and thinning which can be done using available codes in MATLAB or Mathematica. In our experience, this step is rather straightforward and provides excellent results as the maps are heavily de-noised and the SNR is thus very high (see Figure 1—figure supplement 1B).</p><disp-quote content-type="editor-comment"><p>These are the steps where the algorithm needs to account for streak crossings, junctions, and gaps.</p></disp-quote><p>The previous version of KymoButler that was included with the initial submission only analysed unidirectional particle traces, i.e. particles that did not change direction and only varied moderately in speed. As explained in the manuscript, we took care of crossings by training our neural network to only recognise traces that have negative slopes (from left upper to right lower corners). This network could then be run on the raw kymograph and its reflection to extract traces without crossings and junctions. This is possible since the data that we used to train and benchmark KymoButler does not exhibit any crossings/junctions of lines with similar slopes, i.e. they are unidirectional with almost uniform speeds. Gaps however, were taken care of by the larger convolutions of our neural network: If a particle becomes invisible for a few frames the neural network is able to bridge the gap by assigning larger scores to pixels that lie in the gap.</p><fig id="respfig2"><label>Author response image 2.</label><caption><title>Zoom into the old Figure 1E from our manuscript.</title><p>(<bold>A</bold>) depicts the raw kymograph and (<bold>B</bold>) the pixel score map from our neural network. The track highlighted in red exhibits two gaps where the particle becomes invisible for three frames each. As seen in (<bold>B</bold>), the network has no problem to bridge those gaps and assign high scores throughout the gap. Scale bars 2 µm (horizontal), 25 sec vertical. See also our new Figures 2 and 3, where we benchmark KymoButler (and its capability to bridge gaps) against ground truth data.</p></caption><graphic xlink:href="elife-42288-resp-fig2-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><p>As the reviewers pointed out, the software presented in the original manuscript was unable to track bidirectional particles. However, with the new version of KymoButler this is now possible. As explained in more detail in the main text, we first use a segmentation module to filter all connected lines from a given kymograph. The resulting image comprises several thin lines that might form repeated crossings and junctions but no gaps. We then automatically trace each of these thin lines until a junction/crossing is met. To predict what happens next we designed a decision module that solves the junction/crossing by finding the most likely future path (see new Figure 1—figure supplement 1B for resulting kymographs). We are confident that this approach will allow any user to analyse their specific kymographs.</p><p>Furthermore, we now generated synthetic data with available ground truth that contained gaps and crossings/junctions, which we benchmark the performance of KymoButler against (new Figures 2 and 3). This test clearly demonstrated that KymoButler accurately identifies that vast majority of all traces, and that is does perform as well as expert human experimentalists.</p><disp-quote content-type="editor-comment"><p>The authors largely wipe this under the rug.</p></disp-quote><p>It was not our intention to sweep anything under the rug, and we apologize if parts of our manuscript came across this way. To avoid such misunderstandings, we expanded on the description of our approaches in the main text and added the new Figures 2 and 3 to the manuscript. Furthermore, our new KymoButler deals with these issues, and we evaluated the performance of the new software with respect to crossings, junctions, and gaps.</p><disp-quote content-type="editor-comment"><p>The results in Figures 2 and 3 reveal numerous places where exactly these two steps went wrong.</p></disp-quote><p>We would be grateful if the reviewer could highlight these occurrences so that we can debug our code and put more weight on training instances that show similar behavior. We could only identify 2 such cases in the kymographs shown in Figures 2/3F of our initial submission (see <xref ref-type="fig" rid="respfig3">Author response image 3</xref>).</p><p>When we re-analyzed this figure with our new software, the two problematic points were resolved correctly (<xref ref-type="fig" rid="respfig3">Author response image 3C</xref>).</p><fig id="respfig3"><label>Author response image 3.</label><caption><title>The kymograph shown in Figure 2 of our original manuscript analysed with the original KymoButler and the new version.</title><p>We highlighted the two errors we could identify in the old version in panel B (red frames). In the upper one, a junction was not resolved properly, and in the lower one two lines were so close to each other that they were segmented as one. Neither of these errors showed up when we re-ran the data in the new KymoButler. Scale bars: 2 µm (horizontal), 25 sec (vertical).</p></caption><graphic xlink:href="elife-42288-resp-fig3-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><disp-quote content-type="editor-comment"><p>And the authors probably intentionally pick data examples with relatively low particle density and/or fairly uniform particle motion.</p></disp-quote><p>We would never cherry pick data. It would be disingenuous and fraudulent, not to mention a violation of the very most basic principles of research.</p><p>The kymographs shown in the old Figures 2 and 3 were representative kymographs of our biological case study. However, we have now replaced these data with previously published, much more complex and dense kymographs, and hope that the reviewer agrees that KymoButler now performs excellently even in kymographs with high particle density and non-uniform particle motion.</p><disp-quote content-type="editor-comment"><p>As the particle density and motion heterogeneity between and within trajectories increases, these limitations become overwhelming.</p></disp-quote><p>We agree with the reviewer, this was a limitation of our initial software. We became aware of this problem when we were contacted by several other labs immediately after uploading our manuscript on bioRxiv, requesting a bidirectional analysis software. Hence, we have been working on a solution since, which is now included in the revised version of the manuscript.</p><disp-quote content-type="editor-comment"><p>Second, the applicability of kymographs is very limited at large. Again, the authors wipe under the table that the task of selecting profiles is left to the user.</p></disp-quote><p>We both agree and disagree with the sentiment of the reviewer. While kymograph applicability may seem limited, this is true of many other techniques and methods in biology (e.g. atomic force microscopy analysis / traction force microscopy analysis). However, this does not detract from how useful these methods and tools are to the researchers using them. Researchers studying neuronal transport, actin flow, or cilia/flagella, would arguably find kymographs, and new tools to analyze them reliably, extremely useful. The preprint of our software attracted significant attention (top 5% of all research outputs scored by Altmetric), and we were provided with training data by six other laboratories immediately to develop the current bidirectional KymoButler. We hope that this is a fair indicator of this work being both highly interesting and useful to many others in the community.</p><p>The task of selecting the profile along which a kymograph is made is highly specific to the biological application and is best left to a given user. Automating this process is non-trivial, particularly if we intend to make a kymograph analyser suitable for a myriad of applications. Furthermore, as mentioned in the manuscript, there are numerous packages available that make it very easy to extract kymographs from live imaging data. Hence, we did not wish to duplicate these efforts.</p><disp-quote content-type="editor-comment"><p>When I was accepting to review this manuscript, I was hoping to see a creative solution to this very problem. The described software is far from providing an automated solution. In general, kymographs only work in scenarios where particles repeatedly follow a stationary path. As soon as trajectories deviate from the path, e.g. because of cell movement of deformation, the trajectory lifetime is unreliable and the velocity measurement, relying on streak integration in space and time, can get unstable/impossible. And even if sufficiently stationary paths exist, e.g. because the particle motion is much faster than the confounding cell movement, the paths are in all generality curvilinear and very difficult to define. For these reasons there is a sizeable community of labs developing single particle tracking methods. I do give the authors credit, however, for picking two scenarios, where the kymograph works. Axonal particle transport is one, Paul Foscher's glued down Aplysia is another. In both cases it is straightforward to manually identify a general particle path, and the required time scale separation between particle dynamics and path deformation holds up. The authors should have discussed at least the rare conditions under which kymograph analysis is valid. And, in this context, it would have been advantageous to present one full example where kymograph analysis would offer a new biological insight (like most high-profile methods paper do).</p></disp-quote><p>We agree with the reviewer that in a large number of scenarios, kymographs cannot be applied and particle tracking is the superior method. However, in scenarios where kymographs <italic>can</italic> be applied, they have a number of advantages compared to particle tracking approaches, as they allow researchers to simultaneously visualise and analyse their data, and they are well suited for low signal to noise ratio data. Kymographs are applicable to many diverse studies of cytoskeletal dynamics and protein/molecular/vesicle trafficking. We feel that a profound discussion of the usefulness of kymographs is beyond the scope of the current study; however, we have now included information about when kymographs are useful in the first part of the Introduction. We would like to re-emphasize that the intent of this work is to provide a tool to the community which enables automated kymograph analysis, since many laboratories still struggle with this task.</p><disp-quote content-type="editor-comment"><p>Third, the authors benchmark the performance of their kymograph analysis against a particle tracker. As the senior author of the plusTipTracker paper (published 2011) used as the benchmark reference, I get the shivers when I read that 'plusTipTracker is the gold standard for microtubule tracking'. Many advances have been made by us and other labs over the past 8 years.</p></disp-quote><p>We apologise for our wording. We should have stated “plusTipTracker (and derived software packages, e.g. utrack) is a gold standard for microtubule tracking”, as it is still widely used in the community. When we started analysing microtubule plus end dynamics, we looked into available software and tested many different programs. In our hands, the plusTipTracker showed the most excellent performance. We have not only compared the performance of KymoButler against that of plusTipTracker but also against a number of other software packages but thought it only worth showing the comparison against the best performing algorithm available. We do apologize if this comparison may have come across as unflattering or critical, rather than as an advancement on what we believed to be the current best tool for the purpose.</p><p>However, we appreciate the comment of the reviewer, and as we do not focus on microtubule dynamics any longer but rather on more complex problems as suggested by the reviewers, we have replaced the comparison with the plusTipTracker by software that was designed to analyse kymographs in the revised version of the manuscript. In the new Figure 2, we benchmark KymoButler against the Fourier filtering module of Kymograph Direct, and in the new Figure 3 against a custom-written wavelet filtering algorithm as suggested by the reviewer (see below), as Kymograph Direct here did not return any meaningful data (Figure 3—figure supplement 1).</p><disp-quote content-type="editor-comment"><p>But, this is not the issue. The objection I raise here is that a kymograph analysis is tested against a particle tracking approach (whether it is plusTipTracker or any other software) in the one scenario the kymograph approach really has advantages over particle tracking. To repeat the statement above, the kymograph works well when particles follow a stationary path. And especially with bidirectional particle motion along this path, a more generic particle tracker that does not have the prior of directional stationarity, has a higher chance for confusion. This argument is trivial. My lab, for example, has taken the kymograph approach – also in an automated fashion – when we worked on bidirectional, axonal transport (PMID: 22398725).</p></disp-quote><p>As mentioned above, we agree with the reviewer that there are many scenarios where kymographs cannot be applied, and particle tracking approaches are superior, and we apologize for not explicitly stating this in the original version of our manuscript. In the revised Introduction, we now acknowledge the importance of particle tracking approaches and explain in which specific scenarios kymographs are useful.</p><p>We thank the reviewer for the reference provided, which we now cite. However, we are unsure to what extent this publication is comparable to our manuscript. In the Materials and methods section of that paper, the following statement is made: “The output of this step contains mostly trajectory segments, so an additional process was performed to link these segments into full trajectories using a multiple hypothesis-testing algorithm (Blackman, 1999). A <underline>manual</underline> process was subsequently used to recover trajectories the software was unable to recover. Finally, all recovered trajectories were <italic>individually inspected, and errors were corrected</italic>.” We are unclear on how reliably the automated parts of this process work without manual supervision, and as no open source software was provided with the paper, we could not compare it with our tool easily. If the software could be made publically available, we would be happy to try it.</p><disp-quote content-type="editor-comment"><p>And, in my view, one of the most impressive examples of how the particle tracking and kymograph frameworks can be algorithmically merged to increase tracking robustness in cases of path stationarity is the work by Sibarita more than a decade ago (PMID:17371444). Neither one of these papers is discussed.</p></disp-quote><p>We are very grateful for bringing this publication to our attention, which we now also cite. The authors here very nicely demonstrated the power and versatility of 4D kymogram analysis. We hope that we can also extend our approach to 4D in the future. However, while the authors of PMID:17371444 developed an automated approach for the extraction of tracks from 4D kymograms, the software does not appear to be publically available. Additionally, the analysed cells were <italic>Hela</italic> cells that overexpressed pEGFP-Rab6, leading to very high signal to noise ratios (but potentially also to changes in protein dynamics) in contrast to endogenously expressed EB1GFP encountered in many studies on microtubule dynamics and in our original manuscript. Hence, it is not clear how versatile this software is or how applicable it is to the type of questions we (and other groups using kymograph analysis) ask.</p><disp-quote content-type="editor-comment"><p>If the authors want to benchmark their Deep Learning streak detector, then they should use automatic kymograph analyses as a reference (there are such tools in ImageJ, etc.). And in the spirit of my first comment, they should also benchmark against approaches that use steerable line filters or curvelets to generate the same score map. I am pretty sure that the Deep Learner will be widely outperformed by a filter that is designed to detect streaks a priori.</p></disp-quote><p>We would like to reiterate that KymoButler is not a streak detector (see <xref ref-type="fig" rid="respfig2">Author response image 2</xref>). We are not aware of any fully automated kymograph analysis tools in ImageJ, plugins such as MultiKymograph, KymoGraphBuilder and other tools, e.g. KymographTracker (ICY), can be used to generate and threshold kymographs but require manual ROI selection for data analysis. We would be grateful if the reviewer could advise us on existing algorithms that are fully automated.</p><p>As suggested by the reviewer, we now benchmark the unidirectional module of KymoButler against KymographDirect (PMID: 27099372), which implements semi-automated analysis through Fourier filtering and local thresholding. Additionally, we benchmark the bidirectional module against a custom wavelet filtering algorithm as we did not obtain any satisfactory results with any available software we tested (Figure 3—figure supplement 1). Both tests showed that our Deep Learning Software clearly outperforms any other publically available software; it actually performs as accurately as manual expert annotation, just much faster and without any unconscious bias.</p><disp-quote content-type="editor-comment"><p>Even if the authors were to address these points, the innovation of this manuscript does not meet the bar of what I would expect to see in a journal like eLife. This tool may give the authors some useful results in their own research, but I do not think this is worth more than a subsection in a Materials and methods section.</p></disp-quote><p>The cloud-based software has been used on over 1800 unique images by the wider community since the paper was uploaded on bioRxiv 5 months ago. We have received plenty of positive feedback on the use of KymoButler for kymograph analysis workflows from many other groups at different institutions worldwide at conferences, via email, and even through social media. Hence, we strongly believe that KymoButler is very useful to a much broader base than our research group alone. We are now in close collaboration with several of these laboratories, who are sending us their data, and helping us improve our network further. The inclusion of bidirectional tracking in our fully automated KymoButler is certainly of wide interest to an even larger community already working with kymographs. Furthermore, a fully automated software has great potential to convince more groups to work with kymographs in the future.</p><p>Reviewer #3:</p><disp-quote content-type="editor-comment"><p>In this manuscript, Jakobs et al. provide a machine-learning tool to analyze kymographs. This is a worthy addition to the state of the art, as kymograph analysis (or the related linear tracking) are notoriously difficult to perform and to automate. I think this tool could have a significant impact on a large community (researchers interested on quantifying subcellular mobility), if it can be made easily available, easy to use and broadly applicable. Related to this, I have a number of questions about the current manuscript I would like to discuss before its eventual publication in eLife.</p></disp-quote><p>We would like to thank the reviewer for this very positive evaluation of the impact of our tool.</p><disp-quote content-type="editor-comment"><p>The first point is about how this software is made available. Two authors of this manuscript (M. Jakobs and A. Dimitracopoulos) have launched a website/company, https://deepmirror.ai, for AI-based image processing, offering custom service and including the KymoButler software,. In addition, deepmirror.ai is providing the web-based KymoButler tool that is mentioned in the current manuscript (also, it looks like the web app now uses the deep network rather than the shallow one). This is a bit confusing, a clarification on what will be 1. open source 2. free to use (not necessarily open source) 3. commercial would be useful, as would be a more detailed &quot;Competing Interests&quot; section if needed.</p></disp-quote><p>We apologise for the confusion the webpage might have caused. We launched deepmirror.ai as a platform to promote the use of AI-based technologies for biological data analysis. We will be publishing tutorials and sample code to help people get started with developing their own machine learning software. We also intend to publish our work on KymoButler and future publications of our AI-based software on the website. All of this will be free of charge and available to all. Further in the future, we plan to also start offering paid professional services for customers who want to set up custom AI-based software for applications, in case they are not covered by our research. This software may or may not be made available on deepmirror.ai, depending on our clients’ requests. We amended the “Competing Interests” section in our manuscript to clarify our position.</p><p>We apologise for changing the neural network in the webform during the review process. We realised that some of our users had much better results with the deeper version, and hence decided to optimise that network and make it available on the Cloud. We also include a link to a Mathematica library in the manuscript, as a set of functions that can be easily integrated into other workflows. Additionally, we provide a link to a Mathematica script which enables experienced users to train their own network. We would like to reiterate that the software presented in this paper will <underline>not</underline> be commercialized, but will be freely available to every user.</p><disp-quote content-type="editor-comment"><p>The second point is the current limit to monodirectional tracking on kymographs. While this works for the few particular cases highlighted in the present manuscript (end-binding proteins on microtubules; actin speckles), this significantly limits the general usefulness of the tool. Kymograph analyses are generally done for bidirectional transport of cargoes (vesicles or organelles), usually along linear processes such as axons. Including in the present manuscript the extension of KymoButler to bidirectional tracking on kymographs (as is currently developed, as stated here: https://deepmirror.ai/2018/09/25/improvements-to-kymobutler/) and making it available in the open source/free tool would be a big leap in usefulness for the community. This would make a very strong case for publication in a high-profile, broad-readership journal such as eLife, and justify the general aspect of the current title &quot;A deep learning software for automated kymograph anaysis&quot;.</p></disp-quote><p>We thank the reviewer for this excellent suggestion and the encouraging comments on the importance and usefulness of our software. As mentioned in the general comments above, the new version of KymoButler allows exactly what the reviewer recognizes to be needed by the community. The improved KymoButler will replace our old version and be made available free-to-use.</p><disp-quote content-type="editor-comment"><p>The third point is about network robustness. AI in image analysis is very useful, but the limiting factor for its adoption by biologists is the proper validation of deep-learning tools for different images, different situations etc. Due to a lack of knowledge from potential users, an improper use of deep-learning algorithms outside of their validated range is a real concern. In the current manuscript, the authors show that their trained algorithm can ba applied to a variety of cases (EBs in different cells, actin speckles) by adjusting the prediction probability threshold p. Do the authors think that the current tool could be generally used without the need of re-training?</p></disp-quote><p>We are glad that the reviewer touched upon this important point, which is true of most (if not all) automated data analysis tools. Often, a network trained for a specific task can result in catastrophic artefacts when applied to a different problem. However, the new bidirectional KymoButler should be applicable to most if not all Kymograph problems. Nevertheless, it is good practice for users to visually inspect analysed kymographs to confirm the suitability of this tool. In our web-based application, the input image is overlaid with KymoButler’s predicted traces before the result is downloaded to ensure that no result is obtained without a visual inspection. In addition, we have a webform should users need to contact us with any concerns, e.g. if the network produces unexpected results. We now explicitly point at this in the Discussion.</p><disp-quote content-type="editor-comment"><p>What advice/metric would help the interested researcher to validate results on one's own data?</p></disp-quote><p>We would strongly suggest comparing any automated software to manual annotation as the best benchmark, which we now mention in the Discussion.</p><disp-quote content-type="editor-comment"><p>Would it be difficult (for a non-specialist) to re-train the open-source version of KymoButler for a particular application? Is this option considered as realistic by the authors? In my experience, this is usually the bottleneck of AI approaches in image analysis, because tools can be simple to use but the training part is usually beyond the skills of a non-specialist.</p></disp-quote><p>The bidirectional KymoButler should be applicable to all kymographs from biological data, and the software will constantly be updated as users upload their kymographs. However, should the user wish to repurpose and retrain our software, the code is freely available (see above). This could be a bottleneck, as it is not straightforward to retrain the networks with new data. Nonetheless, it should be possible for a non-specialist with some experience in programming to accomplish, and we will design tutorials for this purpose and make them available on our website. In addition, we will be contactable for consultations, should a user face difficulties with repurposing the networks.</p><p>[Editors’ note: the author responses to the re-review follow.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>We are impressed with the substantial improvements to the analysis tool and the presentation in the revised manuscript. We all agree that KymoButler is a promising tool for the field. Nevertheless, some issues remain and should be addressed.</p><p>1) The authors have shown that the current version of the software can manage unidirectional and bidirectional movements. But, the capabilities of the software should be tested more thoroughly, both on synthetic and also on &quot;real&quot; data. For instance:</p><p>– Synthetic data: the limit of systems performances should be explored by varying density of tracings and signal/noise ratio. Moreover, the authors should also comment on the similarities and differences of synthetic data to real kymographs.</p></disp-quote><p>This is an excellent suggestion. We added a new supplementary figure (Figure 1—figure supplement 5) investigating the effects of increasing noise and density on the track recall/precision for both unidirectional and bidirectional kymographs. We find that KymoButler performs well down to SNRs of ~1.2 and a signal coverage of up to ~60%, similar to manual analysis at comparable SNRs and signal coverage. We also now discuss the differences between synthetic and real data in the Discussion (paragraph five).</p><disp-quote content-type="editor-comment"><p>– Real data: the tool must be tested on several examples of different real situations presented (Rab11, mitochondria, microtubule plus-ends), distinct form the training data, and average performance should be quantified against manual expert tracing and best-performing available tool.</p><p>Since the authors already have a lot of manually annotated kymographs, they could remove some of them from the training set and get hard numbers on the performance of KymoButler on such data.</p><p>In addition, for both synthetic and real data, quantitative comparison should go as far as the end result that people seek to obtain from kymographs: average directionality,% of reversals, average speed, processivity, pause duration, etc. These quantitative parameters should be compared between Kymobutler, manual expert tracing and automatic alternatives in order to evaluate how the difference in tracing detection affects the final measurements.</p></disp-quote><p>As previously mentioned in the Materials and methods, we already had both a training and a validation data set. We could not disclose our raw training and validation data in this manuscript, as all of the data are currently unpublished, and we agreed with all laboratories providing us with their (unpublished) kymographs not to publish the raw data. However, to address this important point, we now added two supplementary figures to the manuscript (Figure 2—figure supplement 1, Figure 3—figure supplement 2) that quantify KymoButler’s relative average performance against manual expert tracing and best-performing available tool on synthetic as well as real data, investigating all parameters mentioned above.</p><p>The real data used in our new comparison include 6 kymographs with labelled microtubule plus ends for the unidirectional analysis, 2 kymographs showing axonal mitochondria dynamics, one showing molecular motor processivity, and 3 kymographs depicting vesicle movement in axons and dendrites. Since we do not have a ground truth for the real kymographs, we cannot compare recall/precision, gap detection, and crossing detection in those.</p><disp-quote content-type="editor-comment"><p>2) The authors should state clearly in Introduction and Discussion what can and cannot be done with their software.</p><p>Considering that biological objects move in a stochastic 3D environment, the first paragraph of the Introduction should state explicitly which classes of motions are appropriate for kymographs and for KymoButler, respectively. It is not clearly described in the current version.</p><p>What a &quot; stationary path&quot; means should be explained.</p></disp-quote><p>We are grateful for these suggestions and changed the first paragraph of the Introduction to clarify when kymographs and hence KymoButler should be used. Additionally, we now explain what a stationary path is in both the Introduction and the legend of Figure 1A.</p><disp-quote content-type="editor-comment"><p>In addition, it would be helpful to the user to know how KymoButler manages the stochastic versus directed motions as well as the constraints on the frequency of changes in direction. These points should be added in the Discussion.</p></disp-quote><p>We added more quantification of KymoButler’s performance on both directed (Figure 2—figure supplement 1) and stochastic motion (Figure 3—figure supplement 2) including a quantification of the likelihood of reversals. Notably, KymoButler does not have any restraints on the frequency of changes in direction (stochasticity) as shown in <xref ref-type="fig" rid="respfig4">Author response image 4</xref>. However, if an increase in stochastic movements leads to an increase in the number of crossings in dense kymographs, KymoButler’s performance is affected adversely (see new Figure 1—figure supplement 5), which however would also be true for expert manual data analysis. We now discuss this in paragraph two of the Discussion.</p><fig id="respfig4"><label>Author response image 4.</label><caption><title>Track recall and precision as a function of number of particle direction changes.</title></caption><graphic xlink:href="elife-42288-resp-fig4-v2.tif" mimetype="image" mime-subtype="tiff"/></fig><disp-quote content-type="editor-comment"><p>3) It also appears that KymoButler's usability can be further improved and that some issues in the program should be fixed.</p><p>For instance, when an error occurs there is no possibility to try again, but instead one has to go back to previous screen, drag the input files over again, remember the old sensitivity threshold, submit again and hope for the best. Likewise, there is no possibility to simply change the sensitivity once you see the oversegmented results. One has to go back, drag all the data over, and repeat. Considering the allowed image size, simple thresholding should be fast enough to be interactive in the browser.</p><p>From a visual inspection, it is unclear how sensitivity parameter affects the outcome. It appears that the less sensitive setting (-1) does not pick some faint traces, and also breaks trajectories in pieces, resulting in a similar trace number as at the medium sensitivity setting.</p></disp-quote><p>We are very grateful for the reviewer’s suggestions how to improve our software. We changed the webform so that one first uploads a kymograph now and then dynamically adjusts the threshold, allowing fast screening of different segmentations. Note that simple thresholding only applies to unidirectional kymographs, while bidirectional track segmentation takes longer as here paths have to be traced. Additionally, we now also provide an application programming interface (upon request), that enables programmers to call our software from any other programming language to incorporate it in their workflow. We also removed the misleading sensitivity parameter and replaced it with a single threshold value, which is the same as the threshold discussed in the manuscript. Finally, we added more post-processing functionality by allowing users to discard small fragmented tracks and added quantity averages and plots to the output.</p></body></sub-article></article>