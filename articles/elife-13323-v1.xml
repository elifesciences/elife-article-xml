<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">13323</article-id><article-id pub-id-type="doi">10.7554/eLife.13323</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature article</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Research</subject></subj-group></article-categories><title-group><article-title>NIH peer review percentile scores are poorly predictive of grant productivity</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-45407"><name><surname>Fang</surname><given-names>Ferric C</given-names></name><x>is in the</x><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-48046"><name><surname>Bowen</surname><given-names>Anthony</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1064-8372</contrib-id><x>is in the</x><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-45406"><name><surname>Casadevall</surname><given-names>Arturo</given-names></name><x>is in the</x><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="corresp" rid="cor3">*</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Departments of Laboratory Medicine and Microbiology</institution>, <institution>University of Washington School of Medicine</institution>, <addr-line><named-content content-type="city">Seattle</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Microbiology and Immunology</institution>, <institution>Albert Einstein College of Medicine</institution>, <addr-line><named-content content-type="city">New York</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Molecular Microbiology and Immunology</institution>, <institution>Johns Hopkins Bloomberg School of Public Health</institution>, <addr-line><named-content content-type="city">Baltimore</named-content></addr-line>, <country>United States</country></aff></contrib-group><author-notes><corresp id="cor1"><email>fcfang@u.washington.edu</email> (FCF);</corresp><corresp id="cor2"><email>anthony.bowen@med.einstein.yu.edu</email> (AB);</corresp><corresp id="cor3"><email>acasade1@jhu.edu</email> (AC)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>16</day><month>02</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>5</volume><elocation-id>e13323</elocation-id><history><date date-type="received"><day>25</day><month>11</month><year>2015</year></date><date date-type="accepted"><day>12</day><month>01</month><year>2016</year></date></history><permissions><copyright-statement>© 2016, Fang et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Fang et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-13323-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.13323.001</object-id><p>Peer review is widely used to assess grant applications so that the highest ranked applications can be funded. A number of studies have questioned the ability of peer review panels to predict the productivity of applications, but a recent analysis of grants funded by the National Institutes of Health (NIH) in the US found that the percentile scores awarded by peer review panels correlated with productivity as measured by citations of grant-supported publications. Here, based on a re-analysis of these data for the 102,740 funded grants with percentile scores of 20 or better, we report that these percentile scores are a poor discriminator of productivity. This underscores the limitations of peer review as a means of assessing grant applications in an era when typical success rates are often as low as about 10%.</p> <p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13323.001">http://dx.doi.org/10.7554/eLife.13323.001</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author Keywords</title><kwd>peer review</kwd><kwd>research funding</kwd><kwd>policy</kwd><kwd>national institute of health</kwd><kwd>grants</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research Organism</title><kwd>None</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.5</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Peer review scores were poorly predictive of research project success in this large dataset, suggesting that reviewers cannot reliably predict which meritorious applications are most likely to be productive.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Most funding agencies employ panels in which experts review proposals and assign scores to them based on a number of factors (such as expected impact and scientific quality). However, several studies have suggested significant problems with the current system of grant peer review. One problem is that the number of reviewers is typically inadequate to provide statistical precision (<xref ref-type="bibr" rid="bib13">Kaplan et al., 2008</xref>). Researchers have also found considerable variation among scores and disagreement regarding review criteria (<xref ref-type="bibr" rid="bib18">Mayo et al., 2006</xref>; <xref ref-type="bibr" rid="bib10">Graves et al., 2011</xref>; <xref ref-type="bibr" rid="bib1">Abdoul et al., 2012</xref>), and a Bayesian hierarchical statistical model of 18,959 applications to the NIH found evidence of reviewer bias that influenced as much as a quarter of funding decisions (<xref ref-type="bibr" rid="bib12">Johnson, 2008</xref>).</p><p>Although there is general agreement that peer review can discriminate sound grant applications from those containing serious flaws, it is uncertain whether peer review can accurately predict those meritorious applications that are most likely to be productive. An analysis of over 400 competing renewal grant applications at one NIH institute (the National Institute of General Medical Sciences) found no correlation between percentile score and publication productivity of funded grants (<xref ref-type="bibr" rid="bib3">Berg, 2013</xref>). A subsequent study of 1492 grants at another NIH institute (the National Heart, Lung and Blood Institute) similarly found no correlation between the percentile score and publication or citation productivity, even after correction for numerous variables (<xref ref-type="bibr" rid="bib5">Danthi et al., 2014</xref>). These observations suggest that once grant applications have been determined to be meritorious, expert reviewers cannot accurately predict their productivity.</p><p>In contrast, a recent analysis of over 130,000 grant applications funded by the NIH between 1980 and 2008 concluded that better percentile scores consistently correlate with greater productivity (<xref ref-type="bibr" rid="bib15">Li and Agha, 2015</xref>). Although the limitations of using retrospective publication/citation productivity to validate peer review are acknowledged (<xref ref-type="bibr" rid="bib17">Lindner et al., 2015</xref>; <xref ref-type="bibr" rid="bib14">Lauer and Nakamura, 2015</xref>), this large study has been interpreted as vindicating grant peer review (<xref ref-type="bibr" rid="bib19">Mervis, 2015</xref>; <xref ref-type="bibr" rid="bib23">Williams, 2015</xref>). However, the relevance of those findings for the current situation is questionable since the analysis included many funded grants with poor percentile scores (&gt;40th percentile) that would not be considered competitive today. Moreover, this study did not examine the important question of whether percentile scores can accurately stratify meritorious applications to identify those most likely to be productive.</p><p>We therefore performed a re-analysis of the same dataset to specifically address this question. Our analysis focused on subset of grants in the earlier study (<xref ref-type="bibr" rid="bib15">Li and Agha, 2015</xref>) that were awarded a percentile score of 20 or better: this subset contained 102,740 grants. This percentile range is most relevant because NIH paylines (that is, the lowest percentile score that is funded) seldom exceed the 20th percentile and have hovered around the 10th percentile for some institutes in recent years.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Publication and citation productivity in relation to percentile score</title> <p>A median of 6 publications was supported per grant. A plot of publication productivity versus percentile score (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) shows a wide range of publication productivity at any given percentile score with no significant difference in productivity between any adjacent cohorts. A plot of citation productivity versus percentile score (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) shows similar variability in productivity. Applications with percentile scores of 2 or better generated significantly more citations than applications receiving poorer scores; however, this was not true for applications receiving scores of 3 or higher. Although it was not possible to adjust citation numbers according to the year of award, a previous analysis of the dataset found that cohort effects relating to fiscal year did not influence the relationship between percentile score and productivity (<xref ref-type="bibr" rid="bib15">Li and Agha, 2015</xref>). While our re-analysis confirms that there is a correlation between percentile score and publication or citation productivity for applications with scores in the top 20 percentiles, the correlation is quite modest (slope = −0.132 ± 0.005 publications and −9.6 ± 0.337 citations for each percentile score increment, r<sup>2</sup> = 0.0078), suggesting that the overall ability of review groups to predict application success is weak at best. A random forest model indicated that only ~1% of the variance in productivity could be accounted for by percentile ranking (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), suggesting that all of the effort currently spent in peer review has a minimal impact in stratifying meritorious applications relative to what would be expected from a random ranking.<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.13323.002</object-id><label>Figure 1.</label><caption><title>Publication and citation productivity in relation to percentile score.</title><p>(<bold>A</bold>) The number of publications acknowledging support from grants within five years of grant approval (from PubMed) versus the percentile score: the bar shows the mean number of publications for all grants with that percentile score. (<bold>B</bold>) The number of citations that the papers in (<bold>A</bold>) received until the end of 2013 (data from Web of Science) versus the percentile score: the bar shows the mean number of citations for all grants with that percentile score. The lowest percentile scores are the most favorable. n = 102,740. Error bars = SDM. *Pink bars indicate significantly different from all cohorts of grants receiving poorer scores by one-way ANOVA. Black and gray bars do not differ significantly from their neighbors and are shown in different shades to allow easier visualization.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13323.002">http://dx.doi.org/10.7554/eLife.13323.002</ext-link></p></caption><graphic xlink:href="elife-13323-fig1-v1"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.13323.003</object-id><label>Figure 1—figure supplement 1.</label><caption><title>Random forest model of grant percentile score as a predictor of citation productivity.</title><p>A non-parametric model was constructed with 500 trees to measure grant percentile score as a predictor of citation productivity. The results indicate that 0.98% of variance in productivity can be accounted for by percentile score. The mean of squared residuals converges to 366,620.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13323.003">http://dx.doi.org/10.7554/eLife.13323.003</ext-link></p></caption><graphic xlink:href="elife-13323-fig1-figsupp1-v1"/></fig></fig-group></p></sec><sec id="s2-2"><title>Percentile scores of grants stratified on the basis of citation and publication productivity</title><p>This preceding analysis shows that percentile scores cannot accurately predict the subsequent productivity of a research grant in the critical 3–20 percentile range. This can be demonstrated by dividing the sample into equal halves based on publication productivity (the top 50.8% had ≥6 publications) and citation productivity (the top 49.99% had ≥128 citations). Substantial overlap is evident between percentile scores for grants in the upper and lower halves based upon both publication productivity (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) and citation productivity (<xref ref-type="fig" rid="fig2">Figure 2B</xref>): for example, if only half of applications could be funded, a grant in the top half of applications based on citations would only have a 58% chance of being funded if decisions were based on percentile scores.<fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.13323.004</object-id><label>Figure 2.</label><caption><title>Grants stratified on the basis of publication and citation productivity for different percentile scores.</title><p>Graphs showing, for percentile scores of 20 or better, the number of grants in the top half (left bar) and bottom half (right right) of grants on the basis of publications (<bold>A</bold>) and citations (<bold>B</bold>). Grants in the top half on the basis of publication productivity (<bold>A</bold>) had ≥ 6 publications: mean percentile score of top half 9.244 ± 5.583, median 9; mean percentile score of bottom half 9.947 ± 5.612, median 10. Grants in the top half on the basis of citation productivity (<bold>B</bold>) had ≥ 128 citations: mean percentile score of top half 9.242 ± 5.625, median 9; mean percentile score of bottom half 9.939 ± 5.571, median 10. Fewer grants received a percentile score of zero as a result of rounding to the nearest whole number, as well as a change in the NIH percentiling algorithm since 2009.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13323.004">http://dx.doi.org/10.7554/eLife.13323.004</ext-link></p></caption><graphic xlink:href="elife-13323-fig2-v1"/></fig></p></sec><sec id="s2-3"><title>Receiver operating characteristic (ROC) curve of grant peer review</title><p>An ROC curve (<xref ref-type="fig" rid="fig3">Figure 3</xref>) confirms the very poor discriminatory ability of percentile score for grants receiving scores in the 20th percentile or better, with an AUC (area under the curve) of only 0.54. Even an outstanding percentile score is no guarantor of project success, as 17% (334 of 1987) of grants with a percentile score of zero failed to produce any citations.<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.13323.005</object-id><label>Figure 3.</label><caption><title>Receiver operating characteristic curve of grant percentile score as a predictor of citation productivity (low/high).</title><p>Area under the curve (AUC) = 0.54 (95% confidence interval: 0.53–0.54) for citation productivity greater than the median. An AUC of 1.0 corresponds to a perfect test; an AUC of 0.5 indicates performance equivalent to random chance alone.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.13323.005">http://dx.doi.org/10.7554/eLife.13323.005</ext-link></p></caption><graphic xlink:href="elife-13323-fig3-v1"/></fig></p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>These observations suggest that despite the overall ability of reviewers to discriminate between extremely strong grant applications and the remainder, they have limited ability to accurately predict future productivity of meritorious applications in the range relevant to current paylines. This may contribute to a pervasive sense of arbitrariness with regard to funding decisions and dissatisfaction with the peer review system (<xref ref-type="bibr" rid="bib20">Pagano, 2006</xref>; <xref ref-type="bibr" rid="bib6">Fang and Casadevall, 2009</xref>; <xref ref-type="bibr" rid="bib4">Costello, 2010</xref>; <xref ref-type="bibr" rid="bib9">Germain, 2015</xref>). Perhaps most importantly, these findings contradict the notion that peer review can determine which applications are most likely to be productive. The excellent productivity exhibited by many projects with relatively poor scores and the poor productivity exhibited by some projects with outstanding scores demonstrate the inherent unpredictability of scientific research. The data also suggest that current paylines are inadequate to fund the most productive applications and that considerable potential productivity is being left on the table at current funding levels (<xref ref-type="bibr" rid="bib2">Berg, 2011</xref>).</p><p>It should be noted that percentile scores and publication/citation productivity are not necessarily independent. Citation practices have evolved over time, and citations per paper approximately doubled between 1980 and 2004 (<xref ref-type="bibr" rid="bib22">Wallace et al., 2009</xref>). This may have contributed to greater citation productivity for more recent grants, which would tend to have more favorable percentile scores than older ones, as paylines have declined over time, resulting in a correlation between percentile score and citation productivity that is unrelated to the discriminatory ability of the peer review process. In addition, applications that are funded despite scoring worse than the payline tend to be funded later in the fiscal year and are often subject to budgetary reductions that could adversely impact their future productivity. According to a report from the US Government Accountability Office (GAO), 13% of R01 applications funded in fiscal years 2003–2007 were exceptions with scores worse than the payline (<xref ref-type="bibr" rid="bib8">GAO, 2009</xref>). This may be an additional contributing factor to the correlation between percentile score and publication and citation productivity that is not indicative of peer review discrimination.</p><p>These observations have important implications for the grant peer review system. If reviewers are unable to reliably predict which meritorious applications are most likely to be productive, then reviewers might save time and resources by simply identifying the top 20% and awarding funding within this group on a random basis or according to programmatic priorities. In this regard, we refer to our recent suggestion that the NIH consider a modified lottery system (<xref ref-type="bibr" rid="bib7">Fang and Casadevall, 2014</xref>) and note that the New Zealand Health Research Council has already moved to a lottery system to select proposals for funding in its Explorer Grants program (<xref ref-type="bibr" rid="bib11">Health Research Council of New Zealand, 2015</xref>).</p><p>In summary, while our analysis confirms that peer review has some ability to discriminate between the quality of proposals based on citation productivity over the entire range of percentile scores (<xref ref-type="bibr" rid="bib15">Li and Agha, 2015</xref>), this does not extend to the critical range of percentile scores between 3 and 20, which is the most relevant subset of applications at current paylines. Furthermore, the best science is not necessarily receiving support under the present system since most applications in this percentile range, whether above or below current paylines, are indistinguishable with regard to citation productivity. The manner in which scarce research funds are allocated deserves greater attention.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>A database derived from 102,740 research project (R01) grants funded by the NIH from 1980 through 2008 was obtained from Danielle Li (Harvard University) and Richard Nakamura (NIH Center for Scientific Review). The data included the number of publications acknowledging grant support within 5 years of grant approval (from PubMed), the number of citations of those publications through 2013 (Web of Science) and the percentile score of the application (rounded to the nearest whole number). Parametric statistical analyses were performed in Prism (GraphPad Software, San Diego, CA). The receiver operating characteristic (ROC) and random forest analyses were performed using R (Vienna, Austria) and the pROC (<xref ref-type="bibr" rid="bib21">Robin et al., 2011</xref>) and randomForest (<xref ref-type="bibr" rid="bib16">Liaw and Wiener, 2002</xref>) packages.</p></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors thank Michael Lauer and Jeremy Berg for informative discussions and thank Danielle Li and Richard Nakamura for providing the dataset analyzed here. This paper makes use of restricted access data available from the NIH. Those wishing to replicate its results may apply for access following the procedures outlined in the NIH Data Access Policy document available at <ext-link ext-link-type="uri" xlink:href="http://report.nih.gov/pdf/DataAccessPolicy.pdf">http://report.nih.gov/pdf/DataAccessPolicy.pdf</ext-link>.</p> </ack><fn-group content-type="competing-interest"><title>Competing interests</title> <fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>FCF, Conception and design, Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn> <fn fn-type="con" id="con2"><p>AB, Analysis and interpretation of data, Drafting or revising the article</p></fn> <fn fn-type="con" id="con3"><p>AC, Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn> </fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdoul</surname><given-names>H</given-names></name><name><surname>Perrey</surname><given-names>C</given-names></name><name><surname>Amiel</surname><given-names>P</given-names></name><name><surname>Tubach</surname><given-names>F</given-names></name><name><surname>Gottot</surname><given-names>S</given-names></name><name><surname>Durand-Zaleski</surname><given-names>I</given-names></name><name><surname>Alberti</surname><given-names>C</given-names></name><name><surname>Gagnier</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Peer review of grant applications: criteria used and qualitative study of reviewer practices</article-title><source>PLoS ONE</source><volume>7</volume><elocation-id>e46054</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0046054</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Productivity metrics and peer review scores</article-title><source>NIGMS Feedback Loop Blog</source><uri xlink:href="https://loop.nigms.nih.gov/2011/06/productivity-metrics-and-peer-review-scores/">https://loop.nigms.nih.gov/2011/06/productivity-metrics-and-peer-review-scores/</uri></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>On deck chairs and lifeboats</article-title><source>ASBMB Today</source><uri xlink:href="http://www.asbmb.org/asbmbtoday/asbmbtoday_article.aspx?id=32362">http://www.asbmb.org/asbmbtoday/asbmbtoday_article.aspx?id=32362</uri></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costello</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Is NIH funding the “best science by the best scientists”? A critique of the NIH R01 research grant review policies</article-title><source>Academic Medicine</source><volume>85</volume><fpage>775</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1097/ACM.0b013e3181d74256</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danthi</surname><given-names>N</given-names></name><name><surname>Wu</surname><given-names>CO</given-names></name><name><surname>Shi</surname><given-names>P</given-names></name><name><surname>Lauer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Percentile ranking and citation impact of a large cohort of national heart, lung, and blood institute-funded cardiovascular R01 grants</article-title><source>Circulation Research</source><volume>114</volume><fpage>600</fpage><lpage>606</lpage><pub-id pub-id-type="doi">10.1161/CIRCRESAHA.114.302656</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>FC</given-names></name><name><surname>Casadevall</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>NIH peer review reform--change we need, or lipstick on a pig?</article-title><source>Infection and Immunity</source><volume>77</volume><fpage>929</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1128/IAI.01567-08</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>FC</given-names></name><name><surname>Casadevall</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Taking the powerball approach to medical research</article-title><source>Wall Street Journal</source><uri xlink:href="http://www.wsj.com/articles/SB10001424052702303532704579477530153771424">http://www.wsj.com/articles/SB10001424052702303532704579477530153771424</uri></element-citation></ref><ref id="bib8"><element-citation publication-type="other"><person-group person-group-type="author"><collab>GAO</collab></person-group><year iso-8601-date="2009">2009</year><article-title>National institutes of health: completion of comprehensive risk management program essential to effective oversight</article-title></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Germain</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Healing the NIH-funded biomedical research enterprise</article-title><source>Cell</source><volume>161</volume><fpage>1485</fpage><lpage>1491</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.05.052</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>N</given-names></name><name><surname>Barnett</surname><given-names>AG</given-names></name><name><surname>Clarke</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Funding grant proposals for scientific research: retrospective analysis of scores by members of grant review panel</article-title><source>BMJ</source><volume>343</volume><fpage>d4797</fpage><pub-id pub-id-type="doi">10.1136/bmj.d4797</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Health Research Council of New Zealand</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Explorer Grants</article-title><uri xlink:href="http://www.hrc.govt.nz/funding-opportunities/researcher-initiated-proposals/explorer-grants">http://www.hrc.govt.nz/funding-opportunities/researcher-initiated-proposals/explorer-grants</uri></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>VE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Statistical analysis of the National Institutes of Health peer review system</article-title><source>Proceedings of the National Academy of Sciences of the USA</source><volume>105</volume><fpage>11076</fpage><lpage>11080</lpage><pub-id pub-id-type="doi">10.1073/pnas.0804538105</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaplan</surname><given-names>D</given-names></name><name><surname>Lacetera</surname><given-names>N</given-names></name><name><surname>Kaplan</surname><given-names>C</given-names></name><name><surname>Tregenza</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Sample size and precision in NIH peer review</article-title><source>PLoS ONE</source><volume>3</volume><elocation-id>e2761</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0002761</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>MS</given-names></name><name><surname>Nakamura</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reviewing peer review at the NIH</article-title><source>New England Journal of Medicine</source><volume>373</volume><fpage>1893</fpage><lpage>1895</lpage><pub-id pub-id-type="doi">10.1056/NEJMp1507427</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Agha</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Big names or big ideas: do peer-review panels select the best science proposals?</article-title><source>Science</source><volume>348</volume><fpage>434</fpage><lpage>438</lpage><pub-id pub-id-type="doi">10.1126/science.aaa0185</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liaw</surname><given-names>A</given-names></name><name><surname>Wiener</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Classification and regression by randomForest</article-title><source>R News</source><volume>2</volume><fpage>18</fpage><lpage>22</lpage></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindner</surname><given-names>MD</given-names></name><name><surname>Nakamura</surname><given-names>RK</given-names></name><name><surname>Smalheiser</surname><given-names>NR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Examining the predictive validity of NIH peer review scores</article-title><source>PLoS ONE</source><volume>10</volume><elocation-id>e0126938</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0126938</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mayo</surname><given-names>NE</given-names></name><name><surname>Brophy</surname><given-names>J</given-names></name><name><surname>Goldberg</surname><given-names>MS</given-names></name><name><surname>Klein</surname><given-names>MB</given-names></name><name><surname>Miller</surname><given-names>S</given-names></name><name><surname>Platt</surname><given-names>RW</given-names></name><name><surname>Ritchie</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Peering at peer review revealed high degree of chance associated with funding of grant applications</article-title><source>Journal of Clinical Epidemiology</source><volume>59</volume><fpage>842</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1016/j.jclinepi.2005.12.007</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mervis</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>NIH's peer review stands up to scrutiny</article-title><source>Science</source><volume>348</volume><fpage>384</fpage><pub-id pub-id-type="doi">10.1126/science.348.6233.384</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagano</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>American idol and NIH grant review</article-title><source>Cell</source><volume>126</volume><fpage>637</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2006.08.004</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robin</surname><given-names>X</given-names></name><name><surname>Turck</surname><given-names>N</given-names></name><name><surname>Hainard</surname><given-names>A</given-names></name><name><surname>Tiberti</surname><given-names>N</given-names></name><name><surname>Lisacek</surname><given-names>F</given-names></name><name><surname>Sanchez</surname><given-names>J-C</given-names></name><name><surname>Müller</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>PROC: an open-source package for r and s+ to analyze and compare ROC curves</article-title><source>BMC Bioinformatics</source><volume>12</volume><fpage>77</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-12-77</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>ML</given-names></name><name><surname>Larivière</surname><given-names>V</given-names></name><name><surname>Gingras</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Modeling a century of citation distributions</article-title><source>Journal of Informetrics</source><volume>3</volume><fpage>296</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1016/j.joi.2009.03.010</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Study: peer review predicts success</article-title><source>The Scientist</source><uri xlink:href="http://www.the-scientist.com/?articles.view/articleNo/42787/title/Study-Peer-Review-Predicts-Success/">http://www.the-scientist.com/?articles.view/articleNo/42787/title/Study-Peer-Review-Predicts-Success/</uri></element-citation></ref></ref-list></back></article>