<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">77907</article-id><article-id pub-id-type="doi">10.7554/eLife.77907</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Contribution of behavioural variability to representational drift</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-160996"><name><surname>Sadeh</surname><given-names>Sadra</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8159-5461</contrib-id><email>s.sadeh@imperial.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-17772"><name><surname>Clopath</surname><given-names>Claudia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4507-8648</contrib-id><email>c.clopath@imperial.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Department of Bioengineering, Imperial College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>30</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e77907</elocation-id><history><date date-type="received" iso-8601-date="2022-02-15"><day>15</day><month>02</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-08-24"><day>24</day><month>08</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-01-02"><day>02</day><month>01</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.01.02.474731"/></event></pub-history><permissions><copyright-statement>© 2022, Sadeh and Clopath</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Sadeh and Clopath</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-77907-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-77907-figures-v2.pdf"/><abstract><p>Neuronal responses to similar stimuli change dynamically over time, raising the question of how internal representations can provide a stable substrate for neural coding. Recent work has suggested a large degree of drift in neural representations even in sensory cortices, which are believed to store stable representations of the external world. While the drift of these representations is mostly characterized in relation to external stimuli, the behavioural state of the animal (for instance, the level of arousal) is also known to strongly modulate the neural activity. We therefore asked how the variability of such modulatory mechanisms can contribute to representational changes. We analysed large-scale recording of neural activity from the Allen Brain Observatory, which was used before to document representational drift in the mouse visual cortex. We found that, within these datasets, behavioural variability significantly contributes to representational changes. This effect was broadcasted across various cortical areas in the mouse, including the primary visual cortex, higher order visual areas, and even regions not primarily linked to vision like hippocampus. Our computational modelling suggests that these results are consistent with independent modulation of neural activity by behaviour over slower timescales. Importantly, our analysis suggests that reliable but variable modulation of neural representations by behaviour can be misinterpreted as representational drift if neuronal representations are only characterized in the stimulus space and marginalized over behavioural parameters.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>representational drift</kwd><kwd>behavioural variability</kwd><kwd>behavioural state</kwd><kwd>arousal</kwd><kwd>running</kwd><kwd>visual processing</kwd><kwd>pupillometry</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>225412/Z/22/Z</award-id><principal-award-recipient><name><surname>Sadeh</surname><given-names>Sadra</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>200790/Z/16/Z</award-id><principal-award-recipient><name><surname>Clopath</surname><given-names>Claudia</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/N013956/1</award-id><principal-award-recipient><name><surname>Clopath</surname><given-names>Claudia</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>BB/N019008/1</award-id><principal-award-recipient><name><surname>Clopath</surname><given-names>Claudia</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>564408</award-id><principal-award-recipient><name><surname>Clopath</surname><given-names>Claudia</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/R035806/1</award-id><principal-award-recipient><name><surname>Clopath</surname><given-names>Claudia</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>By analysing large-scale datasets from the Allen Brain Observatory, we found that changes in the behavioural state of the animal can significantly contribute to the changes in representational similarity over time.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neuronal responses to stimuli, contexts, or tasks change over time, creating a drift of representations from their original patterns (<xref ref-type="bibr" rid="bib11">Deitch et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Driscoll et al., 2017</xref>; <xref ref-type="bibr" rid="bib25">Lütcke et al., 2013</xref>; <xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>; <xref ref-type="bibr" rid="bib38">Schoonover et al., 2021</xref>; <xref ref-type="bibr" rid="bib46">Ziv et al., 2013</xref>). This representational drift can reflect the presence of intrinsic noise or plasticity in the circuitry and, depending on its origin, can be detrimental to or beneficial for the neural code (<xref ref-type="bibr" rid="bib7">Clopath et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Rule et al., 2019</xref>). Understanding the mechanisms contributing to the emergence of representational drift can therefore shed light on its relevance for neural computation (<xref ref-type="bibr" rid="bib25">Lütcke et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Rule et al., 2019</xref>).</p><p>Representational drift can arise from a variety of sources, including bottom-up mechanisms, like changes in the feedforward input to neurons or from a dynamic reorganization of recurrent interactions in the network. Another important source of variability that can contribute to representational drift is changes in the behavioural state of the animal. Spontaneous behaviour has in fact been shown to heavily modulate responses in awake behaving animals (<xref ref-type="bibr" rid="bib30">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="bib40">Stringer et al., 2019</xref>). Drift of behavioural state – for example, gradual changes in attention, arousal, or running – can therefore change the way neural activity is modulated by top-down mechanisms (<xref ref-type="bibr" rid="bib32">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="bib42">Vinck et al., 2015</xref>) over different timescales.</p><p>The exact manner in which such top-down mechanisms modulate the neural activity (<xref ref-type="bibr" rid="bib8">Cohen-Kashi Malina et al., 2021</xref>; <xref ref-type="bibr" rid="bib12">Dipoppa et al., 2018</xref>; <xref ref-type="bibr" rid="bib18">Fu et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Garcia Del Molino et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Pakan et al., 2016</xref>) would in turn determine how behavioural variability affects the representational drift. One possibility is that stimulus-evoked responses are just scaled by arousal or running, as suggested by gain models (<xref ref-type="bibr" rid="bib15">Ferguson and Cardin, 2020</xref>). Under this scenario, the behavioural state of the animal can modulate the similarity of sensory representations across multiple repeats of the same stimulus (representational similarity) by increasing or decreasing the signal-to-noise ratio. Another possibility is that the behaviour contributes independently to neuronal activity, and hence representational similarity is better described in a parameter space where internal and external parameters conjointly define the neural code. Under the latter scenario, variability in behavioural ‘signal’ could be perceived as noise from the viewpoint of sensory representations, and could therefore be potentially mistaken as representational drift.</p><p>To delineate the contribution of behavioural variability to representational drift and shed light on the involved mechanisms, we analysed publicly available datasets from the Allen Brain Observatory (<xref ref-type="bibr" rid="bib10">de Vries et al., 2020</xref>; <xref ref-type="bibr" rid="bib39">Siegle et al., 2021</xref>). These datasets provide a good opportunity to systematically address this question as standardized visual stimulation and large-scale recordings of population activity in response to multiple repeats of the same stimuli are combined with systematic measurements of behavioural parameters (like running speed and pupil size) across a large number of animals. Our analysis suggested that changes in the behavioural state of the animal can strongly modulate the similarity of neuronal representations in response to multiple repeats of the same stimulus. In fact, within the datasets analysed, a significant fraction of what may be described as representational drift in a sensory cortex can be attributed to behavioural variability. Our results suggest that the contribution of behavioural variability to changes in neuronal activity should be carefully quantified and considered in the analysis of representational similarity and representational drift.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Representational similarity depends on the behavioural state of the animal</title><p>We analysed publicly available, large-scale, standardized in vivo physiology datasets recently published by the Allen Brain Observatory (<xref ref-type="bibr" rid="bib39">Siegle et al., 2021</xref>). The electrophysiology datasets obtained via Neuropixels probes (<xref ref-type="bibr" rid="bib23">Jun et al., 2017</xref>) provide the possibility of studying the spiking activity of a large number of units to visual stimuli (see ‘Methods’). We studied similarity of neural activity in response to multiple repeats of the same natural movie (<xref ref-type="fig" rid="fig1">Figure 1a</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Representational similarity depends on the behavioural state of the animal.</title><p>(<bold>a</bold>) Illustration of the response of a population of neurons to a stimulus (e.g. a natural movie) which is shown twice (left versus right). (<bold>b</bold>) Neuronal response to different repetitions of the same stimulus can remain the same (left) or change from the original pattern (right), leading to a change in neural representations. This change can arise from an added stimulus-independent noise, which is randomly and independently changing over each repetition. Alternatively, it can be a result of another signal, which is modulated according to a stimulus-independent parameter (e.g. behaviour) that is changing over each repetition. (<bold>c</bold>) Population activity composed of activity of 483 units in an example recording session in response to different stimuli (timing of different stimulus types denoted on the top). Spiking activity of units is averaged in bins of 1 s and z-scored across the entire session for each unit. Units in primary visual cortex (V1; 58 units) and the two blocks of presentation of natural movie 1 (NM1) are highlighted by the black lines. Bottom: pupil size and running speed of the animal (z-scored). (<bold>d</bold>) Representational similarity between different presentations of NM1. It is calculated as the correlation coefficient of vectors of population response of V1 units to movie repeats (see ‘Methods’). Left: the matrix of representational similarity for all pairs of movie repeats within and across the two blocks of presentation. Right: representational similarity as a function of the pupil change, which is quantified as the normalized absolute difference of the average pupil size during presentations (see ‘Methods’). The best-fitted regression line (using least-squares method) and the R<sup>2</sup> value are shown. Filled circles show the average values within and between blocks. (<bold>e</bold>) Same as (<bold>d</bold>) right for all recording sessions. Left: data similar to (<bold>d</bold>) grey dots are concatenated across all mice and the best-fitted regression line to the whole data is plotted. Black line shows the fit when movie repeats with significant change in the average running speed of the animal is considered (80th percentile). Right: the average values within and between blocks filled circles in (<bold>d</bold>) are plotted for all mice and the fitted regression line to these average values is plotted. Grey lines and R<sup>2</sup> values indicate the fit to within-block data only. N: number of mice. (<bold>f–h</bold>) Same as (<bold>c–e</bold>) for a different dataset. Source data (for normalized changes in pupil width and representational similarity between pairs of movie repeats) are provided for individual sessions across the two datasets (<xref ref-type="supplementary-material" rid="fig1sdata1">Figure 1—source data 1</xref>).</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig1">Figure 1</xref>.</title><p>Source data (for normalized changes in pupil width and representational similarity between pairs of movie repeats) for individual sessions across the two Neuropixels datasets.</p></caption><media mimetype="application" mime-subtype="xls" xlink:href="elife-77907-fig1-data1-v2.xls"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Characterization and quantification of representational similarity (RS) and representational drift.</title><p>(<bold>a</bold>) Left: illustration of response of a population of neurons (#1 to #n; upper) to a stimulus (lower), composed of binary values (ON: red; OFF: white). (<bold>b</bold>) The population response to two other repetitions of the same stimulus can remain the same (upper), demonstrating a stable and reliable code, or it can change from the original pattern (lower), leading to a drift of representations. Representational drift can be quantified by a representational drift index (RDI), which compares the correlation of population responses within the same session/block of presentation (<inline-formula><mml:math id="inf1"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) with the correlation of population responses across sessions/blocks (<inline-formula><mml:math id="inf2"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>; see ‘Methods’). (<bold>c</bold>) The degree of change or constancy of representations can be assayed by comparing the population responses to repeats of the same stimulus. The degree of similarity is quantified by RS, which is quantified by the correlation coefficient (CC) of the concatenated (across neurons) vector of population responses to two repeats (PV(i) and PV(j)). (<bold>d</bold>) Stimulus reliability (SR) is calculated for each unit individually, from the CC of the vector of responses of that unit to two stimulus presentations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig1-figsupp1-v2.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Relation between behavioural changes and representational similarity in other sessions and for all units.</title><p>(<bold>a</bold>) Same as <xref ref-type="fig" rid="fig1">Figure 1d and g</xref> for four other example recording sessions. Left: examples from Neuropixels dataset 1; session numbers and the number of V1 units (#), respectively: 762602078 (#75), 750332458 (#63), 760345702 (#72), 751348571 (#49). Left: examples from Neuropixels dataset 2; session numbers and the number of V1 units (#), respectively: 766640955(#52), 787025148(#68), 771990200(#54), 829720705(#52). Only sessions with #&gt;40 units are included in the analysis. (<bold>b</bold>) Same as <xref ref-type="fig" rid="fig1">Figure 1e and h</xref> when all recorded units are included (instead of only V1 units).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig1-figsupp2-v2.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Dependence of representational similarity on behavioural change when calculated from z-scored activity and from activity rendered in longer time bins.</title><p>(<bold>a</bold>) Same as <xref ref-type="fig" rid="fig1">Figure 1e and h</xref> when population vectors are composed of z-scored activity of units in V1 (upper) or all regions (lower). Z-scored activity of unit <inline-formula><mml:math id="inf3"><mml:mi>i</mml:mi></mml:math></inline-formula> is calculated as <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , where <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the average and SD of the activity of unit (<inline-formula><mml:math id="inf7"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) during the two blocks of presentation of natural movie 1. Left: Neuropixels dataset 1; right: Neuropixels dataset 2. (<bold>b</bold>) Same as <xref ref-type="fig" rid="fig1">Figure 1e and h</xref> when representational similarity is calculated from the activity of units rendered in time bins of 2 s (instead of original 1 s; see ‘Methods’).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig1-figsupp3-v2.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Dependence of representational similarity on behavioural change in wild-type (WT) mice and for male/female animals separately.</title><p>Same as <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> when (<bold>a</bold>) only WT mice are included in the analysis or (<bold>b</bold>) when the analysis is performed for V1 units in female and male mice separately (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for details). Left: Neuropixels dataset 1; right: Neuropixels dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig1-figsupp4-v2.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Representational drift between the two blocks of stimulus presentation in animals with different levels of behavioural variability.</title><p>(<bold>a</bold>) Matrices of representational similarity (calculated from population vectors of V1 units) between repeats of natural movie 1 (similar to <xref ref-type="fig" rid="fig1">Figure 1d and g</xref>, left) for sessions across the two datasets. Upper: Neuropixels dataset 1; lower: Neuropixels dataset 2; the colour code is the same as the example shown in (<bold>c</bold>). The numbers on the bottom denote the average change in the pupil width between the two blocks (average of the absolute normalized change between all pairs of repeats in the first and the second block), and the sessions are sorted according to that (from the least change to the highest change). The numbers on the top denote the representational drift index (RDI; see ‘Methods’) between the two blocks, which is calculated as <inline-formula><mml:math id="inf8"><mml:mi>R</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>–</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> , where <inline-formula><mml:math id="inf9"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represent the average representational similarity within sessions and between sessions of the two blocks, respectively. <inline-formula><mml:math id="inf11"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is obtained as the average of average CC within the first block and the second block. (<bold>b</bold>) Relationship between the average change in pupil size (numbers denoted on the bottom of plots in <bold>a</bold>) and representational drift for different sessions in the two datasets. Red lines show the best-fitted regression lines, with values of R<sup>2</sup> denoted in red. (<bold>c</bold>) The example session, highlighted with the red box in (<bold>a</bold>) is singled out (top), as being an outlier in terms of the relation between pupil change and RD between the two blocks red circle in (<bold>b</bold>). The average pupil width and running speed for each movie repeat is shown for this session on the bottom. (<bold>d, e</bold>) Same as (<bold>b, a</bold>) respectively, for all units (instead of V1 units).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig1-figsupp5-v2.tif"/></fig></fig-group><p>Previous studies have reported significant changes in stimulus-evoked representations even in sensory cortices, over the timescales of multiple hours to multiple days and weeks (<xref ref-type="bibr" rid="bib11">Deitch et al., 2021</xref>; <xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>; <xref ref-type="bibr" rid="bib38">Schoonover et al., 2021</xref>). The Neuropixels electrophysiology datasets provide the opportunity of studying these representational changes while accounting for behavioural changes, although over a faster timescale (hours). Similar representational drift has in fact been reported for another dataset (obtained via two-photon calcium imaging) over the course of multiple days (<xref ref-type="bibr" rid="bib11">Deitch et al., 2021</xref>). The insights obtained from this analysis may therefore help in understanding the mechanisms underlying representational drift over longer timescales.</p><p>To shed light on the involved mechanisms, we contrasted two potential scenarios (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Changes in population activity in response to the same stimulus can arise from a completely random and independently added noise. Alternatively, modulation of activity by other (stimulus-independent) factors like behavioural modulation can also contribute to these changes (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). To delineate between these two scenarios, we characterized how neuronal representations change across repetitions of the same stimulus (<xref ref-type="fig" rid="fig1">Figure 1a and b</xref>). This was quantified by a measure of representational similarity (RS), which was characterized as the similarity of responses, at the population level, to multiple repeats of the stimulus (see ‘Methods’ and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Representational drift was calculated as the difference of RS within and between multiple repeats of the natural movies (see ‘Methods’).</p><p>Metrics of representational drift (e.g. representational drift index [RDI]; <xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>; see ‘Methods’) are often calculated from RS. We therefore based our analysis primarily on comparing RS and its relationship with sensory and behavioural parameters as this contained more information. RDI was calculated as a single metric to quantify representational drift between blocks of presentation in each session. The drop in representational similarity between individual pairs of movie repeats was used to analyse representational drift on a finer scale (this drop is mathematically related to representational drift; see ‘Methods’). The main behavioural parameters we analysed were the size of pupil and the running speed of animals. We refer to more slowly changing dynamics of these parameters (compared to sensory-evoked responses) as the behavioural state of the animal, and changes in these states are described as behavioural variability.</p><p>Our analysis was performed in two datasets with different structure of stimulus presentations (<xref ref-type="fig" rid="fig1">Figure 1c and f</xref>; see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). In each dataset, the natural movie (30 s long) is presented multiple times in each block of presentation (10 and 30 repeats for dataset 1 and dataset 2, respectively). We analysed the data for two blocks of presentation separated by more than an hour (<xref ref-type="fig" rid="fig1">Figure 1c and f</xref>). For each presentation of the natural movie, we calculated a population vector of responses from the average activity of all the units recorded in the primary visual cortex (V1), in bin widths of 1 s starting from the onset of movie presentation (‘Methods’). Representational similarity between two repeats of the natural movie was quantified by the correlation coefficient of the population vectors (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref> and ‘Methods’).</p><p>Previous analysis has shown that representational similarity (as defined above) is higher within a block of presentation and decreases significantly between different blocks, both in Neuropixels and calcium imaging datasets (<xref ref-type="bibr" rid="bib11">Deitch et al., 2021</xref>). Our results confirmed this observation, but we also found that representational similarity is strongly modulated by the behavioural state of the animal. This was most visible in sessions where the behavioural state (as assayed by pupil diameter and the average running speed) changed clearly between the two repeats of the movie (<xref ref-type="fig" rid="fig1">Figure 1c and f</xref>). We observed that, firstly, change in the behavioural state strongly reduced the representational similarity between the two blocks (<xref ref-type="fig" rid="fig1">Figure 1d and g</xref>), reminiscent of the representational drift which has been reported over the scale of hours to days (<xref ref-type="bibr" rid="bib11">Deitch et al., 2021</xref>; <xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>; <xref ref-type="bibr" rid="bib38">Schoonover et al., 2021</xref>). Secondly, increased pupil diameter and running during the second block of presentation in fact increased the similarity of responses to the same movie within that block (<xref ref-type="fig" rid="fig1">Figure 1d and g</xref>, left). Overall, there was a significant drop of representational similarity between the movie repeats in which the animal experienced the most changes in the average pupil size (<xref ref-type="fig" rid="fig1">Figure 1d and g</xref>, right). These results indicate that the behavioural state of the animal can bidirectionally modulate the representational similarity across repeats of the same stimulus.</p><p>We found similar dependence of representational similarity on the pupil change for other sessions (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a</xref>) and across all animals (<xref ref-type="fig" rid="fig1">Figure 1e and h</xref>). The effect was more prominent when focusing on movie repeats with significant changes in the average running (<xref ref-type="fig" rid="fig1">Figure 1e and h</xref>, left, black lines). Similar trend was also observed when considering units from all recorded regions, instead of only focusing on V1 units (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2b</xref>). We also observed the same trend when repeating the analysis within blocks (<xref ref-type="fig" rid="fig1">Figure 1d–h</xref>, right, grey lines, and <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2b</xref>), although the drop of representational similarity across blocks was more prominent due to more drastic behavioural changes between the blocks, which is expected from the slow timescale of changes in behavioural states.</p><p>In the above analysis, we considered the actual spiking activity of the units to build the population vectors. Calculating the representational similarity from these vectors can potentially bias the estimate by increasing the impact of highly active neurons. For instance, if the units which are firing higher remain consistently active, they may lead to some similarity of population vectors even independent of stimulus-evoked responses. To control for variance in the average activity of units, we repeated our analysis for population vectors composed of z-scored responses (as shown in <xref ref-type="fig" rid="fig1">Figure 1c and f</xref>; see ‘Methods’). Overall, representational similarity diminished when calculated from the z-scored activity (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3a</xref>). However, we observed the same trend in terms of dependence on the behavioural state, whereby larger changes in pupil size were correlated with larger changes in representational similarity (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3a</xref>).</p><p>We performed our original analysis with vectors of activity rendered in time bins of 1 s, corresponding to the interval of presentation of each frame of the natural movie (‘Methods’). We tested whether our results hold for responses rendered in different time bins by testing longer time bins (2 s; <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3b</xref>). Overall, representational similarity was higher as expected from averaging the activity. However, we observed similar drop of representational similarity with increases in pupil size (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3b</xref>).</p><p>Our previous analyses were performed on wild-type mice as well as mice from three different transgenic lines (Pvalb-IRES-Cre × Ai32, n = 8; Sst-IRES-Cre × Ai32, n = 12; and Vip-IRES-Cre × Ai32, n = 8; see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>; <xref ref-type="bibr" rid="bib39">Siegle et al., 2021</xref>). To control for possible differences between different strains, we repeated our analysis for recording sessions in wild-type mice only and observed similar results (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4a</xref>). Our results also held when analysing female and male animals separately (female mice comprised a smaller fraction of the datasets; ~20%) (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4b</xref>).</p><p>In our results above, we observed large changes in representational similarity between blocks of stimulus presentation with strong behavioural changes. But this effect can be confounded by the passage of time between the blocks, which may lead to other sources of variability such as changes in the excitability of neurons for instance. To account for this, we analysed the average block-wise representational similarity in individual animals (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>). For each animal, we quantified the RDI between the two blocks as the normalized change of average correlations within and across blocks (<xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>; see ‘Methods’).</p><p>If the passage of time contributes strongly to the drop in representational similarity between the two blocks, we should see comparable levels of representational drift across animals with different levels of behavioural variability. Conversely, if the change in behaviour is the main factor contributing to the drop in representational similarity between the two blocks, we should see stronger levels of representational drifts for animals with larger behavioural variability. We indeed found evidence in favour of the latter: representational similarity remained rather stable for those animals which did not show large behavioural changes between the two blocks (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5a</xref>). That is, passage of time per se did not contribute strongly to representational drift. Largest representational drifts were observed for animals with the largest changes in the average pupil width between the two blocks (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5a</xref>). In fact, there was a good correlation between the two across animals (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5b</xref>).</p><p>The relationship was weaker in Neuropixels dataset 2, whereby more repeats of the natural movie are shown in each block (60 repeats for the total of 30 min versus 20 repeats for 10 min in Neuropixels dataset 1). Longer blocks of stimulus presentation increase the chance of behavioural changes within the blocks, which can in turn make the average block-wise representational similarity a less reliable metric. In line with this reasoning, further scrutiny into an outlier (with small average pupil changes but rather large representational drift between the two blocks; <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5b</xref>, right) revealed that changes in the running speed of the animal within each block can be the factor contributing to changes in representational similarity (<xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5c</xref>).</p><p>Taken together, these results suggest that, in awake behaving animals, variability of the behavioural state of the animal can be an important factor contributing to the modulation of representational similarity.</p></sec><sec id="s2-2"><title>Evidence for independent modulation of responses by stimulus and behaviour</title><p>What is the mechanism by which behavioural state modulates representational similarity? Changes in pupil area are correlated with the level of arousal (<xref ref-type="bibr" rid="bib5">Bradley et al., 2008</xref>), which can modulate the neuronal gain (<xref ref-type="bibr" rid="bib8">Cohen-Kashi Malina et al., 2021</xref>). We therefore studied a possible gain model in which changes in pupil size modulate the neuronal responses to sensory inputs (see <xref ref-type="fig" rid="fig2">Figure 2a</xref> and ‘Methods’). Alternatively, rather than scaling the stimulus-induced signal, behaviour can contribute independently to neuronal activity (<xref ref-type="bibr" rid="bib12">Dipoppa et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Saleem et al., 2013</xref>). We therefore compared the gain model to a model in which the neural tuning was obtained by an independent mixing of stimulus and behavioural signals (see <xref ref-type="fig" rid="fig2">Figure 2b</xref> and ‘Methods’).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Independent modulation of activity by stimulus and behaviour.</title><p>(<bold>a</bold>) Schematic of a signal-gain model in which behaviour controls the gain with which the stimulus-driven signal is scaled. Individual units are driven differently with the stimulus, leading to different tuning curves which determines their stimulus signal, <inline-formula><mml:math id="inf12"><mml:mi>S</mml:mi></mml:math></inline-formula>. Behavioural parameter, B, sets the gain, <inline-formula><mml:math id="inf13"><mml:mi>g</mml:mi></mml:math></inline-formula>, with which the stimulus signal is scaled, before being combined with the noise term, N, to give rise to the final response. S is the same across repetitions of the stimulus, while N is changing on every repeat (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref> and ‘Methods’). (<bold>b</bold>) An alternative model (independent-mixing) in which the response of a unit is determined by the summation of its independent tuning to stimulus, S (red) and behaviour, B (black: high B; grey: low B), combined with noise, N (see ‘Methods’ for details). (<bold>c</bold>) Representational similarity of population responses to different repeats of the stimulus as a function of the relative behavioural parameter (<inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) in the signal-gain model. Black line shows the average (in 20 bins). (<bold>d</bold>) Same as (<bold>c</bold>), for the independent-mixing model. (<bold>e</bold>) Same as (<bold>c, d</bold>) for the experimental data from Neuropixels dataset 1 (red) or dataset 2 (blue). For each pair of movie repeats, the average representational similarity of population responses (left: V1 units; right: all units) are plotted against the relative pupil size (<inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). (<bold>f, g</bold>) Relation between behavioural modulation and stimulus reliability of units in different models. The stimulus signal-gain model predicts a strong dependence between behavioural modulation and stimulus reliability of units (<bold>f</bold>), whereas the independent-mixing model predicts no correlation between the two (<bold>g</bold>). Best-fitted regression lines and R<sup>2</sup> values are shown for each case. Marginal distributions are shown on each axis. (<bold>h</bold>) Data from V1 units in the two datasets show no relationship between the stimulus reliability of units and their absolute behavioural modulation, as quantified by the best-fitted regression lines and R<sup>2</sup> values. Stimulus reliability is computed as the average correlation coefficient of each unit’s activity vector across repetitions of the natural movie, and behavioural modulation is calculated as the correlation coefficient of each unit’s activity with the pupil size (see ‘Methods’). Marginal distributions are shown. Source data (for stimulus reliability of V1 units and their modulation by pupil size) are provided for individual sessions across the two datasets (<xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>).</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>Source data (for stimulus reliability of V1 units and their modulation by pupil size) for individual sessions across the two Neuropixels datasets.</p></caption><media mimetype="application" mime-subtype="xls" xlink:href="elife-77907-fig2-data1-v2.xls"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Dependence of representational similarity on behaviour in different gain models and in different experimental datasets.</title><p>(<bold>a</bold>) A model neuron which integrates the signal and the noise components in its inputs. The signal has the same pattern over multiple repetition (rep#) of the stimulus, while the noise changes in each repeat. (<bold>b</bold>) Representational similarity as a function of relative gain of the repeats (<inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , where <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the gains in the <inline-formula><mml:math id="inf19"><mml:mi>i</mml:mi></mml:math></inline-formula>th and <inline-formula><mml:math id="inf20"><mml:mi>j</mml:mi></mml:math></inline-formula>th repeats) for three models, where both signal and noise (left), only noise (middle), or only signal (right) components of the input are scaled by behaviour (see ‘Methods’ for details). Change in the gain did not change representational similarity when both signal and noise were scaled (left), consistent with our theoretical analysis (see ‘Methods’). When arousal scaled noise only, there was a small decrease in the average representational similarity (middle). The most prominent effect was observed when arousal scaled signal only. For this scenario, a general increase in the average representational similarity was obtained, with the maximum increase happening at equal gains (<inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) (right). (<bold>c</bold>) Representational similarity as a function of relative pupil size (obtained by the division of the average pupil sizes in a pair of movie repeats) for all recorded units. (<bold>d</bold>) The average representational similarity of all mice shown in (<bold>c</bold>) for datasets 1 (red) and 2 (blue) separately. Bottom: same when V1 units are only included, from the sessions with more than 40 units (the inclusion criterion). (<bold>e</bold>) Same as (<bold>c</bold>) when V1 units are only included in the analysis. There are fewer individual sessions here because not all sessions contained more than 40 V1 units. (<bold>f</bold>) Same as (<bold>d</bold>) for V1 units. (<bold>g</bold>) Same as <xref ref-type="fig" rid="fig1">Figure 1g</xref>, for a stronger value of noise (x2 N; left) or a weaker value of behavioural signal (x0.5 B; right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Wide and mixed distribution of stimulus and behavioural modulations.</title><p>(<bold>a</bold>) Distribution of stimulus reliability for all V1 units from all sessions in Neuropixels dataset 1 (red) and dataset 2 (blue). The average stimulus selectivity for the two datasets are (0.32, 0.36), respectively. Grey lines show the distribution of stimulus reliability when it is calculated for each block separately and then average across the two blocks. The average values obtained in this manner are (0.36, 0.40) for the two datasets, respectively. (<bold>b</bold>) Sample activity of V1 units with high stimulus reliability (indicated by the numbers on the top) from each dataset. Top: the activity in response to each movie repeat; bottom: average activity in each block of presentation. (<bold>c</bold>) Distribution of behavioural modulation of all V1 units for the two datasets. Behavioural modulation is obtained as the correlation coefficient (CC) of each unit’s activity with pupil size. (<bold>d</bold>) Sample activity of V1 units with strong modulation by pupil size (numbers indicated on the top). Top: tuning of unit’s activity with pupil size. Bottom: the activity of units in response to repeats of the natural movie, showing different levels of modulation by stimulus within and across blocks of presentation (denoted by the value of stimulus reliability on top). (<bold>e, f</bold>) Activity of units can be weakly or strongly modulated by stimulus or behaviour, giving rise to four possible quadrants. Sample V1 units from each quadrant are shown for Neuropixels dataset 1 (<bold>e</bold>) and dataset 2 (<bold>f</bold>). For each sample, z-score activity of the unit across different repetitions of the movie is plotted (left), with the number on top denoting stimulus reliability of the unit. The average activity of each unit as a function of average pupil size (during each movie repeat) is plotted on the right, with the number on the top denoting behavioural modulation of the unit (CC with pupil size).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Stimulus-independent behavioural modulation of CA1.</title><p>(<bold>a</bold>) Top: average stimulus reliability across units in V1 and CA1 for different mice in each dataset. Bottom: same for the average (across units) of the absolute value of behavioural modulation. Filled circles: average across mice. Red: Neuropixels dataset 1; blue: Neuropixels dataset 2. (<bold>b</bold>) Top: sample activity of CA1 units (from Neuropixels dataset 1) with considerable modulation by pupil size (numbers indicated on the top). Bottom: the activity of units in response to repeats of the natural movie. (<bold>c</bold>) Same as (<bold>b</bold>) for Neuropixels dataset 2. (<bold>d</bold>) Schematic representation of population responses with stimulus-evoked (red) and behaviourally induced (green) components to the repeats of the same stimulus. Even if the stimulus-evoked component is different between repeats (red), the population vector of responses (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>) can have some similarity due to the constancy of the component set by the behaviour (green). (<bold>e</bold>) Average representational similarity as a function of change in pupil width (similar to <xref ref-type="fig" rid="fig1">Figure 1e and h</xref>, right) for V1 (left) and CA1 units (right). Red: Neuropixels dataset 1; blue: Neuropixels dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig2-figsupp3-v2.tif"/></fig></fig-group><p>In each model, we calculated representational similarity from the neuronal responses to presentations of the same stimulus and plotted that against the relative behavioural parameter (B) across the repeats (<inline-formula><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , for the <inline-formula><mml:math id="inf23"><mml:mi>i</mml:mi></mml:math></inline-formula>th and <inline-formula><mml:math id="inf24"><mml:mi>j</mml:mi></mml:math></inline-formula>th repeats) (<xref ref-type="fig" rid="fig2">Figure 2c and d</xref>). Both models showed, on average, a similar dependence of representational similarity on relative behaviour (<xref ref-type="fig" rid="fig2">Figure 2c and d</xref>; the gain model only showed the same pattern if the signal was scaled by the behaviour; we observed different patterns, if behaviour scaled the noise, or both the signal and the noise; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a and b</xref>).</p><p>To compare different models with the experimental results, we took the relative pupil size as a proxy for relative behaviour and plotted the representational similarity of all units against it (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). This revealed a similar average dependence as the signal-gain model and the independent-mixing model (<xref ref-type="fig" rid="fig2">Figure 2c–e</xref>). We observed a similar dependence for both datasets, and for most individual recording sessions within each dataset (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1c–f</xref>). Similar results were observed when representational similarity was calculated from V1 units or all recorded units (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1c–f</xref>).</p><p>We then asked how, at the level of individual units, the modulations of responses by stimulus and behaviour are related to each other (<xref ref-type="fig" rid="fig2">Figure 2f and g</xref>). To this end, instead of calculating representational similarity at the population level, we quantified the similarity of individual units’ responses to multiple repeats of the stimulus (stimulus reliability; see ‘Methods’ and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1d</xref>). In the signal-gain model, stimulus reliability was highly correlated with behavioural modulation of units (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). This is a consequence of the scaling of the signal by the behaviour, which implies that neurons with higher signal component also show higher modulation with the behavioural parameter (see ‘Methods’). The signal-gain model therefore predicts that neurons which are strongly modulated by the stimulus also show strong modulation by the behaviour (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). In contrast, the independent-mixing model predicted an independent relationship between stimulus and behavioural modulation of individual units (<xref ref-type="fig" rid="fig2">Figure 2g</xref>).</p><p>We tested these predictions in experimental data by calculating behavioural modulation and stimulus reliability of individual units in all mice across both datasets. Behavioural modulation was calculated as the correlation of each unit’s activity with pupil size, and stimulus reliability was obtained as the average correlation of each unit’s activity vectors across multiple repeats of the natural movie (see ‘Methods’ and <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1d</xref>). As opposed to the signal-gain model, we did not observe a correlation between stimulus and behavioural modulation (<xref ref-type="fig" rid="fig2">Figure 2h</xref>). In fact, a regression analysis suggested that the two modulations are independent of each other in both datasets (<xref ref-type="fig" rid="fig2">Figure 2h</xref>), consistent with the independent-mixing model. The marginal distributions matched better with experimental distributions when we increased noise (x1.5 N) or decreased the behavioural signal (x0.5 B) (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1g</xref>).</p><p>Overall, there was a wide distribution of stimulus reliability (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2a</xref>) and behavioural modulation (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2c</xref>) across recorded units, with patterns highly consistent across the two datasets. Most V1 units showed variable responses to repeats of the natural movie, as indicated by the peak of the distribution at low stimulus reliability (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2a</xref>). However, the distribution had a long tail composed of units with high stimulus reliability, which showed highly reliable responses across repeats of the movie (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2a and b</xref>). There was a wide spectrum of behavioural modulation too, with most units showing positive correlations with pupil size (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2c and d</xref>), and a smaller population of negatively modulated units (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2c</xref>).</p><p>The units that showed significant modulation with the stimulus were not necessarily modulated strongly by the behaviour, and vice versa; in fact, it was possible to find example units from all four combinations of weak/strong × stimulus/behavioural modulations (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2e and f</xref>). A clear example of the segregation of stimulus and behavioural modulation was observed in CA1, where the units showed, on average, very weak stimulus reliability across movie repeats, consistently across different mice and datasets (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3a</xref>). However, they were largely modulated by behaviour, to an extent comparable to V1 units (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3a–c</xref>). Taken together, these results suggest that, rather than scaling the stimulus-evoked responses, behaviour modulates the activity in a more independent and heterogeneous manner.</p></sec><sec id="s2-3"><title>Behavioural variability modulates the low-dimensional components of population activity independent of stimulus reliability</title><p>If the behavioural state of the animal modulates the neuronal responses independently of the stimulus, it should be possible to see its signature in the low-dimensional space of neural activity. To test this, we analysed the principal components (PCs) of population responses in individual sessions (<xref ref-type="fig" rid="fig3">Figure 3</xref>; see ‘Methods’). For the two example sessions we analysed previously (shown in <xref ref-type="fig" rid="fig1">Figure 1c and f</xref>), the first two PCs explained a significant fraction of variance (<xref ref-type="fig" rid="fig3">Figure 3a and g</xref>). Low-dimensional population activity showed a distinct transition between two behavioural states, which were corresponding to low versus high arousal, as quantified by different pupil sizes (<xref ref-type="fig" rid="fig3">Figure 3b and h</xref>). The first PC, which explained most of the variance, was strongly correlated with both pupil size and running speed (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1a</xref>). These results suggest that behavioural modulation contributes significantly to the low-dimensional variability of neural activity.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Behavioural variability modulates the low-dimensional components of population activity independent of stimulus reliability.</title><p>(<bold>a</bold>) Relative contribution of the first 10 principal components (PCs) of population responses to the variability of activity (quantified by the fraction of explained variance by each PC) for an example session (same as shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>). (<bold>b</bold>) Population activity in the low-dimensional space of the first three PCs (see ‘Methods’ for details). Pseudo colour code shows the pupil size at different times, indicating that the sudden transition in the low-dimensional space of activity is correlated with changes in the behavioural state. (<bold>c</bold>) Projection of units’ activity over PC1 (black) or PC2 (grey) (respective PC loadings) versus stimulus reliability of the units. It reveals no correlation between the two, as quantified by best-fitted regression lines in each case (the best-fitted regression lines and R<sup>2</sup> values shown by respective colours). PC loadings are normalized to the maximum value in each case. (<bold>d</bold>) Fraction of variance explained by PCs 1–3 for all sessions. (<bold>e</bold>) Distance on PC1 versus the difference of the average pupil size between different time points on the trajectory for all sessions. For each pair of non-identical time points on the PC trajectory, the absolute difference of PC1 components is calculated as the distance on PC and plotted against the absolute difference of the average pupil width (normalized by z-scoring across the entire session). Best-fitted regression line and the corresponding R<sup>2</sup> value are shown in red for all data points, and in blue for data points with small time difference between the pairs (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). (<bold>f</bold>) Same as (<bold>c</bold>) for all units from all sessions in dataset 1. (<bold>g–l</bold>) Same as (<bold>a–f</bold>) for Neuropixels dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Relation of the principal components (PCs) of neural activity to the average activity of units and their stimulus reliability.</title><p>(<bold>a</bold>) Strong correlation between PC1 and the behavioural state of animal, as assayed by either pupil size (left) or running speed (right). Both pupil size and running speed are normalized by z-scoring across the entire session. (<bold>b</bold>) Projections of the activity of units in example sessions over the first three PCs (<xref ref-type="fig" rid="fig3">Figure 3a, b, g and h</xref>), with the average activity of each unit indicated by the pseudo colour code. (<bold>c</bold>) Projection of units’ activity over PC1/PC2 versus the average activity of the unit. The best-fitted regression lines and R<sup>2</sup> values in each case are shown. (<bold>d</bold>) Similar to <xref ref-type="fig" rid="fig3">Figure 3c and i</xref> for individual sessions. R<sup>2</sup> values of regression lines fitted to the projection of units’ activity over PC1/PC2 versus stimulus reliability of the respective units are plotted for individual sessions. Upper: Neuropixels dataset 1; lower: Neuropixels dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Principal component (PC)1 and its relation to difference in pupil size and time.</title><p>(<bold>a</bold>) Relationship between distance on PC1 (<xref ref-type="fig" rid="fig3">Figure 3e and k</xref>), change in pupil width and passage of time. Data points highlighted in blue show the cluster with smaller passage of time (corresponding to movie repeats within the same blocks of presentation). Note that the two clusters arise from two blocks of stimulus presentation, which are separated from each other for &gt;80 min in both datasets. (<bold>b</bold>) Distance on PC1 versus change in pupil width for data points with small passage of time (blue data points in <bold>a</bold>). The best-fitted regression line and its R<sup>2</sup> value are denoted in blue. (<bold>c</bold>) Distance on PC1 versus passage of time. The best-fitted regression line and its R<sup>2</sup> value are denoted in red.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig3-figsupp2-v2.tif"/></fig></fig-group><p>To link the low-dimensional modulation of activity by behaviour to single neurons, we next analysed the projection of units’ activity over the PCs by looking at PC loadings. Individual units were projected in the PC space according to their respective PC loadings (PC projections). Visualizing the average activity of units in the space of PC projections suggested a spectrum of weakly to highly active units (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1b</xref>). In fact, neural projections over the first two PCs were correlated with the average activity of neurons (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1c</xref>). In contrast to the average activity, the PC projections did not reveal any relationship with the stimulus reliability of units (<xref ref-type="fig" rid="fig3">Figure 3c and i</xref>), suggesting that the low-dimensional neural activity is modulated independently of stimulus-evoked modulation of responses.</p><p>These results were remarkably consistent across different datasets and across difference mice. The first two PCs explained similar levels of variance across more than 20 mice in each dataset (<xref ref-type="fig" rid="fig3">Figure 3d and j</xref>). The distance over PC1 was highly correlated with the difference of pupil size on the trajectory (<xref ref-type="fig" rid="fig3">Figure 3e and k</xref>). Importantly, this relationship also held when controlled for the passage of time (<xref ref-type="fig" rid="fig3">Figure 3e and k</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). In both datasets, the regression analysis revealed no relationship between the two PC projections and the stimulus reliability of units (<xref ref-type="fig" rid="fig3">Figure 3f and l</xref>; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1d</xref> for individual sessions). We therefore conclude that behaviour significantly modulates the low-dimensional components of neural activity, but this modulation does not specifically enhance the activity of neurons which are more reliably representing the stimulus.</p></sec><sec id="s2-4"><title>Behaviour modulates the setpoint of responses</title><p>To gain further insight into how the behaviour modulates the low-dimensional pattern of population activity, we analysed the relation between behavioural parameters and the average activity of units. In the two example sessions analysed previously (shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>), there was a transition in the average pupil size and running speed in the second block, which was correlated with an overall increase in the average population activity (<xref ref-type="fig" rid="fig4">Figure 4a and e</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1a</xref>). In general, change in the pupil size explained a significant fraction of changes in population activity of V1 units in all sessions (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1b</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Behaviour modulates the setpoint of responses.</title><p>(<bold>a</bold>), Average population activity and behavioural parameters (pupil size and running speed) during the first and second blocks of presentation of natural movie 1 (same examples sessions as <xref ref-type="fig" rid="fig1">Figure 1</xref>). Grey, first block; black, second block; each point corresponding to the average in one repeat of the movie. (<bold>b</bold>) Setpoint similarity is calculated as the correlation coefficient of population vectors composed of average activity of units during each repeat of movie presentation. Change in the behavioural state (as quantified by the pupil size) between the two blocks is correlated with a drastic decrease in the average between-block setpoint similarity. Note that transient changes of pupil within each block also modulate the setpoint similarity. (<bold>c</bold>), Setpoint similarity (as in <bold>b</bold>) as a function of change in pupil size (z-scored pupil width) between the movie repeats, when the population vectors of setpoints are composed of V1 units (left) or all recorded units (right). (<bold>d</bold>), Dependence of setpoint similarity on pupil change for all sessions, calculated from within-block and across-block averages in each session. (<bold>e–h</bold>), Same as (<bold>a–d</bold>) for dataset 2. Source data (for the change in pupil width and setpoint similarity between pairs of movie repeats) are provided for individual sessions across the two datasets (<xref ref-type="supplementary-material" rid="fig4sdata1">Figure 4—source data 1</xref>).</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig4">Figure 4</xref>.</title><p>Source data (for the change in average z-scored pupil width and setpoint similarity between pairs of movie repeats) for individual sessions across the two Neuropixels datasets.</p></caption><media mimetype="application" mime-subtype="xls" xlink:href="elife-77907-fig4-data1-v2.xls"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Average activity of units is modulated by behavioural state.</title><p>(<bold>a</bold>) Average population activity (left y-axis) and running speed (right y-axis) as a function of pupil size, for the example shown in <xref ref-type="fig" rid="fig4">Figure 4a</xref> from Neuropixels dataset 1. (<bold>b</bold>) Average population activity of V1 units during each movie presentation as a function of pupil size from all recorded sessions. Pupil size for each repeat is normalized (within each session) by subtracting the mean value (across repeats) and dividing by it. (<bold>c</bold>) For the example session in <xref ref-type="fig" rid="fig4">Figure 4</xref>, the average (across movie frames) activity of V1 units is calculated and their mean and SD across movie repetitions in each block are shown. Units are sorted in both blocks according to the mean in the first block. (<bold>d</bold>) Average activity (across movie frames and repeats) of units during the second block versus the first. Note the logarithmic scales. (<bold>e</bold>) R<sup>2</sup> values of the regression fits to the data like <xref ref-type="fig" rid="fig4">Figure 4c</xref>, when the population vectors are composed of the average activity of units during presentation of each individual frame (1 s long) of the natural movie. (<bold>f–j</bold>) Same as (<bold>a–e</bold>) for Neuropixels dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Nonmonotonic relationship between setpoint similarity and behaviour.</title><p>(<bold>a</bold>) Relationship between setpoint similarity and change in pupil size for the same example session as <xref ref-type="fig" rid="fig4">Figure 4g</xref>. (<bold>b</bold>) Same data as (<bold>a</bold>) when setpoint similarity is plotted as a function of both changes in pupil size and change in running speed between movie repeats. Red data points indicated movie repeats with small difference of average running speed between them (<inline-formula><mml:math id="inf25"><mml:mi>Δ</mml:mi></mml:math></inline-formula> run. speed &lt;10 cm/s), and blue data points indicate movie repeats with large changes of average pupil width (|<inline-formula><mml:math id="inf26"><mml:mi>Δ</mml:mi></mml:math></inline-formula> pupil width| &gt; 1.25). Black: other data points. (<bold>c</bold>) Same as (<bold>a</bold>) for red data points in (<bold>b</bold>) (for movie repeats with small changes of average running speed between them). (<bold>d</bold>) Average pupil width (normalized by z-scoring) during each movie presentation in the first (repeat #1–30) and the second (repeat #31–60) blocks of presentations. Red and blue bars show, respectively, the movie presentations above (&gt;–0.8; red dashed line) and below (&lt;–1.2; blue dashed line) the average of the pupil size in the first block. (<bold>e</bold>) Same as (<bold>a</bold>) when setpoint similarity between specific pairs of movie repeats are colour coded. For pairs of movie repeats with one in the first block and the other in the second block, setpoint similarity is colour coded according to the average pupil width during the movie repeat in the first block, with red and blue circles corresponding to blue and red bars in (<bold>d</bold>), respectively. (<bold>f</bold>), Shift in the eye position across movie presentations. <inline-formula><mml:math id="inf27"><mml:mi>Δ</mml:mi><mml:mi>X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf28"><mml:mi>Δ</mml:mi><mml:mi>Y</mml:mi></mml:math></inline-formula> denote deviation of the average centre of the pupil (X and Y) from their grand average value during the whole session.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Changes in the pupil centre and their relation to setpoint similarity and changes in pupil width (for sessions in Neuropixels dataset 1).</title><p>First column: same as <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2f</xref>, showing shift in the eye position across movie presentations for each session (session # indicated on the top). Second and third columns: same as <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2a</xref>, when setpoint similarity is plotted against changes in the average eye position between movie repeats. Fourth column: relation between changes in eye position and changes in pupil width.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig4-figsupp3-v2.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Changes in the pupil centre and their relation to setpoint similarity and changes in pupil width (for sessions in Neuropixels dataset 2).</title><p>Same as <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> for Neuropixels dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig4-figsupp4-v2.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Overall average activity of units and average pupil width gradually increase during the recording session.</title><p>Average activity of units (across all recorded units and all sessions/animals) across time during the recording session. Each recording session (~2.5 hr) is broken to 10 time bins and the mean value within that time bin is calculated. The average z-scored value of pupil width (averaged across animals and within the same time bins) is shown on the right (red bars). (<bold>a</bold>) Neuropixels dataset 1; (<bold>b</bold>) Neuropixels dataset 2. Note that for both datasets the average activity increases gradually from ~6 spikes/s to ~9 spikes/s at the end (~50% increase). Also, note the similar, gradual increase of pupil width.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig4-figsupp5-v2.tif"/></fig></fig-group><p>We also looked at the average activity of individual units across all movie frames and repetitions (their setpoints). Units had a wide range of setpoints, which were relatively stable within each block (small variance across repetitions relative to the mean) (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1c</xref>). However, the setpoints changed upon transition to the next block, with most units increasing their setpoints, without an apparent link to their previous setpoint levels (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1d</xref>). The population vectors composed of setpoints in each repeat can be further used to quantify setpoint similarity (<xref ref-type="fig" rid="fig4">Figure 4b and f</xref>). Within-block correlations were high, indicating the stability of setpoints when behavioural changes were minimum – although occasional, minor changes of pupil size still modulated these correlations (<xref ref-type="fig" rid="fig4">Figure 4b and f</xref>). Most changes in setpoint similarity, however, appeared between the blocks, when the animal experienced the largest change in its behavioural state.</p><p>Quantifying the dependence of setpoint similarity on changes in pupil size revealed a strong relationship, both for V1 units and for all recorded units (<xref ref-type="fig" rid="fig4">Figure 4c and g</xref>). The relationship was rather stable when calculated from responses to single frames of movie presentation, instead of the average activity across the entire movie (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1e</xref>). We obtained similar results when the dependence was calculated from the average block activity across all mice from both datasets (<xref ref-type="fig" rid="fig4">Figure 4d and h</xref>).</p><p>The relationship of setpoint similarity with changes in pupil size was not always monotonic. For the example session in <xref ref-type="fig" rid="fig4">Figure 4g</xref>, despite an overall drop of setpoint similarity with increased changes in pupil width, another trend was observed for large pupil changes. To understand what underlies this nonmonotonic trend we plotted setpoint similarity as a function of changes in both pupil size and running speed (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). The multidimensional space of changes in behaviour suggests that different trends exist at different regimes of behavioural changes (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2b</xref>). Notably, limiting our analysis to the pairs of movie repeats with small changes in running speed revealed a monotonic relationship between setpoint similarity and changes in pupil size, with similar levels of regression between the two parameters (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2a–c</xref>).</p><p>The opposite trend emerged at higher levels of changes in pupil size and running speed, and seemed to be related to changes in pupil size in the first block of presentation (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2d and e</xref>). Indeed, movie presentations with higher (/lower) average pupil size in the first block showed smaller (/larger) setpoint similarity with the movie repeats in the second block, on average (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2d and e</xref>). This trend was accompanied, and might be explained, by an average movement in the centre of the pupil between the two blocks (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2f</xref>). Such a movement can shift the receptive fields, and therefore change the setpoint activity of units, leading to changes in setpoint similarity between the two blocks. This effect is expected to be higher for higher pupil sizes, and therefore the setpoint similarity is specifically reduced for movie presentations with larger pupil sizes in the first block.</p><p>The resulting change in the average visual input may also explain why the nonmonotonic trend is specifically present in V1 units and tends to disappear when all units are considered (<xref ref-type="fig" rid="fig4">Figure 4g</xref>). Consistent with this reasoning, shifts in the average centre of pupil also showed correlation with changes in setpoint similarity across many sessions (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplements 3</xref> and <xref ref-type="fig" rid="fig4s4">4</xref>). This was reminiscent of the dependence of setpoint similarity on pupil width and tended to be higher for sessions with strong correlation between changes in pupil size and position (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplements 3</xref> and <xref ref-type="fig" rid="fig4s4">4</xref>).</p><p>Our results therefore suggest that the changes in setpoint similarity can arise from complex interaction between multiple behavioural parameters and their modulation of neural activity. On a case-by-case basis, it remains to be seen how behavioural parameters and their interactions specifically modulate neural activity on finer scales. On average, however, there was a gradual increase of the average pupil width during the recording session, which was paralleled by a gradual increase of the average activity of units (~50% increase for the average activity of all units across all mice in both datasets; <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>). The results of our analysis therefore suggest that the behavioural signal can modulate the setpoint of neural activity independent of and in addition to stimulus, and, in doing so, induce a similarity (/dissimilarity) of population responses when behaviour is stable (/changing).</p><p>Note that an unintuitive connotation of this finding is that quantifying response similarity from population vectors may reveal representational drift upon behavioural changes, even independent of stimulus-evoked modulation of activity. This is because the constancy of setpoint activity of units would lead to some degree of similarity between population vectors, even if the stimulus-evoked component is different (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3d</xref>). The behaviourally induced component of similarity changes more slowly, leading to a drop in representational similarity on a longer timescale (e.g. between blocks of stimulus presentation, rather than within them). In line with this reasoning, we observed a similar drop of representational similarity in CA1 (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3e</xref>), although individual units in this region had, on average, no reliable visual representations (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3a</xref>). Modulation of the average setpoint activity – and hence setpoint similarity – by the behaviour can, therefore, contribute to representational similarity, in addition to specific stimulus-induced tuning of responses.</p></sec><sec id="s2-5"><title>Behaviour reliably modulates responses during active states</title><p>What distinguishes an independent behavioural signal from a modulatory component or from noise is that it brings about reliable responses for different states of behaviour. That is, there should exist a reliable and independent tuning of units with behavioural parameters (like pupil size or running speed). We therefore investigated more precisely how the neural activity is modulated by behaviour (<xref ref-type="fig" rid="fig5">Figure 5</xref>). We used the correlation of units’ activity with running as a metric for behavioural tuning. To obtain a measure of significance of correlations, we calculated bootstrapped correlations (see ‘Methods’). More than half of the units showed significant modulation by running, and the fraction and distribution of significant correlations were similar between the two blocks and across the two datasets (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Behaviour reliably modulates responses during active states.</title><p>(<bold>a</bold>) Correlation of activity with running during second block against the first block for all V1 units (left) and for selected sessions and units (right). In the latter case, only sessions with similar average running between the two blocks, and units with significant correlation with running, are selected (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> and ‘Methods’ for details). In addition to the regression analysis (quantified by the value of R<sup>2</sup>), a second metric (sign constancy index [SCI]) is also used to quantify the reliability of modulations by behaviour. For each unit, the sign of correlation with running in different blocks is compared and the fraction of units with the same sign is reported as SCI. (<bold>b</bold>) Upper: average z-scored value of running in the first and second block across all units/sessions (all; red) and for selected ones (sel; magenta). Lower: correlation of all V1 units with pupil size against their correlation with running in first (grey) and second (black) blocks. Magenta: regression fits for selected units/sessions only. (<bold>c</bold>) Same as (<bold>a</bold>) for recorded units from all regions. (<bold>d–f</bold>) Same as (<bold>a–c</bold>) for dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Significant modulation of units by behaviour.</title><p>(<bold>a</bold>) Left: distribution of correlation of V1 units’ activity with running during first and second blocks of presentation of natural movie 1. Right: distribution of bootstrapped correlations with running. Correlation coefficient (CC) of each unit’s activity with 100 randomly shuffled versions of the running speed is calculated. The z-score of bootstrapped correlation (Z) is calculated by subtracting the mean of this distribution from the unshuffled CC and dividing it by the SD of the distribution (see ‘Methods’ for details). Bootstrapped correlations are calculated during the first (grey) and second (black) blocks separately. Significant correlations are taken as units for which |Z| &gt; 2 (indicated by dashed red lines). Fractions of significant correlations during the first and second blocks are indicated on the top, respectively. (<bold>b</bold>) same as (<bold>a</bold>) for all recorded units. (<bold>c, d</bold>) Same as (<bold>a, b</bold>) for dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Consistent modulation of neuronal responses by behaviour across blocks of presentation of drifting gratings and across stimuli.</title><p>(<bold>a</bold>) Left: distribution of correlations with running during the first and second blocks of presentation of drifting gratings across all sessions. Right: distribution of the z-score of bootstrapped correlations (Z) with running (see ‘Methods’ and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Significant correlations with running are defined as |Z| &gt; 0.2. (<bold>b</bold>) Correlation with running of units during the second block against the first block for all units and sessions (left; red) and for selected units (right; magenta), where sessions with similar levels of running between the two blocks and units with significant correlations are selected. (<bold>c</bold>) Upper: average running during the first and second blocks for all sessions (all; red) and for selected units (sel; magenta). Lower: correlation of all units with pupil versus their correlation with running during the first (grey) and second (black) blocks. Magenta: regression fits for selected units only. (<bold>d</bold>) Correlation of all units with running speed during the presentation of drifting gratings versus correlations with running obtained during the presentation of natural movie 1 in the first and second blocks of presentations, respectively. (<bold>e–h</bold>) Same as (<bold>a–d</bold>) for dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Consistent modulation of neuronal responses by behaviour across stimuli, regions, and datasets.</title><p>(<bold>a, b</bold>) Analysis of the reliability of behavioural tuning in two parts of each session in the two datasets. The composition of stimulus sets in each session type is shown, with the type, sequences, and the length of each stimulus presentation indicated. In both datasets, correlation of units with running speed is calculated in two parts: first part from 30 to 90 min, and the second part from 90 to 150 min. (<bold>c, d</bold>) Similar to <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a–c</xref> for the first and second parts of the sessions in dataset 1 (<bold>c</bold>) and dataset 2 (<bold>d</bold>). (<bold>e</bold>) Upper: distribution of correlations with running for units recorded from different regions. Results for the two parts of the sessions (lighter lines denoting the first part) and both datasets (red: dataset 1; blue: dataset 2) are overlayed. Sessions where the average running between the two parts are too different are excluded (exclusion criteria: |Z2-Z1| &gt; 0.3, where Z1 and Z2 are the average of the z-scored value of running speed in the first and second parts, respectively). Lower: correlation with running in the second part against the first part in each region for dataset 1 (red) and dataset 2 (blue), respectively. Lines show the best-fitted least-square regression lines, with numbers denoting the R<sup>2</sup> values of the fit in each case. (<bold>f</bold>) Tuning reliability (average R<sup>2</sup> values in <bold>e</bold>) for different regions across the two datasets. Regions key: [visual cortex, VIS] VISp: primary visual cortex; VISl: lateromedial area; VISrl: rostrolateral area; VISal: anterolateral area; VISam: posteromedial area; VISam: anteromedial area. [Hippocampal formation] CA1: cornu ammonis 1; CA3: cornu ammonis 3; DG: dentate gyrus; SUB: subiculum; ProS: prosubiculum. [Thalamus] LGd: lateral geniculate nucleus; LP: lateral posterior nucleus. [Midbrain] APN: anterior pretectal nucleus.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig5-figsupp3-v2.tif"/></fig></fig-group><p>Another way to assay the reliability of behavioural tuning is to test whether the correlation of units with behaviour remains stable between the two blocks of presentation (<xref ref-type="fig" rid="fig5">Figure 5a and d</xref>). Random correlations with running should be uncorrelated across the two repeats. In contrast, regression analysis revealed a good correlation between the two blocks (<xref ref-type="fig" rid="fig5">Figure 5a and d</xref>, left). The distributions of correlations with behaviour were also similar between the two blocks (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Notably, focusing on sessions with similar levels of running between the two blocks (<xref ref-type="fig" rid="fig5">Figure 5b and e</xref>), and on units with significant behavioural modulation, improved the similarity of tuning between the two repeats (<xref ref-type="fig" rid="fig5">Figure 5a and d</xref>, right). Specifically, most units which were positively (/negatively) modulated during the first block remained positively (/negatively) modulated in the second block (<xref ref-type="fig" rid="fig5">Figure 5a and d</xref>, right). These results therefore suggest that a significant fraction of the population shows reliable modulation by running – similar result is expected for pupil, as we observed a high correlation between modulation of units with running and pupil in both datasets (<xref ref-type="fig" rid="fig5">Figure 5b and e</xref>, lower).</p><p>Our results held when repeating the analysis for all units instead of V1 units only (<xref ref-type="fig" rid="fig5">Figure 5c and f</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We also observed similar results when quantifying the reliability of tuning between two blocks of presentation of another stimuli (drifting grating; <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). Notably, the tuning of units remained stable from one stimulus type to another: modulation of units during presentation of drifting gratings had a good correlation with their modulation during natural movie presentations for both blocks (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d and h</xref>). The tuning with running was even reliable between the first (30–90 min) and second (90–150 min) parts of the entire session, with each part containing different stimuli (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). We did a region-specific analysis of this reliability and found that reliable tuning exists in various regions (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). Overall, these analyses suggest that behaviour reliably and independently modulates neuronal responses.</p></sec><sec id="s2-6"><title>Stimulus dependence of behavioural variability and setpoint similarity</title><p>External stimulus directly modulates the responses by activating selective receptive fields of neurons, which can be measured under anaesthesia (<xref ref-type="bibr" rid="bib31">Niell and Stryker, 2008</xref>; <xref ref-type="bibr" rid="bib44">Yoshida and Ohki, 2020</xref>). In awake behaving animals, however, it is possible that different stimulus types indirectly modulate the responses by inducing different patterns of behavioural variability. We indeed found that this was the case when comparing natural movies with an unnatural stimulus (drifting gratings) (<xref ref-type="fig" rid="fig6">Figure 6</xref>). Natural movies induced more variability of pupil size and running in the animals across the two blocks of stimulus presentations: both measures significantly increased during the second block for natural movies, whereas changes were not significant for drifting gratings (<xref ref-type="fig" rid="fig6">Figure 6a and d</xref>). The result was consistent across the two datasets with different length and sequence of stimulus presentations (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3a and b</xref>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Stimulus dependence of behavioural variability and setpoint similarity.</title><p>(<bold>a</bold>) Average pupil size and running speed during the first (grey) and second (black) blocks of presentation of natural movies (left) and drifting gratings (right) for different sessions (empty circles). Filled circles: the mean across sessions. Pupil size and running speed are z-scored across each session, respectively. p-Values on top show the result of two-sample <italic>t</italic>-tests between the two blocks. NS, p&gt;0.05; *p≤0.05; **p≤0.01; ***p≤0.001; ****p≤0.0001. (<bold>b</bold>) Average setpoint similarity between the two blocks of presentation of natural movie 1 (NM) and drifting gratings (DG) for different sessions. Sessions are sorted according to their average setpoint similarity for NM. Population vectors are built out of the average responses of all units to 30 randomly chosen frames (1 s long). The correlation coefficient between a pair of population vectors from different blocks (within the same stimulus type) is taken as setpoint similarity. The procedure is repeated for 100 pairs in each session and the average value is plotted. Error bars show the SD across the repeats. (<bold>c</bold>), Left: setpoint similarity as a function of the difference in average running, <inline-formula><mml:math id="inf29"><mml:mi>Δ</mml:mi><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , where <inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the average running during randomly chosen frames in the first and second block, respectively. The lines show the average of the points in 40 bins from the minimum <inline-formula><mml:math id="inf32"><mml:mi>Δ</mml:mi><mml:mi>Z</mml:mi></mml:math></inline-formula> to the maximum. Right: distribution of changes in running for different stimuli. The probability density function (pdf) is normalized to the interval chosen (0.25). (<bold>d–f</bold>) Same as (<bold>a–c</bold>) for dataset 2. Source data (for the average z-scored pupil width and running speed in each block) are provided for individual sessions across the two datasets (<xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1</xref>).</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Related to <xref ref-type="fig" rid="fig6">Figure 6</xref>.</title><p>Source data (for the average z-scored values of pupil width and running speed in each block) for individual sessions across the two Neuropixels datasets.</p></caption><media mimetype="application" mime-subtype="xls" xlink:href="elife-77907-fig6-data1-v2.xls"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig6-v2.tif"/></fig><p>To see if and how this difference affects response similarity of units, we calculated average setpoint similarity (<xref ref-type="fig" rid="fig4">Figure 4</xref>) between the two blocks of presentations from the shuffled activity of units in response to different stimuli (see ‘Methods’). Average setpoint similarity was high for both stimuli, but it was significantly larger for drifting gratings for most sessions (<xref ref-type="fig" rid="fig6">Figure 6b and e</xref>). Plotting setpoint similarity as a function of behavioural changes for the entire distribution revealed its composition across the two stimulus types. Responses to drifting gratings showed, on average, a higher setpoint similarity for similar behavioural states (small behavioural changes) (<xref ref-type="fig" rid="fig6">Figure 6c and f</xref>), arguing for more stability of average responses even independent of behavioural variability. Larger behavioural changes were more prevalent for the natural movie presentations, and units’ responses showed a large drop of setpoint similarity at these deviations (<xref ref-type="fig" rid="fig6">Figure 6c and f</xref>), leading to a significant drop of average setpoint similarity compared to drifting gratings. Taken together, these results suggest that stability of population responses to different stimulus types might be determined by the combined effect of stimulus-evoked reliability of responses and its indirect modulation by behavioural variability.</p></sec><sec id="s2-7"><title>Decoding generalizability improves by focusing on reliable units</title><p>How does behavioural variability affect the decoding of stimulus-related information, and how can decoding strategies be optimized to circumvent the drift of representations? Our analyses so far suggested that behaviour modulates the responses in addition to and independently of stimulus-evoked modulations (independent model in <xref ref-type="fig" rid="fig2">Figure 2</xref>). This independent behavioural modulation would be perceived as noise, if a downstream decoder is decoding stimulus-related signals, and can compromise the generalizability of decoding. For instance, the activity of a subpopulation of units (A) might be assigned to stimulus A by the decoder in the absence of significant behavioural modulation. If the decoder is now tested in a new condition where behaviour modulates the responses independently of the presented stimulus, the activity of subpopulation A can be interpreted as presence of stimulus A, independent of the presented stimulus. This is in contrast to the gain model (signal-gain model in <xref ref-type="fig" rid="fig2">Figure 2b</xref>) in which behavioural state scales the stimulus-evoked signal, and can therefore not compromise the generalizability of decoding (subpopulation A only responds to stimulus A, but with different gains). In the signal-gain model, focusing on units which are strongly modulated by behaviour should in fact enhance the decoding generalizability under behavioural variability, whereas in the independent model the focus should be on units with more stimulus reliability.</p><p>We tested these two alternatives directly by training a linear decoder to discriminate between different frames of the natural movie (see <xref ref-type="fig" rid="fig7">Figure 7a</xref> and ‘Methods’). The decoder was trained on the activity of units in the first block to detect a target frame; it was then tested on the second block of presentation to discriminate between the target frame and other frames in order to evaluate the generalizability of decoding (i.e. out-of-distribution transfer) (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). When the decoder was trained on the activity of all units in the first block, discriminability (<inline-formula><mml:math id="inf33"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>) was very low in the second block (<xref ref-type="fig" rid="fig7">Figure 7b, c, e and f</xref>). However, focusing on the reliable units (units with high stimulus reliability) shifted the distribution of <inline-formula><mml:math id="inf34"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> to larger values and increased the average discriminability (<xref ref-type="fig" rid="fig7">Figure 7c and f</xref>). Focusing on units with strong behavioural modulation, on the other hand, did not yield higher discriminability in the second block (<xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). These results suggest that behavioural modulation is detrimental to generalizability of stimulus decoding, and that this problem can be circumvented by focusing on units with more stimulus information.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Decoding generalizability of natural images improves by focusing on reliable units.</title><p>(<bold>a</bold>) Schematic of a decoder which is trained on the population activity during the first block of presentation of the natural movie (upper) and tested on the second block of presentation to discriminate different frames of natural images from each other (out-of-distribution transfer; see ‘Methods’ for details). (<bold>b</bold>) Matrix of discriminability index (‘Methods’), <italic>d</italic>′, between all combinations of movie frames as target and test, when all units (left) or only units with high stimulus reliability (right) are included in training and testing of the decoder. (<bold>c</bold>) Distribution of <italic>d</italic>′ from the example discriminability matrices shown in (<bold>b</bold>) for decoders based on all units (black) and reliable units (green). Reliable units are chosen as units with stimulus reliability (‘Methods’) of more than 0.5. (<bold>d</bold>) Average <italic>d</italic>′ for all mice, when all units or only reliable units are included. Size of each circle is proportionate to the number of units available in each session (sessions with &gt;10 reliable units are included). Filled markers: average across mice. (<bold>e–g</bold>) Same as (<bold>b–d</bold>) for dataset 2. Data in (<bold>b, c</bold>) and (<bold>e, f</bold>) are from the same example sessions shown in <xref ref-type="fig" rid="fig1">Figure 1c</xref> and <xref ref-type="fig" rid="fig1">Figure 1f</xref>, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Decoding natural images does not improve by focusing on behaviourally modulated units.</title><p>Same as <xref ref-type="fig" rid="fig7">Figure 7c and d</xref> when reliable (Rel.) units are chosen as units with strong behavioural modulation (correlation with running speed of more than 0.5), instead of units with strong stimulus reliability (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Relation between average <italic>d</italic>′ and the number of units available for decoding in each session (all units or behaviourally reliable units) is plotted on the bottom.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Decoding behavioural states from the population activity.</title><p>(<bold>a</bold>) A linear decoder is trained to predict the pupil width (upper) and the running speed (lower) of the animal from the activity of all recorded units in each session. It is trained on the data from half of randomly chosen movie presentations from both blocks (not shown) and tested on the other half of movie presentations (orange traces). The target values (test) for a specific randomization are shown in blue (note that the order of movie repeats is randomized, while the order of movie frames within each movie presentation is preserved). The quality of prediction is quantified by the correlation coefficient (CC) of test and predicted traces in each case (indicated for the example sessions on top of each behaviour, respectively). Example sessions are the same as in <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig7">Figure 7</xref>. (<bold>b</bold>) Average correlation of the predictions with behaviour for multiple repeats of the decoding with different random choices of train and test sets, for the example sessions shown in (<bold>a</bold>) (100 repeats) and for all sessions (10 repeats per each session). The mean and SD of the correlations across repeats are plotted. (<bold>c</bold>) The optimal weights obtained from the decoders in (<bold>b</bold>) are used to weight the neural activity and obtain a readout (similar to predictions in <bold>a</bold>). The readout activity is then used to assay the stimulus-related content of the behavioural decoder. A matrix of CC of the readout activity between different repeats of the movie is calculated. The average of the off-diagonal entries of the matrix is taken as the correlation of reconstructed activity across multiple stimulus presentations. The mean and SD of this across multiple randomisations (as in <bold>b</bold>) are plotted in (<bold>c</bold>) Left: Neuropixels dataset 1; right: Neuropixels dataset 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-77907-fig7-figsupp2-v2.tif"/></fig></fig-group><p>This effect was consistent across mice in both datasets (<xref ref-type="fig" rid="fig7">Figure 7d and g</xref>). In dataset 2, we observed higher average <inline-formula><mml:math id="inf35"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> in the second block, when the decoder was trained and tested on all units. This could be due to more presentations of the natural movie in dataset 2 (30 repetitions in each block compared to 10 in dataset 1). Larger training samples can help the decoder in learning the signal from the noise, suggesting that the effect of behavioural ‘noise’ on corrupting the stimulus signal is more significant for small sample sizes. On the other hand, longer presentations can lead to sampling from responses under more behavioural variability, which can in turn inform the decoder on how to ignore the stimulus-irrelevant modulation by behaviour. Altogether, these results corroborate our previous analysis that the contribution of behavioural variability to neural activity is orthogonal to stimulus modulations and suggest that such behavioural noise limits the decoding capacity especially with limited data.</p><p>We also analysed how the behaviour can be decoded from the population activity (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>). A decoder was trained on half of movie presentations (randomly chosen from the two blocks of presentation) to predict the pupil width and running speed, and was then tested on the other half of movie presentations (see ‘Methods’). For the example sessions (shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>), both behavioural parameters were predicted with high accuracy (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2a</xref>). The accuracy was higher for dataset 2 (correlation of more than 90% for both parameters; compared to 60 and 80% for pupil width and running speed predictions, respectively, in dataset 1; <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2a and b</xref>). Similar results were obtained when the decoding of behavioural parameters was performed for other mice in both datasets (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2b</xref>).</p><p>Higher prediction accuracy in dataset 2 is consistent with our reasoning above that longer episodes of stimulus presentation provide a higher chance for behavioural variability, which can in turn enable a decoder to learn the modulations arising from the behaviour better. An extreme case for this was when the decoder was trained on the first block in the example sessions and tested on the second block. For both datasets, the accuracy was very low (results not shown). This was a consequence of drastic changes of behaviour between the two blocks and specifically very little running in the first block (<xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig5">Figure 5</xref>); in such cases, the decoder cannot learn how behaviour modulates the neural activity from the training data due to paucity of relevant samples.</p><p>We also asked whether the decoding of behaviour may create any systematic bias in processing of sensory stimulus. If the behavioural modulation has a systematic relation with the modulation of neuronal activity by stimulus, a decoder which is predicting the animals’ behaviour might be biased to infer illusionary stimuli due to the correlations between behaviour and stimulus. To test this, we weighted the responses of neurons to each movie presentation with the weights optimized for the behavioural decoder. We then calculated the correlation of the overall response of the decoder across multiple repeats of the natural movie. Systematic sensory bias in the read-out of the behavioural decoder should lead to significant correlations between the repeats of the same stimulus. Contrary to this, we observed correlations close to zero between different repeats, for the example sessions in both datasets, and across all mice (<xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2c</xref>). These results suggest that behavioural decoding can be performed independent of sensory inference.</p><p>Taken together, the results of our behavioural decoding reveal two main insights: first, the behavioural state of the animal can reliably be predicted from the neural activity. This is consistent with our results on reliable modulation of neuronal activity by behaviour (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><p>Second, behavioural decoding did not create a significant bias in sensory inference, which supports our previous results on independent modulation of neuronal responses by behaviour and sensory stimulus (<xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig3">Figure 3</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The results of our analysis here suggest that variability of the behavioural state of animal can contribute significantly to changes in representational similarity. We found that population responses to different repeats of the same natural movie were the most similar when behavioural parameters like pupil size and running speed changed the least. This was a result of an independent modulation of neural activity by behaviour, which was mixed with stimulus-evoked responses to represent a multidimensional code. Our results are consistent with a view in which behaviour modulates the low-dimensional, slowly changing setpoints of neurons, upon which faster operations like sensory processing are performed.</p><p>Small modulation of ongoing neural dynamics by sensory stimuli was reported before in awake, freely viewing animals (<xref ref-type="bibr" rid="bib16">Fiser et al., 2004</xref>), in line with other reports on the significance of internal signals even in sensory cortices (<xref ref-type="bibr" rid="bib1">Arieli et al., 1996</xref>; <xref ref-type="bibr" rid="bib24">Kenet et al., 2003</xref>; <xref ref-type="bibr" rid="bib41">Tsodyks et al., 1999</xref>). Our results here are consistent with these reports, and our analysis provides a mechanism by which variability of the internal state can contribute to ongoing signal correlations. It suggests that two distinct sources of response similarity exist in neuronal networks, with one set by baseline responses modulated on a slower timescale via internal parameters (setpoint similarity), and the other arising from finer and faster modulations invoked by sensory stimuli. Importantly, changes in representational similarity (which can lead to representational drift) can arise from changes in both sources, and hence attributing it purely to the drift of the sensory component might be inaccurate.</p><p>Internal, behavioural states of the animal can contribute independently to neural processing, or can act as a modulator for external stimuli, for instance, by increasing the input gain and enhancing the saliency of the sensory signal. Notably, our results could not be explained by a model in which behaviour acted as a gain controller for sensory inputs. Such a model would predict a direct relationship between the stimulus modulation and behavioural modulation of neurons. One would therefore expect that the most reliable neurons in representing sensory information to be modulated the most by arousal or running states. However, we found that the reliability of stimulus-evoked responses to different repeats of the same natural movie was independent of behavioural modulation, in line with a previous report (<xref ref-type="bibr" rid="bib12">Dipoppa et al., 2018</xref>).</p><p>A gain-model account of behavioural modulation would only change the signal-to-noise ratio of sensory representations by behaviour. Therefore, if the level of arousal or attention of the animal drifts away over time, the signal component of the representations becomes weaker compared to the noise, leading to some drop in representational similarity. In contrast, independent modulation of neuronal responses by behaviour affects representational similarity in more complex ways. First, similarity of population vectors across repeats of the same stimuli can be due, at least partly, to the behavioural signal rather than stimulus-evoked responses. Second, changes in behavioural signal might be perceived as sensory-irrelevant noise if the parameter space of representations (composed of internal and external parameters) is only analysed over the external stimulus dimension (<xref ref-type="bibr" rid="bib29">Montijn et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Stringer et al., 2019</xref>). Reliable changes in behavioural signals might, therefore, be misinterpreted as the drift of stimulus-induced representations in the latter scenario.</p><p>Note that our results here do not rule out the contribution of other parameters, like slow latent learning and plasticity within the circuitry, leading to the drift of sensory representations, especially over longer timescales (days to weeks). Our analysis here revealed changes in neural representations in two blocks of multiple presentations of natural movies within the same day. To determine whether neuronal representations are gradually changing, there must be multiple (more than two) different compared time points (<xref ref-type="bibr" rid="bib7">Clopath et al., 2017</xref>). Gradual change of neural representations over multiple time points can distinguish between representational drift and random neuronal variability which can arise independently in each repetition. In fact, we observed a gradual <italic>increase</italic> of both pupil width and average activity of units during the entire session (<xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>). Changes in the setpoint similarity arising from such gradual changes can therefore lead to representational drift over multiple time points. It would be interesting to repeat our analysis in future studies on other datasets which contain multiple blocks of stimulus presentations within and across days.</p><p>A recent analysis of similar datasets from the Allen Brain Observatory reported similar levels of representational drift within a day and over several days (<xref ref-type="bibr" rid="bib11">Deitch et al., 2021</xref>). The study showed that tuning curve correlations between different repeats of the natural movies were much lower than population vector and ensemble rate correlations (<xref ref-type="bibr" rid="bib11">Deitch et al., 2021</xref>) it would be interesting to see if, and to which extent, similarity of population vectors due to behavioural signal that we observed here (<xref ref-type="fig" rid="fig4">Figure 4</xref>) may contribute to this difference. In fact, previous studies showed gradual changes in the cells' activity rates during periods of spontaneous activity, suggesting that these changes can occur independently of the presented stimulus (<xref ref-type="bibr" rid="bib11">Deitch et al., 2021</xref>; <xref ref-type="bibr" rid="bib38">Schoonover et al., 2021</xref>).</p><p>The fact that there are changes that are not purely related to the tuning of cells is demonstrated in several previous studies. For instance, place cells in the mouse hippocampal CA1 can drop in and out of the ensemble during different visits to the same familiar environment over weeks, leading to a gradual change in the population of place cells that encode the location of the animal during the task (<xref ref-type="bibr" rid="bib20">Gonzalez et al., 2019</xref>; <xref ref-type="bibr" rid="bib46">Ziv et al., 2013</xref>). These changes, which reflect changes in activity rates, were independent of the position the neurons encode and were found in place cells and non-place cells alike. A similar turnover of the active cells in the cortex was also shown by <xref ref-type="bibr" rid="bib2">Aschauer et al., 2022</xref> and <xref ref-type="bibr" rid="bib13">Driscoll et al., 2017</xref>. Notably, <xref ref-type="bibr" rid="bib35">Rubin et al., 2015</xref> showed that hippocampal representations of two distinct environments (which had different place-cell representations) co-evolve over timescales of days/weeks, with the shared component of the drift stemming from gradual context-independent changes in activity rates.</p><p>The stimulus-independent component of representational drift due to behavioural variability is a global phenomenon that can affect all regions, even independent of their involvement in the processing of natural images. In fact, we found similar representational drift in many areas, including regions like CA1 (see also <xref ref-type="bibr" rid="bib11">Deitch et al., 2021</xref>), although units in this region had no reliable representation of natural images (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3a</xref>). Global, low-dimensional modulation of activity by behavioural state, like arousal and running, or other aspects of behaviour that we did not have access to their quantification here (e.g. whisking, posture, or body movements other than running), might underlie such changes in representational similarity – although we cannot rule out the contribution of other factors like contextual modulations (as discussed above) or the passage of time (but see <xref ref-type="bibr" rid="bib33">Nilchian et al., 2022</xref>), which might be more relevant to modulation of activity in regions like CA1. Drawing further conclusions about stimulus dependences of representational drift in visual cortex – and other sensory cortices – thus needs a critical evaluation by teasing apart the contribution of different components (stimulus-induced and stimulus-independent).</p><p>Another recent study reported stimulus-dependent representational drift in the visual cortex, whereby responses to natural images experienced large representational drift over weeks compared to responses to drifting gratings (<xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>). In line with the finding of this study, we found here that responses to drifting gratings were more robust to behavioural variability in general. However, we also observed that different stimulus types can induce variable patterns of behaviour, thus highlighting the combined contribution of behaviour and stimulus to representational drift. Notably, the mentioned study <xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref> found a dependence of representational drift on the pupil size (see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3c</xref> in <xref ref-type="bibr" rid="bib27">Marks and Goard, 2020</xref>), with larger decreases in pupil size over time correlating with more representational drift for both stimulus types (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1d</xref> in <xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>). Such consistent changes of behaviour may contribute to representational drift over longer timescales (days to weeks) by recruiting similar mechanisms as we described here for shorter intervals (e.g. changes in setpoint similarity). Mapping behavioural changes over longer times and per individual animal can shed light on the specific contribution of behaviour to representational drift. It would, for instance, be interesting to see if the large variability of representational drift across different animals (see <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref> in the same study; <xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>) might be linked to their behavioural variability.</p><p>Behavioural variability might be more pertinent to other modalities for which active sensing is less constrained during experiments. While eye movements are minimized in head-fixed mice, in other modalities (like olfaction) it might be more difficult to control for the behavioural variability arising from active sensing (e.g. sniffing) over time and across animals. A recent study demonstrated significant representational drift over weeks in the primary olfactory cortex of mouse (<xref ref-type="bibr" rid="bib38">Schoonover et al., 2021</xref>). The surprising finding that sensory representations are not stable in a sensory cortex was hypothesized to be linked to the different structure of piriform cortex compared to other sensory cortices with more structured connectivity. It would be interesting to see if, and to which extent, other factors like changes in the gating of olfactory bulb by variable top-down modulations (<xref ref-type="bibr" rid="bib4">Boyd et al., 2012</xref>; <xref ref-type="bibr" rid="bib26">Markopoulos et al., 2012</xref>), or changes in the sniffing patterns of animals, may contribute to this. Similar to the general decline over time of the pupil size reported in the visual cortex (<xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>), animals may change their sniffing patterns during experiments, which can in turn lead to a general or specific suppression or amplification of odours, depending on the level of interest and engagement of individual animals in different sessions.</p><p>Behavioural modulation might be more systematically present depending on the design of specific tasks, for instance, if a sensory task is novel or otherwise engages behavioural states. Interpretation of the results of neural response to novel or surprising stimuli might, therefore, be compromised if one ascribes the changes in neural activity to local computations only, without the analysis of behaviour and without controlling for more global signals (e.g. arising from arousal, running, whisking, or licking). Low-dimensional signals associated with behavioural and internal state of the animal have in fact been suggested to create a potential confound for multisensory processing, with sound eliciting eye and body movements that modulate visual cortex, independent of projections from auditory cortex (<xref ref-type="bibr" rid="bib3">Bimbard et al., 2021</xref>).</p><p>How can the animals perform reliable sensory processing in the face of extensive modulation of neural activity by behavioural changes? Our results suggest that the reliable units, which are stably encoding sensory information across behavioural states, could serve as a stable core for the brain to rely on for coping with changing neuronal responses (<xref ref-type="fig" rid="fig7">Figure 7</xref>). However, the distribution of stimulus reliability was surprisingly skewed towards lower reliability values (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). In fact, at the population level, units were more reliably modulated by behaviour (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). It is therefore likely that the brain would rely on both stimulus- and behaviour-related information, rather than focusing on a small subset of reliable units, to cope with changing representations.</p><p>Beyond sensory processing, variability of internal state can also contribute to other cognitive processes in various cortices (<xref ref-type="bibr" rid="bib22">Joshi and Gold, 2020</xref>). A recent study in monkey found that changes in the perceptual behaviour were modulated by a slow drift in its internal state, as measured by pupil size (<xref ref-type="bibr" rid="bib9">Cowley et al., 2020</xref>). This was correlated with a slow drift of activity in V4 and PFC, along with changes in the impulsivity of the animal (as reflected in the hit rates), which overrode the sensory evidence. These results, in another species, are in agreement with our findings here on the contribution of behavioural drift to changes in neural representations. Interestingly, the sensory bias model in the study could not capture the effect of the slow drift on decoding accuracy; instead, an alternative impulsivity model, which introduced the effect of slow drift as an independent behavioural parameter, matched with the data (Figure 6 in <xref ref-type="bibr" rid="bib9">Cowley et al., 2020</xref>).</p><p>Another study in monkey M1 found that learning a new BCI task was modulated along the dimension of neural engagement of the population activity, which in turn was correlated with pupil size (<xref ref-type="bibr" rid="bib21">Hennig et al., 2021</xref>). Neural engagement increased abruptly at the beginning, and decreased gradually over the course of learning, where output-null and output-potent components of neural engagement differentially attuned for different targets. Notably, exploiting behavioural perturbations in this study enabled an interactive interrogation of the neural code during learning. Behavioural perturbations, combined with large-scale recording and perturbation of neural activity (<xref ref-type="bibr" rid="bib14">Emiliani et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Yizhar et al., 2011</xref>; <xref ref-type="bibr" rid="bib45">Zhang et al., 2018</xref>), which are more feasible in mice, can pave the way for a more precise (and potentially causal) interrogation of the neural mechanisms underlying representational drift. It would specifically be interesting to see how the bidirectional modulation of activity by behaviour we observed here emerges and which circuit mechanisms (<xref ref-type="bibr" rid="bib8">Cohen-Kashi Malina et al., 2021</xref>; <xref ref-type="bibr" rid="bib15">Ferguson and Cardin, 2020</xref>; <xref ref-type="bibr" rid="bib18">Fu et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Pakan et al., 2016</xref>) contribute to it.</p><p>In summary, our analysis reveals new insights on representational drift from the viewpoint of behaviour. Conceptually, it argues for the primacy of internal parameters (<xref ref-type="bibr" rid="bib6">Buzsáki, 2019</xref>) and suggests that representational similarity could be better understood and characterized in a multidimensional parameter space where the contributions of both external and internal parameters are considered. Computationally, it argues for an independent mixing of stimulus-evoked and behavioural signals rather than a simple gain modulation of sensory inputs by behaviour. Technically, it asks for further controls and analysis of behavioural variability in the characterisation of representational drift. Future studies will hopefully probe the multidimensional code underlying representations in the brain by combining large-scale recordings of neural activity with simultaneous measurement and quantification of behaviour.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Curation and preprocessing of the data</title><sec id="s4-1-1"><title>Data curation</title><p>Publicly available data provided by the Allen Brain Observatory (<xref ref-type="bibr" rid="bib10">de Vries et al., 2020</xref>; <xref ref-type="bibr" rid="bib39">Siegle et al., 2021</xref>) was accessed via AllenSDK (<ext-link ext-link-type="uri" xlink:href="https://allensdk.readthedocs.io">https://allensdk.readthedocs.io</ext-link>). We analysed recording sessions in which neuronal responses to visual stimuli were measured via electrophysiology techniques by Neuropixels probes (<ext-link ext-link-type="uri" xlink:href="https://portal.brain-map.org/explore/circuits/visual-coding-neuropixels">https://portal.brain-map.org/explore/circuits/visual-coding-neuropixels</ext-link>). The data composed of 58 sessions/mice in two separate datasets: brain observatory dataset (dataset 1; n = 32) and functional connectivity (dataset 2; n = 26) (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Similar stimuli (including natural moves and drifting gratings) were shown to the animals, with different length and sequence of presentations in each dataset (<ext-link ext-link-type="uri" xlink:href="https://allensdk.readthedocs.io/en/latest/_static/neuropixels_stimulus_sets.png">https://allensdk.readthedocs.io/en/latest/_static/neuropixels_stimulus_sets.png</ext-link>; see <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3a and b</xref> for illustration of different stimulus sets). We used the spiking activity of units which was already extracted by Kilosort2 (<xref ref-type="bibr" rid="bib40">Stringer et al., 2019</xref>), and we included units in our analysis which passed the default quality criteria. Invalid intervals were treated as not a number (NaN) values. For further details on the preparation of animals, visual stimulation, data acquisition, and default preprocessing of data, see the <ext-link ext-link-type="uri" xlink:href="https://brainmapportal-live-4cc80a57cd6e400d854-f7fdcae.divio-media.net/filer_public/80/75/8075a100-ca64-429a-b39a-569121b612b2/neuropixels_visual_coding_-_white_paper_v10.pdf">Technical White Paper</ext-link> from the Allen Brain Observatory on Neuropixels Visual Coding.</p></sec><sec id="s4-1-2"><title>Preprocessing of data</title><p>For our analysis here, we rendered the spiking activity of units in bins of 1 s. When analysis was focused on specific stimulus types (e.g. presentation of natural movie 1 as in <xref ref-type="fig" rid="fig1">Figure 1d and g</xref>), the activity was rendered from the onset of presentation of each block of the stimulus. When the analysis was across all stimuli and involved the activity during the whole session (e.g. data shown in <xref ref-type="fig" rid="fig1">Figure 1c and f</xref>), the activity was rendered from the beginning of the session or an arbitrary time (e.g. time frames specified in <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). Behavioural information was obtained in similar time frames. Locomotion was quantified for all animals as the average running speed. Level of arousal was quantified by pupil size, as measured by pupil width (whenever pupillometry was available; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><p>To normalize the parameters (e.g. to normalize for different size of pupil across animals), we calculated their z-score values. For parameter <inline-formula><mml:math id="inf36"><mml:mi>x</mml:mi></mml:math></inline-formula> (units’ activity, pupil size, or running speed), it was obtained as <inline-formula><mml:math id="inf37"><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , where <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the mean and standard deviation of <inline-formula><mml:math id="inf40"><mml:mi>x</mml:mi></mml:math></inline-formula> during the entire session or a specified time window.</p></sec></sec><sec id="s4-2"><title>Data analysis</title><sec id="s4-2-1"><title>Representational similarity</title><p>Representational similarity of population activity was quantified by calculating the correlation of responses to different repeats of the same stimulus (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>). Let <inline-formula><mml:math id="inf41"><mml:mi>v</mml:mi></mml:math></inline-formula> be a vector of responses of <inline-formula><mml:math id="inf42"><mml:mi>N</mml:mi></mml:math></inline-formula> recorded units to <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> 1-s-long chunks of a natural movie (the natural movie is broken down to <inline-formula><mml:math id="inf44"><mml:mi>M</mml:mi></mml:math></inline-formula> chunks, or frames, each lasting for 1 s, corresponding to the bin width the neural activity is rendered in). <inline-formula><mml:math id="inf45"><mml:mi>v</mml:mi></mml:math></inline-formula> is a <inline-formula><mml:math id="inf46"><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mi>M</mml:mi></mml:math></inline-formula> population vector composed of the concatenated activity of units (either the actual activity, i.e. average spiking activity, or the z-scored activity of each unit). Denote <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as vectors of responses to two repeats of the same natural movie. Representational similarity is quantified as the Pearson correlation coefficient of these two population vectors:<disp-formula id="equ1"><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>RDI is calculated from representational similarity. Similar to previous metrics (<xref ref-type="bibr" rid="bib28">Marks and Goard, 2021</xref>), we defined RDI between two blocks of stimulus presentation as<disp-formula id="equ2"><mml:math id="m2"><mml:mi>R</mml:mi><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf49"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are, respectively, the average correlation coefficient of population vectors within and between sessions of presentation (see <xref ref-type="fig" rid="fig1s5">Figure 1—figure supplement 5</xref>). <inline-formula><mml:math id="inf51"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was obtained from the average of <inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for all pairs of repeats with <inline-formula><mml:math id="inf53"><mml:mi>i</mml:mi></mml:math></inline-formula>th repeat in the first block and the <inline-formula><mml:math id="inf54"><mml:mi>j</mml:mi></mml:math></inline-formula>th repeat in the second block of presentation. <inline-formula><mml:math id="inf55"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is obtained from the average of <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for all non-identical pairs of repeats where the <inline-formula><mml:math id="inf57"><mml:mi>i</mml:mi></mml:math></inline-formula>th and <inline-formula><mml:math id="inf58"><mml:mi>j</mml:mi></mml:math></inline-formula>th repeats are both within the same block, respectively.</p><p>Note that RDI can also be defined, in principle, for a single pair of movie presentations (corresponding to blocks of presentation with 1 movie in each block). In this case, <inline-formula><mml:math id="inf59"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is 1, by definition, and we therefore have<disp-formula id="equ3"><mml:math id="m3"><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>showing the relation between representational drift and representational similarity in its most simplified case.</p></sec><sec id="s4-2-2"><title>Stimulus reliability</title><p>We also quantified the reliability of how single units respond individually to repetitions of the stimuli (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1d</xref>). To quantify that, we calculated a stimulus reliability metric, which is obtained as the average correlation coefficient of each unit’s activity vector (<inline-formula><mml:math id="inf60"><mml:mi>r</mml:mi></mml:math></inline-formula>) across repetitions of the stimulus (e.g. the natural movie). Let <inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> be the vectors of response of the <inline-formula><mml:math id="inf63"><mml:mi>k</mml:mi></mml:math></inline-formula>th unit to the <inline-formula><mml:math id="inf64"><mml:mi>i</mml:mi></mml:math></inline-formula>th and <inline-formula><mml:math id="inf65"><mml:mi>j</mml:mi></mml:math></inline-formula>th repetitions of the natural movie. Similarity of the unit’s response between these two repeats can be quantified by the Pearson correlation coefficient of the responses as before:<disp-formula id="equ4"><mml:math id="m4"><mml:msubsup><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Stimulus reliability of the unit <inline-formula><mml:math id="inf66"><mml:mi>k</mml:mi></mml:math></inline-formula> is obtained as the average correlation across all pairs of (non-identical) repetitions of the stimulus:<disp-formula id="equ5"><mml:math id="m5"><mml:msup><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf67"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of repetitions of the stimulus. Note that to each single unit we can ascribe a stimulus reliability index since this is calculated from the individual vectors of single units’ responses (<inline-formula><mml:math id="inf68"><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>); on the other hand, representational similarity is calculated from the population vector of responses (<inline-formula><mml:math id="inf69"><mml:mi>v</mml:mi></mml:math></inline-formula>) and indicates a single population metric ascribed to the activity of a group of neurons (e.g. V1 units or all recorded units).</p></sec><sec id="s4-2-3"><title>Behavioural tuning</title><p>To obtain a measure of how single units are modulated by behaviour, we calculated the correlation of units’ responses with behavioural parameter, <inline-formula><mml:math id="inf70"><mml:mi>β</mml:mi></mml:math></inline-formula>:<disp-formula id="equ6"><mml:math id="m6"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf71"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the vector of response of the <inline-formula><mml:math id="inf72"><mml:mi>i</mml:mi></mml:math></inline-formula>th unit, and <inline-formula><mml:math id="inf73"><mml:mi>β</mml:mi></mml:math></inline-formula> is the vector of respective behavioural parameter (either pupil size or running speed) rendered during the same time window and with the same bin width as unit’s activity.</p><p>To obtain a measure of reliability of this modulation by behaviour, we calculated bootstrap correlations. The activity of each unit was shuffled for 100 times and the correlation with behaviour was calculated. The mean (<inline-formula><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mtext>sh</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>) and SD (<inline-formula><mml:math id="inf75"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>sh</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>) of the distribution of shuffled correlations were then used to obtain the z-scored, bootstrapped correlation:<disp-formula id="equ7"><mml:math id="m7"><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>ρ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mtext>sh</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mtext>sh</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf76"><mml:mi>ρ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the unshuffled correlation of the unit with behaviour.</p></sec><sec id="s4-2-4"><title>Principal component analysis</title><p>To analyse the low-dimensional patterns of activity, we performed PC analysis on the vectors of population activity. The vectors of neuronal activity from which response similarity was calculated were of size <inline-formula><mml:math id="inf77"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf78"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of recorded units, <inline-formula><mml:math id="inf79"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of repeats of the stimulus, and <inline-formula><mml:math id="inf80"><mml:mi>M</mml:mi></mml:math></inline-formula> is the length of rendered activity in chunks of 1 s (corresponding to image frames of the natural movie). We concatenated these vectors to obtain a <inline-formula><mml:math id="inf81"><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mi>M</mml:mi></mml:math></inline-formula>, with each row denoting the total activity of each unit to all the repeats. PCs resulting from the PC analysis, therefore, represented a vector of length <inline-formula><mml:math id="inf82"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mi>M</mml:mi></mml:math></inline-formula>, denoting the low-dimensional activity of the population in the same time frame (each data point in <xref ref-type="fig" rid="fig3">Figure 3b and h</xref> corresponding to 1 s of activity). PC loadings (of size <inline-formula><mml:math id="inf83"><mml:mi>N</mml:mi></mml:math></inline-formula> for each PC) are used to represent the individual units in the space of PCs (<xref ref-type="fig" rid="fig3">Figure 3c, f, i and l</xref>).</p></sec></sec><sec id="s4-3"><title>Modelling</title><sec id="s4-3-1"><title>Gain models</title><p>To gain mechanistic insight into the contribution of behavioural changes to modulation of representational similarity, we explored two different models. First, we developed a gain model, in which the integration of the signal and the noise by neurons was differently modulated by behaviour (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref>). For a population of <inline-formula><mml:math id="inf84"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> neurons, let <inline-formula><mml:math id="inf85"><mml:mi>u</mml:mi></mml:math></inline-formula> be the <inline-formula><mml:math id="inf86"><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> vector of responses of neurons upon presentation of a stimulus. This is assumed to be composed of signal (<inline-formula><mml:math id="inf87"><mml:mi>S</mml:mi></mml:math></inline-formula>) and noise (<inline-formula><mml:math id="inf88"><mml:mi>N</mml:mi></mml:math></inline-formula>) components. Change in the behavioural parameters (for instance, pupil size) is supposed to change a gain parameter, <inline-formula><mml:math id="inf89"><mml:mi>g</mml:mi></mml:math></inline-formula>, which in turn differently modulate the signal (<inline-formula><mml:math id="inf90"><mml:mi>S</mml:mi></mml:math></inline-formula>) and noise (<inline-formula><mml:math id="inf91"><mml:mi>N</mml:mi></mml:math></inline-formula>). The vector of population activity, <inline-formula><mml:math id="inf92"><mml:mi>u</mml:mi></mml:math></inline-formula>, is obtained as a linear combination of weighted components by the behavioural/gain parameter. If the signal and the noise are both scaled by the behavioural parameter, it is given as <inline-formula><mml:math id="inf93"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mi>N</mml:mi></mml:math></inline-formula>. If either the noise or the signal is scaled, it is given as <inline-formula><mml:math id="inf94"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mi>N</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf95"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula>, respectively (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>).</p><p><inline-formula><mml:math id="inf96"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf97"><mml:mi>N</mml:mi></mml:math></inline-formula> are both vectors of size <inline-formula><mml:math id="inf98"><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , where each element is drawn from a random uniform distribution between <inline-formula><mml:math id="inf99"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> . The population activity is simulated for <inline-formula><mml:math id="inf100"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> repeats of the stimulus. The stimulus signal, <inline-formula><mml:math id="inf101"><mml:mi>S</mml:mi></mml:math></inline-formula>, remains the same for all the repeats (frozen noise drawn from the same range as before, <inline-formula><mml:math id="inf102"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>), while the noise component, <inline-formula><mml:math id="inf103"><mml:mi>N</mml:mi></mml:math></inline-formula>, is instantiated randomly on each repeat (from the same range, <inline-formula><mml:math id="inf104"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> , as the signal). The behavioural parameter (e.g. pupil size) is assumed to change on every repeat too, which changes the gain parameter, <inline-formula><mml:math id="inf105"><mml:mi>g</mml:mi></mml:math></inline-formula>, as a result. <inline-formula><mml:math id="inf106"><mml:mi>g</mml:mi></mml:math></inline-formula> was therefore assumed to be a random number uniformly drawn from the range <inline-formula><mml:math id="inf107"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0.5,2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> for each repeat. We chose <inline-formula><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math></inline-formula>.</p><p>Representational similarity for different models was calculated, similar to the procedure in analysing the experimental data, as<disp-formula id="equ8"><mml:math id="m8"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf111"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are population responses to the <inline-formula><mml:math id="inf112"><mml:mi>i</mml:mi></mml:math></inline-formula>th and <inline-formula><mml:math id="inf113"><mml:mi>j</mml:mi></mml:math></inline-formula>th repeat of the stimulus, obtained from different gain models. This value is plotted against the relative gain (obtained as the ratio of the gains in the two repeats, <inline-formula><mml:math id="inf114"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf115"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>.</p></sec><sec id="s4-3-2"><title>Extended gain model</title><p>To match better with the experimental data on a single unit level, we extended the previous signal-gain model to have stimulus tuning for individual units (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Whereas before the stimulus was assumed to be a single, fixed value between <inline-formula><mml:math id="inf116"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> for each neuron, now the stimulus itself is extended (corresponding to different frames of the natural movie or different orientations of drifting gratings). The stimulus, <inline-formula><mml:math id="inf117"><mml:mi>s</mml:mi></mml:math></inline-formula>, is assumed to be a vector of fixed random values between [0,1] with size <inline-formula><mml:math id="inf118"><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . Each neuron, <inline-formula><mml:math id="inf119"><mml:mi>k</mml:mi></mml:math></inline-formula>, has a different stimulus drive/tuning, <inline-formula><mml:math id="inf120"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , with which the stimulus vector is multiplied. <inline-formula><mml:math id="inf121"><mml:mi>T</mml:mi></mml:math></inline-formula> is a vector of size <inline-formula><mml:math id="inf122"><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (number of neurons in the population), randomly drawn from <inline-formula><mml:math id="inf123"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> . Response of the <inline-formula><mml:math id="inf124"><mml:mi>k</mml:mi></mml:math></inline-formula>th neuron to each repeat of the stimulus is composed of its stimulus signal (<inline-formula><mml:math id="inf125"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi></mml:math></inline-formula>), which is multiplied by the behavioural gain (<inline-formula><mml:math id="inf126"><mml:mi>g</mml:mi></mml:math></inline-formula>), and an added noise term (<inline-formula><mml:math id="inf127"><mml:mi>N</mml:mi></mml:math></inline-formula>), which is independently drawn for each stimulus and repeat from the range <inline-formula><mml:math id="inf128"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> . <inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf131"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:math></inline-formula>.</p></sec><sec id="s4-3-3"><title>Independent model</title><p>We also developed an alternative model, whereby the effect of behaviour on population responses was modelled as an independent signal (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Here, instead of scaling signal or noise components of the input, behaviour enters as an independent parameter:<disp-formula id="equ9"><mml:math id="m9"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf132"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf133"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are stimulus-evoked and behavioural signals and <inline-formula><mml:math id="inf134"><mml:mi>N</mml:mi></mml:math></inline-formula> is the noise. <inline-formula><mml:math id="inf135"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf136"><mml:mi>N</mml:mi></mml:math></inline-formula> were instantiated as before, while <inline-formula><mml:math id="inf137"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was determined based on two factors. First, the behavioural parameter, <inline-formula><mml:math id="inf138"><mml:mi>β</mml:mi></mml:math></inline-formula>, which was changing on every repeat, and was simulated, similarly as the behavioural gains before, by a random number between <inline-formula><mml:math id="inf139"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0.5,2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> for each repeat. Second, the vector of tuning (<inline-formula><mml:math id="inf140"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) of different neurons in the population with the behavioural parameter, which was modelled as a random number between <inline-formula><mml:math id="inf141"><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> for each neuron. The behavioural signal was obtained as <inline-formula><mml:math id="inf142"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . Representational similarity was computed as before for the population vectors and plotted against the relative behavioural parameters.</p></sec><sec id="s4-3-4"><title>Decoding model</title><p>To directly compare the stimulus-induced information available in different blocks of stimulus presentation, we developed a decoding model (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). A linear decoder is trained on the neural activity (composed of the average activity of units in response to different repeats of the natural movie) during the first block of presentation to discriminate different frames (1 s long) of the natural movie (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, upper). The weights of the readout (W) for each target frame were optimized to maximize its classification (C = 1) against other, non-target frames (C = 0). The decoder is then tested on the data in the second block (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, lower). The population activity in response to each frame (the vector of average responses of neurons to a single frame across different repeats) is passed through the decoder to determine whether it is the target (D = 1) or not (D = 0). Performance of the decoder is quantified by calculating the discriminability (<italic>d′</italic>) as<disp-formula id="equ10"><mml:math id="m10"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf143"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf144"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the average and SD of D for target frame across repetitions (within the second block), and <inline-formula><mml:math id="inf145"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are similar values for non-target frames. The discrimination matrix (<xref ref-type="fig" rid="fig7">Figure 7b and e</xref>) then shows the discriminability (<italic>d</italic>′) of each movie frame as a target when presented against all other frames.</p></sec></sec><sec id="s4-4"><title>Theoretical analysis</title><sec id="s4-4-1"><title>Gain models</title><p>Representational similarity for the responses in the gain models can be calculated as follows.</p><p>In the absence of any scaling of the signal or the noise, <inline-formula><mml:math id="inf147"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula>, the representational similarity is obtained as the correlation coefficient of responses to a pair of stimulus repeats:<disp-formula id="equ11"><mml:math id="m11"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>cov</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf148"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf149"><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . Assuming that <inline-formula><mml:math id="inf150"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf151"><mml:mi>N</mml:mi></mml:math></inline-formula> have zero means, we can write<disp-formula id="equ12"><mml:math id="m12"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf152"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf153"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the std of <inline-formula><mml:math id="inf154"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf155"><mml:mi>N</mml:mi></mml:math></inline-formula>, respectively. This indicates that representational similarity can be expressed as a function of the relative variability of the signal and the noise. If modulation of the responses due to signal is dominant over the noise, <inline-formula><mml:math id="inf156"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf157"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>.</p><p>If both the signal and the noise are scaled by the behavioural parameter, by the gain factor <inline-formula><mml:math id="inf158"><mml:mi>g</mml:mi></mml:math></inline-formula>, as <inline-formula><mml:math id="inf159"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mi>N</mml:mi></mml:math></inline-formula>, we obtain:<disp-formula id="equ13"><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf160"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf161"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the gains in the <inline-formula><mml:math id="inf162"><mml:mi>i</mml:mi></mml:math></inline-formula>th and <inline-formula><mml:math id="inf163"><mml:mi>j</mml:mi></mml:math></inline-formula>th repeat of the stimulus, respectively. Representational similarity, therefore, remains the same under similar scaling of <inline-formula><mml:math id="inf164"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf165"><mml:mi>N</mml:mi></mml:math></inline-formula>.</p><p>If only the noise is scaled by behaviour, we obtain<disp-formula id="equ14"><mml:math id="m14"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msqrt><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:msqrt></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>showing that the larger the gain, the smaller the representational similarity.</p><p>Similarly, if only the signal is scaled, representational similarity can be obtained as follows:<disp-formula id="equ15"><mml:math id="m15"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msqrt><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:msqrt></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>which, if rewritten as<disp-formula id="equ16"><mml:math id="m16"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msqrt><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mtext>/</mml:mtext><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mtext>/</mml:mtext><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:msqrt></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>shows that larger gains effectively decrease the significance of noise, and hence enhance representational similarity. Specifically, in the limit of very large gains for both repetitions (<inline-formula><mml:math id="inf166"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≫</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≫</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>), we have <inline-formula><mml:math id="inf167"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>.</p><p>For the specific case where gains are the same between the two repeats (<inline-formula><mml:math id="inf168"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi></mml:math></inline-formula>), the equation simplifies to<disp-formula id="equ17"><mml:math id="m17"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mtext>/</mml:mtext><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Thus, for similar behavioural states (and hence gains) between the two repeats of the stimulus, representational similarity increases if <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and decreases if <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-4-2"><title>Independent model</title><p>For the model in which the stimulus and the behaviour contributes independently to neural responses, representational similarity in response to the same stimulus can be expressed as<disp-formula id="equ18"><mml:math id="m18"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf171"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf172"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , and <inline-formula><mml:math id="inf173"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denote the variability of the population response induced by stimulus, behaviour, and noise components, respectively. In deriving the above equation, we have assumed that the stimulus and behavioural components of the signal are independent, that is, <inline-formula><mml:math id="inf174"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> (in addition to the noise term being independent of <inline-formula><mml:math id="inf175"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf176"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , respectively). We also assumed that the behavioural signal, <inline-formula><mml:math id="inf177"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:math></inline-formula>, remained the same between the two repeats (i.e. the behavioural parameter was the same: <inline-formula><mml:math id="inf178"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi></mml:math></inline-formula>). If the behavioural parameter changes between the repeats, the equation can, in turn, be written as<disp-formula id="equ19"><mml:math id="m19"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msqrt><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:msqrt></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p>Note that, when representational similarity is only characterized in terms of the stimulus part of the signal, the contribution of behavioural variability is similar to a noise term – decreasing <inline-formula><mml:math id="inf179"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for larger values of <inline-formula><mml:math id="inf180"><mml:mi>β</mml:mi></mml:math></inline-formula>. Changes in the behavioural state cannot, thus, be distinguished from random variability of the ‘signal’.</p></sec><sec id="s4-4-3"><title>Relation between representational similarity and stimulus reliability</title><p>As explained above, representational similarity and stimulus reliability are calculated to quantify the similarity of population and single units’ responses, respectively, to the repeats of the same stimulus. In fact, representational similarity of a population vector composed of one single unit is the same as the stimulus reliability of that unit. Similarly, if all the units in a population of neurons had the same response profile in response to the stimulus, the stimulus reliability of units would be the same as the representational similarity of the population responses. Although these two measures are related (similar to lifetime sparseness and population sparseness <xref ref-type="bibr" rid="bib17">Froudarakis et al., 2014</xref>), they are, however, not always directly equivalent to each other.</p><p>Consider a single unit, <inline-formula><mml:math id="inf181"><mml:mi>k</mml:mi></mml:math></inline-formula>, which has a constant baseline firing rate of <inline-formula><mml:math id="inf182"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and a component which is modulated by the stimulus, <inline-formula><mml:math id="inf183"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> : <inline-formula><mml:math id="inf184"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> . If the stimulus-modulated component of the response is randomly changing between different repeats of the stimulus, the neuron would have a stimulus reliability of zero: <inline-formula><mml:math id="inf185"><mml:msup><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. A population of units with this behaviour would have an average stimulus reliability of zero. However, the representational similarity of the responses of this population is not necessarily zero. In fact, we may obtain high values of population-level representational similarity, if the baseline component of the responses is significantly larger than their modulation (<inline-formula><mml:math id="inf186"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). Under this scenario, representational similarity is calculated from the baseline component of the population responses (<inline-formula><mml:math id="inf187"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), which indeed remains constant across repeats, hence <inline-formula><mml:math id="inf188"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Supervision, Funding acquisition, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Information of recording sessions in different datasets.</title></caption><media xlink:href="elife-77907-supp1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-77907-transrepform1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>Source Code for <xref ref-type="fig" rid="fig1">Figures 1</xref>—<xref ref-type="fig" rid="fig4">4</xref>.</title></caption><media xlink:href="elife-77907-code1-v2.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data needed to evaluate the conclusions in the paper are presented in the paper and/or the Supplementary Materials. Source Data Files have been provided for Figures 1, 2, 4 and 6 (uploaded as Excel files). Analysis code is uploaded as Source Code for Figures 1-4.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank KA Wilmes and DF Tome for their comments on the manuscript, and all the members of the Clopath Lab for insightful discussions. This work was supported by BBSRC BB/N013956/1, BB/N019008/1, Wellcome Trust 200790/Z/16/Z, Wellcome Trust 225412/Z/22/Z, Simons Foundation 564408 and EPSRC EP/R035806/1. All data needed to evaluate the conclusions in the paper are presented in the paper and/or the Supplementary Materials. Codes for reproducing the main analyses are provided.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arieli</surname><given-names>A</given-names></name><name><surname>Sterkin</surname><given-names>A</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Dynamics of ongoing activity: explanation of the large variability in evoked cortical responses</article-title><source>Science</source><volume>273</volume><fpage>1868</fpage><lpage>1871</lpage><pub-id pub-id-type="doi">10.1126/science.273.5283.1868</pub-id><pub-id pub-id-type="pmid">8791593</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aschauer</surname><given-names>DF</given-names></name><name><surname>Eppler</surname><given-names>JB</given-names></name><name><surname>Ewig</surname><given-names>L</given-names></name><name><surname>Chambers</surname><given-names>AR</given-names></name><name><surname>Pokorny</surname><given-names>C</given-names></name><name><surname>Kaschube</surname><given-names>M</given-names></name><name><surname>Rumpel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Learning-induced biases in the ongoing dynamics of sensory representations predict stimulus generalization</article-title><source>Cell Reports</source><volume>38</volume><elocation-id>110340</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.110340</pub-id><pub-id pub-id-type="pmid">35139386</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bimbard</surname><given-names>C</given-names></name><name><surname>Sit</surname><given-names>TP</given-names></name><name><surname>Lebedeva</surname><given-names>A</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Behavioral Origin of Sound-Evoked Activity in Mouse Visual Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.07.01.450721</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyd</surname><given-names>AM</given-names></name><name><surname>Sturgill</surname><given-names>JF</given-names></name><name><surname>Poo</surname><given-names>C</given-names></name><name><surname>Isaacson</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical feedback control of olfactory bulb circuits</article-title><source>Neuron</source><volume>76</volume><fpage>1161</fpage><lpage>1174</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.020</pub-id><pub-id pub-id-type="pmid">23259951</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>MM</given-names></name><name><surname>Miccoli</surname><given-names>L</given-names></name><name><surname>Escrig</surname><given-names>MA</given-names></name><name><surname>Lang</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The pupil as a measure of emotional arousal and autonomic activation</article-title><source>Psychophysiology</source><volume>45</volume><fpage>602</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2008.00654.x</pub-id><pub-id pub-id-type="pmid">18282202</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>The Brain from Inside Out</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oso/9780190905385.001.0001</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name><name><surname>Rose</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Variance and invariance of neuronal long-term representations</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>372</volume><elocation-id>20160161</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0161</pub-id><pub-id pub-id-type="pmid">28093555</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen-Kashi Malina</surname><given-names>K</given-names></name><name><surname>Tsivourakis</surname><given-names>E</given-names></name><name><surname>Kushinsky</surname><given-names>D</given-names></name><name><surname>Apelblat</surname><given-names>D</given-names></name><name><surname>Shtiglitz</surname><given-names>S</given-names></name><name><surname>Zohar</surname><given-names>E</given-names></name><name><surname>Sokoletsky</surname><given-names>M</given-names></name><name><surname>Tasaka</surname><given-names>G-I</given-names></name><name><surname>Mizrahi</surname><given-names>A</given-names></name><name><surname>Lampl</surname><given-names>I</given-names></name><name><surname>Spiegel</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>NDNF interneurons in layer 1 gain-modulate whole cortical columns according to an animal’s behavioral state</article-title><source>Neuron</source><volume>109</volume><fpage>2150</fpage><lpage>2164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.05.001</pub-id><pub-id pub-id-type="pmid">34038743</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowley</surname><given-names>BR</given-names></name><name><surname>Snyder</surname><given-names>AC</given-names></name><name><surname>Acar</surname><given-names>K</given-names></name><name><surname>Williamson</surname><given-names>RC</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Slow drift of neural activity as a signature of impulsivity in macaque visual and prefrontal cortex</article-title><source>Neuron</source><volume>108</volume><fpage>551</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.021</pub-id><pub-id pub-id-type="pmid">32810433</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Lecoq</surname><given-names>JA</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Millman</surname><given-names>D</given-names></name><name><surname>Roll</surname><given-names>K</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Keenan</surname><given-names>T</given-names></name><name><surname>Kuan</surname><given-names>L</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Olsen</surname><given-names>S</given-names></name><name><surname>Thompson</surname><given-names>C</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Barber</surname><given-names>C</given-names></name><name><surname>Berbesque</surname><given-names>N</given-names></name><name><surname>Blanchard</surname><given-names>B</given-names></name><name><surname>Bowles</surname><given-names>N</given-names></name><name><surname>Caldejon</surname><given-names>SD</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Cross</surname><given-names>S</given-names></name><name><surname>Dang</surname><given-names>C</given-names></name><name><surname>Dolbeare</surname><given-names>T</given-names></name><name><surname>Edwards</surname><given-names>M</given-names></name><name><surname>Galbraith</surname><given-names>J</given-names></name><name><surname>Gaudreault</surname><given-names>N</given-names></name><name><surname>Gilbert</surname><given-names>TL</given-names></name><name><surname>Griffin</surname><given-names>F</given-names></name><name><surname>Hargrave</surname><given-names>P</given-names></name><name><surname>Howard</surname><given-names>R</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Jewell</surname><given-names>S</given-names></name><name><surname>Keller</surname><given-names>N</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Larkin</surname><given-names>JD</given-names></name><name><surname>Larsen</surname><given-names>R</given-names></name><name><surname>Lau</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>F</given-names></name><name><surname>Leon</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Long</surname><given-names>F</given-names></name><name><surname>Luviano</surname><given-names>J</given-names></name><name><surname>Mace</surname><given-names>K</given-names></name><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Perkins</surname><given-names>J</given-names></name><name><surname>Robertson</surname><given-names>M</given-names></name><name><surname>Seid</surname><given-names>S</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Shi</surname><given-names>J</given-names></name><name><surname>Sjoquist</surname><given-names>N</given-names></name><name><surname>Slaughterbeck</surname><given-names>C</given-names></name><name><surname>Sullivan</surname><given-names>D</given-names></name><name><surname>Valenza</surname><given-names>R</given-names></name><name><surname>White</surname><given-names>C</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Witten</surname><given-names>DM</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>138</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0550-9</pub-id><pub-id pub-id-type="pmid">31844315</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deitch</surname><given-names>D</given-names></name><name><surname>Rubin</surname><given-names>A</given-names></name><name><surname>Ziv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Representational drift in the mouse visual cortex</article-title><source>Current Biology</source><volume>31</volume><fpage>4327</fpage><lpage>4339</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.07.062</pub-id><pub-id pub-id-type="pmid">34433077</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Ranson</surname><given-names>A</given-names></name><name><surname>Krumin</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Vision and locomotion shape the interactions between neuron types in mouse visual cortex</article-title><source>Neuron</source><volume>98</volume><fpage>602</fpage><lpage>615</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.037</pub-id><pub-id pub-id-type="pmid">29656873</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driscoll</surname><given-names>LN</given-names></name><name><surname>Pettit</surname><given-names>NL</given-names></name><name><surname>Minderer</surname><given-names>M</given-names></name><name><surname>Chettih</surname><given-names>SN</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Dynamic reorganization of neuronal activity patterns in parietal cortex</article-title><source>Cell</source><volume>170</volume><fpage>986</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2017.07.021</pub-id><pub-id pub-id-type="pmid">28823559</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emiliani</surname><given-names>V</given-names></name><name><surname>Cohen</surname><given-names>AE</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>All-optical interrogation of neural circuits</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>13917</fpage><lpage>13926</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2916-15.2015</pub-id><pub-id pub-id-type="pmid">26468193</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferguson</surname><given-names>KA</given-names></name><name><surname>Cardin</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mechanisms underlying gain modulation in the cortex</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>80</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1038/s41583-019-0253-y</pub-id><pub-id pub-id-type="pmid">31911627</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Chiu</surname><given-names>C</given-names></name><name><surname>Weliky</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Small modulation of ongoing cortical dynamics by sensory input during natural vision</article-title><source>Nature</source><volume>431</volume><fpage>573</fpage><lpage>578</lpage><pub-id pub-id-type="doi">10.1038/nature02907</pub-id><pub-id pub-id-type="pmid">15457262</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Yatsenko</surname><given-names>D</given-names></name><name><surname>Saggau</surname><given-names>P</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Population code in mouse V1 facilitates readout of natural scenes through increased sparseness</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>851</fpage><lpage>857</lpage><pub-id pub-id-type="doi">10.1038/nn.3707</pub-id><pub-id pub-id-type="pmid">24747577</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Y</given-names></name><name><surname>Tucciarone</surname><given-names>JM</given-names></name><name><surname>Espinosa</surname><given-names>JS</given-names></name><name><surname>Sheng</surname><given-names>N</given-names></name><name><surname>Darcy</surname><given-names>DP</given-names></name><name><surname>Nicoll</surname><given-names>RA</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A cortical circuit for gain control by behavioral state</article-title><source>Cell</source><volume>156</volume><fpage>1139</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.01.050</pub-id><pub-id pub-id-type="pmid">24630718</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia Del Molino</surname><given-names>LC</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Mejias</surname><given-names>JF</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Paradoxical response reversal of top-down modulation in cortical circuits with three interneuron types</article-title><source>eLife</source><volume>6</volume><elocation-id>e29742</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.29742</pub-id><pub-id pub-id-type="pmid">29256863</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez</surname><given-names>WG</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Harutyunyan</surname><given-names>A</given-names></name><name><surname>Lois</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Persistence of neuronal representations through time and damage in the hippocampus</article-title><source>Science</source><volume>365</volume><fpage>821</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1126/science.aav9199</pub-id><pub-id pub-id-type="pmid">31439798</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennig</surname><given-names>JA</given-names></name><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Bahureksa</surname><given-names>LA</given-names></name><name><surname>Sadtler</surname><given-names>PT</given-names></name><name><surname>Quick</surname><given-names>KM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning is shaped by abrupt changes in neural engagement</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>727</fpage><lpage>736</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00822-8</pub-id><pub-id pub-id-type="pmid">33782622</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>S</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Pupil size as a window on neural substrates of cognition</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>466</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.03.005</pub-id><pub-id pub-id-type="pmid">32331857</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Barbarits</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Aydın</surname><given-names>Ç</given-names></name><name><surname>Barbic</surname><given-names>M</given-names></name><name><surname>Blanche</surname><given-names>TJ</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Dutta</surname><given-names>B</given-names></name><name><surname>Gratiy</surname><given-names>SL</given-names></name><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lopez</surname><given-names>CM</given-names></name><name><surname>Mitelut</surname><given-names>C</given-names></name><name><surname>Musa</surname><given-names>S</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Putzeys</surname><given-names>J</given-names></name><name><surname>Rich</surname><given-names>PD</given-names></name><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>W-L</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kenet</surname><given-names>T</given-names></name><name><surname>Bibitchkov</surname><given-names>D</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name><name><surname>Arieli</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Spontaneously emerging cortical representations of visual attributes</article-title><source>Nature</source><volume>425</volume><fpage>954</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1038/nature02078</pub-id><pub-id pub-id-type="pmid">14586468</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lütcke</surname><given-names>H</given-names></name><name><surname>Margolis</surname><given-names>DJ</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Steady or changing? long-term monitoring of neuronal population activity</article-title><source>Trends in Neurosciences</source><volume>36</volume><fpage>375</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2013.03.008</pub-id><pub-id pub-id-type="pmid">23608298</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markopoulos</surname><given-names>F</given-names></name><name><surname>Rokni</surname><given-names>D</given-names></name><name><surname>Gire</surname><given-names>DH</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Functional properties of cortical feedback projections to the olfactory bulb</article-title><source>Neuron</source><volume>76</volume><fpage>1175</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.028</pub-id><pub-id pub-id-type="pmid">23259952</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Marks</surname><given-names>TD</given-names></name><name><surname>Goard</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Stimulus-Dependent Representational Drift in Primary Visual Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.12.10.420620</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marks</surname><given-names>TD</given-names></name><name><surname>Goard</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Stimulus-dependent representational drift in primary visual cortex</article-title><source>Nature Communications</source><volume>12</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1038/s41467-021-25436-3</pub-id><pub-id pub-id-type="pmid">34453051</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montijn</surname><given-names>JS</given-names></name><name><surname>Meijer</surname><given-names>GT</given-names></name><name><surname>Lansink</surname><given-names>CS</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Population-level neural codes are robust to single-neuron variability from a multidimensional coding perspective</article-title><source>Cell Reports</source><volume>16</volume><fpage>2486</fpage><lpage>2498</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.07.065</pub-id><pub-id pub-id-type="pmid">27545876</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Highly selective receptive fields in mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>7520</fpage><lpage>7536</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0623-08.2008</pub-id><pub-id pub-id-type="pmid">18650330</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nilchian</surname><given-names>P</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>Sanders</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Animal-to-animal variability in partial hippocampal remapping in repeated environments</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>5268</fpage><lpage>5280</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3221-20.2022</pub-id><pub-id pub-id-type="pmid">35641190</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pakan</surname><given-names>JMP</given-names></name><name><surname>Lowe</surname><given-names>SC</given-names></name><name><surname>Dylda</surname><given-names>E</given-names></name><name><surname>Keemink</surname><given-names>SW</given-names></name><name><surname>Currie</surname><given-names>SP</given-names></name><name><surname>Coutts</surname><given-names>CA</given-names></name><name><surname>Rochefort</surname><given-names>NL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Behavioral-state modulation of inhibition is context-dependent and cell type specific in mouse visual cortex</article-title><source>eLife</source><volume>5</volume><elocation-id>e14985</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.14985</pub-id><pub-id pub-id-type="pmid">27552056</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>A</given-names></name><name><surname>Geva</surname><given-names>N</given-names></name><name><surname>Sheintuch</surname><given-names>L</given-names></name><name><surname>Ziv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hippocampal ensemble dynamics timestamp events in long-term memory</article-title><source>eLife</source><volume>4</volume><elocation-id>e12247</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.12247</pub-id><pub-id pub-id-type="pmid">26682652</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rule</surname><given-names>ME</given-names></name><name><surname>O’Leary</surname><given-names>T</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Causes and consequences of representational drift</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>141</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.08.005</pub-id><pub-id pub-id-type="pmid">31569062</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Ayaz</surname><given-names>A</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Integration of visual motion and locomotion in mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1864</fpage><lpage>1869</lpage><pub-id pub-id-type="doi">10.1038/nn.3567</pub-id><pub-id pub-id-type="pmid">24185423</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoonover</surname><given-names>CE</given-names></name><name><surname>Ohashi</surname><given-names>SN</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Fink</surname><given-names>AJP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Representational drift in primary olfactory cortex</article-title><source>Nature</source><volume>594</volume><fpage>541</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03628-7</pub-id><pub-id pub-id-type="pmid">34108681</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Durand</surname><given-names>S</given-names></name><name><surname>Gale</surname><given-names>S</given-names></name><name><surname>Bennett</surname><given-names>C</given-names></name><name><surname>Graddis</surname><given-names>N</given-names></name><name><surname>Heller</surname><given-names>G</given-names></name><name><surname>Ramirez</surname><given-names>TK</given-names></name><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Luviano</surname><given-names>JA</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ahmed</surname><given-names>R</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Brown</surname><given-names>D</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Caldejon</surname><given-names>S</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Chvilicek</surname><given-names>M</given-names></name><name><surname>Cox</surname><given-names>TC</given-names></name><name><surname>Dai</surname><given-names>K</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Dietzman</surname><given-names>R</given-names></name><name><surname>Esposito</surname><given-names>L</given-names></name><name><surname>Farrell</surname><given-names>C</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Galbraith</surname><given-names>J</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Gelfand</surname><given-names>EC</given-names></name><name><surname>Hancock</surname><given-names>N</given-names></name><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Howard</surname><given-names>R</given-names></name><name><surname>Hu</surname><given-names>B</given-names></name><name><surname>Hytnen</surname><given-names>R</given-names></name><name><surname>Iyer</surname><given-names>R</given-names></name><name><surname>Jessett</surname><given-names>E</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><name><surname>Kato</surname><given-names>I</given-names></name><name><surname>Kiggins</surname><given-names>J</given-names></name><name><surname>Lambert</surname><given-names>S</given-names></name><name><surname>Lecoq</surname><given-names>J</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Leon</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Liang</surname><given-names>E</given-names></name><name><surname>Long</surname><given-names>F</given-names></name><name><surname>Mace</surname><given-names>K</given-names></name><name><surname>Melchior</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>D</given-names></name><name><surname>Mollenkopf</surname><given-names>T</given-names></name><name><surname>Nayan</surname><given-names>C</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Ngo</surname><given-names>K</given-names></name><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Nicovich</surname><given-names>PR</given-names></name><name><surname>North</surname><given-names>K</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Ollerenshaw</surname><given-names>D</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Perkins</surname><given-names>J</given-names></name><name><surname>Reding</surname><given-names>M</given-names></name><name><surname>Reid</surname><given-names>D</given-names></name><name><surname>Robertson</surname><given-names>M</given-names></name><name><surname>Ronellenfitch</surname><given-names>K</given-names></name><name><surname>Seid</surname><given-names>S</given-names></name><name><surname>Slaughterbeck</surname><given-names>C</given-names></name><name><surname>Stoecklin</surname><given-names>M</given-names></name><name><surname>Sullivan</surname><given-names>D</given-names></name><name><surname>Sutton</surname><given-names>B</given-names></name><name><surname>Swapp</surname><given-names>J</given-names></name><name><surname>Thompson</surname><given-names>C</given-names></name><name><surname>Turner</surname><given-names>K</given-names></name><name><surname>Wakeman</surname><given-names>W</given-names></name><name><surname>Whitesell</surname><given-names>JD</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Williford</surname><given-names>A</given-names></name><name><surname>Young</surname><given-names>R</given-names></name><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Naylor</surname><given-names>S</given-names></name><name><surname>Phillips</surname><given-names>JW</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Survey of spiking in the mouse visual system reveals functional hierarchy</article-title><source>Nature</source><volume>592</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-03171-x</pub-id><pub-id pub-id-type="pmid">33473216</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id><pub-id pub-id-type="pmid">31000656</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Kenet</surname><given-names>T</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name><name><surname>Arieli</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Linking spontaneous activity of single cortical neurons and the underlying functional architecture</article-title><source>Science</source><volume>286</volume><fpage>1943</fpage><lpage>1946</lpage><pub-id pub-id-type="doi">10.1126/science.286.5446.1943</pub-id><pub-id pub-id-type="pmid">10583955</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>Batista-Brito</surname><given-names>R</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Cardin</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Arousal and locomotion make distinct contributions to cortical activity patterns and visual encoding</article-title><source>Neuron</source><volume>86</volume><fpage>740</fpage><lpage>754</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.028</pub-id><pub-id pub-id-type="pmid">25892300</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yizhar</surname><given-names>O</given-names></name><name><surname>Fenno</surname><given-names>LE</given-names></name><name><surname>Davidson</surname><given-names>TJ</given-names></name><name><surname>Mogri</surname><given-names>M</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Optogenetics in neural systems</article-title><source>Neuron</source><volume>71</volume><fpage>9</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.004</pub-id><pub-id pub-id-type="pmid">21745635</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshida</surname><given-names>T</given-names></name><name><surname>Ohki</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Natural images are reliably represented by sparse and variable populations of neurons in visual cortex</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>872</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-14645-x</pub-id><pub-id pub-id-type="pmid">32054847</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Russell</surname><given-names>LE</given-names></name><name><surname>Packer</surname><given-names>AM</given-names></name><name><surname>Gauld</surname><given-names>OM</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Closed-loop all-optical interrogation of neural circuits in vivo</article-title><source>Nature Methods</source><volume>15</volume><fpage>1037</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0183-z</pub-id><pub-id pub-id-type="pmid">30420686</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziv</surname><given-names>Y</given-names></name><name><surname>Burns</surname><given-names>LD</given-names></name><name><surname>Cocker</surname><given-names>ED</given-names></name><name><surname>Hamel</surname><given-names>EO</given-names></name><name><surname>Ghosh</surname><given-names>KK</given-names></name><name><surname>Kitch</surname><given-names>LJ</given-names></name><name><surname>El Gamal</surname><given-names>A</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Long-term dynamics of CA1 hippocampal place codes</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>264</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nn.3329</pub-id><pub-id pub-id-type="pmid">23396101</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77907.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.01.02.474731" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.02.474731"/></front-stub><body><p>This work builds on rapidly accumulating evidence for the importance of measuring and accounting for behaviour in neural data, and will be of interest to a broad neuroscience audience. Analyses of Allen Brain Atlas datasets show that sensory representations change and match up reliably with behavioural state. The article's main conclusions are supported by the data and analyses, and the work raises important questions about previous accounts of the sources of representational drift in sensory areas of the brain.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77907.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Palmer</surname><given-names>Stephanie E</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Ziv</surname><given-names>Yaniv</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0316ej306</institution-id><institution>Weizmann Institute of Science</institution></institution-wrap><country>Israel</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.01.02.474731">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.01.02.474731v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Contribution of behavioural variability to representational drift&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Yaniv Ziv (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The reviewers all enjoyed the paper and feel that the work is timely and well-executed. However, some common questions and concerns arose in the reviews which were discussed by the group. These are the essential revisions for the manuscript. Please also see the detailed comments from each reviewer below, which should improve the overall impact of the manuscript.</p><p>(1) Statistical analyses should not be limited to Pearson correlation and should include more sophisticated metrics, given the clear non-monotonic relationships in the data.</p><p>(2) Clarify the terms used that can seem at odds or confounding. Example: &quot;representational drift&quot; and &quot;representational similarity&quot; shouldn't be used interchangeably. For example, it may also be useful to provide working definitions of (and differences between) &quot;drift of behavioral state&quot;, &quot;behavioral drift&quot; and &quot;behavior&quot; at the beginning of the manuscript.</p><p>(3) Related to point 2, it is crucial to explain the stance taken in the paper on how behavior and elapsed time are to be separated. Please see the relevant major prompts from Reviewers 2 and 3 who have suggested potential new analyses that would make this distinction most clear.</p><p>(4) The decoding results as currently reported seem obvious and underdeveloped. To make this have a bit more power, this should be expanded to include a new calculation, perhaps on decoding behavioral state along with the stimulus.</p><p>(5) Improve the figure layout to make the work more engaging to a broad readership. A diagram in Figure 1 would be useful.</p><p>(6) Show how these results depend on other behavioral parameters measured in these datasets, such as pupil location.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Overall the work is careful, and the presentation of the results is thorough. Some questions and suggestions for revision remain, however:</p><p>(1) The decoding results seem obvious – if the downstream decoder does not &quot;know&quot; about the behavioral state, then the neurons that are shifted by the behavioral state should be weighted less. Is there anything more meaningful or surprising that can be calculated here? What if the behavioral state is also read out from the population? Does the reliable shift in representation by behavior have any particular illusory effect, meaning does this create errors in a particular direction in visual space that could serve some functional purpose?</p><p>(2) What should one expect from this kind of analysis if a sensory task is novel or otherwise engages behavioral states more explicitly? Could the work be extended to comment on how to disentangle slow learning signals from the low-D modes that were found to be engaged by behavioral shifts?</p><p>(3) The figures, especially Figure 1, are dense and somewhat unintuitive. It would be nice to see a diagram of the analysis and main results first. This could engage a wider readership. Throughout the figures, it feels like there might be more visually impactful ways of representing the results. Instead, the figures feel repetitive, even though they are highlighting different effects and analyses.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. Contribution of behavior vs. elapsed time: Given that it is already well-established that behavioral variability contributes to variability in neuronal responses (e.g. Niell and Stryker, Neuron 2010; Polack et al., Nat. Neurosci. 2013; Vinck et al., Neuron 2015; Musall et al., Nat. Neurosci. 2019; Stringer et al., Science 2019), it is not clear if the authors try to make the case that drift is an artifact of behavioral variability – i.e. drift is not a genuine phenomenon, it's all behavioral – as the abstract and main text strongly suggests by using the words &quot;mistaken&quot; and &quot;misinterpreted&quot;; or that behavior and time can both contribute to the observed changes in representations. These are two different messages that require a different set of analyses and will likely impact the field very differently, so it is crucial to clarify this point.</p><p>The authors state that &quot;our results suggest that a significant fraction of what has been described as representational drift in a sensory cortex can be attributed to behavioral variability&quot;. However, quantifying the contribution of behavioral variability to representational drift (as the title of the paper claims) requires analysis that controls for the effects of the passage of time, and this was not done in this paper.</p><p>It is hard to conclude that both time and behavior modulation contribute differently to representational similarity of different presentations of the same video as the two analyzed video blocks are both well separated by time and differ in terms of the animal's behavior (quantified by pupil width and running speed). This will result in a correlation between time and behavioral changes that preclude the possibility to differentiate between the unique contribution of time and behavior to representational similarity.</p><p>One way to quantify the contribution of behavioral variability to representational similarity, is to repeat the analyses of Figure 1 and Figure 4, – i.e. the calculation of the correlation between the absolute change in the behavioral variables and the values of the chosen metric for representational similarity, while holding the interval between video repeats as a constant (subsequent video repeats within the same blocks or video repeats with the same absolute interval between them in general).</p><p>Alternatively, the authors can calculate and report the correlation between the change in behavior and representational similarity either between video repeats within or between video repeats across blocks. To control for the effects of time, the authors should separately calculate the linear relationship between the variables using only the light and dark gray data points (within block), and only the orange data points (between blocks), instead of the correlation indicated in red which uses the entire dataset. Additionally, the authors should report the statistics of these correlations (the p-value and the number of samples) in the corresponding figure legend.</p><p>The authors are suggesting that changes in representational similarity that were previously attributed to time are in fact only changes in the animal behavior over time. This could be tested using a multivariate regression analysis within each individual animal and quantifying the unique contribution of each variable (e.g., time, pupil size, running speed, etc.) as well as testing the significance of each variable to the fitted model, e.g. using GLM (see Driscroll et al., Cell 2017; Musall et al., Nat. Neurosci. 2019 for a similar approach).</p><p>Furthermore, if the authors are suggesting that representational drift merely reflects gradual changes in behavior, then it would be convincing to show that when the same analyses (e.g. multiple regression) are performed on a different set of stimuli with less abrupt changes in the behavioral state of the animals then there is no significant decrease in representational similarity. To do that, the authors can try to compare the similarity between the representations (for both population-vector correlations and setpoint similarity) across blocks in which the behavior didn't change significantly, and show there is no significant decrease in representational similarity. For example, the authors report in Figure 6 that there is less variability in behavior across blocks of drifting gratings and that there is no consistent and significant change in the behavior across the population of mice. Therefore, one way to test if there is higher representational similarity within blocks compared with across blocks (i.e., representational drift) without changes in behavior is to select mice that didn't exhibit behavioral changes across blocks and perform the analyses on them. This should also be done on the two blocks of 'Natural video 3', which are more proximal in time and therefore are likely to be more similar in behavior.</p><p>2. Cases of non-linear/monotonic relationship between behavioral changes and representational similarity and inappropriate use of correlation: In three of the main figures (specifically: Figure 1B, C, F, G, Figure 3C, I, and Figure 4C, D, G, H) the authors are using Pearson's correlation to quantify the relationship between the absolute change in pupil width between two video repeats and the representational similarity between the same repeats. In many of the plots, the data should not be fitted using Pearson's correlation since some of the assumptions of this model are not met. The most concerning issue is that the relationship between the two variables (change in behavior and representational similarity) does not always follow a linear or even a monotonic trend. This suggests that the relationship between the changes in pupil width and representational similarity are not correctly captured using a univariate linear model (e.g., Pearson's or Spearman's correlations). Additionally, in many of the plots, the data points fall into two or more dense clusters. This can lead to the false impression that there is a strong monotonic relationship between the two variables, even though there is a weak (or even opposite) relationship within each cluster (e.g., as in Figure 3 C, I and Figure 4G) (see an example in Aggarwal and Ranganathan, Perspect Clin Res 2016). This is a crucial point since the clusters of data points most likely represent different blocks that occurred at different times. Likewise, in Figure 5A, C, D, F right panels (and Extended Data Figure 10B, F), excluding the middle-range data points – which are the majority of data points – is unjustified and the use of Pearson's correlation in this case is inappropriate and misleading.</p><p>3. Stimulus dependent representational drift: A key statement of the current manuscript is that slow and gradual changes in behavior may change the setpoints (activity rates) of different neurons over time, leading to the appearance of a gradual decrease in representational similarity metrics such as 'population vector correlation' even in brain areas that do not, (literary) represent (or encode) the presented stimuli. This point was raised both as a critical evaluation of the representational similarity metrics chosen in the past to characterize the stability of visual representations or as a criticism of the use of the term 'representational drift'.</p><p>The fact that there are changes that are not purely related to the tuning of cells is not new and was demonstrated in several previous studies on coding stability. For instance, Ziv et al., Nature Neurosci. 2013 and Gonzales et al., Science 2019, have shown that place cells in the mouse hippocampal CA1 can drop in and out of the ensemble during different visits to the same familiar environment over weeks, leading to a gradual change in the population of place cells that encode the location of the animal during the task. These changes, which reflect changes in activity rates, were independent of the position the neurons encode and were found in place cells and non-place cells alike. Likewise, Driscoll et al., Cell 2017 and Aschauer et al., Cell Reports 2022, showed a similar turnover of the active cells in the cortex. Notably, Rubin et al., <italic>eLife</italic> 2015, showed that hippocampal representations of two distinct environments (which had very distinct place-cell representations) co-evolve over timescales of days-weeks. This shared component of the drift stems from gradual context-independent changes in activity rates. In fact, the prevalent use of the term representational drift (coined by Rule et al., Curr Op in Neurobiol 2019) is based on these above-mentioned studies and served to capture the entire range of gradual changes in neuronal activity over time. More recently, Schoonover et al., Nature 2021, and Deitch et al., Current Biology 2021 separately analyzed changes in activity rates and changes in tuning, and showed gradual changes in the cells' activity rates during periods of spontaneous activity, explicitly stating that these changes can occur independently of the presented stimulus.</p><p>Crucially, cells' activity rates are very relevant to stimulus encoding, as cells exhibit 'rate remapping' between environments or stimuli. Thus, equating stimulus-independent changes and changes in activity rates (setpoint similarity) is problematic because part of the changes in activity rates can be stimulus-dependent. This can be seen even in the visual cortex by calculating the setpoint similarity between different stimuli (e.g. Deitch et al., 2021 Figure S3H): the average activity rates of blocks of the same stimuli are more similar than between blocks of different stimuli. Thus, code stability is a function of two key factors: (1) stability in tuning and (2) stability in activity rates. Deitch et al., 2021 showed that the gradual changes over time that occur with respect to these factors are nearly independent of each other.</p><p>Furthermore, the fact that there are changes in setpoint similarity in CA1, although this area doesn't reliably encode visual stimuli, cannot, in itself, be used as an argument for the role of behavioral changes in representational drift since these changes can also be associated with elapsed time (see our point #1 above).</p><p>Overall, we agree that it is important to carefully dissociate between the effects of behavior on changes in neuronal activity that are stimulus-dependent or independent, but we feel that the criticism raised by the authors ignores the findings of the relevant literature, which (1) did not purely attribute the observed changes to the sensory component, and (2) did dissociate between stimulus-dependent changes (in tuning) and off-context/stimulus-independent changes (in activity rates).</p><p>We propose that the authors tone down their interpretations throughout the paper, and especially in the discussion: e.g., &quot;changes in representational similarity (i.e. representational drift) can arise from changes in both sources, and hence attributing it purely to the drift of the sensory component might be inaccurate&quot; and &quot;Drawing further conclusions about stimulus-dependences of representational drift in visual cortex – and other sensory cortices – thus needs a critical evaluation by teasing apart the contribution of different components (stimulus-induced and stimulus-independent)&quot;.</p><p>4. The use of the term &quot;Representational similarity&quot; versus &quot;Representational drift&quot;:</p><p>It is important that the authors explicitly define the term representational drift and edit the paper in a way that uses the terms &quot;representational similarity&quot; and &quot;representational drift&quot; in a consistent way throughout the manuscript. Most studies have demonstrated drift (even if not using the term 'drift') as a decreasing similarity between neuronal responses to the same stimulus/task as a function of the time interval between experiences/stimulus presentations under the same experimental conditions (Ziv et al., Nat. Neurosci. 2013; Lee et al., Cell 2020, Driscoll et al., Cell 2017; Rule et al., 2019; Schoonover et al., 2021; Deitch et al., 2021; Marks and Goard et al., Nat. Comm. 2021; Jensen et al., BioRxiv 2021 and Aschauer et al., Cell Reports 2022). This point regarding the differences between drift and variability in neuronal responses is nicely illustrated and discussed in a recent review paper (Clopath et al., Philos Trans, 2017). However, throughout the current manuscript, the authors refer to any change in representational similarity as representational drift and use these terms interchangeably regardless of the interval between the compared timepoints. For example:</p><p>&quot;…Importantly, changes in representational similarity (i.e. representational drift)…&quot;</p><p>In most of the above-mentioned studies about drift, the behavior or performance of the animal was at a steady state throughout the examined time intervals, suggesting that the observed changes in neuronal activity are not due to gradual changes in the behavior (e.g., due to learning, habituation, or changes in arousal). Thus, while the behavior itself may vary across different time points, as long as it is not changing gradually throughout the experiment, it should not lead to the appearance of drift.</p><p>To determine whether neuronal representations are gradually changing, there must be at least three (not necessarily equally spaced) different compared time points (see Clopath et al; 2017). We suggest adding a paragraph that explicitly explains the difference between neuronal variability and drift, how to differentiate between the two cases, and including an additional time point in the illustration presented in Extended Data Figure 1A (which now only includes two times points).</p><p>5. Focusing on reliable units improves time-lapse decoding: The analysis presented in Figure 7 shows that using reliable units (i.e., units that don't show changes in their tuning over time) results in higher decoding accuracy (i.e. more stable population code). Given that stability at the single-cell level should directly contribute to stability at the population level, this analysis is circular and therefore the conclusion that &quot;Decoding generalizability of natural images improves by focusing on reliable units&quot; is trivial.</p><p>Irrespective of this issue, we agree that it is a reasonable idea that reliable units could serve as a stable core for the brain to rely on for coping with changing neuronal responses. However, the distribution of stimulus reliability is not bi-modal (as shown in Figure 2H), but actually skewed towards lower reliability values. Thus, it is unclear how focusing on a small, unrepresentative subset of reliable units informs us how the brain copes with changing representations.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>– In the introduction it may be useful to provide working definitions of (and differences between) &quot;drift of behavioural state&quot;, &quot;behavioural drift&quot; and &quot;behaviour&quot;.</p><p>– To rule out any potential artifact resulting from bin width choice being correlated with behavioral timescale, it would be useful to see the effect of varying bin width in computing population vectors.</p><p>– The Siegle et al. dataset methods imply that pupil position is available as well; thus do the same results apply if using position in addition to diameter? It would be nice to mention if any other behavioural measures are available that were not analyzed, as ignoring these seemed to lead previous accounts of drift astray.</p><p>– Pg. 5 bottom paragraph: &quot;Inclusion of multiple cell types…to control for this…&quot; – this control actually seems to be for possible strain differences; it is not clear in the Siegle at al. dataset how many cell types were presented (i.e. which cells were opto-tagged), so this information should be present if discussed.</p><p>– In the discussion of Figure 6, there is a nod to the expected results if pupil diameter correlation was used as a separate measure of behavioural tuning, which references Figure 6 b,e (pg. 13..should be Figure 5b,e?).</p><p>– Pg.19 &quot;in line with previous reports&quot; should be &quot;in line with a previous report&quot; unless more citations are provided.</p><p>– Pg. 20 &quot;with a more decrease in pupil size&quot; should be &quot;with larger decreases in pupil size&quot; or similar.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.77907.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers all enjoyed the paper and feel that the work is timely and well-executed. However, some common questions and concerns arose in the reviews which were discussed by the group. These are the essential revisions for the manuscript. Please also see the detailed comments from each reviewer below, which should improve the overall impact of the manuscript.</p><p>(1) Statistical analyses should not be limited to Pearson correlation and should include more sophisticated metrics, given the clear non-monotonic relationships in the data.</p></disp-quote><p>That is an important point and we tried to address it by performing more sophisticated analyses and by developing new metrics, documented in multiple new figures (Figure 1—figure supplement 5; Figure 3—figure supplement 2; Figure 4—figure supplement 2; Figure 5). Specifically, we analyzed the source of the non-monotonic relationship (mentioned in Figure 4) in Figure 4—figure supplement 2 (see below for details). We also showed that the passage of time and the potential clustering arising from it is not a confound for our correlation analysis (see responses to point 3 below). In general, we took the approach of investigating the source of non-monotonicity by evaluating the contribution of different factors including various aspects of behavior and controlling for others like the passage of time. Importantly, by performing new analyses, we have now shown that the result of the linear correlation analysis is not compromised by the non-monotonic trend.</p><disp-quote content-type="editor-comment"><p>(2) Clarify the terms used that can seem at odds or confounding. Example: &quot;representational drift&quot; and &quot;representational similarity&quot; shouldn't be used interchangeably. For example, it may also be useful to provide working definitions of (and differences between) &quot;drift of behavioral state&quot;, &quot;behavioral drift&quot; and &quot;behavior&quot; at the beginning of the manuscript.</p></disp-quote><p>Many thanks for this point. We clarified the definitions upfront, and explained the difference between different terms at the beginning of the Results section. We also clarified them in the Methods, in the figures and the captions<italic>.</italic></p><disp-quote content-type="editor-comment"><p>(3) Related to point 2, it is crucial to explain the stance taken in the paper on how behavior and elapsed time are to be separated. Please see the relevant major prompts from Reviewers 2 and 3 who have suggested potential new analyses that would make this distinction most clear.</p></disp-quote><p>This is an important point. To elucidate that, we performed new analyses to show that elapsed time between the two blocks of presentations is not the major contributing factor to representational drift (Figure 1—figure supplement 5; see also Figure 3—figure supplement 2). More details are provided in response to the specific points raised by the reviewers below.</p><disp-quote content-type="editor-comment"><p>(4) The decoding results as currently reported seem obvious and underdeveloped. To make this have a bit more power, this should be expanded to include a new calculation, perhaps on decoding behavioral state along with the stimulus.</p></disp-quote><p>The suggestion of extending decoding to behavior is really interesting, and in fact it helped to further consolidate the main messages of the paper. To that end, we extended our decoding framework to predict behavior from neural activity (Figure 7—figure supplement 2). We found that, first, the behavioral state of the animal could reliably be predicted from the neural activity. This is consistent with our previous results on reliable modulation of neuronal activity by behavior (Figure 5). Second, we found that behavioral decoding did not create a significant bias in sensory inference, which supported our previous results on independent modulation of neuronal responses by behavior and sensory stimulus.</p><disp-quote content-type="editor-comment"><p>(5) Improve the figure layout to make the work more engaging to a broad readership. A diagram in Figure 1 would be useful.</p></disp-quote><p>We added diagrams at the beginning of Figure 1 to explain the main idea better, enhanced the illustration of the modelling figure (Figure 2), and generally tried to make other figures more engaging and intuitive.</p><disp-quote content-type="editor-comment"><p>(6) Show how these results depend on other behavioral parameters measured in these datasets, such as pupil location.</p></disp-quote><p>We analyzed and documented the dependence of setpoint similarity on pupil location (along with changes in average pupil location in each video presentation and the relationship of pupil location and pupil width) in two new figures (Figure 4—figure supplement 3,4). This information was also used, and proven to be helpful, in another analysis (see Figure 4—figure supplement 2).</p><p>Another parameter was pupil height, but this was highly correlated with pupil width, and we therefore decided to use one as a proxy for pupil size.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Overall the work is careful, and the presentation of the results is thorough. Some questions and suggestions for revision remain, however:</p><p>(1) The decoding results seem obvious – if the downstream decoder does not &quot;know&quot; about the behavioral state, then the neurons that are shifted by the behavioral state should be weighted less. Is there anything more meaningful or surprising that can be calculated here? What if the behavioral state is also read out from the population? Does the reliable shift in representation by behavior have any particular illusory effect, meaning does this create errors in a particular direction in visual space that could serve some functional purpose?</p></disp-quote><p>We agree that the decoding results were rather intuitive. The decoding framework was originally used to corroborate the main findings of our previous analyses (especially with regard to the relation between the modulation of units by stimulus and behavior), and to show how reliable sensory decoding can be possible in the face of behavioral variability.</p><p>The reviewer raises an interesting point as to how the decoding of behavior might work in tandem with sensory processing, and how this may affect sensory inference. To explore this question, we analyzed how different behavioral parameters (pupil width and running speed) can be decoded from the population activity (Figure 7—figure supplement 2).</p><p>First, we found that if the decoder is trained on stimulus representations that have enough behavioral variability, it can predict the behavior very well in the test sessions. This was consistent with our results on reliable modulation of neuronal activity by behavior. Second, we asked whether the decoding of behavior creates any systematic bias in sensory inference. If the behavioral modulation has a systematic relation with the modulation of neuronal activity with stimulus, a decoder which is predicting the animals’ behavior might be biased to infer illusionary stimuli due to the correlations between behavior and stimulus. To test this, we weighted the responses of neurons to each video presentation with the weights optimized for behavioral decoding. We then calculated the correlation of the readout response across multiple repeats of the natural video. We found very small correlations between repeats, indicating that behavioral decoding would not create illusionary cross-talks with the stimulus space. We think this is also consistent with our previous results on the independent modulation of activity by behavior.</p><disp-quote content-type="editor-comment"><p>(2) What should one expect from this kind of analysis if a sensory task is novel or otherwise engages behavioral states more explicitly? Could the work be extended to comment on how to disentangle slow learning signals from the low-D modes that were found to be engaged by behavioral shifts?</p></disp-quote><p>That is a very pertinent point. We believe it can specifically create confounds for tasks involving novelty as the animal would be engaged (in terms of attention or arousal) or even outwardly moving (in terms of running or whisking / licking) more during these stimuli. Interpretation of the results of neural response to novel or surprising stimuli might, therefore, be compromised, if one ascribes the changes (positive or negative) in neural activity to local computations, without the analysis of behavior and controlling for more global signals. Interestingly, another more recent dataset from the Allen Brain Institute has such behavioral tasks (https://portal.brain-map.org/explore/circuits/visual-behavior-2p) and it would be interesting to see what the analysis of neural activity and animals’ behavior reveals in this case. We added this to the Discussion.</p><disp-quote content-type="editor-comment"><p>(3) The figures, especially Figure 1, are dense and somewhat unintuitive. It would be nice to see a diagram of the analysis and main results first. This could engage a wider readership. Throughout the figures, it feels like there might be more visually impactful ways of representing the results. Instead, the figures feel repetitive, even though they are highlighting different effects and analyses.</p></disp-quote><p>Thank you for the feedback. We made Figure 1 simpler by removing more example sessions (moved to supplementary figures), and added illustrations of the main idea (in terms of analysis and results) at the beginning to communicate the gist upfront. We also tried to improve the other figures.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. Contribution of behavior vs. elapsed time: Given that it is already well-established that behavioral variability contributes to variability in neuronal responses (e.g. Niell and Stryker, Neuron 2010; Polack et al., Nat. Neurosci. 2013; Vinck et al., Neuron 2015; Musall et al., Nat. Neurosci. 2019; Stringer et al., Science 2019), it is not clear if the authors try to make the case that drift is an artifact of behavioral variability – i.e. drift is not a genuine phenomenon, it's all behavioral – as the abstract and main text strongly suggests by using the words &quot;mistaken&quot; and &quot;misinterpreted&quot;; or that behavior and time can both contribute to the observed changes in representations. These are two different messages that require a different set of analyses and will likely impact the field very differently, so it is crucial to clarify this point.</p></disp-quote><p>Sorry about the misunderstanding. We do not intend to claim that behavioral variability is the only source of drift. Drift of neuronal representations can arise due to a variety of sources, including synaptic turnover or synaptic plasticity, and via different mechanisms like feedforward, recurrent or top-down interactions (as we mention in the Introduction). Here, we are analyzing and clarifying the “contribution” of behavioral variability as one factor to this drift, and suggest that to reveal the contribution of other mechanisms, we need to control for the changes that can potentially arise from behavior (otherwise, those “real” effects might be mistaken or misinterpreted). We are sorry if this point was not clear in our previous writing, we tried to edit the text to reflect this message and tone better in the revised version. We also tried to clarify this by new analyses (Figure 1—figure supplement 5, and Figure 3—figure supplement 2).</p><disp-quote content-type="editor-comment"><p>The authors state that &quot;our results suggest that a significant fraction of what has been described as representational drift in a sensory cortex can be attributed to behavioral variability&quot;. However, quantifying the contribution of behavioral variability to representational drift (as the title of the paper claims) requires analysis that controls for the effects of the passage of time, and this was not done in this paper.</p><p>It is hard to conclude that both time and behavior modulation contribute differently to representational similarity of different presentations of the same video as the two analyzed video blocks are both well separated by time and differ in terms of the animal's behavior (quantified by pupil width and running speed). This will result in a correlation between time and behavioral changes that preclude the possibility to differentiate between the unique contribution of time and behavior to representational similarity.</p></disp-quote><p>This is an excellent point, we addressed it in the response above (also, see the new Figure 1—figure supplement 5, and the associated text and analyses.)</p><disp-quote content-type="editor-comment"><p>One way to quantify the contribution of behavioral variability to representational similarity, is to repeat the analyses of Figure 1 and Figure 4, – i.e. the calculation of the correlation between the absolute change in the behavioral variables and the values of the chosen metric for representational similarity, while holding the interval between video repeats as a constant (subsequent video repeats within the same blocks or video repeats with the same absolute interval between them in general).</p><p>Alternatively, the authors can calculate and report the correlation between the change in behavior and representational similarity either between video repeats within or between video repeats across blocks. To control for the effects of time, the authors should separately calculate the linear relationship between the variables using only the light and dark gray data points (within block), and only the orange data points (between blocks), instead of the correlation indicated in red which uses the entire dataset. Additionally, the authors should report the statistics of these correlations (the p-value and the number of samples) in the corresponding figure legend.</p><p>The authors are suggesting that changes in representational similarity that were previously attributed to time are in fact only changes in the animal behavior over time. This could be tested using a multivariate regression analysis within each individual animal and quantifying the unique contribution of each variable (e.g., time, pupil size, running speed, etc.) as well as testing the significance of each variable to the fitted model, e.g. using GLM (see Driscroll et al., Cell 2017; Musall et al., Nat. Neurosci. 2019 for a similar approach).</p></disp-quote><p>We agree that it would be ideal to have a GLM model to explain the contribution of each behavioral parameter and time to the neural activity. However, we did not find this approach easy or informative in this dataset. We think the reason is a complex, nonlinear and state-dependent modulation of neural activity by different aspects of behavior (see e.g. our new analysis in Figure 4—figure supplement 2), which hinders such a straightforward approach. We tried different approaches to address this issue, however, as we explain below.</p><disp-quote content-type="editor-comment"><p>Furthermore, if the authors are suggesting that representational drift merely reflects gradual changes in behavior, then it would be convincing to show that when the same analyses (e.g. multiple regression) are performed on a different set of stimuli with less abrupt changes in the behavioral state of the animals then there is no significant decrease in representational similarity. To do that, the authors can try to compare the similarity between the representations (for both population-vector correlations and setpoint similarity) across blocks in which the behavior didn't change significantly, and show there is no significant decrease in representational similarity. For example, the authors report in Figure 6 that there is less variability in behavior across blocks of drifting gratings and that there is no consistent and significant change in the behavior across the population of mice. Therefore, one way to test if there is higher representational similarity within blocks compared with across blocks (i.e., representational drift) without changes in behavior is to select mice that didn't exhibit behavioral changes across blocks and perform the analyses on them. This should also be done on the two blocks of 'Natural video 3', which are more proximal in time and therefore are likely to be more similar in behavior.</p></disp-quote><p>We performed such an analysis by quantifying the average behavioral change between the two blocks of presentation for individual mice and calculating a representational drift index (RDI) for each case (Figure 1—figure supplement 5). We found that strong representational drift mainly existed in those animals/sessions with large behavioral changes between the two blocks: there was in fact a strong correlation between average change in pupil width and RDI. Despite the passage of time, such drift was minimal in animals/sessions with small behavioral changes.</p><p>If the passage of time contributes mainly to changes in representational similarity between the two blocks, we should see comparable levels of representational drifts across animals with different levels of behavioural variability. Conversely, if changes in behavioural variability contributes mainly to changes in representational similarity between the two blocks, we should see stronger levels of representational drifts for animals with larger behavioural variability. We indeed found evidence in favour of the latter: Representational similarity remained rather stable for those animals which did not show large behavioural changes between the two blocks (Figure 1—figure supplement 5a). That is, passage of time per se did not contribute strongly to representational drift. Largest representational drifts were observed for animals with the largest changes in average pupil width between the two blocks (Figure 1—figure supplement 5a). In fact, there was a good correlation between the two across animals (Figure 1—figure supplement 5b).</p><p>The relationship was weaker in Neuropixels dataset2, whereby more repeats of the natural video are shown in each block (60 repeats for the total of 30 minutes, versus 20 repeats for 10 minutes in Neurpixels dataset1). Longer blocks of stimulus presentation increases the chance of behavioural changes within the blocks, which can in turn make the average block-wise representational similarity a less reliable metric. In line with this reasoning, further scrutiny into an outlier (with small average pupil changes but rather large representational drift between the two blocks; Figure 1—figure supplement 5b, right) revealed that changes in the running speed of the animal within each block can be the main contributing factor to changes in representational similarity (Figure 1—figure supplement 5c).”</p><disp-quote content-type="editor-comment"><p>2. Cases of non-linear/monotonic relationship between behavioral changes and representational similarity and inappropriate use of correlation: In three of the main figures (specifically: Figure 1B, C, F, G, Figure 3C, I, and Figure 4C, D, G, H) the authors are using Pearson's correlation to quantify the relationship between the absolute change in pupil width between two video repeats and the representational similarity between the same repeats. In many of the plots, the data should not be fitted using Pearson's correlation since some of the assumptions of this model are not met. The most concerning issue is that the relationship between the two variables (change in behavior and representational similarity) does not always follow a linear or even a monotonic trend. This suggests that the relationship between the changes in pupil width and representational similarity are not correctly captured using a univariate linear model (e.g., Pearson's or Spearman's correlations). Additionally, in many of the plots, the data points fall into two or more dense clusters. This can lead to the false impression that there is a strong monotonic relationship between the two variables, even though there is a weak (or even opposite) relationship within each cluster (e.g., as in Figure 3 C, I and Figure 4G) (see an example in Aggarwal and Ranganathan, Perspect Clin Res 2016). This is a crucial point since the clusters of data points most likely represent different blocks that occurred at different times. Likewise, in Figure 5A, C, D, F right panels (and Extended Data Figure 10B, F), excluding the middle-range data points – which are the majority of data points – is unjustified and the use of Pearson's correlation in this case is inappropriate and misleading.</p></disp-quote><p>We performed new analyses to shed light on the source of non-linear/non-monotonic relationships in our data. We show that this is arising as a result of more complex interactions between different aspects of behavior (Figure 4—figure supplement 2,3,4), and that it does not compromise our results when the separate clouds are excluded from the analysis (Figure 4—figure supplement 2).</p><p>In general, the opposite trend that exists in some sessions/animals can only compromise the linear regression analysis. The concern about the clustering arising from different blocks at different times is addressed separately (Figure 1—figure supplement 5, and Figure 3—figure supplement 2). It seems that the non-monotonic relationship is due to abrupt changes in different aspects of behavior (pupil width, location and running) and the way they modulate neuronal activity and representations. Precise analysis of this relationship needs more advanced multivariate and nonlinear / state-dependent models that can capture these complex interactions. But the relationship was not simple enough to be approached by simple addition of our univariate linear model to multivariate ones. Also, the nature of nonlinearity did not seem to be straightforward or the same across examples in the datasets, to be able to capture it by nonlinear correlation methods.</p><p>We want to emphasize that we did not use the linear regression to imply a linear relationship between the parameters, but to show and quantify the first-order dependence of representational (or setpoint) similarity on changes in behavior. More fine-tuned nuances and relationships should, of course, be further captured by more advanced analyses. We do not claim to have addressed the latter, as we believe such an analysis is beyond the scope of this work, which is mainly concerned with the first-order effect. By performing new analyses, we have now shown that the first-order relationship is neither compromised by the non-monotonic trends (Figure 4—figure supplement 2), nor by the concerns regarding the contribution of other factors like the passage of time to representational drift (Figure 1—figure supplement 5).</p><p>We hope to be able to expand our analysis in future studies to provide more explanatory and predictive power regarding the relationship between behavioral variability and neural activity. This is a hard problem, however, as the best models on the market can hardly capture more than 50% of variance (even in the absence of active behavior; see e.g. for a recent example: The Sensorium competition on predicting large-scale mouse primary visual cortex activity; https://arxiv.org/abs/2206.08666). But we hope that the insights obtained from our analysis here help in obtaining better models of encoding / decoding which are both informed by behavioral and sensory parameters.</p><p>With regard to quantification of behavioral tuning in Figure 5: The data points that are excluded (in Figure 5A, C, D, F right) are based on two criteria: bootstrap analysis (excluding units that do not show significant correlation with behavior), and comparable behavioral parameters between the two blocks (as shown in panels b,e). We are showing here that units that show significant modulation with running remain consistently modulated between the two blocks, provided that the animal has comparable level of running between the two blocks. Even if a unit would be modulated by behavior in principle, different levels of running could change the tuning (as we have discussed for extreme examples in our new decoding figure; Figure 7—figure supplement 2). Our results show that, focusing on such significantly modulated units and sessions, the units which are positively (/negatively) modulated by running in the first block remain positively (/negatively) modulated in the second block, and this trend is quantified by the linear regression.</p><p>Alternatively, we can quantify the number of units keeping their signs of modulation compared to those changing the sign between the two blocks, as a measure of constancy of tuning. We calculated a sign constancy index (SCI), which is the fraction of units preserving their sign of modulation. This quantification also led to similar results: when all units were included, we obtained a SCI of ~70% for both V1 and all units and for both datasets; when significantly modulated units in sessions with comparable levels of running were considered, SCI rose to more than 90% for both datasets, corroborating the results of our linear regression analysis.</p><disp-quote content-type="editor-comment"><p>3. Stimulus dependent representational drift: A key statement of the current manuscript is that slow and gradual changes in behavior may change the setpoints (activity rates) of different neurons over time, leading to the appearance of a gradual decrease in representational similarity metrics such as 'population vector correlation' even in brain areas that do not, (literary) represent (or encode) the presented stimuli. This point was raised both as a critical evaluation of the representational similarity metrics chosen in the past to characterize the stability of visual representations or as a criticism of the use of the term 'representational drift'.</p><p>The fact that there are changes that are not purely related to the tuning of cells is not new and was demonstrated in several previous studies on coding stability. For instance, Ziv et al., Nature Neurosci. 2013 and Gonzales et al., Science 2019, have shown that place cells in the mouse hippocampal CA1 can drop in and out of the ensemble during different visits to the same familiar environment over weeks, leading to a gradual change in the population of place cells that encode the location of the animal during the task. These changes, which reflect changes in activity rates, were independent of the position the neurons encode and were found in place cells and non-place cells alike. Likewise, Driscoll et al., Cell 2017 and Aschauer et al., Cell Reports 2022, showed a similar turnover of the active cells in the cortex. Notably, Rubin et al., eLife 2015, showed that hippocampal representations of two distinct environments (which had very distinct place-cell representations) co-evolve over timescales of days-weeks. This shared component of the drift stems from gradual context-independent changes in activity rates. In fact, the prevalent use of the term representational drift (coined by Rule et al., Curr Op in Neurobiol 2019) is based on these above-mentioned studies and served to capture the entire range of gradual changes in neuronal activity over time. More recently, Schoonover et al., Nature 2021, and Deitch et al., Current Biology 2021 separately analyzed changes in activity rates and changes in tuning, and showed gradual changes in the cells' activity rates during periods of spontaneous activity, explicitly stating that these changes can occur independently of the presented stimulus.</p></disp-quote><p>We agree with that. We appreciate the hint into the previous literature, we added the reference to and discussion of these previous works to our Discussion.</p><disp-quote content-type="editor-comment"><p>Crucially, cells' activity rates are very relevant to stimulus encoding, as cells exhibit 'rate remapping' between environments or stimuli. Thus, equating stimulus-independent changes and changes in activity rates (setpoint similarity) is problematic because part of the changes in activity rates can be stimulus-dependent. This can be seen even in the visual cortex by calculating the setpoint similarity between different stimuli (e.g. Deitch et al., 2021 Figure S3H): the average activity rates of blocks of the same stimuli are more similar than between blocks of different stimuli. Thus, code stability is a function of two key factors: (1) stability in tuning and (2) stability in activity rates. Deitch et al., 2021 showed that the gradual changes over time that occur with respect to these factors are nearly independent of each other.</p></disp-quote><p>Thanks for raising this important point. We agree that the average activity can be different for different stimuli (e.g. as a result of different feedforward drive from full-filed gratings and natural images), that this can lead to different average setpoint similarity between repeats of stimulus types (as shown in Figure 6b,c,e,f), and that this can be very relevant for stimulus encoding. We did not intend to downplay the role of this difference. The aim was rather to highlight, within the same stimulus type, the difference between the average activity of a unit (independent of variations in stimulus – e.g. different frames of a natural video) and specific modulations arising from the change in the stimulus parameters (e.g. different image frames or orientations of gratings). We wanted to demonstrate that, within the same stimulus type, changes in the average activity independent of changes in the stimulus space can distort the measure of representational similarity that is often attributed to the variance in stimulus features (e.g. selectivity to natural images or preference to specific orientations). We agree that our language could be confusing, as the reviewer(s) mentioned, with regard to a more general picture of stimulus processing, which spans both mean and variance of stimulus-evoked responses, within and across different stimulus types. We therefore tried to refine our wording to avoid such confusion and clarify this distinction in the text now.</p><disp-quote content-type="editor-comment"><p>Furthermore, the fact that there are changes in setpoint similarity in CA1, although this area doesn't reliably encode visual stimuli, cannot, in itself, be used as an argument for the role of behavioral changes in representational drift since these changes can also be associated with elapsed time (see our point #1 above).</p></disp-quote><p>As we showed before, representational drift across animals/sessions was strongly determined by the degree of behavioral changes between the two blocks of presentation rather than passage of time (Figure 1—figure supplement 5). Such an effect is expected to be broadcasted to other regions too, and our analysis for all units show the same trend (Figure 1—figure supplement 5d,e). However, we cannot rule out the effect of other factors like the passage of time here. Note that passage of time in itself can be confounded by other (behavioral) changes. There might be changes in other aspects of behavior that we do not have access to their quantification (e.g. whisking, posture, or other body movements, as mentioned by the third reviewer below) and might be more relevant to regions like CA1. To err on the side of caution, we added a cautionary note to the discussion of our results now.</p><disp-quote content-type="editor-comment"><p>Overall, we agree that it is important to carefully dissociate between the effects of behavior on changes in neuronal activity that are stimulus-dependent or independent, but we feel that the criticism raised by the authors ignores the findings of the relevant literature, which (1) did not purely attribute the observed changes to the sensory component, and (2) did dissociate between stimulus-dependent changes (in tuning) and off-context/stimulus-independent changes (in activity rates).</p><p>We propose that the authors tone down their interpretations throughout the paper, and especially in the discussion: e.g., &quot;changes in representational similarity (i.e. representational drift) can arise from changes in both sources, and hence attributing it purely to the drift of the sensory component might be inaccurate&quot; and &quot;Drawing further conclusions about stimulus-dependences of representational drift in visual cortex – and other sensory cortices – thus needs a critical evaluation by teasing apart the contribution of different components (stimulus-induced and stimulus-independent)&quot;.</p></disp-quote><p>We edited our text and changed the language and tone to accommodate for these suggestions in order to appreciate the contribution of the average response, to refer to previous analyses, and to clarify what we mean in terms of the needed controls in future studies.</p><disp-quote content-type="editor-comment"><p>4. The use of the term &quot;Representational similarity&quot; versus &quot;Representational drift&quot;:</p><p>It is important that the authors explicitly define the term representational drift and edit the paper in a way that uses the terms &quot;representational similarity&quot; and &quot;representational drift&quot; in a consistent way throughout the manuscript. Most studies have demonstrated drift (even if not using the term 'drift') as a decreasing similarity between neuronal responses to the same stimulus/task as a function of the time interval between experiences/stimulus presentations under the same experimental conditions (Ziv et al., Nat. Neurosci. 2013; Lee et al., Cell 2020, Driscoll et al., Cell 2017; Rule et al., 2019; Schoonover et al., 2021; Deitch et al., 2021; Marks and Goard et al., Nat. Comm. 2021; Jensen et al., BioRxiv 2021 and Aschauer et al., Cell Reports 2022). This point regarding the differences between drift and variability in neuronal responses is nicely illustrated and discussed in a recent review paper (Clopath et al., Philos Trans, 2017). However, throughout the current manuscript, the authors refer to any change in representational similarity as representational drift and use these terms interchangeably regardless of the interval between the compared timepoints. For example:</p><p>&quot;…Importantly, changes in representational similarity (i.e. representational drift)…&quot;</p></disp-quote><p>That is an important point and we tried to address it in the revised manuscript. Specifically, we clarified the definitions of the two terms and how we use them at the beginning.</p><p>We explain here why this confusion arises in our terminology, and how we tried to mitigate it.</p><p>First, we totally agree with the authors on the “conventional” definition of representational drift.</p><p>However, the datasets we were analyzing here were different in terms of timescale (hours) with the usual studies of drift (days to weeks), and we had access to limited repetitions of the stimuli. Nevertheless, the datasets are insightful and informative as they have recorded responses of large numbers of units in different areas to standardized stimuli across many animals while quantifying important (although yet incomplete) aspects of behavior.</p><p>We therefore focused on analyzing how representational similarity will be modulated by changes in behavior. As we explained before, any measure of representational drift (which tries to quantify the gradual changes over longer time scales) is eventually a function of representational similarity. We therefore based the main part of our analyses and results on describing how “changes in representational similarity” can be related to “behavioral changes”.</p><p>Once this is established, a consequence of our analysis would be that, if behavior gradually changes over the longer time course (for example, an animal loses motivation to be engaged in a task, or employs different strategies to sample the visual or olfactory stimulus space), this can lead to the representational drift (over the conventional time course).</p><p>Ideally, we wanted to have access to datasets with such a time course of repetition of stimuli and measurements of behavior. But in the absence of such standardized measurements, we used the current datasets to establish the dependence of “changes in representational similarity” on “behavioral changes” – which can in turn be the basis for “representational drift” in the conventional sense.</p><p>We, nevertheless, calculated a conventional representational drift index (RDI) between the blocks of presentation in the revised manuscript, and clarified our terminology to avoid confusion between those two terms.</p><disp-quote content-type="editor-comment"><p>In most of the above-mentioned studies about drift, the behavior or performance of the animal was at a steady state throughout the examined time intervals, suggesting that the observed changes in neuronal activity are not due to gradual changes in the behavior (e.g., due to learning, habituation, or changes in arousal). Thus, while the behavior itself may vary across different time points, as long as it is not changing gradually throughout the experiment, it should not lead to the appearance of drift.</p></disp-quote><p>We agree that many aspects of behavior might be at steady state over longer time courses.</p><p>But even over such long time courses, some gradual trends have been reported (e.g., gradual decrease of average pupil size or eye movement over weeks in the study of Marks and Goard, Nature Communications 2021: Supplementary Figure 11d and Supplementary Figure 10).</p><p>When active sensing is involved, it might be specifically difficult to control for changes in the behavioral strategy of the animal (e.g. active sampling of the visual or olfactory space).</p><p>Interestingly, a recent study has shown that accounting for different behavioral variability / strategies in bats reduces the apparent variability of the neural code (Liberti et al., Nature 2022). More studies with such datasets can shed light on the longer time scales of correlations and co-dependence between behavioral variability and the stability of neural code.</p><disp-quote content-type="editor-comment"><p>To determine whether neuronal representations are gradually changing, there must be at least three (not necessarily equally spaced) different compared time points (see Clopath et al; 2017). We suggest adding a paragraph that explicitly explains the difference between neuronal variability and drift, how to differentiate between the two cases, and including an additional time point in the illustration presented in Extended Data Figure 1A (which now only includes two times points).</p></disp-quote><p>We added a paragraph to our Discussion explaining the difference between neural variability and drift, commented on the gradual changes of the parameters, and added an additional time point in our illustration in Figure 1—figure supplement 1, as suggested by the reviewers.</p><p>The reviewers are completely right that establishing the drift of neural representations as a function of behavioral variability ultimately needs to be analyzed over longer time scales where a gradual drift of behavior is documented. However, as mentioned above, we did not have the needed data on this long time scale of recording (days to weeks) in the current datasets.</p><p>We therefore aimed to show how behavioral variability, even over this time scale, may affect the analysis of representational similarity (which would be the basis of representational drift), with the hope that the insights obtained from such analysis can be used in future for the datasets which measure large-scale population activity and quantify behavior over long time scales.</p><p>Within the same datasets, we now performed a new analysis to shed light on gradual changes (Figure 4—figure supplement 5). Our analysis shows that the average pupil size and the average activity of units are showing a concomitant gradual <italic>increase</italic> during the recording session – with a pattern remarkably similar between the two datasets (~50% increase in the average firing rate). A mechanism similar to what we analyzed in Figure 4 in terms of setpoint similarity can, therefore, contribute to gradual changes of representational similarity (and hence representational drift), for multiple repeats of stimulus presentations along this trajectory. It is also conceivable that similar gradual changes, e.g. <italic>decrease</italic> in pupil size over days and weeks (Marks and Goard, 2021), lead to similar effects.</p><disp-quote content-type="editor-comment"><p>5. Focusing on reliable units improves time-lapse decoding: The analysis presented in Figure 7 shows that using reliable units (i.e., units that don't show changes in their tuning over time) results in higher decoding accuracy (i.e. more stable population code). Given that stability at the single-cell level should directly contribute to stability at the population level, this analysis is circular and therefore the conclusion that &quot;Decoding generalizability of natural images improves by focusing on reliable units&quot; is trivial.</p><p>Irrespective of this issue, we agree that it is a reasonable idea that reliable units could serve as a stable core for the brain to rely on for coping with changing neuronal responses. However, the distribution of stimulus reliability is not bi-modal (as shown in Figure 2H), but actually skewed towards lower reliability values. Thus, it is unclear how focusing on a small, unrepresentative subset of reliable units informs us how the brain copes with changing representations.</p></disp-quote><p>The main point of our (sensory) decoding analysis was to show the other side: that, in the face of strong behavioral variability, a decoder cannot generalize what it has learned from one block of stimulus presentation to another. We believe this is not an intuitive result in itself. We then continued to show that focusing on reliable units can rescue this and enable transfer of learning. We also used this decoding analysis to corroborate our previous results and analyses, on the independent modulation of neural responses by stimulus and behavior, and on reliable modulation of activity by behavior. We now also expanded our decoding analysis to decode behavior (Figure 7—figure supplement 2). These results might help to understand how the brain can cope with changing representations, if decoding of stimulus and behavior are both the targets. However, the questions about the reasons behind – and consequences of – strong modulation by behavior (and low reliability of stimulus responses) remain to be answered.</p><p>We speculated on the latter, along with the lines suggested by the reviewers, in a new paragraph in the Discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>– In the introduction it may be useful to provide working definitions of (and differences between) &quot;drift of behavioural state&quot;, &quot;behavioural drift&quot; and &quot;behaviour&quot;.</p></disp-quote><p>This is an important point. We added these definitions and clarified their differences at the beginning of the text.</p><disp-quote content-type="editor-comment"><p>– To rule out any potential artifact resulting from bin width choice being correlated with behavioral timescale, it would be useful to see the effect of varying bin width in computing population vectors.</p></disp-quote><p>We extracted population vectors rendered in larger time bins and found similar dependence of representational similarity (calculated from these population vectors) with changes in pupil size (Figure 1—figure supplement 3b).</p><disp-quote content-type="editor-comment"><p>– The Siegle et al. dataset methods imply that pupil position is available as well; thus do the same results apply if using position in addition to diameter? It would be nice to mention if any other behavioural measures are available that were not analyzed, as ignoring these seemed to lead previous accounts of drift astray.</p></disp-quote><p>Many thanks for this great suggestion. We added the documentation of changes in eye position to the revised manuscript, and analyzed the dependence of changes in setpoint similarity on shift in the pupil center, and how this parameter was correlated with pupil width (Figure 4—figure supplement 3,4). This information was also helpful in other analyses we performed (Figure 4—figure supplement 2). The other behavioral parameter that is measured in the datasets and we did not include is pupil height. Since this was highly correlated with pupil width, we decided to use one as a proxy for pupil size.</p><disp-quote content-type="editor-comment"><p>– Pg. 5 bottom paragraph: &quot;Inclusion of multiple cell types…to control for this…&quot; – this control actually seems to be for possible strain differences; it is not clear in the Siegle at al. dataset how many cell types were presented (i.e. which cells were opto-tagged), so this information should be present if discussed.</p></disp-quote><p>That is correct. We have not analyzed the optotagging part of the experiments at the end. We have changed the wording to state that this control is for different transgenic mice, according to the information provided in Supplementary File 1.</p></body></sub-article></article>