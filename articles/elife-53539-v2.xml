<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">53539</article-id><article-id pub-id-type="doi">10.7554/eLife.53539</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Altered functional connectivity during speech perception in congenital amusia</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-164516"><name><surname>Jasmin</surname><given-names>Kyle</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9723-8207</contrib-id><email>kyle.jasmin.11@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-15750"><name><surname>Dick</surname><given-names>Frederic</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2933-3912</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-165582"><name><surname>Stewart</surname><given-names>Lauren</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-117250"><name><surname>Tierney</surname><given-names>Adam Taylor</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7624-6918</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychological Sciences, Birkbeck University of London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>UCL Institute of Cognitive Neuroscience, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Department of Experimental Psychology, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution>Department of Psychology, Goldsmiths University of London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Oxenham</surname><given-names>Andrew J</given-names></name><role>Reviewing Editor</role><aff><institution>University of Minnesota</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>07</day><month>08</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e53539</elocation-id><history><date date-type="received" iso-8601-date="2019-11-12"><day>12</day><month>11</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-08-03"><day>03</day><month>08</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Jasmin et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Jasmin et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-53539-v2.pdf"/><abstract><p>Individuals with congenital amusia have a lifelong history of unreliable pitch processing. Accordingly, they downweight pitch cues during speech perception and instead rely on other dimensions such as duration. We investigated the neural basis for this strategy. During fMRI, individuals with amusia (N = 15) and controls (N = 15) read sentences where a comma indicated a grammatical phrase boundary. They then heard two sentences spoken that differed only in pitch and/or duration cues and selected the best match for the written sentence. Prominent reductions in functional connectivity were detected in the amusia group between left prefrontal language-related regions and right hemisphere pitch-related regions, which reflected the between-group differences in cue weights in the same groups of listeners. Connectivity differences between these regions were not present during a control task. Our results indicate that the reliability of perceptual dimensions is linked with functional connectivity between frontal and perceptual regions and suggest a compensatory mechanism.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Spoken language is colored by fluctuations in pitch and rhythm. Rather than speaking in a flat monotone, we allow our sentences to rise and fall. We vary the length of syllables, drawing out some, and shortening others. These fluctuations, known as prosody, add emotion to speech and denote punctuation. In written language, we use a comma or a period to signal a boundary between phrases. In speech, we use changes in pitch – how deep or sharp a voice sounds – or in the length of syllables.</p><p>Having more than one type of cue that can signal emotion or transitions between sentences has a number of advantages. It means that people can understand each other even when factors such as background noise obscure one set of cues. It also means that people with impaired sound perception can still understand speech. Those with a condition called congenital amusia, for example, struggle to perceive pitch, but they can compensate for this difficulty by placing greater emphasis on other aspects of speech.</p><p>Jasmin et al. showed how the brain achieves this by comparing the brain activities of people with and without amusia. Participants were asked to read sentences on a screen where a comma indicated a boundary between two phrases. They then heard two spoken sentences, and had to choose the one that matched the written sentence. The spoken sentences used changes in pitch and/or syllable duration to signal the position of the comma. This provided listeners with the information needed to distinguish between &quot;after John runs the race, ...&quot; and &quot;after John runs, the race...&quot;, for example.</p><p>When two brain regions communicate, they tend to increase their activity at around the same time. The brain regions are then said to show functional connectivity. Jasmin et al. found that compared to healthy volunteers, people with amusia showed less functional connectivity between left hemisphere brain regions that process language and right hemisphere regions that process pitch. In other words, because pitch is a less reliable source of information for people with amusia, they recruit pitch-related brain regions less when processing speech.</p><p>These results add to our understanding of how brains compensate for impaired perception. This may be useful for understanding the neural basis of compensation in other clinical conditions. It could also help us design bespoke hearing aids or other communication devices, such as computer programs that convert text into speech. Such programs could tailor the pitch and rhythm characteristics of the speech they produce to suit the perception of individual users.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>perception</kwd><kwd>amusia</kwd><kwd>speech</kwd><kwd>functional connectivity</kwd><kwd>fMRI</kwd><kwd>auditory</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome</institution></institution-wrap></funding-source><award-id>109719/15/Z</award-id><principal-award-recipient><name><surname>Tierney</surname><given-names>Adam Taylor</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>ECF-2017-151</award-id><principal-award-recipient><name><surname>Jasmin</surname><given-names>Kyle</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014693</institution-id><institution>Society for Education, Music and Psychology Research</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Jasmin</surname><given-names>Kyle</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Individuals with amusia, who have unreliable pitch processing, show decreased functional connectivity between right auditory and left language-related cortex during speech perception, demonstrating a neural basis for compensatory dimensional weighting.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Congenital amusia is a rare condition characterized by impaired perception of and memory for pitch (<xref ref-type="bibr" rid="bib41">Peretz et al., 2002</xref>). Although congenital amusia presents as an auditory condition, auditory cortical responses are normal (<xref ref-type="bibr" rid="bib36">Moreau et al., 2013</xref>; <xref ref-type="bibr" rid="bib38">Norman-Haignere et al., 2016</xref>), as is subcortical encoding of pitch (<xref ref-type="bibr" rid="bib30">Liu et al., 2015b</xref>). The dominant view of amusia’s neural basis is that connectivity between right inferior frontal cortex and right auditory cortex is impaired, resulting in impaired conscious access to pitch information for guiding behavior (<xref ref-type="bibr" rid="bib21">Hyde et al., 2011</xref>; <xref ref-type="bibr" rid="bib1">Albouy et al., 2013</xref>; <xref ref-type="bibr" rid="bib27">Leveque et al., 2016</xref>; <xref ref-type="bibr" rid="bib62">Zendel et al., 2015</xref>; see <xref ref-type="bibr" rid="bib44">Peretz, 2016</xref> for review). While congenital amusia is believed to be innate, there is evidence that recovery is possible through training (<xref ref-type="bibr" rid="bib58">Whiteford and Oxenham, 2018</xref>).</p><p>Although pitch is usually associated with music, it is also important for cueing categories in spoken language (<xref ref-type="bibr" rid="bib11">de Pijper and Sanderman, 1994</xref>; <xref ref-type="bibr" rid="bib53">Streeter, 1978</xref>) and conveying emotion in speech (<xref ref-type="bibr" rid="bib13">Frick, 1985</xref>). In highly controlled laboratory tasks in which speech perception judgments must be made based on pitch alone, only minor deficits have been observed in amusia (<xref ref-type="bibr" rid="bib29">Liu et al., 2015a</xref>; <xref ref-type="bibr" rid="bib39">Patel et al., 2008</xref>). In naturalistic speech perception contexts, people with amusia rarely report any difficulties (<xref ref-type="bibr" rid="bib28">Liu et al., 2010</xref>). This may be because, in natural speech, pitch variation tends to co-occur with variation in other acoustic dimensions, such as duration and amplitude. Our lab has shown that in such cases where multiple redundant cues are available, English-speaking individuals with amusia tend to rely less on pitch than non-amusic controls, suggesting they may calibrate their perception by down-weighting the cues that are less reliable for them (<xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>). As for emotional prosody in speech, individuals with amusia can recognize emotions in spoken sentences, but not in short samples such as isolated vowels (<xref ref-type="bibr" rid="bib46">Pralus et al., 2019</xref>), or when speech has been filtered to remove high-frequency non-pitch cues (<xref ref-type="bibr" rid="bib31">Lolli et al., 2015</xref>).</p><p>It is unknown how decreased reliance on a particular acoustic cue during speech perception (such as pitch cues in amusia) is reflected in the brain. Previous neural studies of cue integration have focused on integration of multiple modalities, for example the ‘weighted connections’ model of multisensory integration. In this model, the relative reliability of the modalities involved with perception of a stimulus is related to differential connectivity strength (<xref ref-type="bibr" rid="bib3">Beauchamp et al., 2010</xref>; <xref ref-type="bibr" rid="bib49">Rohe and Noppeney, 2018</xref>). For example, when participants simultaneously view and feel touches to the hand, and reliability of visual and tactile perception is manipulated experimentally via introduction of noise, connection strength (effective connectivity measured with functional MRI and structural equation modeling) between unimodal and multimodal sensory areas adjusts accordingly. More concretely, when visual information is degraded, the connection strength between lateral occipital cortex (a visual area) and intraparietal sulcus (a multimodal area) decreases, and when tactile perception is made noisier, connection strength between secondary somatosensory cortex and intraparietal sulcus becomes weaker (<xref ref-type="bibr" rid="bib3">Beauchamp et al., 2010</xref>). Similarly, effective connectivity between the (multimodal) superior temporal sulcus (STS) and visual and auditory areas has shown similar modulations during processing of audiovisual speech: connection strength between auditory cortex and the STS is weaker when noise has been introduced to the auditory speech, and conversely connection strength between visual cortex and STS is weaker if visual noise is introduced (<xref ref-type="bibr" rid="bib37">Nath and Beauchamp, 2011</xref>).</p><p>Just as connectivity differences have been shown to reflect the precision of different sensory modalities during multi<italic>sensory</italic> integration, an analogous phenomenon may be at work within a single modality during multi<italic>dimensional</italic> integration. As mentioned, the acoustic speech signal carries multiple co-occurring acoustic dimensions (e.g. roughly described as voice pitch, duration, and amplitude), which often provide redundant cues to disambiguate a linguistic category (<xref ref-type="bibr" rid="bib40">Patel, 2014</xref>; <xref ref-type="bibr" rid="bib60">Winter, 2014</xref>; <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>). Individuals with typical pitch perception have learned through a lifetime of experience with speech acoustics that vocal pitch is a useful and reliable cue. By contrast, individuals with amusia, who have unreliable perception of and memory for pitch (analogous to the ‘noise’ introduced in the multisensory integration studies cited above), would have learned that, for them, pitch is not a reliable cue for processing spoken language. Thus, by analogy to the multisensory weighting results described above, we hypothesize that amusics may exhibit decreased connectivity between language regions and pitch-related areas during speech processing.</p><p>The neural foundations of perceptual weighting in speech have thus far not been investigated in atypical individuals. Indeed, only one previous functional neuroimaging study has examined the neural processing of spoken material in people with amusia. In this study, no group differences were detected in task-related activation or functional connectivity during processing of speech (whereas group differences were observed during processing of tones; <xref ref-type="bibr" rid="bib2">Albouy et al., 2019</xref>). However, the connectivity analyses in this study focused on the silent retention interval in a task in which participants needed to maintain phonemic and not pitch-related information in memory; the analyses also used broader bilateral ROIs within networks associated with language processing. It remains an open question how functional connectivity in amusic and non-amusic participants may differ during speech encoding in pitch-related language tasks within regions of interest selected with a whole-brain data-driven approach.</p><p>To determine whether the relative reliability of auditory dimensions in speech perception is reflected in functional connectivity, we used functional magnetic resonance imaging to scan 15 individuals with amusia and 15 controls. Participants matched spoken sentences with visually presented ones on the basis of the position of intonational phrase boundaries. These intonation changes were conveyed differently, in three conditions: Pitch-Informative (where only pitch cues could be used to make the judgment), Duration-Informative (where only duration cues could be used) or Both-Informative (both pitch and duration cues could be used; <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>; <xref ref-type="bibr" rid="bib24">Jasmin et al., 2020b</xref>). Functional connectivity was then examined using a data-driven approach that allowed us to identify the largest group differences, without the need for regions of interest to be selected <italic>a priori</italic>. The benefit of this approach is that any set of regions could emerge, not only ones reported in previous literature. Crucially, task performance was matched between the groups (based on prior behavioural testing; <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>), ensuring that any neural differences did not simply represent an inability to perform the task. Finally, functional connectivity between these areas was analyzed with respect to prosodic cue weights obtained outside the scanner, and also compared to functional connectivity calculated from different scanning runs with a passive listening task.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>In-scanner behavior</title><p>On each trial, participants read one visually presented text sentence, then heard two auditory versions of the sentence, only one of which contained an acoustically conveyed phrase boundary in the same place as in the text sentence (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for schematic and example sentences). Trials were scored as correct if a participant pressed the button associated with the auditory sentence that correctly matched the text sentence. Proportions of correct judgments (<xref ref-type="fig" rid="fig2">Figure 2</xref>) were subjected to a repeated-measures analysis of variance. Overall, proportion correct across amusia and control groups was matched (main effect of Group, F(1,84) = 0.16, p=0.69, interaction of Group by Condition, F(2,84) = 0.374, p=0.96). This lack of interaction was predicted based on previous results obtained from a similar paradigm using out-of-scanner data but from the same participants (<xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>). There was a main effect of condition (F(2,84) = 3.32, p=0.04). Follow-up post-hoc testing indicated that performance in the Both-Informative condition (with pitch and duration cues simultaneously present) was more accurate than either Pitch-Informative (t(84) = 2.31, p=0.023) or Duration-Informative (t(84) = 2.15, p=0.03), a result that was also predicted and which replicates the behavioral findings in <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>. One outlier control participant’s performance was less than 0.3. Re-analysis of the data without this participant did not change the results pattern.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic of experimental paradigm.</title><p>(<bold>A</bold>) Example spectrograms of the early closure (top) and late closure (bottom) stimuli for the Both-Informative condition. Fundamental frequency contours are indicated with blue lines. The relative duration of the critical words are indicated with orange and green boxes. To the right, syntactic trees for the two sentences are shown to highlight the grammatical structure indicated by the phrase boundaries. (<bold>B</bold>) The time course of a single trial. Participants read a text version of the sentence from the screen, which was either early or late closure. This was followed by auditory presentation of the late and early closure versions. After both recordings were played, participants chose whether the first or second recording they heard matched the visually presentence sentence better. A single whole-brain volume was acquired after the button press, timed to capture the peak of the hemodynamic response roughly around presentation of the second sentence.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53539-fig1-v2.tif"/></fig><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>In-scanner performance.</title><p>Prosodic categorization performance measured in the scanner (proportion correct); each point represents the performance of a single participant.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>n-scanner performance.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-53539-fig2-data1-v2.txt"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53539-fig2-v2.tif"/></fig></sec><sec id="s2-2"><title>Neuroimaging - whole-brain connectedness</title><p>Results from these analyses are available online (see Data Availability Statement for details). A data-driven approach was taken to identify brain regions with the largest group- and condition-related differences in functional connectivity (see Materials and methods). Comparing whole-brain connectedness values by group (Amusia vs. Controls) revealed four significant locations (where <italic>z</italic> of peak vertices &gt; 4.61, FDR-corrected p&lt;0.05) that showed greater whole-brain connectedness for the control than for the amusia group (see <xref ref-type="fig" rid="fig3">Figure 3</xref>, yellow crosses). All group differences were located in the inferior frontal cortex: two left hemisphere vertices (inferior frontal gyrus <italic>p. triangularis</italic> and dorsolateral prefrontal cortex) and two right hemisphere vertices (inferior frontal gyrus <italic>p. triangularis</italic> and <italic>p. orbitalis</italic>). There were no areas where whole-brain connectedness differed by Condition, or showed an interaction of Group and Condition.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Seed locations and group differences in seed-to-whole brain functional connectivity.</title><p>Inflated surfaces show the locations of False Discovery Rate-corrected group differences (Control &gt; Amusia) in whole-brain connectivity (yellow crosses, minimum Z &gt; 4.61), which were used as seeds in subsequent analyses (minimum Z &gt; 3.57; warm colors indicate greater connectivity in the control than amusia participants). All four seed vertices were located in inferior frontal cortices (left inferior frontal gyrus, left DLPFC, right inferior frontal gyrus <italic>p. triangularis,</italic> and right inferior frontal gyrus <italic>p. orbitalis</italic>) (<bold>A</bold>) Significant group differences (Control &gt; Amusia) in functional connectivity with left hemisphere seeds. The largest decreases in connectivity in the amusia group were located in right superior temporal plane and gyrus, the posterior middle temporal gyrus onto the inferior bank of the superior temporal sulcus, and anterior insula. (<bold>B</bold>) Significant group differences (control vs amusia) in functional connectivity with right hemisphere seeds. Prominent decreases in connectivity with the right inferior frontal gyrus in individuals with amusia were observed in the superior temporal plane and regions of occipital, frontal, and parietal cortex.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53539-fig3-v2.tif"/></fig></sec><sec id="s2-3"><title>Follow-up seed-to-whole brain tests</title><p>Follow-up testing was conducted on the four significant regions (Control &gt; Amusia, collapsed across the three conditions) identified above to characterize the specific cortical regions driving these group connectivity differences (<xref ref-type="bibr" rid="bib4">Berman et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Gotts et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Jasmin et al., 2019</xref>; <xref ref-type="bibr" rid="bib50">Song et al., 2015</xref>). Relative to control participants, amusic participants' left inferior frontal gyrus seed region showed particularly notable decreases in connectivity with the right posterior superior temporal and inferior parietal cortex, as well as with the right posterior superior temporal sulcus (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Analysis of subcortical connectivity indicated that there was also weaker connectivity with the right nucleus accumbens (<xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Significant main effects of Group involving functional connectivity between seed areas and subcortical Structures.</title><p>All effects are Control &gt; Amusia.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Seed</th><th valign="top">Region of interest</th><th valign="top">F(1,87)</th><th valign="top">p</th></tr></thead><tbody><tr><td valign="top">L IFG</td><td valign="top">R Accumbens</td><td valign="top">15.43</td><td valign="top">0.0002</td></tr><tr><td valign="top">L DLPFC</td><td valign="top">L Putamen</td><td valign="top">15.78</td><td valign="top">0.0001459</td></tr><tr><td valign="top"/><td valign="top">R Putamen</td><td valign="top">17.78</td><td valign="top">0.00006047</td></tr><tr><td valign="top"/><td valign="top">L Caudate</td><td valign="top">25.23</td><td valign="top">0.0000027</td></tr><tr><td valign="top"/><td valign="top">R Caudate</td><td valign="top">11.51</td><td valign="top">0.001044</td></tr><tr><td valign="top"/><td valign="top">L Cerebellum</td><td valign="top">24.47</td><td valign="top">0.00000364</td></tr><tr><td valign="top"/><td valign="top">R Cerebellum</td><td valign="top">16.23</td><td valign="top">0.0001194</td></tr><tr><td valign="top"/><td valign="top">L Pallidum</td><td valign="top">14.60</td><td valign="top">0.0002484</td></tr><tr><td valign="top"/><td valign="top">R Pallidum</td><td valign="top">12.44</td><td valign="top">0.0006739</td></tr><tr><td valign="top"/><td valign="top">L Thalamus</td><td valign="top">14.83</td><td valign="top">0.0002245</td></tr><tr><td valign="top"/><td valign="top">R Thalamus</td><td valign="top">15.72</td><td valign="top">0.0001501</td></tr><tr><td valign="top">R IFG (orbit)</td><td valign="top">L Thalamus</td><td valign="top">14.83</td><td valign="top">0.0002245</td></tr><tr><td valign="top">R IFG (triang.)</td><td valign="top">L Accumbens</td><td valign="top">10.10</td><td valign="top">0.002054</td></tr></tbody></table></table-wrap><p>The left dorsolateral prefrontal cortex in amusic participants showed decreased functional connectivity with the mid portions of the right superior temporal gyrus, posterior part of the right middle temporal gyrus extending into the inferior bank of the superior temporal sulcus, and the right anterior insula (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Several subcortical structures - bilateral caudate nucleus and putamen, bilateral pallidum, bilateral cerebellum, and bilateral thalamus - also showed significantly reduced (FDR-corrected) connectivity with the seed in amusics (<xref ref-type="table" rid="table1">Table 1</xref>).</p><p>The right <italic>pars triangularis</italic> seed showed Control &gt; Amusic connectivity with right dorsolateral prefrontal cortex and left posterior superior temporal gyrus (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). It also showed decreased connectivity with left nucleus accumbens. Right <italic>pars orbitalis</italic> showed decreased connectivity with right dorsolateral prefrontal cortex (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). There was also decreased connectivity with the left thalamus (<xref ref-type="table" rid="table1">Table 1</xref>).</p></sec><sec id="s2-4"><title>Correlations between functional connectivity levels and prosodic cue weights</title><p>Of the 30 participants in this study, 21 took part in an experiment that measured the degree to which they relied on pitch versus duration to categorize prosody, that is, their ‘normalized prosodic cue weights’, which ranged from 0 to 1, with values greater than 0.5 indicating greater reliance on pitch than duration, and values less than 0.5 indicating greater reliance on duration than pitch (Experiment 1, <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>). These cue weights were assessed with respect to the functional connectivity results reported above. Across this subset of participants, normalized cue weights were correlated with L-DLPFC &lt;=&gt; R insula connectivity (Spearman R = 0.78, p=0.000037), and L-DLPFC &lt;=&gt; R auditory cortex connectivity (Spearman R = 0.75, p=0.000154; <xref ref-type="fig" rid="fig4">Figure 4</xref>). This indicated that participants who relied least on pitch information to process speech had the weakest functional connectivity between these areas, while those who relied most on pitch had the strongest.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Connectivity between L DLPFC and insular (left) and auditory (right) cortex is modulated by normalized cue weights measured outside the scanner.</title><p>Correlation coefficients are Spearman rho.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Brain-behaviour correlations.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-53539-fig4-data1-v2.txt"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53539-fig4-v2.tif"/></fig><p>Although analyzing the control and amusic groups independently results in extremely small sample sizes, this pattern also held (albeit with ‘marginal significance’) within the 11 control participants alone, for both auditory cortex connectivity (R = 0.58, p=0.06) and insular connectivity (R = 0.55, p=0.08). Both these correlations were in the predicted direction, suggesting that even non-amusics may perform dimensional reweighting of acoustic dimensions and functional connectivity. Correlations within the (much more variable) amusic group alone were weaker and non-significant (although again, the group size is very small).</p></sec><sec id="s2-5"><title>Comparison with task-free data</title><p>To ensure that the pattern of connectivity we observed between groups (decreased right auditory cortex and right insula with L-DLPFC connectivity) was not due to intrinsic, task-irrelevant differences in neural architecture, the data from the language task was compared to that collected during passive listening to tone sequences. Whereas during speech perception amusic subjects showed reduced functional connectivity between left frontal and right insula/auditory ROIs relative to controls (p=0.0001 for both ROIs; in line with the whole-brain imaging analyses), this pattern did not hold during passive listening to tones (Amusia vs Control connectivity, p=0.29, Group (Amusic, Control) by Task (Speech Perception, Passive Tone Listening) interaction p=0.045 for the insula ROI; Amusia vs Control p=0.30, Group by Task interaction p=0.035 for the auditory cortex ROI - see <xref ref-type="fig" rid="fig5">Figure 5</xref>). These interactions suggest that our neural connectivity results are specifically linked to speech perception, rather than reflecting an overall connectivity difference between groups regardless of task state.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Connectivity between L DLPFC and right insula (left) and between L DLPFC and right auditory cortex (right) were reduced in the amusia group during speech perception (Control &gt;Amusia, p=0.0001 for both ROI pairs), but not during passive tone perception.</title><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>LDLPFC-ROI connectivity.</title></caption><media mime-subtype="plain" mimetype="text" xlink:href="elife-53539-fig5-data1-v2.txt"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-53539-fig5-v2.tif"/></fig></sec><sec id="s2-6"><title>Activation results for the speech processing task</title><p>Although we were concerned with functional connectivity rather than activation, we also tested for differences in activation levels between groups and conditions. False Discovery Rate correction was used to correct for multiple comparisons across both hemispheres for each test (Group, Condition and Group X Condition). No significant differences were detected for the main effects of group and condition, nor the interaction of those factors.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We found that individuals with amusia, who have been previously shown to rely less on pitch than controls to process spoken language (<xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>), exhibited decreased functional connectivity between left frontal areas and right hemisphere pitch-related regions. In our task, participants matched spoken sentences with visually presented sentences based on pitch, duration, or both these acoustic dimensions together. Using a data-driven approach, we identified four regions in left and right inferior frontal cortex for which the amusic group exhibited decreased functional connectivity with several other sites in frontal, temporal and occipital cortex. The most prominent of these results was decreased connectivity between left frontal regions classically implicated in language processing (left IFG and DLPFC) and right hemisphere regions —in the superior temporal gyrus and sulcus, Heschl’s gyrus, and anterior insula—that have been implicated in pitch processing (<xref ref-type="bibr" rid="bib26">Lee et al., 2011</xref>; <xref ref-type="bibr" rid="bib14">Garcea et al., 2017</xref>; <xref ref-type="bibr" rid="bib56">Warren et al., 2003</xref>; <xref ref-type="bibr" rid="bib17">Hohmann et al., 2018</xref>). We suggest that this decreased connectivity between right hemisphere pitch and left hemisphere frontal cortices may relate to the unreliability of the amusics’ perception of and memory for pitch. This is similar to the ‘weighted connections’ model of multisensory integration, where a more (or less) reliable modality is given a stronger (or weaker) weight (<xref ref-type="bibr" rid="bib3">Beauchamp et al., 2010</xref>).</p><p>Congenital amusia is often described as a disorder related to structural and functional connectivity within the right hemisphere, particularly between right inferior frontal cortices and right posterior temporal cortex (see <xref ref-type="bibr" rid="bib44">Peretz, 2016</xref> for review). Consistent with this proposal, we found in the present study that right inferior frontal cortex exhibited strongly decreased functional connectivity in the amusia group, and follow-up seed testing revealed that right auditory areas were involved as well. However, we also found that sites in <italic>left</italic> frontal cortex also showed large decreases in connectivity in amusia, also most prominently with right hemisphere auditory areas. Our results are consistent with an account that right hemisphere auditory areas are not only abnormally connected to right frontal areas (as observed during tonal tasks) but are less integrated with frontal left hemisphere regions when processing speech and language.</p><p>Our null results for group differences in activation during speech processing are consistent with prior reports that amusics and controls do not differ in pitch representations within sensory regions. For example, the extent of pitch-responsive regions within auditory cortex has been shown to be similar in participants with amusia and controls (<xref ref-type="bibr" rid="bib38">Norman-Haignere et al., 2016</xref>). Brainstem encoding of pitch in speech and musical stimuli is similarly unimpaired in individuals with amusia (<xref ref-type="bibr" rid="bib30">Liu et al., 2015b</xref>). Moreover, in oddball EEG paradigms, amusics show similar pre-attentive mismatch negativity responses to small pitch deviants, but impaired attention-dependent P300 responses (<xref ref-type="bibr" rid="bib35">Moreau et al., 2009</xref>; <xref ref-type="bibr" rid="bib43">Peretz et al., 2009</xref>; <xref ref-type="bibr" rid="bib34">Mignault Goulet et al., 2012</xref>; <xref ref-type="bibr" rid="bib36">Moreau et al., 2013</xref>). These findings, along with the fact that amusics show intact non-volitional behavioral responses (unconscious pitch shifts) when presented with pitch-altered feedback of their own voice (<xref ref-type="bibr" rid="bib19">Hutchins and Peretz, 2012</xref>), have been interpreted as evidence that amusia is a disorder of pitch awareness rather than one of low-level pitch processing (<xref ref-type="bibr" rid="bib43">Peretz et al., 2009</xref>), with differences in structural connectivity as one possible foundation of this putative impaired pitch awareness (<xref ref-type="bibr" rid="bib20">Hyde et al., 2006</xref>; <xref ref-type="bibr" rid="bib32">Loui et al., 2009</xref>; but see <xref ref-type="bibr" rid="bib6">Chen et al., 2015</xref>).</p><p>Our interpretation of differences in functional connectivity between amusics and controls diverges somewhat from these previous approaches: we argue that down-weighting of pitch information during perceptual categorization in both speech and music is adaptive, inasmuch as amusics have learned that pitch is an unreliable source of evidence relative to other perceptual dimensions. The evidence above suggesting that encoding of pitch in the brainstem and auditory cortex and pre-attentive responses to pitch changes are unaffected in amusia can be interpreted as suggesting that the fundamental deficit in amusia may not be increased perceptual noise or decreased pitch awareness but difficulties with retention of pitch information in memory (see <xref ref-type="bibr" rid="bib55">Tillmann et al., 2016</xref> for review). Our task arguably taxed working memory resources: in a similar paradigm performed by the same participants in quiet listening conditions (<xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>), the mean reaction time measured from the end of the second auditory stimulus was 1.64 s, indicating that participants needed some time to compare both auditory presentations and make their judgments. This interpretation is consistent with evidence suggesting that amusics have difficulty with pitch sequence processing tasks even when discrimination thresholds are accounted for (<xref ref-type="bibr" rid="bib54">Tillmann et al., 2009</xref>), as well as the finding that delaying the time interval between standard and comparison tones exacerbates pitch discrimination impairment in individuals with amusia (<xref ref-type="bibr" rid="bib59">Williamson et al., 2010</xref>). Moreover, the pitch awareness account of amusia cannot explain the <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref> finding that pitch cues are downweighted only during longer-scale suprasegmental speech perception, while pitch weighting is not different between amusics and controls during shorter-scale segmental speech perception, despite pitch cues being arguably more subtle in the segmental condition. However, this finding can be explained by the pitch memory account, as the suprasegmental task requires detection of and memory for pitch patterns within a complex sequence, while the segmental task does not. Furthermore, an account of amusia which suggests that the disorder primarily stems from differences in structural connectivity cannot account for the recent finding that functional connectivity patterns do not differ between amusics and controls during a verbal memory task (<xref ref-type="bibr" rid="bib2">Albouy et al., 2019</xref>), as well as our finding that amusics and controls show similar functional connectivity patterns during passive listening to tone sequences. We suggest, therefore, that amusics neglect pitch because they have implicitly learned that their memory for pitch is unreliable, and that this down-weighting of pitch is reflected in decreased functional connectivity between right auditory areas and downstream task-relevant areas which integrate information from perceptual regions. One way to test this hypothesis would be to examine functional connectivity during perceptual categorization of consonant-vowel syllables as voiced versus unvoiced based on a pitch cue (F0 of the following vowel) and a durational cue (voice onset time). We predict, based on our previous findings (<xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>), that functional connectivity will not differ between amusics and controls on this task, a finding which would not be predicted by the pitch awareness account of amusia.</p><p>We note that a previous fMRI study on amusia detected group differences in functional connectivity during passive listening to tones. That study used task-defined seed voxels in bilateral auditory cortex and found, in the amusia group, increased connectivity between left and right auditory cortex, but decreased connectivity between right auditory cortex and right inferior frontal gyrus (<xref ref-type="bibr" rid="bib21">Hyde et al., 2011</xref>). The present study does not necessarily clash with these findings, as we used different seed ROIs selected with a different procedure.</p><p>We did not observe any differences in functional connectivity between conditions in our speech task. This may be because our functional imaging protocol was timed to capture the peak in the BOLD signal corresponding to the presentation of the second auditory stimulus. Participants never knew (even implicitly) which acoustic dimension might be useful on any given trial until after they had heard both spoken sentences and needed to compare them to make their response. Furthermore, pitch fluctuations in the stimuli were above participants’ thresholds , even in the Duration-Informative condition (where the standard deviation of F0 over each spoken utterance was, on average, 2.7 semitones), and so it is unsurprising that functional connectivity did not change on a trial-by-trial basis, and instead the same ‘neural strategy’ was employed to process speech regardless of the trial type.</p><p>Several other future directions are suggested by our results, particularly for examining cue weighting during auditory/speech perception. In the multimodal integration studies mentioned above (<xref ref-type="bibr" rid="bib3">Beauchamp et al., 2010</xref>; <xref ref-type="bibr" rid="bib37">Nath and Beauchamp, 2011</xref>), reliability of two different sensory modalities was manipulated experimentally by severely degrading input channels with noise, resulting in changes in connectivity. Similarly, aspects of speech could be selectively masked with noise in order to make them less reliable, which in turn could cause corresponding changes in functional or effective connectivity. Indeed, behavioral work has indicated that when fundamental frequency (pitch) or durational aspects of speech are manipulated to be unreliable cues, categorization behavior shifts such that participants place less relative weight on the dimension that has been made less reliable (<xref ref-type="bibr" rid="bib18">Holt and Lotto, 2010</xref>). Certain groups, such as tone language speakers, are known to have fine-grained pitch perception abilities, and tend to place greater weight on pitch even when processing speech from a second, non-tonal language that they have learned (e.g. English; <xref ref-type="bibr" rid="bib61">Yu and Andruski, 2010</xref>; <xref ref-type="bibr" rid="bib64">Zhang and Francis, 2010</xref>, <xref ref-type="bibr" rid="bib63">Zhang et al., 2008</xref>; <xref ref-type="bibr" rid="bib47">Qin et al., 2017</xref>; <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>). Given the increased reliability of their pitch perception, tone language speakers may exhibit correspondingly high connectivity strength between right hemisphere auditory regions and left hemisphere ‘language regions’ when pitch cues are present (more so than native non-tonal language speakers). Expert musicians also have extensive pitch-related experience and training and could also serve as a population to examine in future work.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Participants, 15 individuals with amusia (10 F, age = 60.2 ± 9.4, range = 43–74) and 15 controls (10 F, age = 61.3 ± 10.4, range = 38–74), were recruited from the UK and were native British English speakers. The amusic group sample size reflected the maximum number of participants that could be screened and tested during our data collection period. The control group sample size was matched to this. All participants gave informed consent, and ethical approval was obtained from the relevant UCL and Birkbeck ethics committees. Amusia status was obtained using the Montreal Battery for the Evaluation of Amusia (MBEA). Participants with a composite score (summing the Scale, Contour and Interval tests scores) of 65 or less were classified as having amusia (<xref ref-type="bibr" rid="bib42">Peretz et al., 2003</xref>). We also note that the amusics defined using the MBEA had higher pitch thresholds than controls (Wilcoxon Rank Sum W = 29, p=0.001) but did not differ from controls in tone duration discrimination (W = 129, p=0.74), speech-in-noise threshold (W = 155.5, p=0.17), or audiometric hearing thresholds (t(28) = 1.33, p=0.20; see <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref> for detailed methods for these procedures).</p></sec><sec id="s4-2"><title>Stimuli</title><p>The stimuli were 42 compound sentences that consisted of a pre-posed subordinate clause followed by a main clause (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for an example, and <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>, <xref ref-type="bibr" rid="bib24">Jasmin et al., 2020b</xref> for details). There were two versions of each sentence: (1) an ‘early closure’ version, where the verb of the subordinate clause was used intransitively and the following noun was the subject of a new clause [‘After Jane dusts, the dining table [is clean]”]; and (2), ‘late closure’, where the verb was transitive and took the following noun as its object, moving the phrase boundary to a slightly later position in the sentence [‘After Jane dusts the dining table, [it is clean]”]. The words in both versions of the sentence were identical from the start of the sentence until the end of the second noun (‘After Jane dusts the dining table …”), and only the lexically identical portions of the sentences were presented to participants; thus the two stimuli did not differ in words spoken.</p><p>A native British English speaking male (who had previously trained as an actor) recorded early closure and late closure versions of each sentence in a sound-proofed room. The recordings were cropped such that only the portions with the same words remained, and silent pauses after phrase breaks were removed. Synthesized versions of these sentences were created with STRAIGHT voice-morphing software (<xref ref-type="bibr" rid="bib25">Kawahara and Irino, 2005</xref>). First, the two versions of the sentence were manually time-aligned by marking corresponding ‘anchor points’ in the two recordings. Then, morphed speech was synthesized by varying the degree to which the early closure and late closure recordings contributed duration and pitch information. We synthesized pairs of stimuli in three conditions: (1) In the Pitch-Informative condition, the stimulus pair had exactly the same durational properties (that is, the length of phonemes, syllables, and words was the average between the two original recordings) but the vocal pitch indicated early or late closure at a morphing level of 80%; (2) in the Duration-Informative condition, vocal pitch in the stimulus pair was identical (at 50% between both versions) but the durational characteristics indicated early or late closure at a morphing level of 80%; (3) in the Both-Informative condition, both pitch and time cued early or late closure simultaneously at 80%. The morphed speech varied only in duration and pitch, while all other aspects of the acoustics (such as amplitude and spectral characteristics other than pitch) were the same, held constant at 50% between the two original recordings during morphing. This stimulus set is freely available (<xref ref-type="bibr" rid="bib24">Jasmin et al., 2020b</xref>). Across all stimuli, F0 (vocal pitch) differences between early and late closure versions were large, with a mean of maximum difference of 7.7 semitones and range of 4.0–12.6 semitones. Thus, even the stimulus pair with the smallest pitch difference (4.0 semitones) exceeded the ~1.5 semitone pitch change detection threshold of the ‘most impaired’ participant in the amusia group (<xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>), which increased the chances that the amusia group would not suffer from poor performance, thereby avoiding a performance-related confound with our experimental design (see <xref ref-type="bibr" rid="bib7">Church et al., 2010</xref> for discussion).</p></sec><sec id="s4-3"><title>MRI data collection</title><p>Subjects were scanned with a Siemens Avanto 1.5 Tesla magnetic resonance imaging scanner with a 32-channel head coil, with sounds presented via Sensimetrics S14 earbuds, padded around the ear with NoMoCo memory foam cushions. Functional data were collected using a slow event-related design with sparse temporal sampling to allow presentation of auditory stimuli in quiet. We used an echo planar image sequence, with 40 slices, slice time 85 ms, slab tilted to capture the entire cerebrum and dorsal cerebellum, ascending sequential acquisition; 3 × 3 × 3 mm voxel size; silent stimulus and response period = 8.7 s, volume acquisition time = 3.4 s, total inter-trial interval = 12.1 s, flip angle = 90 degrees, bandwidth = 2298 Hz/pixel, echo time (TE) = 50 ms. After collecting functional runs, a high-resolution T1-weighted structural scan was collected (MPRAGE, 176 slices, sagittal acquisition, 2x GRAPPA acceleration, 1 mm isotropic voxels, acquisition matrix = 224 × 256).</p></sec><sec id="s4-4"><title>Procedure (see schematic in <xref ref-type="fig" rid="fig1">Figure 1</xref>)</title><p>Each run began with three dummy scans to allow magnetic stabilization. Each trial (repetition time) lasted 12.1 s. The start of each trial was triggered by a pulse corresponding to the start of a volume acquisition (which acquired neural data from the previous trial, at a delay). At <italic>t</italic> = 1 s into the trial, the sentence appeared on the screen; before scanning participants were instructed to read each sentence silently to themselves. At <italic>t</italic> = 5 s (plus or minus a random 100 ms jitter) participants heard a spoken version of the first part of the sentence. At <italic>t</italic> = 7.4 s (plus or minus 100 ms jitter) the second version was presented. The two spoken versions contained the same words but their pitch and/or timing characteristics cued a phrase boundary that occurred earlier or later in the sentence. Following this, there were approximately 2 s of silence during which the participant responded with the button box, before the scanner began acquiring the next volume at <italic>t</italic> = 12.1 s. Participants performed three blocks of 42 trials (14 each of Pitch-Informative, Duration-Informative, and Both-Informative) with 8 Rest trials interspersed within each block.</p></sec><sec id="s4-5"><title>Comparison task - passive listening to tones</title><p>Following data collection for this task and the structural scan, participants took part in two task-free fMRI scanning runs in which they watched a silent film (<italic>The General,</italic> starring Buster Keaton, or an episode of the <italic>Planet Earth</italic> series played without sound) while being presented auditorily with semi-random tone sequences. Stimuli consisted of sequences of ‘pips’ - 30 ms 6-harmonic complex tones. The fundamental frequencies of the pips were either 440, 466.16, 493.88 or 523.25 Hz, and the time between tone onsets was 0.075, 0.125, 0.175, or 0.225 s. The transition probabilities (determining whether pip N+1 had the same pitch or duration properties as pitch N) were set at either 0.1 and 0.9 for duration and either 0.3 and 0.7 for pitch. These two transition parameters were ‘crossed’ to create four design cells, and 25 random sequences were generated for each cell. MRI scanning parameters were identical to those used in the active, prosody task, except the time between volume acquisitions was 17.1 s. Participants listened to 100 tone sequences across two runs (50 per run). Matlab code used to create the stimuli can be found online (see Data Availability Statement).</p></sec><sec id="s4-6"><title>MRI pre-processing</title><p>Image preprocessing was performed with FreeSurfer 6.0.0 (<xref ref-type="bibr" rid="bib12">Fischl, 2012</xref>) and AFNI-SUMA 18.1.18 (<xref ref-type="bibr" rid="bib10">Cox, 1996</xref>). Anatomical images were registered to the third echo planar image of the first run using Freesurfer’s <italic>bbregister</italic> and processed with FreeSurfer’s automated pipeline for segmenting tissue types, generating cortical surface models, and parcellating subcortical structures. Masks of inferior colliculi were obtained by manually examining individual subjects’ anatomical images and selecting a single EPI voxel located at its centre, bilaterally. Freesurfer cortical surface models were imported to AFNI with the @SUMA_Make_Spec_FS program. Then a standard pre-processing pipeline using AFNI’s <italic>afniproc.py</italic> program was used: all echo planar image volumes were aligned to the third repetition time of the first run using AFNI’s 3DAllineate, intersected with the cortical surface with SUMA, smoothed along the surface with a 2D 6-mm-FWHM kernel, and converted to a standard mesh (std.141) for group analyses, separately for each hemisphere, where each vertex in the mesh (198812 per hemisphere) is aligned to the 'same' location in the cortex across subjects, using curvature-based morphing. Preprocessing of the passive listening experiment data was identical.</p></sec><sec id="s4-7"><title>Motion</title><p>The magnitude of transient head motion was calculated from the six motion parameters obtained during image realignment and aggregated as a single variable using AFNI's @1dDiffMag to calculate a Motion Index (<xref ref-type="bibr" rid="bib4">Berman et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Gotts et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Jasmin et al., 2019</xref>). This measure is similar to average Frame Displacement over a scan (<xref ref-type="bibr" rid="bib45">Power et al., 2012</xref>) and is in units of mm per repetition time. The difference in average motion between the groups was small (amusia group mean motion = 0.31 mm/TR; control group mean = 0.28 mm/TR) and amounted to 32 micrometers (~1/30<sup>th</sup> of a millimeter) per TR. The mean and distribution of motion did not differ statistically between groups (two sample <italic>t</italic>-test p=0.70, two-tailed).</p></sec><sec id="s4-8"><title>Beta series analysis of context-modulated functional connectivity</title><p>Given the previous reports (described above) of changes in connection strength between unimodal and multimodal areas in response to noise (<xref ref-type="bibr" rid="bib3">Beauchamp et al., 2010</xref>; <xref ref-type="bibr" rid="bib37">Nath and Beauchamp, 2011</xref>), we chose a connectivity-based analysis approach for our study. <italic>Beta series correlation</italic> (<xref ref-type="bibr" rid="bib48">Rissman et al., 2004</xref>) is a technique for examining functional connectivity and its modulation by task, using correlations in trial-by-trial responses. It has been shown to be more powerful than alternatives such as generalized psycho-physiological interaction (gPPI) for event-related designs (<xref ref-type="bibr" rid="bib8">Cisler et al., 2014</xref>). In a beta series analysis, one beta weight is calculated for each trial in the experiment (rather than for each condition). All the trial-wise betas associated with a given condition are then serially ordered to form a ‘beta series’. Finally, using the beta series in the same way as a standard BOLD fMRI time series, functional connectivity (measured as Pearson correlations) is calculated between seed regions of interest and the rest of the brain. Differences in functional connectivity can then be examined by comparing groups, comparing conditions, or examining the interaction of these factors.</p></sec><sec id="s4-9"><title>Obtaining trial-wise beta weights</title><p>Our experiment used a slow event-related design with a long repetition time (12.1 s) and sparse temporal sampling (with volume acquisition separated by silent periods). Therefore, the time between acquisitions was long enough for the haemodynamic response to return to baseline, and each echo planar image acquisition corresponded to exactly one trial (<xref ref-type="fig" rid="fig1">Figure 1</xref>). For this reason, we did not convolve the echo planar image time series with a basis function during subject-level statistical analysis (<xref ref-type="bibr" rid="bib16">Hall et al., 1999</xref>). In the design matrix for obtaining trial-wise betas, 126 column regressors were used (one for each non-rest trial). Each column vector was of length 150 (corresponding to all trials, including rest trials) and had a single ‘one’ in the position where the trial associated with that column occurred, while zeros were located in every other position. Polynomials up to second degree were also included in the model, on a run-wise basis, to remove the mean and any linear or quadratic trends. Fitting the trial regressors on a subject-wise basis resulted in cortical surface models of beta weights for each of the 126 trials, at each vertex on the reduced-vertex icosahedral cortical surface, with beta weights reflecting the neural response associated with that trial. As noted above, trial-wise betas were then serially ordered to form beta series separately for each of the three experimental conditions (Pitch-Informative, Duration-Informative, and Both-Informative) (<xref ref-type="bibr" rid="bib48">Rissman et al., 2004</xref>). Because there were 30 participants, this procedure resulted in a total of 90 beta series (30 participants × 3 conditions=90 beta series). As for the passive tone listening data, because all ‘trials’ were of the same type, it was not necessary to separate them into conditions and perform a first-level model to obtain betas. However, polynomials up to second degree were detrended from the pre-processed data (as was done with the task data).</p></sec><sec id="s4-10"><title>Defining seed regions of interest</title><p>Beta series analysis requires initial seed voxels, vertices, or regions to be identified, whose trial-to-trial changes in activity are then compared to those of the rest of the brain. Rather than choose <italic>a priori</italic> seeds derived from the literature, which used mainly musical tasks or resting state, we used a data-driven approach to search for the largest group and condition differences in functional connectivity (<xref ref-type="bibr" rid="bib4">Berman et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Cole et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">Gotts et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Jasmin et al., 2019</xref>; <xref ref-type="bibr" rid="bib33">Meoded et al., 2015</xref>; <xref ref-type="bibr" rid="bib50">Song et al., 2015</xref>; <xref ref-type="bibr" rid="bib51">Steel et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Stoddard et al., 2016</xref>; <xref ref-type="bibr" rid="bib57">Watsky et al., 2018</xref>). To do this, we first calculated the ‘whole-brain connectedness’ of each cortical vertex (a procedure available in AFNI as the <italic>3dTCorrMap</italic> function). The whole-brain connectedness of a given vertex is defined as the Pearson correlation of activity within that vertex/voxel and the average signal across all neural gray matter in the rest of the brain. Mathematically, this is equivalent to calculating thousands of Pearson correlations, of a given vertex/voxel series and every other vertex/voxel series in the brain, and then taking the mean of those correlations (<xref ref-type="bibr" rid="bib9">Cole et al., 2010</xref>), then repeating the process for every individual voxel/vertex. As such, it represents the global connectedness (or ‘global correlation’) of a vertex/voxel.</p><p>To calculate whole-brain connectedness, first, the average of trial-wise betas in gray matter across the brain was calculated in volume space, separately for each subject and for each condition, by running first-level (subject) models. The statistical models were identical to those conducted on the cortical surface, described above, but were performed on volumetric Talairach images instead of the cortical surfaces. The reason for this choice was so that voxels in cortex and subcortex would contribute equally to our measure of global (whole-brain) connectivity. First, the average gray-matter beta value was calculated for each trial by intersecting each image in the beta series with a whole-brain gray matter mask (which excluded white matter and ventricles) and calculating the average beta value within the mask (<xref ref-type="bibr" rid="bib15">Gotts et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Jasmin et al., 2019</xref>). Next, this gray matter average was correlated with each cortical surface vertex’s beta series, separately for each subject and condition, to obtain whole-brain connectedness maps. These values were then subjected to a statistical analysis based on our 2 (Group) × 3 (Condition) experimental design. Linear mixed effects models (AFNI’s <italic>3dLME</italic>) (<xref ref-type="bibr" rid="bib5">Chen et al., 2013</xref>) were constructed whose dependent variables were the vertex-wise whole-brain connectedness maps from each beta series. Group and Condition and their interaction were included as fixed effects. Participant was treated as a random intercept. Results of this step were corrected vertex-wise for multiple comparisons with False Discovery Rate (<italic>q &lt; </italic>0.05), separately for each test (Main Effect of Group; Main Effect of Condition; Interaction of Group by Condition) by pooling the p<italic>-</italic>values from both hemispheres’ cortical surfaces. This False Discovery Rate threshold corresponded to uncorrected p&lt;4 × 10<sup>−6</sup> for the Main Effect of Group. Four significant results (contiguous significant vertices) survived this threshold and were taken forward for the next analysis step. For the Main Effect of Condition and Interaction of Condition x Group, no results survived statistical correction at FDR (<italic>q</italic> &lt; 0.05). An analogous procedure was run on the passive tone listening data, in which whole-brain connectedness values were compared by group (amusic vs. control) in a linear mixed effects model. No significant FDR-corrected group differences were detected, nor at a reasonable uncorrected threshold of p&lt;0.001.</p><p>A similar procedure was performed for subcortical structures. Beta series were obtained for each subject, structure, and experimental condition, from their standard Freesurfer subcortical parcellations by masking the EPI data within each structure and calculating the average of the voxels. Each structure’s beta series was then correlated with the whole-brain gray matter beta average, separately for each condition, and the resulting values were subjected to linear mixed effects models with the same factors as above. Tests for Main Effect of Condition, of Group, and the Interaction of these factors was performed. All p-values were greater than p&gt;0.001 and no results survived an FDR-correction calculated over them.</p></sec><sec id="s4-11"><title>Follow-up seed-to-whole-brain testing</title><p>The first analysis step (seed definition, described above) identified which, if any, brain areas showed the largest connectivity differences between groups. However, this step is insufficient to localize the other specific regions driving this pattern. An analogy is in Analysis of Variance, where a significant omnibus test indicates a difference exists, but follow-up testing is required to determine where in the model differences exist (<xref ref-type="bibr" rid="bib15">Gotts et al., 2012</xref>). Thus, to locate the regions driving this pattern, we undertook a second step: follow-up seed-to-whole-brain testing (<xref ref-type="bibr" rid="bib9">Cole et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">Gotts et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Jasmin et al., 2019</xref>). Each seed region was examined with respect to its connectivity pattern with every cortical vertex and subcortical structure.</p><p>For each of the 90 beta series (30 subjects by three conditions), values within the seed vertices were averaged and then correlated with the beta series for every vertex in the brain. These correlations were Fisher Z-transformed and used as the dependent variables in linear mixed effects models (<italic>3dLME</italic>) with the same fixed and random effects as above. For each of the seeds, we tested for the group difference (Amusia vs Control) in connectivity. Results were False Discovery Rate corrected to (<italic>q</italic> &lt; 0.05) across all eight follow-up tests [four seeds × 2 hemispheres] corresponding to a threshold of p&lt;0.00035. Similarly, for the subcortical structures, each seed beta series was correlated with subcortical structure beta series, with resulting values subject to statistical testing. An FDR correction over all tests involving subcortex was applied. For display in figures, the data were converted from SUMA’s standard mesh (std.141) to Freesurfer’s standard surface (fsaverage) using AFNI’s SurfToSurf program and mapping values from the closest nodes (i.e. vertices).</p></sec><sec id="s4-12"><title>Correlation between functional connectivity and cue weights</title><p>To determine whether the functional connectivity patterns we observed were related to the importance placed on acoustic dimensions during prosodic categorization (cue weighting), the functional connectivity results were analyzed with respect to previously acquired cue weights obtained behaviorally from a subset of participants (<xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>). The right anterior insula and right auditory cortex results were used as ROIs (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The beta series for each ROI (averaged across vertices) was correlated with the beta series within the L-DLPFC seed area, separately for each condition, then averaged and Fisher Z-transformed. For the 21 participants for whom we had prosodic cue weight data (from <xref ref-type="bibr" rid="bib23">Jasmin et al., 2020a</xref>), these cue weights were analyzed with respect to the functional connectivity between the L-DLPFC seed and the two ROIs using Spearman correlations.</p></sec><sec id="s4-13"><title>Comparison between the speech task and passive tone listening</title><p>As described above, functional connectivity between L-DLPFC, and right auditory cortex and right insula was calculated using data from the passive tone listening task, using ROIs derived from the active speech perception task. After pre-processing and de-trending, the averaged value from the tone listening experiment within these ROIs was extracted, as well as the LDLPFC seed, for each experiment. Correlations between signal within the seed and the two ROIs was calculated and Fisher Z-transformed. As mentioned above, because all trials in the tone-listening experiment were analyzed as the same type, it was not necessary to use a first-level model to obtain trial-wise betas. Similarly for the data from the speech task, the average value within the seed region and both ROIs was extracted, separately for each of the 3 Beta series (Pitch-, Time- and Both-Informative), and the seed and ROI series were correlated. The mean of these three correlation coefficients was calculated and Fisher Z-transformed. Finally, statistics were performed using a mixed ANOVA with Experiment (Speech or Tones) as the within-subject factor and Group (Amusia or Control) as the between-subject factor.</p></sec><sec id="s4-14"><title>Analysis of activation</title><p>A standard General Linear Model comparing activation strength (rather than connectivity) was also conducted. As in the General Linear Model for obtaining beta weights, no basis function was used, and polynomials up to second degree were included in the models.</p></sec><sec id="s4-15"><title>Data availability</title><p>The data that support the findings of this study are openly available in the Birkbeck repository (<ext-link ext-link-type="uri" xlink:href="https://researchdata.bbk.ac.uk/65/">https://researchdata.bbk.ac.uk/65/</ext-link>), as are the speech stimuli (<xref ref-type="bibr" rid="bib24">Jasmin et al., 2020b</xref>; <ext-link ext-link-type="uri" xlink:href="https://researchdata.bbk.ac.uk/37/">https://researchdata.bbk.ac.uk/37/</ext-link>). The speech task can be demoed at the following link: (Gorilla Open Materials; <ext-link ext-link-type="uri" xlink:href="https://gorilla.sc/openmaterials/102786">https://gorilla.sc/openmaterials/102786</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank our study participants.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Visualization, Methodology</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All participants gave informed consent and ethical approval was obtained from the UCL Research Ethics Committee (fMRI/2016/001) and the Birkbeck Department of Psychology Research Ethics Committee (161711).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-53539-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The data that support the findings of this study are openly available in the Birkbeck repository (<ext-link ext-link-type="uri" xlink:href="https://researchdata.bbk.ac.uk/65/">https://researchdata.bbk.ac.uk/65/</ext-link>), as are the speech stimuli (Jasmin et al., 2020b; <ext-link ext-link-type="uri" xlink:href="https://researchdata.bbk.ac.uk/37/">https://researchdata.bbk.ac.uk/37/</ext-link>). The speech task can be demoed at the following link: (Gorilla Open Materials; <ext-link ext-link-type="uri" xlink:href="https://gorilla.sc/openmaterials/102786">https://gorilla.sc/openmaterials/102786</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Jasmin</surname><given-names>K</given-names></name><name><surname>Dick</surname><given-names>F</given-names></name><name><surname>Stewart</surname><given-names>L</given-names></name><name><surname>Tierney</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Altered functional connectivity during speech perception in congenital amusia</data-title><source>Birkbeck Research Data</source><pub-id assigning-authority="other" pub-id-type="doi">10.18743/DATA.00065</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albouy</surname> <given-names>P</given-names></name><name><surname>Mattout</surname> <given-names>J</given-names></name><name><surname>Bouet</surname> <given-names>R</given-names></name><name><surname>Maby</surname> <given-names>E</given-names></name><name><surname>Sanchez</surname> <given-names>G</given-names></name><name><surname>Aguera</surname> <given-names>PE</given-names></name><name><surname>Daligault</surname> <given-names>S</given-names></name><name><surname>Delpuech</surname> <given-names>C</given-names></name><name><surname>Bertrand</surname> <given-names>O</given-names></name><name><surname>Caclin</surname> <given-names>A</given-names></name><name><surname>Tillmann</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Impaired pitch perception and memory in congenital amusia: the deficit starts in the auditory cortex</article-title><source>Brain</source><volume>136</volume><fpage>1639</fpage><lpage>1661</lpage><pub-id pub-id-type="doi">10.1093/brain/awt082</pub-id><pub-id pub-id-type="pmid">23616587</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albouy</surname> <given-names>P</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name><name><surname>Bermudez</surname> <given-names>P</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Tillmann</surname> <given-names>B</given-names></name><name><surname>Caclin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Specialized neural dynamics for verbal and tonal memory: fmri evidence in congenital amusia</article-title><source>Human Brain Mapping</source><volume>40</volume><fpage>855</fpage><lpage>867</lpage><pub-id pub-id-type="doi">10.1002/hbm.24416</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname> <given-names>MS</given-names></name><name><surname>Pasalar</surname> <given-names>S</given-names></name><name><surname>Ro</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neural substrates of reliability-weighted visual-tactile multisensory integration</article-title><source>Frontiers in Systems Neuroscience</source><volume>4</volume><elocation-id>25</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2010.00025</pub-id><pub-id pub-id-type="pmid">20631844</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname> <given-names>RA</given-names></name><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>McAdams</surname> <given-names>HM</given-names></name><name><surname>Greenstein</surname> <given-names>D</given-names></name><name><surname>Lalonde</surname> <given-names>F</given-names></name><name><surname>Clasen</surname> <given-names>L</given-names></name><name><surname>Watsky</surname> <given-names>RE</given-names></name><name><surname>Shora</surname> <given-names>L</given-names></name><name><surname>Ordonez</surname> <given-names>AE</given-names></name><name><surname>Raznahan</surname> <given-names>A</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name><name><surname>Gogtay</surname> <given-names>N</given-names></name><name><surname>Rapoport</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Disrupted sensorimotor and social–cognitive networks underlie symptoms in childhood-onset schizophrenia</article-title><source>Brain</source><volume>139</volume><fpage>276</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1093/brain/awv306</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>G</given-names></name><name><surname>Saad</surname> <given-names>ZS</given-names></name><name><surname>Britton</surname> <given-names>JC</given-names></name><name><surname>Pine</surname> <given-names>DS</given-names></name><name><surname>Cox</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Linear mixed-effects modeling approach to FMRI group analysis</article-title><source>NeuroImage</source><volume>73</volume><fpage>176</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.01.047</pub-id><pub-id pub-id-type="pmid">23376789</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>JL</given-names></name><name><surname>Kumar</surname> <given-names>S</given-names></name><name><surname>Williamson</surname> <given-names>VJ</given-names></name><name><surname>Scholz</surname> <given-names>J</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Stewart</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Detection of the arcuate fasciculus in congenital amusia depends on the tractography algorithm</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00009</pub-id><pub-id pub-id-type="pmid">25653637</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Church</surname> <given-names>JA</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name><name><surname>Schlaggar</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The &quot;Task B problem&quot; and other considerations in developmental functional neuroimaging</article-title><source>Human Brain Mapping</source><volume>31</volume><fpage>852</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1002/hbm.21036</pub-id><pub-id pub-id-type="pmid">20496376</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisler</surname> <given-names>JM</given-names></name><name><surname>Bush</surname> <given-names>K</given-names></name><name><surname>Steele</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A comparison of statistical methods for detecting context-modulated functional connectivity in fMRI</article-title><source>NeuroImage</source><volume>84</volume><fpage>1042</fpage><lpage>1052</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.09.018</pub-id><pub-id pub-id-type="pmid">24055504</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname> <given-names>MW</given-names></name><name><surname>Pathak</surname> <given-names>S</given-names></name><name><surname>Schneider</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Identifying the brain's most globally connected regions</article-title><source>NeuroImage</source><volume>49</volume><fpage>3132</fpage><lpage>3148</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.001</pub-id><pub-id pub-id-type="pmid">19909818</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Pijper</surname> <given-names>JR</given-names></name><name><surname>Sanderman</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>On the perceptual strength of prosodic boundaries and its relation to suprasegmental cues</article-title><source>The Journal of the Acoustical Society of America</source><volume>96</volume><fpage>2037</fpage><lpage>2047</lpage><pub-id pub-id-type="doi">10.1121/1.410145</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frick</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Communicating emotion: the role of prosodic features</article-title><source>Psychological Bulletin</source><volume>97</volume><fpage>412</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.97.3.412</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcea</surname> <given-names>FE</given-names></name><name><surname>Chernoff</surname> <given-names>BL</given-names></name><name><surname>Diamond</surname> <given-names>B</given-names></name><name><surname>Lewis</surname> <given-names>W</given-names></name><name><surname>Sims</surname> <given-names>MH</given-names></name><name><surname>Tomlinson</surname> <given-names>SB</given-names></name><name><surname>Teghipco</surname> <given-names>A</given-names></name><name><surname>Belkhir</surname> <given-names>R</given-names></name><name><surname>Gannon</surname> <given-names>SB</given-names></name><name><surname>Erickson</surname> <given-names>S</given-names></name><name><surname>Smith</surname> <given-names>SO</given-names></name><name><surname>Stone</surname> <given-names>J</given-names></name><name><surname>Liu</surname> <given-names>L</given-names></name><name><surname>Tollefson</surname> <given-names>T</given-names></name><name><surname>Langfitt</surname> <given-names>J</given-names></name><name><surname>Marvin</surname> <given-names>E</given-names></name><name><surname>Pilcher</surname> <given-names>WH</given-names></name><name><surname>Mahon</surname> <given-names>BZ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Direct electrical stimulation in the human brain disrupts melody processing</article-title><source>Current Biology</source><volume>27</volume><fpage>2684</fpage><lpage>2691</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.07.051</pub-id><pub-id pub-id-type="pmid">28844645</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>Simmons</surname> <given-names>WK</given-names></name><name><surname>Milbury</surname> <given-names>LA</given-names></name><name><surname>Wallace</surname> <given-names>GL</given-names></name><name><surname>Cox</surname> <given-names>RW</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fractionation of social brain circuits in autism spectrum disorders</article-title><source>Brain</source><volume>135</volume><fpage>2711</fpage><lpage>2725</lpage><pub-id pub-id-type="doi">10.1093/brain/aws160</pub-id><pub-id pub-id-type="pmid">22791801</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname> <given-names>DA</given-names></name><name><surname>Haggard</surname> <given-names>MP</given-names></name><name><surname>Akeroyd</surname> <given-names>MA</given-names></name><name><surname>Palmer</surname> <given-names>AR</given-names></name><name><surname>Summerfield</surname> <given-names>AQ</given-names></name><name><surname>Elliott</surname> <given-names>MR</given-names></name><name><surname>Gurney</surname> <given-names>EM</given-names></name><name><surname>Bowtell</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>&quot;Sparse&quot; temporal sampling in auditory fMRI</article-title><source>Human Brain Mapping</source><volume>7</volume><fpage>213</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)7:3&lt;213::AID-HBM5&gt;3.0.CO;2-N</pub-id><pub-id pub-id-type="pmid">10194620</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hohmann</surname> <given-names>A</given-names></name><name><surname>Loui</surname> <given-names>P</given-names></name><name><surname>Li</surname> <given-names>CH</given-names></name><name><surname>Schlaug</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reverse engineering Tone-Deafness: disrupting Pitch-Matching by creating temporary dysfunctions in the Auditory-Motor network</article-title><source>Frontiers in Human Neuroscience</source><volume>12</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2018.00009</pub-id><pub-id pub-id-type="pmid">29441004</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holt</surname> <given-names>LL</given-names></name><name><surname>Lotto</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Speech perception as categorization</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>72</volume><fpage>1218</fpage><lpage>1227</lpage><pub-id pub-id-type="doi">10.3758/APP.72.5.1218</pub-id><pub-id pub-id-type="pmid">20601702</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutchins</surname> <given-names>S</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Amusics can imitate what they cannot discriminate</article-title><source>Brain and Language</source><volume>123</volume><fpage>234</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2012.09.011</pub-id><pub-id pub-id-type="pmid">23117156</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyde</surname> <given-names>KL</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Lerch</surname> <given-names>JP</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Morphometry of the amusic brain: a two-site study</article-title><source>Brain</source><volume>129</volume><fpage>2562</fpage><lpage>2570</lpage><pub-id pub-id-type="doi">10.1093/brain/awl204</pub-id><pub-id pub-id-type="pmid">16931534</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyde</surname> <given-names>KL</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional MRI evidence of an abnormal neural network for pitch processing in congenital amusia</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>292</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq094</pub-id><pub-id pub-id-type="pmid">20494966</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jasmin</surname> <given-names>K</given-names></name><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>Xu</surname> <given-names>Y</given-names></name><name><surname>Liu</surname> <given-names>S</given-names></name><name><surname>Riddell</surname> <given-names>CD</given-names></name><name><surname>Ingeholm</surname> <given-names>JE</given-names></name><name><surname>Kenworthy</surname> <given-names>L</given-names></name><name><surname>Wallace</surname> <given-names>GL</given-names></name><name><surname>Braun</surname> <given-names>AR</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Overt social interaction and resting state in young adult males with autism: core and contextual neural features</article-title><source>Brain</source><volume>142</volume><fpage>808</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1093/brain/awz003</pub-id><pub-id pub-id-type="pmid">30698656</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jasmin</surname> <given-names>K</given-names></name><name><surname>Dick</surname> <given-names>F</given-names></name><name><surname>Holt</surname> <given-names>LL</given-names></name><name><surname>Tierney</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Tailored perception: individuals’ speech and music perception strategies fit their perceptual abilities</article-title><source>Journal of Experimental Psychology: General</source><volume>149</volume><fpage>914</fpage><lpage>934</lpage><pub-id pub-id-type="doi">10.1037/xge0000688</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jasmin</surname> <given-names>K</given-names></name><name><surname>Dick</surname> <given-names>F</given-names></name><name><surname>Tierney</surname> <given-names>AT</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>The multidimensional battery of prosody perception (MBOPP)</article-title><source>Wellcome Open Research</source><volume>5</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.12688/wellcomeopenres.15607.1</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kawahara</surname> <given-names>H</given-names></name><name><surname>Irino</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>Underlying Principles of a High-quality Speech Manipulation System STRAIGHT and Its Application to Speech Segregation</chapter-title><person-group person-group-type="editor"><name><surname>Divenyi</surname> <given-names>P</given-names></name></person-group><source>Speech Separation by Humans and Machines</source><publisher-loc>Boston</publisher-loc><publisher-name>Kluwer Academic Publishers</publisher-name><fpage>167</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1007/0-387-22794-6_11</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>YS</given-names></name><name><surname>Janata</surname> <given-names>P</given-names></name><name><surname>Frost</surname> <given-names>C</given-names></name><name><surname>Hanke</surname> <given-names>M</given-names></name><name><surname>Granger</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Investigation of melodic contour processing in the brain using multivariate pattern-based fMRI</article-title><source>NeuroImage</source><volume>57</volume><fpage>293</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.02.006</pub-id><pub-id pub-id-type="pmid">21315158</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leveque</surname> <given-names>Y</given-names></name><name><surname>Fauvel</surname> <given-names>B</given-names></name><name><surname>Groussard</surname> <given-names>M</given-names></name><name><surname>Caclin</surname> <given-names>A</given-names></name><name><surname>Albouy</surname> <given-names>P</given-names></name><name><surname>Platel</surname> <given-names>H</given-names></name><name><surname>Tillmann</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Altered intrinsic connectivity of the auditory cortex in congenital amusia</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>88</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1152/jn.00663.2015</pub-id><pub-id pub-id-type="pmid">27009161</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>F</given-names></name><name><surname>Patel</surname> <given-names>AD</given-names></name><name><surname>Fourcin</surname> <given-names>A</given-names></name><name><surname>Stewart</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Intonation processing in congenital amusia: discrimination, identification and imitation</article-title><source>Brain</source><volume>133</volume><fpage>1682</fpage><lpage>1693</lpage><pub-id pub-id-type="doi">10.1093/brain/awq089</pub-id><pub-id pub-id-type="pmid">20418275</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>F</given-names></name><name><surname>Jiang</surname> <given-names>C</given-names></name><name><surname>Wang</surname> <given-names>B</given-names></name><name><surname>Xu</surname> <given-names>Y</given-names></name><name><surname>Patel</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>A music perception disorder (congenital amusia) influences speech comprehension</article-title><source>Neuropsychologia</source><volume>66</volume><fpage>111</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.11.001</pub-id><pub-id pub-id-type="pmid">25445781</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>F</given-names></name><name><surname>Maggu</surname> <given-names>AR</given-names></name><name><surname>Lau</surname> <given-names>JCY</given-names></name><name><surname>Wong</surname> <given-names>PCM</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Brainstem encoding of speech and musical stimuli in congenital amusia: evidence from cantonese speakers</article-title><source>Frontiers in Human Neuroscience</source><volume>8</volume><elocation-id>35</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.01029</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lolli</surname> <given-names>SL</given-names></name><name><surname>Lewenstein</surname> <given-names>AD</given-names></name><name><surname>Basurto</surname> <given-names>J</given-names></name><name><surname>Winnik</surname> <given-names>S</given-names></name><name><surname>Loui</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Sound frequency affects speech emotion perception: results from congenital amusia</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>1340</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.01340</pub-id><pub-id pub-id-type="pmid">26441718</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loui</surname> <given-names>P</given-names></name><name><surname>Alsop</surname> <given-names>D</given-names></name><name><surname>Schlaug</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Tone deafness: a new disconnection syndrome?</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>10215</fpage><lpage>10220</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1701-09.2009</pub-id><pub-id pub-id-type="pmid">19692596</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meoded</surname> <given-names>A</given-names></name><name><surname>Morrissette</surname> <given-names>AE</given-names></name><name><surname>Katipally</surname> <given-names>R</given-names></name><name><surname>Schanz</surname> <given-names>O</given-names></name><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>Floeter</surname> <given-names>MK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cerebro-cerebellar connectivity is increased in primary lateral sclerosis</article-title><source>NeuroImage: Clinical</source><volume>7</volume><fpage>288</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1016/j.nicl.2014.12.009</pub-id><pub-id pub-id-type="pmid">25610792</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mignault Goulet</surname> <given-names>G</given-names></name><name><surname>Moreau</surname> <given-names>P</given-names></name><name><surname>Robitaille</surname> <given-names>N</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Congenital amusia persists in the developing brain after daily music listening</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e36860</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0036860</pub-id><pub-id pub-id-type="pmid">22606299</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreau</surname> <given-names>P</given-names></name><name><surname>Jolicoeur</surname> <given-names>P</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Automatic brain responses to pitch changes in congenital amusia</article-title><source>Annals of the New York Academy of Sciences</source><volume>1169</volume><fpage>191</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2009.04775.x</pub-id><pub-id pub-id-type="pmid">19673779</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreau</surname> <given-names>P</given-names></name><name><surname>Jolicœur</surname> <given-names>P</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Pitch discrimination without awareness in congenital amusia: evidence from event-related potentials</article-title><source>Brain and Cognition</source><volume>81</volume><fpage>337</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2013.01.004</pub-id><pub-id pub-id-type="pmid">23434917</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname> <given-names>AR</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Dynamic changes in superior temporal sulcus connectivity during perception of noisy audiovisual speech</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>1704</fpage><lpage>1714</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4853-10.2011</pub-id><pub-id pub-id-type="pmid">21289179</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname> <given-names>SV</given-names></name><name><surname>Albouy</surname> <given-names>P</given-names></name><name><surname>Caclin</surname> <given-names>A</given-names></name><name><surname>McDermott</surname> <given-names>JH</given-names></name><name><surname>Kanwisher</surname> <given-names>NG</given-names></name><name><surname>Tillmann</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Pitch-Responsive cortical regions in congenital amusia</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>2986</fpage><lpage>2994</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2705-15.2016</pub-id><pub-id pub-id-type="pmid">26961952</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname> <given-names>AD</given-names></name><name><surname>Wong</surname> <given-names>M</given-names></name><name><surname>Foxton</surname> <given-names>J</given-names></name><name><surname>Lochy</surname> <given-names>A</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Speech intonation perception deficits in musical tone deafness (CONGENITAL amusia)</article-title><source>Music Perception</source><volume>25</volume><fpage>357</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1525/mp.2008.25.4.357</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Can nonlinguistic musical training change the way the brain processes speech? the expanded OPERA hypothesis</article-title><source>Hearing Research</source><volume>308</volume><fpage>98</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2013.08.011</pub-id><pub-id pub-id-type="pmid">24055761</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peretz</surname> <given-names>I</given-names></name><name><surname>Ayotte</surname> <given-names>J</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Mehler</surname> <given-names>J</given-names></name><name><surname>Ahad</surname> <given-names>P</given-names></name><name><surname>Penhune</surname> <given-names>VB</given-names></name><name><surname>Jutras</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Congenital amusia: a disorder of fine-grained pitch discrimination</article-title><source>Neuron</source><volume>33</volume><fpage>185</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00580-3</pub-id><pub-id pub-id-type="pmid">11804567</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peretz</surname> <given-names>I</given-names></name><name><surname>Champod</surname> <given-names>AS</given-names></name><name><surname>Hyde</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Varieties of musical disorders. the Montreal battery of evaluation of amusia</article-title><source>Annals of the New York Academy of Sciences</source><volume>999</volume><fpage>58</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1196/annals.1284.006</pub-id><pub-id pub-id-type="pmid">14681118</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peretz</surname> <given-names>I</given-names></name><name><surname>Brattico</surname> <given-names>E</given-names></name><name><surname>Järvenpää</surname> <given-names>M</given-names></name><name><surname>Tervaniemi</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The amusic brain: in tune, out of key, and unaware</article-title><source>Brain</source><volume>132</volume><fpage>1277</fpage><lpage>1286</lpage><pub-id pub-id-type="doi">10.1093/brain/awp055</pub-id><pub-id pub-id-type="pmid">19336462</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peretz</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neurobiology of congenital amusia</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>857</fpage><lpage>867</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.09.002</pub-id><pub-id pub-id-type="pmid">27692992</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname> <given-names>JD</given-names></name><name><surname>Barnes</surname> <given-names>KA</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Schlaggar</surname> <given-names>BL</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion</article-title><source>NeuroImage</source><volume>59</volume><fpage>2142</fpage><lpage>2154</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.018</pub-id><pub-id pub-id-type="pmid">22019881</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pralus</surname> <given-names>A</given-names></name><name><surname>Fornoni</surname> <given-names>L</given-names></name><name><surname>Bouet</surname> <given-names>R</given-names></name><name><surname>Gomot</surname> <given-names>M</given-names></name><name><surname>Bhatara</surname> <given-names>A</given-names></name><name><surname>Tillmann</surname> <given-names>B</given-names></name><name><surname>Caclin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Emotional prosody in congenital amusia: impaired and spared processes</article-title><source>Neuropsychologia</source><volume>134</volume><elocation-id>107234</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2019.107234</pub-id><pub-id pub-id-type="pmid">31647961</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname> <given-names>Z</given-names></name><name><surname>Chien</surname> <given-names>Y</given-names></name><name><surname>Tremblay</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Processing of word-level stress by Mandarin-speaking second language learners of english</article-title><source>Applied Psycholinguistics</source><volume>38</volume><fpage>541</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1017/S0142716416000321</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rissman</surname> <given-names>J</given-names></name><name><surname>Gazzaley</surname> <given-names>A</given-names></name><name><surname>D'Esposito</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Measuring functional connectivity during distinct stages of a cognitive task</article-title><source>NeuroImage</source><volume>23</volume><fpage>752</fpage><lpage>763</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.06.035</pub-id><pub-id pub-id-type="pmid">15488425</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname> <given-names>T</given-names></name><name><surname>Noppeney</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reliability-Weighted integration of audiovisual signals can be modulated by Top-down attention</article-title><source>Eneuro</source><volume>5</volume><elocation-id>ENEURO.0315-17.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0315-17.2018</pub-id><pub-id pub-id-type="pmid">29527567</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname> <given-names>S</given-names></name><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>Dayan</surname> <given-names>E</given-names></name><name><surname>Cohen</surname> <given-names>LG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Practice structure improves unconscious transitional memories by increasing synchrony in a premotor network</article-title><source>Journal of Cognitive Neuroscience</source><volume>27</volume><fpage>1503</fpage><lpage>1512</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00796</pub-id><pub-id pub-id-type="pmid">25761004</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steel</surname> <given-names>A</given-names></name><name><surname>Song</surname> <given-names>S</given-names></name><name><surname>Bageac</surname> <given-names>D</given-names></name><name><surname>Knutson</surname> <given-names>KM</given-names></name><name><surname>Keisler</surname> <given-names>A</given-names></name><name><surname>Saad</surname> <given-names>ZS</given-names></name><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>Wassermann</surname> <given-names>EM</given-names></name><name><surname>Wilkinson</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Shifts in connectivity during procedural learning after motor cortex stimulation: a combined transcranial magnetic stimulation/functional magnetic resonance imaging study</article-title><source>Cortex</source><volume>74</volume><fpage>134</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.10.004</pub-id><pub-id pub-id-type="pmid">26673946</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoddard</surname> <given-names>J</given-names></name><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>Brotman</surname> <given-names>MA</given-names></name><name><surname>Lever</surname> <given-names>S</given-names></name><name><surname>Hsu</surname> <given-names>D</given-names></name><name><surname>Zarate</surname> <given-names>C</given-names></name><name><surname>Ernst</surname> <given-names>M</given-names></name><name><surname>Pine</surname> <given-names>DS</given-names></name><name><surname>Leibenluft</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Aberrant intrinsic functional connectivity within and between corticostriatal and temporal-parietal networks in adults and youth with bipolar disorder</article-title><source>Psychological Medicine</source><volume>46</volume><fpage>1509</fpage><lpage>1522</lpage><pub-id pub-id-type="doi">10.1017/S0033291716000143</pub-id><pub-id pub-id-type="pmid">26924633</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Streeter</surname> <given-names>LA</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Acoustic determinants of phrase boundary perception</article-title><source>The Journal of the Acoustical Society of America</source><volume>64</volume><fpage>1582</fpage><lpage>1592</lpage><pub-id pub-id-type="doi">10.1121/1.382142</pub-id><pub-id pub-id-type="pmid">739094</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tillmann</surname> <given-names>B</given-names></name><name><surname>Schulze</surname> <given-names>K</given-names></name><name><surname>Foxton</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Congenital amusia: a short-term memory deficit for non-verbal, but not verbal sounds</article-title><source>Brain and Cognition</source><volume>71</volume><fpage>259</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2009.08.003</pub-id><pub-id pub-id-type="pmid">19762140</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tillmann</surname> <given-names>B</given-names></name><name><surname>Lévêque</surname> <given-names>Y</given-names></name><name><surname>Fornoni</surname> <given-names>L</given-names></name><name><surname>Albouy</surname> <given-names>P</given-names></name><name><surname>Caclin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Impaired short-term memory for pitch in congenital amusia</article-title><source>Brain Research</source><volume>1640</volume><fpage>251</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2015.10.035</pub-id><pub-id pub-id-type="pmid">26505915</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname> <given-names>JD</given-names></name><name><surname>Uppenkamp</surname> <given-names>S</given-names></name><name><surname>Patterson</surname> <given-names>RD</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Separating pitch chroma and pitch height in the human brain</article-title><source>PNAS</source><volume>100</volume><fpage>10038</fpage><lpage>10042</lpage><pub-id pub-id-type="doi">10.1073/pnas.1730682100</pub-id><pub-id pub-id-type="pmid">12909719</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watsky</surname> <given-names>RE</given-names></name><name><surname>Gotts</surname> <given-names>SJ</given-names></name><name><surname>Berman</surname> <given-names>RA</given-names></name><name><surname>McAdams</surname> <given-names>HM</given-names></name><name><surname>Zhou</surname> <given-names>X</given-names></name><name><surname>Greenstein</surname> <given-names>D</given-names></name><name><surname>Lalonde</surname> <given-names>FM</given-names></name><name><surname>Gochman</surname> <given-names>P</given-names></name><name><surname>Clasen</surname> <given-names>LS</given-names></name><name><surname>Shora</surname> <given-names>L</given-names></name><name><surname>Ordóñez</surname> <given-names>AE</given-names></name><name><surname>Gogtay</surname> <given-names>N</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name><name><surname>Barch</surname> <given-names>DM</given-names></name><name><surname>Rapoport</surname> <given-names>JL</given-names></name><name><surname>Liu</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Attenuated resting-state functional connectivity in patients with childhood- and adult-onset schizophrenia</article-title><source>Schizophrenia Research</source><volume>197</volume><fpage>219</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1016/j.schres.2018.01.003</pub-id><pub-id pub-id-type="pmid">29310911</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whiteford</surname> <given-names>KL</given-names></name><name><surname>Oxenham</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning for pitch and melody discrimination in congenital amusia</article-title><source>Cortex</source><volume>103</volume><fpage>164</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2018.03.012</pub-id><pub-id pub-id-type="pmid">29655041</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williamson</surname> <given-names>VJ</given-names></name><name><surname>McDonald</surname> <given-names>C</given-names></name><name><surname>Deutsch</surname> <given-names>D</given-names></name><name><surname>Griffiths</surname> <given-names>TD</given-names></name><name><surname>Stewart</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Faster decline of pitch memory over time in congenital amusia</article-title><source>Advances in Cognitive Psychology</source><volume>6</volume><fpage>15</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.2478/v10053-008-0073-5</pub-id><pub-id pub-id-type="pmid">20689638</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winter</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Spoken language achieves robustness and evolvability by exploiting degeneracy and neutrality</article-title><source>BioEssays</source><volume>36</volume><fpage>960</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1002/bies.201400028</pub-id><pub-id pub-id-type="pmid">25088374</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>VY</given-names></name><name><surname>Andruski</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>A cross-language study of perception of lexical stress in english</article-title><source>Journal of Psycholinguistic Research</source><volume>39</volume><fpage>323</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1007/s10936-009-9142-2</pub-id><pub-id pub-id-type="pmid">20033291</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zendel</surname> <given-names>BR</given-names></name><name><surname>Lagrois</surname> <given-names>MÉ</given-names></name><name><surname>Robitaille</surname> <given-names>N</given-names></name><name><surname>Peretz</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Attending to pitch information inhibits processing of pitch information: the curious case of amusia</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>3815</fpage><lpage>3824</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3766-14.2015</pub-id><pub-id pub-id-type="pmid">25740512</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Nissen</surname> <given-names>SL</given-names></name><name><surname>Francis</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Acoustic characteristics of english lexical stress produced by native mandarin speakers</article-title><source>The Journal of the Acoustical Society of America</source><volume>123</volume><fpage>4498</fpage><lpage>4513</lpage><pub-id pub-id-type="doi">10.1121/1.2902165</pub-id><pub-id pub-id-type="pmid">18537399</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Francis</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The weighting of vowel quality in native and non-native listeners’ perception of English lexical stress</article-title><source>Journal of Phonetics</source><volume>38</volume><fpage>260</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1016/j.wocn.2009.11.002</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53539.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Oxenham</surname><given-names>Andrew J</given-names></name><role>Reviewing Editor</role><aff><institution>University of Minnesota</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>We found this paper to be an interesting demonstration of a link between behavior and functional brain connectivity when comparing people with amusia and a control group. The results suggest a potential neural basis for the down-weighting of pitch cues in people with amusia that has parallels with recent findings involving cue weighting between sensory modalities. The work opens up the possibility of further such studies that might tackle neural correlates of individual differences in cue weighting within a single sensory modality in the wider population.</p><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted an appeal for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your work entitled &quot;Altered functional connectivity during speech perception in congenital amusia&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by a Senior Editor, a Reviewing Editor, and two reviewers. The reviewers have opted to remain anonymous.</p><p>Our decision has been reached after consultation between the reviewers. Based on these discussions and the individual reviews below, we regret to inform you that your work will not be considered further for publication in <italic>eLife</italic>.</p><p>Although both reviewers found the scientific question and hypothesis very interesting and the data intriguing, both noted a lack of a brain-behavior relationship in the current study. It is possible that additional data could address the concerns outlined below. However, because of the substantial nature of such data collection, it would warrant a new submission rather than a revision.</p><p><italic>Reviewer #1:</italic></p><p>Jasmin et al. present an fMRI study on speech in congenital amusia. 15 amusics and 15 controls were scanned in the fMRI while they matched auditory and visual sentences, presumably using different acoustic cues.</p><p>I think the article sets out to address a fascinating puzzle. The puzzle is that although pitch information is in both music and language, people with amusia, who have trouble with pitch information in music, do not report difficulties in naturalistic speech perception. Here, the authors hypothesize that amusics calibrate their perception by down-weighing the cues that are less reliable for them, i.e. pitch, relying instead on other cues that co-occur with pitch.</p><p>I think the hypothesis is a fine and thoughtful one.</p><p>During task fMRI, amusics showed lower functional connectivity in left and right prefrontal regions, and between left prefrontal regions and right auditory and anterior insula. But the results as presented right now don't show any relationship between task performance and the neural findings. I would really like to see some brain-behavioral correlations to identify the specific compensatory mechanisms that amusics might be using to approach the task of sentence processing. WIthout that, it is unclear whether and how the findings in fMRI connectivity differences pertain to the task. Especially since there were no task-related differences in behavior nor were there any differences in task-related activations, one wonders if a task-free resting state fMRI would have shown the same results, thus suggesting intrinsic connectivity differences rather than addressing the differential weighting mechanisms so interestingly described in the Introduction.</p><p><italic>Reviewer #2:</italic></p><p>In this paper Jasmin et al. present fMRI data from 15 persons with amusia and 15 controls as they perform a linguistic task that requires judgments based either on pitch cues or duration cues, or both. The findings hinge on the fact that functional connectivity differs for the two groups, first in terms of global connectedness, in four frontal regions, two in each hemisphere, and second in terms of inter-regional functional connectivity between these frontal regions and posterior regions, including auditory areas. The authors interpret these findings in terms of their idea that persons with amusia down-regulate pitch cues because they know them to be unreliable, and this is reflected by the functional connectivity differences.</p><p>The study is well-conducted and analyzed, and the paper is well-written. But I am not persuaded about the interpretation of the results. The main problem is the lack of any explicit brain-behavior relationship. This issue takes two forms. First, the brain data and the behavioral data are never related to one another. The authors make the claim that the functional connectivity differences are related to the behavior, but this is not shown by any analysis. Without any such link, it is hard to know whether the connectivity differences observed have anything to do with the particular ability sampled by this task, or if they are related to something else entirely. Second, the behavior itself does not differ across participant groups. The authors specifically desired this outcome so that any imaging differences would not be driven by group behavioral differences. But that idea does not make sense to me because there were two conditions, one using pitch cues and another using duration cues. Presumably, amusic persons have trouble with the former but not the latter. Yet this was not the case in the behavior (no condition X group interaction), nor in the imaging data (the FC effects were not influenced by condition either). So if the FC results are somehow related to the downweighting of pitch cues it should show up specifically in the condition where such cues were available, and not in the condition where duration cues were available. Without this crucial dissociation, the interpretation does not hold together.</p><p>In the Discussion section the authors argue that the deficit in amusia has more to do with tonal retention than perception, an idea which is supported by other authors, although the bulk of the evidence suggests that both features are present. But it is not entirely clear why a retention deficit would be so important in the task used, which does not appear to require a lot of working memory since the spoken stimuli simply have to be matched to a visual representation of the sentence. Another related issue is that in amusia there is often a wide range of perceptual performance, with some individuals scoring near chance on pitch discrimination and others scoring at the same level as non-amusic persons. It's not clear how that heterogeneity may have influenced performance on this task, especially as the pitch intervals used in the stimuli are not documented in the manuscript, which seems like a significant omission. It would be important to know whether the excursions in F0 for the pitch-varying sentences (which are typically much larger than one or two semitones) were smaller than the thresholds that the amusics can perceive or not. In general, there is no discussion of individual differences in the data, which might have helped to understand the findings.</p><p>[Editors’ note: what follows is the editors’ decision letter after consideration of the letter of appeal. Further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for choosing to send your work entitled &quot;Altered functional connectivity during speech perception in congenital amusia&quot; for consideration at eLife. Your letter of appeal has been considered by a Senior Editor and a Reviewing editor, and we are prepared to consider a revised submission with no guarantees of acceptance.</p><p>Beyond the new data, please be sure to address other concerns of the reviewers, including the lack of interaction between group and condition in the behavior and in the imaging data. Perhaps the lack of interaction in the behavior is due to ceiling performance in both groups (please show the data), but if the functional connectivity results are related to the downweighting of pitch cues in amusics, shouldn't this only manifest itself in the condition where such cues were available, and not in the condition where duration cues were available? Ensuring that the data are fully compatible with the interpretation will play a critical role in a final decision.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your revised article &quot;Altered functional connectivity during speech perception in congenital amusia&quot; for consideration by eLife. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by Andrew Oxenham as Reviewing Editor and Barbara Shinn-Cunningham as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Anne Caclin (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Based on the reviews and the discussions between myself and the reviewers, we have come to the conclusion that the manuscript is potentially publishable in <italic>eLife</italic> but will need some further revision before it can be accepted. The areas where revisions are necessary are listed below.</p><p>Essential revisions:</p><p>Brain-behavior correlations. The inclusion of the current Figure 5 certainly strengthens the manuscript and is an important addition. However, because the correlation is driven primarily by the between-group differences, the importance of the overall correlation should not be overstated. In particular, it is probably not appropriate to mention it in the abstract without the necessary caveats. Instead just a statement, saying that the between-group differences in connectivity reflect the between-group differences in performance in the same groups of listeners, would suffice.</p><p>Behavior task. There are still some questions remaining on this front. In particular, there remain some questions as to why the amusics should show the same amount of behavioral gain when pitch cues are added. Is this thought to be because the pitch changes are so large as to influence performance despite impaired pitch processing? A little more discussion of this aspect of the results will likely help readers follow the logic better. There is also some skepticism regarding when and whether participants knew which cues were informative on each trial (Discussion section). For instance, in many cases participants would probably have been able to complete the task following just the first interval, rather than have to wait for the second interval and make a comparison. Although the 2AFC paradigm makes the analysis of performance easier, it cannot be assumed that participants always make full use (or need to make use) of the information from both intervals. This observation has some implications for the extent to which memory processes are involved.</p><p>Additional analysis. There are two areas where additional analyses may be illuminated. First, the main conclusions of the study rely in part on the results of the correlations between pitch cue weights and connectivity measures. Duration cue weights, which do not differ between groups, were also measured. It appears important, as a negative control, to comment on the absence (or not) of correlations between these duration cue weights and connectivity measures (subsection “Correlations between functional connectivity levels and prosodic cue weights”).</p><p>Second, the comparison of connectivity between the active and passive (tones) task was based on the ROIs derived from the passive task, with the assumption that the passive task should not result in differences in connectivity. However, as far as we could tell, the initial data-driven connectivity tests were not run separately on the passive task. If that were done just on the passive task, would any connectivity differences between the groups emerge? If not, that would provide a further important negative control.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.53539.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors appealed the original decision. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Although both reviewers found the scientific question and hypothesis very interesting and the data intriguing, both noted a lack of a brain-behavior relationship in the current study. It is possible that additional data could address the concerns outlined below. However, because of the substantial nature of such data collection, it would warrant a new submission rather than a revision.</p></disp-quote><p>Both you and the reviewers flagged up the lack of a reported brain-behaviour correlation as the primary weakness of the study, and you suggested that because collecting the data required would be a substantial task and revision, a new submission would be warranted. Reviewer 1 also noted a lack of comparison with another task or with resting state as a weakness.</p><p>Because the reviewers seemed otherwise quite enthusiastic about the work, we thought we should write to make you aware that we have existing data that speaks directly to both of these issues, collected for other studies, but with the same participants. In retrospect, we should have included these analyses originally. The only reason we did not was in the interest of scope. Nevertheless, we agree with both reviewers that these are very pertinent to our interpretation and are pleased to include them now.</p><p>1) Brain-behaviour correlations</p><p>Of the 30 participants in the study we submitted to <italic>eLife</italic>, 21 took part in an experiment that measured the degree to which they relied on pitch versus duration to categorize prosody, i.e. their ‘normalized prosodic cue weights’ (Experiment 1, Jasmin, et al., 2019, Journal of Experimental Psychology: General). We have analyzed the key results from the imaging analysis in the present work – connectivity between left dorsolateral prefrontal (L-DLPFC) and right auditory and insular cortices during speech perception – with respect to these cue weights, within these 21 participants. Across this subset of participants, normalized pitch cue weights were associated with L-DLPFC-R auditory cortex connectivity (Spearman R = 0.78, p=.000037), and LDLPFC-R insula connectivity (Spearman R = 0.75, p=.000154; Figure 5). We used non-parametric analyses because the data appear to be heteroskedastic, but the Pearson correlations are also significant.</p><p>Although analyzing the control and amusic groups independently results in extremely small sample sizes, this pattern also held (albeit with “marginal” significance) within the 11 control participants alone, for both auditory connectivity (R = 0.58, p=.06) and insular connectivity (R=.55, p=.08). These were both in the predicted direction, suggesting that even non-amusics may perform dimensional reweighting. Correlations within the (much more variable) amusic group alone were weaker and non-significant (although again, the group size is very small).</p><p>Since the phenomenon we set out to explain in this study is the downweighting of pitch cues in amusia, it seems natural that cue weights themselves are the most appropriate behavioural measure to compare with the neural data. They were also measured with a high degree of precision, in a quiet laboratory setting and over 490 individual trials. However, we note that reviewer 2 suggested that we correlate our in-scanner behavioural measure with the neural data. The reason we did not use this data is that the in-scanner test was designed to be relatively easy, with the intention of matching the groups on performance, so as to avoid the confound of in-scanner behavioural differences potentially driving differential activation (a topic often discussed in the developmental fMRI literature, e.g., Church, Schlaggar and Petersen, 2010). Indeed, the groups did not differ in performance, even on the Pitch condition.</p><p>As a further ‘check’ that the neural differences were related specifically to cue weighting, we also correlated these ROI data with data from a test of the ability to perceive linguistic focus from pitch cues, a measure on which these participants had previously been shown to differ (Jasmin et al., 2019). These correlations were not significant across all 30 participants, either for the insula (r = 0.2, p=0.3) or auditory cortex data (r = 0.14, p=0.46).</p><p>2) Comparison with task-free data</p><p>Reviewer 1 flagged that our results could have reflected an ‘intrinsic’ difference in connectivity that might persist even in a resting state, in which case one could not claim that they are specifically related to more active speech or auditory processing perception. We have (unpublished) additional data that speaks to this issue. The 30 participants in our study also took part in an experiment in which they passively listened to tone sequences while watching a silent film during a ‘sparse scanning’ protocol similar to that used during the active task we report in the paper, and with an identical EPI sequence. We have taken the L-DLPFC seed and the auditory cortex and insula ROIs, extracted the average time series within them, and compared these time series to corresponding data from the speech perception task. Whereas during speech perception, amusic subjects showed much less functional connectivity between left frontal and right insula/auditory ROIs relative to controls (p=.0001 for both ROIs; in line with the whole-brain imaging analyses), this pattern did not hold during passive listening to tones (Group by Task interaction p = 0.045 for the insula ROI, p = 0.035 for the auditory cortex ROI – see Figure 6). These interactions suggest that our neural connectivity results are specifically linked to speech perception, rather than reflecting an overall connectivity difference between groups regardless of task.</p><p>[Editors’ note: what follows is the authors’ response to the additional feedback provided by the editor, after the authors’ appeal.]</p><disp-quote content-type="editor-comment"><p>Beyond the new data, please be sure to address other concerns of the reviewers, including the lack of interaction between group and condition in the behavior and in the imaging data.</p></disp-quote><p>Below we have addressed the concerns about the lack of interactions, as well as the secondary issues.</p><disp-quote content-type="editor-comment"><p>Perhaps the lack of interaction in the behavior is due to ceiling performance in both groups (please show the data).</p></disp-quote><p>Performance on the task is now plotted in new Figure 1. This shows performance was not at ceiling for either the amusia or control group during the Pitch-Informative condition, so we suggest ceiling effects were not the cause of the lack of interaction. (Indeed, neither was there a ceiling effect in the version of this experiment we ran outside the scanner in quiet listening conditions (Jasmin et al., 2020)).</p><disp-quote content-type="editor-comment"><p>But if the functional connectivity results are related to the downweighting of pitch cues in amusics, shouldn't this only manifest itself in the condition where such cues were available, and not in the condition where duration cues were available?</p></disp-quote><p>This is an important point and we apologize for not being clearer. Pitch cues were always <italic>available</italic> on every trial, but pitch contour only conveyed information about sentence phrase boundaries in two of three conditions. In the manuscript, we have now renamed the conditions to make this clearer: Pitch-Informative, Duration-Informative, Both-Informative.</p><p>The lack of main effect of Condition, or Condition by Group interaction in our fMRI data is what one would predict given the specific task structure. Here, participants never knew – even implicitly – which acoustic dimension might be useful on a trial until after they had heard both spoken sentences and then compared them to make their decision. (Note that the fMRI protocol was timed to capture the BOLD signal peak related to the second auditory stimulus.) Thus, there was no overt acoustic cue that signalled whether duration or pitch was the diagnostic auditory dimension on each trial. We have added a paragraph to the Discussion section to clarify this:</p><p>“We did not observe any differences in functional connectivity differences between conditions. This may be because our functional imaging protocol was timed to capture the peak in the BOLD signal corresponding to the presentation of the second auditory stimulus. Participants never knew (even implicitly) which acoustic dimension might be useful on any given trial until after they had heard both spoken sentences and needed to compare them to make their response. Furthermore, pitch fluctuations in the stimuli were above participants’ pitch thresholds, even in the Duration-Informative condition (where the standard deviation of F0 over each spoken utterance was, on average, 2.7 semitones), and so it is unsurprising that functional connectivity did not change on a trial-by-trial basis, and instead the same ‘neural strategy’ was employed to process speech regardless of the trial type.”</p><p>Our interpretation of the group differences in functional connectivity is that they reflect a difference in task strategy between the amusic and control participants: amusics down-weight pitch relative to other dimensions during prosody perception (see Jasmin et al., 2020). When pitch and duration present conflicting information, one would predict that amusics will base their decision more on duration (relative to controls). However, if pitch cues are large enough, and duration cues are not informative, amusics should still succeed in the task (see Peretz et al., 2005) – while still weighting pitch information differently because that dimension is typically unreliable for them. To make sure our fMRI design was not confounded with performance differences (see Church et al., 2010 for discussion), pitch differences between stimulus pairs were designed to exceed all participants’ pitch discrimination thresholds, ensuring that the task was relatively easy for all participants, even for amusics on the Pitch-Informative condition. We have now made this explicit in the manuscript in subsection “Stimuli”:</p><p>“Across all stimuli, F0 (vocal pitch) differences between Early and Late closure versions were large, with a mean of maximum difference of 7.7 semitones and range of 4.0-12.6 semitones. Thus, even the stimulus pair with the smallest pitch difference (4.0 semitones) exceeded the ~1.5 semitone pitch change detection threshold of the ‘most impaired’ participant in the amusia group (Jasmin et al., 2020), which increased the chances that the amusia group would not suffer from poor performance, thereby avoiding a performance-related confound with our experimental design (see Church et al., 2010 for discussion).”</p><p>Importantly, inducing a shift in pre-existing perceptual cue weights in the laboratory appears to require extreme changes in co-occurrence statistics, for instance by completely reversing the correlation between cues (Idemaru and Holt, 2011), or severely degrading one of the input channels (Beauchamp et al., 2010; Nath and Beauchamp, 2011). Our manipulation did not resemble either of these processes – both pitch and duration changes were present across all trials, with only their relative task utility differing with condition. Given previous experimental results, this manipulation was likely too subtle to affect cue weights and functional connectivity that had developed from a lifetime of learning. However, it would be interesting in future work to manipulate stimulus statistics in order to assess the neural correlates of short-term changes in cue weighting (as we suggest in the Discussion section).</p><p>“Several other future directions are suggested by our results, particularly for examining cue weighting during auditory/speech perception. In the multimodal integration studies mentioned above (Beauchamp et al., 2010; Nath and Beauchamp, 2011), reliability of two different sensory modalities was manipulated experimentally, by severely degrading input channels with noise, resulting in changes in connectivity. Similarly, aspects of speech could be selectively masked with noise in order to make them less reliable, which in turn could cause corresponding changes in functional or effective connectivity.”</p><disp-quote content-type="editor-comment"><p>Ensuring that the data are fully compatible with the interpretation will play a critical role in a final decision.</p></disp-quote><p>We appreciate that both reviewers and editors raised this important issue and hope that our interpretation of the data has now been presented more clearly.</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>[…]</p><p>During task fMRI, amusics showed lower functional connectivity in left and right prefrontal regions, and between left prefrontal regions and right auditory and anterior insula. But the results as presented right now don't show any relationship between task performance and the neural findings. I would really like to see some brain-behavioral correlations to identify the specific compensatory mechanisms that amusics might be using to approach the task of sentence processing. Without that, it is unclear whether and how the findings in fMRI connectivity differences pertain to the task.</p></disp-quote><p>Thank you for highlighting this point. Per our previous letter, we now include correlations between functional connectivity levels and cue weights measured outside the scanner. These correlations support our interpretation that individual differences in functional connectivity reflect idiosyncratic perceptual weighting strategies. The following is now included in the manuscript:</p><p>From the Materials and methods section:</p><p>“To determine whether the functional connectivity patterns we observed were related to the importance placed on acoustic dimensions during prosodic categorization (cue weighting), the functional connectivity results were analyzed with respect to previously acquired cue weights obtained behaviorally from a subset of participants (Jasmin et al., 2020a) The right anterior insula and right auditory cortex results were used as ROIs (Figure 3). The beta series for each ROI (averaged across vertices) was correlated with the beta series within the L-DLPFC seed area, separately for each condition, then averaged and Fisher Z-transformed. For the 21 participants for whom we had prosodic cue weight data (from Jasmin et al., 2020a) these cue weights were analyzed with respect to the functional connectivity between the L-DLPFC seed and the two ROIs using Spearman correlations.”</p><p>From the Results section:</p><p>“Of the 30 participants in this study, 21 took part in an experiment that measured the degree to which they relied on pitch versus duration to categorize prosody, i.e. their ‘normalized prosodic cue weights’ (Experiment 1, Jasmin, et al., 2020a). […] Both these correlations were in the predicted direction, suggesting that even non-amusics may perform dimensional reweighting of acoustic dimensions and functional connectivity. Correlations within the (much more variable) amusic group alone were weaker and non-significant (although again, the group size is very small).”</p><disp-quote content-type="editor-comment"><p>2) Especially since there were no task-related differences in behavior nor were there any differences in task-related activations, one wonders if a task-free resting state fMRI would have shown the same results, thus suggesting intrinsic connectivity differences rather than addressing the differential weighting mechanisms so interestingly described in the Introduction.</p></disp-quote><p>Thank you for the suggestion; we agree it’s important to show that these group differences in functional connectivity are task-specific, rather than intrinsic. We now include data from a task-free passive tone listening session in the same participants, and compare them statistically with the task-related functional connectivity measures initially reported. These analyses show that the group differences in connectivity elicited in the prosody perception task are not present in the passive tone listening task, suggesting that they reflect differential perceptual weighting during auditory language comprehension, rather than intrinsic connectivity differences:</p><p>From the Materials and methods section:</p><p>“[…] functional connectivity between L-DLPFC, and right auditory cortex and right insula was calculated using data from the passive tone listening task. After pre-processing and de-trending, the averaged value from the tone listening experiment within these ROIs was extracted, as well as the LDLPFC seed, for each experiment. Correlations between signal within the seed and the two ROIs was calculated and Fisher Z-transformed. As mentioned above, because all trials in the tone-listening experiment were analyzed as the same type, it was not necessary to use a first-level model to obtain trial-wise betas. Similarly for the data from the speech task, the average value within the seed region and both ROIs was extracted, separately for each of the 3 Β series (Pitch-, Time- and Both-Informative), and the seed and ROI series were correlated. The mean of these 3 correlation coefficients was calculated and Fisher Z-transformed. Finally, statistics were performed using a mixed ANOVA with Experiment (Speech or Tones) as the within-subject factor and Group (Amusia or Control) as the between-subject factor.”</p><p>From the Results section:</p><p>“To ensure the pattern of connectivity we observed between groups (decreased right auditory cortex and right insula to L-DLPFC connectivity) was not due to intrinsic, task-irrelevant differences in neural architecture, the data from the language task was compared to that collected during passive listening to tone sequences. Whereas during speech perception, amusic subjects showed reduced functional connectivity between left frontal and right insula/auditory ROIs relative to controls (p=0.0001 for both ROIs; in line with the whole-brain imaging analyses), this pattern did not hold during passive listening to tones (Amusia vs. Control connectivity, p = 0.29, Group (Amusic, Control) by Task (Speech Perception, Passive Tone Listening) interaction p = 0.045 for the insula ROI; Amusia vs. Control p = 0.30, Group by Task interaction p = 0.035 for the auditory cortex ROI – see Figure 6). These interactions suggest that our neural connectivity results are specifically linked to speech perception, rather than reflecting an overall connectivity difference between groups regardless of task state.”</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…]</p><p>The study is well-conducted and analyzed, and the paper is well-written. But I am not persuaded about the interpretation of the results. The main problem is the lack of any explicit brain-behavior relationship. This issue takes two forms. First, the brain data and the behavioral data are never related to one another. The authors make the claim that the functional connectivity differences are related to the behavior, but this is not shown by any analysis. Without any such link, it is hard to know whether the connectivity differences observed have anything to do with the particular ability sampled by this task, or if they are related to something else entirely.</p></disp-quote><p>Thanks to you and reviewer 1 for bringing this up – we have taken all of your suggestions to heart and run the analyses. To avoid adding yet more text, please see reviewer 1 point 1.</p><disp-quote content-type="editor-comment"><p>Second, the behavior itself does not differ across participant groups. The authors specifically desired this outcome so that any imaging differences would not be driven by group behavioral differences. But that idea does not make sense to me because there were two conditions, one using pitch cues and another using duration cues. Presumably, amusic persons have trouble with the former but not the latter. Yet this was not the case in the behavior (no condition X group interaction), nor in the imaging data (the FC effects were not influenced by condition either). So if the FC results are somehow related to the downweighting of pitch cues it should show up specifically in the condition where such cues were available, and not in the condition where duration cues were available. Without this crucial dissociation, the interpretation does not hold together.</p></disp-quote><p>Please see our responses to the second and third editorial comments.</p><disp-quote content-type="editor-comment"><p>In the Discussion section the authors argue that the deficit in amusia has more to do with tonal retention than perception, an idea which is supported by other authors, although the bulk of the evidence suggests that both features are present. But it is not entirely clear why a retention deficit would be so important in the task used, which does not appear to require a lot of working memory since the spoken stimuli simply have to be matched to a visual representation of the sentence.</p></disp-quote><p>We would argue that the present task can be suggested to have a non-trivial ‘working memory’ component. Each auditory stimulus was about 2.5 s long and participants needed to retain both of them in memory while comparing them to the visual stimulus. Moreover, memory is inextricably involved with prosody perception because acoustic cues to prosodic features are compared to the preceding context. With regards to pitch, for example, pitch accents and phrase boundaries are conveyed via a change relative to the contextual baseline F0. Of course, this is somewhat true for segmental speech perception as well, inasmuch as the acoustic cues associated with phonemes are modulated by coarticulation. However, the time scale over which these contextual effects take place is much longer in suprasegmental than in segmental perception.</p><disp-quote content-type="editor-comment"><p>Another related issue is that in amusia there is often a wide range of perceptual performance, with some individuals scoring near chance on pitch discrimination and others scoring at the same level as non-amusic persons. It's not clear how that heterogeneity may have influenced performance on this task, especially as the pitch intervals used in the stimuli are not documented in the manuscript, which seems like a significant omission. It would be important to know whether the excursions in F0 for the pitch-varying sentences (which are typically much larger than one or two semitones) were smaller than the thresholds that the amusics can perceive or not. In general, there is no discussion of individual differences in the data, which might have helped to understand the findings.</p></disp-quote><p>Thank you for pointing out the omission; we now supply details regarding the size of the F0 differences between stimulus pairs in the prosody stimuli. In every case, these differences were larger than the pitch discrimination threshold of even the ‘most impaired’ amusic participant. We note this in the Discussion section.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Brain-behavior correlations. The inclusion of the current Figure 5 certainly strengthens the manuscript and is an important addition. However, because the correlation is driven primarily by the between-group differences, the importance of the overall correlation should not be overstated. In particular, it is probably not appropriate to mention it in the abstract without the necessary caveats. Instead just a statement, saying that the between-group differences in connectivity reflect the between-group differences in performance in the same groups of listeners, would suffice.</p></disp-quote><p>Thanks for raising this. To avoid overstating the importance of the brain-behavior correlations, we have removed mention of them in the Abstract and included, as suggested, a statement saying that the between-group differences in connectivity reflect the between-group differences in performance in the same groups of listeners.</p><p>“[..] reflected the between-group differences in cue weights in the same groups of listeners.”</p><disp-quote content-type="editor-comment"><p>Behavior task. There are still some questions remaining on this front. In particularly, there remain some questions as to why the amusics should show the same amount of behavioral gain when pitch cues are added. Is this thought to be because the pitch changes are so large as to influence performance despite impaired pitch processing? A little more discussion of this aspect of the results will likely help readers follow the logic better. There is also some skepticism regarding when and whether participants knew which cues were informative on each trial (Discussion section). For instance, in many cases participants would probably have been able to complete the task following just the first interval, rather than have to wait for the second interval and make a comparison. Although the 2AFC paradigm makes the analysis of performance easier, it cannot be assumed that participants always make full use (or need to make use) of the information from both intervals. This observation has some implications for the extent to which memory processes are involved.</p></disp-quote><p>First, so that readers can experience the task themselves, we have created a simulation of the in-scanner task in Gorilla Open Materials so that readers may find that – as we do – the task is not particularly easy even in quiet listening conditions (https://gorilla.sc/openmaterials/102786). We have additionally included this link to the task in the manuscript in the Data Availability Statement. Second, we have analyzed reaction times in the task, measured in quiet listening conditions outside the scanner in the same participants and found that they were quite long, with an average of 1.64 seconds after the end of the second auditory presentation. This suggests that participants did not respond immediately, but needed time to mentally compare the two auditory presentations in memory in order to make their judgments. We have added this text to the Discussion section accordingly:</p><p>“Our task arguably taxed working memory resources: in a similar paradigm performed by the same participants in quiet listening conditions (Jasmin et al., 2020a), the mean reaction time measured from the end of the second auditory stimulus was 1.64 seconds, indicating that participants needed some time to compare both auditory presentations and make their judgments.”</p><disp-quote content-type="editor-comment"><p>Additional analysis. There are two areas where additional analyses may be illuminated. First, the main conclusions of the study rely in part on the results of the correlations between pitch cue weights and connectivity measures. Duration cue weights, which do not differ between groups, were also measured. It appears important, as a negative control, to comment on the absence (or not) of correlations between these duration cue weights and connectivity measures (subsection “Correlations between functional connectivity levels and prosodic cue weights”).</p></disp-quote><p>Thank you for suggesting this analysis. We should have referred to the cue weights more consistently in the manuscript to avoid confusion. The ‘normalized prosodic cue weights’ used in the paper are relative weights that reflect the degree to which participants relied upon pitch versus duration. They are normalized with respect to one another, such that possible values range from 0 to 1, with values greater than 0.5 indicating greater reliance on pitch than duration, and values less than 0.5 indicate greater reliance on duration than pitch. So-called ‘raw’ pitch and duration cue weights, regression coefficients used to calculate relative normalized weights, are not easily interpretable in isolation because the raw weights can be influenced by factors such as the general degree of attention to the task. For this reason, it is not possible to perform the suggested control analysis, but to address the issue of confusing language we have changed all instances of ‘pitch cue weight’ to ‘normalized cue weights’ in both the main text and the figure axis labels, and clarified as follows in the main text:</p><p>“Of the 30 participants in this study, 21 took part in an experiment that measured the degree to which they relied on pitch versus duration to categorize prosody, i.e. their ‘normalized prosodic cue weights’, which ranged from 0 to 1, with values greater than 0.5 indicating greater reliance on pitch than duration, and values less than 0.5 indicate greater reliance on duration than pitch (Experiment 1, Jasmin, et al., 2020a).”</p><disp-quote content-type="editor-comment"><p>Second, the comparison of connectivity between the active and passive (tones) task was based on the ROIs derived from the passive task, with the assumption that the passive task should not result in differences in connectivity. However, as far as we could tell, the initial data-driven connectivity tests were not run separately on the passive task. If that were done just on the passive task, would any connectivity differences between the groups emerge? If not, that would provide a further important negative control.</p></disp-quote><p>This is a good point: indeed only the passive task (tone) data were analyzed with respect to ROIs generated from the active task. As you suggest, we have run an analogous version of the first step of the data-driven connectivity analysis on the passive task data. No group differences in global connectivity were detected at the threshold used in the paper (P&lt;4.61) nor indeed even at an uncorrected threshold of p&lt;.001. We now have added this to the text, thank you for the suggestion.</p><p>“An analogous procedure was run on the passive tone listening data, in which whole-brain connectedness values were compared by Group (amusic vs. control) in a linear mixed effects model. No significant FDR-corrected group differences were detected, nor at a reasonable uncorrected threshold of p&lt;.001.”</p></body></sub-article></article>