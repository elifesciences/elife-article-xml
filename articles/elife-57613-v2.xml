<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">57613</article-id><article-id pub-id-type="doi">10.7554/eLife.57613</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Plant Biology</subject></subj-group></article-categories><title-group><article-title>Accurate and versatile 3D segmentation of plant tissues at cellular resolution</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-184727"><name><surname>Wolny</surname><given-names>Adrian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2794-4266</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-184726"><name><surname>Cerrone</surname><given-names>Lorenzo</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184725"><name><surname>Vijayan</surname><given-names>Athul</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-172956"><name><surname>Tofanelli</surname><given-names>Rachele</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5196-1122</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184724"><name><surname>Barro</surname><given-names>Amaya Vilches</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184723"><name><surname>Louveaux</surname><given-names>Marion</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">‡</xref></contrib><contrib contrib-type="author" id="author-80285"><name><surname>Wenzl</surname><given-names>Christian</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-22806"><name><surname>Strauss</surname><given-names>Sören</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184722"><name><surname>Wilson-Sánchez</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184721"><name><surname>Lymbouridou</surname><given-names>Rena</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184720"><name><surname>Steigleder</surname><given-names>Susanne S</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184719"><name><surname>Pape</surname><given-names>Constantin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184718"><name><surname>Bailoni</surname><given-names>Alberto</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con13"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184717"><name><surname>Duran-Nebreda</surname><given-names>Salva</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con14"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-184716"><name><surname>Bassel</surname><given-names>George W</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con15"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-2273"><name><surname>Lohmann</surname><given-names>Jan U</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3667-187X</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con16"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-13242"><name><surname>Tsiantis</surname><given-names>Miltos</given-names></name><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con17"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-135013"><name><surname>Hamprecht</surname><given-names>Fred A</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con18"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23479"><name><surname>Schneitz</surname><given-names>Kay</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6688-0539</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con19"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-11165"><name><surname>Maizel</surname><given-names>Alexis</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con20"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-133512"><name><surname>Kreshuk</surname><given-names>Anna</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1334-6388</contrib-id><email>anna.kreshuk@embl.de</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con21"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Heidelberg Collaboratory for Image Processing, Heidelberg University</institution><addr-line><named-content content-type="city">Heidelberg</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>EMBL</institution><addr-line><named-content content-type="city">Heidelberg</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>School of Life Sciences Weihenstephan, Technical University of Munich</institution><addr-line><named-content content-type="city">Freising</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution>Centre for Organismal Studies, Heidelberg University</institution><addr-line><named-content content-type="city">Heidelberg</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution>Department of Comparative Development and Genetics, Max Planck Institute for Plant Breeding Research</institution><addr-line><named-content content-type="city">Cologne</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution>School of Life Sciences, University of Warwick</institution><addr-line><named-content content-type="city">Coventry</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Hardtke</surname><given-names>Christian S</given-names></name><role>Senior Editor</role><aff><institution>University of Lausanne</institution><country>Switzerland</country></aff></contrib><contrib contrib-type="editor"><name><surname>Bergmann</surname><given-names>Dominique C</given-names></name><role>Reviewing Editor</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>‡</label><p>Institute Pasteur, Paris, France</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>29</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e57613</elocation-id><history><date date-type="received" iso-8601-date="2020-04-06"><day>06</day><month>04</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-07-28"><day>28</day><month>07</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Wolny et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Wolny et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-57613-v2.pdf"/><abstract><p>Quantitative analysis of plant and animal morphogenesis requires accurate segmentation of individual cells in volumetric images of growing organs. In the last years, deep learning has provided robust automated algorithms that approach human performance, with applications to bio-image analysis now starting to emerge. Here, we present PlantSeg, a pipeline for volumetric segmentation of plant tissues into cells. PlantSeg employs a convolutional neural network to predict cell boundaries and graph partitioning to segment cells based on the neural network predictions. PlantSeg was trained on fixed and live plant organs imaged with confocal and light sheet microscopes. PlantSeg delivers accurate results and generalizes well across different tissues, scales, acquisition settings even on non plant samples. We present results of PlantSeg applications in diverse developmental contexts. PlantSeg is free and open-source, with both a command line and a user-friendly graphical interface.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>instance segmentation</kwd><kwd>cell segmentation</kwd><kwd>deep learning</kwd><kwd>image analysis</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>A. thaliana</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>FOR2581</award-id><principal-award-recipient><name><surname>Lohmann</surname><given-names>Jan U</given-names></name><name><surname>Tsiantis</surname><given-names>Miltos</given-names></name><name><surname>Hamprecht</surname><given-names>Fred A</given-names></name><name><surname>Schneitz</surname><given-names>Kay</given-names></name><name><surname>Maizel</surname><given-names>Alexis</given-names></name><name><surname>Kreshuk</surname><given-names>Anna</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000275</institution-id><institution>Leverhulme Trust</institution></institution-wrap></funding-source><award-id>RPG-2016-049</award-id><principal-award-recipient><name><surname>Bassel</surname><given-names>George</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Convolutional neural networks and graph partitioning algorithms can be combined into an easy-to-use tool for segmentation of cells in dense plant tissue volumes imaged with light microscopy.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Large-scale quantitative study of morphogenesis in a multicellular organism entails an accurate estimation of the shape of all cells across multiple specimen. State-of-the-art light microscopes allow for such analysis by capturing the anatomy and development of plants and animals in terabytes of high-resolution volumetric images. With such microscopes now in routine use, segmentation of the resulting images has become a major bottleneck in the downstream analysis of large-scale imaging experiments. A few segmentation pipelines have been proposed (<xref ref-type="bibr" rid="bib15">Fernandez et al., 2010</xref>; <xref ref-type="bibr" rid="bib54">Stegmaier et al., 2016</xref>), but these either do not leverage recent developments in the field of computer vision or are difficult to use for non-experts.</p><p>With a few notable exceptions, such as the Brainbow experiments (<xref ref-type="bibr" rid="bib69">Weissman and Pan, 2015</xref>), imaging cell shape during morphogenesis relies on staining of the plasma membrane with a fluorescent marker. Segmentation of cells is then performed based on their boundary prediction. In the early days of computer vision, boundaries were usually found by edge detection algorithms (<xref ref-type="bibr" rid="bib8">Canny, 1986</xref>). More recently, a combination of edge detectors and other image filters was commonly used as input for a machine learning algorithm, trained to detect boundaries (<xref ref-type="bibr" rid="bib36">Lucchi et al., 2012</xref>). Currently, the most powerful boundary detectors are based on Convolutional Neural Networks (CNNs) (<xref ref-type="bibr" rid="bib34">Long et al., 2015</xref>; <xref ref-type="bibr" rid="bib31">Kokkinos, 2015</xref>; <xref ref-type="bibr" rid="bib75">Xie and Tu, 2015</xref>). In particular, the U-Net architecture (<xref ref-type="bibr" rid="bib51">Ronneberger et al., 2015</xref>) has demonstrated excellent performance on 2D biomedical images and has later been further extended to process volumetric data (<xref ref-type="bibr" rid="bib10">Çiçek et al., 2016</xref>).</p><p>Once the boundaries are found, other pixels need to be grouped into objects delineated by the detected boundaries. For noisy, real-world microscopy data, this post-processing step still represents a challenge and has attracted a fair amount of attention from the computer vision community (<xref ref-type="bibr" rid="bib60">Turaga et al., 2010</xref>; <xref ref-type="bibr" rid="bib43">Nunez-Iglesias et al., 2014</xref>; <xref ref-type="bibr" rid="bib5">Beier et al., 2017</xref>; <xref ref-type="bibr" rid="bib71">Wolf et al., 2018</xref>; <xref ref-type="bibr" rid="bib18">Funke et al., 2019a</xref>). If centroids (‘seeds’) of the objects are known or can be learned, the problem can be solved by the watershed algorithm (<xref ref-type="bibr" rid="bib11">Couprie et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Cerrone et al., 2019</xref>). For example, in <xref ref-type="bibr" rid="bib13">Eschweiler et al., 2018</xref> a 3D U-Net was trained to predict cell contours together with cell centroids as seeds for watershed in 3D confocal microscopy images. This method, however, suffers from the usual drawback of the watershed algorithm: misclassification of a single cell centroid results in sub-optimal seeding and leads to segmentation errors.</p><p>Recently, an approach combining the output of two neural networks and watershed to detect individual cells showed promising results on segmentation of cells in 2D (<xref ref-type="bibr" rid="bib68">Wang et al., 2019</xref>). Although this method can in principle be generalized to 3D images, the necessity to train two separate networks poses additional difficulty for non-experts.</p><p>While deep learning-based methods define the state-of-the-art for all image segmentation problems, only a handful of software packages strives to make them accessible to non-expert users in biology (reviewed in [<xref ref-type="bibr" rid="bib42">Moen et al., 2019</xref>]). Notably, the U-Net segmentation plugin for ImageJ (<xref ref-type="bibr" rid="bib14">Falk et al., 2019</xref>) conveniently exposes U-Net predictions and computes the final segmentation from simple thresholding of the probability maps. CDeep3M (<xref ref-type="bibr" rid="bib20">Haberl et al., 2018</xref>) and DeepCell (<xref ref-type="bibr" rid="bib64">Van Valen et al., 2016</xref>) enable, via the command-line, the thresholding of the probability maps given by the network, and DeepCell allows instance segmentation as described in <xref ref-type="bibr" rid="bib68">Wang et al., 2019</xref>. More advanced post-processing methods are provided by the ilastik Multicut workflow (<xref ref-type="bibr" rid="bib6">Berg et al., 2019</xref>), however, these are not integrated with CNN-based prediction.</p><p>Here, we present PlantSeg, a deep learning-based pipeline for volumetric instance segmentation of dense plant tissues at single-cell resolution. PlantSeg processes the output from the microscope with a CNN to produce an accurate prediction of cell boundaries. Building on the insights from previous work on cell segmentation in electron microscopy volumes of neural tissue (<xref ref-type="bibr" rid="bib5">Beier et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Funke et al., 2019a</xref>), the second step of the pipeline delivers an accurate segmentation by solving a graph partitioning problem. We trained PlantSeg on 3D confocal images of fixed <italic>Arabidopsis thaliana</italic> ovules and 3D+t light sheet microscope images of developing lateral roots, two standard imaging modalities in the studies of plant morphogenesis. We investigated a range of network architectures and graph partitioning algorithms and selected the ones which performed best with regard to extensive manually annotated ground truth. We benchmarked PlantSeg on a variety of datasets covering a range of samples and image resolutions. Overall, PlantSeg delivers excellent results on unseen data and, as we show through quantitative and qualitative evaluation, even non-plant datasets do not necessarily require network retraining. Combining the repository of accurate neural networks trained on the two common microscope modalities and going beyond just thresholding or watershed with robust graph partitioning strategies is the main strength of our package. PlantSeg is an open-source tool which contains the complete pipeline for segmenting large volumes. Each step of the pipeline can be adjusted via a convenient graphical user interface while expert users can modify configuration files and run PlantSeg from the command line. Users can also provide their own pre-trained networks for the first step of the pipeline using a popular 3D U-Net implementation (<ext-link ext-link-type="uri" xlink:href="https://github.com/wolny/pytorch-3dunet">https://github.com/wolny/pytorch-3dunet</ext-link>), which was developed as a part of this project. Although PlantSeg was designed to segment 3D images, one can directly use it to segment 2D stacks. Besides the tool itself, we provide all the networks we trained for the confocal and light sheet modalities at different resolution levels and make all our training and validation data publicly available All datasets used to support the findings of this study have been deposited in <ext-link ext-link-type="uri" xlink:href="https://osf.io/uzq3w">https://osf.io/uzq3w</ext-link>. All the source code can be found at (<xref ref-type="bibr" rid="bib72">Wolny, 2020a</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/hci-unihd/plant-seg">https://github.com/hci-unihd/plant-seg</ext-link>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/plant-seg">https://github.com/elifesciences-publications/plant-seg</ext-link>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A pipeline for segmentation of plant tissues into cells</title><p>The segmentation algorithm we propose contains two major steps. In the first step, a fully convolutional neural network (a variant of U-Net) is trained to predict cell boundaries. Afterwards, a region adjacency graph is constructed from the pixels with edge weights computed from the boundary predictions. In the second step, the final segmentation is computed as a partitioning of this graph into an unknown number of objects (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). Our choice of graph partitioning as the second step is inspired by a body of work on segmentation for nanoscale connectomics (segmentation of cells in electron microscopy images of neural tissue), where such methods have been shown to outperform more simple post-processing of the boundary maps (<xref ref-type="bibr" rid="bib5">Beier et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Funke et al., 2019a</xref>; <xref ref-type="bibr" rid="bib7">Briggman et al., 2009</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Segmentation of plant tissues into cells using PlantSeg.</title><p>First, PlantSeg uses a 3D UNet neural network to predict the boundaries between cells. Second, a volume partitioning algorithm is applied to segment each cell based on the predicted boundaries. The neural networks were trained on ovules (top, confocal laser scanning microscopy) and lateral root primordia (bottom, light sheet microscopy) of <italic>Arabidopsis thaliana</italic>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig1-v2.tif"/></fig><sec id="s2-1-1"><title>Datasets</title><p>To make our tool as generic as possible, we used both fixed and live samples as core datasets for design, validation and testing. Two microscope modalities common in studies of morphogenesis were employed, followed by manual and semi-automated annotation of ground truth segmentation.</p><p>The first dataset consists of fixed <italic>Arabidopsis thaliana</italic> ovules at all developmental stages acquired by confocal laser scanning microscopy with a voxel size of 0.075 × 0.075 × 0.235 μm. 48 volumetric stacks with hand-curated ground truth segmentation were used. A complete description of the image acquisition settings and the ground truth creation protocol is reported in <xref ref-type="bibr" rid="bib58">Tofanelli et al., 2019</xref>.</p><p>The second dataset consists of three time-lapse videos showing the development of <italic>Arabidopsis thaliana</italic> lateral root primordia (LRP). Each recording was obtained by imaging wild-type Arabidopsis plants expressing markers for the plasma membrane and the nuclei (<xref ref-type="bibr" rid="bib65">Vilches Barro et al., 2019</xref>) using a light sheet fluorescence microscope (LSFM). Stacks of images were acquired every 30 min with constant settings across movies and time points, with a voxel size of 0.1625 × 0.1625 × 0.250 μm. The first movie consists of 52 time points of size 2048 × 1050 × 486 voxels. The second movie consists of 90 time points of size 1940 × 1396 × 403 voxels and the third one of 92 time points of size 2048 × 1195 × 566 voxels. The ground truth was generated for 27 images depicting different developmental stages of LRP coming from the three movies (see Appendix 1 Groundtruth Creation).</p><p>The two datasets were acquired on different types of microscopes and differ in image quality. To quantify the differences we used the peak signal-to-noise (PSNR) and the structural similarity index measure (SSIM) (<xref ref-type="bibr" rid="bib23">Horé and Ziou, 2010</xref>). We computed both metrics using the input images and their ground truth boundary masks; higher values show better quality. The average PSNR measured on the light sheet dataset was 22.5 ± 6.5 dB (average ± SD), 3.4 dB lower than the average PSNR computed on the confocal dataset (25.9 ± 5.7). Similarly, the average SSIM is 0.53 ± 0.12 for the light sheet, 0.1 lower than 0.63 ± 0.13 value measured on the confocal images. Both datasets thus contain a significant amount of noise. LSFM images are noisier and more difficult to segment, not only because of the noise, but also due to part of nuclear labels being in the same channel as membrane staining.</p><p>In the following we describe in detail the design and performance of each of the two steps of the pipeline.</p></sec><sec id="s2-1-2"><title>Step 1: cell boundary detection</title><p>Being the current state of the art in bioimage segmentation, U-Net (<xref ref-type="bibr" rid="bib51">Ronneberger et al., 2015</xref>) was chosen as the base model architecture for predicting the boundaries between cells. Aiming for the best performance across different microscope modalities, we explored various components of neural network design critical for improved generalization and robustness to noise, namely: the network architecture, loss function, normalization layers and size of patches used for training. For the final PlantSeg package we trained one set of CNNs for each dataset as the ovule and lateral root datasets are substantially different.</p><p>In more detail, with regard to the network architecture we compared the regular 3D U-Net as described in <xref ref-type="bibr" rid="bib10">Çiçek et al., 2016</xref> with a Residual U-Net from <xref ref-type="bibr" rid="bib33">Lee et al., 2017</xref>. We tested two loss functions commonly used for the semantic segmentation task: binary cross-entropy (BCE) (<inline-formula><mml:math id="inf1"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) (<xref ref-type="bibr" rid="bib51">Ronneberger et al., 2015</xref>) and Dice loss (<inline-formula><mml:math id="inf2"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) (<xref ref-type="bibr" rid="bib56">Sudre et al., 2017</xref>), as well as their linear combination (<inline-formula><mml:math id="inf3"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) termed BCE-Dice. The patch size and normalization layers were investigated jointly by comparing training on a single large patch, versus training on multiple smaller patches per network iteration. For single patch we used group normalization (<xref ref-type="bibr" rid="bib74">Wu and He, 2018</xref>) whereas standard batch normalization (<xref ref-type="bibr" rid="bib25">Ioffe and Szegedy, 2015</xref>) was used for the multiple patches.</p><p>In the ovule dataset, 39 stacks were randomly selected for training, two for validation and seven for testing. In the LRP dataset, 27 time points were randomly selected from the three videos for training, two time points were used for validation and four for testing.</p><p>The best performing CNN architectures and training procedures is illustrated by the precision/recall curves evaluated at different threshold levels of the predicted boundary probability maps (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Training with a combination of binary cross-entropy and Dice loss (<inline-formula><mml:math id="inf4"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) performed best on average across the two datasets in question contributing to 3 out of 6 best performing network variants. BCE-Dice loss also generalized well on the out of sample data described in 2.1.4 Performance on external plant datasets. Due to the regularity of cell shapes, the networks do not benefit from broader spatial context when only cell membrane signal is present in input images. Indeed, training the networks with bigger patch sizes does not noticeably increase the performance as compared to training with smaller patches. 4 out of 6 best performing networks use smaller patches and batch normalization (<italic>Batch-Norm</italic>) whereas only 2 out of 6 use bigger patches and group normalization (<italic>Group-Norm</italic>). Residual U-Net architecture (<italic>3D-ResUnet</italic>) performed best on the LRP dataset (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), whereas standard U-Net architecture (<italic>3D-Unet</italic>) was better on the ovule datasets (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). For a complete overview of the performance of investigated models see also <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="table" rid="app5table1">Appendix 5—table 1</xref>. In conclusion, choosing the right loss function and normalization layers increased the final performance on the task of boundary prediction on both microscope modalities.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Precision-recall curves for different CNN variants on the ovule (<bold>A</bold>) and lateral root primordia (LRP) (<bold>B</bold>) datasets.</title><p>Six training procedures that sample different type of architecture (3D U-Net <italic>vs.</italic> 3D Residual U-Net), loss function (BCE <italic>vs.</italic> Dice <italic>vs.</italic> BCE-Dice) and normalization (Group-Norm <italic>vs.</italic> Batch-Norm) are shown. Those variants were chosen based on the accuracy of boundary prediction task: three best performing models on the ovule and three best performing models on the lateral root datasets (see <xref ref-type="table" rid="app5table1">Appendix 5—table 1</xref> for a detailed summary). Points correspond to averages of seven (ovules) and four (LRP) values and the shaded area represent the standard error. For a detailed overview of precision-recall curves on individual stacks we refer to <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>. Source files used to generate the plot are available in the <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Source data for precision/recall curves of different CNN variants in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>The archive contains: 'pmaps_root' - directory containing precision/recall values computed on the test set from the Lateral Root dataset. 'pmaps_ovules' - directory with precision/recall values computed on the test set from the Ovules dataset. 'fig2_precision_recall.ipynb' - Jupyter notebook to generate the plots.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-57613-fig2-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig2-v2.tif"/></fig></sec><sec id="s2-1-3"><title>Step 2: segmentation of tissues into cells using graph partitioning</title><p>After the cell boundaries are predicted, segmentation of the cells can be formulated as a generic graph partitioning problem, where the graph is built as the region adjacency graph of the image voxels. However, solving the partitioning problem directly at voxel-level is computationally expensive for volumes of biologically relevant size. To make the computation tractable, we first cluster the voxels into so-called supervoxels by running a watershed algorithm on the distance transform of the boundary map, seeded by all local maxima of the distance transform smoothed by a Gaussian blur. The region adjacency graph is then built directly on supervoxels and partitioned into an unknown number of segments to deliver a segmentation. We tested four different partitioning strategies: Multicut (<xref ref-type="bibr" rid="bib28">Kappes et al., 2011</xref>), hierarchical agglomeration as implemented in GASP average (GASP) (<xref ref-type="bibr" rid="bib2">Bailoni et al., 2019</xref>), Mutex watershed (Mutex) (<xref ref-type="bibr" rid="bib71">Wolf et al., 2018</xref>) as well as the distance transform (DT) watershed (<xref ref-type="bibr" rid="bib50">Roerdink and Meijster, 2000</xref>) as a baseline since similar methods have been proposed previously (<xref ref-type="bibr" rid="bib13">Eschweiler et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Wang et al., 2019</xref>).</p><p>To quantify the accuracy of the four segmentation strategies, we use Adapted Rand error (ARand) for the overall segmentation quality and two other metrics based on the variation of information (<xref ref-type="bibr" rid="bib41">Meilă, 2007</xref>) (see Metrics used for evaluation), measuring the tendency to over-split (<italic>VOI</italic><sub><italic>split</italic></sub>) or over-merge (<italic>VOI</italic><sub><italic>merge</italic></sub>). GASP, Multicut and Mutex watershed consistently produced accurate segmentation on both datasets with low ARand errors and low rates of merge and split errors (<xref ref-type="fig" rid="fig3">Figure 3A–C</xref> and <xref ref-type="table" rid="app5table2">Appendix 5—table 2</xref>). As expected DT watershed tends to over-segment with higher split error and resulting higher ARand error. Multicut solves the graph partitioning problem in a globally optimal way and is therefore expected to perform better compared to greedy algorithms such as GASP and Mutex watershed. However, in our case the gain was marginal, probably due to the high quality of the boundary predictions.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Segmentation using graph partitioning.</title><p>(<bold>A–C</bold>) Quantification of segmentation produced by Multicut, GASP, Mutex watershed (Mutex) and DT watershed (DT WS) partitioning strategies. The Adapted Rand error (<bold>A</bold>) assesses the overall segmentation quality whereas <italic>VOI</italic><sub><italic>merge</italic></sub> (<bold>B</bold>) and <italic>VOI</italic><sub><italic>split</italic></sub> (<bold>C</bold>) assess erroneous merge and splitting events (lower is better). Box plots represent the distribution of values for seven (ovule, magenta) and four (LRP, green) samples. (<bold>D, E</bold>) Examples of segmentation obtained with PlantSeg on the lateral root (<bold>D</bold>) and ovule (<bold>E</bold>) datasets. Green boxes highlight cases where PlantSeg resolves difficult cases whereas red ones highlight errors. We obtained the boundary predictions using the <italic>generic-confocal-3d-unet</italic> for the ovules dataset and the <italic>generic-lightsheet-3d-unet</italic> for the root. All agglomerations have been performed with default parameters. 3D superpixels instead of 2D superpixels were used. Source files used to create quantitative results shown in (<bold>A–C</bold>) are available in the <xref ref-type="supplementary-material" rid="fig3sdata1">Figure 3—source data 1</xref>.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Source data for panes A, B and C in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>The archive contains CSV files with evaluation metrics computed on the Lateral Root and Ovules test sets. 'root_final_16_03_20_110904.csv' - evaluation metrics for the Lateral Root, 'ovules_final_16_03_20_113546.csv' - evaluation metrics for the Ovules, 'fig3_evaluation_and_supptables.ipynb' - Juputer notebook for generating panes A, B, C in <xref ref-type="fig" rid="fig3">Figure 3</xref> as well as <xref ref-type="table" rid="app5table2">Appendix 5—table 2</xref>.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-57613-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig3-v2.tif"/></fig><p>The performance of PlantSeg was also assessed qualitatively by expert biologists. The segmentation quality for both datasets is very satisfactory. For example in the lateral root dataset, even in cases where the boundary appeared masked by the high brightness of the nuclear signal, the network correctly detected it and separated the two cells (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, green box). On the ovule dataset, the network is able to detect weak boundaries and correctly separate cells in regions where the human expert fails (<xref ref-type="fig" rid="fig3">Figure 3E</xref> , green box). The main mode of error identified in the lateral root dataset is due to the ability of the network to remove the nuclear signal which can weaken or remove part of the adjacent boundary signal leading to missing or blurry cell contour. For example, the weak signal of a newly formed cell wall close to two nuclei was not detected by the network and the cells were merged (<xref ref-type="fig" rid="fig3">Figure 3D</xref>, red box). For the ovule dataset, in rare cases of very weak boundary signal, failure to correctly separate cells could also be observed (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, red box).</p><p>Taken together, our analysis shows that segmentation of plant tissue using graph partitioning handles robustly boundary discontinuities present in plant tissue segmentation problems.</p></sec><sec id="s2-1-4"><title>Performance on external plant datasets</title><p>To test the generalization capacity of PlantSeg, we assessed its performance on data for which no network training was performed. To this end, we took advantage of the two publicly available datasets: Arabidopsis 3D Digital Tissue Atlas (<ext-link ext-link-type="uri" xlink:href="https://osf.io/fzr56">https://osf.io/fzr56</ext-link>) composed of eight stacks of eight different <italic>Arabidopsis thaliana</italic> organs with hand-curated ground truth (<xref ref-type="bibr" rid="bib4">Bassel, 2019</xref>), as well as the developing leaf of the Arabidopsis (<xref ref-type="bibr" rid="bib16">Fox et al., 2018</xref>) with 3D segmentation given by the SimpleITK package (<xref ref-type="bibr" rid="bib35">Lowekamp et al., 2013</xref>). The input images from the digital tissue atlas are confocal stacks of fixed tissue with stained cell contours and thus similar to the images of the Arabidopsis ovules, whereas the images of the leaf were acquired through the use of live confocal imaging. It’s important to note that the latter image stacks contain highly lobed epidermal cells, which are difficult to segment with classical watershed-based algorithms. We fed the confocal stacks to PlantSeg and qualitatively assessed the resulting segmentation. Quantitative assessment was performed only for the digital tissue atlas, where the ground truth labels are available.</p><p>Qualitatively, PlantSeg performed well on both datasets, giving satisfactory results on all organs from the 3D Digital Tissue Atlas, correctly segmenting even the oval non-touching cells of the anther and leaf: a cell shape not present in the training data (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Our pipeline yielded especially good segmentation results when applied to the complex epidermal cells, visibly outperforming the results obtained using the SimpleITK framework (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>PlantSeg segmentation of different plant organs of the 3D Digital Tissue Atlas dataset, not seen in training.</title><p>The input image, ground truth and segmentation results using PlantSeg are presented for each indicated organ.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig4-v2.tif"/></fig><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Qualitative results on the highly lobed epidermal cells from <xref ref-type="bibr" rid="bib16">Fox et al., 2018</xref>.</title><p>First two rows show the visual comparison between the SimpleITK (middle) and PlantSeg (right) segmentation on two different image stacks. PlantSeg’s results on another sample is shown in the third row. In order to show pre-trained networks’ ability to generalized to external data, we additionally depict PlantSeg’s boundary predictions (third row, middle). We obtained the boundary predictions using the <italic>generic-confocal-3d-unet</italic> and segmented using GASP with default values. A value of 0.7 was chosen for the under/over segmentation factor.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig5-v2.tif"/></fig><p>Quantitatively, the performance of PlantSeg out of the box (default parameters) on the 3D Digital Tissue Atlas is on par with the scores reported on the LRP and ovules datasets on the anther, leaf, and the root, but lower for the other tissues (<xref ref-type="table" rid="table1">Table 1</xref>, left). Default parameters have been chosen to deliver good results on most type of data, however we show that a substantial improvement can be obtained by parameter tuning (see Appendix 6: PlantSeg - Parameters Guide for an overview of the pipeline’s hyperparameters and Appendix 7: Empirical Example of parameter tuning for a detailed guide on empirical parameter tuning), in case of the tissue 3D Digital Tissue Atlas tuning improved segmentation by a factor of two as measured with the ARand error (<xref ref-type="table" rid="table1">Table 1</xref>, right). It should be noted that the ground truth included in the dataset was created for analysis of the cellular connectivity network, with portions of the volumes missing or having low quality ground truth (see e.g filament and sepal in <xref ref-type="fig" rid="fig4">Figure 4</xref>). For this reason, the performance of PlantSeg on these datasets may be underestimated.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Quantification of PlantSeg performance on the 3D Digital Tissue Atlas, using PlantSeg .</title><p>The Adapted Rand error (ARand) assesses the overall segmentation quality whereas <italic>VOI</italic><sub><italic>merge</italic></sub> and <italic>VOI</italic><sub><italic>split</italic></sub> assess erroneous merge and splitting events. The petal images were not included in our analysis as they are very similar to the leaf and the ground truth is fragmented, making it difficult to evaluate the results from the pipeline in a reproducible way. Segmented images are computed using GASP partitioning with default parameters (left table) and fine-tuned parameters described in Appendix 7: Empirical Example of parameter tuning (right table).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dataset</th><th colspan="3">PlantSeg (default parameters)</th><th colspan="3">PlantSeg (tuned parameters)</th></tr><tr><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th></tr></thead><tbody><tr><td>Anther</td><td>0.328</td><td>0.778</td><td>0.688</td><td>0.167</td><td>0.787</td><td>0.399</td></tr><tr><td>Filament</td><td>0.576</td><td>1.001</td><td>1.378</td><td>0.171</td><td>0.687</td><td>0.487</td></tr><tr><td>Leaf</td><td>0.075</td><td>0.353</td><td>0.322</td><td>0.080</td><td>0.308</td><td>0.220</td></tr><tr><td>Pedicel</td><td>0.400</td><td>0.787</td><td>0.869</td><td>0.314</td><td>0.845</td><td>0.604</td></tr><tr><td>Root</td><td>0.248</td><td>0.634</td><td>0.882</td><td>0.101</td><td>0.356</td><td>0.412</td></tr><tr><td>Sepal</td><td>0.527</td><td>0.746</td><td>1.032</td><td>0.257</td><td>0.690</td><td>0.966</td></tr><tr><td>Valve</td><td>0.572</td><td>0.821</td><td>1.315</td><td>0.300</td><td>0.494</td><td>0.875</td></tr></tbody></table></table-wrap><p>Altogether, PlantSeg performed well qualitatively and quantitatively on datasets acquired by different groups, on different microscopes, and at different resolutions than the training data. This demonstrates the generalization capacity of the pre-trained models from the PlantSeg package.</p></sec><sec id="s2-1-5"><title>Performance on a non-plant benchmark</title><p>For completeness, we compared PlantSeg performance with state-of-the-art methods on a non-plant open benchmark consisting of epithelial cells of the <italic>Drosophila</italic> wing disc (<xref ref-type="bibr" rid="bib19">Funke et al., 2019b</xref>). Visually, the benchmark images are quite similar to the ovules dataset: membrane staining is used along with a confocal microscope, and the cell shapes are compact and relatively regular. Although we did not train the neural networks on the benchmark datasets and used only the ovule pre-trained models provided with the PlantSeg package, we found out that PlantSeg is very competitive qualitatively and quantitatively and faster than the state-of-the-art methods, all of which rely on networks trained directly on the benchmark dataset (see Appendix 3: Performance of PlantSeg on an independent reference benchmark for detailed overview of the benchmark results). We argue that the large selection of pre-trained networks and graph partitioning algorithms make PlantSeg versatile enough to work well on wide variety of membrane stained tissues, beyond plant samples.</p></sec><sec id="s2-1-6"><title>A package for plant tissue segmentation and benchmarking</title><p>We release PlantSeg as an open-source software for the 2D and 3D segmentation of cells with cell contour staining. PlantSeg allows for data pre-processing, boundary prediction with neural networks and segmentation of the network output with a choice of four partitioning methods: Multicut, GASP, Mutex watershed and DT watershed.</p><p>PlantSeg can be executed via a simple graphical user interface or via the command line. The critical parameters of the pipeline are specified in a configuration file. Running PlantSeg from the graphical user interface is well suited for processing of single experiments, while the use of the command line utility enables large scale batch processing and remote execution. Our software can export both the segmentation results and the boundary probability maps as Hierarchical Data Format (HDF5) or Tagged Image File Format (TIFF). Both file formats are widely supported and the results can be further processed by other bioimage analysis tools, such as ilastik, MorphographX or Fiji. In particular: the final segmentation is exported as a labeled volume where all pixels of each segmented cell are assigned the same integer value. It is best viewed with a random lookup table, such as ‘glasbey’ in Fiji. In exported boundary probability maps each pixel has a floating point number between 0 and 1 reflecting a probability of that pixel belonging to a cell boundary. PlantSeg comes with several 2D and 3D networks pre-trained on different voxel size of the Arabidopsis ovule and LRP datasets. Users can select from the available set of pre-trained networks the one with features most similar to their datasets. Alternatively, users can let PlantSeg select the pre-trained network based on the microscope modality (light sheet or confocal) and voxel size. PlantSeg also provides a command-line tool for training the network on new data when none of the pre-trained network is suitable to the user’s needs.</p><p>PlantSeg is publicly available <ext-link ext-link-type="uri" xlink:href="https://github.com/hci-unihd/plant-seg">https://github.com/hci-unihd/plant-seg.</ext-link> The repository includes a complete user guide and the evaluation scripts used for quantitative analysis. Besides the source code, we provide a Linux conda package and a docker image which allows to run PlantSeg on non-Linux operating systems. The software is written in Python, the neural networks use the PyTorch framework <xref ref-type="bibr" rid="bib45">Paszke et al., 2019</xref>. We also make available the raw microscopic images as well as the ground truth used for training, validation and testing.</p></sec></sec><sec id="s2-2"><title>Applications of PlantSeg</title><p>Here, we show case applications of PlantSeg to the analysis of the growth and differentiation of plant organs at cellular resolution.</p><sec id="s2-2-1"><title>Variability in cell number of ovule primordia</title><p>Ovule development in <italic>Arabidopsis thaliana</italic> has been described to follow a stereotypical pattern (<xref ref-type="bibr" rid="bib49">Robinson-Beers et al., 1992</xref>; <xref ref-type="bibr" rid="bib53">Schneitz et al., 1995</xref>). However, it is unclear if ovules within a pistil develop in a synchronous fashion.</p><p>Taking advantage of PlantSeg we undertook an initial assessment of the regularity of primordia formation between ovules developing within a given pistil (<xref ref-type="fig" rid="fig6">Figure 6</xref>). We noticed that spacing between primordia is not uniform (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Our results further indicated that six out of the eight analyzed stage 1 primordia (staging according to <xref ref-type="bibr" rid="bib53">Schneitz et al., 1995</xref>) showed a comparable number of cells (140.5 ± 10.84, mean ± SD, ovules 1–5, 7) (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). However, two primordia exhibited a smaller number of cells with ovule #6 being composed of 91 and ovule #8 of 57 cells, respectively. Interestingly, we observed that the cell number of a primordium does not necessarily translate into its respective height or proximal-distal (PD) extension. For example, ovule #2, which is composed of 150 cells and thus of the second largest number of cells of the analyzed specimen, actually represents the second shortest of the eight primordia with a height of <inline-formula><mml:math id="inf5"><mml:mrow><mml:mn>26.5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). Its comparably large number of cells relates to a wide base of the primordium. Taken together, this analysis indicates that ovule primordium formation within a pistil is relatively uniform, however, some variability can be observed.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Ovule primordium formation in <italic>Arabidopsis thaliana</italic>.</title><p>(<bold>A</bold>) 3D reconstructions of individually labeled stage 1 primordia of the same pistil are shown (stages according to <xref ref-type="bibr" rid="bib53">Schneitz et al., 1995</xref>). The arrow indicates an optical mid-section through an unlabeled primordium revealing the internal cellular structure. The raw 3D image data were acquired by confocal laser scanning microscopy according to <xref ref-type="bibr" rid="bib58">Tofanelli et al., 2019</xref>. Using MorphographX <xref ref-type="bibr" rid="bib3">Barbier de Reuille et al., 2015</xref>, quantitative analysis was performed on the three-dimensional mesh obtained from the segmented image stack. Cells were manually labeled according to the ovule specimen (from <italic>#</italic>1 to <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi mathvariant="normal">#</mml:mi><mml:mo>⁢</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>B, C</bold>) Quantitative analysis of the ovule primordia shown in (<bold>A</bold>). (<bold>B</bold>) shows a graph depicting the total number of cells per primordium. (<bold>C</bold>) shows a graph depicting the proximal-distal (PD) extension of the individual primordia (distance from the base to the tip). Scale bar: 20 µm. Source files used for creation of the scatter plots (<bold>B, C</bold>) are available in the <xref ref-type="supplementary-material" rid="fig6sdata1">Figure 6—source data 1</xref>.</p><p><supplementary-material id="fig6sdata1"><label>Figure 6—source data 1.</label><caption><title>Source data for panes B and C in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</title><p>The archive contains: 'ovule-results.csv' - number of cells and extension for different ovule primordium, 'ovule-scatter.ipynb' - Jupyter notebook for generating panes B and C.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-57613-fig6-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig6-v2.tif"/></fig></sec><sec id="s2-2-2"><title>Asymmetric division of lateral root founder cells</title><p><italic>Arabidopsis thaliana</italic> constantly forms lateral roots (LRs) that branch from the main root. These LRs arise from a handful of founder cells located in the pericycle, a layer of cells located deep within the primary root. Upon lateral root initiation, these founder cells invariably undergo a first asymmetric division giving rise to one small and one large daughter cell. Although the asymmetric nature of this division has long been reported (<xref ref-type="bibr" rid="bib32">Laskowski et al., 1995</xref>; <xref ref-type="bibr" rid="bib38">Malamy and Benfey, 1997</xref>) and its importance realised (<xref ref-type="bibr" rid="bib67">von Wangenheim et al., 2016</xref>), it is still unknown how regular the volume partitioning is between the daughter cells. We used the segmentation of the LR dataset produced by PlantSeg to quantify this ratio. The asymmetric divisions were identified by visual examination during the first 12 hr of the recording and the volumes of the two daughter cells retrieved (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). The analysis of cell volume ratios confirmed that the first division of the founder cell is asymmetric with a volume ratio between the two daughter cells of 0.65 ± 0.22 (mean ± SD, <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>23</mml:mn></mml:mrow></mml:math></inline-formula>) (<xref ref-type="fig" rid="fig7">Figure 7C</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Asymmetric cell division of lateral root founder cells.</title><p>(<bold>A</bold>) Schematic representation of <italic>Arabidopsis thaliana</italic> with lateral roots (LR). The box depicts the region of the main root that initiates LRs. (<bold>B</bold>) 3D reconstructions of LR founder cells seen from the side and from the front at the beginning of recording (<bold>t</bold>) and after 12 hr (<italic>t+12</italic>). The star and brackets indicate the two daughter cells resulting from the asymmetric division of a LR founder cell. (<bold>C</bold>) Half-violin plot of the distribution of the volume ratio between the daughter cells for three different movies (<italic>#</italic>1, <italic>#</italic>2 and <italic>#</italic>3). The average ratio of 0.6 indicates that the cells divided asymmetrically. Source files used for analysis and violin plot creation are available in <xref ref-type="supplementary-material" rid="fig7sdata1">Figure 7—source data 1</xref>.</p><p><supplementary-material id="fig7sdata1"><label>Figure 7—source data 1.</label><caption><title>Source data for asymmetric cell division measurements in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</title><p>A detailed description of how to generate the pane C can be found in '<xref ref-type="fig" rid="fig7">Figure 7C</xref>.pdf'.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-57613-fig7-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig7-v2.tif"/></fig></sec><sec id="s2-2-3"><title>Epidermal cell volumes in a shoot apical meristem</title><p>Epidermal cell morphologies in the shoot apical meristem of <italic>Arabidopsis thaliana</italic> are genetically controlled and even subtle changes can have an impact on organogenesis and pattern formation. To quantify respective cell shapes and volumes in the newly identified <italic>big cells in epidermis</italic> (<italic>bce</italic>) mutant we used the PlantSeg package to analyze image volumes of six <italic>Arabidopsis thaliana</italic> meristems (three wild type and three <italic>bce</italic> specimens).</p><p>Inflorescence meristems of <italic>Arabidopsis thaliana</italic> were imaged using confocal las[er scanning microscopy after staining cell walls with DAPI. Image volumes <inline-formula><mml:math id="inf8"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>167</mml:mn><mml:mo>×</mml:mo><mml:mn>167</mml:mn><mml:mo>×</mml:mo><mml:mn>45</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> were used to obtain 3D cell segmentations using PlantSeg: in this case a 3D UNet trained on the <italic>Arabidopsis</italic> ovules was used in combination with the Multicut algorithm. This segmentation procedure allows to determine epidermal cell volumes for wild-type (<xref ref-type="fig" rid="fig8">Figure 8A</xref>) and the <italic>bce</italic> mutant (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). Cells within a radius of 33 µm around the manually selected center of the meristem (colored cells in <xref ref-type="fig" rid="fig8">Figure 8A and B</xref>) were used for the cell volume quantification shown in <xref ref-type="fig" rid="fig8">Figure 8C</xref>. The mean volume of epidermal cells in the <italic>bce</italic> mutant is increased by roughly 50% whereas overall meristem size is only slightly reduced which implicates changes in epidermal cell division in mutant meristems.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Volume of epidermal cell in the shoot apical meristem of Arabidopsis.</title><p>Segmentation of epidermal cells in <italic>wildtype</italic> (<bold>A</bold>) and <italic>bce</italic> mutant (<bold>B</bold>). Cells located at the center of the meristem are colored. Scale bar: 20 µm. (<bold>C</bold>) Quantification of cell volume (µm<sup>3</sup>) in three different <italic>wildtype</italic> and <italic>bce</italic> mutant specimens. Source files used for cell volume quantification are available in the <xref ref-type="supplementary-material" rid="fig8sdata1">Figure 8—source data 1</xref>.</p><p><supplementary-material id="fig8sdata1"><label>Figure 8—source data 1.</label><caption><title>Source data for volume measurements of epidermal cells in the shoot apical meristem (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</title><p>Volume measurements can be found in 'cell_volume_data.csv'. 'fig8_mutant.ipynb' contains the script to generate the plot in pane C.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-57613-fig8-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig8-v2.tif"/></fig></sec><sec id="s2-2-4"><title>Analysis of leaf growth and differentiation</title><p>Leaves are a useful system to study morphogenesis in the context of evolution because final organ forms of different species show considerable variation despite originating from almost indistinguishable buds (<xref ref-type="bibr" rid="bib29">Kierzkowski et al., 2019</xref>). To track leaf growth, the same sample is imaged over the course of several days, covering a volume much larger than ovules or meristems. To reduce stress and growth arrest it is required to use relatively low resolution and laser intensity which makes an accurate full 3D segmentation more challenging. Because leaves grow mainly in two dimensions, their morphogenesis can be tracked on the organ surface. We therefore use the software platform MorphoGraphX which is specialized in creating and analyzing curved surface segmentations (<xref ref-type="bibr" rid="bib3">Barbier de Reuille et al., 2015</xref>). It offers a semi-automatic surface segmentation pipeline using a seeded watershed algorithm (Figure 10A–C) but segmentation errors require extensive curation by the user (<xref ref-type="bibr" rid="bib29">Kierzkowski et al., 2019</xref>). We tested whether PlantSeg can improve the segmentation pipeline of MorphoGraphX by using PlantSeg’s membrane prediction and 3D segmented output files as additional input for creating the surface segmentation in MorphoGraphX. We used confocal laser scanning microscopy stacks of <italic>Arabidopsis thaliana</italic> and <italic>Cardamine hirsuta</italic> leaves fluorescently tagged at the cell boundaries (Figure 10A). Voxel sizes ranged from <inline-formula><mml:math id="inf9"><mml:mrow><mml:mn>0.33</mml:mn><mml:mo>×</mml:mo><mml:mn>0.33</mml:mn><mml:mo>×</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf10"><mml:mrow><mml:mrow><mml:mn>0.75</mml:mn><mml:mo>×</mml:mo><mml:mn>0.75</mml:mn><mml:mo>×</mml:mo><mml:mn>0.6</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>We compared the auto-segmentation produced by MorphoGraphX using the original raw stacks as input (RawAutoSeg) to the one produced by MorphoGraphX using PlantSeg’s wall prediction (PredAutoSeg) or by projecting PlantSeg’s 3D segmentation (Proj3D) (Figure 10B,C). We tested six different samples and computed the quality measures for the results of all different methods: ARand, <italic>VOI</italic><sub><italic>merge</italic></sub> and <italic>VOI</italic><sub><italic>split</italic></sub> as well as the accuracy (% of correctly segmented cells compared to the GT). The two methods using PlantSeg input produced lower ARand scores and higher accuracy than using the raw input (<xref ref-type="fig" rid="fig9">Figure 9</xref>). Therefore, combining PlantSeg with MorphographX produced segmentations more similar to the GT at the vertex and cell levels.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Leaf surface segmentation results.</title><p>Reported are the ARand error (<bold>A</bold>) that assesses the overall segmentation quality and the accuracy (<bold>B</bold>) measured as percentage of correctly segmented cells (by manual assessment of a trained biologist). For more detailed results, see <xref ref-type="table" rid="app5table3">Appendix 5—table 3</xref>.</p><p><supplementary-material id="fig9sdata1"><label>Figure 9—source data 1.</label><caption><title>Source data for leaf surface segmentation in <xref ref-type="fig" rid="fig9">Figure 9</xref>.</title><p>The archive contains: 'final_mesh_evaluation - Sheet1.csv' - CSV file with evaluation scores computed on individual meshes, 'Mesh_boxplot.pdf' - detailed steps to reproduce the graphs, 'Mesh_boxplot.ipynb' - python script for generating the graph.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-57613-fig9-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig9-v2.tif"/></fig><p>Next, we used the PlantSeg segmentation to measure growth over 24 hr at cellular resolution and compare differentiation in <italic>A. thaliana</italic> and <italic>C. hirsuta</italic> 600 µm-long leaves. Growth was slow in the midrib and distal margin cells, whereas the remaining blade displayed a gradient along the proximal-distal axis with the maximum values at the basal margin (<xref ref-type="fig" rid="fig10">Figure 10D</xref>). Tissue differentiation typically starts at the apex of leaves and progresses basipetally influencing this growth gradient. To compare this process between <italic>A. thaliana</italic> and <italic>C. hirsuta</italic> leaves, for each cell, we extracted its distance to the leaf base together with its area and lobeyness, attributes positively correlated with differentiation (<xref ref-type="bibr" rid="bib29">Kierzkowski et al., 2019</xref>). Overall, <italic>A. thaliana</italic> leaves showed higher cell size and lobeyness, and this difference accentuated towards the apex, confirming earlier differentiation onset in this species (<xref ref-type="fig" rid="fig10">Figure 10E,F</xref>).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Creation of cellular segmentations of leaf surfaces and downstream quantitative analyses.</title><p>(<bold>A–C</bold>) Generation of a surface segmentation of a <italic>C. hirsuta</italic> leaf in MorphoGraphX assisted by PlantSeg. (<bold>A</bold>) Confocal image of a 5-day-old <italic>C. hirsuta</italic> leaf (leaf number 5) with an enlarged region. (<bold>B</bold>) Top: Segmentation pipeline of MorphoGraphX: a surface mesh is extracted from the raw confocal data and used as a canvas to project the epidermis signal. A seed is placed in each cell on the surface for watershed segmentation. Bottom: PlantSeg facilitates the segmentation process in two different ways (red arrows): By creating clean wall signals which can be projected onto the mesh instead of the noisy raw data and by projecting the labels of the 3D segmentation onto the surface to obtain accurate seeds for the cells. Both methods reduce segmentation errors with the first method to do so more efficiently. (<bold>C</bold>) Fully segmented mesh in MorphoGraphX. (<bold>D–F</bold>) Quantification of cell parameters from segmented meshes. (<bold>D</bold>) Heatmap of cell growth in an <italic>A. thaliana</italic> 8th-leaf 4 to 5 days after emergence. (<bold>E</bold>) Comparison of cell lobeyness between <italic>A. thaliana</italic> and <italic>C. hirsuta</italic> 600 µm-long leaves. (<bold>F</bold>) Average cell lobeyness and area in <italic>A. thaliana</italic> and <italic>C. hirsuta</italic> binned by cell position along the leaf proximal-distal axis. Scale bars: 50 µm (<bold>A, C</bold>), 100 µm (<bold>D, E</bold>), 5 µm (inset in A, (<bold>B</bold>). Source files used for generating quantitative results (<bold>D–F</bold>) are available in <xref ref-type="supplementary-material" rid="fig10sdata1">Figure 10—source data 1</xref>.</p><p><supplementary-material id="fig10sdata1"><label>Figure 10—source data 1.</label><caption><title>Source data for pane F in <xref ref-type="fig" rid="fig10">Figure 10</xref> (cell area and lobeyness analysis).</title><p>'Figure 10-source data 1.xlsx' contains all the measurements used to generate the plot in pane F.</p></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-57613-fig10-data1-v2.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-fig10-v2.tif"/></fig></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Taking advantage of the latest developments in machine learning and computer vision we created PlantSeg, a simple, powerful, and versatile tool for plant cell segmentation. Internally, it implements a two-step algorithm: the images are first passed through a state-of-the-art convolutional neural network to detect cell boundaries. In the second step, the detected boundaries are used to over-segment the image using the distance transform watershed and then a region adjacency graph of the image superpixels is constructed. The graph is partitioned to deliver accurate segmentation even for noisy live imaging of dense plant tissue.</p><p>PlantSeg was trained on confocal images of <italic>Arabidopsis thaliana</italic> ovules and light sheet images of the lateral root primordia and delivers high-quality segmentation on images from these datasets never seen during training as attested by both qualitative and quantitative benchmarks. We experimented with different U-Net designs and hyperparameters, as well as with different graph partitioning algorithms, to equip PlantSeg with the ones that generalize the best. This is illustrated by the excellent performance of PlantSeg without retraining of the CNNs on a variety of plant tissues and organs imaged using confocal microscopy (3D Cell Atlas Dataset) including the highly lobed epidermal cells (<xref ref-type="bibr" rid="bib16">Fox et al., 2018</xref>). This feature underlines the versatility of our approach for images presenting similar features to the ones used in training. In addition, PlantSeg comes with scripts to train a CNN on a new set of images and evaluate its performance. Given the importance of ground truth for training of CNNs we also provide instructions on how to generate ground truth in the Appendix 1: Groundtruth Creation. Besides the plant data, we compared PlantSeg to the state-of-the-art on an open benchmark for the segmentation of epithelial cells in the <italic>Drosophila</italic> wing disc. Using only the pre-trained networks, PlantSeg performance was shown to be close to the benchmark leaders, while additional training on challenge data has narrowed the gap even further.</p><p>We demonstrate the usefulness of PlantSeg on four concrete biological applications that require accurate extraction of cell geometries from complex, densely packed 3D tissues. First, PlantSeg allowed to sample the variability in the development of ovules in a given pistil and reveal that those develop in a relatively synchronous manner. Second, PlantSeg allowed the precise computation of the volumes of the daughter cells resulting from the asymmetric division of the lateral root founder cell. This division results in a large and a small daughter cells with volume ratio of <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> between them. Third, segmentation of the epidermal cells in the shoot apical meristem revealed that these cells are enlarged in the <italic>bce</italic> mutant compared to wild type. Finally, we showed that PlantSeg can be used to improve the automated surface segmentation of time-lapse leaf stacks which enables different downstream analyses such as growth tracking at cell resolution. Accurate and versatile extraction of cell outlines rendered possible by PlantSeg opens the door to rapid and robust quantitative morphometric analysis of plant cell geometry in complex tissues. This is particularly relevant given the central role plant cell shape plays in the control of cell growth and division (<xref ref-type="bibr" rid="bib47">Rasmussen and Bellinger, 2018</xref>).</p><p>Unlike intensity-based segmentation methods used, for example, to extract DAPI-stained cell nuclei, our approach relies purely on boundary information derived from cell contour detection. While this approach grants access to the cell morphology and cell-cell interactions, it brings additional challenges to the segmentation problem. Blurry or barely detectable boundaries lead to discontinuities in the membrane structure predicted by the network, which in turn might cause cells to be under-segmented. The segmentation results produced by PlantSeg on new datasets are not fully perfect and still require proof-reading to reach 100% accuracy. For our experiments we used Paintera (<xref ref-type="bibr" rid="bib21">Hanslovsky et al., 2019</xref>) for manually correcting the 3D segmentation results. Importantly the newly proof-read results can then be used to train a better network that can be applied to this type of data in the future (see the Appendix 1: Groundtruth Creation for an overview of this process). If nuclei are imaged along with cell contours, nuclear signal can be leveraged for improving the segmentation as we have explored in <xref ref-type="bibr" rid="bib44">Pape et al., 2019</xref> (see Appendix 2: Exploiting nuclei staining to improve the lateral root cells' segmentation for detailed procedure). In future work, we envision developing new semi-supervised approaches that would exploit the vast amounts of unlabeled data available in the plant imaging community.</p><p>During the development of PlantSeg, we realised that very few benchmark datasets were available to the community for plant cell segmentation tasks, a notable exception being the 3D Tissue Atlas (<xref ref-type="bibr" rid="bib4">Bassel, 2019</xref>). To address this gap, we publicly release our sets of images and the corresponding hand-curated ground truth in the hope to catalyse future development and evaluation of cell instance segmentation algorithms.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Biological material and imaging</title><p>Imaging of the <italic>Arabidopsis thaliana</italic> ovules was performed as described in <xref ref-type="bibr" rid="bib58">Tofanelli et al., 2019</xref>. Imaging of the shoot apical meristem was performed as previously described <xref ref-type="bibr" rid="bib66">von Wangenheim et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Heisler and Ohno, 2014</xref> with a confocal laser scanning microscope (Nikon A1, 25 × NA = 1.1) after staining cell walls with DAPI (0.2 mg/ml).</p><p>For imaging of <italic>Arabidopsis thaliana</italic> lateral root, seedlings of the line sC111 (<italic>UB10<sub>pro</sub> :: PIP 1,4-3 × GFP/GAT A23<sub>pro</sub> :: H2B : 3 × mCherry/DR5v2<sub>pro</sub> :: 3 × YFPnls/RPS5A<sub>pro</sub> :: dtTomato : NLS</italic>, described in <xref ref-type="bibr" rid="bib65">Vilches Barro et al., 2019</xref>) were used at 5 day post germination. Sterilized seeds were germinated on top of 4.5 mm long Blaubrand micropipettes (Cat 708744; 100 μl) that were immobilised on the bottom of a petri dish and covered with <inline-formula><mml:math id="inf12"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:math></inline-formula> MS-phytagel (<xref ref-type="bibr" rid="bib37">Maizel et al., 2011</xref>). Before sowing, the top of the micropipettes is exposed by removing the phytagel with a razor blade and one seed is sowed per micropipette. Plates were stratified for two days and transferred to a growth incubator (23°C, 16 h day light). Imaging was performed on a Luxendo MuViSPIM (<ext-link ext-link-type="uri" xlink:href="https://luxendo.eu/products/muvi-spim/">https://luxendo.eu/products/muvi-spim/</ext-link>) equipped with two 10×NA=0.3for illumination and 40×NA=0.8for detection. The following settings were used for imaging: image size <inline-formula><mml:math id="inf13"><mml:mrow><mml:mn>2048</mml:mn><mml:mo>×</mml:mo><mml:mn>2048</mml:mn></mml:mrow></mml:math></inline-formula>, exposure time 75 ms, channel #1 illumination 488 nm 10% power, detection 497–553 nm band pass filter, channel #2 illumination 561 nm 10% power, detection 610–628 nm band pass filter. Stacks encompassing the whole volume of the root were acquired every 30 min. Images from the two cameras were fused using the Luxendo Image processing tool and registered to correct any 3D drift using the BigDataProcessor (<xref ref-type="bibr" rid="bib57">Tischer et al., 2019</xref>) in Fiji (<xref ref-type="bibr" rid="bib52">Schindelin et al., 2012</xref>).</p><p>Leaves were grown and imaged as described previously (<xref ref-type="bibr" rid="bib29">Kierzkowski et al., 2019</xref>). Cells were visualized either by expressing of UBQ10::acyl:YFP (<xref ref-type="bibr" rid="bib70">Willis et al., 2016</xref>) or by staining with 10 mg/mL propidium iodide for 15 min. The bce mutant is a yet uncharacterised recessive mutant obtained in J. Lohmanns lab. The phenotype was observed after T-DNA transformation of Arabidopsis Col-0 plants.</p></sec><sec id="s4-2"><title>Creation of leaf surface segmentations</title><p>To compare the segmentations created by MorphoGraphX alone with the ones using PlantSeg’s files as input, we first obtained a ground-truth segmentation using the MorphographX auto-segmentation pipeline as described in <xref ref-type="bibr" rid="bib55">Strauss et al., 2019</xref> (<xref ref-type="fig" rid="fig10">Figure 10B</xref>) and manually fixed all segmentation errors using processes in MorphoGraphX. We then fed the confocal stacks to PlantSeg to compute wall predictions and 3D segmentations using the network trained on the ovule confocal data and the GASP method. Note that for samples with weaker cell wall signal we processed the raw input data in MorphoGraphX by adding a 2 µm thick layer of signal under the surface mesh and fed these to PlantSeg which tended to improve the PlantSeg output greatly. We then created surface segmentation using three methods: First, using the raw stack and the auto-segmentation pipeline in MorphoGraphX (method RawAutoSeg, <xref ref-type="fig" rid="fig10">Figure 10B</xref>, top). Second, using PlantSeg’s wall prediction values as input for the auto-segmentation process in MorphoGraphX (method PredAutoSeg, <xref ref-type="fig" rid="fig10">Figure 10B</xref>, left red arrow) and third, using PlantSeg’s fully segmented stack and projecting the resulting 3D labels onto the surface mesh using a custom process in MorphoGraphX (method Proj3D, <xref ref-type="fig" rid="fig10">Figure 10B</xref>, right red arrow).</p></sec><sec id="s4-3"><title>Neural network training and inference</title><sec id="s4-3-1"><title>Training</title><p>2D and 3D U-Nets were trained to predict the binary mask of cell boundaries. Ground truth cell contours where obtained by taking the ground truth label volume, finding a two voxels-thick boundaries between labeled regions (<inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">_</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> function from the Scikit-image package (<xref ref-type="bibr" rid="bib62">van der Walt et al., 2014</xref>) and applying a Gaussian blur on the resulting boundary image. Gaussian smoothing reduces the high frequency components in the boundary image, which helps prevent over-fitting and makes the boundaries thicker, increasing the amount of foreground signal during training. Transforming the label image <inline-formula><mml:math id="inf15"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">𝒮</mml:mi><mml:mi mathvariant="bold">𝐱</mml:mi></mml:msub></mml:math></inline-formula> into the boundary image <inline-formula><mml:math id="inf16"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℐ</mml:mi><mml:mi mathvariant="bold">𝐱</mml:mi></mml:msub></mml:math></inline-formula> is depicted in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">ℐ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>if </mml:mtext></mml:mstyle><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">𝒮</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>otherwise </mml:mtext></mml:mstyle></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> transforms the labeled volume into the boundary image, <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>G</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:math></inline-formula> is the isotropic Gaussian kernel and * denotes a convolution operator. We use <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math></inline-formula> in our experiments. Both standard and residual U-Net architectures were trained using Adam optimizer (<xref ref-type="bibr" rid="bib30">Kingma and Ba, 2014</xref>) with <inline-formula><mml:math id="inf20"><mml:mrow><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, L2 penalty of 0.00001 and initial learning rate <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0002</mml:mn></mml:mrow></mml:math></inline-formula>. Networks were trained until convergence for 150K iterations, using the PyTorch framework (<xref ref-type="bibr" rid="bib45">Paszke et al., 2019</xref>) on 8 NVIDIA GeForce RTX 2080 Ti GPUs. For validation during training, we used the adjusted Rand error computed between the ground truth segmentation and segmentation obtained by thresholding the probability maps predicted by the network and running the connected components algorithm. The learning rate was being reduced by a factor of 2 once the learning stagnated during training, that is, no improvements were observed on the validation set for a given number of iterations. We choose as best network the one with lowest Arand error values. For training with small patch sizes we used 4 patches of shape <inline-formula><mml:math id="inf22"><mml:mrow><mml:mn>100</mml:mn><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mo>×</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula> and batch normalization (<xref ref-type="bibr" rid="bib25">Ioffe and Szegedy, 2015</xref>) per network iteration. When training with a single large patch (size <inline-formula><mml:math id="inf23"><mml:mrow><mml:mn>170</mml:mn><mml:mo>×</mml:mo><mml:mn>170</mml:mn><mml:mo>×</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math></inline-formula>), we used group normalization layers (<xref ref-type="bibr" rid="bib74">Wu and He, 2018</xref>) instead of batch normalization. The reason is that batch normalization with a single patch per iteration becomes an instance normalization (<xref ref-type="bibr" rid="bib61">Ulyanov et al., 2016</xref>) and makes the estimated batch statistics weaker. All networks use the same layer ordering where the normalization layer is followed by the 3D convolution and a rectified linear unit (ReLU) activation. This order of layers consistently performed better than alternative orderings. During training and inference, input images were standardized by subtracting mean intensity and dividing by the standard deviation. We used random horizontal and vertical flips, random rotations in the XY-plane, elastic deformations (<xref ref-type="bibr" rid="bib51">Ronneberger et al., 2015</xref>) and noise augmentations (additive Gaussian, additive Poisson) of the input image during training in order to increase network generalization on unseen data. The performance of CNNs is sensitive to changes in voxel size and object sizes between training and test images (<xref ref-type="bibr" rid="bib63">van Noord and Postma, 2017</xref>). We thus also trained the networks using the original datasets downscaled by a factor of 2 and 3 in the XY dimension.</p><p>3D U-Nets trained at different scales of our two core datasets (light-sheet lateral root, confocal ovules) are made available as part of the PlantSeg package. All released networks were trained according to the procedure described above using a combination of binary cross-entropy and Dice loss:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>(we set <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> in our experiments) and follow the standard U-Net architecture (<xref ref-type="bibr" rid="bib51">Ronneberger et al., 2015</xref>) with two minor modifications: batch normalization (<xref ref-type="bibr" rid="bib25">Ioffe and Szegedy, 2015</xref>) is replaced by group normalization (<xref ref-type="bibr" rid="bib74">Wu and He, 2018</xref>) and same convolutions are used instead of valid convolutions. For completeness we also publish 2D U-Nets trained using the Z-slices from the original 3D stacks, enabling segmentation of 2D images with PlantSeg.</p></sec><sec id="s4-3-2"><title>Inference</title><p>During inference we used mirror padding on the input image to improve the prediction at the boundaries of the volume. We kept the same patch sizes as during training since increasing it during inference might lead to lower quality of the predictions, especially on the borders of the patch. We also parse the volume patch-by-patch with a 50% overlap between consecutive tiles and average the probability maps. This strategy prevents checkerboard artifacts and reduces noise in the final prediction.</p><p>The code used for training and inference can be found at <xref ref-type="bibr" rid="bib73">Wolny, 2020b</xref> <ext-link ext-link-type="uri" xlink:href="https://github.com/wolny/pytorch-3dunet">https://github.com/wolny/pytorch-3dunet</ext-link> copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/pytorch-3dunet">https://github.com/elifesciences-publications/pytorch-3dunet</ext-link>.</p></sec></sec><sec id="s4-4"><title>Segmentation using graph partitioning</title><p>The boundary predictions produced by the CNN are treated as a graph <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where nodes <italic>V</italic> are represented by the image voxels, and the edges <italic>E</italic> connect adjacent voxels. The weight <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> of each edge is derived from the boundary probability maps. On this graph we first performed an over-segmentation by running the DT watershed (<xref ref-type="bibr" rid="bib50">Roerdink and Meijster, 2000</xref>). For this, we threshold the boundary probability maps at a given value δ to get a binary image (<inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:math></inline-formula> was chosen empirically in our experiments). Then we compute the distance transform from the binary boundary image, apply a Gaussian smoothing (<inline-formula><mml:math id="inf29"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>2.0</mml:mn></mml:mrow></mml:math></inline-formula>) and assign a seed to every local minimum in the resulting distance transform map. Finally we remove small regions (&lt;50 voxels). Standalone DT watershed already delivers accurate segmentation and can be used as is in simple cases when, for example noise is low and/or boundaries are sharp.</p><p>For Multicut (<xref ref-type="bibr" rid="bib28">Kappes et al., 2011</xref>), GASP (<xref ref-type="bibr" rid="bib2">Bailoni et al., 2019</xref>), and Mutex watershed (<xref ref-type="bibr" rid="bib71">Wolf et al., 2018</xref>) algorithms, we used the DT watershed as an input. Although all three algorithms could be run directly on the boundary predictions produced by the CNN (voxel level), we choose to run them on a region adjacency graph (RAG) derived from the DT watershed to reduce the computation time. In the region adjacency graph each node represents a region and edges connect adjacent regions. We compute edge weights by using the mean value of the probabilities maps along the boundary. We then run Multicut, GASP or Mutex watershed with a hyperparameter <inline-formula><mml:math id="inf30"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn></mml:mrow></mml:math></inline-formula> that balances over- and under-segmentation (with higher <inline-formula><mml:math id="inf31"><mml:mi>β</mml:mi></mml:math></inline-formula> tending to over-segment). As a general guideline for choosing the partitioning strategy on a new data is to start with GASP algorithm, which is the most generic. If needed, one may try to improve the results with multicut or mutex watershed. If none of the three strategies give satisfactory segmentation results we recommend to over-segment provided stack using the distance transform watershed and proofread the result manually using Paintera software (<xref ref-type="bibr" rid="bib21">Hanslovsky et al., 2019</xref>).</p><p>A detailed overview of the parameters exposed via the PlantSeg’s UI can be found on the project’s GitHub page <ext-link ext-link-type="uri" xlink:href="https://github.com/hci-unihd/plant-seg">https://github.com/hci-unihd/plant-seg</ext-link> as well as in <xref ref-type="table" rid="app6table3">Appendix 6—table 3</xref>.</p></sec><sec id="s4-5"><title>Metrics used for evaluation</title><p>For the boundary predictions we used precision (number of pixels positively predicted as boundary divided by the number of boundary pixels in the ground truth), recall (number of positively predicted boundary pixels divided by the sum of positively and negatively predicted boundary pixels) and F1 score<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mrow><mml:mtext>?</mml:mtext><mml:mo>⁢</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mtext>?????????</mml:mtext><mml:mo>⋅</mml:mo><mml:mtext>??????</mml:mtext></mml:mrow><mml:mrow><mml:mtext>?????????</mml:mtext><mml:mo>+</mml:mo><mml:mtext>??????</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the final segmentation, we used the inverse of the Adjusted Rand Index (AdjRand) <xref ref-type="bibr" rid="bib46">Rand, 1971</xref> defined as <inline-formula><mml:math id="inf32"><mml:mrow><mml:mtext>ARand error</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mtext>AdjRand</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib12">CREMI, 2017</xref>) which measures the distance between two clustering as global measure of accuracy between PlantSeg prediction and ground truth. An ARand error of 0 means that the PlantSeg results are identical to the ground truth, whereas one shows no correlation between the ground truth and the segmentation results. To quantify the rate of merge and split errors, we used the Variation of Information (VOI) which is an entropy based measure of clustering quality (<xref ref-type="bibr" rid="bib40">Meila, 2005</xref>). It is defined as:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mtext>VOI</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>seg</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>GT</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>GT</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>seg</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>H</italic> is the conditional entropy function and the Seg and GT the predicted segmentation and ground truth segmentation respectively. <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>seg</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>GT</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> defines the split mistakes (<italic>VOI</italic><sub><italic>split</italic></sub>) whereas <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>GT</mml:mtext><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mtext>Seg</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> corresponds to the merge mistakes (<italic>VOI</italic><sub><italic>merge</italic></sub>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Kerem Celikay, Melanie Holzapfel and Boyko Vodenicharski for their help in the early steps of the project. We are grateful to Natalie Dye from MPI-CBG for sharing the flywing data and annotations. We further acknowledge the support of the Center for Advanced Light Microscopy (CALM) at the TUM School of Life Sciences. GWB and SD-N were funded by Leverhulme Grant RPG-2016–049. CP and AK were funded by Baden-Wuerttemberg Stiftung. MT acknowledges support from the German Federal Ministry of Education and Research (BMBF, grant number 031B0189B), in the context of the project ‘Enhancing Crop Photosynthesis (EnCroPho)”. This work was supported by the DFG FOR2581 to the Hamprecht (P3), Kreshuk (P3), Lohmann (P5), Maizel (P6), Schneitz (P7) and Tsiantis (P9) labs.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Formal analysis</p></fn><fn fn-type="con" id="con5"><p>Data curation, Formal analysis</p></fn><fn fn-type="con" id="con6"><p>Data curation</p></fn><fn fn-type="con" id="con7"><p>Data curation, Visualization, Writing - original draft</p></fn><fn fn-type="con" id="con8"><p>Data curation, Software, Formal analysis, Investigation, Visualization, Writing - original draft</p></fn><fn fn-type="con" id="con9"><p>Data curation, Software, Formal analysis, Investigation, Visualization, Writing - original draft</p></fn><fn fn-type="con" id="con10"><p>Data curation</p></fn><fn fn-type="con" id="con11"><p>Data curation</p></fn><fn fn-type="con" id="con12"><p>Data curation, Software, Methodology</p></fn><fn fn-type="con" id="con13"><p>Software, Methodology</p></fn><fn fn-type="con" id="con14"><p>Resources, Software</p></fn><fn fn-type="con" id="con15"><p>Resources</p></fn><fn fn-type="con" id="con16"><p>Resources, Supervision</p></fn><fn fn-type="con" id="con17"><p>Conceptualization, Resources, Supervision</p></fn><fn fn-type="con" id="con18"><p>Conceptualization, Resources, Software, Supervision, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con19"><p>Conceptualization, Software, Formal analysis, Supervision, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con20"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Investigation, Methodology, Writing - original draft, Project administration</p></fn><fn fn-type="con" id="con21"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing - original draft, Project administration</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-57613-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data used in this study have been deposited in Open Science Framework: <ext-link ext-link-type="uri" xlink:href="https://osf.io/uzq3w">https://osf.io/uzq3w</ext-link>.</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Wilson-Sánchez</surname><given-names>D</given-names></name><name><surname>Lymbouridou</surname><given-names>R</given-names></name><name><surname>Strauss</surname><given-names>S</given-names></name><name><surname>Tsiantis</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>CLSM Leaf</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/kfx3d/">10.17605/OSF.IO/KFX3D</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Wenzl</surname><given-names>C</given-names></name><name><surname>Lohmann</surname><given-names>JU</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Inflorescence Meristem</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/295su/">10.17605/OSF.IO/295SU</pub-id></element-citation></p><p><element-citation id="dataset3" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Louveaux</surname><given-names>M</given-names></name><name><surname>Maizel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>A. Thaliana Lateral Root</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/2rszy/">10.17605/OSF.IO/2RSZY</pub-id></element-citation></p><p><element-citation id="dataset4" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Tofanelli</surname><given-names>R</given-names></name><name><surname>Vijayan</surname><given-names>A</given-names></name><name><surname>Schneitz</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>A. Thaliana Ovules</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/w38uf/">10.17605/OSF.IO/W38UF</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation id="dataset5" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Duran-Nebreda</surname><given-names>S</given-names></name><name><surname>Bassel</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Arabidopsis 3D Digital Tissue Atlas</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/fzr56/">OSF</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Aigouy</surname> <given-names>B</given-names></name><name><surname>Umetsu</surname> <given-names>D</given-names></name><name><surname>Eaton</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Segmentation and quantitative analysis of epithelial tissues</chapter-title><person-group person-group-type="editor"><name><surname>Dahmann</surname> <given-names>C</given-names></name></person-group><source>Drosophila Methods in Molecular Biology</source><publisher-name>Humana Press</publisher-name><fpage>420</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-6371-3_13</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bailoni</surname> <given-names>A</given-names></name><name><surname>Pape</surname> <given-names>C</given-names></name><name><surname>Wolf</surname> <given-names>S</given-names></name><name><surname>Beier</surname> <given-names>T</given-names></name><name><surname>Kreshuk</surname> <given-names>A</given-names></name><name><surname>Hamprecht</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A generalized framework for agglomerative clustering of signed graphs applied to instance segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.11713">https://arxiv.org/abs/1906.11713</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbier de Reuille</surname> <given-names>P</given-names></name><name><surname>Routier-Kierzkowska</surname> <given-names>A-L</given-names></name><name><surname>Kierzkowski</surname> <given-names>D</given-names></name><name><surname>Bassel</surname> <given-names>GW</given-names></name><name><surname>Schüpbach</surname> <given-names>T</given-names></name><name><surname>Tauriello</surname> <given-names>G</given-names></name><name><surname>Bajpai</surname> <given-names>N</given-names></name><name><surname>Strauss</surname> <given-names>S</given-names></name><name><surname>Weber</surname> <given-names>A</given-names></name><name><surname>Kiss</surname> <given-names>A</given-names></name><name><surname>Burian</surname> <given-names>A</given-names></name><name><surname>Hofhuis</surname> <given-names>H</given-names></name><name><surname>Sapala</surname> <given-names>A</given-names></name><name><surname>Lipowczan</surname> <given-names>M</given-names></name><name><surname>Heimlicher</surname> <given-names>MB</given-names></name><name><surname>Robinson</surname> <given-names>S</given-names></name><name><surname>Bayer</surname> <given-names>EM</given-names></name><name><surname>Basler</surname> <given-names>K</given-names></name><name><surname>Koumoutsakos</surname> <given-names>P</given-names></name><name><surname>Roeder</surname> <given-names>AHK</given-names></name><name><surname>Aegerter-Wilmsen</surname> <given-names>T</given-names></name><name><surname>Nakayama</surname> <given-names>N</given-names></name><name><surname>Tsiantis</surname> <given-names>M</given-names></name><name><surname>Hay</surname> <given-names>A</given-names></name><name><surname>Kwiatkowska</surname> <given-names>D</given-names></name><name><surname>Xenarios</surname> <given-names>I</given-names></name><name><surname>Kuhlemeier</surname> <given-names>C</given-names></name><name><surname>Smith</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>MorphoGraphX: a platform for quantifying morphogenesis in 4D</article-title><source>eLife</source><volume>4</volume><elocation-id>e05864</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05864</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bassel</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>Arabidopsis 3d Digital Tissue Atlas OSF</source></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beier</surname> <given-names>T</given-names></name><name><surname>Pape</surname> <given-names>C</given-names></name><name><surname>Rahaman</surname> <given-names>N</given-names></name><name><surname>Prange</surname> <given-names>T</given-names></name><name><surname>Berg</surname> <given-names>S</given-names></name><name><surname>Bock</surname> <given-names>DD</given-names></name><name><surname>Cardona</surname> <given-names>A</given-names></name><name><surname>Knott</surname> <given-names>GW</given-names></name><name><surname>Plaza</surname> <given-names>SM</given-names></name><name><surname>Scheffer</surname> <given-names>LK</given-names></name><name><surname>Koethe</surname> <given-names>U</given-names></name><name><surname>Kreshuk</surname> <given-names>A</given-names></name><name><surname>Hamprecht</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multicut brings automated neurite segmentation closer to human performance</article-title><source>Nature Methods</source><volume>14</volume><fpage>101</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1038/nmeth.4151</pub-id><pub-id pub-id-type="pmid">28139671</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname> <given-names>S</given-names></name><name><surname>Kutra</surname> <given-names>D</given-names></name><name><surname>Kroeger</surname> <given-names>T</given-names></name><name><surname>Straehle</surname> <given-names>CN</given-names></name><name><surname>Kausler</surname> <given-names>BX</given-names></name><name><surname>Haubold</surname> <given-names>C</given-names></name><name><surname>Schiegg</surname> <given-names>M</given-names></name><name><surname>Ales</surname> <given-names>J</given-names></name><name><surname>Beier</surname> <given-names>T</given-names></name><name><surname>Rudy</surname> <given-names>M</given-names></name><name><surname>Eren</surname> <given-names>K</given-names></name><name><surname>Cervantes</surname> <given-names>JI</given-names></name><name><surname>Xu</surname> <given-names>B</given-names></name><name><surname>Beuttenmueller</surname> <given-names>F</given-names></name><name><surname>Wolny</surname> <given-names>A</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name><name><surname>Koethe</surname> <given-names>U</given-names></name><name><surname>Hamprecht</surname> <given-names>FA</given-names></name><name><surname>Kreshuk</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ilastik: interactive machine learning for (bio)image analysis</article-title><source>Nature Methods</source><volume>16</volume><fpage>1226</fpage><lpage>1232</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0582-9</pub-id><pub-id pub-id-type="pmid">31570887</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Briggman</surname> <given-names>K</given-names></name><name><surname>Denk</surname> <given-names>W</given-names></name><name><surname>Seung</surname> <given-names>S</given-names></name><name><surname>Helmstaedter</surname> <given-names>MN</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2009">2009</year><chapter-title>Maximin affinity learning of image segmentation</chapter-title><person-group person-group-type="editor"><name><surname>Jordan</surname> <given-names>MI</given-names></name><name><surname>LeCun</surname> <given-names>Y</given-names></name><name><surname>Solla</surname> <given-names>SA</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>The MIT Press</publisher-name><fpage>1865</fpage><lpage>1873</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Canny</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>A computational approach to edge detection</article-title><conf-name>IEEE Transactions on Pattern Analysis and Machine Intelligence</conf-name><fpage>679</fpage><lpage>698</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.1986.4767851</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cerrone</surname> <given-names>L</given-names></name><name><surname>Zeilmann</surname> <given-names>A</given-names></name><name><surname>Hamprecht</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>End-to-end learned random walker for seeded image segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1905.09045">https://arxiv.org/abs/1905.09045</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Çiçek</surname> <given-names>Ö</given-names></name><name><surname>Abdulkadir</surname> <given-names>A</given-names></name><name><surname>Lienkamp</surname> <given-names>SS</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name><name><surname>Ronneberger</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>3d u-net: learning dense volumetric segmentation from sparse annotation</article-title><conf-name>International Conference on Medical Image Computing and Computer-Assisted Intervention</conf-name><fpage>424</fpage><lpage>432</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Couprie</surname> <given-names>C</given-names></name><name><surname>Grady</surname> <given-names>L</given-names></name><name><surname>Najman</surname> <given-names>L</given-names></name><name><surname>Talbot</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Power watershed: a unifying Graph-Based optimization framework</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>33</volume><fpage>1384</fpage><lpage>1399</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2010.200</pub-id><pub-id pub-id-type="pmid">21079274</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="software"><person-group person-group-type="author"><collab>CREMI</collab></person-group><year iso-8601-date="2017">2017</year><data-title>Cremi. miccai challenge on circuit reconstruction from electron microscopy images</data-title><publisher-name>Cremi</publisher-name><ext-link ext-link-type="uri" xlink:href="https://cremi.org">https://cremi.org</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Eschweiler</surname> <given-names>D</given-names></name><name><surname>Spina</surname> <given-names>T</given-names></name><name><surname>Choudhury</surname> <given-names>RC</given-names></name><name><surname>Meyerowitz</surname> <given-names>E</given-names></name><name><surname>Cunha</surname> <given-names>A</given-names></name><name><surname>Stegmaier</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cnn-based preprocessing to optimize watershed-based cell segmentation in 3d confocal microscopy images</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.06933">https://arxiv.org/abs/1810.06933</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falk</surname> <given-names>T</given-names></name><name><surname>Mai</surname> <given-names>D</given-names></name><name><surname>Bensch</surname> <given-names>R</given-names></name><name><surname>Çiçek</surname> <given-names>Ö</given-names></name><name><surname>Abdulkadir</surname> <given-names>A</given-names></name><name><surname>Marrakchi</surname> <given-names>Y</given-names></name><name><surname>Böhm</surname> <given-names>A</given-names></name><name><surname>Deubner</surname> <given-names>J</given-names></name><name><surname>Jäckel</surname> <given-names>Z</given-names></name><name><surname>Seiwald</surname> <given-names>K</given-names></name><name><surname>Dovzhenko</surname> <given-names>A</given-names></name><name><surname>Tietz</surname> <given-names>O</given-names></name><name><surname>Dal Bosco</surname> <given-names>C</given-names></name><name><surname>Walsh</surname> <given-names>S</given-names></name><name><surname>Saltukoglu</surname> <given-names>D</given-names></name><name><surname>Tay</surname> <given-names>TL</given-names></name><name><surname>Prinz</surname> <given-names>M</given-names></name><name><surname>Palme</surname> <given-names>K</given-names></name><name><surname>Simons</surname> <given-names>M</given-names></name><name><surname>Diester</surname> <given-names>I</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name><name><surname>Ronneberger</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>U-Net: deep learning for cell counting, detection, and morphometry</article-title><source>Nature Methods</source><volume>16</volume><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id><pub-id pub-id-type="pmid">30559429</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez</surname> <given-names>R</given-names></name><name><surname>Das</surname> <given-names>P</given-names></name><name><surname>Mirabet</surname> <given-names>V</given-names></name><name><surname>Moscardi</surname> <given-names>E</given-names></name><name><surname>Traas</surname> <given-names>J</given-names></name><name><surname>Verdeil</surname> <given-names>JL</given-names></name><name><surname>Malandain</surname> <given-names>G</given-names></name><name><surname>Godin</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Imaging plant growth in 4D: robust tissue reconstruction and lineaging at cell resolution</article-title><source>Nature Methods</source><volume>7</volume><fpage>547</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1472</pub-id><pub-id pub-id-type="pmid">20543845</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname> <given-names>S</given-names></name><name><surname>Southam</surname> <given-names>P</given-names></name><name><surname>Pantin</surname> <given-names>F</given-names></name><name><surname>Kennaway</surname> <given-names>R</given-names></name><name><surname>Robinson</surname> <given-names>S</given-names></name><name><surname>Castorina</surname> <given-names>G</given-names></name><name><surname>Sánchez-Corrales</surname> <given-names>YE</given-names></name><name><surname>Sablowski</surname> <given-names>R</given-names></name><name><surname>Chan</surname> <given-names>J</given-names></name><name><surname>Grieneisen</surname> <given-names>V</given-names></name><name><surname>Marée</surname> <given-names>AFM</given-names></name><name><surname>Bangham</surname> <given-names>JA</given-names></name><name><surname>Coen</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spatiotemporal coordination of cell division and growth during organ morphogenesis</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2005952</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2005952</pub-id><pub-id pub-id-type="pmid">30383040</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Funke</surname> <given-names>J</given-names></name><name><surname>Tschopp</surname> <given-names>F</given-names></name><name><surname>Grisaitis</surname> <given-names>W</given-names></name><name><surname>Sheridan</surname> <given-names>A</given-names></name><name><surname>Singh</surname> <given-names>C</given-names></name><name><surname>Saalfeld</surname> <given-names>S</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A deep structured learning approach towards automating connectome reconstruction from 3d electron micrographs</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1709.02974">https://arxiv.org/abs/1709.02974</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funke</surname> <given-names>J</given-names></name><name><surname>Tschopp</surname> <given-names>F</given-names></name><name><surname>Grisaitis</surname> <given-names>W</given-names></name><name><surname>Sheridan</surname> <given-names>A</given-names></name><name><surname>Singh</surname> <given-names>C</given-names></name><name><surname>Saalfeld</surname> <given-names>S</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Large scale image segmentation with structured loss based deep learning for connectome reconstruction</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>41</volume><fpage>1669</fpage><lpage>1680</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2018.2835450</pub-id><pub-id pub-id-type="pmid">29993708</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Funke</surname> <given-names>J</given-names></name><name><surname>Mais</surname> <given-names>L</given-names></name><name><surname>Champion</surname> <given-names>A</given-names></name><name><surname>Dye</surname> <given-names>N</given-names></name><name><surname>Kainmueller</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019b</year><source>A Benchmark for Epithelial Cell Tracking</source><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haberl</surname> <given-names>MG</given-names></name><name><surname>Churas</surname> <given-names>C</given-names></name><name><surname>Tindall</surname> <given-names>L</given-names></name><name><surname>Boassa</surname> <given-names>D</given-names></name><name><surname>Phan</surname> <given-names>S</given-names></name><name><surname>Bushong</surname> <given-names>EA</given-names></name><name><surname>Madany</surname> <given-names>M</given-names></name><name><surname>Akay</surname> <given-names>R</given-names></name><name><surname>Deerinck</surname> <given-names>TJ</given-names></name><name><surname>Peltier</surname> <given-names>ST</given-names></name><name><surname>Ellisman</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>CDeep3M-Plug-and-Play cloud-based deep learning for image segmentation</article-title><source>Nature Methods</source><volume>15</volume><fpage>677</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0106-z</pub-id><pub-id pub-id-type="pmid">30171236</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Hanslovsky</surname> <given-names>P</given-names></name><name><surname>Leite</surname> <given-names>V</given-names></name><name><surname>Saalfeld</surname> <given-names>S</given-names></name><name><surname>Pisarev</surname> <given-names>I</given-names></name><name><surname>Funke</surname> <given-names>J</given-names></name><name><surname>Pietzsch</surname> <given-names>T</given-names></name><name><surname>Günther</surname> <given-names>U</given-names></name><name><surname>Bogovic</surname> <given-names>J</given-names></name><name><surname>Schmidt</surname> <given-names>U</given-names></name><name><surname>Nunez-Iglesias</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>saalfeldlab/paintera paintera-0.20.1</data-title><source>saalfeldlab/paintera</source><version designator="0.20.1">0.20.1</version></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Heisler</surname> <given-names>MG</given-names></name><name><surname>Ohno</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Live-Imaging of the Arabidopsis Inflorescence Meristem</chapter-title><person-group person-group-type="editor"><name><surname>Riechmann</surname> <given-names>J. L</given-names></name><name><surname>Wellmer</surname> <given-names>F</given-names></name></person-group><source>Flower Development: Methods and Protocols, Methods in Molecular Biology</source><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name><fpage>431</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1007/978-1-4614-9408-9</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Horé</surname> <given-names>A</given-names></name><name><surname>Ziou</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Image Quality Metrics: Psnr vs. Ssim</article-title><conf-name>2010 20th International Conference on Pattern Recognition</conf-name><fpage>2366</fpage><lpage>2369</lpage><pub-id pub-id-type="doi">10.1109/ICPR.2010.579</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Horňáková</surname> <given-names>A</given-names></name><name><surname>Lange</surname> <given-names>J-H</given-names></name><name><surname>Andres</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Analysis and Optimization of Graph Decompositions by Lifted Multicuts</article-title><conf-name>Proceedings of the 34th International Conference on Machine Learning</conf-name><fpage>1539</fpage><lpage>1548</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ioffe</surname> <given-names>S</given-names></name><name><surname>Szegedy</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Januszewski</surname> <given-names>M</given-names></name><name><surname>Maitin-Shepard</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>P</given-names></name><name><surname>Kornfeld</surname> <given-names>J</given-names></name><name><surname>Denk</surname> <given-names>W</given-names></name><name><surname>Jain</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Flood-filling networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1611.00421">http://arxiv.org/abs/1611.00421</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jug</surname> <given-names>F</given-names></name><name><surname>Levinkov</surname> <given-names>E</given-names></name><name><surname>Blasse</surname> <given-names>C</given-names></name><name><surname>Myers</surname> <given-names>EW</given-names></name><name><surname>Andres</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Moral lineage tracing</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1511.05512">http://arxiv.org/abs/1511.05512</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kappes</surname> <given-names>JH</given-names></name><name><surname>Speth</surname> <given-names>M</given-names></name><name><surname>Andres</surname> <given-names>B</given-names></name><name><surname>Reinelt</surname> <given-names>G</given-names></name><name><surname>Schn</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Globally optimal image partitioning by multicuts</article-title><conf-name>International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition</conf-name><fpage>31</fpage><lpage>44</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kierzkowski</surname> <given-names>D</given-names></name><name><surname>Runions</surname> <given-names>A</given-names></name><name><surname>Vuolo</surname> <given-names>F</given-names></name><name><surname>Strauss</surname> <given-names>S</given-names></name><name><surname>Lymbouridou</surname> <given-names>R</given-names></name><name><surname>Routier-Kierzkowska</surname> <given-names>AL</given-names></name><name><surname>Wilson-Sánchez</surname> <given-names>D</given-names></name><name><surname>Jenke</surname> <given-names>H</given-names></name><name><surname>Galinha</surname> <given-names>C</given-names></name><name><surname>Mosca</surname> <given-names>G</given-names></name><name><surname>Zhang</surname> <given-names>Z</given-names></name><name><surname>Canales</surname> <given-names>C</given-names></name><name><surname>Dello Ioio</surname> <given-names>R</given-names></name><name><surname>Huijser</surname> <given-names>P</given-names></name><name><surname>Smith</surname> <given-names>RS</given-names></name><name><surname>Tsiantis</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A Growth-Based framework for leaf shape development and diversity</article-title><source>Cell</source><volume>177</volume><fpage>1405</fpage><lpage>1418</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2019.05.011</pub-id><pub-id pub-id-type="pmid">31130379</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kokkinos</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Pushing the boundaries of boundary detection using deep learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.07386">https://arxiv.org/abs/1511.07386</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laskowski</surname> <given-names>MJ</given-names></name><name><surname>Williams</surname> <given-names>ME</given-names></name><name><surname>Nusbaum</surname> <given-names>HC</given-names></name><name><surname>Sussex</surname> <given-names>IM</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Formation of lateral root meristems is a two-stage process</article-title><source>Development</source><volume>121</volume><fpage>3303</fpage><lpage>3310</lpage><pub-id pub-id-type="pmid">7588064</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>K</given-names></name><name><surname>Zung</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>P</given-names></name><name><surname>Jain</surname> <given-names>V</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Superhuman accuracy on the snemi3d connectomics challenge</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.00120">https://arxiv.org/abs/1706.00120</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Long</surname> <given-names>J</given-names></name><name><surname>Shelhamer</surname> <given-names>E</given-names></name><name><surname>Darrell</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fully Convolutional Networks for Semantic Segmentation</article-title><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowekamp</surname> <given-names>BC</given-names></name><name><surname>Chen</surname> <given-names>DT</given-names></name><name><surname>Ibáñez</surname> <given-names>L</given-names></name><name><surname>Blezek</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The design of SimpleITK</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>45</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00045</pub-id><pub-id pub-id-type="pmid">24416015</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucchi</surname> <given-names>A</given-names></name><name><surname>Smith</surname> <given-names>K</given-names></name><name><surname>Achanta</surname> <given-names>R</given-names></name><name><surname>Knott</surname> <given-names>G</given-names></name><name><surname>Fua</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Supervoxel-based segmentation of mitochondria in em image stacks with learned shape features</article-title><source>IEEE Transactions on Medical Imaging</source><volume>31</volume><fpage>474</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1109/TMI.2011.2171705</pub-id><pub-id pub-id-type="pmid">21997252</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maizel</surname> <given-names>A</given-names></name><name><surname>von Wangenheim</surname> <given-names>D</given-names></name><name><surname>Federici</surname> <given-names>F</given-names></name><name><surname>Haseloff</surname> <given-names>J</given-names></name><name><surname>Stelzer</surname> <given-names>EH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>High-resolution live imaging of plant growth in near physiological bright conditions using light sheet fluorescence microscopy</article-title><source>The Plant Journal</source><volume>68</volume><fpage>377</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1111/j.1365-313X.2011.04692.x</pub-id><pub-id pub-id-type="pmid">21711399</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malamy</surname> <given-names>JE</given-names></name><name><surname>Benfey</surname> <given-names>PN</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Organization and cell differentiation in lateral roots of <italic>Arabidopsis thaliana</italic></article-title><source>Development</source><volume>124</volume><fpage>33</fpage><lpage>44</lpage><pub-id pub-id-type="pmid">9006065</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maška</surname> <given-names>M</given-names></name><name><surname>Ulman</surname> <given-names>V</given-names></name><name><surname>Svoboda</surname> <given-names>D</given-names></name><name><surname>Matula</surname> <given-names>P</given-names></name><name><surname>Matula</surname> <given-names>P</given-names></name><name><surname>Ederra</surname> <given-names>C</given-names></name><name><surname>Urbiola</surname> <given-names>A</given-names></name><name><surname>España</surname> <given-names>T</given-names></name><name><surname>Venkatesan</surname> <given-names>S</given-names></name><name><surname>Balak</surname> <given-names>DM</given-names></name><name><surname>Karas</surname> <given-names>P</given-names></name><name><surname>Bolcková</surname> <given-names>T</given-names></name><name><surname>Streitová</surname> <given-names>M</given-names></name><name><surname>Carthel</surname> <given-names>C</given-names></name><name><surname>Coraluppi</surname> <given-names>S</given-names></name><name><surname>Harder</surname> <given-names>N</given-names></name><name><surname>Rohr</surname> <given-names>K</given-names></name><name><surname>Magnusson</surname> <given-names>KE</given-names></name><name><surname>Jaldén</surname> <given-names>J</given-names></name><name><surname>Blau</surname> <given-names>HM</given-names></name><name><surname>Dzyubachyk</surname> <given-names>O</given-names></name><name><surname>Křížek</surname> <given-names>P</given-names></name><name><surname>Hagen</surname> <given-names>GM</given-names></name><name><surname>Pastor-Escuredo</surname> <given-names>D</given-names></name><name><surname>Jimenez-Carretero</surname> <given-names>D</given-names></name><name><surname>Ledesma-Carbayo</surname> <given-names>MJ</given-names></name><name><surname>Muñoz-Barrutia</surname> <given-names>A</given-names></name><name><surname>Meijering</surname> <given-names>E</given-names></name><name><surname>Kozubek</surname> <given-names>M</given-names></name><name><surname>Ortiz-de-Solorzano</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A benchmark for comparison of cell tracking algorithms</article-title><source>Bioinformatics</source><volume>30</volume><fpage>1609</fpage><lpage>1617</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu080</pub-id><pub-id pub-id-type="pmid">24526711</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Meila</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Comparing Clusterings: An Axiomatic View</article-title><conf-name>Proceedings of the 22nd International Conference on Machine Learning</conf-name><fpage>577</fpage><lpage>584</lpage></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meilă</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Comparing clusterings—an information based distance</article-title><source>Journal of Multivariate Analysis</source><volume>98</volume><fpage>873</fpage><lpage>895</lpage><pub-id pub-id-type="doi">10.1016/j.jmva.2006.11.013</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moen</surname> <given-names>E</given-names></name><name><surname>Bannon</surname> <given-names>D</given-names></name><name><surname>Kudo</surname> <given-names>T</given-names></name><name><surname>Graf</surname> <given-names>W</given-names></name><name><surname>Covert</surname> <given-names>M</given-names></name><name><surname>Van Valen</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning for cellular image analysis</article-title><source>Nature Methods</source><volume>16</volume><fpage>1233</fpage><lpage>1246</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id><pub-id pub-id-type="pmid">31133758</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nunez-Iglesias</surname> <given-names>J</given-names></name><name><surname>Kennedy</surname> <given-names>R</given-names></name><name><surname>Plaza</surname> <given-names>SM</given-names></name><name><surname>Chakraborty</surname> <given-names>A</given-names></name><name><surname>Katz</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Graph-based active learning of agglomeration (GALA): a Python library to segment 2D and 3D neuroimages</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>34</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00034</pub-id><pub-id pub-id-type="pmid">24772079</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pape</surname> <given-names>C</given-names></name><name><surname>Matskevych</surname> <given-names>A</given-names></name><name><surname>Wolny</surname> <given-names>A</given-names></name><name><surname>Hennies</surname> <given-names>J</given-names></name><name><surname>Mizzon</surname> <given-names>G</given-names></name><name><surname>Louveaux</surname> <given-names>M</given-names></name><name><surname>Musser</surname> <given-names>J</given-names></name><name><surname>Maizel</surname> <given-names>A</given-names></name><name><surname>Arendt</surname> <given-names>D</given-names></name><name><surname>Kreshuk</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Leveraging domain knowledge to improve microscopy image segmentation with lifted multicuts</article-title><source>Frontiers in Computer Science</source><volume>1</volume><elocation-id>6</elocation-id><pub-id pub-id-type="doi">10.3389/fcomp.2019.00006</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname> <given-names>A</given-names></name><name><surname>Gross</surname> <given-names>S</given-names></name><name><surname>Massa</surname> <given-names>F</given-names></name><name><surname>Lerer</surname> <given-names>A</given-names></name><name><surname>Bradbury</surname> <given-names>J</given-names></name><name><surname>Chanan</surname> <given-names>G</given-names></name><name><surname>Killeen</surname> <given-names>T</given-names></name><name><surname>Lin</surname> <given-names>Z</given-names></name><name><surname>Gimelshein</surname> <given-names>N</given-names></name><name><surname>Antiga</surname> <given-names>L</given-names></name><name><surname>Desmaison</surname> <given-names>A</given-names></name><name><surname>Kopf</surname> <given-names>A</given-names></name><name><surname>Yang</surname> <given-names>E</given-names></name><name><surname>DeVito</surname> <given-names>Z</given-names></name><name><surname>Raison</surname> <given-names>M</given-names></name><name><surname>Tejani</surname> <given-names>A</given-names></name><name><surname>Chilamkurthy</surname> <given-names>S</given-names></name><name><surname>Steiner</surname> <given-names>B</given-names></name><name><surname>Fang</surname> <given-names>L</given-names></name><name><surname>Bai</surname> <given-names>J</given-names></name><name><surname>Chintala</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Pytorch: An imperative style, high-performance deep learning library</chapter-title><person-group person-group-type="editor"><name><surname>Wallach</surname> <given-names>H</given-names></name><name><surname>Larochelle</surname> <given-names>H</given-names></name><name><surname>Beygelzimer</surname> <given-names>A</given-names></name><name><surname>dAlché-Buc</surname> <given-names>F</given-names></name><name><surname>Garnett</surname> <given-names>R</given-names></name><name><surname>Fox</surname> <given-names>E</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>8024</fpage><lpage>8035</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rand</surname> <given-names>WM</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Objective criteria for the evaluation of clustering methods</article-title><source>Journal of the American Statistical Association</source><volume>66</volume><fpage>846</fpage><lpage>850</lpage><pub-id pub-id-type="doi">10.1080/01621459.1971.10482356</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rasmussen</surname> <given-names>CG</given-names></name><name><surname>Bellinger</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An overview of plant division-plane orientation</article-title><source>New Phytologist</source><volume>219</volume><fpage>505</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.1111/nph.15183</pub-id><pub-id pub-id-type="pmid">29701870</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rempfler</surname> <given-names>M</given-names></name><name><surname>Lange</surname> <given-names>J</given-names></name><name><surname>Jug</surname> <given-names>F</given-names></name><name><surname>Blasse</surname> <given-names>C</given-names></name><name><surname>Myers</surname> <given-names>EW</given-names></name><name><surname>Menze</surname> <given-names>BH</given-names></name><name><surname>Andres</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Efficient algorithms for moral lineage tracing</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1702.04111">https://arxiv.org/abs/1702.04111</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson-Beers</surname> <given-names>K</given-names></name><name><surname>Pruitt</surname> <given-names>RE</given-names></name><name><surname>Gasser</surname> <given-names>CS</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Ovule development in Wild-Type Arabidopsis and two Female-Sterile mutants</article-title><source>The Plant Cell</source><volume>4</volume><fpage>1237</fpage><lpage>1249</lpage><pub-id pub-id-type="doi">10.2307/3869410</pub-id><pub-id pub-id-type="pmid">12297633</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roerdink</surname> <given-names>JBTM</given-names></name><name><surname>Meijster</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The watershed transform: definitions, algorithms and parallelization strategies</article-title><source>Fundamenta Informaticae</source><volume>41</volume><fpage>187</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.3233/FI-2000-411207</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ronneberger</surname> <given-names>O</given-names></name><name><surname>Fischer</surname> <given-names>P</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>U-net: convolutional networks for biomedical image segmentation</article-title><conf-name>International Conference on Medical Image Computing and Computer-Assisted Intervention</conf-name><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindelin</surname> <given-names>J</given-names></name><name><surname>Arganda-Carreras</surname> <given-names>I</given-names></name><name><surname>Frise</surname> <given-names>E</given-names></name><name><surname>Kaynig</surname> <given-names>V</given-names></name><name><surname>Longair</surname> <given-names>M</given-names></name><name><surname>Pietzsch</surname> <given-names>T</given-names></name><name><surname>Preibisch</surname> <given-names>S</given-names></name><name><surname>Rueden</surname> <given-names>C</given-names></name><name><surname>Saalfeld</surname> <given-names>S</given-names></name><name><surname>Schmid</surname> <given-names>B</given-names></name><name><surname>Tinevez</surname> <given-names>JY</given-names></name><name><surname>White</surname> <given-names>DJ</given-names></name><name><surname>Hartenstein</surname> <given-names>V</given-names></name><name><surname>Eliceiri</surname> <given-names>K</given-names></name><name><surname>Tomancak</surname> <given-names>P</given-names></name><name><surname>Cardona</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fiji: an open-source platform for biological-image analysis</article-title><source>Nature Methods</source><volume>9</volume><fpage>676</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id><pub-id pub-id-type="pmid">22743772</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneitz</surname> <given-names>K</given-names></name><name><surname>Hulskamp</surname> <given-names>M</given-names></name><name><surname>Pruitt</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Wild-type ovule development in <italic>Arabidopsis thaliana</italic>: a light microscope study of cleared whole-mount tissue</article-title><source>The Plant Journal</source><volume>7</volume><fpage>731</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1046/j.1365-313X.1995.07050731.x</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stegmaier</surname> <given-names>J</given-names></name><name><surname>Amat</surname> <given-names>F</given-names></name><name><surname>Lemon</surname> <given-names>WC</given-names></name><name><surname>McDole</surname> <given-names>K</given-names></name><name><surname>Wan</surname> <given-names>Y</given-names></name><name><surname>Teodoro</surname> <given-names>G</given-names></name><name><surname>Mikut</surname> <given-names>R</given-names></name><name><surname>Keller</surname> <given-names>PJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Real-time three-dimensional cell segmentation in Large-Scale microscopy data of developing embryos</article-title><source>Developmental Cell</source><volume>36</volume><fpage>225</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.devcel.2015.12.028</pub-id><pub-id pub-id-type="pmid">26812020</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strauss</surname> <given-names>S</given-names></name><name><surname>Sapala</surname> <given-names>A</given-names></name><name><surname>Kierzkowski</surname> <given-names>D</given-names></name><name><surname>Smith</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="2019">2019</year><chapter-title>Quantifying plant growth and cell proliferation with morphographx</chapter-title><person-group person-group-type="editor"><name><surname>Cvrčková</surname> <given-names>F</given-names></name></person-group><source>Plant Cell Morphogenesis</source><publisher-name>Springer</publisher-name><fpage>269</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1007/978-1-62703-643-6</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sudre</surname> <given-names>CH</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name><name><surname>Vercauteren</surname> <given-names>T</given-names></name><name><surname>Ourselin</surname> <given-names>S</given-names></name><name><surname>Cardoso</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1707.03237">https://arxiv.org/abs/1707.03237</ext-link></element-citation></ref><ref id="bib57"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Tischer</surname> <given-names>C</given-names></name><name><surname>Norlin</surname> <given-names>N</given-names></name><name><surname>Pepperkok</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><source>BigDataProcessor: Fiji plugin for big image data inspection and processing</source></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tofanelli</surname> <given-names>R</given-names></name><name><surname>Vijayan</surname> <given-names>A</given-names></name><name><surname>Scholz</surname> <given-names>S</given-names></name><name><surname>Schneitz</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Protocol for rapid clearing and staining of fixed Arabidopsis ovules for improved imaging by confocal laser scanning microscopy</article-title><source>Plant Methods</source><volume>15</volume><elocation-id>120</elocation-id><pub-id pub-id-type="doi">10.1186/s13007-019-0505-x</pub-id><pub-id pub-id-type="pmid">31673277</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tu</surname> <given-names>Z</given-names></name><name><surname>Bai</surname> <given-names>X</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Auto-context and its application to high-level vision tasks and 3d brain image segmentation</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>32</volume><fpage>1744</fpage><lpage>1757</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2009.186</pub-id><pub-id pub-id-type="pmid">20724753</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turaga</surname> <given-names>SC</given-names></name><name><surname>Murray</surname> <given-names>JF</given-names></name><name><surname>Jain</surname> <given-names>V</given-names></name><name><surname>Roth</surname> <given-names>F</given-names></name><name><surname>Helmstaedter</surname> <given-names>M</given-names></name><name><surname>Briggman</surname> <given-names>K</given-names></name><name><surname>Denk</surname> <given-names>W</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Convolutional networks can learn to generate affinity graphs for image segmentation</article-title><source>Neural Computation</source><volume>22</volume><fpage>511</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1162/neco.2009.10-08-881</pub-id><pub-id pub-id-type="pmid">19922289</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ulyanov</surname> <given-names>D</given-names></name><name><surname>Vedaldi</surname> <given-names>A</given-names></name><name><surname>Lempitsky</surname> <given-names>VS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Instance normalization: the missing ingredient for fast stylization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1607.08022">https://arxiv.org/abs/1607.08022</ext-link></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname> <given-names>S</given-names></name><name><surname>Schönberger</surname> <given-names>JL</given-names></name><name><surname>Nunez-Iglesias</surname> <given-names>J</given-names></name><name><surname>Boulogne</surname> <given-names>F</given-names></name><name><surname>Warner</surname> <given-names>JD</given-names></name><name><surname>Yager</surname> <given-names>N</given-names></name><name><surname>Gouillart</surname> <given-names>E</given-names></name><name><surname>Yu</surname> <given-names>T</given-names></name><collab>scikit-image contributors</collab></person-group><year iso-8601-date="2014">2014</year><article-title>scikit-image: image processing in Python</article-title><source>PeerJ</source><volume>2</volume><elocation-id>e453</elocation-id><pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Noord</surname> <given-names>N</given-names></name><name><surname>Postma</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning scale-variant and scale-invariant features for deep image classification</article-title><source>Pattern Recognition</source><volume>61</volume><fpage>583</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2016.06.005</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Valen</surname> <given-names>DA</given-names></name><name><surname>Kudo</surname> <given-names>T</given-names></name><name><surname>Lane</surname> <given-names>KM</given-names></name><name><surname>Macklin</surname> <given-names>DN</given-names></name><name><surname>Quach</surname> <given-names>NT</given-names></name><name><surname>DeFelice</surname> <given-names>MM</given-names></name><name><surname>Maayan</surname> <given-names>I</given-names></name><name><surname>Tanouchi</surname> <given-names>Y</given-names></name><name><surname>Ashley</surname> <given-names>EA</given-names></name><name><surname>Covert</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep learning automates the quantitative analysis of individual cells in Live-Cell imaging experiments</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005177</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005177</pub-id><pub-id pub-id-type="pmid">27814364</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vilches Barro</surname> <given-names>A</given-names></name><name><surname>Stöckle</surname> <given-names>D</given-names></name><name><surname>Thellmann</surname> <given-names>M</given-names></name><name><surname>Ruiz-Duarte</surname> <given-names>P</given-names></name><name><surname>Bald</surname> <given-names>L</given-names></name><name><surname>Louveaux</surname> <given-names>M</given-names></name><name><surname>von Born</surname> <given-names>P</given-names></name><name><surname>Denninger</surname> <given-names>P</given-names></name><name><surname>Goh</surname> <given-names>T</given-names></name><name><surname>Fukaki</surname> <given-names>H</given-names></name><name><surname>Vermeer</surname> <given-names>JEM</given-names></name><name><surname>Maizel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cytoskeleton dynamics are necessary for early events of lateral root initiation in Arabidopsis</article-title><source>Current Biology</source><volume>29</volume><fpage>2443</fpage><lpage>2454</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.06.039</pub-id><pub-id pub-id-type="pmid">31327713</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Wangenheim</surname> <given-names>D</given-names></name><name><surname>Daum</surname> <given-names>G</given-names></name><name><surname>Lohmann</surname> <given-names>JU</given-names></name><name><surname>Stelzer</surname> <given-names>EK</given-names></name><name><surname>Maizel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Live imaging of Arabidopsis development</article-title><source>Methods in Molecular Biology</source><volume>1062</volume><fpage>539</fpage><lpage>550</lpage><pub-id pub-id-type="doi">10.1007/978-1-62703-580-4_28</pub-id><pub-id pub-id-type="pmid">24057385</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Wangenheim</surname> <given-names>D</given-names></name><name><surname>Fangerau</surname> <given-names>J</given-names></name><name><surname>Schmitz</surname> <given-names>A</given-names></name><name><surname>Smith</surname> <given-names>RS</given-names></name><name><surname>Leitte</surname> <given-names>H</given-names></name><name><surname>Stelzer</surname> <given-names>EH</given-names></name><name><surname>Maizel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Rules and Self-Organizing properties of Post-embryonic plant organ cell division patterns</article-title><source>Current Biology</source><volume>26</volume><fpage>439</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.12.047</pub-id><pub-id pub-id-type="pmid">26832441</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>W</given-names></name><name><surname>Taft</surname> <given-names>DA</given-names></name><name><surname>Chen</surname> <given-names>YJ</given-names></name><name><surname>Zhang</surname> <given-names>J</given-names></name><name><surname>Wallace</surname> <given-names>CT</given-names></name><name><surname>Xu</surname> <given-names>M</given-names></name><name><surname>Watkins</surname> <given-names>SC</given-names></name><name><surname>Xing</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learn to segment single cells with deep distance estimator and deep cell detector</article-title><source>Computers in Biology and Medicine</source><volume>108</volume><fpage>133</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2019.04.006</pub-id><pub-id pub-id-type="pmid">31005005</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weissman</surname> <given-names>TA</given-names></name><name><surname>Pan</surname> <given-names>YA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Brainbow: new resources and emerging biological applications for multicolor genetic labeling and analysis</article-title><source>Genetics</source><volume>199</volume><fpage>293</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1534/genetics.114.172510</pub-id><pub-id pub-id-type="pmid">25657347</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willis</surname> <given-names>L</given-names></name><name><surname>Refahi</surname> <given-names>Y</given-names></name><name><surname>Wightman</surname> <given-names>R</given-names></name><name><surname>Landrein</surname> <given-names>B</given-names></name><name><surname>Teles</surname> <given-names>J</given-names></name><name><surname>Huang</surname> <given-names>KC</given-names></name><name><surname>Meyerowitz</surname> <given-names>EM</given-names></name><name><surname>Jönsson</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cell size and growth regulation in the <italic>Arabidopsis thaliana</italic> apical stem cell niche</article-title><source>PNAS</source><volume>113</volume><fpage>E8238</fpage><lpage>E8246</lpage><pub-id pub-id-type="doi">10.1073/pnas.1616768113</pub-id><pub-id pub-id-type="pmid">27930326</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wolf</surname> <given-names>S</given-names></name><name><surname>Pape</surname> <given-names>C</given-names></name><name><surname>Bailoni</surname> <given-names>A</given-names></name><name><surname>Rahaman</surname> <given-names>N</given-names></name><name><surname>Kreshuk</surname> <given-names>A</given-names></name><name><surname>Kothe</surname> <given-names>U</given-names></name><name><surname>Hamprecht</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Mutex Watershed: Efficient, Parameter-Free Image Partitioning</article-title><conf-name>Proceedings of the European Conference on Computer Vision (ECCV)</conf-name><fpage>546</fpage><lpage>562</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wolny</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020a</year><data-title>PlantSeg</data-title><source>GitHub</source><version designator="caa8e94">caa8e94</version><ext-link ext-link-type="uri" xlink:href="https://github.com/hci-unihd/plant-seg">https://github.com/hci-unihd/plant-seg</ext-link></element-citation></ref><ref id="bib73"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Wolny</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020b</year><data-title>pytorch-3dunet</data-title><source>GitHub</source><version designator="29bb0ad">29bb0ad</version><ext-link ext-link-type="uri" xlink:href="https://github.com/wolny/pytorch-3dunet">https://github.com/wolny/pytorch-3dunet</ext-link></element-citation></ref><ref id="bib74"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wu</surname> <given-names>Y</given-names></name><name><surname>He</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Group normalization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.08494">https://arxiv.org/abs/1803.08494</ext-link></element-citation></ref><ref id="bib75"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xie</surname> <given-names>S</given-names></name><name><surname>Tu</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Holistically-nested edge detection</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1504.06375">http://arxiv.org/abs/1504.06375</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Groundtruth creation</title><p>Training state-of-the-art deep neural network for semantic segmentation requires a large amount of densely annotated samples. Groundtruth creation for the ovule dataset has been described previously (<xref ref-type="bibr" rid="bib58">Tofanelli et al., 2019</xref>), we briefly describe here how the dense groundtruth labeling of cells for the lateral root was generated.</p><p>We bootstrapped the process using the Autocontext Workflow (<xref ref-type="bibr" rid="bib59">Tu and Bai, 2010</xref>) of the open-source ilastik software (<xref ref-type="bibr" rid="bib6">Berg et al., 2019</xref>) which is used to segment cell boundaries from sparse user input (scribbles). It is followed by the ilastik’s multicut workflow (<xref ref-type="bibr" rid="bib5">Beier et al., 2017</xref>) which takes the boundary segmentation image and produces the cell instance segmentation. These initial segmentation results were iteratively refined (see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>). First, the segmentation is manually proofread in a few selected regions of interest using the open-source Paintera software (<xref ref-type="bibr" rid="bib21">Hanslovsky et al., 2019</xref>). Second, a state-of-the-art neural network is trained for boundary detection on the manually corrected regions. Third, PlantSeg framework consisting of neural network prediction and image partitioning algorithm is applied to the entire dataset resulting in a more refined segmentation. The 3-step iterative process was repeated until an instance segmentation of satisfactory quality was reached. A final round of manual proofreading with Paintera is performed to finalize the groundtruth.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Groundtruth creation process.</title><p>Starting from the input image (1), an initial segmentation is obtained using ilastik Autocontext followed by the ilastik multicut workflow (2). Paintera is used to proofread the segmentation (3a) which is used for training a 3D UNet for boundary detection (3b). A graph partitioning algorithm is used to segment the volume (3 c). Steps 3a, 3b and 3 c are iterated until a final round of proofreading with Paintera (4) and the generation of satisfactory final groundtruth labels (5).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-app1-fig1-v2.tif"/></fig></sec></boxed-text></app><app id="appendix-2"><title>Appendix 2</title><boxed-text><sec id="s9" sec-type="appendix"><title>Exploiting nuclei staining to improve the lateral root cells' segmentation</title><p>The lateral root dataset contains a nuclei marker in a separate channel. In such cases we can take advantage of the fact that a cell contains only one nucleus and use this information as an additional clue during segmentation.</p><p>For this, we first segmented the nuclei using a simple and accurate segmentation pipeline that consists of a 3D U-Net trained to predict the binary nuclei mask followed by thresholding of the probability maps and connected components. We then incorporated this additional information into the multicut formulation, called lifted multicut (<xref ref-type="bibr" rid="bib44">Pape et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Horňáková et al., 2017</xref>), where additional repulsive edges are introduced between the nodes in the graph corresponding to the different nuclei segmented from the second channel.</p><p>We compared the scores of this lifted multicut algorithm to the scores for GASP, multicut and mutex watershed (see <xref ref-type="table" rid="app5table2">Appendix 5—table 2</xref>). We see that lifted multicut outperforms not only the standard multicut, but also all the other algorithms. This is because lifted multicut is able to separate two cells incorrectly merged into one region by the segmentation algorithm, as long as the region contains the two different nuclei instances corresponding to the merged cells.</p><p>A 3D U-Net trained to predict nuclei mask is available in the PlantSeg package. Lifted multicut segmentation can be executed via the PlantSeg’s command line interface. We refer to the project’s GitHub page for instructions.</p></sec></boxed-text></app><app id="appendix-3"><title>Appendix 3</title><boxed-text><sec id="s10" sec-type="appendix"><title>Performance of PlantSeg on an independent reference benchmark</title><p>To test the versatility of our approach, we assessed PlantSeg performance on a non-plant dataset consisting of 2D+t videos of membrane-stained developing <italic>Drosophila</italic> epithelial cells (<xref ref-type="bibr" rid="bib1">Aigouy et al., 2016</xref>). The benchmark dataset and the results of four state-of-the-art segmentation pipelines are reported in <xref ref-type="bibr" rid="bib19">Funke et al., 2019b</xref>. Treating the movie sequence as 3D volumetric images not only resembles the plant cell images shown in our study, but also allows to pose the 2D+t segmentation as a standard 3D segmentation problem.</p><p>We compared the performance of PlantSeg on the 8 movies of this dataset to the four reported pipelines: MALA (<xref ref-type="bibr" rid="bib17">Funke et al., 2017</xref>), Flood Filling Networks (FFN) (<xref ref-type="bibr" rid="bib26">Januszewski et al., 2016</xref>), Moral Lineage Tracing (MLT) (<xref ref-type="bibr" rid="bib27">Jug et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Rempfler et al., 2017</xref>) and Tissue Analyzer (TA) (<xref ref-type="bibr" rid="bib19">Funke et al., 2019b</xref>). On our side, we evaluate two PlantSeg predictions: for the first one, we use the boundary detection network trained on the <italic>Arabidopsis thaliana</italic> ovules. This experiment gives an estimate of how well our pre-trained networks generalize to non-plant tissues. For the second evaluation, we retrain the network on the training data of the benchmark and obtain an estimate of the overall PlantSeg approach accuracy on non-plant data. Note that unlike other methods reported in the benchmark, we do not introduce any changes to account for the data being 2D+t rather than 3D, that is, we do not enforce the lineages to be moral as the authors of <xref ref-type="bibr" rid="bib19">Funke et al., 2019b</xref> did with their segmentation methods.</p><p>For the first experiment, peripodial cells were segmented using the 3D U-Net trained on the ovule dataset together with GASP segmentation, whereas proper disc cells were segmented with 2D U-Net trained on the ovule dataset in combination with Multicut algorithm. Both networks are part of the PlantSeg package. Qualitative results of our pipeline are shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>: PlantSeg produces very good segmentations on both the peripodial and proper imaginal disc cells. A few over-segmentation (peripodial cells) and under-segmentation (proper disc) errors are marked in the figure. This impression is confirmed by quantitative benchmark results in <xref ref-type="table" rid="app3table1">Appendix 3—table 1</xref>.</p><p>For the second experiment, we trained the network on the ground truth labels included in the benchmark (<italic>PlantSeg (trained)</italic>). Here, our pipeline is comparable to state-of-the-art. The difference in SEG metric between ’vanilla’ PlantSeg and <italic>PlantSeg (trained)</italic> is 6.9 percent points on average, which suggests that for datasets sufficiently different from the ones PlantSeg networks were trained on, re-training the models might be necessary. Looking at the average run-times of the methods reported in the benchmark shows that PlantSeg pipeline is clearly the fastest approach with the average run-time of 3 min per movie when run on a server with a modern GPU versus 35 min (MALA), 42 min (MLT) and 90 min (FFN).</p><p>Thus, PlantSeg achieves results which are competitive with other proven methods in terms of accuracy, without explicitly training the boundary detection networks on the epithelial cell ground truth or accounting for the 2d+t nature of the data. PlantSeg outperforms all methods in term of computing time.</p><table-wrap id="app3table1" position="float"><label>Appendix 3—table 1.</label><caption><title>Epithelial Cell Benchmark results.</title><p>We compare PlantSeg to four other methods using the standard SEG metric (<xref ref-type="bibr" rid="bib39">Maška et al., 2014</xref>) calculated as the mean of the Jaccard indices between the reference and the segmented cells in a given movie (higher is better). Mean and standard deviation of the SEG score are reported for peripodial (three movies) and proper disc (five movies) cells. Additionally we report the scores of PlantSeg pipeline executed with a network trained explicitly on the epithelial cell dataset (last row).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Peripodial</th><th>Proper disc</th></tr></thead><tbody><tr><td>MALA</td><td>0.907 0.029</td><td>0.817 0.009</td></tr><tr><td>FFN</td><td>0.879 0.035</td><td>0.796 0.013</td></tr><tr><td>MLT-GLA</td><td>0.904 0.026</td><td>0.818 0.010</td></tr><tr><td>TA</td><td>-</td><td>0.758 0.009</td></tr><tr><td>PlantSeg</td><td>0.787 0.063</td><td>0.761 0.035</td></tr><tr><td>PlantSeg (trained)</td><td>0.885 0.056</td><td>0.800 0.015</td></tr></tbody></table></table-wrap><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Qualitative results on the Epithelial Cell Benchmark.</title><p>From top to bottom: Peripodial cells (<bold>A</bold>), Proper disc cells (<bold>B</bold>). From left to right: raw data, groundtruth segmentation, PlantSeg segmentation results. PlantSeg provides accurate segmentation of both tissue types using only the networks pre-trained on the <italic>Arabidopsis</italic> ovules dataset. Red rectangles show sample over-segmentation (<bold>A</bold>) and under-segmentation (<bold>B</bold>) errors. Boundaries between segmented regions are introduced for clarity and they are not present in the pipeline output.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-app3-fig1-v2.tif"/></fig></sec></boxed-text></app><app id="appendix-4"><title>Appendix 4</title><boxed-text><sec id="s11" sec-type="appendix"><title>Supplemental figures</title><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title>Precision-recall curves on individual stacks for different CNN variants on the ovule (<bold>A</bold>) and lateral root primordia (<bold>B</bold>) datasets.</title><p>Efficiency of boundary prediction was assessed for seven training procedures that sample different type of architecture (3D U-Net <italic>vs.</italic> 3D Residual U-Net), loss function (BCE <italic>vs.</italic> Dice <italic>vs.</italic> BCE-Dice)) and normalization (Group-Norm (GN) <italic>vs.</italic> Batch-Norm (BN)). The larger the area under the curve, the better the precision. Source files used to generate the precision-recall curves are available in the <xref ref-type="supplementary-material" rid="app4fig1sdata1">Appendix 4—figure 1—source data 1</xref>.</p><p><supplementary-material id="app4fig1sdata1"><label>Appendix 4—figure 1—source data 1.</label><caption><title>Source data for precision/recall curves of different CNN variants evaluated on individual stacks.</title><p>'pmaps_root' contains precision/recall values computed on the test set from the Lateral Root dataset, 'pmaps_ovules' contains precision/recall values computed on the test set from the Ovules dataset, 'fig2_precision_recall.ipynb' is a Jupyter notebook generating the plots.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-57613-app4-fig1-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-app4-fig1-v2.tif"/></fig></sec></boxed-text></app><app id="appendix-5"><title>Appendix 5</title><boxed-text><sec id="s12" sec-type="appendix"><title>Supplemental tables</title><table-wrap id="app5table1" position="float"><label>Appendix 5—table 1.</label><caption><title>Ablation study of boundary detection accuracy.</title><p>Accuracy of boundary prediction was assessed for twelve training procedures that sample different type of architecture (3D U-Net <italic>vs.</italic> 3D Residual U-Net), loss function (BCE <italic>vs.</italic> Dice <italic>vs.</italic> BCE-Dice) and normalization (Group-Norm <italic>vs.</italic> Batch-Norm). All entries are evaluated at a fix threshold of 0.5. Reported values are the means and standard deviations for a set of seven specimen for the ovules and four for the lateral root primordia. Source files used to create the table are available in the <xref ref-type="supplementary-material" rid="app5table1sdata1">Appendix 5—table 1—source data 1</xref>.</p><p><supplementary-material id="app5table1sdata1"><label>Appendix 5—table 1—source data 1.</label><caption><title>Source data for the ablation study of boundary detection accuracy in Source data for the average segmentation accuracy of different segmentation algorithms in <xref ref-type="table" rid="app5table1">Appendix 5—table 1</xref>.</title><p>'pmaps_root' contains evaluation metrics computed on the test set from the Lateral Root dataset, 'pmaps_ovules' contains evaluation metrics computed on the test set from the Ovules dataset, 'fig2_precision_recall.ipynb' is a Jupyter notebook generating the plots.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-57613-app5-table1-data1-v2.zip"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Network and resolution</th><th>Accuracy (%)</th><th>Precision</th><th>Recall</th><th>F 1</th></tr></thead><tbody><tr><td>Ovules</td><td/><td/><td/><td/></tr><tr><td>3D-Unet, <italic>L</italic><sub><italic>bce</italic></sub>, Group-Norm</td><td>97.9 1.0</td><td>0.812 0.083</td><td>0.884 0.029</td><td>0.843 0.044</td></tr><tr><td>3D-Unet, <italic>L</italic><sub><italic>bce</italic></sub>, Batch-Norm</td><td>98.0 1.1</td><td>0.815 0.084</td><td>0.892 0.035</td><td>0.849 0.047</td></tr><tr><td>3D-Unet, <italic>L</italic><sub><italic>dice</italic></sub>, Group-Norm</td><td>97.6 1.0</td><td>0.765 0.104</td><td>0.905 0.023</td><td>0.824 0.063</td></tr><tr><td>3D-Unet, <italic>L</italic><sub><italic>dice</italic></sub>, Batch-Norm</td><td>97.8 1.1</td><td>0.794 0.084</td><td>0.908 0.030</td><td>0.844 0.048</td></tr><tr><td>3D-Unet, <inline-formula><mml:math id="inf35"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, Group-Norm</td><td>97.8 1.1</td><td>0.793 0.086</td><td>0.907 0.026</td><td>0.843 0.048</td></tr><tr><td>3D-Unet, <inline-formula><mml:math id="inf36"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, Batch-Norm</td><td>97.9 0.9</td><td>0.800 0.081</td><td>0.898 0.025</td><td>0.843 0.041</td></tr><tr><td>3D-Resunet, <italic>L</italic><sub><italic>bce</italic></sub>, Group-Norm</td><td>97.9 0.9</td><td>0.803 0.090</td><td>0.880 0.021</td><td>0.837 0.050</td></tr><tr><td>3D-Resunet, <italic>L</italic><sub><italic>bce</italic></sub>, Batch-Norm</td><td>97.9 1.0</td><td>0.811 0.081</td><td>0.881 0.031</td><td>0.841 0.042</td></tr><tr><td>3D-Resunet, <italic>L</italic><sub><italic>dice</italic></sub>, Group-Norm</td><td>95.9 2.6</td><td>0.652 0.197</td><td>0.889 0.016</td><td>0.730 0.169</td></tr><tr><td>3D-Resunet, <italic>L</italic><sub><italic>dice</italic></sub>, Batch-Norm</td><td>97.9 1.1</td><td>0.804 0.087</td><td>0.894 0.035</td><td>0.844 0.051</td></tr><tr><td>3D-Resunet, <inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, Group-Norm</td><td>97.8 1.1</td><td>0.812 0.085</td><td>0.875 0.026</td><td>0.839 0.044</td></tr><tr><td>3D-Resunet, <inline-formula><mml:math id="inf38"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, Batch-Norm</td><td>98.0 1.0</td><td>0.815 0.087</td><td>0.892 0.035</td><td>0.848 0.050</td></tr><tr><td>Lateral Root Primordia</td><td/><td/><td/><td/></tr><tr><td>3D-Unet, <italic>L</italic><sub><italic>bce</italic></sub>, Group-Norm</td><td>97.1 1.0</td><td>0.731 0.027</td><td>0.648 0.105</td><td>0.684 0.070</td></tr><tr><td>3D-Unet, <italic>L</italic><sub><italic>bce</italic></sub>, Batch-Norm</td><td>97.2 1.0</td><td>0.756 0.029</td><td>0.637 0.114</td><td>0.688 0.080</td></tr><tr><td>3D-Unet, <italic>L</italic><sub><italic>dice</italic></sub>, Group-Norm</td><td>96.1 1.1</td><td>0.587 0.116</td><td>0.729 0.094</td><td>0.644 0.098</td></tr><tr><td>3D-Unet, <italic>L</italic><sub><italic>dice</italic></sub>, Batch-Norm</td><td>97.0 0.9</td><td>0.685 0.013</td><td>0.722 0.103</td><td>0.700 0.056</td></tr><tr><td>3D-Unet, <inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, Group-Norm</td><td>96.9 1.0</td><td>0.682 0.029</td><td>0.718 0.095</td><td>0.698 0.060</td></tr><tr><td>3D-Unet, <inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, Batch-Norm</td><td>97.0 0.8</td><td>0.696 0.012</td><td>0.716 0.101</td><td>0.703 0.055</td></tr><tr><td>3D-Resunet, <italic>L</italic><sub><italic>bce</italic></sub>, Group-Norm</td><td>97.3 1.0</td><td>0.766 0.039</td><td>0.668 0.089</td><td>0.712 0.066</td></tr><tr><td>3D-Resunet, <italic>L</italic><sub><italic>bce</italic></sub>, Batch-Norm</td><td>97.0 1.1</td><td>0.751 0.042</td><td>0.615 0.116</td><td>0.673 0.086</td></tr><tr><td>3D-Resunet, <italic>L</italic><sub><italic>dice</italic></sub>, Group-Norm</td><td>96.5 0.9</td><td>0.624 0.095</td><td>0.743 0.092</td><td>0.674 0.083</td></tr><tr><td>3D-Resunet, <italic>L</italic><sub><italic>dice</italic></sub>, Batch-Norm</td><td>97.0 0.9</td><td>0.694 0.019</td><td>0.724 0.098</td><td>0.706 0.055</td></tr><tr><td>3D-Resunet, <inline-formula><mml:math id="inf41"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, Group-Norm</td><td>97.2 1.0</td><td>0.721 0.048</td><td>0.735 0.076</td><td>0.727 0.059</td></tr><tr><td>3D-Resunet, <inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, Batch-Norm</td><td>97.0 0.9</td><td>0.702 0.024</td><td>0.703 0.105</td><td>0.700 0.063</td></tr></tbody></table></table-wrap><table-wrap id="app5table2" position="float"><label>Appendix 5—table 2.</label><caption><title>Average segmentation accuracy for different segmentation algorithms.</title><p>The average is computed from a set of seven specimen for the ovules and four for the lateral root primordia (LRP), while the error is measured by standard deviation. The segmentation is produced by multicut, GASP, mutex watershed (Mutex) and DT watershed (DTWS) clustering strategies. We additionally report the scores given by the lifted multicut on the LRP dataset. The Metrics used are the Adapted Rand error to asses the overall segmentation quality, the <italic>VOI</italic><sub><italic>merge</italic></sub> and <italic>VOI</italic><sub><italic>split</italic></sub> respectively assessing erroneous merge and splitting events (lower is better for all metrics). Source files used to create the table are available in the <xref ref-type="supplementary-material" rid="app5table2sdata1">Appendix 5—table 2—source data 1</xref>.</p><p><supplementary-material id="app5table2sdata1"><label>Appendix 5—table 2—source data 1.</label><caption><title>Source data for the average segmentation accuracy of different segmentation algorithms in <xref ref-type="table" rid="app5table2">Appendix 5—table 2</xref>.</title><p>The archive contains CSV files with evaluation metrics computed on the Lateral Root and Ovules test sets. 'root_final_16_03_20_110904.csv' - evaluation metrics for the Lateral Root, 'ovules_final_16_03_20_113546.csv' - evaluation metrics for the Ovules.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-57613-app5-table2-data1-v2.zip"/></supplementary-material></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Segmentation</th><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th></tr></thead><tbody><tr><td>Ovules</td><td/><td/><td/></tr><tr><td>DTWS</td><td>0.135 0.036</td><td>0.585 0.042</td><td>0.320 0.089</td></tr><tr><td>GASP</td><td>0.114 0.059</td><td>0.357 0.066</td><td>0.354 0.109</td></tr><tr><td>MultiCut</td><td>0.145 0.080</td><td>0.418 0.069</td><td>0.429 0.124</td></tr><tr><td>Mutex</td><td>0.115 0.059</td><td>0.359 0.066</td><td>0.354 0.108</td></tr><tr><td>Lateral Root Primordia</td><td/><td/><td/></tr><tr><td>DTWS</td><td>0.550 0.158</td><td>1.869 0.174</td><td>0.159 0.073</td></tr><tr><td>GASP</td><td>0.037 0.029</td><td>0.183 0.059</td><td>0.237 0.133</td></tr><tr><td>MultiCut</td><td>0.037 0.029</td><td>0.190 0.067</td><td>0.236 0.128</td></tr><tr><td>Lifted Multicut</td><td>0.040 0.039</td><td>0.162 0.068</td><td>0.287 0.207</td></tr><tr><td>Mutex</td><td>0.105 0.118</td><td>0.624 0.812</td><td>0.542 0.614</td></tr></tbody></table></table-wrap><table-wrap id="app5table3" position="float"><label>Appendix 5—table 3.</label><caption><title>Average segmentation accuracy on leaf surfaces.</title><p>The evaluation was computed on six specimen (data available under: <ext-link ext-link-type="uri" xlink:href="https://osf.io/kfx3d">https://osf.io/kfx3d</ext-link>) with the segmentation methodology presented in section <italic>Analysis of leaf growth and differentiation</italic>. The Metrics used are: the ARand error to asses the overall segmentation quality, the <italic>VOI</italic><sub><italic>merge</italic></sub> and <italic>VOI</italic><sub><italic>split</italic></sub> assessing erroneous merge and splitting events respectively, and accuracy (Accu.) measured as percentage of correctly segmented cells (lower is better for all metrics except accuracy). For the Proj3D method a limited number of cells (1.04% mean across samples) was missing due to segmentation errors and required manual seeding. While it is not possible to quantify the favorable impact on the ARand and VOIs scores, we can assert that the Proj3D accuracy has been overestimated by approximately 1.04%.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Segmentation</th><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th><th>Accu. (%)</th><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th><th>Accu. (%)</th></tr><tr><th/><th colspan="4">Sample 1 (Arabidopsis, Col0_07 T1)</th><th colspan="4">Sample 2 (Arabidopsis, Col0_07 T2)</th></tr></thead><tbody><tr><td>PredAutoSeg</td><td>0.387</td><td>0.195</td><td>0.385</td><td>91.561</td><td>0.269</td><td>0.171</td><td>0.388</td><td>89.798</td></tr><tr><td>Proj3D</td><td>0.159</td><td>0.076</td><td>0.273</td><td>82.700</td><td>0.171</td><td>0.078</td><td>0.279</td><td>84.697</td></tr><tr><td>RawAutoSeg</td><td>0.481</td><td>0.056</td><td>0.682</td><td>75.527</td><td>0.290</td><td>0.064</td><td>0.471</td><td>75.198</td></tr><tr><td/><td colspan="4">Sample 3 (Arabidopsis, Col0_03 T1)</td><td colspan="4">Sample 4 (Arabidopsis, Col0_03 T2)</td></tr><tr><td>PredAutoSeg</td><td>0.079</td><td>0.132</td><td>0.162</td><td>90.651</td><td>0.809</td><td>0.284</td><td>0.944</td><td>90.520</td></tr><tr><td>Proj3D</td><td>0.065</td><td>0.156</td><td>0.138</td><td>88.655</td><td>0.181</td><td>0.228</td><td>0.406</td><td>91.091</td></tr><tr><td>RawAutoSeg</td><td>0.361</td><td>0.101</td><td>0.412</td><td>88.130</td><td>0.295</td><td>0.231</td><td>0.530</td><td>85.037</td></tr><tr><td/><td colspan="4">Sample 5 (Cardamine, Ox T1)</td><td colspan="4">Sample 6 (Cardamine, Ox T2)</td></tr><tr><td>PredAutoSeg</td><td>0.087</td><td>0.162</td><td>0.125</td><td>98.858</td><td>0.052</td><td>0.083</td><td>0.077</td><td>97.093</td></tr><tr><td>Proj3D</td><td>0.051</td><td>0.065</td><td>0.066</td><td>95.958</td><td>0.037</td><td>0.060</td><td>0.040</td><td>98.470</td></tr><tr><td>RawAutoSeg</td><td>0.429</td><td>0.043</td><td>0.366</td><td>93.937</td><td>0.267</td><td>0.033</td><td>0.269</td><td>89.288</td></tr></tbody></table></table-wrap></sec></boxed-text></app><app id="appendix-6"><title>Appendix 6</title><boxed-text><sec id="s13" sec-type="appendix"><title>PlantSeg - Parameters guide</title><p>The PlantSeg workflow can be customized and optimized by tuning the pipeline’s hyperparameters. The large number of available options can be intimidating for new user, therefore we provide a short guide to explain them. For more detailed and up-to-date guidelines please visit the project’s GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/hci-unihd/plant-seg">https://github.com/hci-unihd/plant-seg</ext-link>.</p><fig id="app6fig1" position="float"><label>Appendix 6—figure 1.</label><caption><title>PlantSeg GUI.</title><p>The interface allows to configure all execution steps of the segmentation pipeline, such as: selecting the neural network model and specifying hyperparameters of the partitioning algorithm. <xref ref-type="table" rid="app6table1">Appendix 6—table 1</xref> describes the Pre-processing (<bold>A</bold>) parameters. <xref ref-type="table" rid="app6table2">Appendix 6—table 2</xref> provides parameters guide for the CNN Predictions and Post-processing (<bold>B, D</bold>). Hyperparameters for Segmentation and Post-processing (<bold>C, D</bold>) are described in <xref ref-type="table" rid="app6table3">Appendix 6—table 3</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57613-app6-fig1-v2.tif"/></fig><table-wrap id="app6table1" position="float"><label>Appendix 6—table 1.</label><caption><title>Parameters guide for Data Pre-processing.</title><p>Menu A in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Process type</th><th>Parameter name</th><th>Description</th><th>Range</th><th>Default</th></tr></thead><tbody><tr><td>Data Pre-processing</td><td>Save Directory</td><td>Create a new sub folder where all results will be stored.</td><td>text</td><td>‘PreProcessing’</td></tr><tr><td/><td>Rescaling (z, y, z)</td><td>The rescaling factor can be used to make the data resolution match the resolution of the dataset used in training. Pressing the ‘Guided’ button in the GUI a widget will help the user setting up the right rescaling</td><td>tuple</td><td><inline-formula><mml:math id="inf43"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1.0</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></td></tr><tr><td/><td>Interpolation</td><td>Defines order of the spline interpolation. The order 0 is equivalent to nearest neighbour interpolation, one is equivalent to linear interpolation and two quadratic interpolation.</td><td>menu</td><td>2</td></tr><tr><td/><td>Filter</td><td>Optional: perform Gaussian smoothing or median filtering on the input. Filter has an additional parameter that set the sigma (gaussian) or disc radius (median).</td><td>menu</td><td>Disabled</td></tr></tbody></table></table-wrap><table-wrap id="app6table2" position="float"><label>Appendix 6—table 2.</label><caption><title>Parameters guide for CNN Predictions and Post-processing.</title><p>Menu B and D in <xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Process type</th><th>Parameter name</th><th>Description</th><th>Range</th><th>Default</th></tr></thead><tbody><tr><td>CNN Prediction</td><td>Model Name</td><td>Trained model name. Models trained on confocal (model name: ‘generic_confocal_3D_unet’) and lightsheet (model name: ‘generic_confocal_3D_unet’) data as well as their multi-resolution variants are available: More info on available models and importing custom models can be found in the project repository.</td><td>text</td><td>‘generic_confocal…’</td></tr><tr><td/><td>Patch Size</td><td>Patch size given to the network. A bigger patches cost more memory but can give a slight improvement in performance. For 2D segmentation the Patch size relative to the z axis has to be set to 1.</td><td>tuple</td><td><inline-formula><mml:math id="inf44"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></td></tr><tr><td/><td>Stride</td><td>Specifies the overlap between neighboring patches. The bigger the overlap the the better predictions at the cost of additional computation time. In the GUI the stride values are automatically set, the user can choose between: Accurate (50% overlap between patches), Balanced (25% overlap between patches), Draft (only 5% overlap between patches).</td><td>menu</td><td>Balanced</td></tr><tr><td/><td>Device Type</td><td>If a CUDA capable gpu is available and setup correctly, ‘cuda’ should be used, otherwise one can use ‘cpu’ for cpu only inference (much slower).</td><td>menu</td><td>‘cpu’</td></tr><tr><td>Prediction Post-processing</td><td>Convert to tiff</td><td>If True the prediction is exported as tiff file.</td><td>bool</td><td>False</td></tr><tr><td/><td>Cast Predictions</td><td>Predictions stacks are generated in ‘float32’. Or ‘uint8’ can be alternatively used to reduce the memory footprint.</td><td>menu</td><td>‘data_float32’</td></tr></tbody></table></table-wrap><table-wrap id="app6table3" position="float"><label>Appendix 6—table 3.</label><caption><title>Parameters guide for Segmentation.</title><p>Menu C and D in <xref ref-type="fig" rid="app6fig1">Appendix 6—figure 1</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Process type</th><th>Parameter name</th><th>Description</th><th>Range</th><th>Default</th></tr></thead><tbody><tr><td>Segmentation</td><td>Algorithm</td><td>Defines which algorithm will be used for segmentation.</td><td>menu</td><td>‘GASP’</td></tr><tr><td/><td>Save Directory</td><td>Create a new sub folder where all results will be stored.</td><td>text</td><td>‘GASP’</td></tr><tr><td/><td>Under/Over seg. fac.</td><td>Define the tendency of the algorithm to under of over segment the stack. Small value bias the result towards under-segmentation and large towards over-segmentation.</td><td>(0.0…1.0)</td><td>0.6</td></tr><tr><td/><td>Run Watersed in 2D</td><td>If True the initial superpixels partion will be computed slice by slice, if False in the whole 3D volume at once. While slice by slice drastically improve the speed and memory consumption, the 3D is more accurate.</td><td>bool</td><td>True</td></tr><tr><td/><td>CNN Prediction Threshold</td><td>Define the threshold used for superpixels extraction and Distance Transform Watershed. It has a crucial role for the watershed seeds extraction and can be used similarly to the ‘Unde/Over segmentation factor’ to bias the final result. An high value translate to less seeds being placed (more under segmentation), while with a low value more seeds are placed (more over segmentation).</td><td>(0.0…1.0)</td><td>0.5</td></tr><tr><td/><td>Watershed Seeds Sigma</td><td>Defines the amount of smoothing applied to the CNN predictions for seeds extraction. If a value of 0.0 used no smoothing is applied.</td><td>float</td><td>2.0</td></tr><tr><td/><td>Watershed Boundary Sigma</td><td>Defines the amount of Gaussian smoothing applied to the CNN predictions for the seeded watershed segmentation. If a value of 0.0 used no smoothing is applied.</td><td>float</td><td>0.0</td></tr><tr><td/><td>Superpixels Minimum Size</td><td>Superpixels smaller than the threshold (voxels) will be merged with a the nearest neighbour segment.</td><td>integer</td><td>50</td></tr><tr><td/><td>Cell Minimum Size</td><td>Cells smaller than the threshold (voxels) will be merged with a the nearest neighbour cell.</td><td>integer</td><td>50</td></tr><tr><td>Segmentation Post-processing</td><td>Convert to tiff</td><td>If True the segmentation is exported as tiff file.</td><td>bool</td><td>False</td></tr></tbody></table></table-wrap></sec></boxed-text></app><app id="appendix-7"><title>Appendix 7</title><boxed-text><sec id="s14" sec-type="appendix"><title>Empirical example of parameter tuning</title><p>PlantSeg’s default parameters have been chosen to yield the best result on our core datasets (Ovules, LRP). Furthermore, not only segmentation performances but also resource requirements have been taken into account, e.g. super-pixels are extracted in 2D by default in order to reduce the pipeline runtime and memory usage. Nevertheless, when applying PlantSeg on a new dataset tuning parameters tuningthe default parameters can lead to improved segmentation results. Unfortunately, image stacks can vary in: imaging acquisition, voxels resolutions, noise, cell size and cell morphology. All those factors make it very hard to define generic guidelines and optimal results will always require some trial and error.</p><p>Here we present an example of empirical parameter tuning where we use a 3D Cell Atlas as a test dataset since we can quantitatively evaluate the pipeline’s performance on it. We will show how we can improve on top of the results presented in <xref ref-type="table" rid="table1">Table 1</xref> of the main manuscript. Since the number of parameters does not allow for a complete grid search we will focus only on three key aspects: model/rescaling, over/under segmentation factor and 3D vs 2D super pixels.</p><list list-type="bullet"><list-item><p><italic>model/rescaling</italic>: As we showed in the main text, on this particular data the default confocal CNN model already provides a solid performance. If we now take into consideration the voxel resolution, we have two possible choices. Using a CNN model trained on a more similar resolution or if this is not possible using the rescaling factor to reduce the resolution gap further. In <xref ref-type="table" rid="app7table1">Appendix 7—table 1</xref> we can see how the results vary if we take into consideration those two aspects. In particular we can observe that in this case the rescaling of voxels depth of a factor 3x considerably improved the overall scores.</p></list-item><list-item><p><italic>over/under segmentation factor</italic>: Now that we have tuned the network predictions we can move to tuning the segmentation performance. The main parameter we can use is the <italic>over/under segmentation factor</italic>, this will try to compensate the over- or under-segmentation. From the results in <xref ref-type="table" rid="app7table1">Appendix 7—table 1</xref> we can observe a strong tendency towards under-segmentation, this suggest that increasing the <italic>over/under segmentation factor</italic> will balance the segmentation. In table <xref ref-type="table" rid="app7table2">Appendix 7—table 2</xref> we can see the results for three different values, increasing the <italic>over/under segmentation factor</italic> as the desired effect and overall improved the results.</p></list-item><list-item><p><italic>3D vs 2D super pixels</italic>: Already tuning this two aspects of the pipeline drastically improved the segmentation according to our metrics. as a final tweak we can switch to 3D super pixels to further improve results. In <xref ref-type="table" rid="app7table3">Appendix 7—table 3</xref> we present the final results. Overall the final improvement is roughly a factor of ×2 in terms of ARand score compared to PlantSeg default.</p></list-item></list><p>Further fine tuning could be performed on the PlantSeg parameters to further improve the scores.</p><table-wrap id="app7table1" position="float"><label>Appendix 7—table 1.</label><caption><title>Comparison between the <italic>generic confocal</italic> CNN model (default in PlantSeg), the closest confocal model in terms of xy plant voxels resolution <italic>ds3 confocal</italic> and the combination of <italic>ds3 confocal</italic> and rescaling (in order to mach the training data resolution a rescaling factor of <inline-formula><mml:math id="inf45"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> zxy has been used).</title><p>The later combination showed the best overall results. To be noted that <italic>ds3 confocal</italic> was trained on almost isotropic data, while the 3D Digital Tissue Atlas is not isotropic. Therefore poor performances without rescaling are expected. Segmentation obtained with GASP and default parameters</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dataset</th><th colspan="3">Generic confocal (Default)</th><th colspan="3">ds3 confocal</th><th colspan="3">ds3 confocal + rescaling</th></tr><tr><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th></tr></thead><tbody><tr><td>Anther</td><td>0.328</td><td>0.778</td><td>0.688</td><td>0.344</td><td>1.407</td><td>0.735</td><td>0.265</td><td>0.748</td><td>0.650</td></tr><tr><td>Filament</td><td>0.576</td><td>1.001</td><td>1.378</td><td>0.563</td><td>1.559</td><td>1.244</td><td>0.232</td><td>0.608</td><td>0.601</td></tr><tr><td>Leaf</td><td>0.075</td><td>0.353</td><td>0.322</td><td>0.118</td><td>0.718</td><td>0.384</td><td>0.149</td><td>0.361</td><td>0.342</td></tr><tr><td>Pedicel</td><td>0.400</td><td>0.787</td><td>0.869</td><td>0.395</td><td>1.447</td><td>1.082</td><td>0.402</td><td>0.807</td><td>1.161</td></tr><tr><td>Root</td><td>0.248</td><td>0.634</td><td>0.882</td><td>0.219</td><td>1.193</td><td>0.761</td><td>0.123</td><td>0.442</td><td>0.592</td></tr><tr><td>Sepal</td><td>0.527</td><td>0.746</td><td>1.032</td><td>0.503</td><td>1.293</td><td>1.281</td><td>0.713</td><td>0.652</td><td>1.615</td></tr><tr><td>Valve</td><td>0.572</td><td>0.821</td><td>1.315</td><td>0.617</td><td>1.404</td><td>1.548</td><td>0.586</td><td>0.578</td><td>1.443</td></tr><tr><td>Average</td><td>0.389</td><td>0.731</td><td>0.927</td><td>0.394</td><td>1.289</td><td>1.005</td><td>0.353</td><td>0.600</td><td>0.915</td></tr></tbody></table></table-wrap><table-wrap id="app7table2" position="float"><label>Appendix 7—table 2.</label><caption><title>Comparison between the results obtained with three different <italic>over/under segmentation factor</italic> <inline-formula><mml:math id="inf46"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>0.7</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</title><p>The effect of tuning this parameter is mostly reflected in the VOIs scores. In this case the best result have been obtained by steering the segmentation towards the over segmentation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="10">ds3 confocal + rescaling</th></tr><tr><th rowspan="2">Dataset</th><th colspan="3">Over/under factor 0.5</th><th colspan="3">Over/under factor 0.6 (Default)</th><th colspan="3">Over/under factor 0.7</th></tr><tr><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th></tr></thead><tbody><tr><td>Anther</td><td>0.548</td><td>0.540</td><td>1.131</td><td>0.265</td><td>0.748</td><td>0.650</td><td>0.215</td><td>1.130</td><td>0.517</td></tr><tr><td>Filament</td><td>0.740</td><td>0.417</td><td>1.843</td><td>0.232</td><td>0.608</td><td>0.601</td><td>0.159</td><td>0.899</td><td>0.350</td></tr><tr><td>Leaf</td><td>0.326</td><td>0.281</td><td>0.825</td><td>0.149</td><td>0.361</td><td>0.342</td><td>0.117</td><td>0.502</td><td>0.247</td></tr><tr><td>Pedicel</td><td>0.624</td><td>0.585</td><td>2.126</td><td>0.402</td><td>0.807</td><td>1.161</td><td>0.339</td><td>1.148</td><td>0.894</td></tr><tr><td>Root</td><td>0.244</td><td>0.334</td><td>0.972</td><td>0.123</td><td>0.442</td><td>0.592</td><td>0.113</td><td>0.672</td><td>0.485</td></tr><tr><td>Sepal</td><td>0.904</td><td>0.494</td><td>2.528</td><td>0.713</td><td>0.652</td><td>1.615</td><td>0.346</td><td>0.926</td><td>1.211</td></tr><tr><td>Valve</td><td>0.831</td><td>0.432</td><td>2.207</td><td>0.586</td><td>0.578</td><td>1.443</td><td>0.444</td><td>0.828</td><td>1.138</td></tr><tr><td>Average</td><td>0.602</td><td>0.441</td><td>1.662</td><td>0.353</td><td>0.600</td><td>0.915</td><td>0.248</td><td>0.872</td><td>0.691</td></tr></tbody></table></table-wrap><table-wrap id="app7table3" position="float"><label>Appendix 7—table 3.</label><caption><title>Comparison between 2D vs 3D super pixels.</title><p>From out experiments, segmentation quality is almost always improved by the usage of 3D super pixels. On the other side, the user should be aware that this improvement comes at the cost of a large slow-down of the pipeline (roughly × 4.5 on our system Intel Xenon E5-2660, RAM 252 Gb).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="9">ds3 confocal + rescaling</th></tr><tr><th colspan="9">Over/under factor 0.7</th></tr><tr><th rowspan="2">Dataset</th><th colspan="4">Super Pixels 2D (Default)</th><th colspan="4">Super Pixels 3D</th></tr><tr><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th><th>time (s)</th><th>ARand</th><th><italic>VOI</italic><sub><italic>split</italic></sub></th><th><italic>VOI</italic><sub><italic>merge</italic></sub></th><th>time (s)</th></tr></thead><tbody><tr><td>Anther</td><td>0.215</td><td>1.130</td><td>0.517</td><td>600</td><td>0.167</td><td>0.787</td><td>0.399</td><td>2310</td></tr><tr><td>Filament</td><td>0.159</td><td>0.899</td><td>0.350</td><td>120</td><td>0.171</td><td>0.687</td><td>0.487</td><td>520</td></tr><tr><td>Leaf</td><td>0.117</td><td>0.502</td><td>0.247</td><td>800</td><td>0.080</td><td>0.308</td><td>0.220</td><td>3650</td></tr><tr><td>Pedicel</td><td>0.339</td><td>1.148</td><td>0.894</td><td>450</td><td>0.314</td><td>0.845</td><td>0.604</td><td>2120</td></tr><tr><td>Root</td><td>0.113</td><td>0.672</td><td>0.485</td><td>210</td><td>0.101</td><td>0.356</td><td>0.412</td><td>920</td></tr><tr><td>Sepal</td><td>0.346</td><td>0.926</td><td>1.211</td><td>770</td><td>0.257</td><td>0.690</td><td>0.966</td><td>3420</td></tr><tr><td>Valve</td><td>0.444</td><td>0.828</td><td>1.138</td><td>530</td><td>0.300</td><td>0.494</td><td>0.875</td><td>2560</td></tr><tr><td>Average</td><td>0.248</td><td>0.872</td><td>0.691</td><td>500</td><td>0.199</td><td>0.595</td><td>0.566</td><td>2210</td></tr></tbody></table></table-wrap></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.57613.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Bergmann</surname><given-names>Dominique C</given-names></name><role>Reviewing Editor</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Bergmann</surname><given-names>Dominique C</given-names></name><role>Reviewer</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Graeff</surname><given-names>Moritz</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>In this manuscript we are introduced to PlantSeg, an automated, versatile new image analysis pipeline with features that work especially well on plant tissues. PlantSeg uses machine learning (convolutional neural networks) to identify cellular boundaries and to segment complex tissues into their constituent cells. This pipeline performs well over a range of different tissues, is accessible to novice and expert users, and can be combined with other software to enable quantitative assessment of biological features.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Accurate and versatile 3D segmentation of plant tissues at cellular resolution&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Dominique C Bergmann as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Christian Hardtke as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Moritz Graeff (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require a modest amount of additional new data, as they do with your paper, we are asking that the manuscript be revised to either limit claims to those supported by data in hand, or to explicitly state that the relevant conclusions require additional supporting data.</p><p>Our expectation is that the authors will eventually carry out the additional experiments and report on how they affect the relevant conclusions either in a preprint on bioRxiv or medRxiv, or if appropriate, as a Research Advance in <italic>eLife</italic>, either of which would be linked to the original paper.</p><p>Summary:</p><p>PlantSeg, a tool described in Wolny et al.: &quot;Accurate and versatile 3D segmentation of plant tissues at cellular resolution&quot;, harnesses recent advances in computer vision to perform volumetric segmentation of plant tissues. It describes an image analysis pipeline utilising machine learning for cellular segmentation of plant tissues, an essential but often time-consuming part of many recent studies in plant development. The authors provide an automated, versatile new pipeline that utilises convolutional neural networks to identify cellular boundaries before cell segmentation. This pipeline performs well over a range of different tissues, is accessible to novice and expert users, and will likely attract great interest within the plant (and animal) scientific community</p><p>Three reviewers concluded that this tool addressed a pressing need for the community, and in general, found the design, rationale, and performance good. There were several suggestions to improve these points as well as to improve the ease of installation and use. Below, we divide essential revisions into two categories: performance and usability, and these should be addressed in a revision and the response to reviewers.</p><p>Essential revisions:</p><p>Performance:</p><p>1) The authors compare nine different CNN designs, varying in the network architecture, loss function, and training protocol (including image augmentation and changes of layer orders within the training protocol). It is unclear how the tested designs were chosen out of the large number of possible design permutations. In particular, it is unclear why the authors do not include further design permutations using the design they consider the most robust (based on the Lbce + Ldice loss function) – only a single design uses this loss function, although it can reasonably be expected that other designs using this function might further improve CNN performance. Can the authors either include such networks, or explain why they chose not to, and furthermore lay out a clear rationale for choosing the networks presented here?</p><p>PlantSeg is accessible to non-experts, but its exact advantages over other user-friendly tools such as the U-Net ImageJ plug-in, CDeep3M, or ilastik could use further elaboration. Relative to these methods, what are PlantSeg's primary contributions-that it combines CNN-based predictions and more sophisticated post-processing methods for plant tissues?</p><p>2) In the sections assessing the performance of PlantSeg in different datasets, the authors do not specify which CNN they used for boundary detection, and which segmentation strategy they used. Did they preselect their strategy based on microscope type and voxel size, as recommended later on for other users? Or did they test multiple combinations and identified the best performing one? Considering it is not feasible to generate a ground truth for every dataset analysed, it is of great interest to the reader to understand the range in performance of the different CNN/segmentation combinations available in the PlantSeg pipeline. If the authors tested multiple combinations, they should report their results. If they used a single combination, they should explain how this was chosen.</p><p>3) In the subsection &quot;Analysis of leaf growth and differentiation&quot;, the authors specify the mean number of segmentation errors to assess the quality of the PlantSeg pipeline compared to MorphoGraphX. It is not clear how the ground truth for these comparisons was generated, and also, why the authors deviate from their more detailed assessment of segmentation quality used before (subsections &quot;Step 2: segmentation of tissues into cells using graph partitioning&quot; and &quot;Performance on external plant datasets&quot;).</p><p>4) It is unclear how PlantSeg perform on one of the most characteristic (and problematic) cell types-the highly lobed epidermal cells. Lobes have presented challenges to older watershed-based algorithms. The runs of PlantSeq on sepals, which contain these cells do not seem to have behaved well (likely due to the low quality of the input data) and the work on Cardamine leaves appears to be a combination of MorphoGraphX and PlantSeg. There are other published datasets that include lobed cells from Arabidopsis leaves and also maize leaves (where lobing is of a different nature) and these should be analyzed with PlantSeg alone to demonstrate its effectiveness at segmenting such cells.</p><p>Usability:</p><p>5) This is a Linux-based program, and this diminishes its usability especially for people who might want to use it at home on iOS/PC systems during the current pandemic. While we recognize that changing the structure to run on these systems is a big request and it is not an absolute requirement for this manuscript to be accepted, it is something that needs to be acknowledged. Writing early in the text (even in the Abstract) that this is Linux-based should cue in the reader about requirements.</p><p>6) To generate a tool that is both accurate and generalizable, you experimented with several design choices, including the network architecture, loss function, patch size, order of operations within a U-Net level, and partitioning strategy. The pre-trained networks are included in the software package and can be specified via the graphical user interface (GUI) or the command line. Here, non-experts would benefit from more guidance as to which pre-trained networks they should specify for which datasets. For example, beyond considerations such as microscope modality and voxel size, what are the guiding principles for which partitioning strategy should be selected? If this information is already available, please refer to its location in the main text.</p><p>7) Additionally, the GUI allows users to adjust a number of parameters. To expand the userbase, consider providing an appendix that (1) explains what these parameters mean and (2) outlines the circumstances under which they should be adjusted.</p><p>8) A valuable addition would be a table that lists how PlantSeg interfaces with other image analysis tools, specifically including software packages (in addition to MorphoGraphX) that can perform cell counting, cell tracking, and cell volume and shape measurements on the outputs of PlantSeg.</p><p>9) Finally, in the GitHub repository, the read-me document is helpful, but the folders and files are not named in an intuitive way for non-experts to navigate. Please rename and/or provide a short description so it is clear what each folder contains.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.57613.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Performance:</p><p>1) The authors compare nine different CNN designs, varying in the network architecture, loss function, and training protocol (including image augmentation and changes of layer orders within the training protocol). It is unclear how the tested designs were chosen out of the large number of possible design permutations. In particular, it is unclear why the authors do not include further design permutations using the design they consider the most robust (based on the Lbce + Ldice loss function) – only a single design uses this loss function, although it can reasonably be expected that other designs using this function might further improve CNN performance. Can the authors either include such networks, or explain why they chose not to, and furthermore lay out a clear rationale for choosing the networks presented here?</p></disp-quote><p>We thank the reviewers for pointing out the missing reasoning behind the particular network design choices used in our work. The rationale for choosing those particular aspects of neural network training is now described in the subsection “Step 1: cell boundary detection” which says:</p><p>“Aiming for the best performance across different microscope modalities, we explored various components of neural network design critical for improved generalization and robustness to noise, namely: the network architecture, loss function, normalization layers and size of patches used for training”.</p><p>Additionally we’ve also updated the Appendix 5—table 1 in which our complete grid search exploration is now presented, where we compare 2 different architecture variants, 3 different loss functions and 2 different normalization/patch size specifications. Clearly, we have not explored all possible design choices, but only the most popular variants. For network architecture in particular, other block or layer designs could be used, and networks other than U-nets have been introduced in the literature. However, the U-net remains the method of choice for microscopy images, it is widely used, robust and easy to train. We believe our non-expert users will benefit more from a solid implementation of the well-studied state-of-the-art network. Still, to make PlantSeg future-proof, we allow users to supply their own pre-trained networks and we will ourselves monitor the field and add new champion networks to PlantSeg once they emerge in the bioimage computing community.</p><disp-quote content-type="editor-comment"><p>PlantSeg is accessible to non-experts, but its exact advantages over other user-friendly tools such as the U-Net ImageJ plug-in, CDeep3M, or ilastik could use further elaboration. Relative to these methods, what are PlantSeg's primary contributions-that it combines CNN-based predictions and more sophisticated post-processing methods for plant tissues?</p></disp-quote><p>Regarding the PlantSeg’s primary contributions relative to other tools (ImageJ’s U-Net plugin, CDeep3M), we have added the following to the Introduction section:</p><p>“Combining the repository of state-of-the-art neural networks trained on the two common microscope modalities and going beyond just thresholding or watershed with robust graph partitioning strategies is the main strength of our package.”</p><p>In more in detail:</p><p>We differ from ImageJ’s U-Net plugin mainly by the segmentation approach. Our robust graph partitioning is especially well-suited for cell segmentation in densely packed tissue. In contrast the ImageJ’s U-Net plugin relies on a background/foreground segmentation approach that is mostly suited for more sparsely placed cells, as no advanced post-processing is made available.</p><p>CDeep3M main target application is electron microscopy images. Both the neural network architecture and the pre-trained models are optimized for this modality. Therefore, it cannot be used in the context of confocal and light sheet microscopy without re-training the models.</p><p>Moreover, we provide a tool that can be easily installed on local hardware, while CDeep3M relies on the Amazon cloud infrastructure.</p><p>The current beta version of ilastik supports inference with neural networks. However, this version is not in production yet and, since the scope of ilastik is much larger than the scope of PlantSeg, it would take a while until this part reaches full maturity, especially for training (note that ilastik is developed in Anna Kreshuk's lab at EMBL, so we are fully aware of the internal planning of its future). While we do envision eventual incorporation of the PlantSeg functionality into ilastik, we believe PlantSeg has a lot of value as a standalone product that does one thing and does it well.</p><disp-quote content-type="editor-comment"><p>2) In the sections assessing the performance of PlantSeg in different datasets, the authors do not specify which CNN they used for boundary detection, and which segmentation strategy they used. Did they preselect their strategy based on microscope type and voxel size, as recommended later on for other users? Or did they test multiple combinations and identified the best performing one? Considering it is not feasible to generate a ground truth for every dataset analysed, it is of great interest to the reader to understand the range in performance of the different CNN/segmentation combinations available in the PlantSeg pipeline. If the authors tested multiple combinations, they should report their results. If they used a single combination, they should explain how this was chosen.</p></disp-quote><p>We agree with the reviewer that the parameters choice should have been reported in the manuscript. We now mention the relevant parameters in each caption where quantitative results are shown.</p><p>The results presented in the manuscript for the performance of PlantSeg in different datasets are mostly obtained with default parameters which were chosen by quantitative evaluation on the Ovules and LRP datasets. Our aim was to show the performance of PlantSeg out of the box, before more advanced parameter tuning.</p><p>As pointed out by the reviewers PlantSeg offers a large number of options for fine-tuning the pipeline result. This freedom makes it very challenging to find shared parameters that are optimal for all datasets presented. Moreover, the variety of voxels resolutions, noise, cell size and cell morphology makes it very hard to define truly generic guidelines, and optimal results will always require some trial and error. These observations are reported in the appendix “Empirical Example of Parameter Tuning”, the paragraph reads:</p><p>“PlantSeg's default parameters have been chosen to yield the best result on our core datasets (Ovules, LRP). […] Unfortunately, image stacks can vary in: imaging acquisition, voxels resolutions, noise, cell size and cell morphology. All those factors make it very hard to define generic guidelines and optimal results will always require some trial and error.”</p><p>To illustrate our guidelines, we added an empirical example of how to tune PlantSeg for a particular dataset in the appendix “Empirical Example of Parameter Tuning”. Here we show how to tune PlantSeg parameters in three simple steps. The improvement can be noticed qualitatively, but we also evaluate it quantitatively on the Arabidopsis 3D cell atlas dataset.</p><disp-quote content-type="editor-comment"><p>3) In the subsection &quot;Analysis of leaf growth and differentiation&quot;, the authors specify the mean number of segmentation errors to assess the quality of the PlantSeg pipeline compared to MorphoGraphX. It is not clear how the ground truth for these comparisons was generated, and also, why the authors deviate from their more detailed assessment of segmentation quality used before (subsections &quot;Step 2: segmentation of tissues into cells using graph partitioning&quot; and &quot;Performance on external plant datasets&quot;).</p></disp-quote><p>We updated the main text to make it more clear why surface segmentations and MorphoGraphX were used and how PlantSeg can assist with creating surface segmentations. Furthermore, we extended the segmentation quality assessment by the previously introduced quality measures ARand, VOI<sub>split</sub> and VOI<sub>merge</sub>. Also, we revisited the methods to state more explicitly how the ground truth was generated. This part now reads as follows:</p><p>“To track leaf growth, the same sample is imaged over the course of several days, covering a volume much larger than ovules or meristems. […] In contrast, the RawAutoSeg method achieved the poorest mean scores in both measures demonstrating that using PlantSeg improved the surface segmentation (see Figure 9).”</p><p>In addition, a more complete explanation of the leaf surface workflow and ground truth generation has been added in the new subsection “Creation of leaf surface segmentations”. In particular the new text regarding the ground truth generation now reads as follows:</p><p>“To compare the segmentations created by MorphoGraphX alone with the ones using PlantSeg’s files as input, we first obtained a ground-truth segmentation using the MorphographX auto-segmentation pipeline as described in Strauss et al., 2019, (Figure 10B) and manually fixed all segmentation errors using processes in MorphoGraphX.”</p><disp-quote content-type="editor-comment"><p>4) It is unclear how PlantSeg perform on one of the most characteristic (and problematic) cell types-the highly lobed epidermal cells. Lobes have presented challenges to older watershed-based algorithms. The runs of PlantSeq on sepals, which contain these cells do not seem to have behaved well (likely due to the low quality of the input data) and the work on Cardamine leaves appears to be a combination of MorphoGraphX and PlantSeg. There are other published datasets that include lobed cells from Arabidopsis leaves and also maize leaves (where lobing is of a different nature) and these should be analyzed with PlantSeg alone to demonstrate its effectiveness at segmenting such cells.</p></disp-quote><p>We thank the reviewers for pointing out these published datasets. We fully agree that highly lobed cells make for an interesting use case and we have now evaluated Plant Seg on the data from Foxe et al. [1].</p><p>The results are presented in subsection: “Performance on external plant datasets”. We included a new figure to show them qualitatively and updated the text of the section as follows:</p><p>“Figure 5. Qualitative results on the highly lobed epidermal cells from Fox et al., 2018. […] In order to show pre-trained networks' ability to generalize to external data, we additionally depict PlantSeg's boundary predictions (third row, middle).”</p><p>We have also changed the text in subsection “Performance on external plants dataset” accordingly to the additional experiments. The section now reads as follow:</p><p>“To test the generalization capacity of PlantSeg, we assessed its performance on data for which no network training was performed. […] This demonstrates the generalization capacity of the pre-trained models from the PlantSeg package.”</p><disp-quote content-type="editor-comment"><p>Usability:</p><p>5) This is a Linux-based program, and this diminishes its usability especially for people who might want to use it at home on iOS/PC systems during the current pandemic. While we recognize that changing the structure to run on these systems is a big request and it is not an absolute requirement for this manuscript to be accepted, it is something that needs to be acknowledged. Writing early in the text (even in the Abstract) that this is Linux-based should cue in the reader about requirements.</p></disp-quote><p>We thank the reviewers for pointing out all of the PlantSeg usability issues found in the manuscript. Regarding the support of other operating systems: both MacOS and Windows 10 are supported via the docker images containing the PlantSeg package. The instructions of how to run docker images on MacOS and Windows 10 are included in the project README page on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/hci-unihd/plant-seg#docker-image">https://github.com/hci-unihd/plant-seg#docker-image</ext-link>).</p><p>In order to improve the usability of the package even more we’re currently working towards releasing the binaries for Windows 10 and MacOS by the end of the year, so that no additional software (in this case the Docker engine) will be necessary to run PlantSeg on Windows and MacOS.</p><disp-quote content-type="editor-comment"><p>6) To generate a tool that is both accurate and generalizable, you experimented with several design choices, including the network architecture, loss function, patch size, order of operations within a U-Net level, and partitioning strategy. The pre-trained networks are included in the software package and can be specified via the graphical user interface (GUI) or the command line. Here, non-experts would benefit from more guidance as to which pre-trained networks they should specify for which datasets. For example, beyond considerations such as microscope modality and voxel size, what are the guiding principles for which partitioning strategy should be selected? If this information is already available, please refer to its location in the main text.</p></disp-quote><p>We thank the reviewers for pointing out the lack of clear explanation of how the appropriate network and the partitioning strategy can be chosen by the user.</p><p>Regarding the choice of the network, the voxel size and microscope modality are the only and most critical parameters which have to be specified by the user. This fact has been already explained in the subsection “A package for plant tissue segmentation and benchmarking”:</p><p>“Users can select from the available set of pre-trained networks the one with features most similar to their datasets. Alternatively, users can let PlantSeg select the pre-trained network based on the microscope modality (light sheet or confocal) and voxel size”.</p><p>The specific architectural design of the pre-trained networks are hidden from the user. Those were only relevant in the exploratory phase where we looked for the right combination of the network architecture, loss function and the normalization layers that would work best on our two core datasets. After finding the best combination of those design choices, we fixed them and trained separate networks for different microscope modalities and voxel sizes.</p><p>With regard to the partitioning strategy, in the subsection “Segmentation using graph partitioning” we now also explain how to choose the it most effectively:</p><p>“As a general guideline for choosing the partitioning strategy on a new data is to start with the GASP algorithm, which is the most generic. […] Importantly the newly proof-read results can then be used to train a better network that can be applied to this type of data in the future (see the appendix \nameref{sec:ground_truth_creation} for an overview of this process).”</p><disp-quote content-type="editor-comment"><p>7) Additionally, the GUI allows users to adjust a number of parameters. To expand the userbase, consider providing an appendix that (1) explains what these parameters mean and (2) outlines the circumstances under which they should be adjusted.</p></disp-quote><p>We agree that including a parameters guide in the manuscript appendix would greatly help new users. The newly added Appendix 6 contains description of all main parameters available in PlantSeg, while Appendix 7 provides an example of parameter tuning for a particular dataset from the 3D Tissue Atlas.</p><disp-quote content-type="editor-comment"><p>8) A valuable addition would be a table that lists how PlantSeg interfaces with other image analysis tools, specifically including software packages (in addition to MorphoGraphX) that can perform cell counting, cell tracking, and cell volume and shape measurements on the outputs of PlantSeg.</p></disp-quote><p>We agree with the reviewers that since PlantSeg was used in combination with e.g. Paintera for proofreading and MorphographX for mesh extraction and surface segmentation including an explanation of how to integrate PlantSeg with other tools would be helpful for the end users. In the subsection “A package for plant tissue segmentation and benchmarking” we explain that results from PlantSeg can easily be loaded into other tools for further processing of the segmentation results:</p><p>“Our software can export both the segmentation results and the boundary probability maps as Hierarchical Data Format (HDF5) or Tagged Image File Format (TIFF). […] In exported boundary probability maps each pixel has a floating point number between 0 and 1 reflecting a probability of that pixel belonging to a cell boundary.”</p><disp-quote content-type="editor-comment"><p>9) Finally, in the GitHub repository, the read-me document is helpful, but the folders and files are not named in an intuitive way for non-experts to navigate. Please rename and/or provide a short description so it is clear what each folder contains.</p></disp-quote><p>The naming conventions used in the repository follow a standard structure for Python packages (described in: <ext-link ext-link-type="uri" xlink:href="https://docs.python-guide.org/writing/structure/">https://docs.python-guide.org/writing/structure</ext-link>). Nevertheless, we agree that it can be hard to navigate, so we now included an index with descriptions in the repository’s README page (https://github.com/hci-unihd/plant-seg/blob/master/README.md#repository-index).</p></body></sub-article></article>