<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="editorial" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">00855</article-id><article-id pub-id-type="doi">10.7554/eLife.00855</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Editorial</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Science Policy</subject></subj-group></article-categories><title-group><article-title>Reforming research assessment</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-1032"><name><surname>Schekman</surname><given-names>Randy</given-names></name><role>Editor-in-Chief</role><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-1002"><name><surname>Patterson</surname><given-names>Mark</given-names></name><role>Executive Director</role><email>editorial@elifesciences.org</email><xref ref-type="fn" rid="conf1"/></contrib></contrib-group><pub-date date-type="pub" publication-format="electronic"><day>16</day><month>05</month><year>2013</year></pub-date><pub-date pub-type="collection"><year>2013</year></pub-date><volume>2</volume><elocation-id>e00855</elocation-id><permissions><copyright-statement>© 2013, Schekman and Patterson</copyright-statement><copyright-year>2013</copyright-year><copyright-holder>Schekman and Patterson</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-00855-v1.pdf"/><abstract><p>It is time for the research community to rethink how the outputs of scientific research are evaluated and, as the San Francisco Declaration on Research Assessment makes clear, this should involve replacing the journal impact factor with a broad range of more meaningful approaches.</p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>research assessment</kwd><kwd>science policy</kwd><kwd>scientific publishing</kwd><kwd>publishing</kwd><kwd>eLife</kwd></kwd-group></article-meta></front><body><p>One of the aims of <italic>eLife</italic> is to publish research articles in all areas of the life sciences and biomedicine, ranging from insights into basic biology through to translational and more applied work, and to date we have published articles on topics ranging from genome editing and plant-predator interactions to global life expectancy and the neurobiology of walking.</p><p>The impacts of such a broad range of research topics will be similarly diverse. Some articles will stimulate further research by other scientists in the same field, some will lead to clinical or commercial applications, some will be covered in the media and be of interest to the public, some will achieve all of the above and some, inevitably, will have limited impact. The recently released San Francisco Declaration on Research Assessment (<ext-link ext-link-type="uri" xlink:href="http://www.ascb.org/SFdeclaration.html">http://www.ascb.org/SFdeclaration.html</ext-link>) aims to ‘improve the ways in which the output of scientific research is evaluated by funding agencies, academic institutions, and other parties’.</p><p>Currently, however, there is a widespread perception that research assessment is dominated by a single metric, the journal impact factor, which is the average rate of citation to a given journal over a short period. There are many reasons why the impact factor of a journal cannot and should not be used as a proxy for the importance of individual articles in the journal (<xref ref-type="bibr" rid="bib6">Seglen, 1992</xref>; <xref ref-type="bibr" rid="bib2">Adler et al., 2008</xref>; <xref ref-type="bibr" rid="bib3">Campbell, 2008</xref>; <xref ref-type="bibr" rid="bib4">Curry, 2012</xref>). Yet even though most of these reasons are well known, the most frequently asked question for any journal is ‘what’s your impact factor?’</p><p>The consequences of such a narrow view of research assessment have been discussed many times (<xref ref-type="bibr" rid="bib7">Vale, 2012</xref>; <xref ref-type="bibr" rid="bib8">Vosshall, 2012</xref>). There is intense competition for publication in high-impact-factor journals, frequently resulting in multiple rounds of review and revision; and if the manuscript is ultimately rejected, the whole depressing cycle is often repeated at a new journal. The resultant delays in the communication of new findings hinder scientific progress and waste limited resources. The focus on publication in a high-impact-factor journal as the prize also distracts attention from other important responsibilities of researchers—such as teaching, mentoring and a host of other activities (including the review of manuscripts for journals!). For the sake of science, the emphasis needs to change.</p><p>Anecdotally, we as scientists and editors hear time and again from junior and senior colleagues alike that publication in high-impact-factor journals is essential for career advancement. However, deans and heads of departments send out a different message, saying that letters of recommendation hold more sway than impact factors in promotion and tenure decisions (<xref ref-type="bibr" rid="bib1">Abbott et al, 2010</xref>; <xref ref-type="bibr" rid="bib9">Zare, 2012</xref>). Moreover, some research funders (including the <ext-link ext-link-type="uri" xlink:href="http://www.wellcome.ac.uk/About-us/Policy/Policy-and-position-statements/WTD002766.htm">Wellcome Trust</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://www.rcuk.ac.uk/documents/documents/RCUKOpenAccessPolicy.pdf">Research Councils UK</ext-link>) now stress that assessments of funding applications should focus on the merits of the work proposed rather than the journals (and therefore their impact factors) in which an applicant has published. Similarly, researchers on the sub-panels assessing the quality of research in higher education institutions in the UK as part of the <ext-link ext-link-type="uri" xlink:href="http://www.ref.ac.uk/faq/all/">Research Excellence Framework</ext-link> (REF) have been told: ‘No sub-panel will make any use of journal impact factors, rankings, lists or the perceived standing of publishers in assessing the quality of research outputs’. However, there is evidence that some universities are making use of journal impact factors when selecting the papers that will be included in their submission to the REF (<xref ref-type="bibr" rid="bib5">Rohn, 2012</xref>). And, it remains sadly true that at many institutions in countries where the internal resources may be inadequate to give proper consideration to expert letters and thoroughly review a candidate’s published work, the impact factor remains a convenient crutch on which to base an imperfect evaluation of merit.</p><p>There are, however, early signs of an encouraging shift in focus from the journal in which a finding is published to the work itself, with this shift being supported by the availability of metrics at the level of individual articles for many journals. PLOS have been pioneers in this area and, since 2009, have been providing a rich array of <ext-link ext-link-type="uri" xlink:href="http://article-level-metrics.plos.org/">metrics</ext-link> on every article published. Using these approaches, assessment can be further extended to a broader array of research outputs, via services that support the deposition of outputs other than full articles, such as Dryad (for datasets), Figshare (for the results of individual experiments, figures, datasets), and Slideshare (for presentations). The emergence of new services, such as Altmetric, Impact Story and Plum Analytics, which aggregate media coverage, citation numbers, social web metrics and so on of individual research outputs, will also provide authors with a more complete picture of the impact of their research.</p><p>The changes that are slowly taking place, and which are being facilitated by new technology and tools, lend support to the view that it is time for the research community to reclaim ownership of research evaluation (<xref ref-type="bibr" rid="bib7">Vale, 2012</xref>). The Declaration on Research Assessment identifies some steps that can now be taken. Recommendations are proposed for all of the key constituencies involved–researchers, publishers, institutions and funders–because it will take commitment and persistence across these groups if we are to reform current practices.</p><p>At <italic>eLife</italic>, we strongly support the improvement of research assessment, and the shift from journal-based metrics to an array of article (and other output) metrics and indicators. If and when <italic>eLife</italic> is awarded an impact factor, we will not promote this metric. Instead, we will continue to support a vision for research assessment that relies on a range of transparent evidence–qualitative as well as quantitative–about the specific impacts and outcomes of a collection of relevant research outputs. In this way, the concept of research impact can be expanded and enriched rather than reduced to a single number or a journal name.</p><p>With less (or ideally no) involvement of impact factors in research assessment, we believe that research communication will undergo substantial improvement. Journals can focus on scientific integrity and quality, and promote the values and services that they offer, supported by appropriate metrics as evidence of their performance. Authors can choose their preferred venue based on service, cost and reputation in their field. All constituencies will then benefit from a deeper understanding of the significance and influence of our collective investment in research, and ultimately a more effective system of research communication.</p></body><back><fn-group content-type="competing-interest"><fn fn-type="conflict" id="conf1"><label>Competing interests:</label><p>RS and MP attended the initial meeting at the ASCB annual meeting in San Francisco that led to the creation of the Declaration on Research Assessment and participated in its drafting.</p></fn></fn-group><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>A</given-names></name><name><surname>Cyranoski</surname><given-names>D</given-names></name><name><surname>Jones</surname><given-names>N</given-names></name><name><surname>Maher</surname><given-names>B</given-names></name><name><surname>Schiermeier</surname><given-names>Q</given-names></name><name><surname>Van Noorden</surname><given-names>R</given-names></name></person-group><year>2010</year><article-title>Do metrics matter?</article-title><source>Nature</source><volume>465</volume><fpage>860</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1038/465860a</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Adler</surname><given-names>R</given-names></name><name><surname>Ewing</surname><given-names>J</given-names></name><name><surname>Taylor</surname><given-names>P</given-names></name></person-group><year>2008</year><article-title>Citation Statistics</article-title><ext-link ext-link-type="uri" xlink:href="http://www.mathunion.org/publications/report/citationstatistics0/">http://www.mathunion.org/publications/report/citationstatistics0/</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>P</given-names></name></person-group><year>2008</year><article-title>Escape from the impact factor</article-title><source>Ethics Sci Environ Polit</source><volume>8</volume><fpage>5</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.3354/esep00078</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Curry</surname><given-names>S</given-names></name></person-group><year>2012</year><article-title>Sick of impact factors</article-title><ext-link ext-link-type="uri" xlink:href="http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/">http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Rohn</surname><given-names>J</given-names></name></person-group><year>2012</year><article-title>Business as usual in judging the worth of a researcher?</article-title><ext-link ext-link-type="uri" xlink:href="http://www.guardian.co.uk/science/occams-corner/2012/nov/30/1">http://www.guardian.co.uk/science/occams-corner/2012/nov/30/1</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seglen</surname><given-names>PO</given-names></name></person-group><year>1992</year><article-title>The skewness of science</article-title><source>J Am Soc Info Sci</source><volume>43</volume><fpage>628</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1097-4571(199210)43:9&lt;628::AID-ASI5&gt;3.0.CO;2-0</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vale</surname><given-names>RD</given-names></name></person-group><year>2012</year><article-title>Evaluating how we evaluate</article-title><source>Mol Biol Cell</source><volume>23</volume><fpage>3285</fpage><lpage>3289</lpage><pub-id pub-id-type="doi">10.1091/mbc.E12-06-0490</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vosshall</surname><given-names>LB</given-names></name></person-group><year>2012</year><article-title>The glacial pace of scientific publishing: why it hurts everyone and what we can do to fix it</article-title><source>FASEB J</source><volume>26</volume><fpage>3589</fpage><lpage>3593</lpage><pub-id pub-id-type="doi">10.1096/fj.12-0901ufm</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zare</surname><given-names>RN</given-names></name></person-group><year>2012</year><article-title>Assessing academic researchers</article-title><source>Angew Chemie Intl Edn</source><volume>51</volume><fpage>7338</fpage><lpage>7339</lpage><pub-id pub-id-type="doi">10.1002/anie.201201011</pub-id></element-citation></ref></ref-list></back></article>