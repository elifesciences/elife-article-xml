<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">65590</article-id><article-id pub-id-type="doi">10.7554/eLife.65590</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A quadratic model captures the human V1 response to variations in chromatic direction and contrast</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-107510"><name><surname>Barnett</surname><given-names>Michael A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8355-4601</contrib-id><email>micalan@sas.upenn.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-31779"><name><surname>Aguirre</surname><given-names>Geoffrey K</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4028-3100</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-115968"><name><surname>Brainard</surname><given-names>David</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9827-543X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychology, University of Pennsylvania</institution><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Neurology, University of Pennsylvania</institution><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Horwitz</surname><given-names>Gregory D</given-names></name><role>Reviewing Editor</role><aff><institution>University of Washington</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution>Stanford University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>03</day><month>08</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e65590</elocation-id><history><date date-type="received" iso-8601-date="2020-12-09"><day>09</day><month>12</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-07-27"><day>27</day><month>07</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-12-04"><day>04</day><month>12</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.12.03.410506"/></event></pub-history><permissions><copyright-statement>© 2021, Barnett et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Barnett et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-65590-v2.pdf"/><abstract><p>An important goal for vision science is to develop quantitative models of the representation of visual signals at post-receptoral sites. To this end, we develop the quadratic color model (QCM) and examine its ability to account for the BOLD fMRI response in human V1 to spatially uniform, temporal chromatic modulations that systematically vary in chromatic direction and contrast. We find that the QCM explains the same, cross-validated variance as a conventional general linear model, with far fewer free parameters. The QCM generalizes to allow prediction of V1 responses to a large range of modulations. We replicate the results for each subject and find good agreement across both replications and subjects. We find that within the LM cone contrast plane, V1 is most sensitive to L-M contrast modulations and least sensitive to L+M contrast modulations. Within V1, we observe little to no change in chromatic sensitivity as a function of eccentricity.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>color vision</kwd><kwd>fMRI</kwd><kwd>V1</kwd><kwd>isoresponse contour</kwd><kwd>forward model</kwd><kwd>contrast response function</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DGE-1845298</award-id><principal-award-recipient><name><surname>Barnett</surname><given-names>Michael A</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>RO1 EY10016</award-id><principal-award-recipient><name><surname>Brainard</surname><given-names>David</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>P30 EY001583</award-id><principal-award-recipient><name><surname>Brainard</surname><given-names>David</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A quantitative forward model captures fMRI responses to chromatic stimuli in human primary visual cortex and quantifies how chromatic sensitivity changes with eccentricity.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The initial stage of human color vision is well characterized. The encoding of light by the three classes of cone photoreceptors (L, M, and S) is described quantitatively by a set of spectral sensitivity functions, one for each class. Knowledge of the spectral sensitivities allows for the calculation of cone excitations from the spectral radiance of the light entering the eye (<xref ref-type="bibr" rid="bib12">Brainard and Stockman, 2010</xref>). This quantitative characterization supports the analysis of the information available to subsequent processing stages (<xref ref-type="bibr" rid="bib25">Geisler, 1989</xref>; <xref ref-type="bibr" rid="bib15">Cottaris et al., 2019</xref>), supports the precise specification of visual stimuli (<xref ref-type="bibr" rid="bib10">Brainard, 1996</xref>; <xref ref-type="bibr" rid="bib11">Brainard et al., 2002</xref>), and enables color reproduction technologies (<xref ref-type="bibr" rid="bib78">Wandell and Silverstein, 2003</xref>; <xref ref-type="bibr" rid="bib33">Hunt, 2004</xref>). An important goal for vision science is to develop similarly quantitative models for the representation of visual signals at post-receptoral sites.</p><p>The second stage of color vision combines the signals from the cones to create three post-receptoral mechanisms. Psychophysical evidence supports the existence of two cone-opponent mechanisms, which represent differences between cone signals (S-(L+M) and L-M), and a luminance mechanism, which represents an additive combination (L+M) (<xref ref-type="bibr" rid="bib43">Krauskopf et al., 1982</xref>; <xref ref-type="bibr" rid="bib72">Stockman and Brainard, 2010</xref>). Physiological evidence shows that this recombination begins in the retina with correlates observed in the responses of retinal ganglion cells and subsequently in the neurons of the lateral geniculate nucleus (<xref ref-type="bibr" rid="bib18">De Valois et al., 1966</xref>; <xref ref-type="bibr" rid="bib19">Derrington et al., 1984</xref>; <xref ref-type="bibr" rid="bib48">Lennie and Movshon, 2005</xref>). While the outlines of this second stage seem well established, the precise links between retinal physiology and visual perception remain qualitative and subject to debate (<xref ref-type="bibr" rid="bib72">Stockman and Brainard, 2010</xref>; <xref ref-type="bibr" rid="bib66">Shevell and Martin, 2017</xref>).</p><p>Studies focused on developing quantitative parametric models of the chromatic response properties of neurons in primary visual cortex of primates (area V1) have not yet converged on a widely accepted model (<xref ref-type="bibr" rid="bib37">Johnson et al., 2004</xref>; <xref ref-type="bibr" rid="bib68">Solomon and Lennie, 2005</xref>; <xref ref-type="bibr" rid="bib74">Tailby et al., 2008</xref>; <xref ref-type="bibr" rid="bib32">Horwitz and Hass, 2012</xref>; <xref ref-type="bibr" rid="bib81">Weller and Horwitz, 2018</xref>). In part, this is due to the considerable heterogeneity of chromatic response properties found across individual cortical neurons (<xref ref-type="bibr" rid="bib24">Gegenfurtner, 2001</xref>; <xref ref-type="bibr" rid="bib48">Lennie and Movshon, 2005</xref>; <xref ref-type="bibr" rid="bib69">Solomon and Lennie, 2007</xref>; <xref ref-type="bibr" rid="bib65">Shapley and Hawken, 2011</xref>; <xref ref-type="bibr" rid="bib31">Horwitz, 2020</xref>). In addition, variation in stimulus properties across studies limits the ability to compare and integrate results.</p><p>The chromatic response of V1 has also been studied using blood oxygen level dependent (BOLD) functional magnetic resonance imaging (fMRI) (<xref ref-type="bibr" rid="bib77">Wandell et al., 2006</xref>). This includes studies that characterize the relative responsiveness of V1 (and other visual areas) to various chromatic and achromatic stimuli (<xref ref-type="bibr" rid="bib20">Engel et al., 1997</xref>; <xref ref-type="bibr" rid="bib29">Hadjikhani et al., 1998</xref>; <xref ref-type="bibr" rid="bib5">Beauchamp et al., 1999</xref>; <xref ref-type="bibr" rid="bib3">Bartels and Zeki, 2000</xref>; <xref ref-type="bibr" rid="bib53">Mullen et al., 2007</xref>; <xref ref-type="bibr" rid="bib28">Goddard et al., 2011</xref>; <xref ref-type="bibr" rid="bib44">Lafer-Sousa et al., 2016</xref>) and how this depends on the spatial and temporal properties of the stimulus (<xref ref-type="bibr" rid="bib49">Liu and Wandell, 2005</xref>; <xref ref-type="bibr" rid="bib16">D'Souza et al., 2016</xref>; <xref ref-type="bibr" rid="bib55">Mullen et al., 2010b</xref>).</p><p>Few studies, however, have pursued a quantitative model of the V1 BOLD response to arbitrary chromatic stimulus modulations. Development of such a model is important, since it would enable generalizations of what is known from laboratory measurements to natural viewing environments, where stimuli rarely isolate single mechanisms. Further, the parameters of such a model provide a succinct summary of processing that could be used to understand the flow of chromatic information through cortex. Notably, <xref ref-type="bibr" rid="bib20">Engel et al., 1997</xref> conducted a pioneering study that varied the chromatic content and temporal frequency of stimuli and observed that the V1 BOLD fMRI signal was maximally sensitive to L-M stimulus modulations.</p><p>In the present study, we focus on the signals that reach V1 from stimulus modulations confined to the L- and M-cone contrast plane (LM contrast plane). Specifically, we measured responses with fMRI to flickering modulations designed to systematically vary combinations of L- and M-cone contrast. Using these data, we developed a model—the quadratic color model (QCM)—that predicts the V1 BOLD fMRI response for any arbitrary stimulus in the LM contrast plane, using a small set of parameters. We validate the QCM through comparison to a less constrained general linear model (GLM). Importantly, the parameters of the QCM are biologically meaningful, and describe the sensitivity of V1 to chromatic modulations. Further, we generate cortical surface maps of model parameters across early visual cortex, allowing us to examine how chromatic sensitivity changes across V1 as a function of visual field eccentricity.</p><sec id="s1-1"><title>Quadratic color model (QCM)</title><p>This section provides an overview of the Quadratic Color Model (QCM); a full mathematical description is provided in the Appendix 1. Given a description of the stimulus, the QCM provides a prediction of the BOLD fMRI response within V1. Our stimuli were full field temporal chromatic modulations that can be specified by their contrast (vector length of the stimulus in the LM contrast plane) and chromatic direction (angle of the stimulus measured counterclockwise with respect to the positive abscissa). From this stimulus specification, the model employs three stages that convert the input to the BOLD fMRI response (<xref ref-type="fig" rid="fig1">Figure 1</xref>). First, a quadratic isoresponse contour is defined that allows for the transformation of contrast and direction into what we term the ‘equivalent contrast’. Second, a single non-linear function transforms the equivalent contrast to a prediction of the population neuronal response underlying the BOLD response. Finally, the neuronal response is converted to a predicted BOLD response by convolution with a hemodynamic response function.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Quadratic color model.</title><p>(<bold>A</bold>) The LM contrast plane representing two example stimuli (c<sub>1</sub> and c<sub>2</sub>) as the green and yellow vectors. The vector length and direction specify the contrast and chromatic direction of the positive arm of the symmetric modulation (see Visual Stimuli in Materials and methods). Using the parameters of an elliptical isoresponse contour (panel A, dashed gray ellipses), fit per subject, we can construct a 2x2 matrix Q that allows us to compute the equivalent contrast of any stimulus in the LM contrast plane (panel <bold>B</bold>; e<sub>1</sub> and e<sub>2</sub>; see Appendix 1). (<bold>B</bold>) Transformation of equivalent contrast to neuronal response. The equivalent contrasts of the two example stimuli from panel A are plotted against their associated neuronal response. A single Naka-Rushton function describes the relationship between equivalent contrast and the underlying neuronal response. (<bold>C</bold>) To predict the BOLD fMRI response, we convolve the neuronal response output of the Naka-Rushton function with a subject-specific hemodynamic response function. Note that the BOLD fMRI response prediction for the green point is greater than the prediction for the yellow point, even though the yellow point has greater cone contrast. This is because of where the stimuli lie relative to the isoresponse contours. The difference in chromatic direction results in the green point producing a greater equivalent contrast, resulting in the larger BOLD response.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig1-v2.tif"/></fig></sec><sec id="s1-2"><title>Isoresponse contours and equivalent contrast</title><p>The first stage of the QCM computes the equivalent contrast of a stimulus from its cone contrast using a subject-specific elliptical isoresponse contour. Equivalent contrast is the effective contrast of a stimulus in V1 once it has been adjusted to account for differences in the neuronal sensitivity to stimulation across different chromatic directions. An isoresponse contour is defined as a set of stimuli that evoke the same neuronal response. In the QCM, the loci of such stimuli form an elliptical isoresponse contour in the LM cone contrast plane. All points on this elliptical isoresponse contour have the same equivalent contrast (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, dashed gray ellipses). As the amplitude of the neuronal response increases, the ellipse that defines the set of stimuli producing that response also grows in overall scale. Importantly, the QCM assumes that the aspect ratio and orientation of elliptical isoresponse contours do not change as a function of the response level; only the overall scale of the ellipse changes. The use of elliptical isoresponse contours is motivated by prior psychophysical (<xref ref-type="bibr" rid="bib59">Poirson et al., 1990</xref>; <xref ref-type="bibr" rid="bib42">Knoblauch and Maloney, 1996</xref>), electrophysiological (<xref ref-type="bibr" rid="bib32">Horwitz and Hass, 2012</xref>), and fMRI experiments (<xref ref-type="bibr" rid="bib20">Engel et al., 1997</xref>) which have successfully used ellipses to model chromatic isoresponse contours.</p><p>The elliptical isoresponse contours are described by a symmetric quadratic function that defines the major and minor axes of the ellipse. We use this quadratic function to compute the equivalent contrast for each stimulus. The vector lengths of all stimuli that lie on a single isoresponse contour provide the cone contrasts required to elicit an equal neuronal response. The minor axis of the elliptical isoresponse contour corresponds to the chromatic direction that requires the least amount of cone contrast to produce this equal neuronal response, and is therefore the most sensitive chromatic direction. The major axis corresponds to the direction of least sensitivity. At this stage, the model is only concerned with the shape of the elliptical contour, thus we adopt the convention of normalizing the ellipse used to define equivalent contrast so that its major axis has unit length. This allows the length of the minor axis to directly represent the relative sensitivity, which is taken as a ratio of the minor axis (maximal sensitivity) to major axis (minimal sensitivity), referred to as the minor axis ratio. The angle of the major axis in the LM contrast plane (ellipse angle) orients these maximally and minimally sensitive directions.</p></sec><sec id="s1-3"><title>Response non-linearity</title><p>Since all of the stimuli that lie on a single isoresponse contour produce the same response, we can represent these points by their common equivalent contrast. The neuronal responses to stimuli across different color directions are a function of this single variable, and therefore we can transform equivalent contrast into predicted neuronal response via a single static non-linear function (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Here, we employ the four-parameter Naka-Rushton function (see Appendix 1).</p></sec><sec id="s1-4"><title>Transformation to BOLD fMRI signal</title><p>To predict the BOLD fMRI signal, we obtain the time-varying neuronal response prediction from the Naka-Rushton function for a stimulus sequence presented in the fMRI experiment. This neuronal response is convolved with a subject-specific hemodynamic response function to produce a prediction of the BOLD fMRI signal (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p></sec><sec id="s1-5"><title>QCM summary</title><p>In summary, the QCM takes as input the temporal sequence of stimulus modulations, defined by their chromatic direction and contrast in the LM cone contrast plane, and outputs a prediction of the BOLD fMRI time course. The QCM has six free parameters: two that define the shape of the normalized elliptical isoresponse contour and four that define the Naka-Rushton equivalent contrast response function.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><p>To evaluate the QCM, three subjects underwent fMRI scanning while viewing stimuli consisting of spatially uniform (0 cycles per degree) chromatic temporal modulations, presented using a block design. Each 12 s block consisted of a 12 Hz bipolar temporal modulation in one of 8 chromatic directions and at one of 5 log-spaced contrast levels. We split the chromatic directions into two sessions and subjects viewed each of the 20 combinations of chromatic direction (four directions) and contrast (five levels) once per run in a pseudorandomized order (see Materials and methods, Figure 10 and 11). For each subject, a measurement set consisted of 20 functional runs conducted across the two scanning sessions. We collected two complete measurement sets (referred to as Measurement Set 1 and 2) for each subject, and fit the model to each set separately to test for the replicability of our findings. We first modeled the data using a conventional GLM that accounts for the response to each of the 40 stimulus modulations independently. The fit of this relatively unconstrained model was used as a benchmark to evaluate the performance of QCM. Results were similar for all three subjects. In the main text, we illustrate our findings with the data from one subject (Subject 2); results from the other two subjects may be found in the supplementary materials.</p><sec id="s2-1"><title>Characterizing cortical responses with a conventional GLM</title><sec id="s2-1-1"><title>Contrast-response functions</title><p>To examine the basic chromatic response properties of V1, we grouped the GLM beta weights by their corresponding chromatic direction and plotted them as a function of contrast, indicated as the filled circles in each of the eight panels of <xref ref-type="fig" rid="fig2">Figure 2</xref> (data from Subject 2). For each chromatic direction, the V1 BOLD response generally increased with contrast, as expected. This result is consistent across the two independent measurement sets for Subject 2, as can be seen by comparing the green and purple points in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Further, the increasing response with stimulus contrast was also observed in both measurement sets for the other two subjects (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref>–<xref ref-type="fig" rid="fig2s2">2</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>V1 contrast response functions for the eight measured chromatic directions from Subject 2.</title><p>Each panel plots the contrast response function of V1, aggregated over 0° to 20° eccentricity, for a single chromatic direction. The x-axis is contrast, the y-axis is the BOLD response (taken as the GLM beta weight for each stimulus). The chromatic direction of each stimulus is indicated in the upper left of each panel. The curves represent the QCM prediction of the contrast response function. Error bars indicate 68% confidence intervals obtained by bootstrap resampling. Measurement Sets 1 and 2 are shown in green and purple. The x-axis range differs across panels as the maximum contrast used varies with chromatic direction. All data shown have had the baseline estimated from the background condition subtracted such that we obtain a 0 beta weight at 0 contrast.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>V1 contrast response functions for the eight measured chromatic directions from Subject 1.</title><p>The format of the figure is the same as <xref ref-type="fig" rid="fig2">Figure 2</xref> in the main text. The x-axis is contrast; the y-axis is the beta weight of the GLM. The chromatic direction of each stimulus is indicated in the upper left of each panel. The curves in each panel represent the contrast response function obtained using the QCM. The error bars indicate 68% confidence intervals obtained using bootstrapping. Measurement sets 1 and 2 are shown in green and purple. The x-axis range differs across panels as the maximum contrast used varies with color direction.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>V1 contrast response functions for the eight measured chromatic directions from Subject 3.</title><p>The format of the figure is the same as <xref ref-type="fig" rid="fig2">Figure 2</xref> in the main text and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig2-figsupp2-v2.tif"/></fig></fig-group><p>The rate at which V1 BOLD responses increase with contrast varied with chromatic direction. This can be seen in <xref ref-type="fig" rid="fig2">Figure 2</xref> by noting that the maximum stimulus contrast differed considerably across chromatic directions, while the maximum response remained similar. For example, a modulation in the 45° direction required ~60% stimulus contrast to elicit a response of 0.6 while stimuli modulated in the −45° direction required only ~12% stimulus contrast to produce a similar response.</p><p>The GLM places no constraints on the values of GLM beta weights, and we observed that these values did not always increase monotonically with contrast. Given the a priori expectation that the BOLD response itself increases monotonically with contrast, this raises the possibility that the GLM overfits the data, using its flexibility to account for the noise as well as the signal in the response. To examine this, we fit a series of more constrained models that enforce the requirement that the fitted response within chromatic direction increases monotonically with contrast. These models employed a Naka-Rushton function to describe the contrast response function in each chromatic direction. Across the models, we constrained varying numbers of the parameters to be constant across chromatic direction. The most general of these models fits a separate Naka-Rushton function to each color direction, allowing all but the offset parameter to be independent across chromatic directions. We also explored locking the amplitude parameter (in addition to the offset), the exponent parameter (in addition to the offset), and the amplitude, exponent, and offset parameters (allowing only the semi-saturation parameter to vary with chromatic direction). To evaluate how well these models fit the data, we ran a cross-validation procedure, described below, to compare the Naka-Rushton model fits with those of the GLM. The cross-validated R<sup>2</sup> for all of the Naka-Rushton models was slightly better than for the GLM, indicating that enforcing smooth monotonicity reduces a slight overfitting. These cross-validation results can be seen in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. For simplicity, and due to the small differences in fit, we retain the GLM as the point of comparison for the performance of the QCM.</p></sec></sec><sec id="s2-2"><title>Quality of GLM time course fit</title><p>We examined how well the GLM fit the measured BOLD response from area V1. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows the fit of the GLM for six example runs from Subject 2. In each panel, the measured BOLD percent signal change is shown as the thin gray line, while the fit obtained from the GLM is shown as the orange line. The orange shaded region represents the 68% confidence interval of the fit found using bootstrap resampling. The GLM fit captured meaningful stimulus-driven variation in the BOLD response, with some variation in fit quality across runs. The median R<sup>2</sup> value across runs was 0.41 for Measurement Set 1 and 0.32 for Measurement Set 2. Fits for the other two subjects are provided as <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s2">2</xref>. Due to the randomized stimulus order within each run, it was not straightforward to determine the degree to which the unmodeled variance was due to stimulus-driven structure not modeled by the GLM (e.g. carry-over effects) as opposed to measurement noise. Overall, the quality of the GLM fits supported using the GLM as a benchmark model, as well as using the GLM beta weights as a measure of the V1 response.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Model fits to the V1 BOLD time course.</title><p>The measured BOLD time course (thin gray line) is shown along with the model fits from the QCM (thick purple line) and GLM (thin orange line) for six runs from Subject 2. Individual runs consisted of only half the total number of chromatic directions. The left column shows data and fits from Measurement Set one and the right column for Measurement Set 2. The three runs presented for each measurement set were chosen to correspond to the highest, median, and lowest QCM R<sup>2</sup> values within the respective measurement set; the ranking of the GLM R<sup>2</sup> values across runs was similar. The R<sup>2</sup> values for the QCM and the GLM are displayed at the top of each panel. The shaded error regions represent the 68% confidence intervals for the GLM obtained using bootstrapping.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Model fits to the V1 BOLD time course from Subject 1.</title><p>The format of the Figure is the same as <xref ref-type="fig" rid="fig3">Figure 3</xref> in the main text. The measured BOLD time course (black line) is shown along with the model outputs from the QCM (thick purple line) and GLM (thin orange line) for six acquisitions. The left column shows data and fits from Measurement Set 1 and the right column for Measurement Set 2. The three acquisitions presented for each measurement set were chosen to correspond to the highest, median, and lowest QCM R<sup>2</sup> values within the respective measurement set. The R<sup>2</sup> values for the QCM and the GLM are displayed at the top of each panel. The shaded regions represent the 68% confidence intervals obtained via the bootstrap analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Model fits to the V1 BOLD time course from Subject 3.</title><p>The format of the figure is the same as <xref ref-type="fig" rid="fig3">Figure 3</xref> in the main text and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> The measured BOLD time course (black line) is shown along with the model outputs from the QCM (thick purple line) and GLM (thin orange line) for six acquisitions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Mean Residuals for the QCM and the GLM.</title><p>Each panel shows the mean model residual for both the QCM (purple) and the GLM (orange) as a function of the chromatic direction. The mean of the model residuals is taken from 4 to 14 TRs after stimulus onset and is averaged for all stimuli within a color direction (collapsing over contrast). The rows show data for each of the three subjects and the columns show the two measument sets.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig3-figsupp3-v2.tif"/></fig></fig-group></sec><sec id="s2-3"><title>Is the QCM a good model of the BOLD response?</title><sec id="s2-3-1"><title>Characterizing cortical responses with the Quadratic Color Model</title><p>The QCM is a parametric special case of the GLM that predicts the BOLD time course using a small number of parameters, and allows for response predictions to modulations in any chromatic direction and contrast in the LM plane. <xref ref-type="fig" rid="fig2">Figure 2</xref> shows the QCM V1 contrast response functions for Subject 2 (the solid lines). The green and purple lines represent fits to Measurement Set 1 and 2, respectively. The shaded region around both lines represent the 68% confidence intervals for the fits obtained using bootstrap resampling. The QCM contrast response functions agree well with the beta weights obtained from the GLM. The QCM contrast response functions increase monotonically with contrast in all chromatic directions, potentially smoothing measurement variability in the GLM beta weights. There was excellent agreement between the fits to both measurement sets for Subject 2. Similar agreement between the QCM and the GLM and between measurement sets was found for the other two subjects (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref>–<xref ref-type="fig" rid="fig2s2">2</xref>).</p><p>We assessed the quality of the QCM fit to the V1 BOLD time course. The purple line in <xref ref-type="fig" rid="fig3">Figure 3</xref> shows the QCM fit to the BOLD time course with the shaded region representing the 68% confidence interval obtained using bootstrapping. The QCM fit of the time course was of similar quality to the GLM fit. Importantly, the QCM fit was based on only six free parameters, compared to the 41 free parameters of the GLM. Similar quality of fits for QCM can be seen for the other two subjects in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s2">2</xref>.</p></sec></sec><sec id="s2-4"><title>Comparison of GLM and QCM</title><p>We used a leave-runs-out cross-validation procedure to compare the GLM and the QCM (see Materials and methods section for details). This cross-validation compares the ability of the models to predict data not used to fit the parameters, accounting for the possibility that more flexible models (such as the GLM) may overfit the data. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows the results of the cross-validation comparison for all subjects. Both models track meaningful variation in the signal, although less so for the data from Subject three in Measurement Set 2. Importantly, we see that the QCM cross-validated R<sup>2</sup> is essentially indistinguishable from the GLM cross-validated R<sup>2</sup>, although in all cases slightly higher.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Cross-validated model comparison for the QCM and the GLM, from the V1 ROI and for all three subjects.</title><p>In each panel, the mean leave-one-out cross-validated R<sup>2</sup> for the QCM (purple bars) and the GLM (orange bars). These values are displayed at the top of each panel. Within each panel, the left group is for Measurement Set 1 and the the right group is for Measurement Set 2.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Cross-validated model comparison for all models, from the V1 ROI.</title><p>The rows show data for each subect and columns are the different measument sets. In each panel, we plot the mean leave-one-out cross-validated R<sup>2</sup> (from left to right) for the General Linear Model, Naka-Rushton, Naka-Rushton common amplitude, Naka-Rushton common exponent, Naka-Rushton common amplitude and exponent, Linear Channels Model with Brouwer and Heeger parameters, Linear Channels Model with Kim et al. parameters, Quadratic Color Model, and Quadratic Color Model with angle locked to 0°. The cross-valided values are also provided numerically in the panels. Details for each model can be found in the main text and Appendix 1.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig4-figsupp1-v2.tif"/></fig></fig-group><p>To further assess differences between the GLM and the QCM, we analyzed the model residuals as a function of the stimulus condition, to check for systemic patterns in the residuals as well as any differences between the two models in this regard. We first examined each direction/contrast pair separately by plotting the residuals of the GLM and QCM over the 14 TRs after the start of each stimulus block. We did not observe any systematic variation in the residuals as a function of contrast level within a single chromatic direction. Therefore, we examined the mean residual value, taken from 4 to 14 TRs after stimulus onset, for all trials in a chromatic direction (collapsed over contrast). We plot these mean residuals for both the GLM and the QCM, as a function of chromatic direction, for each subject and session in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>. From this, we observe no consistent pattern of residuals within or across models. Note that the residual values for the GLM and QCM mostly overlap, despite the GLM having separate parameters for each stimulus direction.</p></sec><sec id="s2-5"><title>QCM generalization</title><p>We also employed a leave-session-out cross-validation procedure to assess the generalizability of the QCM (See Materials and methods for details). Given that Sessions 1 and 2 do not share any common chromatic directions, we were able to evaluate how effectively the QCM generalizes to chromatic directions not used to derive the model parameters. The green contrast response functions shown in <xref ref-type="fig" rid="fig5">Figure 5</xref> result from fitting the QCM to either Session 1 or Session 2, and predicting the responses from the held-out session. The generalization from Session 1 to Session 2 (right-hand subplots) is excellent for this subject. The generalization from Session 2 to Session 1 is also good, albeit with a large confidence interval for the 45° direction. For other subjects and measurement sets, the QCM generalizes reasonably well (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s3">3</xref>). Overall, generalizations from Session 1 to Session 2 perform better than those from Session 2 to Session 1. This finding may reflect the particular set of chromatic directions presented in each session: only Session 1 includes a chromatic direction close to the major axis of the ellipse, which better constrains the QCM fit. Therefore, the QCM is capable of generalizing well to unmeasured chromatic directions, with the requirement that the stimuli include chromatic directions and contrasts that adequately constrain the model parameters.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Leave-sessions-out cross validation.</title><p>The contrast response functions in each panel (green lines) are the result of a leave-sessions-out cross-validation to test the generalizability of the QCM. The QCM was fit to data from four out of the eight tested chromatic directions, either from Session 1 or Session 2. The fits were used to predict the CRFs for the held out four directions. The orange points in each panel are the GLM fits to the full data set. The data shown here are for Subject 2, Measurement Set 1. The shaded green error regions represent the 68% confidence intervals for the QCM prediction obtained using bootstrapping. See <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s3">3</xref> for cross-validation plots from other subjects and measurement sets.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Leave-sessions-out cross validation for Subject 1.</title><p>The format of the Figure is the same as <xref ref-type="fig" rid="fig5">Figure 5</xref> in the main text. The contrast response functions in each panel (green lines) are the result of a leave-sessions-out cross-validation to test the generalizability of the QCM. In both the top and bottom eight panels, the QCM was fit to data from four of the eight tested chromatic directions, either from session 1 or session 2. The fits were used to predict the CRFs for the held-out chromatic directions. The orange points in each panel are the GLM fits to the full data set. The data shown here are for Subject 1 with the top eight panels from Measurement Set 1 and the bottom eight panels from Measurement Set 2. The shaded green error regions represent the 68% confidence intervals for the QCM predictions obtained via the bootstrap analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Leave-sessions-out cross validation for Subject 2.</title><p>The format of the figure is the same as <xref ref-type="fig" rid="fig5">Figure 5</xref> in the main text and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>. The contrast response functions in each panel (green lines) are the result of a leave-sessions-out cross-validation to test the generalizability of the QCM. The orange points in each panel are the GLM fits to the full data set. The data shown here are for Subject 2 Measurement Set 2; Measurement Set 1 can be seen in <xref ref-type="fig" rid="fig5">Figure 5</xref> of the main text. The shaded green error regions represent the 68% confidence intervals for the QCM predictions obtained via the bootstrap analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Leave-sessions-out cross validation for Subject 3.</title><p>The format of the figure is the same as <xref ref-type="fig" rid="fig5">Figure 5</xref> in the main text and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s2">2</xref>. The green lines are the leave-session-out CRF, and the orange points are the GLM fits to the full data set. The data shown here are for Subject 3 with the top eight panels from Measurement Set 1 and the bottom eight panels from Measurement Set 2. The shaded green error regions represent the 68% confidence intervals for the QCM predictions obtained via the bootstrap analysis.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig5-figsupp3-v2.tif"/></fig></fig-group></sec><sec id="s2-6"><title>QCM characterization of V1 BOLD response</title><p>Conceptually, the parameters of the QCM characterize two key model components. The first component defines the contrast-independent shape of elliptical isoresponse contour. This describes the relative sensitivity of V1 to modulations in all chromatic directions within the LM contrast plane. The second component defines the response nonlinearity, which is independent of chromatic direction. It operates on equivalent contrast to produce the underlying neural response.</p></sec><sec id="s2-7"><title>Elliptical isoresponse contours</title><p>The isoresponse contour is described by two parameters: the direction of least sensitivity (ellipse angle; counterclockwise to the positive abscissa) and the ratio of vector lengths between the most and least sensitive directions (minor axis ratio; see Quadratic Color Model Section and Appendix 1). Within the QCM, the angle and minor axis ratio provide a complete description of chromatic sensitivity that is contrast independent.</p><p><xref ref-type="fig" rid="fig6">Figure 6</xref> shows the QCM isoresponse contours for all three subjects and both measurement sets. We found that for all subjects and measurement sets, the angle of the isoresponse contours was oriented at approximately 45°. An ellipse angle of 45° indicates that V1 was least sensitive to stimuli modulated in the L+M direction, and most sensitive to stimuli modulated in the L-M direction. Across all subjects and measurement sets, the minor axis ratio parameters ranged between 0.15 and 0.25. Thus, for the spatial and temporal properties of our modulations, V1 was roughly five times more sensitive to modulations in the L-M direction than the L+M direction. We found good agreement between the isoresponse contours from the independent measurement sets as well as across subjects. </p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>V1 isoresponse contours.</title><p>The normalized elliptical isoresponse contours from the QCM are plotted, for each subject, in the LM contrast plane. The green ellipses show the QCM fits to Measurement Set 1 and the purple ellipses show fits to measurement 2. The angles and minor axis ratios along with their corresponding 68% confidence intervals obtained using bootstrapping are provided in the upper left (Measurement Set 1) and lower right (Measurement Set 2) of each panel.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Isoresponse Contours for the LCM and the QCM.</title><p>Each panel shows the isoresponse contours derived using the LCM with both the <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2009</xref> variant (green) and the <xref ref-type="bibr" rid="bib41">Kim et al., 2020</xref> (dark orange). These are shown along with the elliptical isoresponse contour from the corresponding QCM fit (purple). The rows show the fits to the data from each subject and the columns show the different measurement sets. Note that the isoresponse contour for the <xref ref-type="bibr" rid="bib41">Kim et al., 2020</xref> variant of the LCM, which has more degrees of freedom than the <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2009</xref> variant, closely approximates that of the QCM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig6-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-8"><title>Equivalent contrast nonlinearity</title><p><xref ref-type="fig" rid="fig7">Figure 7</xref> shows the V1 equivalent contrast nonlinearity of the QCM for Subject 2 for both measurement sets. This non-linearity describes how the underlying neuronal response increases with increasing equivalent contrast. We used the isoresponse contour of the QCM to convert the chromatic direction and cone contrast of each stimulus to its equivalent contrast. This allowed us to replot each beta weight derived from the GLM (<xref ref-type="fig" rid="fig2">Figure 2</xref>) on an equivalent contrast axis (<xref ref-type="fig" rid="fig7">Figure 7</xref>; closed circles). For all subjects, the single nonlinearity accurately captured the dependence of the GLM beta weight on equivalent contrast, with no apparent bias across chromatic directions. The agreement between the GLM beta weight points and QCM fits demonstrated that separating the effects of chromatic direction and contrast in the QCM is reasonable. <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref> provides the same plots for Subjects 1 and 3. </p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Equivalent contrast non-linearities of the QCM for V1 from Subject 2.</title><p>The x-axis of each panel marks the equivalent contrast and the y-axis is the neuronal response. The gray curve in each panel is the Naka-Rushton function obtained using the QCM fit. These curves show the relationship between equivalent contrast and response. The parameters of the Naka-Rushton function are reported in upper left of each panel along with the 68% confidence intervals obtained using bootstrapping. The points in each panel are the GLM beta weights mapped via the QCM isoresponse contours of Subject 2 onto the equivalent contrast axis (see Appendix 1). The color of each point denotes the chromatic direction of the stimuli, as shown in the color bar. The left panel is for Measurement Set 1 and the right panel is for Measurement Set 2. Note that our maximum contrast stimuli do not produce a saturated response. Note that our stimuli did not drive the response into the saturated regime.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Equivalent Contrast Non-Linearities of the QCM for V1.</title><p>The format is the same as <xref ref-type="fig" rid="fig7">Figure 7</xref> in the main text. The x-axis of each panel marks the equivalent contrast and the y-axis is the response. The gray curve in each panel is the Naka-Rushton function obtained using the QCM fit. The upper two panel are from Subject 1 and the bottom 2 panels are from Subject 3. The left panels are for Measurement Set 1 and the right panels are for Measurement Set 2. The parameters of the Naka-Rushton function are reported in upper left of each panel along with the 68% confidence intervals obtained via the bootstrap analysis. The points in each panel are the GLM beta weights of the respective measurement set mapped via the QCM isoresponse contours onto the equivalent contrast axis (see Materials and methods). The color of each point denotes the chromatic direction of the stimuli, as shown in the color bar.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig7-figsupp1-v2.tif"/></fig></fig-group></sec><sec id="s2-9"><title>Dependence of chromatic sensitivity on eccentricity </title><sec id="s2-9-1"><title>Isoresponse contour parameter maps</title><p>There is considerable interest in how sensitivity to modulations in the LM contrast plane varies with eccentricity. Understanding such variation is important both for describing visual performance and for drawing inferences regarding the neural circuitry that mediates color vision. Since the QCM separates chromatic sensitivity from the dependence of the response on contrast, examining how the shape of the QCM isoresponse contour varies with eccentricity addresses this question in a contrast-independent manner. We fit the QCM to the BOLD time course of each vertex in the template map of visual areas developed by <xref ref-type="bibr" rid="bib6">Benson et al., 2014</xref>. This allowed us to visualize how the parameters that describe the isoresponse contour varied with eccentricity within V1.</p><p><xref ref-type="fig" rid="fig8">Figure 8</xref> shows the QCM parameter maps for the ellipse angle, the minor axis ratio, and the variance explained displayed on the cortical surface. Here, the data were averaged across all subjects and measurement sets. In all panels, the full extent of V1 is denoted by the black outline on the cortical surface, while the 20° eccentricity ROI used in the V1 analyses above is shown by the black dashed line. Apparent in the maps is that neither parameter varied systematically within V1, a feature of the data that is consistent across measurement sets and subjects. Outside of V1, the R<sup>2</sup> values were markedly lower, and there was higher variability in the QCM parameters.</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>QCM average parameter maps.</title><p>The QCM parameters, fit at all vertices within the visual cortex mask, averaged across all subjects and measurement sets. The top, middle, and bottom rows show maps of the average ellipse angle, minor axis ratio, and variance explained, respectively. The scale of the corresponding color map is presented below each row. The nomenclature in upper left of each surface view indicates the hemisphere (L: left or R: right) and the view (I: inferior, L: lateral, or M: medial). The medial views show the full extent of the V1 ROI on the cortical surface (denoted by the solid black outline). The 20° eccentricity boundary used to define the V1 ROI used for all analyses is shown by the black dashed line.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig8-v2.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Average R<sup>2</sup> map for the GLM for early visual cortex.</title><p>The format is the same as the bottom row of <xref ref-type="fig" rid="fig8">Figure 8</xref> in the main text. This show the R<sup>2</sup> values for the GLM fit to each vertex within the EVC mask averaged across all subjects and sessions. The color bar provides the GLM R<sup>2</sup> scale.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig8-figsupp1-v2.tif"/></fig></fig-group><p>We further examined the variance explained by the GLM, fit to every vertex on the cortical surface. Within early visual cortex (EVC, the spatial extent of the Benson template), we did not observe differences in R<sup>2</sup> larger than 0.03 in non-cross-validated model fits between the GLM and the QCM (GLM – QCM). We generally found that the variance explained by the GLM in vertices outside of V1 was close to zero, with the exception of a small patch of values in the vicinity of hV4/VO1. The GLM variance explained in this area was roughly half of that explained within V1. To more fully characterize these regions, we fit the QCM to the median time course from the subject specific registrations of hV4 and VO1 as defined by the retinotopic atlas from <xref ref-type="bibr" rid="bib79">Wang et al., 2015</xref> (implemented in Neuropythy). The parameters of the QCM fit for hV4 and VO1 were generally consistent with those found for V1, although fit quality was worse. Overall, as our spatially uniform stimuli were not highly effective at eliciting reliable responses outside of V1, we refrain from drawing definitive conclusions about responses outside of V1. The average variance explained map within EVC for the GLM is shown in <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>.</p></sec></sec><sec id="s2-10"><title>No change in V1 chromatic sensitivity with eccentricity</title><p>We leveraged the QCM to examine how chromatic sensitivity varies with eccentricity within V1. <xref ref-type="fig" rid="fig9">Figure 9</xref> plots the V1 QCM parameters as a function of eccentricity, for Subject 2. The left panel shows the minor axis ratio and the right panel shows the ellipse angle. In both plots, individual points represent a single vertex, with the x-axis giving the visual field eccentricity of that vertex obtained from the <xref ref-type="bibr" rid="bib6">Benson et al., 2014</xref> template, and the y-axis giving the parameter value. The transparency of each point indicates the R<sup>2</sup> value of the QCM fit for the corresponding vertex. The maximum R<sup>2</sup> value across vertices for Measurement Sets 1 and 2 were 0.25 and 0.24, respectively. The lines in each panel reflect a robust regression fit to the points. We found that there is little change in either parameter with eccentricity. For Subject 2, the best fit lines had slightly negative slopes for the minor axis ratio and slightly positive slopes for the ellipse angle, with good agreement across measurement sets. The overall change in parameter values from 0° to 20°, however, was small compared to the vertical spread of values at each eccentricity. We compared the change in parameter values from 0° to 20° to the variability across measurement sets for all three subjects (<xref ref-type="table" rid="table1">Table 1</xref> for minor axis ratio, and <xref ref-type="table" rid="table2">Table 2</xref> for ellipse angle). Across subjects, the majority of sessions showed small differences in parameter values from 0° to 20°, but we note that these did in some cases exceed the measurement set-to-set difference in the parameter values obtained for all of V1.</p><fig-group><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>QCM parameters as a function of eccentricity for Subject 2.</title><p>The left and right panels show scatter plots of the minor axis ratio and ellipse angle plotted against their visual field eccentricity, respectively. Each point in the scatter plot shows a parameter value and corresponding eccentricity from an individual vertex. Green indicates Measurement Set 1 and purple indicates Measurement Set 2. The lines in each panel are robust regression obtained for each measurement set separately. The transparency of each point provides the R<sup>2</sup> value of the QCM at that vertex. The color bars provide the R<sup>2</sup> scale for each measurement set.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig9-v2.tif"/></fig><fig id="fig9s1" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 1.</label><caption><title>L+M and L-M responses predicted using QCM as a function of eccentricity.</title><p>The top row shows responses for Subject 1, the middle row show responses for Subject 2, and the bottom row shows responses for Subject 3. The left and right panels show data from Measurement Set 1 and Measurement Set 2, respectively. Each panel shows scatter plots of the QCM response for stimuli chosen in the L+M (gray) and L-M(red) directions plotted against their visual field eccentricity, respectively. The L+M stimulus was chosen at 6% contrast and the L-M stimulus was chosen at 30% contrast (corresponding to the 50% contrast condition). Note that a small percentage of vertices had poor parameter fits. Due to this, the predicted response amplitudes were orders of magnitude larger than what is currently displayed. These points have been excluded from the plot.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig9-figsupp1-v2.tif"/></fig><fig id="fig9s2" position="float" specific-use="child-fig"><label>Figure 9—figure supplement 2.</label><caption><title>QCM parameters as a function of eccentricity.</title><p>The format is the same as <xref ref-type="fig" rid="fig9">Figure 9</xref> in the main text. The top row shows parameter fits from Subject 1 and the bottom row shows fits from Subject 3. The left and right panels show scatter plots of the minor axis ratio and ellipse angle plotted against their visual field eccentricity, respectively. Each point in the scatter plot shows a parameter value and corresponding eccentricity from an individual vertex. Teal indicates Measurement Set 1 and purple indicates Measurement Set 2. The lines in each panel are robust regression obtained for each measurement set separately. The transparency of each point provides the R<sup>2</sup> value of the QCM at that vertex. The color bars provide the R<sup>2</sup> scale for each measurement set.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig9-figsupp2-v2.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Robust regression line parameters summarizing the change in minor axis ratio with eccentricity for all subjects.</title><p>These parameters are the same as seen for Subject 2 in <xref ref-type="fig" rid="fig9">Figure 9</xref>. The subject and set columns indicate the subject and measurement set of the robust regression fit. The slope and offset column show the parameters of the regression line. The ∆ 0° to 20° column is the magnitude of the change in minor axis ratio between 0° and 20° eccentricity. The ∆ Set to Set column shows the absolute difference in the minor axis ratio fit to the V1 median time course between Measurement Set 1 and 2. </p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Subject</th><th valign="top">Set</th><th valign="top">Slope</th><th valign="top">Offset</th><th valign="top">∆ 0° to 20°</th><th valign="top">∆ Set to Set</th></tr></thead><tbody><tr><td valign="top">S1</td><td valign="top">1</td><td valign="top">−1.19e-3</td><td valign="top">0.163</td><td valign="top">0.0238</td><td rowspan="2" valign="top">0.09</td></tr><tr><td valign="top">S1</td><td valign="top">2</td><td valign="top">−4.17e-4</td><td valign="top">0.24</td><td valign="top">0.0084</td></tr><tr><td valign="top">S2</td><td valign="top">1</td><td valign="top">−7.54e-6</td><td valign="top">0.154</td><td valign="top">0.0002</td><td rowspan="2" valign="top">0.00</td></tr><tr><td valign="top">S2</td><td valign="top">2</td><td valign="top">−2.51e-3</td><td valign="top">0.183</td><td valign="top">0.0504</td></tr><tr><td valign="top">S3</td><td valign="top">1</td><td valign="top">−3.27e-3</td><td valign="top">0.114</td><td valign="top">0.0654</td><td rowspan="2" valign="top">0.03</td></tr><tr><td valign="top">S3</td><td valign="top">2</td><td valign="top">−3.9e-4</td><td valign="top">0.158</td><td valign="top">0.0078</td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Robust regression line parameters summarizing the change in ellipse angle with eccentricity for all subjects.</title><p>Columns are formatted the same as <xref ref-type="table" rid="table1">Table 1</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Subject</th><th valign="top">Set</th><th valign="top">Slope</th><th valign="top">Offset</th><th valign="top">∆ 0° to 20°</th><th valign="top">∆ Set to Set</th></tr></thead><tbody><tr><td valign="top">S1</td><td valign="top">1</td><td valign="top">0.039</td><td valign="top">46.2</td><td valign="top">0.78</td><td rowspan="2" valign="top">2.61</td></tr><tr><td valign="top">S1</td><td valign="top">2</td><td valign="top">0.313</td><td valign="top">41.4</td><td valign="top">6.26</td></tr><tr><td valign="top">S2</td><td valign="top">1</td><td valign="top">0.496</td><td valign="top">39.5</td><td valign="top">9.92</td><td rowspan="2" valign="top">3.76</td></tr><tr><td valign="top">S2</td><td valign="top">2</td><td valign="top">0.330</td><td valign="top">45.1</td><td valign="top">6.60</td></tr><tr><td valign="top">S3</td><td valign="top">1</td><td valign="top">0.247</td><td valign="top">39.3</td><td valign="top">4.94</td><td rowspan="2" valign="top">5.62</td></tr><tr><td valign="top">S3</td><td valign="top">2</td><td valign="top">0.425</td><td valign="top">43.3</td><td valign="top">8.50</td></tr></tbody></table></table-wrap><p>The plots shown in <xref ref-type="fig" rid="fig9">Figure 9</xref> examine how the QCM parameters vary with eccentricity. To allow comparison with prior studies of how the BOLD response varies with eccentricity within V1 (<xref ref-type="bibr" rid="bib76">Vanni et al., 2006</xref>; <xref ref-type="bibr" rid="bib53">Mullen et al., 2007</xref>; <xref ref-type="bibr" rid="bib16">D'Souza et al., 2016</xref>), we also used the QCM to predict how the response would vary for stimuli in the L+M and the L-M directions, and plot these predicted responses as a function of eccentricity (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>). This was done on a vertex-by-vertex basis, using within-vertex QCM parameters. Specifically, we chose the 50% contrast stimulus condition for both the L-M and the L+M direction (contrasts of 0.06 and 0.30 respectively). Using these stimuli, we computed the predicted neuronal response by applying the QCM forward model (the transformation to equivalent contrast and the Naka-Rushton steps) using the parameter values corresponding to that particular vertex. Examining the data in this way reveals a negligible change in response as a function of eccentricity for both the L+M and L-M directions, for all subjects and measurement sets. We return in the discussion to consider the relation between our results and those found in prior studies.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We develop a quantitative model of the visual cortex response to chromatic stimuli in the LM contrast plane, the quadratic color model (QCM), and examine its ability to fit V1 BOLD fMRI responses to spatially uniform chromatic stimuli. We find that the QCM accounts for the same cross-validated variance as a conventional GLM, with far fewer free parameters (6 as compared to 41). The model generalizes across both chromatic direction and contrast to predict V1 responses to a set of stimuli that were not used to fit the model parameters. The experiment was replicated for each subject using the same stimuli across separate measurement sets. Both the data and the model fits replicate well for each subject and are similar across subjects, giving us confidence in the power of the measurements.</p><p>The QCM is a separable model with respect to the effects of chromatic direction and contrast. This allowed us to evaluate the chromatic sensitivity in V1 of our subjects in a manner that is independent of the effects of contrast. We find that V1 is most sensitive to L-M contrast modulations and least sensitive to L+M contrast modulations, when contrast is defined using vector length in the LM contrast plane. This was shown in all subjects and measurement sets by the isoresponse contours of each subject being oriented at approximately 45° and having a minor axis that is roughly five times smaller than the major axis. This result is broadly consistent with previous fMRI studies of V1 chromatic contrast sensitivity, although the exact sensitivity ratio varies with the spatial and temporal properties of the stimuli (<xref ref-type="bibr" rid="bib20">Engel et al., 1997</xref>; <xref ref-type="bibr" rid="bib49">Liu and Wandell, 2005</xref>; <xref ref-type="bibr" rid="bib53">Mullen et al., 2007</xref>; <xref ref-type="bibr" rid="bib16">D'Souza et al., 2016</xref>; <xref ref-type="bibr" rid="bib54">Mullen et al., 2010a</xref>). By considering cortical responses in terms of the parameters of the QCM fit, we are able to provide a quantitative account of chromatic sensitivity, as opposed to a categorical assignment of voxels as ‘color’ or ‘luminance’ responsive.</p><p>The QCM also allows us to examine the equivalent contrast response nonlinearity, although doing so is not the focus of this paper. This non-linearity depends on chromatic direction only through a direction-dependent contrast gain that is captured by the isoresponse contour. This can be observed in <xref ref-type="fig" rid="fig7">Figure 7</xref> through the overlap of the non-linearity and the transformed GLM beta weights. Although we used the Naka-Rushton function to fit the nonlinearity, this was a choice of convenience and the precise shape of the non-linearity is not strongly constrained by our data set. This is because our stimuli did not drive the response into the saturating regime (<xref ref-type="fig" rid="fig7">Figure 7</xref>, <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>). A stronger test of the contrast/direction separability embodied by the QCM, as well as stronger constraints on the shape of the non-linearity, would be provided by stimuli that drive the V1 response to saturation.</p><p>Neither the QCM nor the GLM explain all of the variance in the data. Since our experimental design did not involve multiple measurements with the same stimulus sequence (stimulus sequences were randomized across runs and measurement sets), we cannot untangle the degree to which the unexplained variance is due to systematic but unmodeled aspects of the response or to measurement noise. In comparing our reported R<sup>2</sup> values to those in other studies, it is important to bear in mind that R<sup>2</sup> values are expected to be higher in cases where the signal being fit is the average time course over multiple runs with the same stimulus sequence, as compared to when the R<sup>2</sup> values are computed with respect to fits to individual runs, even if there is no difference in the quality of the underlying response model. Another factor that can affect R<sup>2</sup> is un-modeled physiological effects on the BOLD signal due to blinking, breathing, heart beats, etc. We did not collect eye tracking measurements or pulse oximetry, so we cannot model such effects.</p><p>In certain cases, attentional task difficulty can modulate BOLD responses (<xref ref-type="bibr" rid="bib40">Kay and Yeatman, 2017</xref>). We employed only one level of attentional task difficulty and thus do not have data on how varying the attentional task might affect the responses we measured. We do not, however, have any particular reason to think that the chromatic tuning and contrast response functions we measured would have been substantially different in the context of different attentional task difficulty. In this regard, we note that <xref ref-type="bibr" rid="bib75">Tregillus et al., 2021</xref> measured contrast response functions for L-M and S-(L+M) color directions within V1 under two different attentional tasks and found no significant effect of task on the two contrast response functions.</p><sec id="s3-1"><title>Relation to psychophysics</title><p>A goal of systems neuroscience is to link measurements of neuronal properties to measurements of behavior. To make these links, the measurements made in each domain must be placed into a common space for comparison. The QCM provides a way to represent fMRI measurements in a manner that makes such comparisons straightforward. The contrast-invariant isoresponse contour from the QCM provides us with a stimulus-referred characterization of the BOLD fMRI response. Other methodologies, such as psychophysics or electrophysiology, may be used to obtain similar characterizations, allowing for comparisons across response measures within this common framework. For example, an approach to studying chromatic sensitivity is to characterize the isothreshold contour, which specifies the set of stimulus modulations that are equally detectable. <xref ref-type="bibr" rid="bib20">Engel et al., 1997</xref> took this approach and found that for low temporal frequencies the psychophysical isothreshold and BOLD fMRI isoresponse contours in the LM contrast plane were well-described as ellipses and had similar shapes. While some work has argued that psychophysical isothreshold may deviate subtly from ellipses (for review see <xref ref-type="bibr" rid="bib72">Stockman and Brainard, 2010</xref>), two studies that attempted to reject the elliptical form of such contours did not do so (<xref ref-type="bibr" rid="bib59">Poirson et al., 1990</xref>; <xref ref-type="bibr" rid="bib42">Knoblauch and Maloney, 1996</xref>). Consistent with Engel, Zhang, and Wandell (1997), we found elliptical BOLD isoresponse contours at our 12 Hz temporal frequency with highest sensitivity in the L-M direction. As they note, although psychophysical isothreshold contours remain well-described by ellipses at higher temporal frequencies, their orientation changes to favor L+M sensitivity over L-M sensitivity. This dissociation in the particulars of the isothreshold and BOLD isoresponse contours makes it unlikely that the mechanisms that contribute to the BOLD response in V1 limit psychophysical detection at the higher temporal frequencies, unless there are important temporal-frequency dependent changes in response variability that are not captured by the BOLD measurements.</p></sec><sec id="s3-2"><title>Relation to underlying mechanisms</title><p>Many theories of color vision postulate that signals from the L-, M-, and S-cone photoreceptors are combined to form three post-receptoral mechanisms, roughly characterized as an additive combination of L- and M-cone contrast (L+M), an opponent combination of L- and M-cone contrast (L-M), and an opponent combination of S-cone contrast with L- and M-cone contrasts (S-(L+M)) (<xref ref-type="bibr" rid="bib72">Stockman and Brainard, 2010</xref>; <xref ref-type="bibr" rid="bib66">Shevell and Martin, 2017</xref>). Our finding that the major and minor axes of the isoresponse ellipse are well-aligned with the L+M and L-M modulation directions agrees with such theories. More generally, a quadratic isorepsonse contour can be produced by a quadratic mechanism that computes a sum of the squared responses of two underlying linear mechanisms, where the output of each linear mechanism is a weighted sum of L- and M-cone contrasts (<xref ref-type="bibr" rid="bib59">Poirson et al., 1990</xref>). If the two linear mechanisms are L+M and L-M mechanisms with the weights appropriately chosen to represent the relative sensitivities (L-M sensitivity greater than L+M sensitivity), then the isoresponse contour of the resulting quadratic mechanism will be a close match to those we measured.</p><p>Note, however, that other pairs of underlying mechanisms are also consistent with the same elliptical isoresponse contours (<xref ref-type="bibr" rid="bib59">Poirson et al., 1990</xref>), so that our isoresponse contours do not uniquely determine the sensitivity of the underlying linear mechanisms, even within the QCM together with the assumption that there are two such mechanisms.</p><p>More generally, one can construct non-quadratic models whose isoresponse contours approximate the ellipse we found using the QCM, and if this approximation is good our data will not reject such models. To illustrate this point, we developed and fit an alternate model, the Linear Channels Model (LCM), a variation on the Brouwer and Heeger channel model (<xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2009</xref>; <xref ref-type="bibr" rid="bib41">Kim et al., 2020</xref>), that accounts for our data about as well as the QCM (see Appendix 1; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The best fitting isoresponse contours found with the LCM, which could in principle deviate considerably from an ellipse, none-the-less approximate the ellipse we found using the QCM, but are not perfectly elliptical (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). Despite the agreement at the functional level of the isoresponse contours, the properties of the mechanisms underlying the LCM differ from those of the QCM, and these properties also differ across different instantiations of the LCM that account for the data equally well (see Appendix 1).</p><p>Because of the similarity in cross-validated R<sup>2</sup> values across the GLM, Naka-Rushton, QCM and LCM models, the reader may wonder whether the data have sufficient power to reject any isoresponse contour shape. To address this, we fit and cross-validated a form of the QCM with the angle constrained to 0 degrees. This resulted in a noticeably lower cross-validated R<sup>2</sup> for this model as compared to all other models we tested (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>, labeled at ‘QCM locked’) and provides reassurance that the data indeed have power to inform as to the shape of the isoresponse contour. We expect that other isoresponse contour shapes that differ from the best fitting QCM contour to a degree similar to that of the constrained ellipse would also be rejected.</p><p>Thus, while measurements of the BOLD response place constraints on the population response properties of the neuronal mechanisms, these properties are not uniquely determined given the BOLD response alone. The ambiguity is further increased if we consider properties of individual neurons, as the aggregate BOLD response will be shaped both by the response properties of such neurons and the numbers of different types of neurons in the overall neural population. With that caveat, we make some general observations. Our stimuli were large, spatially uniform (effectively 0 c.p.d.) chromatic modulations that were temporally modulated at 12 Hz. These stimuli could drive ‘color’, ‘luminance’ and ‘color-luminance’ cells as described by <xref ref-type="bibr" rid="bib36">Johnson et al., 2001</xref>, depending on the particular chromatic direction. The 0 c.p.d. stimuli would presumably produce strong responses in the ‘color’ cells for our L-M direction, given that these cells are thought to behave as low-pass filters in the spatial domain and have unoriented receptive fields. It is less clear how strongly ‘luminance’ and ‘color-luminance’ cells would respond to our spatially uniform stimuli given that these cells are spatially bandpass. <xref ref-type="bibr" rid="bib64">Schluppeck and Engel, 2002</xref> plot the spatial frequency response function estimated from the data from <xref ref-type="bibr" rid="bib36">Johnson et al., 2001</xref>. These functions plot the average firing rate as a function of spatial frequency for the color, color-luminance, and luminance cells. Taking the lowest spatial frequency present in the dataset (0.1 c.p.d.), firing rates for color cells are roughly 5x times higher than the firing rates for luminance and color-luminance cells. This is the same as our average minor axis ratio which indicates V1 is roughly five times more sensitive to L-M than to L+M. A caveat here is that the 12 Hz flicker rate of our stimuli might shift responses relative to the analysis of <xref ref-type="bibr" rid="bib64">Schluppeck and Engel, 2002</xref>.</p><p>It is also important to note that our data do not distinguish the extent to which the response properties of the BOLD signals we measure in V1 are inherited from the LGN or are shaped by processing within V1. The spatiotemporal properties of our stimuli would robustly drive cells in cell in the LGN that project to V1 (<xref ref-type="bibr" rid="bib45">Lankheet et al., 1998</xref>). Even though the signals measured in our experiment are spatially localized to V1, we cannot ascribe the observed response properties to particular neural processing sites. As such, the V1 sensitivities found from fitting the QCM may be inherited from areas prior to V1. In principle, they could also be affected by feedback from other cortical areas. We also undertook an analysis of data from the LGN, but found that these signals were too noisy to reveal reliable stimulus-driven responses in our data.</p><p>Finally, it is interesting to observe that quadratic models have been used to characterize the isoresponse properties of individual neurons in macaque V1 (<xref ref-type="bibr" rid="bib32">Horwitz and Hass, 2012</xref>). Roughly half of the neurons tested in this paper were best fit by quadratic isoresponse surfaces (in the L-, M-, and S-cone contrast space) while the other half were well fit by a linear model whose isoresponse surfaces were parallel planes. Of the quadratic isoresponse surfaces, some were ellipsoidal while others were hyperbolic. As noted above, despite this qualitative similarity, connecting the diverse population of individual neural responses to the aggregated BOLD fMRI response remains a challenge for future work. The QCM fit to our data aids in this endeavor through the constraint its isoresponse contour places on the aggregated neural response.</p></sec><sec id="s3-3"><title>Change of chromatic sensitivity with retinal eccentricity</title><p>Many aspects of visual function change with eccentricity (<xref ref-type="bibr" rid="bib62">Rosenholtz, 2016</xref>), and understanding and quantifying this variation is a key part of a functional characterization of vision. In addition, prior work attempts to relate such functional variation with eccentricity to variation in the underlying neural mechanisms. Relevant to the present work is the idea that variation of chromatic sensitivity with eccentricity can inform as to how signals from separate cone classes are combined by retinal and cortical neural circuitry (<xref ref-type="bibr" rid="bib47">Lennie et al., 1991</xref>; <xref ref-type="bibr" rid="bib56">Mullen and Kingdom, 1996</xref>; <xref ref-type="bibr" rid="bib82">Wool et al., 2018</xref>; <xref ref-type="bibr" rid="bib4">Baseler and Sutter, 1997</xref>). In this context, we examined how the parameters of the QCM for individual vertices varied with eccentricity across V1. Overall, we find little change in the isoresponse contours with eccentricity within the central 20° of the visual field (<xref ref-type="fig" rid="fig9">Figure 9</xref>; <xref ref-type="table" rid="table1">Tables 1</xref> and <xref ref-type="table" rid="table2">2</xref>; <xref ref-type="fig" rid="fig9s2">Figure 9—figure supplement 2</xref>), and an analysis of how predicted response to L+M and L-M modulations would change with eccentricity also shows little or no effect (<xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref>). In addition, we do not observe any clear change in the parameters of the contrast-response function with eccentricity (analysis not shown). Overall, the BOLD response to chromatic modulations, as evaluated in our data using the QCM, is remarkably stable across V1. That the orientation of the elliptical isoresponse contours does not change with eccentricity is consistent with psychophysical studies that show that the relative L- and M-cone inputs to an L-M mechanism are stable across the visual field (<xref ref-type="bibr" rid="bib58">Newton and Eskew, 2003</xref>; <xref ref-type="bibr" rid="bib63">Sakurai and Mullen, 2006</xref>). The fact that our data do not show a loss in L-M sensitivity relative L+M sensitivity with increasing eccentricity, on the other hand, is not commensurate with psychophysical studies that do show such a loss (<xref ref-type="bibr" rid="bib73">Stromeyer et al., 1992</xref>; <xref ref-type="bibr" rid="bib57">Mullen and Kingdom, 2002</xref>; <xref ref-type="bibr" rid="bib52">Mullen et al., 2005</xref>; <xref ref-type="bibr" rid="bib30">Hansen et al., 2009</xref>). As discussed above, BOLD fMRI sensitivity in V1 does not always mirror psychophysical sensitivity. Thus we focus below on comparison between our results and other fMRI studies of how sensitivity in V1 varies with eccentricity.</p><p>Several prior studies have used BOLD fMRI to examine how visual cortex responses vary with eccentricity to stimuli modulated in L-M and L+M directions (<xref ref-type="bibr" rid="bib76">Vanni et al., 2006</xref>; <xref ref-type="bibr" rid="bib53">Mullen et al., 2007</xref>; <xref ref-type="bibr" rid="bib16">D'Souza et al., 2016</xref>). Both <xref ref-type="bibr" rid="bib53">Mullen et al., 2007</xref> and <xref ref-type="bibr" rid="bib76">Vanni et al., 2006</xref> report a decrease in the V1 response to L-M modulations with eccentricity, while the response to L+M remains roughly constant. This differs from our result, and the size of the effects in these papers are large enough that we would expect that if they were present in our data they would be visible in the analysis shown in <xref ref-type="fig" rid="fig9s1">Figure 9—figure supplement 1</xref> (see Figure 8 in <xref ref-type="bibr" rid="bib53">Mullen et al., 2007</xref> and Figure 8 in <xref ref-type="bibr" rid="bib76">Vanni et al., 2006</xref>). Both speculate that the mechanism underlying this observation is non-selective (random) connections between L and M cones and retinal ganglion cell receptive fields. If these connections are non-selective, L-M sensitivity would be expected to decrease with eccentricity. This is because the area in which receptive fields pool cone inputs increases with distance from the fovea, progressively reducing the likelihood that random L- and M-cone inputs to the center and surround will produce chromatic opponency (<xref ref-type="bibr" rid="bib47">Lennie et al., 1991</xref>; <xref ref-type="bibr" rid="bib56">Mullen and Kingdom, 1996</xref>; <xref ref-type="bibr" rid="bib82">Wool et al., 2018</xref>). In contrast, <xref ref-type="bibr" rid="bib16">D'Souza et al., 2016</xref> find, for the majority of spatial frequencies studied, no change in L-M response relative to the response to isochromatic luminance modulations. This result is generally in line with our data, although at their lowest spatial frequencies D’Souza and colleagues observe a modest decline in relative L-M sensitivity. Following the same line of reasoning as <xref ref-type="bibr" rid="bib53">Mullen et al., 2007</xref> and <xref ref-type="bibr" rid="bib76">Vanni et al., 2006</xref> but reaching the opposite conclusion, <xref ref-type="bibr" rid="bib16">D'Souza et al., 2016</xref> take their result as supporting the idea that connections between cones and some classes of ganglion cells are selective for cone type and preserve chromatic sensitivity across the retina.</p><p>Comparison across our and the prior studies is complicated by variation in the stimuli used. Indeed, a dependency on spatial frequency is indicated by the data of <xref ref-type="bibr" rid="bib16">D'Souza et al., 2016</xref>. We used spatially uniform fields, while other studies use stimuli with higher-spatial frequency content. More generally, other factors could also lead to variation across studies, as well as complicate inferring the properties of retinal wiring from how psychophysical thresholds or measurements of cortical response vary with eccentricity.</p><p>One such factor is the changes in cone spectral sensitivity with eccentricity, caused primarily by variation in macular pigment and photopigment optical density. Variation in macular pigment and photopigment optical density can produce eccentricity-dependent deviations in the degree of actual cone contrast reaching the photoreceptors. Prior studies do not account for this variation, leading to the possibility that effects of eccentricity on sensitivity are due to receptoral, rather than post-receptoral mechanisms. In our study, we designed spectral modulations that produce the same contrasts in both 2-degree and 15-degree cone fundamentals (see Materials and methods and <xref ref-type="table" rid="table5">Tables 5</xref>–<xref ref-type="table" rid="table7">7</xref>). This reduces the change in cone contrast with eccentricity for our stimuli.</p><p>Another factor, not emphasized in previous work, is that the size of the effects will depend on where on the underlying contrast response functions the responses to the stimuli in the chromatic directions being compared lie. To understand this issue, consider the example in which the response for one chromatic direction is well into the saturated regime of the contrast-response function, while the other direction is not. This could lead to artifacts in the measured ratio of the response to the two directions with eccentricity, where any change in response for the saturated direction is hidden by a ceiling effect. Our study minimizes the role of this factor through measurement and modeling of the contrast response functions in each chromatic direction, allowing the QCM to extract a contrast independent shape for the isoresponse contours.</p><p>Although changes in sensitivity with eccentricity can be caused by mechanisms at many levels of the visual system, the lack of such variation in our data is parsimoniously explained by retinal output that preserves chromatic sensitivity with eccentricity. This interpretation is challenged by studies that show random (<xref ref-type="bibr" rid="bib82">Wool et al., 2018</xref>) or close to random (<xref ref-type="bibr" rid="bib22">Field et al., 2010</xref>) inputs from L- and M-cones to midget ganglion cell centers in the retinal periphery. Not all studies of midget cell chromatic responses or their parvocelluar LGN counterparts agree with non-selective wiring (<xref ref-type="bibr" rid="bib61">Reid and Shapley, 1992</xref>; <xref ref-type="bibr" rid="bib50">Martin et al., 2001</xref>; <xref ref-type="bibr" rid="bib51">Martin et al., 2011</xref>; <xref ref-type="bibr" rid="bib46">Lee et al., 2012</xref>). One possible cortical mechanism that could compensate for the reduced L-M signal-to-noise ratio with eccentricity as predicted by random wiring models is supra-threshold compensation for reduced signals. Mechanisms of this sort have been postulated in the domain of contrast perception (<xref ref-type="bibr" rid="bib26">Georgeson and Sullivan, 1975</xref>) and anomalous trichromatic vision (<xref ref-type="bibr" rid="bib8">Boehm et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Tregillus et al., 2021</xref>). Another possibility is a differential change in stimulus integration area across chromatic directions and with eccentricity. However, this latter possibility is not supported by fMRI population receptive field (pRF) measurements for modulations in different chromatic directions in V1 (<xref ref-type="bibr" rid="bib80">Welbourne et al., 2018</xref>).</p></sec><sec id="s3-4"><title>Generalizing the QCM</title><p>In our study, we only measured responses to stimuli confined to the LM contrast plane. A more general account of chromatic contrast sensitivity requires modulating stimuli in all three-dimension of the full L-, M-, and S-cone contrast space. The QCM may be generalized in a straightforward manner to handle this expanded stimulus set by replacing the elliptical isoresponse contours with ellipsoidal isoresponse surfaces, but we have yet to test this generalization.</p><p>Another way in which the QCM may be generalized, even within the LM contrast plane, is to consider modulations at other temporal frequencies. In our experiment, we fixed the temporal modulation of the stimulus at 12 Hz. The QCM could be readily fit to data from modulations at various other temporal frequencies with the goal of observing how the chromatic sensitivity changes. With this, one could further examine the BOLD response to mixtures of different temporal frequency modulations. This is particularly interesting in that any arbitrary complex temporal modulation can be decomposed into an additive mixture of modulations at different temporal frequencies and phases (<xref ref-type="bibr" rid="bib9">Bracewell, 1978</xref>). If the response to temporal frequency mixtures can be predicted via a simple rule of combination (such as linearity), establishing the QCM parameters for a well-chosen set of temporal frequencies would enable prediction of the BOLD response to chromatic stimuli modulated with complex temporal sequences.</p><p>Just as we had a fixed temporal frequency in our experiment, the stimulus presented also had a fixed spatial frequency (0 c.p.d.). Similar to the temporal domain, complex spatial images can be broken down into a combination of oriented two-dimensional sine wave patterns at single spatial frequencies and phases. Examining how the QCM fits change with changing spatial frequencies might allow for models of the BOLD response to arbitrary complex spatial stimuli. Consistent with this general goal, recent work has developed quantitative forward models of the BOLD response of early visual cortex to a variety of achromatic modulations with different spatial patterns (<xref ref-type="bibr" rid="bib38">Kay et al., 2013a</xref>; <xref ref-type="bibr" rid="bib39">Kay et al., 2013b</xref>). These models have sequential stages of processing that operate on an input image and transform it into a model of the BOLD response. How such models should be generalized to handle chromatic modulations is not known. If the QCM holds for other spatial frequencies, such a result would place important constraints on the appropriate generalization for such forward models to incorporate color.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>Three subjects (age 23, 25, and 26 years; two female) took part in the fMRI experiment. All subjects had normal or corrected to normal acuity and normal color vision. The research was approved by the University of Pennsylvania Institutional Review Board. All subjects gave informed written consent and were financially compensated for their participation.</p></sec><sec id="s4-2"><title>Experimental overview</title><p>Each subject participated in four sessions of data collection. The first two sessions constituted Measurement Set 1, and the second two sessions Measurement Set 2 (Measurement Set 2 being a replication of Measurement Set 1). In both sessions, subjects underwent 48 min of fMRI scanning, with Session 1 also including two anatomical scans (a T1-weighted and a T2-weighted scan). Subjects were tested for color vision deficiencies in a separate session, using the Ishihara pseudoischromatic plates (<xref ref-type="bibr" rid="bib34">Ishihara, 1977</xref>). All subjects passed with no errors. The experimental procedures for Measurement Set 1 were preregistered (<ext-link ext-link-type="uri" xlink:href="https://osf.io/wgfzy/">https://osf.io/wgfzy/</ext-link>), and an addendum describes the replication Measurement Set 2 (<ext-link ext-link-type="uri" xlink:href="https://osf.io/zw6jp/">https://osf.io/zw6jp/</ext-link>).</p></sec><sec id="s4-3"><title>Digital light synthesis and silent substitution</title><p>All stimuli were generated using a digital light synthesis device (OneLight Spectra). This device produces desired spectra through the use of a digital micro-mirror device chip that in the configuration we used allows for the mixture of 56 independent primaries with a FWHM of ~16 nm and a refresh rate of 100 Hz.</p><p>Stimuli were generated to evoke specific photoreceptor responses through the use of silent substitution (<xref ref-type="bibr" rid="bib21">Estévez and Spekreijse, 1982</xref>). Silent substitution operates on the principle that there exist sets of light spectra that, when exchanged, selectively modulate the activity of specified cone photoreceptors. Thus, stimulus modulations relative to a background can be generated such that they nominally modulate the activity of only the L-, M-, or S-cones, or combinations of cone classes at specified contrasts. Additional information on how the stimuli were generated is provided in <xref ref-type="bibr" rid="bib70">Spitschan et al., 2015</xref>.</p><p>The stimuli account for differences in the cone fundamentals between the fovea and the periphery. This was done by treating the L, M, and S cones in the 2- and 15-degree CIE physiologically-based cone fundamentals (<xref ref-type="bibr" rid="bib14">CIE, 2007</xref>) as six classes of photoreceptors. For any desired set of L-, M-, and S-cone contrasts, we designed modulations that attempted to provide the same contrasts on the 2- and 15-degree L-cone fundamentals, on the 2- and 15-degree M-cone fundamentals, and on the 2- and 15-degree S-cone fundamentals. This is possible because our device has 56 primaries, rather than the typical 3 of RGB displays. Our procedure has the effect of creating light spectra that reduce differences in the L-, M-, and S-cone contrasts produced across the retina. The cone fundamentals were tailored to the age of each subject, to account for age-related differences in typical lens density (<xref ref-type="bibr" rid="bib14">CIE, 2007</xref>). See <xref ref-type="table" rid="table5">Tables 5</xref>–<xref ref-type="table" rid="table7">7</xref> for the central and peripheral maximum stimulus contrast values for each subject and measurement set.</p><p>Spectroradiometric measurements of the stimuli were made before and after each experimental session. During the measurements made prior to the experiment, a correction procedure was run in which the spectral power distribution of the modulation in each chromatic direction were adjusted to minimize the difference between the measured and desired cone contrasts for the 2- and 15-degree cone fundamentals. These corrections were made to the modulation spectra at the maximum contrast used in each direction. The order in which spectroradiometric measurements were taken during an experimental session was (1) five pre-correction measurements for each chromatic direction used in the session, (2) the corrections procedure, (3) five post-correction measurements per direction, and (4) five post-experiment measurements per direction. The mean of post-correction and post-experiment cone contrast measurements for the individual subjects and measurement sets are provided in <xref ref-type="table" rid="table5">Tables 5</xref>–<xref ref-type="table" rid="table7">7</xref>.</p></sec><sec id="s4-4"><title>Visual stimuli</title><p>The stimuli were confined to the LM plane of cone contrast space (<xref ref-type="fig" rid="fig10">Figure 10A</xref>; see also <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Cone contrast space has three axes that are defined by the relative change in the quantal catch of the L, M, and S cones when modulating between the light spectra of interest and a specified reference spectrum. We refer to the reference spectrum used to calculate this relative change in cone excitations as the background (nominal chromaticity; x = 0.459, y = 0.478, luminance Y = 426 cd/m^2; chromaticity and luminance computed with respect to the XYZ 10° physiologically-relevant color matching functions, as provided at <ext-link ext-link-type="uri" xlink:href="https://cvrl.org">https://cvrl.org</ext-link>). The background corresponds to the origin of cone contrast space. The LM contrast plane is a subspace of cone contrast space consisting of modulations that affect only L- and M-cone excitations, but which leave S-cone excitations unchanged relative to the background. A point in cone contrast space specifies how much L- and M-cone contrast is produced by modulating from the background to the specified stimulus. Points lying along the x-axis of <xref ref-type="fig" rid="fig10">Figure 10A</xref> modulate only L-cone contrast while M-cone contrast remains constant. Points lying along the y-axis modulate only M-cone contrast while keeping L-cone contrast constant. Points in intermediate directions modulate both L- and M-cone contrast, in proportion to the x- and y-axis components.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Stimulus space and temporal modulations.</title><p>(<bold>A</bold>) The LM contrast plane. A two-dimensional composed of axes that represent the change in L- and M-cone activity relative to the background cone activation, in units of cone contrast. Each aligned pair of vectors in this space represents the positive (increased activation) and negative (decreased activation) arms of the bipolar temporal modulations. We refer to each modulation by the angle of the positive arm in the LM contrast plane, with positive ∆L/L being at 0°. The black dashed lines show the maximum contrast used in each direction. The gray dashed circle shows 100% contrast. The ‘1’ or ‘2’ next to each positive arm denotes the session in which a given direction presented. The grouping was the same for Measurement Set 1 and Measurement Set 2. (<bold>B</bold>) The temporal profile of a single bipolar chromatic modulation. This shows how the cone contrast of a stimulus changed over time between the positive and negative arms for a given chromatic direction. The particular direction plotted corresponds to the 45° modulation at 12 Hz temporal frequency. The temporal profile was the same for all chromatic directions. (<bold>C</bold>) Schematic of the block structure of an functional run. Blocks lasted 12 s and all blocks were modulated around the same background. The amplitude of the modulation represents the contrast scaling, relative to its maximum contrast, for that block. Each run lasted a total of 288 s. The dark gray vertical bar represents an attentional event in which the light stimulus was dimmed for 500 ms.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig10-v2.tif"/></fig><p>All stimuli were spatially uniform (0 c.p.d.), full-field temporal modulations with a radius of 30° visual angle. The temporal modulations were bipolar sinusoids around the reference background, with the positive and negative arms designed to increase and decrease targeted cone excitations in a symmetric fashion. All stimuli were modulated at 12 Hz (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). A single modulation is thus described by pair of vectors in the LM contrast plane that have an angle of 180° between them, corresponding to the positive and negative arms of the modulation (<xref ref-type="fig" rid="fig10">Figure 10A</xref>). The entries of the vectors are the L- and M-cone contrasts of the end points of each arm. In this paper, we refer to a modulation by the angle made between its positive arm and the positive x-axis (corresponding to 0° in the LM contrast plane), with angle increasing counterclockwise. We refer to each angle tested as a chromatic direction. In total, we tested the eight chromatic directions: −45°, −22.5°, 0°, 22.5°,45°, 67.5°, 90°, and 112.5°. The −45°, 0°, 45°, and 90° directions correspond to L-M, L-cone isolating, L+M, and M-cone isolating directions, respectively. For all chromatic directions, the spectra were designed to produce constant S-cone contrast.</p><p>We express the stimulus contrast of a modulation as the vector length (L2 norm) of the cone contrast representation of its positive arm (<xref ref-type="fig" rid="fig10">Figure 10A</xref>). Gamut limitations of the light synthesis engine result in different maximum stimulus contrasts for different chromatic directions (see heavy dashed contour in <xref ref-type="fig" rid="fig10">Figure 10A</xref>). The maximum contrast used in each direction is provided in <xref ref-type="table" rid="table3">Table 3</xref>. For all directions, we tested five contrast levels. The contrast levels tested for each chromatic direction were selected to be log spaced relative to the maximum contrast used. The relative contrasts were 100, 50, 25, 12.5, and 6.25 percent. We also measured a 0 contrast reference condition in which the background without modulation was presented.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Table of the nominal maximum contrast per direction.</title><p>The top row indicates the chromatic direction in the LM plane.The L, M, and S contrast rows show the desired contrast on the L, M, and S cones, respectively. The total contrast is the vectorlength of stimuli made up of the L, M, and S cone contrast components and is the definition of contrast used in this study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top">Direction</th><th valign="top">−45°</th><th valign="top">−22.5°</th><th valign="top">0°</th><th valign="top">22.5°</th><th valign="top">45°</th><th valign="top">67.5°</th><th valign="top">90°</th><th valign="top">112.5°</th></tr></thead><tbody><tr><td valign="top">L-Contrast</td><td valign="top">8.49%</td><td valign="top">7.85%</td><td valign="top">14%</td><td valign="top">18.48%</td><td valign="top">42.43%</td><td valign="top">15.31%</td><td valign="top">0%</td><td valign="top">4.98%</td></tr><tr><td valign="top">M-Contrast</td><td valign="top">8.49%</td><td valign="top">3.25%</td><td valign="top">0%</td><td valign="top">7.65%</td><td valign="top">42.43%</td><td valign="top">36.96%</td><td valign="top">22%</td><td valign="top">12.01%</td></tr><tr><td valign="top">S-Contrast</td><td valign="top">0%</td><td valign="top">0%</td><td valign="top">0%</td><td valign="top">0%</td><td valign="top">0%</td><td valign="top">0%</td><td valign="top">0%</td><td valign="top">0%</td></tr><tr><td valign="top">Total Contrast</td><td valign="top">12%</td><td valign="top">8.5%</td><td valign="top">14%</td><td valign="top">20%</td><td valign="top">60%</td><td valign="top">40%</td><td valign="top">22%</td><td valign="top">13%</td></tr></tbody></table></table-wrap></sec><sec id="s4-5"><title>Experimental design</title><p>We measured whole brain BOLD fMRI responses to stimuli modulated in eight different chromatic directions, each with five contrast levels, using a block design. In total, we tested 40 different combinations of contrast levels and chromatic directions. We split the eight chromatic directions into two separate scanning sessions of four directions each (<xref ref-type="fig" rid="fig11">Figure 11A</xref>). In Session 1, we tested −45° (L-M), 0° (L Isolating), 45° (L+M), and 90° (M Isolating). In Session 2, we tested the other four directions (−22.5°, 22.5°, 67.5°, and 112.5°). The order of data collection for the two sessions was randomized across subjects. A session consisted of 10 runs and each run had a duration of 288 s. Within a run, each contrast/direction pair was presented within 12 s blocks (<xref ref-type="fig" rid="fig11">Figure 11</xref>). The order of contrast/direction pairs was psuedorandomized within each run. Along with four presentations of a background-only block, each run consisted of 24 blocks. The background-only blocks contained no temporal contrast modulation, providing a reference condition for data analysis. We chose 12 Hz modulations based upon prior work showing that for stimuli similar to the ones used in this study (L+M and L-M), this frequency elicited a robust response in V1 (<xref ref-type="bibr" rid="bib71">Spitschan et al., 2016</xref>). Within a block, modulations were ramped on and off using a 500 ms half-cosine. <xref ref-type="fig" rid="fig10">Figure 10C</xref> provides a schematic of the structure of an functional run.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Experimental design.</title><p>Panels <bold>A</bold> and <bold>B</bold> show the block design used for all runs and sessions. Panel <bold>A</bold> shows Measurement Set 1 which contained two separate MRI sessions. Each session contained four of the eight chromatic directions. The split of directions across the two sessions was the same for all subjects, but which session each subject started with was randomized. Within a session we collected 10 functional runs, each containing 24 blocks. The 24 blocks consisted of 20 direction/contrast paired stimulus blocks (depicted by the gradient squares with direction noted at top and the contrast at bottom of each square) and 4 background blocks (squares marked 'B'). The order of blocks within each run was randomized, with each contrast/direction pair shown once per run. Each run had a duration of 288 second. Panel B shows Measurement Set 2, which was a replication of Measurement Set 1, with session order and order of blocks within run re-randomized. There were 960 blocks across both measurement sets.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-fig11-v2.tif"/></fig><p>During each functional run, subjects engaged in an attention task. This task consisted of pressing a button every time the stimulus dimmed (<xref ref-type="fig" rid="fig10">Figure 10C</xref>). Each attentional event lasted for 500 ms. The probability of an attentional event occurring in a block was 66% in Measurement Set 1 and a 33% in Measurement Set 2. The onset time of an attentional event within a block was random except that the event could not occur during the on and off half-cosine ramp. The purpose of the attention task was to encourage and monitor subject wakefulness throughout the scan session. All subjects responded to 100% of the attentional events throughout all runs and sessions.</p><p>Each subject was studied during an initial pair of scanning sessions that we call Measurement Set 1 (<xref ref-type="fig" rid="fig11">Figure 11A</xref>), and a subsequent pair of replication scans that we call Measurement Set 2 (<xref ref-type="fig" rid="fig11">Figure 11B</xref>). Measurement Set 2 tested stimuli with the same chromatic directions and contrast levels as Measurement Set 1. The grouping of chromatic directions within a session was the same across measurement sets. The two measurement sets used different pseudo-randomized presentation orders. Both measurement sets also randomly assigned which session was acquired first. Across both sessions and measurements sets, we collected a total of 960 blocks per subject. The two measurement sets were analyzed separately.</p></sec><sec id="s4-6"><title>MRI data acquisition</title><p>MRI scans made use of the Human Connectome Project LifeSpan protocol (VD13D) implemented on a 3-Tesla Siemens Prisma with a 64-channel Siemens head coil. A T1-weighted, 3D, magnetization-prepared rapid gradient-echo (MPRAGE) anatomical image was acquired for each subject in axial orientation with 0.8 mm isotropic voxels, repetition time (TR) = 2.4 s, echo time (TE) = 2.22 ms, inversion time (TI) = 1000 ms, field of view (FoV) = 256 mm, flip angle = 8°. BOLD fMRI data were obtained over 72 axial slices with 2 mm isotropic voxels with multi-band = 8, TR = 800 ms, TE = 37 ms, FOV = 208 mm, flip angle = 52°. Head motion was minimized with foam padding.</p><p>During MRI scanning, subjects were supine inside the magnet. Visual stimuli were presented through an MRI compatible eyepiece to the right eye of the subject. Stimuli were delivered from the digital light synthesizer through a randomized optical fiber cable (Fiberoptics Technology Inc). The randomization of the fiber optic cable helped to minimize any spatial inhomogeneities in the spectrum of the stimulus. The eye piece provided adjustable focus to account for variation in refractive error. As the stimulus was a spatially uniform field, however, the effect of any spatial blur upon the stimulus was minimal.</p><p>Subjects used either button of a two button MR compatible response device (Current Designs) to respond to attention events during the functional runs.</p></sec><sec id="s4-7"><title>MRI data preprocessing</title><p>Both anatomical and functional data were preprocessed according to the HCP minimal preprocessing pipeline (<xref ref-type="bibr" rid="bib27">Glasser et al., 2013</xref>) Briefly, the anatomical data were passed through the pre-freesurfer, freesurfer, and post-freesurfer steps of the HCP minimal preprocessing pipeline. This was used to create an MNI registration, a Freesurfer segmentation (<xref ref-type="bibr" rid="bib17">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="bib23">Fischl et al., 1999</xref>), and a surface mesh. The functional data were preprocessed with both the volume and surface pipelines. The volume pipeline applied gradient distortion correction, motion correction (FLIRT 6 DoF; <xref ref-type="bibr" rid="bib35">Jenkinson et al., 2002</xref>), top-up phase encoding distortion correction (<xref ref-type="bibr" rid="bib67">Smith et al., 2004</xref>), and registered the functional images to the anatomical images. The surface pipeline mapped the volume data to the CIFTI grayordinate space which includes a left and right 32K cortical surface mesh and subcortical voxels. Finally, the functional data were passed through the ICAFIX pipeline, which uses independent component analysis and noise/not-noise classification to denoise the time course.</p><p>After initial preprocessing, we performed a series of subsequent steps before analyzing the time course data. We used a V1 region of interest (ROI) to extract the time series from primary visual cortex (see below for definition of retinotopic maps). The signals from each voxel were mean centered and converted to percent signal change. We then performed nuisance regression using the relative motion estimates and attentional events as regressors. The relative motion regressors (translation of X, Y, and Z and yaw, pitch, and roll) were mean centered and scaled by their respective standard deviations. The attention event regressor was modeled as a series of delta functions located within the TRs in which the events occurred, convolved with a hemodynamic response function. The nuisance regression was performed using the MATLAB linear regression function <italic>mldivide</italic> (MathWorks) with the residual of the model used as the ‘cleaned’ timed series.</p><p>Next, we time-point censored the time series of all voxels based on the motion estimates. This was done using a modified version of <xref ref-type="bibr" rid="bib60">Power et al., 2014</xref>. We converted the relative yaw, pitch, and roll estimates to millimeters by using the distance that each of these angles subtend on an assumed 50 mm sphere. We then took the L2-norm of the six translation and mm rotation estimates as a metric of frame-wise displacement (FD). We censored three contiguous TRs centered on any time point with an FD &gt; 0.5. Time points that exceeded the threshold were excluded from analysis, and a table of the number of censored frames can be found in <xref ref-type="table" rid="table4">Table 4</xref>. Finally, we applied polynomial detrending by fitting a fifth order polynomial to the time course from each voxel and subtracting it from the signal. Analyses performed at the level of V1 were done using the median value across voxels at each time point to represent the V1 signal. Vertex-wise analyses were performed on the preprocessed time course of individual vertices.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Number of censored fMRI frames per run.</title><p>Values shown are for Subject 1 Measurement Set 2. The top set of rows show data for session one and the bottom set of the show data for session 2. Each set of rows show the number of censored frames per run out of 360 frames. Subjects and sessions not shown mean that no frames were censored in those runs.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="11" valign="top">Subject 1 – Measurement Set 2: Session 1</th></tr><tr><th valign="top">Run Number</th><th valign="top">1</th><th valign="top">2</th><th valign="top">3</th><th valign="top">4</th><th valign="top">5</th><th valign="top">6</th><th valign="top">7</th><th valign="top">8</th><th valign="top">9</th><th valign="top">10</th></tr></thead><tbody><tr><td valign="top">Number of Censored Frames (n/360)</td><td valign="top">0</td><td valign="top">0</td><td valign="top">8</td><td valign="top">18</td><td valign="top">0</td><td valign="top">26</td><td valign="top">47</td><td valign="top">4</td><td valign="top">0</td><td valign="top">0</td></tr><tr><th>Subject 1 – Measurement Set 2: Session 2</th></tr><tr><th valign="top">Run Number</th><th valign="top">1</th><th valign="top">2</th><th valign="top">3</th><th valign="top">4</th><th valign="top">5</th><th valign="top">6</th><th valign="top">7</th><th valign="top">8</th><th valign="top">9</th><th valign="top">10</th></tr><tr><td valign="top">Number of Censored Frames (n/360)</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">7</td><td valign="top">0</td><td valign="top">0</td></tr></tbody></table></table-wrap></sec><sec id="s4-8"><title>Definition of retinotopic maps</title><p>Retinotopic regions of interest (ROI) were defined using the anatomical template neuropythy (<xref ref-type="bibr" rid="bib6">Benson et al., 2014</xref>) which provides eccentricity and polar angle maps for V1, V2, and V3. From this atlas, we defined a V1 ROI using the voxels in area V1 between 0–20° eccentricity and 0–180° polar angle. We set the eccentricity upper bound of the ROI to be 20° to provide a conservative boundary to ensure that we only analyzed stimulated vertices. This accounts for some uncertainty in the exact retinal size of the stimulated area due to, for example, variation in the distance of the eyepiece to the eye of the subject.</p><p>To define the retinotopic regions of interest used for hV4/Vo1 analysis, we registered the <xref ref-type="bibr" rid="bib79">Wang et al., 2015</xref> retinotopic atlas to CIFTI space. This was achieved using the subject specific atlas as defined by Neuropythy (<xref ref-type="bibr" rid="bib7">Benson and Winawer, 2018</xref>). This atlas was then registered to MNI space through the use of ANTs (<xref ref-type="bibr" rid="bib1">Avants et al., 2008</xref>). With the atlas in MNI space, we then transformed it to CITFI greyordinate space and used the hV4 and the VO1 ROIs to extract the time series data.</p></sec><sec id="s4-9"><title>Subject-specific hemodynamic response function</title><p>We derived subject-specific hemodynamic response functions (HRFs) for each subject and measurement set. The HRF was derived using all the functional runs within a measurement set, using the V1 region of interest median data. The time-series data were fit with a non-linear model that simultaneously estimated the beta weights of the GLM to fit stimulus responses, and the parameters of an HRF model. The HRF model was composed of the first three components of the ‘FLOBS’ basis set (<xref ref-type="bibr" rid="bib83">Woolrich et al., 2004</xref>). The best fitting HRF model was then used to fit that subject’s data for both the GLM and QCM models.</p></sec><sec id="s4-10"><title>General linear model</title><p>We used an ordinary least squares regression with a stimulus design matrix that described the stimulus order of a run. The regression matrix contained one regressor per stimulus block as well as a single regressor for the baseline, with the length of the regressor equal to the number of timepoints (360 TRs). The regressor for each stimulus block in a run was modeled by a binary vector that indicated the timepoints when the stimulus was present (ones) or absent (zeros), convolved with the HRF. This resulted in 21 GLM beta weights per run. For all analyses, model fitting was performed using the concatenation of all functional runs within a measurement set. For fitting of the GLM, we concatenated the stimulus design matrices such that like contrast/direction pairs were modeled by the same regressor. The concatenated stimulus design matrix for a measurement set had a total of 41 regressors (20 direction/contrast pairs from Session 1, 20 direction/contrast pairs from Session 2, and a shared baseline regressor) and 7200 timepoints. Additional details of the GLM are provided in the Appendix 1 section.</p></sec><sec id="s4-11"><title>Contrast response functions</title><p>To obtain contrast response function for each color direction, the time course data for each run was fit using a general linear model to obtain the effect that each stimulus had on the BOLD fMRI response. Grouping the GLM beta weights by chromatic direction defined a set of 8 contrast response functions (CRFs), one per direction. A CRF describes the relationship, within a particular chromatic direction, between the contrast of the stimulus and the measured response. The CRFs obtained using the GLM beta weights fit to the concatenated time series of Subject 2 can be seen in <xref ref-type="fig" rid="fig3">Figure 3</xref>. The panels of <xref ref-type="fig" rid="fig3">Figure 3</xref> show the CRFs for the eight different chromatic directions. The CRFs for Subject 1 and 2 can be seen in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref>–<xref ref-type="fig" rid="fig2s2">2</xref>.</p></sec><sec id="s4-12"><title>Error bars</title><p>The error bars and error regions in all figures are the 68 percent confidence intervals. We used 68% confidence intervals as this approximates +/- 1 SEM for a normal distribution. The percentiles we use to estimate error are the result of a bootstrap analysis. The bootstrap analysis was implemented as random sampling with replacement of the runs within a measurement set. The randomly drawn runs in both sessions were concatenated and fit by all models. We performed 200 bootstrap iterations and identified the 68% percent confidence interval from the bootstrap results.</p></sec><sec id="s4-13"><title>Leave-runs-out cross-validation</title><p>To evaluate model performance, we employed a leave-runs-out cross-validation strategy. For each cross-validation iteration, runs from Session 1 and Session 2 were randomly paired within the same measurement set. These pairs of runs were held out and the models were fit to the remaining 18 runs. From these model fits, a time course prediction for the left-out runs were obtained from both models. We computed the R^2 value between these predictions and the time course of the held-out runs. The average R^2 value across the 10 cross-validation iterations was used to compare models.</p></sec><sec id="s4-14"><title>Leave-session-out cross-validation</title><p>To evaluate the generalizability of the QCM, we implemented a leave-session-out cross-validation. As the eight chromatic directions tested were separated into two sessions with the same grouping across all subjects and measurement sets, we could evaluate the ability of the QCM to make predictions for chromatic directions and contrasts not used to fit the model parameters. Within each measurement set, we fit the QCM to Sessions 1 and 2 separately and evaluated how well the parameters of the model predicted responses to stimulus directions in the held-out session. These predicted responses were grouped by chromatic direction in order to construct a set of contrast response functions. The error bars in the CRFs are the 68% confidence intervals computed using bootstrapping, where we randomly sampled runs with replacement and compute the leave-session-out cross-validation a total a 200 times.</p></sec><sec id="s4-15"><title>Surface parameter map generation</title><p>Cortical surface maps were generated to visualize the ellipse angle and minor axis ratio parameters of the QCM on the V1 cortical surface. To generate the surface parameter maps, we fit the QCM, within a single vertex, to the concatenated time course from all runs in a measurement set. We repeat this fit for all vertices within the visual areas template map from neuropythy (<xref ref-type="bibr" rid="bib6">Benson et al., 2014</xref>). To visualize the surface parameter maps in a manner that highlights differences in fits as a function of cortical position, we created a series of scatter plots that relate the minor axis ratio and angle parameters to the eccentricity of their respective vertices. The regression lines in the scatter plots are robust regression lines implemented through the built in MATLAB function <italic>robustfit</italic> which adaptively reweights the data to discount the effects of outliers.</p></sec><sec id="s4-16"><title>Parameter fitting</title><p>We fit QCM the model to the concatenated time series for both sessions within a measurement set for each subject. The data were fit using the MATLAB function <italic>fmincon</italic> to find a set of model parameters that minimize the difference between the actual fMRI time course and the QCM prediction of the time course, computed as the root mean squared error.</p></sec><sec id="s4-17"><title>Preprocessing and analysis code</title><p>All code used to perform analyses in this paper may be found in our public GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/BrainardLab/LFContrastAnalysis_eLife.git">https://github.com/BrainardLab/LFContrastAnalysis_eLife.git</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0f89a00b3aa746868a2561a13d61738b9f534b9d;origin=https://github.com/BrainardLab/LFContrastAnalysis_eLife;visit=swh:1:snp:a0edc7874ae2bf0d7ac02f8157b9b42d7f4d5e5a;anchor=swh:1:rev:c9b4cbe72e69e4d3d623ec8b7fc62076e2fe1a22">swh:1:rev:c9b4cbe72e69e4d3d623ec8b7fc62076e2fe1a22</ext-link>, <xref ref-type="bibr" rid="bib2">Barnett, 2021</xref>).</p></sec><sec id="s4-18"><title>Spectroradiometric stimulus validations</title><p><xref ref-type="table" rid="table5">Tables 5–﻿7</xref> show the stimulus validation measurements for all subjects and sessions. The tables show the mean and standard deviation of stimulus vector angles and lengths computed from 10 validation measurements (five pre-experiment, five post-experiment). Center and periphery denote which set of cone fundamentals were used to calculate cone contrast of the stimuli referring either the 2° or 15° CIE fundamentals, respectively.</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Stimulus validation measurements for Subject 1.</title><p>The top set of rows show data for Measurement Set 1 and the bottom set show data for Measurement Set 2. The dark gray rows show the nominal angle and contrast. Each cell shows the mean and standard deviation of stimulus vector angles and lengths computed from 10 validation measurements (5 pre-experiment, 5 post-experiment). Center and periphery denote which set of cone fundamentals were used to calculate cone contrast of the stimuli referring either the 2° or 15° CIE fundamentals, respectively.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="9" valign="top">Subject 1 – Measurement Set 1</th></tr><tr><th valign="top">Nominal Angle</th><th valign="top">−45°</th><th valign="top">−22.5°</th><th valign="top">0°</th><th valign="top">22.5°</th><th valign="top">45°</th><th valign="top">67.5°</th><th valign="top">90°</th><th valign="top">112.5°</th></tr></thead><tbody><tr><td valign="top">Center Angle</td><td valign="top">−41.23 <break/>±3.13</td><td valign="top">−15.90 <break/>±6.59</td><td valign="top">2.17 <break/>±1.05</td><td valign="top">23.43 <break/>±0.23</td><td valign="top">45.21 <break/>±0.23</td><td valign="top">69.36 <break/>±1.61</td><td valign="top">87.98 <break/>±1.26</td><td valign="top">120.94 <break/>±8.45</td></tr><tr><td valign="top">Periphery Angle</td><td valign="top">−42.85 <break/>±3.04</td><td valign="top">−16.31 <break/>±6.18</td><td valign="top">1.61 <break/>±0.44</td><td valign="top">22.75 <break/>±0.22</td><td valign="top">44.78 <break/>±0.22</td><td valign="top">68.13 <break/>±1.54</td><td valign="top">88.72 <break/>±0.18</td><td valign="top">119.67 <break/>±8.04</td></tr><tr><th valign="top">Nominal Contrast</th><th valign="top">12%</th><th valign="top">8.5%</th><th valign="top">14%</th><th valign="top">20%</th><th valign="top">60%</th><th valign="top">40%</th><th valign="top">22%</th><th valign="top">13%</th></tr><tr><td valign="top">Center Contrast</td><td valign="top">12.14 <break/>±0.05</td><td valign="top">9.02 <break/>±0.75</td><td valign="top">14.44 <break/>±0.37</td><td valign="top">21.05 <break/>±0.79</td><td valign="top">60.92 <break/>±0.09</td><td valign="top">38.01 <break/>±1.97</td><td valign="top">21.39 <break/>±0.85</td><td valign="top">12.56 <break/>±0.38</td></tr><tr><td valign="top">Periphery Contrast</td><td valign="top">12.00 <break/>±0.04</td><td valign="top">8.93 <break/>±0.72</td><td valign="top">13.98 <break/>±0.35</td><td valign="top">20.28 <break/>±0.73</td><td valign="top">58.73 <break/>±0.09</td><td valign="top">37.81 <break/>±1.89</td><td valign="top">21.42 <break/>±0.55</td><td valign="top">12.48 <break/>±0.36</td></tr><tr><th colspan="9" valign="top">Subject 1 - Measurement Set 2</th></tr><tr><th valign="top">Nominal Angle</th><th valign="top">−45°</th><th valign="top">−22.5°</th><th valign="top">0°</th><th valign="top">22.5°</th><th valign="top">45°</th><th valign="top">67.5°</th><th valign="top">90°</th><th valign="top">112.5°</th></tr><tr><td valign="top">Center Angle</td><td valign="top">−45.09 <break/>±0.55</td><td valign="top">−22.44 <break/>±0.98</td><td valign="top">3.49 <break/>±2.27</td><td valign="top">22.97 <break/>±0.15</td><td valign="top">45.18 <break/>±0.03</td><td valign="top">67.75 <break/>±0.24</td><td valign="top">87.81 <break/>±1.69</td><td valign="top">112.99 <break/>±1.03</td></tr><tr><td valign="top">Periphery Angle</td><td valign="top">−46.16 <break/>±0.55</td><td valign="top">−22.32 <break/>±0.99</td><td valign="top">3.24 <break/>±2.77</td><td valign="top">22.39 <break/>±0.19</td><td valign="top">44.92 <break/>±0.02</td><td valign="top">66.85 <break/>±0.21</td><td valign="top">87.33 <break/>±1.24</td><td valign="top">68.34 <break/>±0.92</td></tr><tr><th valign="top">Nominal Contrast</th><th valign="top">12%</th><th valign="top">8.5%</th><th valign="top">14%</th><th valign="top">20%</th><th valign="top">60%</th><th valign="top">40%</th><th valign="top">22%</th><th valign="top">13%</th></tr><tr><td valign="top">Center Contrast</td><td valign="top">11.75 <break/>±0.01</td><td valign="top">8.28 <break/>±0.08</td><td valign="top">13.78 <break/>±0.09</td><td valign="top">20.35 <break/>±0.22</td><td valign="top">60.26 <break/>±0.48</td><td valign="top">37.62 <break/>±0.11</td><td valign="top">21.82 <break/>±0.08</td><td valign="top">12.69 <break/>±0.05</td></tr><tr><td valign="top">Periphery Contrast</td><td valign="top">11.65 <break/>±0.02</td><td valign="top">8.29 <break/>±0.08</td><td valign="top">13.53 <break/>±0.09</td><td valign="top">19.79 <break/>±0.18</td><td valign="top">58.47 <break/>±0.48</td><td valign="top">38.16 <break/>±0.10</td><td valign="top">21.99 <break/>±0.10</td><td valign="top">12.81 <break/>±0.06</td></tr></tbody></table></table-wrap><table-wrap id="table6" position="float"><label>Table 6.</label><caption><title>Stimulus validation measurements for Subject 2.</title><p>The format of this table is the same as <xref ref-type="table" rid="table5">Table 5</xref>. Cells that contain an 'X' mark stimulus directions in which validation measurements were not recorded due to technical difficulty.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="9" valign="top">Subject 2 - Measurement Set 1</th></tr><tr><th valign="top">Nominal Angle</th><th valign="top">−45°</th><th valign="top">−22.5°</th><th valign="top">0°</th><th valign="top">22.5°</th><th valign="top">45°</th><th valign="top">67.5°</th><th valign="top">90°</th><th valign="top">112.5°</th></tr></thead><tbody><tr><td valign="top">Center Angle</td><td valign="top">−45.73 <break/>±1.06</td><td valign="top">−19.23 <break/>±2.51</td><td valign="top">1.71 <break/>±0.98</td><td valign="top">24.04 <break/>±0.39</td><td valign="top">45.43 <break/>±0.09</td><td valign="top">68.36 <break/>±0.53</td><td valign="top">87.87 <break/>±1.64</td><td valign="top">114.92 <break/>±1.90</td></tr><tr><td valign="top">Periphery Angle</td><td valign="top">−47.03 <break/>±1.02</td><td valign="top">−18.91 <break/>±2.40</td><td valign="top">1.63 <break/>±0.77</td><td valign="top">23.65 <break/>±0.38</td><td valign="top">44.99 <break/>±0.09</td><td valign="top">67.18 <break/>±0.52</td><td valign="top">87.65 <break/>±1.09</td><td valign="top">114.11 <break/>±1.86</td></tr><tr><th valign="top">Nominal Contrast</th><th valign="top">12%</th><th valign="top">8.5%</th><th valign="top">14%</th><th valign="top">20%</th><th valign="top">60%</th><th valign="top">40%</th><th valign="top">22%</th><th valign="top">13%</th></tr><tr><td valign="top">Center Contrast</td><td valign="top">12.04 <break/>±0.11</td><td valign="top">8.41 <break/>±0.21</td><td valign="top">14.01 <break/>±0.08</td><td valign="top">20.45 <break/>±0.52</td><td valign="top">60.72 <break/>±0.38</td><td valign="top">39.68 <break/>±0.63</td><td valign="top">21.82 <break/>±0.23</td><td valign="top">12.73 <break/>±0.16</td></tr><tr><td valign="top">Periphery Contrast</td><td valign="top">11.88 <break/>±0.11</td><td valign="top">8.36 <break/>±0.21</td><td valign="top">13.58 <break/>±0.07</td><td valign="top">19.81 <break/>±0.51</td><td valign="top">58.56 <break/>±0.35</td><td valign="top">39.41 <break/>±0.59</td><td valign="top">21.82 <break/>±0.24</td><td valign="top">12.62 <break/>±0.14</td></tr><tr><th colspan="9" valign="top">Subject 2 - Measurement Set 2</th></tr><tr><th valign="top">Nominal Angle</th><th valign="top">−45°</th><th valign="top">−22.5°</th><th valign="top">0°</th><th valign="top">22.5°</th><th valign="top">45°</th><th valign="top">67.5°</th><th valign="top">90°</th><th valign="top">112.5°</th></tr><tr><td valign="top">Center Angle</td><td valign="top">X</td><td valign="top">−25.85 <break/>±3.20</td><td valign="top">X</td><td valign="top">22.72 <break/>±0.38</td><td valign="top">X</td><td valign="top">67.42 <break/>±0.16</td><td valign="top">X</td><td valign="top">112.03 <break/>±1.22</td></tr><tr><td valign="top">Periphery Angle</td><td valign="top">X</td><td valign="top">−26.43 <break/>±3.04</td><td valign="top">X</td><td valign="top">22.35 <break/>±0.35</td><td valign="top">X</td><td valign="top">66.36 <break/>±0.16</td><td valign="top">X</td><td valign="top">110.12 <break/>±1.17</td></tr><tr><th valign="top">Nominal Contrast</th><th valign="top">12%</th><th valign="top">8.5%</th><th valign="top">14%</th><th valign="top">20%</th><th valign="top">60%</th><th valign="top">40%</th><th valign="top">22%</th><th valign="top">13%</th></tr><tr><td valign="top">Center Contrast</td><td valign="top">X</td><td valign="top">8.36 <break/>±0.16</td><td valign="top">X</td><td valign="top">19.77 <break/>±0.23</td><td valign="top">X</td><td valign="top">39.75 <break/>±0.13</td><td valign="top">X</td><td valign="top">12.75 <break/>±0.15</td></tr><tr><td valign="top">Contrast</td><td valign="top">X</td><td valign="top">8.34 <break/>±0.17</td><td valign="top">X</td><td valign="top">19.21 <break/>±0.23</td><td valign="top">X</td><td valign="top">40.04 <break/>±0.15</td><td valign="top">X</td><td valign="top">12.95 <break/>±0.13</td></tr></tbody></table></table-wrap><table-wrap id="table7" position="float"><label>Table 7.</label><caption><title>Stimulus validation measurements for Subject 3.</title><p>The format of this table is the same as <xref ref-type="table" rid="table5">Tables 5</xref>,<xref ref-type="table" rid="table6">6</xref>. Cells that contain an 'X' mark stimulus directions in which validation measurements were not recorded due to technical difficulty.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="9" valign="top">Subject 3 - Measurement Set 1</th></tr><tr><th valign="top">Nominal Angle</th><th valign="top">−45°</th><th valign="top">−22.5°</th><th valign="top">0°</th><th valign="top">22.5°</th><th valign="top">45°</th><th valign="top">67.5°</th><th valign="top">90°</th><th valign="top">112.5°</th></tr></thead><tbody><tr><td valign="top">Center Angle</td><td valign="top">−48.84 <break/>±5.02</td><td valign="top">−27.84 <break/>±6.02</td><td valign="top">5.29 <break/>±4.45</td><td valign="top">23.61 <break/>±0.77</td><td valign="top">45.41 <break/>±0.15</td><td valign="top">67.48 <break/>±0.42</td><td valign="top">85.96 <break/>±3.11</td><td valign="top">108.56 <break/>±4.89</td></tr><tr><td valign="top">Periphery Angle</td><td valign="top">−50.40 <break/>± 4.90</td><td valign="top">−28.48 <break/>±5.97</td><td valign="top">5.09 <break/>±4.72</td><td valign="top">23.26 <break/>±0.77</td><td valign="top">45.02 <break/>±0.13</td><td valign="top">66.33 <break/>±0.43</td><td valign="top">86.12 <break/>±3.24</td><td valign="top">107.49 <break/>±4.78</td></tr><tr><th valign="top">Nominal Contrast</th><th valign="top">12%</th><th valign="top">8.5%</th><th valign="top">14%</th><th valign="top">20%</th><th valign="top">60%</th><th valign="top">40%</th><th valign="top">22%</th><th valign="top">13%</th></tr><tr><td valign="top">Center Contrast</td><td valign="top">12.25 <break/>±0.32</td><td valign="top">8.39 <break/>±0.06</td><td valign="top">13.96 <break/>±0.08</td><td valign="top">19.78 <break/>±0.20</td><td valign="top">61.92 <break/>±1.40</td><td valign="top">40.87 <break/>±1.14</td><td valign="top">22.91 <break/>±1.30</td><td valign="top">13.53 <break/>±0.75</td></tr><tr><td valign="top">Periphery Contrast</td><td valign="top">12.13 <break/>±0.31</td><td valign="top">8.30 <break/>±0.08</td><td valign="top">13.52 <break/>±0.09</td><td valign="top">19.17 <break/>±0.21</td><td valign="top">59.76 <break/>±1.36</td><td valign="top">40.65 <break/>±1.13</td><td valign="top">22.97 <break/>±1.26</td><td valign="top">13.47 <break/>±0.71</td></tr><tr><th colspan="9" valign="top">Subject 3 - Measurement Set 2</th></tr><tr><th valign="top">Nominal Angle</th><th valign="top">−45°</th><th valign="top">−22.5°</th><th valign="top">0°</th><th valign="top">22.5°</th><th valign="top">45°</th><th valign="top">67.5°</th><th valign="top">90°</th><th valign="top">112.5°</th></tr><tr><td valign="top">Center Angle</td><td valign="top">X</td><td valign="top">−24.51 <break/>±1.57</td><td valign="top">X</td><td valign="top">22.92 <break/>±0.07</td><td valign="top">X</td><td valign="top">67.65 <break/>±0.19</td><td valign="top">X</td><td valign="top">111.23 <break/>±1.11</td></tr><tr><td valign="top">Periphery Angle</td><td valign="top">X</td><td valign="top">−24.17 <break/>±1.53</td><td valign="top">X</td><td valign="top">22.65 <break/>±0.06</td><td valign="top">X</td><td valign="top">66.57 <break/>±0.19</td><td valign="top">X</td><td valign="top">69.95 <break/>±1.07</td></tr><tr><th valign="top">Nominal Contrast</th><th valign="top">12%</th><th valign="top">8.5%</th><th valign="top">14%</th><th valign="top">20%</th><th valign="top">60%</th><th valign="top">40%</th><th valign="top">22%</th><th valign="top">13%</th></tr><tr><td valign="top">Center Contrast</td><td valign="top">X</td><td valign="top">8.24 <break/>±0.07</td><td valign="top">X</td><td valign="top">19.93 <break/>±0.09</td><td valign="top">X</td><td valign="top">39.77 <break/>±.0.27</td><td valign="top">X</td><td valign="top">12.88 <break/>±0.11</td></tr><tr><td valign="top">Periphery Contrast</td><td valign="top">X</td><td valign="top">8.23 <break/>±0.07</td><td valign="top">X</td><td valign="top">19.45 <break/>±0.26</td><td valign="top">X</td><td valign="top">40.16 <break/>±0.26</td><td valign="top">X</td><td valign="top">12.92 <break/>±0.10</td></tr></tbody></table></table-wrap></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This work has been supported by the National Institutes of Health (Grants:RO1 EY10016 and Core GrantP30 EY001583; <ext-link ext-link-type="uri" xlink:href="https://www.nih.gov/">https://www.nih.gov/</ext-link>) and National Science Foundation Graduate Research Fellowship (DGE-1845298). We thank Joris Vincent, Jack Ryan, Ozenc Taskin, and Nicolas Cottaris for technical assistance.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Formal analysis, Funding acquisition, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The research was approved by the University of Pennsylvania Institutional Review Board (Protocol: Photoreceptor directed light modulation 817774). All subjects gave informed written consent and were financially compensated for their participation.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-65590-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The raw fMRI data from our experiment have been deposited to OpenNeuro, under the <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds003752.v1.0.0">https://doi.org/10.18112/openneuro.ds003752.v1.0.0</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>M</given-names></name><name><surname>Aguirre</surname><given-names>G</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>LFContrast</data-title><source>OpenNeuro</source><pub-id assigning-authority="other" pub-id-type="doi">10.18112/openneuro.ds003752.v1.0.0</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname> <given-names>BB</given-names></name><name><surname>Epstein</surname> <given-names>CL</given-names></name><name><surname>Grossman</surname> <given-names>M</given-names></name><name><surname>Gee</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><volume>12</volume><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><pub-id pub-id-type="pmid">17659998</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Barnett</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>LFContrastAnalysis</data-title><source>Software Heritage</source><version designator="swh:1:rev:c9b4cbe72e69e4d3d623ec8b7fc62076e2fe1a22 https://archive.softwareheritage.org/swh:1:rev:c9b4cbe72e69e4d3d623ec8b7fc62076e2fe1a22">swh:1:rev:c9b4cbe72e69e4d3d623ec8b7fc62076e2fe1a22 https://archive.softwareheritage.org/swh:1:rev:c9b4cbe72e69e4d3d623ec8b7fc62076e2fe1a22</version></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartels</surname> <given-names>A</given-names></name><name><surname>Zeki</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The architecture of the colour centre in the human visual brain: new results and a review *</article-title><source>European Journal of Neuroscience</source><volume>12</volume><fpage>172</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2000.00905.x</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baseler</surname> <given-names>HA</given-names></name><name><surname>Sutter</surname> <given-names>EE</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>M and P components of the VEP and their visual field distribution</article-title><source>Vision Research</source><volume>37</volume><fpage>675</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1016/S0042-6989(96)00209-X</pub-id><pub-id pub-id-type="pmid">9156212</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname> <given-names>MS</given-names></name><name><surname>Haxby</surname> <given-names>JV</given-names></name><name><surname>Jennings</surname> <given-names>JE</given-names></name><name><surname>DeYoe</surname> <given-names>EA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>An fMRI version of the Farnsworth-Munsell 100-Hue test reveals multiple color-selective Areas in human ventral occipitotemporal cortex</article-title><source>Cerebral Cortex</source><volume>9</volume><fpage>257</fpage><lpage>263</lpage><pub-id pub-id-type="doi">10.1093/cercor/9.3.257</pub-id><pub-id pub-id-type="pmid">10355906</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname> <given-names>NC</given-names></name><name><surname>Butt</surname> <given-names>OH</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Aguirre</surname> <given-names>GK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Correction of distortion in flattened representations of the cortical surface allows prediction of V1-V3 functional organization from anatomy</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003538</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003538</pub-id><pub-id pub-id-type="pmid">24676149</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname> <given-names>NC</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bayesian analysis of retinotopic maps</article-title><source>eLife</source><volume>7</volume><elocation-id>e40224</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.40224</pub-id><pub-id pub-id-type="pmid">30520736</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehm</surname> <given-names>AE</given-names></name><name><surname>MacLeod</surname> <given-names>DI</given-names></name><name><surname>Bosten</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Compensation for red-green contrast loss in anomalous trichromats</article-title><source>Journal of Vision</source><volume>14</volume><elocation-id>19</elocation-id><pub-id pub-id-type="doi">10.1167/14.13.19</pub-id><pub-id pub-id-type="pmid">25413625</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bracewell</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>The Fourier Transform and Its Applications</source><publisher-name>McGraw-Hill</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1996">1996</year><chapter-title>Cone contrast and opponent modulation color spaces</chapter-title><person-group person-group-type="editor"><name><surname>Kaiser</surname> <given-names>P. K</given-names></name><name><surname>Boynton</surname> <given-names>R. M</given-names></name></person-group><source>Human Color Vision</source><publisher-loc>Washington</publisher-loc><publisher-name>Optical Society of America</publisher-name><fpage>563</fpage><lpage>579</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Pelli</surname> <given-names>DG</given-names></name><name><surname>Robson</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2002">2002</year><chapter-title>Display characterization</chapter-title><person-group person-group-type="editor"><name><surname>Hornak</surname> <given-names>J. P</given-names></name></person-group><source>Encylopedia of Imaging Science and Technology</source><publisher-loc>New York</publisher-loc><publisher-name>Wiley</publisher-name><fpage>172</fpage><lpage>188</lpage></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Stockman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><chapter-title>Colorimetry</chapter-title><person-group person-group-type="editor"><name><surname>Bass</surname> <given-names>M</given-names></name><name><surname>DeCusatis</surname> <given-names>C</given-names></name><name><surname>Enoch</surname> <given-names>J</given-names></name><name><surname>Lakshminarayanan</surname> <given-names>V</given-names></name><name><surname>Li</surname> <given-names>G</given-names></name><name><surname>Macdonald</surname> <given-names>C</given-names></name><name><surname>Mahajan</surname> <given-names>V</given-names></name><name><surname>van Stryland</surname> <given-names>E</given-names></name></person-group><source>The Optical Society of America Handbook of Optics Volume III: Vision and Vision Optics</source><publisher-loc>New York</publisher-loc><publisher-name>McGraw Hill</publisher-name><fpage>10.11</fpage><lpage>10.56</lpage></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname> <given-names>GJ</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decoding and reconstructing color from responses in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>13992</fpage><lpage>14003</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3577-09.2009</pub-id><pub-id pub-id-type="pmid">19890009</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="confproc"><person-group person-group-type="author"><collab>CIE</collab></person-group><year iso-8601-date="2007">2007</year><article-title>Fundamental chromaticity diagram with physiological axes – Parts 1 and 2</article-title><conf-name>Technical Report 170-1 Central Bureau of the Commission Internationale De l' Éclairage</conf-name></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cottaris</surname> <given-names>NP</given-names></name><name><surname>Jiang</surname> <given-names>H</given-names></name><name><surname>Ding</surname> <given-names>X</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A computational-observer model of spatial contrast sensitivity: effects of wave-front-based optics, cone-mosaic structure, and inference engine</article-title><source>Journal of Vision</source><volume>19</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.1167/19.4.8</pub-id><pub-id pub-id-type="pmid">30943530</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D'Souza</surname> <given-names>DV</given-names></name><name><surname>Auer</surname> <given-names>T</given-names></name><name><surname>Frahm</surname> <given-names>J</given-names></name><name><surname>Strasburger</surname> <given-names>H</given-names></name><name><surname>Lee</surname> <given-names>BB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dependence of chromatic responses in V1 on visual field eccentricity and spatial frequency: an fMRI study</article-title><source>Journal of the Optical Society of America A</source><volume>33</volume><fpage>A53</fpage><lpage>A64</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.33.000A53</pub-id><pub-id pub-id-type="pmid">26974942</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis I segmentation and surface reconstruction</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Valois</surname> <given-names>RL</given-names></name><name><surname>Abramov</surname> <given-names>I</given-names></name><name><surname>Jacobs</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Analysis of response patterns of LGN cells</article-title><source>Journal of the Optical Society of America</source><volume>56</volume><fpage>966</fpage><lpage>977</lpage><pub-id pub-id-type="doi">10.1364/JOSA.56.000966</pub-id><pub-id pub-id-type="pmid">4959282</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derrington</surname> <given-names>AM</given-names></name><name><surname>Krauskopf</surname> <given-names>J</given-names></name><name><surname>Lennie</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Chromatic mechanisms in lateral geniculate nucleus of macaque</article-title><source>The Journal of Physiology</source><volume>357</volume><fpage>241</fpage><lpage>265</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1984.sp015499</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname> <given-names>S</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Wandell</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Colour tuning in human visual cortex measured with functional magnetic resonance imaging</article-title><source>Nature</source><volume>388</volume><fpage>68</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1038/40398</pub-id><pub-id pub-id-type="pmid">9214503</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Estévez</surname> <given-names>O</given-names></name><name><surname>Spekreijse</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>The &quot;silent substitution&quot; method in visual research</article-title><source>Vision Research</source><volume>22</volume><fpage>681</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(82)90104-3</pub-id><pub-id pub-id-type="pmid">7112962</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname> <given-names>GD</given-names></name><name><surname>Gauthier</surname> <given-names>JL</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Greschner</surname> <given-names>M</given-names></name><name><surname>Machado</surname> <given-names>TA</given-names></name><name><surname>Jepson</surname> <given-names>LH</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Gunning</surname> <given-names>DE</given-names></name><name><surname>Mathieson</surname> <given-names>K</given-names></name><name><surname>Dabrowski</surname> <given-names>W</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional connectivity in the retina at the resolution of photoreceptors</article-title><source>Nature</source><volume>467</volume><fpage>673</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.1038/nature09424</pub-id><pub-id pub-id-type="pmid">20930838</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis. II: Inflation, flattening, and a surface-based coordinate system</article-title><source>NeuroImage</source><volume>9</volume><fpage>195</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0396</pub-id><pub-id pub-id-type="pmid">9931269</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gegenfurtner</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Color in the cortex revisited</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>339</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1038/85963</pub-id><pub-id pub-id-type="pmid">11276215</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname> <given-names>WS</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Sequential ideal-observer analysis of visual discriminations</article-title><source>Psychological Review</source><volume>96</volume><fpage>267</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.96.2.267</pub-id><pub-id pub-id-type="pmid">2652171</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgeson</surname> <given-names>MA</given-names></name><name><surname>Sullivan</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Contrast constancy: deblurring in human vision by spatial frequency channels</article-title><source>The Journal of Physiology</source><volume>252</volume><fpage>627</fpage><lpage>656</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1975.sp011162</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname> <given-names>MF</given-names></name><name><surname>Sotiropoulos</surname> <given-names>SN</given-names></name><name><surname>Wilson</surname> <given-names>JA</given-names></name><name><surname>Coalson</surname> <given-names>TS</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Andersson</surname> <given-names>JL</given-names></name><name><surname>Xu</surname> <given-names>J</given-names></name><name><surname>Jbabdi</surname> <given-names>S</given-names></name><name><surname>Webster</surname> <given-names>M</given-names></name><name><surname>Polimeni</surname> <given-names>JR</given-names></name><name><surname>Van Essen</surname> <given-names>DC</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><collab>WU-Minn HCP Consortium</collab></person-group><year iso-8601-date="2013">2013</year><article-title>The minimal preprocessing pipelines for the human connectome project</article-title><source>NeuroImage</source><volume>80</volume><fpage>105</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.127</pub-id><pub-id pub-id-type="pmid">23668970</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goddard</surname> <given-names>E</given-names></name><name><surname>Mannion</surname> <given-names>DJ</given-names></name><name><surname>McDonald</surname> <given-names>JS</given-names></name><name><surname>Solomon</surname> <given-names>SG</given-names></name><name><surname>Clifford</surname> <given-names>CW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Color responsiveness argues against a dorsal component of human V4</article-title><source>Journal of Vision</source><volume>11</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/11.4.3</pub-id><pub-id pub-id-type="pmid">21467155</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hadjikhani</surname> <given-names>N</given-names></name><name><surname>Liu</surname> <given-names>AK</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Cavanagh</surname> <given-names>P</given-names></name><name><surname>Tootell</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Retinotopy and color sensitivity in human visual cortical area V8</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>235</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1038/681</pub-id><pub-id pub-id-type="pmid">10195149</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname> <given-names>T</given-names></name><name><surname>Pracejus</surname> <given-names>L</given-names></name><name><surname>Gegenfurtner</surname> <given-names>KR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Color perception in the intermediate periphery of the visual field</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>26</elocation-id><pub-id pub-id-type="doi">10.1167/9.4.26</pub-id><pub-id pub-id-type="pmid">19757935</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horwitz</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Signals related to color in the early visual cortex</article-title><source>Annual Review of Vision Science</source><volume>6</volume><fpage>287</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-121219-081801</pub-id><pub-id pub-id-type="pmid">32936735</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horwitz</surname> <given-names>GD</given-names></name><name><surname>Hass</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Nonlinear analysis of macaque V1 color tuning reveals cardinal directions for cortical color processing</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>913</fpage><lpage>919</lpage><pub-id pub-id-type="doi">10.1038/nn.3105</pub-id><pub-id pub-id-type="pmid">22581184</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hunt</surname> <given-names>RWG</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>The Reproduction of Colour</source><publisher-loc>Chichester, England</publisher-loc><publisher-name>John Wiley &amp; Sons</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ishihara</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1977">1977</year><source>Tests for Colour-Blindness</source><publisher-loc>Tokyo</publisher-loc><publisher-name>Kanehara Shuppen Company, Ltd</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Bannister</surname> <given-names>P</given-names></name><name><surname>Brady</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1132</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>EN</given-names></name><name><surname>Hawken</surname> <given-names>MJ</given-names></name><name><surname>Shapley</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The spatial transformation of color in the primary visual cortex of the macaque monkey</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>409</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1038/86061</pub-id><pub-id pub-id-type="pmid">11276232</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname> <given-names>EN</given-names></name><name><surname>Hawken</surname> <given-names>MJ</given-names></name><name><surname>Shapley</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Cone inputs in macaque primary visual cortex</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>2501</fpage><lpage>2514</lpage><pub-id pub-id-type="doi">10.1152/jn.01043.2003</pub-id><pub-id pub-id-type="pmid">14749310</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>KN</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Mezer</surname> <given-names>A</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013a</year><article-title>Compressive spatial summation in human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>481</fpage><lpage>494</lpage><pub-id pub-id-type="doi">10.1152/jn.00105.2013</pub-id><pub-id pub-id-type="pmid">23615546</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>KN</given-names></name><name><surname>Winawer</surname> <given-names>J</given-names></name><name><surname>Rokem</surname> <given-names>A</given-names></name><name><surname>Mezer</surname> <given-names>A</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2013">2013b</year><article-title>A two-stage cascade model of BOLD responses in human visual cortex</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003079</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003079</pub-id><pub-id pub-id-type="pmid">23737741</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname> <given-names>KN</given-names></name><name><surname>Yeatman</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Bottom-up and top-down computations in word- and face-selective cortex</article-title><source>eLife</source><volume>6</volume><elocation-id>e22341</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22341</pub-id><pub-id pub-id-type="pmid">28226243</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>I</given-names></name><name><surname>Hong</surname> <given-names>SW</given-names></name><name><surname>Shevell</surname> <given-names>SK</given-names></name><name><surname>Shim</surname> <given-names>WM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural representations of perceptual color experience in the human ventral visual pathway</article-title><source>PNAS</source><volume>117</volume><fpage>13145</fpage><lpage>13150</lpage><pub-id pub-id-type="doi">10.1073/pnas.1911041117</pub-id><pub-id pub-id-type="pmid">32457156</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knoblauch</surname> <given-names>K</given-names></name><name><surname>Maloney</surname> <given-names>LT</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Testing the indeterminacy of linear color mechanisms from color discrimination data</article-title><source>Vision Research</source><volume>36</volume><fpage>295</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(95)00098-K</pub-id><pub-id pub-id-type="pmid">8594827</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauskopf</surname> <given-names>J</given-names></name><name><surname>Williams</surname> <given-names>DR</given-names></name><name><surname>Heeley</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Cardinal directions of color space</article-title><source>Vision Research</source><volume>22</volume><fpage>1123</fpage><lpage>1131</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(82)90077-3</pub-id><pub-id pub-id-type="pmid">7147723</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lafer-Sousa</surname> <given-names>R</given-names></name><name><surname>Conway</surname> <given-names>BR</given-names></name><name><surname>Kanwisher</surname> <given-names>NG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Color-Biased regions of the ventral visual pathway lie between face- and Place-Selective regions in humans, as in macaques</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1682</fpage><lpage>1697</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3164-15.2016</pub-id><pub-id pub-id-type="pmid">26843649</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lankheet</surname> <given-names>MJ</given-names></name><name><surname>Lennie</surname> <given-names>P</given-names></name><name><surname>Krauskopf</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Distinctive characteristics of subclasses of red-green P-cells in LGN of macaque</article-title><source>Visual Neuroscience</source><volume>15</volume><fpage>37</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1017/S0952523898151027</pub-id><pub-id pub-id-type="pmid">9456503</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>BB</given-names></name><name><surname>Shapley</surname> <given-names>RM</given-names></name><name><surname>Hawken</surname> <given-names>MJ</given-names></name><name><surname>Sun</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spatial distributions of cone inputs to cells of the parvocellular pathway investigated with cone-isolating gratings</article-title><source>Journal of the Optical Society of America A</source><volume>29</volume><fpage>A223</fpage><lpage>A232</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.29.00A223</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lennie</surname> <given-names>P</given-names></name><name><surname>Haake</surname> <given-names>PW</given-names></name><name><surname>Williams</surname> <given-names>DR</given-names></name></person-group><year iso-8601-date="1991">1991</year><chapter-title>The design of chromatically opponent receptive fields</chapter-title><person-group person-group-type="editor"><name><surname>Landy</surname> <given-names>M. S</given-names></name><name><surname>Movshon</surname> <given-names>J. A</given-names></name></person-group><source>Computational Models of Visual Processing</source><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>71</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1017/CBO9780511816772.027</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lennie</surname> <given-names>P</given-names></name><name><surname>Movshon</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Coding of color and form in the geniculostriate visual pathway (invited review)</article-title><source>Journal of the Optical Society of America A</source><volume>22</volume><fpage>2013</fpage><lpage>2033</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.22.002013</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Specializations for chromatic and temporal signals in human visual cortex</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>3459</fpage><lpage>3468</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4206-04.2005</pub-id><pub-id pub-id-type="pmid">15800201</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>PR</given-names></name><name><surname>Lee</surname> <given-names>BB</given-names></name><name><surname>White</surname> <given-names>AJ</given-names></name><name><surname>Solomon</surname> <given-names>SG</given-names></name><name><surname>Rüttiger</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Chromatic sensitivity of ganglion cells in the peripheral primate retina</article-title><source>Nature</source><volume>410</volume><fpage>933</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.1038/35073587</pub-id><pub-id pub-id-type="pmid">11309618</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>PR</given-names></name><name><surname>Blessing</surname> <given-names>EM</given-names></name><name><surname>Buzás</surname> <given-names>P</given-names></name><name><surname>Szmajda</surname> <given-names>BA</given-names></name><name><surname>Forte</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Transmission of colour and acuity signals by parvocellular cells in marmoset monkeys</article-title><source>The Journal of Physiology</source><volume>589</volume><fpage>2795</fpage><lpage>2812</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2010.194076</pub-id><pub-id pub-id-type="pmid">21486786</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullen</surname> <given-names>KT</given-names></name><name><surname>Sakurai</surname> <given-names>M</given-names></name><name><surname>Chu</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Does L/M cone opponency disappear in human periphery?</article-title><source>Perception</source><volume>34</volume><fpage>951</fpage><lpage>959</lpage><pub-id pub-id-type="doi">10.1068/p5374</pub-id><pub-id pub-id-type="pmid">16178149</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullen</surname> <given-names>KT</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>McMahon</surname> <given-names>KL</given-names></name><name><surname>de Zubicaray</surname> <given-names>GI</given-names></name><name><surname>Hess</surname> <given-names>RF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Selectivity of human retinotopic visual cortex to S-cone-opponent, L/M-cone-opponent and achromatic stimulation</article-title><source>European Journal of Neuroscience</source><volume>25</volume><fpage>491</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2007.05302.x</pub-id><pub-id pub-id-type="pmid">17284191</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullen</surname> <given-names>K</given-names></name><name><surname>Dumoulin</surname> <given-names>S</given-names></name><name><surname>Hess</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010a</year><article-title>Color processing in the human LGN and cortex measured with fMRI</article-title><source>Journal of Vision</source><volume>7</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.1167/7.15.4</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullen</surname> <given-names>KT</given-names></name><name><surname>Thompson</surname> <given-names>B</given-names></name><name><surname>Hess</surname> <given-names>RF</given-names></name></person-group><year iso-8601-date="2010">2010b</year><article-title>Responses of the human visual cortex and LGN to achromatic and chromatic temporal modulations: an fMRI study</article-title><source>Journal of Vision</source><volume>10</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1167/10.13.13</pub-id><pub-id pub-id-type="pmid">21106678</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullen</surname> <given-names>KT</given-names></name><name><surname>Kingdom</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Losses in peripheral colour sensitivity predicted from &quot;hit and miss&quot; post-receptoral cone connections</article-title><source>Vision Research</source><volume>36</volume><fpage>1995</fpage><lpage>2000</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(95)00261-8</pub-id><pub-id pub-id-type="pmid">8759439</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullen</surname> <given-names>KT</given-names></name><name><surname>Kingdom</surname> <given-names>FA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Differential distributions of red-green and blue-yellow cone opponency across the visual field</article-title><source>Visual Neuroscience</source><volume>19</volume><fpage>109</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1017/S0952523802191103</pub-id><pub-id pub-id-type="pmid">12180855</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newton</surname> <given-names>JR</given-names></name><name><surname>Eskew</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Chromatic detection and discrimination in the periphery: a postreceptoral loss of color sensitivity</article-title><source>Visual Neuroscience</source><volume>20</volume><fpage>511</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1017/S0952523803205058</pub-id><pub-id pub-id-type="pmid">14977330</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirson</surname> <given-names>AB</given-names></name><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Varner</surname> <given-names>DC</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Surface characterizations of color thresholds</article-title><source>Journal of the Optical Society of America A</source><volume>7</volume><fpage>783</fpage><lpage>789</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.7.000783</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname> <given-names>JD</given-names></name><name><surname>Mitra</surname> <given-names>A</given-names></name><name><surname>Laumann</surname> <given-names>TO</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>Schlaggar</surname> <given-names>BL</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title><source>NeuroImage</source><volume>84</volume><fpage>320</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.048</pub-id><pub-id pub-id-type="pmid">23994314</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reid</surname> <given-names>RC</given-names></name><name><surname>Shapley</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Spatial structure of cone inputs to receptive fields in primate lateral geniculate nucleus</article-title><source>Nature</source><volume>356</volume><fpage>716</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1038/356716a0</pub-id><pub-id pub-id-type="pmid">1570016</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenholtz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Capabilities and limitations of peripheral vision</article-title><source>Annual Review of Vision Science</source><volume>2</volume><fpage>437</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035733</pub-id><pub-id pub-id-type="pmid">28532349</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakurai</surname> <given-names>M</given-names></name><name><surname>Mullen</surname> <given-names>KT</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cone weights for the two cone-opponent systems in peripheral vision and asymmetries of cone contrast sensitivity</article-title><source>Vision Research</source><volume>46</volume><fpage>4346</fpage><lpage>4354</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2006.08.016</pub-id><pub-id pub-id-type="pmid">17014883</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schluppeck</surname> <given-names>D</given-names></name><name><surname>Engel</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Color opponent neurons in V1: a review and model reconciling results from imaging and single-unit recording</article-title><source>Journal of Vision</source><volume>2</volume><fpage>5</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1167/2.6.5</pub-id><pub-id pub-id-type="pmid">12678646</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapley</surname> <given-names>R</given-names></name><name><surname>Hawken</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Color in the cortex: single- and double-opponent cells</article-title><source>Vision Research</source><volume>51</volume><fpage>701</fpage><lpage>717</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2011.02.012</pub-id><pub-id pub-id-type="pmid">21333672</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shevell</surname> <given-names>SK</given-names></name><name><surname>Martin</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Color opponency: tutorial</article-title><source>Journal of the Optical Society of America A</source><volume>34</volume><fpage>1099</fpage><lpage>1108</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.34.001099</pub-id><pub-id pub-id-type="pmid">29036118</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Johansen-Berg</surname> <given-names>H</given-names></name><name><surname>Bannister</surname> <given-names>PR</given-names></name><name><surname>De Luca</surname> <given-names>M</given-names></name><name><surname>Drobnjak</surname> <given-names>I</given-names></name><name><surname>Flitney</surname> <given-names>DE</given-names></name><name><surname>Niazy</surname> <given-names>RK</given-names></name><name><surname>Saunders</surname> <given-names>J</given-names></name><name><surname>Vickers</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>De Stefano</surname> <given-names>N</given-names></name><name><surname>Brady</surname> <given-names>JM</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomon</surname> <given-names>SG</given-names></name><name><surname>Lennie</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Chromatic gain controls in visual cortical neurons</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>4779</fpage><lpage>4792</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5316-04.2005</pub-id><pub-id pub-id-type="pmid">15888653</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomon</surname> <given-names>SG</given-names></name><name><surname>Lennie</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The machinery of colour vision</article-title><source>Nature Reviews Neuroscience</source><volume>8</volume><fpage>276</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1038/nrn2094</pub-id><pub-id pub-id-type="pmid">17375040</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitschan</surname> <given-names>M</given-names></name><name><surname>Aguirre</surname> <given-names>GK</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Selective stimulation of penumbral cones reveals perception in the shadow of retinal blood vessels</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0124328</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0124328</pub-id><pub-id pub-id-type="pmid">25897842</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitschan</surname> <given-names>M</given-names></name><name><surname>Datta</surname> <given-names>R</given-names></name><name><surname>Stern</surname> <given-names>AM</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name><name><surname>Aguirre</surname> <given-names>GK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Human visual cortex responses to rapid cone and Melanopsin-Directed flicker</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1471</fpage><lpage>1482</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1932-15.2016</pub-id><pub-id pub-id-type="pmid">26843631</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stockman</surname> <given-names>A</given-names></name><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="2010">2010</year><chapter-title>Color vision mechanisms</chapter-title><person-group person-group-type="editor"><name><surname>Bass</surname> <given-names>M</given-names></name><name><surname>DeCusatis</surname> <given-names>C</given-names></name><name><surname>Enoch</surname> <given-names>J</given-names></name><name><surname>Lakshminarayanan</surname> <given-names>V</given-names></name><name><surname>Li</surname> <given-names>G</given-names></name><name><surname>Macdonald</surname> <given-names>C</given-names></name><name><surname>Mahajan</surname> <given-names>V</given-names></name><name><surname>van Stryland</surname> <given-names>E</given-names></name></person-group><source>The Optical Society of America Handbook of Optics, Volume III: Vision and Vision Optics</source><publisher-loc>New York</publisher-loc><publisher-name>McGraw Hill</publisher-name><fpage>11</fpage><lpage>104</lpage></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stromeyer</surname> <given-names>CF</given-names></name><name><surname>Lee</surname> <given-names>J</given-names></name><name><surname>Eskew</surname> <given-names>RT</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Peripheral chromatic sensitivity for flashes: a post-receptoral red-green asymmetry</article-title><source>Vision Research</source><volume>32</volume><fpage>1865</fpage><lpage>1873</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(92)90047-M</pub-id><pub-id pub-id-type="pmid">1287984</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tailby</surname> <given-names>C</given-names></name><name><surname>Solomon</surname> <given-names>SG</given-names></name><name><surname>Dhruv</surname> <given-names>NT</given-names></name><name><surname>Lennie</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Habituation reveals fundamental chromatic mechanisms in striate cortex of macaque</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>1131</fpage><lpage>1139</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4682-07.2008</pub-id><pub-id pub-id-type="pmid">18234891</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tregillus</surname> <given-names>KEM</given-names></name><name><surname>Isherwood</surname> <given-names>ZJ</given-names></name><name><surname>Vanston</surname> <given-names>JE</given-names></name><name><surname>Engel</surname> <given-names>SA</given-names></name><name><surname>MacLeod</surname> <given-names>DIA</given-names></name><name><surname>Kuriki</surname> <given-names>I</given-names></name><name><surname>Webster</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Color compensation in anomalous trichromats assessed with fMRI</article-title><source>Current Biology</source><volume>31</volume><fpage>936</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.11.039</pub-id><pub-id pub-id-type="pmid">33326771</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanni</surname> <given-names>S</given-names></name><name><surname>Henriksson</surname> <given-names>L</given-names></name><name><surname>Viikari</surname> <given-names>M</given-names></name><name><surname>James</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Retinotopic distribution of chromatic responses in human primary visual cortex</article-title><source>European Journal of Neuroscience</source><volume>24</volume><fpage>1821</fpage><lpage>1831</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2006.05070.x</pub-id><pub-id pub-id-type="pmid">17004945</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Dumoulin</surname> <given-names>SO</given-names></name><name><surname>Brewer</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Computational neuroimaging; Color signals in the visual pathways</article-title><source>Neuro-Opthalmol. Jpn</source><volume>23</volume><fpage>324</fpage><lpage>343</lpage></element-citation></ref><ref id="bib78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wandell</surname> <given-names>BA</given-names></name><name><surname>Silverstein</surname> <given-names>LD</given-names></name></person-group><year iso-8601-date="2003">2003</year><chapter-title>Digital color reproduction</chapter-title><person-group person-group-type="editor"><name><surname>Shevell</surname> <given-names>S. K</given-names></name></person-group><source>The Science of Color</source><publisher-loc>Oxford</publisher-loc><publisher-name>Optical Society of America</publisher-name><fpage>281</fpage><lpage>316</lpage></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Mruczek</surname> <given-names>RE</given-names></name><name><surname>Arcaro</surname> <given-names>MJ</given-names></name><name><surname>Kastner</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id><pub-id pub-id-type="pmid">25452571</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welbourne</surname> <given-names>LE</given-names></name><name><surname>Morland</surname> <given-names>AB</given-names></name><name><surname>Wade</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Population receptive field (pRF) measurements of chromatic responses in human visual cortex using fMRI</article-title><source>NeuroImage</source><volume>167</volume><fpage>84</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.11.022</pub-id><pub-id pub-id-type="pmid">29155081</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weller</surname> <given-names>JP</given-names></name><name><surname>Horwitz</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Measurements of neuronal color tuning: procedures, pitfalls, and alternatives</article-title><source>Vision Research</source><volume>151</volume><fpage>53</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2017.08.005</pub-id><pub-id pub-id-type="pmid">29133032</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wool</surname> <given-names>LE</given-names></name><name><surname>Crook</surname> <given-names>JD</given-names></name><name><surname>Troy</surname> <given-names>JB</given-names></name><name><surname>Packer</surname> <given-names>OS</given-names></name><name><surname>Zaidi</surname> <given-names>Q</given-names></name><name><surname>Dacey</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Nonselective wiring accounts for Red-Green opponency in midget ganglion cells of the primate retina</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>1520</fpage><lpage>1540</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1688-17.2017</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Smith</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Constrained linear basis sets for HRF modelling using variational bayes</article-title><source>NeuroImage</source><volume>21</volume><fpage>1748</fpage><lpage>1761</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.12.024</pub-id><pub-id pub-id-type="pmid">15050595</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>General linear model</title><p>The model used to provide a benchmark for the quadratic color model (QCM) is the general linear model (GLM). The GLM has the form:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This states that the measurement (<inline-formula><mml:math id="inf1"><mml:mi>Y</mml:mi></mml:math></inline-formula>) is equal to the model matrix (<inline-formula><mml:math id="inf2"><mml:mi>X</mml:mi></mml:math></inline-formula>) times the weights (<inline-formula><mml:math id="inf3"><mml:mi>β</mml:mi></mml:math></inline-formula>) plus the residual error (<inline-formula><mml:math id="inf4"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>). For our GLM, <inline-formula><mml:math id="inf5"><mml:mi>Y</mml:mi></mml:math></inline-formula> is a column vector of the concatenated time series from all the fMRI runs within a measurement set (20 runs):<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi mathvariant="normal">⋮</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi mathvariant="normal">⋮</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The subscript <inline-formula><mml:math id="inf6"><mml:mi>i</mml:mi></mml:math></inline-formula> indicates a particular time point, with the subscript <inline-formula><mml:math id="inf7"><mml:mi>t</mml:mi></mml:math></inline-formula> being the total number of time points in a measurement set. Here a time point corresponds to one TR of the BOLD response, and t = 7200 (20 runs with 360 TRs). Our model comparison was for the aggregated V1 response, and we took each element <italic>y</italic><sub><italic>i</italic></sub> of <inline-formula><mml:math id="inf8"><mml:mi>Y</mml:mi></mml:math></inline-formula> to be the median BOLD fMRI response for the corresponding TR, with the median taken across the voxels in the V1 ROI.</p><p>The model matrix <inline-formula><mml:math id="inf9"><mml:mi>X</mml:mi></mml:math></inline-formula> contains the predictor variables for the linear model. Each column of <inline-formula><mml:math id="inf10"><mml:mi>X</mml:mi></mml:math></inline-formula> is a regressor corresponding to a single stimulus (a single chromatic direction/contrast pair or the baseline (0 contrast) uniform field). The regressors are created by convolving a binary indicator vector with the hemodynamic response function (HRF) that accounts for the sluggish BOLD response to a stimulus event. The binary indicator vectors contain 1 when the stimulus was present and 0 otherwise. The convolution then produces a predictor of the measured BOLD fMRI response for that stimulus condition. In our study, <inline-formula><mml:math id="inf11"><mml:mi>X</mml:mi></mml:math></inline-formula> has 41 columns corresponding to the pairing of the eight chromatic directions and five contrasts levels plus the baseline<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mi mathvariant="normal">…</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>41</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi mathvariant="normal">⋮</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mi/></mml:mtd><mml:mtd columnalign="center"><mml:mi mathvariant="normal">⋮</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mi mathvariant="normal">…</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>41</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi mathvariant="normal">⋮</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mi/></mml:mtd><mml:mtd columnalign="center"><mml:mi mathvariant="normal">⋮</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mi mathvariant="normal">…</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>41</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The general linear model predicts <inline-formula><mml:math id="inf12"><mml:mi>Y</mml:mi></mml:math></inline-formula> as a weighted linear combination of the regressors in the columns of <inline-formula><mml:math id="inf13"><mml:mi>X</mml:mi></mml:math></inline-formula>. The weight applied to each regressor is given by the elements of <inline-formula><mml:math id="inf14"><mml:mi>β</mml:mi></mml:math></inline-formula>:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi mathvariant="normal">⋮</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>β</mml:mi><mml:mn>41</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Using <inline-formula><mml:math id="inf15"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mi>X</mml:mi></mml:math></inline-formula>, we can generate a predicted time course <inline-formula><mml:math id="inf17"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>. We take each <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> to be a proxy for the aggregate V1 BOLD fMRI response for the corresponding stimulus condition. This interpretation is associated with our particular choice of scaling the indicator variables in the regressors before convolution with the HRF (that is the choice of 1 for the TRs during which the stimulus was present). We determined <inline-formula><mml:math id="inf19"><mml:mi>β</mml:mi></mml:math></inline-formula> using linear regression as implemented in the MATLAB mldivide operation, <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>\</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (see <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/help/matlab/ref/mldivide.html">https://www.mathworks.com/help/matlab/ref/mldivide.html</ext-link>). The GLM prediction of the BOLD fMRI response at each time point <italic>y</italic><sub><italic>i</italic></sub> is therefore:<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>41</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>41</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s9" sec-type="appendix"><title>Quadratic color model</title><p>The quadratic color model allows for the prediction of the BOLD fMRI response to any stimulus that lies in the LM contrast plane. This includes predictions for stimuli not in the measurement set used to fit the model. The QCM makes its predictions through three steps. The first step calculates the ‘equivalent contrast’ of the stimulus which can be thought of as the effective contrast of the stimulus in V1 accounting for the differences in sensitivity across chromatic directions. Equivalent contrast is the vector length of the stimuli after this linear transformation. The next step is a response non-linearity applied to the equivalent contrast to predict the underlying neural response. Finally, a convolution of this underlying response with the HRF results in predictions of the BOLD fMRI response. Here, we provide explanations and equations for the model.</p><p>We start by considering a stimulus modulation whose predicted BOLD fMRI response we wish to know. A stimulus modulation is denoted by the column vector <inline-formula><mml:math id="inf21"><mml:mi>c</mml:mi></mml:math></inline-formula> whose two entries are the L and the M cone contrast of the stimulus (<inline-formula><mml:math id="inf22"><mml:mi>l</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:mi>m</mml:mi></mml:math></inline-formula>, respectively): <disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mi>l</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>m</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The figure below outlines how we transform such a stimulus to a response, prior to convolution with the HRF. Panel A shows a 3-dimensional contrast-response space with the (<inline-formula><mml:math id="inf24"><mml:mi>x</mml:mi></mml:math></inline-formula>,<inline-formula><mml:math id="inf25"><mml:mi>y</mml:mi></mml:math></inline-formula>) plane representing the LM contrast plane and the <inline-formula><mml:math id="inf26"><mml:mi>z</mml:mi></mml:math></inline-formula> axis giving the response <inline-formula><mml:math id="inf27"><mml:mi>r</mml:mi></mml:math></inline-formula> corresponding to each point in the LM contrast plane. In this representation, all possible stimuli and responses form an inverted bell shape surface, as illustrated below. At constant values of <inline-formula><mml:math id="inf28"><mml:mi>r</mml:mi></mml:math></inline-formula> (constant height on the <inline-formula><mml:math id="inf29"><mml:mi>z</mml:mi></mml:math></inline-formula> axis), cross sections through this surface shows the elliptical isoresponse contours of the QCM. Each elliptical isoresponse contour describes the set of LM contrast combinations that elicit the same response <inline-formula><mml:math id="inf30"><mml:mi>r</mml:mi></mml:math></inline-formula>. The teal and red dots shown in the LM plane represent example stimulus modulations, chosen in two color directions at contrasts corresponding to the five (dark blue, blue, aqua, green, yellow) isoresponse contours illustrated. </p><p>To obtain the equivalent contrast corresponding to stimulus <inline-formula><mml:math id="inf31"><mml:mi>c</mml:mi></mml:math></inline-formula>, we first apply a linear transformation <inline-formula><mml:math id="inf32"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf33"><mml:mi>c</mml:mi></mml:math></inline-formula> to obtain a transformed representation of the stimulus, <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The vector <inline-formula><mml:math id="inf35"><mml:mi>e</mml:mi></mml:math></inline-formula> is a two-dimensional column vector whose entries we refer to as <italic>e</italic><sub>1</sub> and <italic>e</italic><sub>2</sub>. We call the transformed representation of the stimulus the equivalent contrast space, and as we show below we choose the linear transformation <inline-formula><mml:math id="inf36"><mml:mi>M</mml:mi></mml:math></inline-formula> such that in this space the isoresponse contours are circles. Panel B of the figure shows the same information as in Panel A represented in the equivalent contrast space. Here the (<inline-formula><mml:math id="inf37"><mml:mi>x</mml:mi></mml:math></inline-formula>,<inline-formula><mml:math id="inf38"><mml:mi>y</mml:mi></mml:math></inline-formula>) plane gives the values of <italic>e</italic><sub>1</sub> and <italic>e</italic><sub>2</sub>, and the isoresponse contours plotted with respect to the equivalent contrast plane are circular. Note that in the equivalent contrast plane, the distances between the plotted teal points are the same as the distances between the corresponding plotted red points. This is a consequence of the fact that the transformation <inline-formula><mml:math id="inf39"><mml:mi>M</mml:mi></mml:math></inline-formula> is chosen to make the isoresponse contours circular.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Illustration of cone to equivalent contrast transformation.</title><p>(Panel <bold>A</bold>) A 3-dimensional cone contrast-response space with the (<inline-formula><mml:math id="inf40"><mml:mi>x</mml:mi></mml:math></inline-formula>,<inline-formula><mml:math id="inf41"><mml:mi>y</mml:mi></mml:math></inline-formula>) plane representing the LM contrast plane and the <inline-formula><mml:math id="inf42"><mml:mi>z</mml:mi></mml:math></inline-formula> axis giving the corresponding response <inline-formula><mml:math id="inf43"><mml:mi>r</mml:mi></mml:math></inline-formula> to each point in the (<inline-formula><mml:math id="inf44"><mml:mi>x</mml:mi></mml:math></inline-formula>,<inline-formula><mml:math id="inf45"><mml:mi>y</mml:mi></mml:math></inline-formula>) plane. The teal and red dots shown in the LM plane represent example stimulus modulations, chosen in two color directions at contrasts corresponding to the five isoresponse contours illustrated in dark blue, blue, aqua, green, yellow. (Panel <bold>B</bold>) A three-dimensional equivalent contrast-response space with the (<inline-formula><mml:math id="inf46"><mml:mi>x</mml:mi></mml:math></inline-formula>,<inline-formula><mml:math id="inf47"><mml:mi>y</mml:mi></mml:math></inline-formula>) plane representing equivalent contrast (e<sub>1</sub> and e<sub>2</sub>) and the <inline-formula><mml:math id="inf48"><mml:mi>z</mml:mi></mml:math></inline-formula> axis giving the corresponding response <inline-formula><mml:math id="inf49"><mml:mi>r</mml:mi></mml:math></inline-formula> to each point in the (<inline-formula><mml:math id="inf50"><mml:mi>x</mml:mi></mml:math></inline-formula>,<inline-formula><mml:math id="inf51"><mml:mi>y</mml:mi></mml:math></inline-formula>) plane. The teal and red dots shown correspond to same color dots in Panel <bold>A</bold> after we apply a linear transformation <inline-formula><mml:math id="inf52"><mml:mi>M</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf53"><mml:mi>c</mml:mi></mml:math></inline-formula> (the L- and M-cone contrast representation of the stimuli) to obtain a transformed representation of the stimulus, <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Note that after the application of <inline-formula><mml:math id="inf55"><mml:mi>M</mml:mi></mml:math></inline-formula>, the elliptical contours in Panel <bold>A</bold> are circular and the distances between the plotted teal points are the same as the distances between the corresponding plotted red points. (Panel <bold>C</bold>) The equivalent contrast response function. The x-axis denotes the equivalent contrast and the y-axis marks the response. The teal and red closed circles shown in Panel <bold>C</bold> correspond to both the teal and red points shown in Panels <bold>A</bold> and <bold>B</bold>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-app1-fig1-v2.tif"/></fig><p>We define the equivalent contrast <inline-formula><mml:math id="inf56"><mml:mi>k</mml:mi></mml:math></inline-formula> of a stimulus as the vector length of the transformed stimulus <inline-formula><mml:math id="inf57"><mml:mi>e</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. </p><p>The next step of the model is a non-linearity that maps between equivalent contrast (<inline-formula><mml:math id="inf59"><mml:mi>k</mml:mi></mml:math></inline-formula>) and BOLD fMRI response (dashed black line in Panel C of the figure). This is possible since all stimuli in equivalent contrast space with the same equivalent contrast predict the same underlying response, regardless of the chromatic direction of the stimuli. Therefore, we can focus solely on the relationship between equivalent contrast and the associated response. We call the static non-linearity the ‘equivalent contrast-response function’. The teal and red closed circles shown in Panel C correspond to both the teal and red points shown in Panels A and B, and these overlap since they lie on the same set of isoresponse contours. </p><p>The linear transformation <inline-formula><mml:math id="inf60"><mml:mi>M</mml:mi></mml:math></inline-formula> needed to compute the equivalent contrast representation of <inline-formula><mml:math id="inf61"><mml:mi>c</mml:mi></mml:math></inline-formula> is derived by starting with the equation for an ellipse centered at the origin, in which a positive-definite quadratic function of the cone contrasts is equal to a constant for all points on the ellipse. This equation is given below (left hand side) and re-expressed in matrix-vector form (right hand side).<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt"><mml:mtr><mml:mtd columnalign="center"><mml:mi>l</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mi>m</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mi>A</mml:mi></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>B</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>B</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mi>C</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mi>l</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi>m</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Changing the value of the constant <inline-formula><mml:math id="inf62"><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> changes the scale of the ellipse without changing its shape, and as we will see below <inline-formula><mml:math id="inf63"><mml:mi>k</mml:mi></mml:math></inline-formula> is the equivalent contrast corresponding to each constant-shape elliptical isoresponse contour. We rewrite the matrix representation of the elliptical locus as: <disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This yields<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>Because the coefficients <inline-formula><mml:math id="inf64"><mml:mi>A</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf65"><mml:mi>B</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf66"><mml:mi>C</mml:mi></mml:math></inline-formula> are constrained so that <inline-formula><mml:math id="inf67"><mml:mi>Q</mml:mi></mml:math></inline-formula> is a symmetric positive definite matrix, <inline-formula><mml:math id="inf68"><mml:mi>Q</mml:mi></mml:math></inline-formula> can be factored through its eigenvalue decomposition and rewritten as<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf69"><mml:mi>V</mml:mi></mml:math></inline-formula> is an orthonormal (rotation) matrix and <inline-formula><mml:math id="inf70"><mml:mi mathvariant="normal">Λ</mml:mi></mml:math></inline-formula> is a diagonal matrix with positive entries. Since <inline-formula><mml:math id="inf71"><mml:mi mathvariant="normal">Λ</mml:mi></mml:math></inline-formula> is a diagonal matrix we can further decompose as<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf72"><mml:mi>S</mml:mi></mml:math></inline-formula> is diagonal with entries equal to the square root of the corresponding entries of <inline-formula><mml:math id="inf73"><mml:mi mathvariant="normal">Λ</mml:mi></mml:math></inline-formula>. </p><p>The matrix <inline-formula><mml:math id="inf74"><mml:mi>V</mml:mi></mml:math></inline-formula> expresses a rotation in the cone contrast plane and may be parameterized by the rotation angle <italic>p</italic><sub>1</sub>:<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="5pt" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The matrix <inline-formula><mml:math id="inf75"><mml:mi>S</mml:mi></mml:math></inline-formula> is diagonal and when applied to a vector simply scales the entries of that vector. Although <inline-formula><mml:math id="inf76"><mml:mi>S</mml:mi></mml:math></inline-formula> normally has two degrees of freedom in the general case, we lock the scale of the major axis to 1. Therefore we are only concerned with the scaling of the minor axis (<italic>p</italic><sub>2</sub>). Thus we can define<disp-formula id="equ13"><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mtext> </mml:mtext><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In this formulation, <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub> parameterize the shape of an elliptical isoresponse contour and <inline-formula><mml:math id="inf77"><mml:mi>k</mml:mi></mml:math></inline-formula> parameterizes its scale. The parameter <italic>p</italic><sub>1</sub> is what we call the angle of the major axis in the main text, while <italic>p</italic><sub>2</sub> is the minor axis ratio. </p><p>We set <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and rewrite <inline-formula><mml:math id="inf79"><mml:mi>Q</mml:mi></mml:math></inline-formula> as:<disp-formula id="equ14"><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The matrix <inline-formula><mml:math id="inf80"><mml:mi>M</mml:mi></mml:math></inline-formula> then transforms the cone contrast vector <inline-formula><mml:math id="inf81"><mml:mi>c</mml:mi></mml:math></inline-formula> to the equivalent contrast vector through <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Recall that equivalent contrast is the vector length of <inline-formula><mml:math id="inf83"><mml:mi>e</mml:mi></mml:math></inline-formula>. Therefore, the equivalent contrast of the points on the ellipse corresponding to <inline-formula><mml:math id="inf84"><mml:mi>k</mml:mi></mml:math></inline-formula> is given by<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mi>e</mml:mi><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></disp-formula></p><p>To see this, note that<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:msup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mi>e</mml:mi><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>Q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula></p><p>Thus, given <inline-formula><mml:math id="inf85"><mml:mi>Q</mml:mi></mml:math></inline-formula>, we can compute the equivalent contrast for any stimulus <inline-formula><mml:math id="inf86"><mml:mi>c</mml:mi></mml:math></inline-formula> and apply the equivalent contrast-response function to predict the its response. The specific non-linearity we use for the contrast-response function is a Naka-Rushton function. This function is a four parameter saturating non-linearity that is defined by the following: <disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:msup><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The neural response (<inline-formula><mml:math id="inf87"><mml:mi>r</mml:mi></mml:math></inline-formula>) is a function of the equivalent contrast (<inline-formula><mml:math id="inf88"><mml:mi>k</mml:mi></mml:math></inline-formula>). The parameters of the Naka-Rushton function are the amplitude (<inline-formula><mml:math id="inf89"><mml:mi>a</mml:mi></mml:math></inline-formula>), exponent (<inline-formula><mml:math id="inf90"><mml:mi>n</mml:mi></mml:math></inline-formula>), semi-saturation (<inline-formula><mml:math id="inf91"><mml:mi>s</mml:mi></mml:math></inline-formula>), and the offset (<inline-formula><mml:math id="inf92"><mml:mi>h</mml:mi></mml:math></inline-formula>). These parameters control the gain, the slope, the position along the x axis, and the y-axis offset of the non-linearity, respectively. The shape of the non-linearity controls how the underlying response changes with equivalent contrast. </p><p>Finally, we need to convert the neural response to a prediction of the BOLD fMRI response(<inline-formula><mml:math id="inf93"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>). To do this, we convolve the underlying response with the hemodynamic response function (HRF).<disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⊛</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Predictions of the BOLD fMRI response via the QCM are thus made using six parameters: angle (<italic>p</italic><sub>1</sub>), minor axis ratio (<italic>p</italic><sub>2</sub>), amplitude (<inline-formula><mml:math id="inf94"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula>), exponent (<inline-formula><mml:math id="inf95"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>), semi-saturation (<inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula>), and offset (<inline-formula><mml:math id="inf97"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula>). We can define a parameter vector <inline-formula><mml:math id="inf98"><mml:mi>P</mml:mi></mml:math></inline-formula> for the QCM as:<disp-formula id="equ19"><mml:math id="m19"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mi mathvariant="normal">⋮</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mi>p</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We fit <inline-formula><mml:math id="inf99"><mml:mi>P</mml:mi></mml:math></inline-formula> to the data by using a non-linear parameter search routine fmincon (Matlab, see <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/help/optim/ug/fmincon.html">https://www.mathworks.com/help/optim/ug/fmincon.html</ext-link>) to find the parameter vector that that minimizes the difference between the measured BOLD fMRI time course <inline-formula><mml:math id="inf100"><mml:mi>Y</mml:mi></mml:math></inline-formula> and the prediction <inline-formula><mml:math id="inf101"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> obtained using the QCM. This takes the form of:<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:msup><mml:mpadded lspace="5pt" width="+5pt"><mml:mi>P</mml:mi></mml:mpadded><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:munder accentunder="true"><mml:mrow><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:mi>min</mml:mi></mml:mrow><mml:mo>𝑃</mml:mo></mml:munder></mml:mpadded><mml:mo>⁢</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mstyle><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>t</mml:mi></mml:mfrac></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>where the objective function we are minimizing is the root mean squared error. Once the values of <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are found for an individual subject, we can use them to predict the BOLD fMRI response to any <inline-formula><mml:math id="inf103"><mml:mi>c</mml:mi></mml:math></inline-formula> within the LM cone contrast plane.</p></sec><sec id="s10" sec-type="appendix"><title>Linear channels model</title><p>The linear channels model (LCM) is based on the work of <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2009</xref>. They develop a model that decodes the hue angle of a stimulus a subject was viewing from BOLD fMRI measurements. The stimuli in their study were modulations in the isoluminant plane of the CIELAB color space, and were parameterized by hue angle within this plane. Their model is based on linear channels tuned to the this angular representation, and it did not explicitly consider the effect of stimulus contrast. This was sufficient for their analysis, as they studied only a single contrast for each angle. To develop a version of the LCM that may be applied to our stimuli, we need both to adopt the concepts to apply to stimuli in the LM contrast plane and to add a model component that handles contrast.</p><p>The angles used in our implementation of the LCM are angles in the LM contrast plane, with 0 corresponding to the positive abscissa, rather than angles in the CIELAB isoluminant plane. These angles are computed from the L- and M-cone contrasts of our stimulus modulations, as the arctangent of the ratio of M- to L-cone contrast:<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf104"><mml:mi>θ</mml:mi></mml:math></inline-formula> is then the angle corresponding to one of our stimulus modulations. </p><p>Given the angular stimulus representation, each linear channel is characterized by its sensitivity to stimulation from each possible stimulus angle - basically a tuning function over angle. More specifically, the tuning functions are implemented as half-rectified cosines raised to an exponent. What differs across each channel is the phase of the tuning function (the peak sensitivity). Thus the sensitivity of the <inline-formula><mml:math id="inf105"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> channel may be written as: <disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf106"><mml:mi>i</mml:mi></mml:math></inline-formula> indicates the channel, <inline-formula><mml:math id="inf107"><mml:mi>n</mml:mi></mml:math></inline-formula> is the exponent which controls the narrowness of the channel tuning functions, and <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the angle of peak sensitivity of the <inline-formula><mml:math id="inf109"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> channel. In calculations, the angle <inline-formula><mml:math id="inf110"><mml:mi>θ</mml:mi></mml:math></inline-formula> is discretized, and both stimuli and mechanism tuning can be represented in matrix-vector form, as we describe below.</p><p>The first step of the model is to compute the channel responses. This is done with a vector representation of the stimulus. Each entry of the stimulus row vector represents one angle, and for the stimulus modulations we used, only one entry is non-zero for any given modulation. In our computations, we discretized angle in one degree steps (the resolution of the representation of <inline-formula><mml:math id="inf111"><mml:mi>θ</mml:mi></mml:math></inline-formula>). The magnitude of the non-zero entry represents the contrast of the modulation. Similarly, the sensitivity of a channel may be represented by a column vector over the same angular discretization, with each entry of the vector being the sensitivity of the channel at the corresponding angle. The dot product of a stimulus vector with a channel vector yields the response of the channel to the stimulus.</p><p>The set of channels can be represented by the columns of a matrix C (nAngles x nChannels) and the set of stimuli represented by a matrix S (nStimuli x nAngles). Therefore, the hypothetical channel outputs, <inline-formula><mml:math id="inf112"><mml:mi>H</mml:mi></mml:math></inline-formula> (nStimuli x nChannels), can be calculated as:<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The overall LCM response to a given stimulus is given as a weighted sum of the individual channel responses (<inline-formula><mml:math id="inf113"><mml:mi>H</mml:mi></mml:math></inline-formula>), with the weights, <inline-formula><mml:math id="inf114"><mml:mi>w</mml:mi></mml:math></inline-formula> (nChannels x nVoxels) acting as parameters of the model. If we consider responses across a set of voxels, as was done by <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2009</xref>, the response <inline-formula><mml:math id="inf115"><mml:mi>b</mml:mi></mml:math></inline-formula> (nStimuli x nVoxels) for a set of stimuli is given by: <disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In our analysis, we fit the LCM to the median time course of V1 and therefore we set <inline-formula><mml:math id="inf116"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>In the context of the LCM, isoresponse contours are made up by the set of stimuli that satisfy the following:<disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf117"><mml:mi>k</mml:mi></mml:math></inline-formula> is a constant target response. </p><p>In their work, <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2009</xref> used six channels and an exponent of 2. The channels had peak sensitivities at 0°, 60°, 120°, 180°, 240° and 300°. Since our stimulus modulations were symmetric around around a background, we enforce that channels offset by 180° used the same weights in the linear combination. This results in three symmetric channels with pairs located at 0° and 180°, 60° and 240°, 120° and 300°.</p><p>One key difference between our experiment and the the original <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2009</xref> work is that we varied the stimulus contrast in each chromatic direction. To extend the LCM to handle contrast, we treated the overall LCM response as an equivalent contrast and passed this through a common Naka-Rushton function. This approach mirrors how we handled contrast in the the QCM. To model the BOLD fMRI response, we convolve the output of the Naka-Rushton with the HRF. We fit the LCM to our data using a method analogous to the one used to fit the QCM. More specifically, we found the three linear channel weights and the Naka-Rushton function parameters that yielded the best fit to the BOLD response time course. These parameters were found simultaneously through the use of MATLAB’s fmincon optimization routine. We also fit a variant of the LCM analogous to a model used by <xref ref-type="bibr" rid="bib41">Kim et al., 2020</xref> with sharper (<inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) channel tuning and using eight rather than six underlying mechanisms. </p><p>Both versions of the LCM yield isoresponse contours similar in shape to those obtained with the QCM (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), with cross-validated <inline-formula><mml:math id="inf119"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> values essentially the same as those obtained using the QCM (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Notably, that the version of the LCM with more channels yielded an isoresponse contour that more closely approximated the isoresponse contour of the QCM than did the version of the LCM with fewer channels. </p><p>A key difference between the LCM and the QCM is in how the functional properties of the model relate to the L- and M-cone contrasts that characterize the early visual system responses to the stimuli. As discussed in the main text, the QCM can be implemented as the sum of the squared responses of two mechanisms, each of which responds as weighted sum of L- and M-cone contrast. Thus underlying the QCM is a quadratic (squaring) non-linearity. In the LCM, on the other hand, the underlying linear channels are tuned for angle in the LM contrast plane. Since angle is obtained as the arctangent of the ratio of M- and L-cone contrasts, the LCM is based on a different form of non-linearity than the QCM, which would entail different neural computations. To put it another way, the apparently simple linear form of the LCM when expressed in terms of stimulus angle is less simple when referred back to the L- and M-cone contrast representation. This same basic point applies to LCMs formulated with respect to hue angle in the CIELAB isoluminant plane, as in <xref ref-type="bibr" rid="bib13">Brouwer and Heeger, 2009</xref> and <xref ref-type="bibr" rid="bib41">Kim et al., 2020</xref>. Measurements of the response to mixtures of modulations might be used in the future to differentiate between the non-linearities embodied by the LCM and the QCM.</p></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65590.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Horwitz</surname><given-names>Gregory D</given-names></name><role>Reviewing Editor</role><aff><institution>University of Washington</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Horwitz</surname><given-names>Gregory D</given-names></name><role>Reviewer</role><aff><institution>University of Washington</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Engel</surname><given-names>Stephen A</given-names></name><role>Reviewer</role><aff><institution>University of Minnesota</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/10.1101/2020.12.03.410506">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.12.03.410506v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>A key step towards understanding the cortical representation of color is to develop mathematical models that accurately and parsimoniously describe cortical representations of broad families of colorful stimuli. This study achieves this goal with the quadratic color model, a new benchmark f-or the quantitative assessment of cortical light responses that provides new insights into the underlying mechanisms.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A Quadratic Model Captures the Human V1 Response to Variations in Chromatic Direction and Contrast&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Gregory D Horwitz as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Tirin Moore as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Stephen A Engel (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>(1) The paper would be strengthened by a more thorough analysis of the QCM: comparison of the QCM to models more biologically plausible than the GLM, residuals analyzed for meaningful patterns, and quality-of-fit metrics compared with those of previous studies.</p><p>(2) The new results should be discussed in the context of previous EEG and fMRI studies.</p><p>(3) Trends in the data should be compared with predictions from psychophysics.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The quality of QCM fits could be appraised more rigorously. Are there any consistent patterns in the residuals? If so, what is the nature of the systematic deviations from the QCM, how big are they, and what might they indicate about the underlying biology?</p><p>The GLM allows jagged and non-monotonic contrast-response functions, which is unrealistic. A more useful benchmark would be a model in which contrast-response functions have a Naka-Rushton form, like they do in the QCM.</p><p>Neurons in the LGN respond more strongly to 12 Hz, full-field modulations than do most neurons in V1. One possibility is that much of the BOLD signal recorded from V1 under the conditions used in this study reflects the activity of LGN afferents. Some comments on this and potential consequences for the activation of V1 and beyond would be useful.</p><p>The changes in minor axis ratio and ellipse orientation with eccentricity are modest, but it is unclear how large we should expect them to be. Predictions from psychophysics, neurophysiology, or anatomy would provide a more useful benchmark than estimates of measurement error (&quot;vertical spread of values at each eccentricity&quot; and &quot;set-to-set differences in parameters values&quot;).</p><p>More information should be giving regarding the bootstrap calculation of the 68% confidence intervals. Was the percentile method used? Was a normal distribution of parameter estimates assumed ({plus minus}1 SD of bootstrap resamples)?</p><p>Line 171: The measured BOLD percent signal change is described as &quot;black line&quot; in the text but more accurately as &quot;thin gray line&quot; in the figure legend.</p><p>Lines 626 and 627: Is an experimental run distinct from a functional run?</p><p>The discussions of cone fundamentals changing with eccentricity and contrast-response functions changing with eccentricity might be easier to understand if they were separated (both are described in the paragraph spanning lines 422-459).</p><p>The legends to Figure 12 and Supplementary Figure 9 are missing at least one line of text.</p><p>Typos: &quot;Varience&quot;, &quot;grayoordinate&quot;, &quot;2-dimentional&quot;, &quot;…and the bottom set of the show data for…&quot;</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>This paper presents an important data set and model, and the successes of the paper are laid out in the other portions of this review process. Here I detail specific concerns that could be addressed in revision. Larger ones fall into four categories</p><p>1. Response patterns could be characterized even more completely, aiding readers.</p><p>Line 175: How does this R<sup>2</sup> of the GLM compare to those reported in prior work? It would be good to know if it is in the standard range, which I bet it is.</p><p>Figure 2: Potting more of the results in terms of percent signal change rather than arbitrary units would allow comparison with previous results. This might also help readers interpret the lack of saturation of response as a function of contrast.</p><p>Line 281: It would be good to give readers a better sense of what responses are like outside of V1. Specifically, what quantitative estimates are there to support the claim that the stimuli are not driving activity beyond V1, even in V4 or V01? One solution might be to show a map of GLM variance explained throughout cortex, with some sample timecourses shown for V4ish and V01ish ROIs. It also may not be clear to non-specialists that neurons beyond V1 require spatial contrast to drive response; this could probably be highlighted more.</p><p>2. Additional comparisons of GLM and QCM.</p><p>General: While agreement between GLM and QCM is good, are there small but systematic differences? It might be helpful to include some sort of plot of residuals of the QCM fits. I think that is part of the point of Figure 7, but one could plot residuals explicitly, and it is not clear from the figure (at least to me) whether this plot is evaluating fits of just the contrast non-linearity or the entire QCM model. Figure 8: Another additional analysis that might be useful is a spatial map of a GLM to QCM comparison. It seems possible, at least in principle, that there are parts of the brain where the difference between the two models could be larger than in V1.</p><p>3. Discussion of other work.</p><p>While this is generally quite thorough, the manuscript would be strengthened by discussing the two additions mentioned in the public review: EEG results (e.g.Baseler and Sutter), and Brouwer and Heeger's &quot;channel&quot; model of color responses in cortex.</p><p>4. Discussion of implications for neurons in V1. A few additions to the discussion might be helpful. First, could the present results be dominated by input to V1 rather than action potentials generated within V1? This merits some discussion. Relatedly, readers might like to know if the present data set covered the LGN, which could be used in future work to address this issue.</p><p>Second, as mentioned above, additional discussion of what single unit work says about what kind of neurons will give strong responses to spatially uniform stimuli, and where they are located in cortex, could be useful.</p><p>Finally, and most trickily I suppose, is what conclusions about neurons can one make from the good QCM fit itself (besides lack of eccentricity effects). Can one conclude that for these stimulus conditions neurons in V1 whose preferred color direction is at or near L-M have higher gain than neurons whose preferred color direction is at or near L+M? I realize that there are a lot of assumptions (e.g. about separability and pooling of the signals by the fMRI response) being made in such a statement. I guess one could also build an &quot;LGN-type&quot; model that assumes all signal comes from just L+M or L-M tuned neurons, which could fit comparably to the QCM, but potentially with fewer parameters, since it assumes a 45 degree orientation of one population and a 135 degree orientation of another (almost equivalently a 2 channel version of the Brouwer and Heeger model). This is hinted at in line 370 in terms of psychophysics, but the model can be constrained by single unit data, at least from LGN. Is there anything useful one could conclude from such fits?</p><p>A final possible form this discussion could take is to tackle the question: Are there plausible neural response patterns that would be expected to not be well-fit by the QCM, and so the present data discomfirm them? I understand the authors' desire to not be speculative, but readers may already be speculating.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I think the main weakness here is the link between biology and data. Is it possible to implement a simple but biologically plausible population model of color processing in V1 (involving just the low SF-sensitive neurons) and test it against the data here? Such a model might have more than 6 parameters.… but fewer than 40: as the authors point out, the data are consistent with a model that sums approximately independent inputs from L-M and L+M channels. The authors might then discuss (qualitatively) how they expect the response functions to change as other populations of chromatically-sensitive neurons are excited by, for example, the presence of spatial structure.</p><p>Psychophysical data would also benefit the paper. I know there would be a lot of conditions – but with efficient staircases, data could be collected on at least some subjects to examine, for example, the observation that model parameters do not change across eccentricity (a surprising and interesting result!). Absent that, some more quantitative comparisons with existing psychophysical data (and neuroimaging data – for example Brouwer and Heeger's work) would be nice.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65590.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The quality of QCM fits could be appraised more rigorously. Are there any consistent patterns in the residuals? If so, what is the nature of the systematic deviations from the QCM, how big are they, and what might they indicate about the underlying biology?</p></disp-quote><p>We agree with Reviewers 1 and 2 that an examination of model residuals could reveal systematic deviations from, and thus limitations of, the QCM. To address this, we examined the residuals for both the GLM and the QCM for each subject and session. We approached this as follows:</p><p>First, we analyzed the residuals as a function of stimulus condition, examining each direction/contrast pair separately. To do so, we plotted the residuals of the GLM and QCM fit to the BOLD signal time course over the 14 TRs after the start of each stimulus block. This produced a total of 10 residual curves per model and direction/contrast pair. We did not observe in this analysis any systematic variation in the residuals as a function of contrast level within a single chromatic direction.</p><p>Given this observation, and as a way to summarize the data, we examined the mean residual value, taken from 4 to 14 TRs after stimulus onset, for all trials in a color direction (collapsed over contrast). To check for systematic deviations in fit, we plotted these mean residuals for both the GLM and the QCM, separated by chromatic direction, for each subject and session. We examined these plots for consistent patterns. We observe no such patterns within or across models. Note in particular that the GLM has separate parameters for each stimulus direction and that the residual values for the GLM and QCM mostly overlap. The figure has been added to the manuscript as Figure 3 – supplemental figure 3 and the main text (section “Comparison of GLM and QCM”, starting on Line 227 of the main text) has been updated to convey this point.</p><disp-quote content-type="editor-comment"><p>The GLM allows jagged and non-monotonic contrast-response functions, which is unrealistic. A more useful benchmark would be a model in which contrast-response functions have a Naka-Rushton form, like they do in the QCM.</p></disp-quote><p>The reviewer brings up the good point that the non-monotonicity of the GLM fit to contrast response makes it likely on prior grounds that the GLM is overfitting the data. To provide a benchmark without this issue, we fit the data with models intermediate to the fully free GLM and the more constrained QCM. The most general of these models fit the data in each color direction with a separate Naka-Rushton function, allowing all but the offset parameters of this function to be independent across chromatic directions. More constrained versions of this model yoked additional Naka-Rushton parameters across directions. We explored locking the amplitude parameter (in addition to the offset), the exponent parameter (in addition to the offset), and yoking the amplitude, exponent, and offset parameters (allowing only the semi-saturation parameter to vary with chromatic direction).</p><p>To evaluate how well these models fit the data, we ran the same cross-validation procedure we used previously to compare the GLM to the QCM. Figure 4—figure supplement 1 shows this result for Subject 2 for both measurement sets (ignore the LCM BH, LCM Kim, and QCM locked bars in that plot for now; these will be described in response to other reviewer comments below). The magnitude of the cross-validated R<sup>2</sup> for all of these Naka-Rushton models was only slightly higher than the GLM and not higher than the QCM. These results were similar for all subjects and measurement sets.</p><p>The primary conclusions from this analysis are that any overfitting by GLM is small and that the constraint across color directions imposed by the QCM does not decrease the cross-validated model fit relative to models that simply imposed by a smooth-monotonicity constraint on the contrast-response functions in each chromatic direction. We have updated the text in the section “Characterizing Cortical Responses with a Conventional GLM” (Line 157) to reflect these results. We also provide the cross-validation figures with the Naka-Rushton models for all subjects and measurement sets in the supplemental figures, replacing the previous cross-validation figures (Figure 4- supplemental figure 1). We left the cross-validation figure in the main text unchanged, so as not to take the more casual reader too far afield. For a similar reason, we retain the GLM as our baseline model.</p><p>Also note that we have removed the error bars from the cross-validation R<sup>2</sup> values – we had computed these incorrectly in the initial submission. Although it would be possible to put error bars on these values by running the cross-validation analyses within a bootstrapping loop, this would be computationally intensive and we do not think doing so would provide additional useful information.</p><disp-quote content-type="editor-comment"><p>Neurons in the LGN respond more strongly to 12 Hz, full-field modulations than do most neurons in V1. One possibility is that much of the BOLD signal recorded from V1 under the conditions used in this study reflects the activity of LGN afferents. Some comments on this and potential consequences for the activation of V1 and beyond would be useful.</p></disp-quote><p>We agree. Our data do not distinguish the extent to which the response properties of the BOLD signals we measure in V1 are inherited from the LGN or are shaped by processing within V1.</p><p>To address this point, we have revised the discussion. We now point out that, even though the signals measured in our experiment are spatially localized to V1, we cannot ascribe the observed response properties to particular neural processing sites. As such, the V1 sensitivities found from fitting the QCM may be inherited from areas prior to V1. In principle, they could also be affected by feedback from other cortical areas. This text starts on Line 511 of the paper.</p><p>We also undertook an analysis of signals from the LGN in our data. Specifically, we interrogated the BOLD fMRI signal within individual subject subcortical segmentations defined using Freesurfer 7 and then registered to CIFTI space. Using this, we were able to extract a median time course for the LGN and fit the QCM in the same manner that we fit the time course for V1. For this analysis, the data from the two measurement sets within a subject were aggregated to increase signal-to-noise. Even so, data from two of the three subjects were too noisy to reveal reliable stimulus-driven responses. In one subject we were able to fit the QCM to data from the LGN, and found a similar minor axis ratio and ellipse angle as we found in V1. Yet even for this subject, responses were noisy, with a response amplitude approximately half of what we found in V1. Overall, we do not believe that the LGN responses are sufficiently well measured in our data for us to include these observations in the paper.</p><disp-quote content-type="editor-comment"><p>The changes in minor axis ratio and ellipse orientation with eccentricity are modest, but it is unclear how large we should expect them to be. Predictions from psychophysics, neurophysiology, or anatomy would provide a more useful benchmark than estimates of measurement error (&quot;vertical spread of values at each eccentricity&quot; and &quot;set-to-set differences in parameters values&quot;).</p></disp-quote><p>In the original submission, we presented variation in QCM parameters across measurement sets and across voxels at similar eccentricities. These help estimate measurement precision: changes in parameter values that are consistently outside these ranges would point towards <italic>significant</italic> effects with eccentricity. We agree, however, that the original treatment did not address what might be a <italic>meaningful</italic> change with eccentricity. Consideration of the latter question is worthwhile, and in preparing this revision we have added an analysis and reworked the discussion to better address this question and to better set our measurements in the context of the prior literature. The relevant section of the discussion is “Change of Chromatic Sensitivity with Eccentricity” and begins on Line 531 of the paper.</p><p>The new analysis, shown in Figure 9 – supplemental figure 2 (see Results, Lines 350-363), uses the voxel-wise QCM fits to evaluate how L-M sensitivity and L+M sensitivity vary with eccentricity, a form of the data similar to that provided in earlier fMRI studies of V1. This analysis allows fairly direct comparison to what we think are the most directly relevant prior studies, those that used fMRI in V1 to examine this same question. Our results are in fairly striking contrast with the data of Mullen et al. (2007; their Figure 8) and Vanni et al. (2006; also their Figure 8), which we could take as measurements that define the magnitude of a meaningful effect of eccentricity. Summarizing their experiments and the size of their effects is not easy to do succinctly, so we simply refer the reader to the relevant figures in these two papers and note that if effects of that magnitude had been present in our data, they would have been apparent in our new analysis. At the same time, we note our data are consistent with those from a third prior study, that of D’Souza et al. (2016), who found little or no change in relative L-M and isochromatic sensitivity with increasing eccentricity in V1. We discuss possible reasons for these differences in results across studies (stimulus differences and some methodological issues), but a reconciliation of all of the studies is not currently in hand.</p><p>Prior psychophysical studies tell a fairly consistent story of a decline in L-M sensitivity relative to L+M sensitivity with increasing eccentricity, and we now acknowledge these studies more fully (see text starting at Line 547). We think direct comparison with psychophysics is tenuous because in general the patterns of BOLD fMRI responses in V1 do not always mirror effects found using psychophysical measurements of sensitivity (see section “Relation to Psychophysics in Discussion (Line 424)), so we don’t make much of this beyond noting it.</p><disp-quote content-type="editor-comment"><p>More information should be giving regarding the bootstrap calculation of the 68% confidence intervals. Was the percentile method used? Was a normal distribution of parameter estimates assumed ({plus minus}1 SD of bootstrap resamples)?</p></disp-quote><p>Indeed, we used percentiles to estimate error. We used 68% confidence intervals since this approximates +/- 1 SEM for a normal distribution. We have updated the text in section “Error Bar Generation” to make this clear.</p><disp-quote content-type="editor-comment"><p>Line 171: The measured BOLD percent signal change is described as &quot;black line&quot; in the text but more accurately as &quot;thin gray line&quot; in the figure legend.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>Lines 626 and 627: Is an experimental run distinct from a functional run?</p></disp-quote><p>Good catch. These are the same thing and the wording has been unified in the revision.</p><disp-quote content-type="editor-comment"><p>The discussions of cone fundamentals changing with eccentricity and contrast-response functions changing with eccentricity might be easier to understand if they were separated (both are described in the paragraph spanning lines 422-459).</p></disp-quote><p>We have updated the text in an attempt to better explain these concepts, and made the points noted here in separate paragraphs (now starting on Line 584).</p><disp-quote content-type="editor-comment"><p>The legends to Figure 12 and Supplementary Figure 9 are missing at least one line of text.</p><p>Typos: &quot;Varience&quot;, &quot;grayoordinate&quot;, &quot;2-dimentional&quot;, &quot;…and the bottom set of the show data for…&quot;</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>This paper presents an important data set and model, and the successes of the paper are laid out in the other portions of this review process. Here I detail specific concerns that could be addressed in revision. Larger ones fall into four categories.</p><p>1. Response patterns could be characterized even more completely, aiding readers.</p><p>Line 175: How does this R<sup>2</sup> of the GLM compare to those reported in prior work? It would be good to know if it is in the standard range, which I bet it is.</p></disp-quote><p>We gave this some thought, and went looking for comparable studies to offer, but ultimately decided that this quantitative comparison is difficult to properly frame. Comparison of R<sup>2</sup> values across studies is tricky, given that variance explained will be affected not only by how well the model accounts for the underlying measurements, but also by other factors that affect SNR and that may differ across studies. A particular challenge here is that we fit the concatenated (as opposed to average) time-series data across multiple acquisitions and sessions. As a consequence, the R<sup>2</sup> values will be lower than in those studies that have repeated presentations of the same stimulus sequence and thus have performed signal averaging prior to model fitting and subsequent report of R<sup>2</sup> values. For these reasons we don’t have a particular comparison set of R<sup>2</sup> values to provide from other studies. We continue to feel, however, that expressing the quality of our model fit in terms of R<sup>2</sup> provides a useful metric for comparison across subjects and runs within our study. We now mention these points (see Lines 404).</p><disp-quote content-type="editor-comment"><p>Figure 2: Potting more of the results in terms of percent signal change rather than arbitrary units would allow comparison with previous results. This might also help readers interpret the lack of saturation of response as a function of contrast.</p></disp-quote><p>We agree that presenting work in a way that is interpretable in context of other studies is important. To accomplish this goal, we provide plots of the BOLD fMRI time course, in units of percent signal change, together with the model predictions.</p><disp-quote content-type="editor-comment"><p>Line 281: It would be good to give readers a better sense of what responses are like outside of V1. Specifically, what quantitative estimates are there to support the claim that the stimuli are not driving activity beyond V1, even in V4 or V01? One solution might be to show a map of GLM variance explained throughout cortex, with some sample timecourses shown for V4ish and V01ish ROIs. It also may not be clear to non-specialists that neurons beyond V1 require spatial contrast to drive response; this could probably be highlighted more.</p></disp-quote><p>To address this, we have fit the GLM vertex-wise to the time series data from the whole brain. We generally found that the variance explained in vertices outside of V1 was close to zero, with the exception of a small patch of values in the vicinity of V4/VO1. The GLM variance explained in this area was roughly half of that explained within V1. To further examine these regions, we used the subject-specific registration of the retinotopic atlas from Wang et al., as implemented by Neuropythy. We then transferred the V4 and the VO1 ROIs into HCP CIFTI space and extracted the median time series for each ROI. With these median time series, we fit the QCM in V4 and VO1 for all subjects and measurement sets. The parameters of the QCM fit for hV4 and VO1 were generally consistent with those of V1, although fit quality was overall worse. While examining signals in downstream areas is an exciting research direction, as with our measurements of the LGN, we feel that the data quality prevents us from making concrete statements about the response in these areas in this paper. We provide the average variance explained map for early visual cortex (EVC, the spatial extent of the Benson template) for the GLM. The spatial extent of these maps includes hV4 and VO1. We have added these average EVC maps as Figure 8 figure supplemental 1, and indicate in section “Isoresponse Contour Parameter Maps” (Line 300) that the responses outside of V1 were not sufficiently reliable for us to report model fits to those responses.</p><disp-quote content-type="editor-comment"><p>2. Additional comparisons of GLM and QCM.</p><p>General: While agreement between GLM and QCM is good, are there small but systematic differences? It might be helpful to include some sort of plot of residuals of the QCM fits. I think that is part of the point of Figure 7, but one could plot residuals explicitly, and it is not clear from the figure (at least to me) whether this plot is evaluating fits of just the contrast non-linearity or the entire QCM model.</p></disp-quote><p>We agree. We have addressed this as described in our response to a similar suggestion from Reviewer 1, above.</p><disp-quote content-type="editor-comment"><p>Figure 8: Another additional analysis that might be useful is a spatial map of a GLM to QCM comparison. It seems possible, at least in principle, that there are parts of the brain where the difference between the two models could be larger than in V1.</p></disp-quote><p>We agree that this is a good check. We took the difference between the GLM and the QCM variance explained maps, averaged across subjects. This difference map is provided <xref ref-type="fig" rid="respfig1">Author response image 1</xref>. We did not observe any large deviations in fit in the EVC region of interest. The map contains only positive values showing that the GLM had higher R<sup>2</sup> values, as expected since the QCM is a parametrized subset of the GLM. The largest difference in variance explained between the maps is 0.03, demonstrating nearly equivalent performance in non-cross-validated model fit. We did not add this figure, although we could if the reviewers or and editor feel it important to do so; we do now, however, provide both of the variance explained maps used to produce this figure (Figure 8 (bottom row) and Figure 8 figure supplemental 1).</p><fig id="respfig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65590-resp-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>3. Discussion of other work.</p><p>While this is generally quite thorough, the manuscript would be strengthened by discussing the two additions mentioned in the public review: EEG results (e.g.Baseler and Sutter), and Brouwer and Heeger's &quot;channel&quot; model of color responses in cortex.</p></disp-quote><p>We thank the reviewer for pointing out these papers for comment. We now cite the Baseler and Sutter paper in the section “Change of Chromatic Sensitivity with Retinal Eccentricity” as an example of a prior measurement of variation in chromatic sensitivity with eccentricity. We now also treat the Brouwer and Heeger channel model; this treatment is described fully in the response to Reviewer 3 below.</p><disp-quote content-type="editor-comment"><p>4. Discussion of implications for neurons in V1. A few additions to the discussion might be helpful. First, could the present results be dominated by input to V1 rather than action potentials generated within V1? This merits some discussion. Relatedly, readers might like to know if the present data set covered the LGN, which could be used in future work to address this issue.</p></disp-quote><p>Indeed, the BOLD signals we measured in V1 may inherit their response properties from LGN inputs. Reviewer 1 made a similar comment, please see response above.</p><disp-quote content-type="editor-comment"><p>Second, as mentioned above, additional discussion of what single unit work says about what kind of neurons will give strong responses to spatially uniform stimuli, and where they are located in cortex, could be useful.</p></disp-quote><p>We have added text in the discussion to the section now titled “Relation to Underlying Mechanisms” to discuss briefly the relationship between our stimuli and responses of known cell types. See text in paragraph starting on Line 490.</p><disp-quote content-type="editor-comment"><p>Finally, and most trickily I suppose, is what conclusions about neurons can one make from the good QCM fit itself (besides lack of eccentricity effects). Can one conclude that for these stimulus conditions neurons in V1 whose preferred color direction is at or near L-M have higher gain than neurons whose preferred color direction is at or near L+M? I realize that there are a lot of assumptions (e.g. about separability and pooling of the signals by the fMRI response) being made in such a statement. I guess one could also build an &quot;LGN-type&quot; model that assumes all signal comes from just L+M or L-M tuned neurons, which could fit comparably to the QCM, but potentially with fewer parameters, since it assumes a 45 degree orientation of one population and a 135 degree orientation of another (almost equivalently a 2 channel version of the Brouwer and Heeger model). This is hinted at in line 370 in terms of psychophysics, but the model can be constrained by single unit data, at least from LGN. Is there anything useful one could conclude from such fits?</p></disp-quote><p>As with the comment just above, we have revised the discussion to be more elaborated on this point. See paragraphs starting at Line 490 down to the end of that section of the discussion.</p><disp-quote content-type="editor-comment"><p>A final possible form this discussion could take is to tackle the question: Are there plausible neural response patterns that would be expected to not be well-fit by the QCM, and so the present data discomfirm them? I understand the authors' desire to not be speculative, but readers may already be speculating.</p></disp-quote><p>To address this general point, we added an analysis to show that there are parametric isoresponse contour shapes that our data have sufficient power to reject. To do this, we fit and cross-validated a form of the QCM with the angle constrained to by 0 degrees. See the bar plot provided in Figure 4—figure supplement 1, cross-validated R<sup>2</sup> labeled QCM locked. The lower cross-validated R<sup>2</sup> for this model makes the point, and that R<sup>2</sup> is now provided as part of Figure 4 figure supplement 1. We treat this in the discussion in paragraph starting at Line 481</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I think the main weakness here is the link between biology and data. Is it possible to implement a simple but biologically plausible population model of color processing in V1 (involving just the low SF-sensitive neurons) and test it against the data here? Such a model might have more than 6 parameters.… but fewer than 40: as the authors point out, the data are consistent with a model that sums approximately independent inputs from L-M and L+M channels. The authors might then discuss (qualitatively) how they expect the response functions to change as other populations of chromatically-sensitive neurons are excited by, for example, the presence of spatial structure.</p></disp-quote><p>To our knowledge, the only biologically plausible population model of V1 color processing that incorporates spatial frequency information in the prediction of the BOLD response is the model by Schluppeck and Engel (2002). Their model is based on the primate electrophysiological data of Johnson et al. (2001) and from this, produces isoresponse contours which were compared to BOLD fMRI isoresponse contours. The Schluppeck and Engel model at low spatial frequencies has spatial sensitivity differences for ‘color’, ‘color-luminance’ and ‘luminance’ cells roughly on the order of the average minor axis ratio in our study. Further, given that we have no spatial frequency variations, any variations in model output for our stimuli are driven by the color stage of the model. Schluppeck and Engel showed that the shapes of their predicted isoresponse contours were elliptical and matched those of Engel et al. 1997. Therefore, we believe that their model is consistent with the QCM. Our revised discussion of the link to underlying mechanisms is now in section “Relation to Underlying Mechanisms” starting on Line 449. We discuss some ideas about how one might proceed to think about incorporating spatial structure into measurements and models in the last paragraph of the paper, starting at Line 638.</p><p>We take the point that the high number of parameters of the GLM are highly unlikely to correspond to an equally high number of underlying mechanisms in V1, and we do not mean to imply this. We use the GLM as a comparison point that allows each stimulus condition to have an individual response weighting independent of the other conditions which will provide the best fit to the data, of the models tested. As an approach to the question of what happens as one adds more parameters than the QCM has, but fewer than the GLM, we now fit our data with two variants of the Brouwer and Heeger (2009) model, one having the same number of parameters as the QCM, and a variant of this model as implemented by Kim et al. (2020), which has more parameters. This exercise is described in more detail just below.</p><disp-quote content-type="editor-comment"><p>Psychophysical data would also benefit the paper. I know there would be a lot of conditions – but with efficient staircases, data could be collected on at least some subjects to examine, for example, the observation that model parameters do not change across eccentricity (a surprising and interesting result!). Absent that, some more quantitative comparisons with existing psychophysical data (and neuroimaging data – for example Brouwer and Heeger's work) would be nice.</p></disp-quote><p>We have addressed this comment in two ways. First, we now discuss the psychophysical literature that argues that very careful measurements of isothreshold contours do not reject the hypothesis that these contours are elliptical (section now called “Relation to Psychophysics”, Line 424). Second, as described above we have elaborated our discussion of the effects of eccentricity.</p><p>In terms of fitting the QCM to other datasets. A key feature of the QCM is the fact that it provides an account of how response varies with both chromatic direction and contrast, and indeed estimating isoresponse contours requires either using some sort of procedure that equates response across chromatic directions or (as we have done) varies both chromatic direction and contrast. The Brouwer and Heeger dataset, for example, uses only one contrast per chromatic direction and leverages the variation of response with chromatic direction in its decoding analysis. As a consequence, the Brouwer and Heeger dataset for example would not be appropriate for testing the QCM or more generally any model that describes the shape of the isoresponse contour.</p><p>Reviewer 2 suggests that comparison to a model intermediate to the GLM and QCM would be of interest. To address this, we have added additional model comparisons to the paper. As suggested, we used the work of Brouwer and Heeger as a point of departure. They developed a model based on mechanisms tuned according to cos<sup>2</sup> functions of angle in the isoluminant plane, generated as the squares of linear response mechanisms in the CIELAB color space. They then modelled data collected at a single contrast using a linear combination of the responses of these mechanisms, which implies an isorespone contour. Rather than allowing mechanism tuning to vary, they varied the weights with which the mechanism outputs were combined.</p><p>To apply these concepts to our dataset, we formed mechanisms as cos<sup>2</sup> functions of angle in the LM contrast plane, and combined these linearly to define isocontrast contours. Since our stimuli are symmetric modulations around a white point, we used three symmetric rather than six independent mechanisms. To handle the variations in contrast, we passed the output of the linearly summed mechanisms through a common Naka-Rushton function. We call this the Linear Channels Model (LCM). As we now show in the supplement, this approach accounts for our data about as well as the QCM, and produces a similar isoresponse contour, although the detailed shape varies from that of the ellipse.</p><p>More generally, any model that can describe an isoresponse contour of the general shape we observe is likely to fit the data equally well. We also fit a variant of the LCM with sharper tuning and more parameters (Kim et al. 2020) and show that this again leads to similar fit quality and isoresponse contours. We fit this model to all subjects and measurement sets. Figure 6—figure supplement 1 shows the isoresponse contours derived from the LCM for both the Brouwer and Heeger model and the Kim et al. model plotted with the QCM ellipse for Subject 2. These contours are similar, with the LCM contours having small wiggles around the ellipse. Notably, with more mechanisms, the LCM contours approach the form of an ellipse, and not more convoluted shapes. To assess how well this model performed, we employed the same cross-validation routine used to compare all other models. The results of this can be seen in the cross-validated R<sup>2</sup> bar plot in Figure 4—figure supplement 1. This plot compares the cross-validated variance explained by the GLM, the Naka-Rushton variants, the QCM, and both versions of the LCM. There is very little difference in cross-validated R<sup>2</sup> between the QCM and the two variants of the LCM, with the GLM just slightly worse.</p><p>Thus our data tell us the basic shape of the underlying isoresponse contours, but there are limits to the inferences we can make about the underlying mechanisms that produce the contours. We present these ideas in detail in the Appendix 1, and refer to them in the Discussion of the main paper (section “Relation to Underlying Mechanisms”, Line 449, Figure 6—figure supplement 1). The Appendix 1 also provides some brief comments on differences in the required mechanistic operations required to implement the QCM and LCM.</p></body></sub-article></article>