<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">95802</article-id><article-id pub-id-type="doi">10.7554/eLife.95802</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.95802.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A statistical framework for analysis of trial-level temporal dynamics in fiber photometry experiments</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Loewinger</surname><given-names>Gabriel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0755-8520</contrib-id><email>gloewinger@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Cui</surname><given-names>Erjia</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Lovinger</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Pereira</surname><given-names>Francisco</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>Machine Learning Core, National Institute of Mental Health</institution></institution-wrap><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/017zqws13</institution-id><institution>Division of Biostatistics and Health Data Science, University of Minnesota</institution></institution-wrap><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jzrsm59</institution-id><institution>Laboratory for Integrative Neuroscience, National Institute on Alcohol Abuse and Alcoholism</institution></institution-wrap><addr-line><named-content content-type="city">Rockville</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Iordanova</surname><given-names>Mihaela D</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0420zvk78</institution-id><institution>Concordia University</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>12</day><month>03</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP95802</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-01-15"><day>15</day><month>01</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-01-15"><day>15</day><month>01</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.11.06.565896"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-03-04"><day>04</day><month>03</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95802.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-10-25"><day>25</day><month>10</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.95802.2"/></event></pub-history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-95802-v1.pdf"/><abstract><p>Fiber photometry has become a popular technique to measure neural activity in vivo, but common analysis strategies can reduce the detection of effects because they condense <italic>within-trial</italic> signals into summary measures, and discard trial-level information by averaging <italic>across-trials</italic>. We propose a novel photometry statistical framework based on functional linear mixed modeling, which enables hypothesis testing of variable effects at <italic>every trial time-point</italic>, and uses trial-level signals without averaging. This makes it possible to compare the timing and magnitude of signals across conditions while accounting for between-animal differences. Our framework produces a series of plots that illustrate covariate effect estimates and statistical significance at each trial time-point. By exploiting signal autocorrelation, our methodology yields <italic>joint</italic> 95% confidence intervals that account for inspecting effects across the entire trial and improve the detection of event-related signal changes over common multiple comparisons correction strategies. We reanalyze data from a recent study proposing a theory for the role of mesolimbic dopamine in reward learning, and show the capability of our framework to reveal significant effects obscured by standard analysis approaches. For example, our method identifies two dopamine components with distinct temporal dynamics in response to reward delivery. In simulation experiments, our methodology yields improved statistical power over common analysis approaches. Finally, we provide an open-source package and analysis guide for applying our framework.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>statistics</kwd><kwd>neuroscience</kwd><kwd>fiber photometry</kwd><kwd>functional data analysis</kwd><kwd>dopamine</kwd><kwd>mixed models</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>ZIC-MH002968</award-id><principal-award-recipient><name><surname>Loewinger</surname><given-names>Gabriel</given-names></name><name><surname>Pereira</surname><given-names>Francisco</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000027</institution-id><institution>National Institute on Alcohol Abuse and Alcoholism</institution></institution-wrap></funding-source><award-id>ZIAAA000416</award-id><principal-award-recipient><name><surname>Lovinger</surname><given-names>David</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A fiber photometry analysis framework based on functional mixed models enhances the detection of effects by testing signal-variable associations at each trial timepoint and accounting for between-animal heterogeneity.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Fiber photometry is a photonic technique used to measure neural activity in vivo. The assay quantifies bulk fluorescence emitted from fluorescent biosensors that detect neurotransmitters or physiological processes (e.g. calcium influx) with high neurochemical and cell-type specificity (<xref ref-type="bibr" rid="bib13">Cui et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Gunaydin et al., 2014</xref>; <xref ref-type="bibr" rid="bib42">Simpson et al., 2024</xref>). The popularity of photometry has increased nearly exponentially since its development (<xref ref-type="bibr" rid="bib13">Cui et al., 2013</xref>; <xref ref-type="bibr" rid="bib20">Gunaydin et al., 2014</xref>), with roughly 1500 references to it in the last year alone (see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> for an analysis of the number of references to photometry). Although photometry is an invaluable tool, there is little consensus on analysis strategies for the data produced. Many common analysis procedures were not designed for photometry, specifically, but rather grew organically out of adapting approaches historically applied in the cyclic voltammetry (<xref ref-type="bibr" rid="bib33">Phillips et al., 2003</xref>; <xref ref-type="bibr" rid="bib22">Heien et al., 2005</xref>), EEG (<xref ref-type="bibr" rid="bib2">Adrian and Matthews, 1934</xref>), and electrophysiology communities (<xref ref-type="bibr" rid="bib17">Fatt and Katz, 1952</xref>). Arguably the most common photometry analysis strategy proceeds by: (1) averaging event-aligned signals across trials (‘trial-averaging’) and animals for comparison of different conditions (e.g. treatment/control), (2) graphing each condition’s average signal (‘trace’), (3) calculating a signal summary measure (e.g. Area Under the Curve [AUC]), and (4) conducting hypothesis tests (e.g. ANOVA) on that summary statistic.</p><p>Although these analysis conventions are parsimonious, they may dilute important patterns in the data related to, for example, individual animal differences in the timing and magnitude of signals, and the evolution of signals across trials. Part of the appeal of photometry is that probes can be implanted chronically, thereby enabling its application in sophisticated multi-session (‘longitudinal’) experiments. Such designs yield, however, complex datasets in which associations between the signal and experimental variables can vary across trials (e.g. due to learning) and animals. To illustrate this, we present a typical analysis of photometry data (<xref ref-type="bibr" rid="bib11">Coddington et al., 2023</xref>) in <xref ref-type="fig" rid="fig1">Figure 1</xref>. These measurements were collected on mesolimbic dopamine neurons in well-trained, head-fixed animals performing a Pavlovian reward learning task. <xref ref-type="fig" rid="fig1">Figure 1A</xref> shows that the signals exhibit considerable heterogeneity across animals, suggesting that it can be difficult to identify one summary measure that captures the target effect in all subjects. Even within-animal, traces are highly variable across conditions (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), trials within-session (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), and sessions (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). These figures illustrate how averaging the signal <italic>across trials</italic> can obscure behavior–signal associations, and how summarizing <italic>within trial</italic> signals (e.g. with AUC) can reduce one’s ability to distinguish between trial-level signals that differ in dynamics, but yield similar summary values.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Variability in photometry signals highlights the need for trial-level analyses.</title><p>Signals were recorded from a Pavlovian task in which reward-delivery (sweetened water) followed a stimulus-presentation (0.5 sec auditory cue) after a 1 sec delay. Signals are aligned to cue-onset. (<bold>A</bold>) Signals exhibit heterogeneity across animals. Each trace is a trial-averaged signal on one session for one animal. (<bold>B</bold>) Signals exhibit heterogeneity across animals in the effect of condition. Each trace is from one animal on the same session as in (<bold>A</bold>). Signals were separately averaged across trials in which animals did (Lick+) or did not (Lick-) engage in anticipatory licking. Each trace represents the pointwise <italic>difference</italic> between average Lick+ and Lick- signals. (<bold>C</bold>) Signals exhibit heterogeneity across trials within animal. Each trace is a randomly selected trial from the same animal in the same session. (<bold>D</bold>) Signals exhibit heterogeneity across sessions. Each trace plotted is the trial-averaged signal for one session for one subject. (<bold>E</bold>) Illustration of common summary measures. Depending on the authors, Area-Under-the-Curve (AUC) can be the area of the shaded region or the average signal amplitude. (<bold>F</bold>) Example hypothesis test of Lick+/Lick- differences using peak amplitude as the summary measure. All signals are measurements of calcium dynamics from axons of mesolimbic dopamine neurons recorded from fibers in the nucleus accumbens (<xref ref-type="bibr" rid="bib11">Coddington et al., 2023</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-fig1-v1.tif"/></fig><p>Despite the complexity of the data, there are few analysis methods developed specifically for photometry. <italic>Encoding models</italic> of point-by-point signal-behavior relationships have been used for predicting the signal values from behavioral variables (<xref ref-type="bibr" rid="bib31">Markowitz et al., 2023</xref>; <xref ref-type="bibr" rid="bib44">Willmore et al., 2022</xref>; <xref ref-type="bibr" rid="bib10">Choi et al., 2020</xref>). When used for inference, however, these approaches only test whether or not there is an overall behavioral effect on the signal in the analyzed time-window. By not testing the association at each time-point, it makes it difficult to determine when the temporal dynamics of associations are meaningful. Moreover, one model is fit per animal and thus data is not pooled across subjects, which can substantially reduce statistical power. In <xref ref-type="bibr" rid="bib23">Jean-Richard-Dit-Bressel et al., 2020</xref>, the authors propose to compare photometry signals through the combination of <italic>permutation testing</italic> and non-parametric (cluster/subject-level) bootstraps to construct confidence intervals (CIs). This is restricted to comparisons between two conditions, however, which precludes testing for continuous variables, multi-level factors, or multivariate analyses. Investigators have analyzed data from techniques like photometry and calcium imaging by fitting <italic>pointwise generalized linear models (GLMs</italic>) (<xref ref-type="bibr" rid="bib35">Pinto and Dan, 2015</xref>), or Pearson correlations (<xref ref-type="bibr" rid="bib31">Markowitz et al., 2023</xref>) to assess associations between variables and the signal at each trial time-point. However, standard GLMs and Pearson correlations do not yield valid inference when applied to multi-animal repeated measures datasets. This, therefore, requires one to analyze a single trial-averaged signal per animal, discarding trial-level information. Moreover, the methods do not adjust for multiple comparisons of testing at different time-points, which can inflate Type I errors. <xref ref-type="bibr" rid="bib27">Lee et al., 2019</xref> fit <italic>pointwise linear mixed models</italic> and then apply Benjamini-Hochberg correction. However, this method does not yield <italic>joint</italic> CIs and thus does not exploit the correlation between signal values across time-points. The method, therefore, requires one to adjust for a different test at each sample in a trial’s signal. Thus, two analyses of the same data, down-sampled with different sampling rates, could yield significant results in one analysis and not the other, simply because higher sampling rates require one to correct for more comparisons. More generally, this method can be very conservative and dramatically hinder the detection of effects.</p><p>In sum, we argue that existing photometry analysis approaches reduce the ability to detect effects. Summary measure analyses coarsen information by (1) condensing the photometry signal into a single statistic (e.g. AUC) that summarizes across time-points <italic>within-trial</italic>, and/or (2) averaging <italic>across-trials</italic> for each animal before conducting hypothesis tests. For methods that estimate associations at each trial time-point, effects can be obscured because current approaches do not exploit the correlation across time-points, do not provide <italic>joint</italic> CIs, and thus yield very conservative inference.</p><p>We present an analysis framework that fills this gap and extracts more nuanced information from photometry data by (1) enabling hypothesis testing of the effects of experimental variables on signals at <italic>every trial time-point</italic>, and (2) using the signal from every trial and animal in the analysis. Our proposed approach is based on functional linear mixed models and allows one to compare the temporal evolution (‘temporal dynamics’) of the signal between conditions – in timing and magnitude – while accounting for between-animal differences. The statistical procedure uses (1) mixed effects modeling to enable the analysis of sophisticated nested experiments (e.g. designs that include multiple conditions, sessions, and trials), and (2) functional regression to exploit autocorrelation in the signal to calculate <italic>joint</italic> 95% CIs. These <italic>joint</italic> CIs account for examining effects throughout the entire trial, but are not overly conservative CIs. Our framework outputs a plot for each covariate in the model (e.g. behavior, cue-type), which shows whether that covariate is significantly associated with the photometry signal at each time-point. The framework, therefore, unifies the stages of plotting signals and then conducting hypothesis tests into a joint analysis and visualization procedure.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In this section, we introduce our photometry analysis framework based on functional linear mixed models. We focus on explaining the implementation steps and analysis outputs. We then demonstrate how the approach can be used to formulate the scientific questions posed in a recent paper, by re-analyzing their datasets and expanding their results. Finally, we conduct realistic data-driven simulations to show that our approach has desirable statistical properties for photometry analyses.</p><sec id="s2-1"><title>Functional Linear Mixed Models (<italic>FLMM</italic>)</title><p>Linear mixed models (LMM) are a class of methods for testing the association between covariates (‘independent’ variables) and outcomes (‘dependent’ variables) for repeated measures data (see Methods section <bold>Linear mixed models</bold> for a brief introduction to LMM). They can be used to analyze trial-level summary measures (e.g. AUCs) pooled across all trials and animals, preventing the loss of information from trial-averaging. However, this still requires condensing signals into scalar summary measures, which coarsens within-trial information across time-points. In contrast, functional regression methods can be used to model a photometry time series as a ‘function,’ which makes it possible to test the association between covariates of interest and the signal value <italic>at each time-point in the trial</italic> (see Methods section <bold>Functional linear regression</bold> for a brief introduction to functional regression). However, most functional regression methods require trial averaging of the signals prior to analysis, which discards trial-level information.</p><p>The photometry analysis framework we are introducing is based on Functional Linear Mixed Models (<italic>FLMM</italic>), which combines the benefits of LMM and functional regression to extract the information in the signal both <italic>across-</italic> and <italic>within-</italic> trials (<xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Scheipl et al., 2015</xref>; <xref ref-type="bibr" rid="bib15">Davidson, 2009</xref>; <xref ref-type="bibr" rid="bib32">Morris and Carroll, 2006</xref>). By modeling covariates that can vary between (1) trial (e.g. cue-type, latency-to-press), (2) session (e.g. training stage), and (3) animal (e.g. treatment group), <italic>FLMM</italic> estimates the effects (termed <italic>functional fixed-effects</italic>) and statistical significance of those covariates for longitudinal designs. By further including <italic>functional random-effects</italic>, the framework can also estimate and adjust for between-animal differences in (1) photometry signal dynamics, and (2) effects of a covariate on the signal, within- and across- trials. In essence, this enables one to model between-animal and between-trial variability in both the ‘shape’ of the signal, and the evolution of covariate effects across trial time-points. The result is a <italic>single plot of the coefficient estimates for each covariate</italic>, which visualizes when (and to what extent) the covariate has a statistically significant (fixed-effect) association with the photometry signal during a trial.</p><p>In <xref ref-type="fig" rid="fig2">Figure 2A</xref>, we illustrate the steps in <italic>FLMM</italic> parameter estimation, which are implemented by our software (see Methods section <bold>Functional mixed models</bold> for details). The first input is a matrix <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">Y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where column <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, contains the photometry signal value at trial time-point <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> from every trial in every session and animal, as shown in <xref ref-type="fig" rid="fig2">Figure 2A(1)</xref>. The other inputs are covariate matrices, <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, associated with the fixed-effects and random-effects regression coefficients, respectively. Then, for each time-point, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>, we fit a LMM model to <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> as the outcome variable, with the conditional mean of animal <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> modeled as:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This yields estimates for fixed-effect regression coefficients (<inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, common across animals), random-effect coefficients (<inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, animal-specific), and <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> <italic>pointwise</italic> 95% confidence intervals (CIs), as shown in the last row of <xref ref-type="fig" rid="fig2">Figure 2A(1)</xref>. We then smooth the <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> across trial time-points for each covariate <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, as illustrated in <xref ref-type="fig" rid="fig2">Figure 2A(2)</xref>. The smoothed <italic>pointwise</italic> 95% CI (dark gray) is constructed using a closed form covariance expression when <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is modeled as Gaussian, or through a bootstrap-based procedure for other distributions. To account for the multiple comparisons of inspecting coefficients across the entire trial, we construct a <italic>joint</italic> 95% CI (light gray), yielding the plots in <xref ref-type="fig" rid="fig2">Figure 2A(3)</xref>. We detail the <italic>joint</italic> 95% CI estimation procedure, the subject of our statistical methodology contribution, in section <bold>Functional mixed models</bold> and the <bold>Discussion</bold>. By treating the signal as a ‘function,’ <italic>FLMM</italic> exploits the correlation across trial time-points to construct narrower <italic>joint</italic> CIs, thereby enabling one to identify more significant effects.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title><italic>Functional Linear Mixed Models</italic> estimation.</title><p>(<bold>A</bold>) General procedure. (<bold>B</bold>) Example analysis of <monospace>Latency</monospace>-signal (Latency-to-lick) association. To illustrate how the plots are constructed, we show the procedure at an example trial time-point (<inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1.7</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> sec), corresponding to values in the heatmap [Top right]. Each point in the FLMM <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> coefficient plot [Bottom Left] can be conceptualized as pooling signal values at time <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> across trials/animals (a slice of the heatmap) and correlating that pooled vector, <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1.7</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, against <monospace>Latency</monospace>, via a linear mixed model (LMM). [Bottom Right] shows how functional random-effects can be used to model variability in the <monospace>Latency</monospace>-dopamine (DA) slope across animals. The inset shows how at <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1.7</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the model treats an example animal’s slope (green), <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1.7</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1.7</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, to differ from the shared/common fixed-effect (red), <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1.7</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.82</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-fig2-v1.tif"/></fig><p><xref ref-type="fig" rid="fig2">Figure 2B</xref> illustrates the <italic>FLMM</italic> output of an example analysis. The inset in <xref ref-type="fig" rid="fig2">Figure 2B</xref> [bottom left] shows how the effect at a given time-point is estimated and interpreted: when correlating (with an LMM) the signal values (pooled across trials and animals) measured at trial time-point <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1.7</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> sec with the covariate, <monospace>Latency</monospace>, the slope of that line is <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1.7</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.82</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>. More generally, the interpretation of the <italic>FLMM</italic> plot for covariate <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> at time-point <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, is the ‘average change in the photometry signal at trial time-point <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> associated with a one unit increase in covariate <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, holding other covariates constant.’ <xref ref-type="fig" rid="fig2">Figure 2B</xref> [bottom right] shows estimated functional random-effects for an example animal and illustrates how these random-effects model individual differences in the <monospace>Latency</monospace>–signal association. Each <italic>period</italic> for which the <italic>joint</italic> 95% CI does not contain 0 anywhere denotes that the fixed-effect coefficients are <italic>statistically significantly</italic> different from 0 throughout the entire period. The plot conveys: (1) where effects are statistically significant; (2) the estimated effect magnitudes; and (3) <italic>joint</italic> 95% CIs, thereby providing a <italic>complete</italic>, interpretable, and simplified presentation of statistical results.</p></sec><sec id="s2-2"><title>A photometry study of the role of mesolimbic dopamine in learning</title><p>To demonstrate how our method can be used to answer scientific questions in photometry experiments, we reanalyzed data from a recent article proposing a new model for mesolimbic dopamine’s role in learning (<xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>) (all analyses are on photometry data collected with dLight1.3b in the nucleus accumbens core). Note that this is a different study from the one described in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We used this study for two reasons. First, the dataset exhibits many common characteristics of photometry data that can dilute effects, or even invalidate results if left unaccounted for. Second, the dataset contains data from multiple experiments, which allows us to illustrate how <italic>FLMM</italic> can be used to test hypotheses across a range of experimental designs. In this section, we discuss how <italic>FLMM</italic> handles those characteristics; in subsequent sections, we show how those hypotheses can be posed in our framework.</p><p>One of the most important characteristics of these data is that the dopamine (DA) measurements were collected in within-subject nested longitudinal experiments. For example, in the first behavioral task we discuss, mice were trained across multiple sessions. Each session involved the delivery of a sequence of 100 sucrose rewards. The authors analyzed average dopamine (AUC) during the ‘reward period’ time-window (aligned to the first lick after reward-delivery) as a trial (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Thus trials were nested in session, which were nested in animals (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). Photometry experiments often exhibit this type of nested longitudinal structure, which can induce correlation patterns in the data within-animal, and obscure effects if not accounted for statistically. This structure can occur in both within-subjects and between-subjects designs, as illustrated in <xref ref-type="fig" rid="fig3">Figure 3D</xref>. For example, these data exhibited high within-animal correlations across sessions (<xref ref-type="fig" rid="fig3">Figure 3E</xref>), and across trials within-session (<xref ref-type="fig" rid="fig3">Figure 3F</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Nested longitudinal designs in photometry experiments can result in correlation patterns and missing data that dilute effects if not unaccounted for statistically.</title><p>Descriptive statistics and figures pertain to data from <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>, reanalyzed in section <bold>Using</bold> <italic><bold>FLMM</bold></italic> <bold>to test associations between signal and covariates throughout the trial</bold>. (<bold>A</bold>) Experiment trial time-windows used to construct photometry signal summary measures (Area Under the Curve, AUC). The reward was delivered at random times and signals were aligned to the first lick following reward delivery. Reward delivery may occur during the Baseline Period or Reward Period, depending on the lick time. (<bold>B</bold>) The <monospace>Reward Number</monospace> is defined as the cumulative number of rewards (interchangeably referred to as ‘trials’) pooled across sessions. Each session involved the delivery of 100 trials. (<bold>C</bold>) The time between two rewards (inter-reward interval or <monospace>IRI</monospace>) was a random draw from an exponential distribution (mean 14). (<bold>D</bold>) Examples of experimental designs that exhibit hierarchical nesting structure. Trials/sessions and conditions such as cue-type (e.g. CS+/CS-) contribute to variability within-animal. Between-subject variability can arise from, for example, experimental groups, photometry probe placement, or natural between-animal differences. (<bold>E</bold>) Reward Period AUC values are correlated <italic>across sessions</italic>. Each dot indicates the average reward period AUC value of one trial. <italic>Between-session</italic> correlation in AUC values can be seen <italic>within-subject</italic> since reward period AUC values are similar within-animal on adjacent sessions. <italic>Between-session</italic> correlation can be seen on average <italic>across animals</italic>: session boxplot medians are similar in adjacent sessions. (<bold>F</bold>) Temporal correlation within-subject on session 3, chosen because it is the only session common to all animals. Reward period AUC on each trial for any animal is similar on adjacent trials. (<bold>G</bold>) Lines show association (ordinary least square, OLS) between <monospace>IRI</monospace> and reward period AUC for each animal and session, revealing individual differences in association magnitude. The heterogeneity in line slopes highlights the need for random-effects to account for between-animal and between-session variability. (<bold>H</bold>) Number of sessions and trials per session (that meet inclusion criteria) included varies considerably between animals. For example, one animal’s data was collected in sessions 1–11 while another’s was collected in sessions 1–3.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-fig3-v1.tif"/></fig><p><italic>FLMM</italic> can model effect heterogeneity, as well as correlation patterns within- and across- trials, through the inclusion of functional random-effects. These allow one to estimate what is common across all animals, and what is unique to each. This is critical because photometry datasets often exhibit between-animal variability in covariate-signal associations. For example, in the task described above, the time between rewards, (<monospace>IRI</monospace>: ‘inter-reward interval’) varied between trials unpredictably (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), and the authors reported that ‘reward period’ AUC was correlated with <monospace>IRI</monospace>. <xref ref-type="fig" rid="fig3">Figure 3G</xref> shows that the magnitude of this correlation varied considerably across animals and sessions. <italic>FLMM</italic> can model this variability in <monospace>IRI</monospace>–DA correlation magnitude through the inclusion of animal- and session-specific random effects. More broadly, by varying which covariates and random-effects are included in the model, <italic>FLMM</italic> can analyze data from experimental designs that include within- and between-subject contrasts. For example, one can implement functional versions of methods like correlations, or repeated measures ANOVA, and, more generally, model a wide range of dependence structures. Finally, <italic>FLMM</italic> can accommodate missing data and subjects with different sample sizes, which are often unavoidable characteristics of photometry experiments. In <xref ref-type="fig" rid="fig3">Figure 3H</xref> we show how this dataset included photometry recordings of behavioral sessions at various stages of training that differed across animals. If not accounted for statistically, this can obscure associations between the signal and covariates. For example, average reward period AUC levels were reported to increase across sessions. Thus animals with data collected only on early sessions may appear to have lower AUC levels than other animals, which can increase uncertainty in estimates of covariate effects.</p></sec><sec id="s2-3"><title>Using <italic>FLMM</italic> to test associations between signal and covariates throughout the trial</title><p>We first recreate an analysis of the experiment described in the previous section. <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> reported, in section ‘Tests 1 and 2’, that the trial-level ‘reward period’ AUC was positively correlated with <monospace>IRI</monospace>. The authors fit separate Pearson correlations to data from each animal. To illustrate how to test this question in our framework, we show a recreation of their analysis in <xref ref-type="fig" rid="fig4">Figure 4A, B</xref>, and the <italic>FLMM</italic> analysis estimates of the <monospace>IRI</monospace>–DA association in <xref ref-type="fig" rid="fig4">Figure 4C, D</xref>. While <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> assessed the <monospace>IRI</monospace>–AUC correlation in each animal separately, <italic>FLMM</italic> can test the <monospace>IRI</monospace>–DA association at each trial time-point and in all animals jointly, thereby improving our ability to detect effects. To implement a test most similar to the Pearson correlation, we fit an <italic>FLMM</italic> model with <monospace>IRI</monospace> as the (fixed-effect) covariate. Given the between-animal and between-session heterogeneity in signal profiles highlighted in <xref ref-type="fig" rid="fig3">Figure 3</xref>, we included nested random-effects to account for the variability across both subjects and sessions. The relationship is significantly positive throughout the time-window [–0.5, 1.5] sec.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Functional Linear Mixed Models (<italic>FLMM</italic>) reveals distinct components obscured by summary measure analyses.</title><p>(<bold>A, B</bold>) show a recreation of statistical analyses conducted by <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> on the random inter-trial interval (<monospace>IRI</monospace>) reward delivery experiment, and (<bold>C</bold>,<bold> D</bold>) show our analyses. (<bold>A</bold>) Analysis of <monospace>IRI</monospace>–Area Under the Curve (AUC) correlation on all trials in an example animal, as presented in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. (<bold>B</bold>) Recreation of boxplot summarizing <monospace>IRI</monospace>–AUC correlation coefficients from each animal. (<bold>C</bold>,<bold>D</bold>) Coefficient estimates from <italic>FLMM</italic> analysis of <monospace>IRI</monospace>–dopamine (DA) association: functional intercept estimate (<bold>C</bold>), and functional <monospace>IRI</monospace> slope (<bold>D</bold>). Although we do not use AUC in this analysis, we indicate the trial periods, ‘Baseline’ and ‘Reward Period’, that <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> used to calculate the AUC. They quantified DA by a measure of normalized AUC of <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> during a window ranging from 0.5 sec before to 1 sec after the first lick following reward delivery. All plots are aligned to this first lick after reward delivery. The <monospace>IRI</monospace>–DA association is statistically significantly positive in the time interval ∼[–0.5, 1.75] sec.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-fig4-v1.tif"/></fig><p>These results corroborate the paper’s findings, and provide finer-grained details inaccessible with standard analyses. For example, the temporal dynamics, revealed by <italic>FLMM</italic>, suggest that the neural signal associated with <monospace>IRI</monospace> may be composed of two distinct components. The first component rises rapidly starting at around –0.75 sec and decreases quickly after lick-onset (the first lick after reward-delivery). Since the association reaches its peak before lick-onset, the signal may reflect motivation, movement, or reward detection. The second component begins after lick-onset and rises and falls slower than the first component. The timing suggests it may track sucrose consumption. Importantly, the reward period AUC in the paper averages across these putative components. In contrast, <italic>FLMM</italic> is able to partially disentangle them through their distinct temporal dynamics, and offers insight into the role the components play in behavior.</p><p><italic>FLMM</italic> can also identify results completely obscured by standard methods. We show this by recreating an analysis in which <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> report that reward period AUC was positively correlated with that trial’s <monospace>Reward Number</monospace>. <xref ref-type="fig" rid="fig3">Figure 3B</xref> illustrates the definition of <monospace>Reward Number</monospace>, the cumulative trials/rewards received across sessions. The finding that the <monospace>Reward Number</monospace>–AUC correlation was positive was controversial because it conflicted with the prediction of the Reward Prediction Error (‘RPE’) model, a prevailing hypothesis for the role of DA in reward learning. The authors argued that RPE predicts a negative <monospace>Reward Number</monospace>–DA association, while their model (‘ANCCR’) predicts a positive association. Using <italic>FLMM</italic> on the same data, we estimate that the Reward Number–DA association is in fact <italic>negative</italic> within-session. The discrepancy by findings arises because the analysis in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> pools trials <italic>across sessions</italic> in a manner that does not consider the session number that a reward was delivered on. <italic>FLMM</italic> finds the effect above by accounting for the nested session/trial task structure, and suggests why it occurs by estimating changes at each trial time-point.</p><p>The source of the conflicting results above is most clearly illustrated by fitting <monospace>Reward Number</monospace>–AUC correlations in each animal and session separately. In <xref ref-type="fig" rid="fig5">Figure 5A</xref> and <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref> we show session-specific linear regression fits overlaid on the fits from the session-pooled analysis conducted by <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. We parameterized session-specific models to yield intercepts (highlighted as large black circles) with the interpretation ‘the expected AUC value on the first trial of the session.’ These intercepts tend to increase across sessions, ultimately resulting in a positive overall <monospace>Reward Number</monospace>–AUC correlation in the session-pooled analysis (see <xref ref-type="fig" rid="app4fig1">Appendix 4—figure 1</xref>). However, the majority of the within-session <italic>slopes</italic> are actually negative, indicating that the AUC decreases across trials <italic>within-session</italic>. The apparent disagreement between the positive <monospace>Reward Number</monospace>–AUC correlation estimated in the session-pooled analysis, and the negative <monospace>Trial Number</monospace>–AUC association identified in the within-session analysis, is an example of <italic>Simpson’s paradox</italic>. This is caused by pooling data without taking into account its hierarchical structure.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Functional Linear Mixed Models (<italic>FLMM</italic>) identifies within-session signal decreases obscured by standard analyses.</title><p>(<bold>A</bold>) Visualization of the Simpson’s paradox: the Area Under the Curve (AUC) decreases within-session, but increases across sessions. Plot shows <monospace>Reward Number</monospace>–AUC linear regressions fit to data pooled across sessions (black lines), or fit to each session separately (colored lines) in three example animals. Each colored dot is the AUC value for that animal on the corresponding session and trial. The black dots at the left of each color line indicate the intercept value of the session- and animal-specific linear regression model. Intercepts were parameterized to yield the interpretation as the 'expected AUC value on the first trial of the session for that animal.' The dotted lines indicate the animal-specific median of the intercepts (across sessions) and are included to visualize that the intercepts increase over sessions. (<bold>B, C</bold>) Coefficient estimates from FLMM analysis of the <monospace>Reward Number</monospace>–dopamine (DA) association that models <monospace>Reward Number</monospace> with <monospace>Session Number</monospace> and <monospace>Trial Number</monospace> (linear) effects to capture between-session and within-session effects, respectively. The plots are aligned to the first lick after reward-delivery. The Baseline and Reward Period show the time-windows used to construct AUCs in the summary measure analysis from <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. Pre-lick and post-lick time-windows indicate the portions of the Reward Period that occur before and after the lick, respectively. The <monospace>Session Number</monospace> effect is <italic>jointly</italic> significantly positive roughly in the interval [–0.25,0.5] sec, and peaks before lick-onset. This suggests DA increases across sessions during that interval. The <monospace>Trial Number</monospace> effect is briefly <italic>pointwise</italic> significantly positive around ∼−0.3 sec and <italic>jointly</italic> significantly negative in the interval [0, 2.5] sec. This suggests DA decreases across trials within-session during the interval [0, 2.5] sec. (<bold>D</bold>) Average signal pooled across sessions and animals. Shaded region shows standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-fig5-v1.tif"/></fig><p>To resolve this ‘paradox,’ we fit an <italic>FLMM</italic> that models the nested design by including <italic>both</italic> between-session and within-session fixed-effects, as well as nested random-effects. Estimating the<monospace> Session Number</monospace> and <monospace>Trial Number</monospace> functional coefficients reveals that <italic>within-session</italic> and <italic>between-session</italic> effects have distinct temporal dynamics. For example, <xref ref-type="fig" rid="fig5">Figure 5D, E</xref> shows that the <italic>FLMM</italic> estimates that the mean signal decreases significantly <italic>within-session</italic> beginning immediately post-lick, but <italic>increases across-sessions</italic> for a brief interval around the lick. Importantly, the <monospace>Session Number</monospace>–DA association peaks and begins falling before the lick, while the <monospace>Trial Number</monospace>–DA correlation becomes significant only after the lick. Together these results suggest that DA increases across sessions in response to reward <italic>anticipation</italic>, but decreases within-session during reward <italic>consumption</italic>.</p><p>An alternative interpretation of the opposing within- and between-session effects is that DA rises within-session, but appears to decrease due to photobleaching. We assess this in <bold>Appendix 4.3</bold>. We repeat the above analyses on the signal aligned to reward-delivery, and analyze two other behavioral experiments, collected on the same animals. These show that, <italic>within-session</italic>, DA responses increase to reward-predictive cues, but decrease to reward-delivery (predicted and unpredicted). That we can detect both within-session increases and decreases to separate events (cue-onset and reward-delivery) occurring on the same trials, suggests that photobleaching is not hiding within-session DA increases. In personal communications, <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> agree that while photobleaching cannot be definitively ruled out, a parsimonious interpretation of these findings is that there is a DA reduction to unpredicted rewards within-session, independent of photobleaching.</p></sec><sec id="s2-4"><title>Using <italic>FLMM</italic> to compare signal ‘temporal dynamics’ across conditions</title><p>We next describe an example comparing the signal ‘temporal dynamics’ across conditions, from section ‘Tests 3–7’ of <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. In this experiment, each trial consisted of a presentation of a 2 sec cue, followed by a reward 3 sec after the cue-onset. After many training sessions, the delay was lengthened to 9 sec (<xref ref-type="fig" rid="fig6">Figure 6A–B</xref>). On the first 9 sec delay session, the authors report that “dopaminergic cue responses [(Cue Period AUC)] showed no significant change,” relative to trials from the last short-delay session. <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> argue that their finding conflicts with what an RPE hypothesis of dopamine coding would anticipate (<xref ref-type="bibr" rid="bib26">Kobayashi and Schultz, 2008</xref>). Like the authors, we compared the average signals between the last short-delay and the first long-delay sessions. However, we sought to test whether the signal ‘dynamics’ over trial time-points changed after lengthening the delay. To directly compare the difference in signal magnitudes between short- and long-delay sessions, at each time-point, we fit a <italic>FLMM</italic> model with a single binary covariate representing short/long-delays (similar to a functional paired t-test). The <italic>FLMM</italic> estimated mean DA was significantly lower in the latter parts of the initial cue-elicited DA response (∼1–2.5 sec after cue-onset) on long- compared to short-delay trials (shown in <xref ref-type="fig" rid="fig6">Figure 6H</xref>). This effect may have been occluded because the AUC analyzed in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> was constructed by averaging signal values in a time window that contains opposing effects: the significant (relative) <italic>reduction</italic> in mean DA ∼0.5–2.5 sec after cue-onset (marked as interval (2) in <xref ref-type="fig" rid="fig6">Figure 6H</xref>), identified by <italic>FLMM</italic>, is potentially diluted in the AUC by the (non-significant) <italic>increase</italic> in the cue response in the first ∼0.5 sec (marked as interval (1) in <xref ref-type="fig" rid="fig6">Figure 6H</xref>). In our reanalyses we identified other experiments where effects were obscured by the use of AUCs averaging over opposite effects (see <bold>Appendix 4.2</bold>).</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Functional Linear Mixed Models (<italic>FLMM</italic>) identifies significant temporal dynamics effects missed by summary measure analyses.</title><p>The analysis of the Delay Length change experiment by <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> used the following summary measure: the average <monospace>Cue Period AUC − Baseline AUC</monospace> (Area Under the Curves, AUCs in the windows [0,2] and [–1,0] sec, respectively, relative to cue onset). (<bold>A</bold>, <bold>B</bold>) Behavioral task design and Baseline/Cue Period are illustrated for short-delay (<bold>A</bold>) and long-delay (<bold>B</bold>) sessions. (<bold>C</bold>-<bold>H</bold>) These plots show coefficient estimates from <italic>FLMM</italic> re-analysis of the experiment. (<bold>C</bold>) The coefficient value at time-point <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> on the plot is interpreted as the mean <italic>change</italic> in average dopamine (DA) signal at time-point <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> between long- and short-delay trials (i.e. positive values indicate a larger signal on long-delay trials), aligned to cue onset. (<bold>D</bold>) Same Figure as in (<bold>C</bold>) but the inset shows the interpretation of an example time-point (<inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>9.4</mml:mn></mml:mstyle></mml:math></inline-formula>): the difference in magnitude between the average traces (pooled across animals and trials) of long- and short- delay sessions. (<bold>E, F</bold>) Gold lines indicate the fixed-effect estimates and gray lines indicate animal-specific estimates (calculated as the sum of functional fixed-effect and random-effect estimates (Best Linear Unbiased Predictor)) for the random intercept, and random slope, respectively. (<bold>G, H</bold>) Fixed-effect coefficient estimates shown with expanded time axis. In (<bold>H</bold>), it is clear that long-delay trials exhibit average (relative) increases (sub-interval (1)) and decreases (sub-interval (2)) in the signal that would likely cancel out and dilute the effect, if analyzing with a summary measure (AUC) that averages the signal over the entire Cue Period.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-fig6-v1.tif"/></fig><p>This example shows how <italic>FLMM</italic> can detect subtle effects that are difficult to identify by eye, and thus hard to construct appropriate summary measures for. For example, the delay-change effect is significant only during the falling-portion of the cue-elicited transient (0.5–2.5 sec after cue-onset). This small <italic>relative</italic> reduction (shown in <xref ref-type="fig" rid="fig6">Figure 6H</xref>) during a later portion of the cue-response is overshadowed by the much larger <italic>overall</italic> DA response immediately following (0–0.5 sec) cue-onset (shown in <xref ref-type="fig" rid="fig6">Figure 6G</xref>). Finally, we show animal-specific functional random-effect estimates in <xref ref-type="fig" rid="fig6">Figure 6E, F</xref>. These provide intuition about how <italic>FLMM</italic> adjusts for between-animal differences in the ‘temporal dynamics’ of covariates effects.</p><p>We further analyze this dataset in <bold>Appendix 2</bold>, to compare <italic>FLMM</italic> with the approach applied in <xref ref-type="bibr" rid="bib27">Lee et al., 2019</xref> of fitting <italic>pointwise</italic> LMMs (without any smoothing) and applying a Benjamini–Hochberg (BH) correction. Our hypothesis was that the <xref ref-type="bibr" rid="bib27">Lee et al., 2019</xref> approach would yield substantially different analysis results, depending on the sampling rate of the signal data (since the number of tests being corrected for is determined by the sampling rate). The proportion of time-points at which effects are deemed statistically significant by <italic>FLMM</italic> joint 95% CIs is fairly stable across sampling rates. In contrast, that proportion is both inconsistent and often low (i.e. highly conservative) across sampling rates with the <xref ref-type="bibr" rid="bib27">Lee et al., 2019</xref> approach. These results illustrate the advantages of modeling a trial signal as a function, and conducting estimation and inference in a manner that uses information across the entire trial.</p></sec><sec id="s2-5"><title>Simulation experiments</title><p>We conducted experiments to assess how <italic>FLMM</italic> performs on synthetic data based on the Delay Length experiment data from <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> introduced in <bold>Using FLMM to compare signal ‘temporal dynamics’ across conditions</bold>. We simulated data in the small sample sizes typical in photometry experiments. We incorporated the key characteristics discussed earlier, namely trial-to-trial and session-to-session correlation, as well as animal-to-animal variability in photometry signal (i) magnitudes and (ii) levels of association with the covariates. We did this by treating the parameter estimates from a <italic>FLMM</italic> model, fit to the Delay Length dataset, as the ‘true’ parameter values. This ensured values fell within a realistic range. As the simulated data contained a single binary covariate, this allowed comparison with other statistical approaches discussed in the <bold>Introduction</bold>, namely the permutation method (<xref ref-type="bibr" rid="bib23">Jean-Richard-Dit-Bressel et al., 2020</xref>), a paired samples t-test, and a (non-functional) LMM. The latter two only work with summary measures, so we applied them to the reward period AUC summary measure used in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. <xref ref-type="fig" rid="fig7">Figure 7A</xref> shows average traces of the real data that the simulations are based on. <xref ref-type="fig" rid="fig7">Figure 7B</xref> presents simulated trials from one ‘animal,’ and <xref ref-type="fig" rid="fig7">Figure 7C</xref> presents simulated session averages from seven ‘animals’.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Realistic simulation experiments show that Functional Linear Mixed Models (<italic>FLMM)</italic> exhibits desirable statistical properties for photometry analyses.</title><p>The simulation produces synthetic photometry data similar to <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>, with the same sources of variability across trials and animals. (<bold>A</bold>) Lines show average traces from the original photometry data. The traces are averaged across trials and animals from the last short- and the first long-delay session. The bar shows the cue period analyzed in the paper and in our experiments. (<bold>B</bold>) Each thin line is the signal from a single simulated trial from the same ‘animal’; bold lines show the average trace for each trial type. (<bold>C</bold>) Each line is the trial-averaged trace from one session for seven simulated ‘animals.’ (<bold>D</bold>) <italic>FLMM</italic> exhibits approximately correct <italic>joint</italic> 95% confidence interval (CI) coverage. <italic>Perm</italic> does not provide <italic>joint</italic> CIs and thus its <italic>joint</italic> coverage is low, as expected. (<bold>E</bold>) <italic>FLMM</italic> exhibits approximately correct <italic>pointwise</italic> 95% CI coverage. (<bold>F</bold>) <italic>FLMM</italic> improves statistical power during the cue period compared to standard methods at each sample size tested. Power is calculated for <italic>Perm</italic> based on the full consecutive threshold criteria. For figures (<bold>E</bold>) and (<bold>F</bold>) the linear mixed models (LMM) and t-test were fit on the cue period Area Under the Curve (AUC) and thus each replicate yields one indicator of CI inclusion and statistical significance. We represent the corresponding proportions with a line plot. For other methods, estimates are provided at each time-point. We therefore average performance across the cue period and then summarize the variability of these replicate-specific averages with a 95% confidence band.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-fig7-v1.tif"/></fig><p>The quantitative measures for comparison are <italic>pointwise</italic> and <italic>joint</italic> 95% CI coverage, and statistical power. <xref ref-type="fig" rid="fig7">Figure 7D</xref> shows that the <italic>FLMM</italic> achieves <italic>joint</italic> 95% CI coverage at roughly the nominal level for all sample sizes tested. The permutation method, <italic>Perm</italic>, (<xref ref-type="bibr" rid="bib23">Jean-Richard-Dit-Bressel et al., 2020</xref>) provides only <italic>pointwise</italic> CIs and thus yields low <italic>joint</italic> coverage. We next analyzed the cue period to allow comparison between the average performance of <italic>FLMM</italic>, evaluated at time-points in that time-window, with the performance of standard methods applied to the AUC summary measure. <italic>FLMM</italic> achieved roughly the nominal pointwise coverage (<xref ref-type="fig" rid="fig7">Figure 7E</xref>), while <italic>Perm</italic> does not, likely because the cluster-bootstrap often performs poorly in small sample settings (<xref ref-type="bibr" rid="bib25">Ju, 2015</xref>). <italic>FLMM</italic> yields substantially better statistical power than the other methods (<xref ref-type="fig" rid="fig7">Figure 7F</xref>). The LMM and t-test exhibit lower power, likely because effects are diluted by analyzing summary measures. In <bold>Appendix 5.2</bold>, we present additional simulation results that demonstrate that the performance of summary measure methods is highly sensitive to minor changes in the length of the time-interval that the AUC summarizes. These results show how <italic>FLMM</italic> improves <italic>pointwise</italic> coverage and statistical power compared to standard methods and provides reasonable <italic>joint</italic> coverage.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Technical contribution</title><p>We introduced a Functional Linear Mixed Modeling framework for photometry data analysis that: (1) enables hypothesis testing at every trial time-point; (2) accounts for individual differences and trial-to-trial variability; and (3) allows modeling of longitudinal experimental designs. We extend the <italic>FLMM</italic> framework in <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref> to enable parameter estimation with <italic>pointwise</italic> and <italic>joint</italic> 95% CIs for general random-effect specifications. Previous work provided <italic>FLMM</italic> implementations for either <italic>joint</italic> CIs for simple random-effects models (<xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>; <xref ref-type="bibr" rid="bib40">Sergazinov et al., 2023</xref>; <xref ref-type="bibr" rid="bib12">Crainiceanu et al., 2024</xref>), or <italic>pointwise</italic> CIs for nested models (<xref ref-type="bibr" rid="bib39">Scheipl et al., 2016</xref>). However, they do not allow computation of <italic>joint</italic> 95% CIs in the presence of general random-effects specifications, which is helpful for modeling many characteristics of photometry datasets. When conducting the analyses in this paper, we found that nested random-effects specifications, enabled by our method, substantially improved model fits compared to models without nested random-effects. We describe <italic>FLMM</italic> further in the Methods section <bold>Functional mixed models</bold> and derive our methodology in <bold>Appendix 3</bold>.</p></sec><sec id="s3-2"><title>Evaluation of <italic>FLMM</italic> in real and synthetic datasets</title><p>We demonstrated the capabilities of <italic>FLMM</italic> by reanalyzing datasets from a recently published study <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. First, we showed that the main scientific questions asked by the authors can easily be answered by specifying covariates of interest, fitting an <italic>FLMM</italic> model, and referring to the coefficient estimate plots for results. Furthermore, we showed that <italic>FLMM</italic> allows direct comparison – and testing – of differences in signal time-courses between conditions, something that is difficult with standard methods relying on summary measure tests. Finally, we showed that <italic>FLMM</italic> can reveal significant effects that were tested by the authors, but obscured by summary measure analyses.</p><p>The specific effects identified with <italic>FLMM</italic> bear on one of the key questions in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>, namely which of two competing models – RPE and ANCCR – better describe changes in DA responses to unpredicted rewards over trials. For example, in personal communications, <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> still argue that the observed DA increases <italic>across-sessions</italic> in the random <monospace>IRI</monospace> task may be inconsistent with RPE. However, as a result of our reanalyses, they now argue that the <italic>within-session</italic> decreases, identified by <italic>FLMM</italic>, conflict with the predictions of the initial ANCCR model presented in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. The predictive/explanatory power of RPE and ANCCR as models for measured DA responses depend, however, on their respective model specifications (e.g. RPE reward function) and is thus beyond the scope of the present work. The difficulty in comparing the two models is further compounded by a number of alternative explanations for our findings that cannot be definitively ruled out. For example, satiation or habituation may contribute to within-session DA decreases. An initial exploration suggests that this may not be the sole cause, as the temporal dynamics of within-session reductions vary between-session (<xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref>), despite equal levels of food-restriction. The between-session variability in dynamics may, however, reflect learning-related changes in consummatory behavior, and thus may not rule out satiation as an explanation for the within-session DA reductions. Moreover, the authors suggested that the reward-delivery mechanism produces stimuli that could act as a reward-predictive cue. Some of the DA changes may, therefore, reflect learning about this cue. Finally, between-session DA increases could be due to elevations in indicator expression levels over training. This was, however, explored by <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> and they reported that transients unrelated to reward did not increase in magnitude across sessions (see Figure S8F-H in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>). As the authors note, if reward-related DA increases across sessions were caused by higher indicator levels, one would expect non-reward-related DA activity to also rise.</p><p>In personal correspondence, the authors of <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> offered the following possible explanations for the within-session DA decreases within the ANCCR framework. Specifically, while the ANCCR simulations described in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> did not consider the session a trial occurred on, there may be session boundary-effects in animal learning. For instance, animals may estimate the rate of reward to be lower at the start of a session, due to a degraded memory from prior sessions, or to generalization between the high reward rate within the head-fixed context and the low rate outside it. Then the systematic increase in animals’ estimate of reward rate during the session would result in a corresponding decrease in the learning rate of the reward–reward predecessor representation (<xref ref-type="bibr" rid="bib9">Burke et al., 2023</xref>). If so, the magnitude of the reward-reward predecessor representation increase at reward-delivery would reduce across sessions, and result in a negative <monospace>Session Number</monospace>–DA correlation.</p><p>To further demonstrate the utility of <italic>FLMM</italic> in answering common neuroscience questions, we reanalyze data in which the authors tested whether, across several sessions of Pavlovian learning, DA activity ‘backpropagates’ from reward delivery (3 sec after cue-onset) to the presentation of reward-predictive cues (see <bold>Appendix 4.4</bold>). We argue that <italic>FLMM</italic> is well-suited to assess questions like these: the method tests how the signal <italic>timing</italic> evolves both within- and across-sessions by providing a null hypothesis test (i.e. to assess statistical significance) at each time-point. Our results align with those reported by <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. In contrast, <xref ref-type="bibr" rid="bib3">Amo et al., 2022</xref> reported fiber photometry data that they argue supports the backpropagation hypothesis, despite being collected from a similar brain region and behavioral task. If researchers apply <italic>FLMM</italic> to test the backpropagation hypothesis in the future, we recommend fitting models that account for between-animal heterogeneity. Our initial exploration suggested that if the backpropagation hypothesis holds, the timing of when that ‘backpropagating hump’ begins to transition may differ between animals. In order to thoroughly test that hypothesis, it may, therefore, be necessary to specify an <italic>FLMM</italic> mean model that explicitly accounts for this potential variability in timing.</p><p>Adjudicating between ANCCR and RPE is beyond the scope of this paper, but our findings suggest that future experiments may be needed to test the explanations proposed in light of our results. Even in the Delay Length change experiment, in which <italic>FLMM</italic> identified significant changes obscured by AUC analyses, it is not clear to us whether proposed versions of RPE and ANCCR would predict the specific change in dynamics observed. As such, modeling trial-level mesolimbic DA as a vector (e.g. see <xref ref-type="bibr" rid="bib43">Wärnberg and Kumar, 2023</xref>, <xref ref-type="bibr" rid="bib28">Lee et al., 2024</xref>) or function may improve theories for its role in reward learning. By estimating trial-level temporal dynamics, we hope that <italic>FLMM</italic> will make it possible to further refine these theories.</p><p>For further evaluation of <italic>FLMM</italic>, we reanalyzed a second study (<xref ref-type="bibr" rid="bib11">Coddington et al., 2023</xref>, see <bold>Appendix 6</bold>) that measures calcium dynamics, showing the applicability of the method to different photometry sensors. There, we demonstrate additional capabilities to: (1) model how signal ’dynamics’ early in training predict behavior later in training; and (2) estimate covariate interactions across both trial time-points and sessions. Finally, we carried out simulation experiments on realistic synthetic data to verify that, in small sample settings, the <italic>FLMM</italic> framework yields <italic>joint</italic> and <italic>pointwise</italic> CIs that achieve approximately 95% coverage, and improve statistical power versus the alternative approaches introduced. As <italic>FLMM</italic> commits type I errors at roughly nominal levels and outperforms methods that analyze trial-averaged data, we verify that <italic>FLMM</italic> is not overly susceptible to trial-level noise or spurious correlations.</p></sec><sec id="s3-3"><title>Benefits of applying <italic>FLMM</italic> to neural data</title><p>Beyond the technical capabilities of our method, we believe there are a number of ways in which it can enhance scientific practice. First, <italic>FLMM</italic> may reduce biases arising from summary statistic analyses. Photometry measures aggregate activity from a collection of neurons, which may contain sub-populations with different functions. By coarsening the temporal resolution, summary measures may obscure the heterogeneity of the target neural population, and bias the study of systems towards sub-populations that exhibit larger signals. Through analyzing each time-point, <italic>FLMM</italic> may help disentangle separate signal components arising from different neural sub-populations. Second, we found that <italic>FLMM</italic> made it easier to identify model misspecification, as estimate plots often provided obvious visual indications (e.g. CI magnitude varying enormously across sub-intervals of the trial). We recommend fitting a few models and selecting one based on fit criteria (e.g. AIC/BIC), without considering statistical significance. Finally, we hope <italic>FLMM</italic> will improve reproducibility by removing the need for some signal pre-processing steps. Any data pre-processing will have down-stream consequences on the analysis results and interpretations from most statistical methods. In addition to possibly drowning out effects, some pre-processing steps (e.g. signal smoothing) can reduce reproducibility if the approaches differ across labs. <italic>FLMM</italic> can be used to eliminate some common pre-processing steps by including them as part of the model, or make their effect on the analysis results clearer, as we summarize in the next paragraph.</p><p><italic>FLMM</italic> can help model signal components unrelated to the scientific question of interest, and provides a systematic framework to quantify the additional uncertainty from those modeling choices. For example, analysts sometimes normalize data with trial-specific baselines because longitudinal experiments can induce correlation patterns across trials that standard techniques (e.g. repeated measures ANOVA) may not adequately account for. Even without many standard data pre-processing steps, <italic>FLMM</italic> provides smooth estimation results across trial time-points (the ‘functional domain’), has the ability to adjust for between-trial and -animal heterogeneity, and provides a valid statistical inference approach that quantifies the resulting uncertainty. For instance, session-to-session variability in signal magnitudes or dynamics (e.g. a decreasing baseline within-session from bleaching or satiation) could be accounted for, at least in part, through the inclusion of trial-level fixed or random effects. Similarly, signal heterogeneity due to subject characteristics (e.g. sex, CS+ cue identity) could be incorporated into a model through the inclusion of animal-specific random effects. Inclusion of these effects would then influence the width of the confidence intervals. By expressing one’s ‘beliefs’ in an <italic>FLMM</italic> model specification, one can compare models (e.g. with AIC). Even the level of smoothing in <italic>FLMM</italic> is largely selected as a function of the data, and is accounted for directly in the equations used to construct confidence intervals. This stands in contrast to ‘trying to clean up the data’ with a pre-processing step that may have an unknown impact on the final statistical inferences.</p><p><italic>FLMM</italic> is applicable in a wide range of experimental settings. We provide an analysis guide and open source package, <monospace>fastFMM</monospace> (our package is available on <monospace>CRAN</monospace> and also at <ext-link ext-link-type="uri" xlink:href="https://github.com/gloewing/fastFMM">https://github.com/gloewing/fastFMM</ext-link>, which has links to our analysis guide and examples of using the package in both <monospace>R</monospace> and <monospace>python</monospace>), that is the first, to our knowledge, to provide <italic>joint</italic> CIs for functional mixed models with nested random-effects. For demonstration, we selected a dataset collected on a behavioral paradigm, photometry sensor, and signaling pathway that are well characterized, making it easier to evaluate the framework. Although selecting summary measures and time-windows to quantify is relatively easy here, this may not be the case when collecting data under new conditions. In such cases, <italic>FLMM</italic> can characterize the association between neural activity and covariates without strong assumptions about how to summarize signals. Importantly, the package is fast and maintains a low memory footprint even for complex models and relatively large datasets (see <bold>Functional mixed models package</bold> for an example). The <italic>FLMM</italic> framework may also be applicable to techniques like electrophysiology and calcium imaging. For example, our package can fit functional generalized LMMs with a count distribution (e.g. Poisson). Additionally, our method can be extended to model time-varying covariates. This would enable one to estimate how the level of association between signals, simultaneously recorded from different brain regions, fluctuates across trial time-points. This would also enable modeling of trials that differ in length due to, for example, variable behavioral response times (e.g. latency-to-press). In this paper, we specified <italic>FLMM</italic> models with linear covariate–signal relationships <italic>at a fixed trial time-point</italic> across trials/sessions, to compare the <italic>FLMM</italic> analogue of the analyses conducted in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. However, our package allows the modeling of covariate–signal relationships with non-linear functions of covariates, using splines or other basis functions. One must consider, however, the tradeoff between flexibility and interpretability when specifying potentially complex models, especially since <italic>FLMM</italic> is designed for statistical inference.</p><p>To conclude, we believe research on statistical methods for fiber photometry is necessary, given the widespread use of the technique and the variability of analysis procedures across labs. We hope that our proposed <italic>FLMM</italic> framework and software further enable investigators to extract the rich information contained in photometry signals.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>The framework we introduced is based on <italic>FLMM</italic>, which is a combination of linear mixed modeling and functional regression. In this section, we introduce both of these prior to describing the <italic>FLMM</italic> approach. We then describe the analyses and modeling methods we used to reanalyze data from <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> as well as details of the simulation scheme. We describe our strategy for the analysis of data after pre-processing steps such as the calculation of <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><sec id="s4-1"><title>Linear mixed models</title><p>Linear mixed modeling (LMM, also known as multilevel or hierarchical modeling) is a method for testing the association between covariates (i.e. ‘predictor’ or ‘independent’ variables) and an outcome (‘dependent’) variable for repeated measures data (or multilevel/longitudinal). We provide a brief description of mixed models below, but see <xref ref-type="bibr" rid="bib1">Aarts et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Yu et al., 2022</xref>; <xref ref-type="bibr" rid="bib30">Magezi, 2015</xref>; <xref ref-type="bibr" rid="bib6">Barr et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Barr, 2013</xref>; <xref ref-type="bibr" rid="bib4">Baayen et al., 2008</xref> for detailed descriptions of their use in neuroscience and psychology.</p><p>LMM enables hypothesis testing in repeated measures designs, and can account for and characterize individual differences through the inclusion of ‘random-effects.’ Conceptually, random-effects serve as animal-specific regression coefficients (one may include more than just animal-specific random-effects (e.g. session-level random-effects)). This allows the relationship between a covariate of interest (e.g. locomotion) and the outcome (e.g. lever pressing) to vary between-animals in the model, while still pooling data from all animals to estimate parameters. For example, they can be used to model between-animal differences in photometry signal magnitudes on baseline conditions (e.g. on a control condition). This can allow the model to capture how variables of interest (e.g. behavior) are associated with the signal after ‘adjusting’ for the fact that some animals may, on average, exhibit lower signal magnitudes on all conditions. While LMMs may not feel intuitive initially, they actually share connections to many familiar hypothesis testing procedures. Just as ANOVAs, t-tests, and correlations can be cast as special cases of linear models, repeated measures versions of these tests (e.g. paired sample t-tests, repeated measures ANOVAs, and MANOVAs) have similar connections to linear <italic>mixed</italic> models. We recommend sections 1.2–1.3 of <xref ref-type="bibr" rid="bib18">Fitzmaurice, 2008</xref> for more explanation of these connections. This reference also includes descriptions of the capacity of LMM (and thus FLMM) to accommodate: (1) unbalanced designs, (2) varying sample sizes across individuals, and (3) more complicated correlation structures than methods like repeated measures ANOVA.</p><p>In order to provide better intuition about LMM in the photometry context, we illustrate the role of random-effects in <xref ref-type="fig" rid="fig8">Figure 8</xref>. In panel A, we plot data from five animals (photometry data from <xref ref-type="bibr" rid="bib11">Coddington et al., 2023</xref>, with behavioral data simulated for illustrative purposes). On multiple trials, to examine the association between the signal and behavior on a test session. Averaging the signal across trials and correlating its peak amplitude with average session behavior results in only five observations, shown in the inset. In <xref ref-type="fig" rid="fig8">Figure 8B</xref>, we show the alternative approach that correlates a summary of <italic>each trial</italic> (e.g. peak amplitude) with the trial-level behavioral variable. The linear regression is fit to many more observations than in the session average strategy, but the standard regression hypothesis testing procedure may now be invalid, as we violated the independence assumption by pooling repeated observations within animals. Moreover, it ignores that the number of samples (trials) differ between animals. We magnify the inset from <xref ref-type="fig" rid="fig8">Figure 8B</xref> in <xref ref-type="fig" rid="fig8">Figure 8C</xref> to emphasize how within-subject correlation arises, at least partially, from animal-to-animal variability. Indeed, the linear regressions (Ordinary Least Squares or ‘OLS’) fit to each animal separately differs from the fit obtained if we first pool all animals’ trial-level data (‘All Animals OLS Fit’). This is expected since (1) each animal may have a different ‘true’ regression line (i.e. individual differences), and (2) the estimated lines may differ ‘by chance’ alone due to sampling error. An LMM allows us to pool across animals and trials but still account for individual differences through ‘random-effects,’ thereby striking a <italic>balance</italic> between (i) the line fit to the trial-level data pooled across animals, and (ii) the animal-level fits. In <xref ref-type="fig" rid="fig8">Figure 8D</xref> we show the fit estimated from the LMM (determined by <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), as well as the subject-specific fits that include the random-effects (defined by the <italic>sum</italic>: <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for subject <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>). In <xref ref-type="fig" rid="fig8">Figure 8C</xref> the <italic>All Animals OLS Fit</italic> is ‘pulled’ towards Animal 1’s data because we failed to account for the fact that more trials were recorded from Animal 1. In <xref ref-type="fig" rid="fig8">Figure 8D</xref>, the LMM fit accounts for differing sample sizes across animals and is no longer unduly influenced by Animal 1: it is more similar to the majority of the other animals’ subject-specific LMM fits than the OLS fit. Moreover, we see that Animal 3 in <xref ref-type="fig" rid="fig8">Figure 8C</xref> has an OLS fit that differed considerably from all other animals. The LMM subject-specific fit is, however, ‘shrunk’ towards the population fit, determined by <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Example repeated measures data from a single test session.</title><p>(<bold>A</bold>) <italic>Session-average approach</italic>: photometry signals from all trials for Animal 1 are averaged across trials and summarized (peak amplitude). The session-average summary is then plotted (inset) against the session-average behavior for each of five animals. Example animal’s session average is the filled circle. (<bold>B</bold>) <italic>Trial-level approach</italic>: the trial-level signal summary measure (peak amplitude) is pooled across animals and correlated with trial-level behavior. An example signal from one trial for Animal 1 is highlighted in the trace plot. That example trial is represented as the filled circle in the inset. Each dot in the inset is one trial from one animal; dot color indicates the animal ID. (<bold>C</bold>) Inset from (<bold>B</bold>) is magnified. Linear regressions (OLS: Ordinary least squares) fit separately to each animal’s trial-level data. A global regression fit to the trial-level data pooled across animals is displayed as the dotted black line. (<bold>D</bold>) Linear mixed modeling (LMM) strikes a balance between one model common to all animals and fitting many animal-specific models. The ‘global’ fixed-effects fit (from <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the fits including the subject-level random-effect estimates (Best Linear Unbiased Predictor) are displayed. Subject-specific fit for animal <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is calculated from: <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note the fixed-effects, <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and random-effects, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, are estimated in the same model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-fig8-v1.tif"/></fig><p>The LMM used to estimate the fits presented in <xref ref-type="fig" rid="fig8">Figure 8D</xref> is<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>β</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are the outcome variable (e.g. trial behavior) and covariate (e.g. photometry signal peak amplitude), respectively, associated with subject <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For short hand, we concatenate the ‘fixed’ effects, random-effects and covariates into vectors, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the population regression coefficient estimate that is <italic>shared</italic> across all subjects, and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is subject <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>’s random-effect, its deviation from the shared population slope <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus the slope that determines the fit specific to subject <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. That is, the LMM estimates: (i) what is common among all animals; and (ii) what is unique to each animal. This imbues LMM (and its generalizations) with a flexibility that enables one to account for individual differences and to model correlated data arising from a wide range of complex experimental designs. LMM still requires, however, one to analyze summary measures (e.g. AUCs), and thus their application in photometry analyses leads to discarding of within-trial signal information across time-points the downsides of summary measure analyses are well-characterized in the statistics community (e.g. see section 1.2 of <xref ref-type="bibr" rid="bib18">Fitzmaurice, 2008</xref>).</p></sec><sec id="s4-2"><title>Functional linear regression</title><p>We now describe function-on-scalar regression, a functional data analysis method where the outcome variable is a <italic>function</italic>, rather than a scalar value. We treat the photometry time series from one trial as a ‘function’, which allows us to test the association between any trial-level experimental variables of interest (e.g. cue type, latency-to-press) and the photometry signal value <italic>at each time-point in the trial</italic>. For example, if photometry data for two groups is collected on one trial, one could fit a separate t-test at <italic>every</italic> time-point in the trial. The t-test at time <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> then assesses the mean difference between the signal magnitude of group 1 at time <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> and the signal magnitude of group 2 at time <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>. Plotting the test statistics vs. time yields a <italic>curve</italic> representing how differences between groups change over time. This provides the intuition behind the functional version of a <italic>t-test</italic>, and is a special case of the <italic>functional linear model</italic>. This functional version of the t-test differs from the common approach of comparing the groups’ signals using a single summary measure (e.g. peak amplitude in the trial).</p><p>In essence, the functional linear model (here, we describe the <italic>function-on-scalar</italic> linear model but use the functional linear model as short hand) is a linear regression with a functional coefficient <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> across time-points <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are the photometry value and error term of animal <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> at time-point <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the signal. The <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> vector, <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, is the regression coefficient at time-point <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, applied to the covariates, <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. This functional linear regression framework allows for testing the effects of (scalar) variables at every time-point, adjusting for multiple covariates (continuous or discrete) and their interactions. Functional regression thus exploits <italic>within-trial</italic> signal information. However, it cannot be directly used to analyze multiple trials per animal, since it assumes independence across observations of the signal <italic>between-trials</italic> (see Chapter 5 of <xref ref-type="bibr" rid="bib12">Crainiceanu et al., 2024</xref>).</p></sec><sec id="s4-3"><title>Functional mixed models</title><p>Our framework is based on Functional Linear Mixed Models (<italic>FLMM</italic>), which is a form of function-on-scalar regression that combines the benefits of linear mixed effects modeling and functional linear regression. We provided an informal presentation of the approach in the Results section <bold>Functional Linear Mixed Models (FLMM)</bold>, with the goal of enabling researchers with a wide range of statistical backgrounds to understand its application and results. Here, we provide a high-level overview of the methodology, with further technical details provided in <bold>Appendix 3</bold>. <italic>FLMM</italic> models each trial’s signal as a function that varies smoothly across trial time-points (i.e. along the ‘functional domain’). It is thus a type of non-linear modeling technique over the functional domain, since we do not assume a linear model (straight line). <italic>FLMM</italic> and other functional data analysis methods model data as functions, when there is a natural ordering (e.g. time-series data are ordered by time, imaging data are ordered by x-y coordinates), and are assumed to vary smoothly along the functional domain (e.g. one assumes values of a photometry signal at close time-points in a trial have similar values). Functional data analysis approaches exploit this smoothness and natural ordering to capture additional information during estimation and inference. Our <italic>FLMM</italic> approach is based on an estimation procedure that was first proposed in <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>. We selected this method because (1) it allows for calculation of <italic>joint</italic> 95% CIs, (2) it is readily scalable for the dataset sizes and random-effect specifications needed for neuroscience experiments, and (3) it uses common syntax for model specification from the well-known <monospace>lme4</monospace> package.</p><p>The construction of <italic>joint</italic> CIs in the context of functional data analysis is an important research question; see <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref> and references therein. Each <italic>point</italic> at which the <italic>pointwise</italic> 95% CI does not contain 0 indicates that the coefficient is <italic>statistically significantly</italic> different from 0 at that point. Compared with <italic>pointwise</italic> CIs, <italic>joint</italic> CIs take into account the autocorrelation of signal values across trial time-points (the functional domain). Therefore, instead of interpreting results at a specific time-point, <italic>joint</italic> CIs enable <italic>joint</italic> interpretations at multiple locations along the functional domain. This aligns with interpreting covariate effects on the photometry signals across time-intervals (e.g. a cue period) as opposed to at a single trial time-point. Previous methodological work has provided functional mixed model implementations for either <italic>joint</italic> 95% CIs for simple random-effects models (<xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>), or <italic>pointwise</italic> 95% CIs for nested models (<xref ref-type="bibr" rid="bib39">Scheipl et al., 2016</xref>), but to our knowledge, do not provide explicit formulas or software for computing <italic>joint</italic> 95% CIs in the presence of general random-effects specifications. We found nested random-effects specifications substantially improved model fits in the neuroscience experiments we reanalyzed, and helped model sophisticated behavioral designs (see <bold>Appendix 3.3</bold> for more details). We, therefore, derived an extension of the estimator for the covariance of the random-effects (<xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>), based on a Method of Moments approach described by <xref ref-type="bibr" rid="bib19">Greven et al., 2010</xref>.</p><p>Mixed effects modeling (and thus <italic>FLMM</italic>) requires one to specify the random-effects in the modeling process. In essence, this amounts to selecting which covariate effects (i.e. regression coefficients) might vary across, for example, animals or sessions (see <bold>Appendix 3.3</bold> for more details on specification and interpretation of random-effects). While this may seem unfamiliar at first, in practice, most experimental designs (and the appropriate model formulations) fall under a few categories that can be modeled with extensions of familiar methods (e.g. <italic>FLMM</italic> analogues of repeated measures ANOVA, correlations, etc.). Our method is fully implemented by our package and uses the same syntax as the common mixed effects package <monospace>lme4</monospace>, thereby allowing one to easily fit the models and plot the results. Thus the rich literature on linear mixed model selection and model syntax can be used for <italic>FLMM</italic>, as many of the same principles apply (e.g. see reviews of LMM and applications in neuroscience and psychology in <xref ref-type="bibr" rid="bib1">Aarts et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Yu et al., 2022</xref>; <xref ref-type="bibr" rid="bib30">Magezi, 2015</xref>; <xref ref-type="bibr" rid="bib6">Barr et al., 2013</xref>; <xref ref-type="bibr" rid="bib5">Barr, 2013</xref>; <xref ref-type="bibr" rid="bib4">Baayen et al., 2008</xref>). Users may improve their analyses by evaluating a few candidate models and selecting the one with the best model fit criteria. To that effect, AIC, BIC, and cAIC (<xref ref-type="bibr" rid="bib37">Säfken et al., 2018</xref>) are already provided automatically in our software implementation. We provide the code to fit models with our package below and we include all code used for this paper (e.g. data pre-processing, model-fitting) on the GitHub page: <ext-link ext-link-type="uri" xlink:href="https://github.com/gloewing/photometry_FLMM">https://github.com/gloewing/photometry_FLMM</ext-link> (copy archived at <xref ref-type="bibr" rid="bib29">Loewinger, 2024</xref>). Finally, since fiber photometry sums photon counts over many neurons (the activity of which may be modeled as only weakly dependent given model parameters), we appeal to the central limit theorem to motivate the adoption of a Gaussian-likelihood for the conditional distribution of <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, when photon counts are low, our package can be used to fit functional generalized LMMs with a count distribution like a Poisson (see <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>).</p></sec><sec id="s4-4"><title>Figure 1 methods and analyses</title><p><xref ref-type="fig" rid="fig1">Figure 1</xref> was generated from data (<xref ref-type="bibr" rid="bib11">Coddington et al., 2023</xref>) available at <xref ref-type="bibr" rid="bib16">Dudman, 2023</xref>. All signals are measurements of calcium dynamics in mesolimbic dopamine cells (DAT-Cre::ai32 transgenic mice were injected with a Cre-dependent jRCaMP1b virus across the ventral midbrain) recorded from fibers in the nucleus accumbens. In this experiment, head-fixed mice were exposed to a 0.5 sec stimulus, followed by a reward 1 sec after cue-offset. Signals are aligned to cue-onset.</p><p><xref ref-type="fig" rid="fig1">Figure 1A</xref> was generated from trial-averaging Lick+ trials from session 8 for control animals that received no optogenetic stimulation. To ensure the animals were sufficiently trained we selected session 8, because it is the latest session in which all control animals have recorded data available in this dataset. <xref ref-type="fig" rid="fig1">Figure 1B</xref> was generated from trial-averaging Lick+ and Lick- trials and then plotting the pointwise difference. <xref ref-type="fig" rid="fig1">Figure 1C</xref> was generated arbitrarily from the first control animal in the dataset (i.e. animal ID 1), which we selected without inspecting other animals’ data to avoid biases. The data plotted are a subset of Lick+ trials (a sequence that starts from the first and ends on the last trial and takes every 10th trial) on the final training session for that animal (session 11). We selected this session to ensure the animal was as well trained as possible. <xref ref-type="fig" rid="fig1">Figure 1D</xref> shows session averages on Lick+ trials from sessions 8–11.</p><p>We made <xref ref-type="fig" rid="fig1">Figure 1E and F</xref> for explanatory purposes, and the specific choices we made in creating these figures are not meant to reflect any specific photometry analysis procedures. We nevertheless include a description of how we generated them for completeness. <xref ref-type="fig" rid="fig1">Figure 1E</xref> was the trial-averaged trace from session 8 for Lick- trials for Animal 1. <xref ref-type="fig" rid="fig1">Figure 1F</xref> was the trial-averaged trace from session 8 for Lick- and Lick+ trials separately for Animal 1. The inset contains an individual point for each animal from session 8. A given point was calculated by trial-averaging Lick+ and Lick- for each animal separately, and then finding the maximum difference.</p></sec><sec id="s4-5"><title>Data reanalysis methods</title><p>Our primary experimental results in the <bold>Results</bold> section come from a reanalysis of the data from <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. Here, we provide a high-level description of the methods used, with additional details in <bold>Appendix 4</bold>. We describe the models we implemented and where possible, we explain the authors’ hypotheses, data pre-processing, and analysis procedures. We often center all covariates to be mean zero to allow an interpretation of the intercept as the mean signal for a trial when all covariates are at their average values. For each of the following analyses, we fit a collection of models and selected the final one based on which exhibited the best AIC and BIC. Finally, the <monospace>R</monospace> scripts used to replicate the analyses or derive information from data (e.g. extracting consummatory lick bouts) can be found on the GitHub page: <ext-link ext-link-type="uri" xlink:href="https://github.com/gloewing/photometry_FLMM">https://github.com/gloewing/photometry_FLMM</ext-link> (copy archived at <xref ref-type="bibr" rid="bib29">Loewinger, 2024</xref>).</p><sec id="s4-5-1"><title>Notation</title><p>We denote <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as the <italic>FLMM</italic> mean model of <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, the photometry signal at trial time-point <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, for animal <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, on trial <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of session <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We use the general notation <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as the <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> vector of covariates for the fixed-effects and <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as the <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> vector of covariates for the random-effects. The full form of the covariates that fully specify the columns of the associated design matrices can be quite complicated (e.g. when we include animal-specific random-effects for each session). On the right-hand side of the mean model, we specify the covariates by name when possible (e.g. <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">IRI</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as the inter-reward interval for subject <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on session <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), with the understanding that the vectors <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> contain these covariates where applicable (e.g. this covariate may be included in <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, or both). As mentioned in the main text, the entry in these vectors can change from trial-to-trial but, unlike the outcome, the entries do not change within-trial and, for that reason, we do <italic>not</italic> include the notation <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Finally, in some models the covariates change from session-to-session but not across trials <italic>within</italic> a session. We indicate how the variables change (e.g. trial-to-trial vs. session-to-session but not trial-to-trial) through outcome, covariate, and random-effect subscripts (e.g. <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Delay</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>). Finally, we denote <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as the <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> vector of random effects for subject <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of session <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> at time-point <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-5-2"><title>Reanalysis methods: Using <italic>FLMM</italic> to test associations between signal and covariates throughout the trial</title><p>We analyzed the same set of sessions that were analyzed by the authors (<xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>) in the section ‘Tests 1 and 2 (unpredicted rewards)’. As noted previously, the set of sessions analyzed by the authors differed across animals, and the number of trials per session differed between sessions. The sessions analyzed and the characteristics of these data are presented in <xref ref-type="fig" rid="fig3">Figure 3</xref>. All analyses are on data collected with the photometry dopamine dLight1.3b sensor (AAVDJ-CAG-dLight1.3b virus). The virus was injected and the optical fiber was implanted in the nucleus accumbens core. The authors quantified dopamine (DA), measured with normalized AUC of <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> during a window of 0.5 sec before to 1 sec after the first lick that occurred after reward-delivery. In ‘Data Analysis: Experiment 1’ of the Supplement (p.3), the authors describe that “The [Area Under the Curve (AUC)] during 1.5 sec time window before reward period was subtracted from AUC during reward period to normalize baseline activity. To test dynamics of dopamine response to reward, Pearson’s correlation was calculated between dopamine response and Reward Number, or dopamine response and inter-reward interval (IRI) from the previous reward.” In all of our analyses in this section, we tested <italic>linear effects</italic> of all covariates of interest, given that the authors conducted Pearson correlations in their tests and we wanted to implement the most similar model in our framework (e.g. see Fig. S8 in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>). We followed the methods for excluding trials based on behavioral criteria, and for extracting consummatory lick bouts described in ‘Data Analysis: Experiment 1’ of the Supplement (p.3–4) of the original manuscript. We describe this in more detail in <bold>Appendix 4.2</bold>.</p><sec id="s4-5-2-1"><title><monospace>IRI</monospace> model</title><p>We tested the association between the DA response to sucrose reward and <monospace>IRI</monospace> (<inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">IRI</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), as shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. We first compared models that included, for example, subject- and session-specific random intercepts, random slopes for <monospace>Trial Number</monospace>, random slopes for <monospace>Lick Latency</monospace> (the time between reward-delivery and first lick), and random slopes for <monospace>IRI</monospace>. We considered this collection of models to account for trial-to-trial variability in learning (e.g. <monospace>Lick Latency</monospace> grows faster with learning), within-session effects such as satiation (e.g. <monospace>Trial Number</monospace>), between-session heterogeneity in <monospace>IRI</monospace> slopes, and baseline signal dynamics (e.g. the functional random-intercept). The best model included subject-specific (indexed by <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and session-specific (indexed by <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) random functional intercepts and random functional slopes for <monospace>Lick Latency</monospace> (<inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Lick</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>):<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">IRI</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"/></mml:mtd><mml:mtd><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Lick</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>This can be fit with our package using the code:</p><p><code xml:space="preserve">model_fit = fui(photometry ~ IRI + (lick_latency | id/session), 
                        data = photometry_data, 
                         subj_ID = &quot;id&quot;)</code></p><p>This models <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mover><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where  <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-5-2-2"><title><monospace>Reward Number</monospace> model</title><p>Similarly we tested the association between the DA response to sucrose reward and <monospace>Reward Number</monospace> by modeling it with <monospace>Session Number</monospace> (<inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">SN</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), and <monospace>Trial Number</monospace> (indexed by <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> but written out as <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">TN</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> when used as a covariate), as shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. Our final model included subject-specific (indexed by <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and session-specific (indexed by <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) random functional intercepts and random functional slopes for <monospace>Lick Latency</monospace> (<inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Lick</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>):<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">SN</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">TN</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Lick</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>We fit the above model with the code:</p><p><code xml:space="preserve">model_fit = fui(photometry ~ trial + session + (lick_latency | id/session), 
                          data = photometry_data, 
                          subj_ID = &quot;id&quot;)</code></p><p>This models <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mover><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where  <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec></sec></sec><sec id="s4-6"><title>Reanalysis methods: Using <italic>FLMM</italic> to compare signal ‘temporal dynamics’ across conditions</title><p>We analyzed the data described in the section ‘Experiment 3: Tests 4–5’ (<xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>). The authors noted that “[W]hen a learned delay between cue onset and reward (3 seconds) is extended permanently to a new, longer delay (9 seconds), [Reward Prediction Error] predicts that as animals learn the longer delay…there will be a concomitant reduction in the dopamine cue response due to temporal discounting (46). By contrast, [their model] predicts little to no change in the dopamine cue response as the structure of the task is largely unchanged…Experimentally, [they] observed that although animals learned the new delay rapidly, dopaminergic cue responses showed no significant change” (<xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>).</p><p>Their analysis of DA cue-response was based on a baseline subtracted AUC (AUC [0,2 sec] - [–1,0 sec] AUC relative to cue onset). They write in their supplement (p. 4) that “Experiments 2–5: [T]o analyze learning-dependent dynamics of dopamine response, AUC of ΔF/F during 2 [sec] from CS +onset was normalized by AUC during baseline period.”</p><p>Our final model included a random slope for the long-delay (9 sec) indicator (<inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Delay</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), where <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Delay</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates long-delay trials:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Delay</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We analyzed all trials on the last session of the short-delay (2 sec) and all trials on the first session of the long-delay (8 sec). Because the experimental design included the delay length switch <italic>between</italic> sessions, the covariate <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="monospace">Delay</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> does not include a trial index <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi></mml:mstyle></mml:math></inline-formula> (i.e. all trials on session <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>l</mml:mi></mml:mstyle></mml:math></inline-formula> have the same delay length). This model assumes <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mover><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. This can be fit with the code:</p><p><code xml:space="preserve">model_fit = fui(photometry ~ delay + (delay | id), data = photometry_data)</code></p></sec><sec id="s4-7"><title>Functional mixed models package</title><p>We provide the fastFMM package that implements our proposed methods. The package can be downloaded on <monospace>CRAN</monospace>, or on the GitHub page: <ext-link ext-link-type="uri" xlink:href="https://github.com/gloewing/fastFMM">https://github.com/gloewing/fastFMM</ext-link>. This GitHub also includes our analysis guide, and examples of calling the package from python. We also recommend <monospace>lme4</monospace>’s thorough package resources, since our package is based on the <monospace>lme4</monospace> software and model syntax (e.g. see Table 2 in <xref ref-type="bibr" rid="bib8">Bates, 2014</xref>, Table 1 in <xref ref-type="bibr" rid="bib6">Barr et al., 2013</xref> and <xref ref-type="bibr" rid="bib7">Bates, 2010</xref>). <bold>Appendix 3.4</bold> contains more information about functional data analysis modeling approaches.</p><p>Our <monospace>fastFMM</monospace> package scales to the dataset sizes and model specifications common in photometry. The majority of the analyses presented in the <bold>Results</bold> included fairly simple functional fixed and random effect model specifications because we were implementing the <italic>FLMM</italic> versions of the summary measure analyses presented in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. However, we fit the following <italic>FLMM</italic> to demonstrate the scalability of our method with more complex model specifications:<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">SN</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">TN</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">IRI</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Lick</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">TL</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mn>5</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>We use the same notation as the <monospace>Reward Number</monospace> model in <bold>Reanalysis methods: Using</bold> <italic><bold>FLMM</bold></italic> <bold>to test associations between signal and covariates throughout the trial</bold>, with the additional variable <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">TL</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denoting the <monospace>Total Licks</monospace> on trial <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of session <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for animal <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In a dataset with over 3200 total trials (pooled across animals), this model took ∼1.2 min to fit on a MacBook Pro with an Apple M1 Max chip with 64 GB of RAM. Model fitting had a low memory footprint. This can be fit with the code:</p><p><code xml:space="preserve">model_fit = fui(photometry ~ session + trial +iri + lick_time + licks + 
           (session + trial + iri + lick_time + licks | id), 
           parallel = TRUE, 
           data = photometry_data)</code></p></sec><sec id="s4-8"><title>Simulation experiments</title><p>For the experiments reported in <bold>Using</bold> <italic><bold>FLMM</bold></italic> <bold>to compare signal ‘temporal dynamics’ across conditions</bold>, we generated synthetic photometry signals based on the Delay Length data and model described in <bold>Reanalysis methods: Using</bold> <italic><bold>FLMM</bold></italic> <bold>to compare signal ‘temporal dynamics’ across conditions</bold>. We simulated the photometry signals using the functional linear mixed effects model.<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Delay</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the (functional) intercept at time-point <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on all trials, <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the random (functional) intercept of subject <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> at time-point <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on all trials. The (functional) coefficient <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the average change in the mean signal at trial time-point <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> on long-delay trials (compared to short-delay trials). <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the error term and <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⊥</mml:mo><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mo>⊥</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. All other notation is described in <bold>Notation</bold> and <bold>Reanalysis methods: Using</bold> <italic><bold>FLMM</bold></italic> <bold>to compare signal ‘temporal dynamics’ across conditions</bold>.</p><p>For each simulation replicate, we drew <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> covariance matrix of random-effect <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> across trial time-points, estimated in the analysis described in <bold>Reanalysis Methods: Using FLMM to compare signal ‘temporal dynamics’ across conditions</bold>. For each <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>, we set <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> was the estimated coefficient vector, at <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>, from the model described in <bold>Reanalysis methods: Using</bold> <italic><bold>FLMM</bold></italic> <bold>to compare signal ‘temporal dynamics’ across conditions</bold>. We drew <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> vector of errors across trial time-points. The errors were drawn independently <italic>across</italic> different trials and subjects. To induce additional correlation across time-points within a trial, we set <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>∗</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the estimated covariance matrix of the model residuals fit to the observed photometry data of animal <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For each <italic>simulated</italic> subject, <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the index <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> was drawn uniformly from the observed animal indices <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> without replacement when the simulated sample size <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>≤</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Otherwise, we set the first 7 to the indices <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>, and the remaining <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> were drawn without replacement from the indices <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This induced some correlation between signals from different subjects when <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, which had a small but predictably deleterious effect on method performances.</p><p>For comparisons in the cue period window, we assessed the average performance of <italic>FLMM</italic> evaluated at time-points in that window, against the performance of standard methods that analyzed a summary measure of the window (cue period average-signal, which we denote as AUC) as the outcome variable. The LMM was applied to trial-level AUCs, the paired t-test was applied to subject- and condition-level AUCs. The performances of the <italic>Perm</italic> and <italic>FLMM</italic> were quantified according to the average pointwise performance during the cue period. We present the full consecutive threshold criteria of <italic>Perm</italic> in <xref ref-type="fig" rid="fig7">Figure 7</xref> and show the performance of the 1/2 consecutive threshold criteria in <bold>Appendix 5.2</bold> for completeness. We implemented the consecutive threshold criteria method, adapted to our simulated sampling rate of 15 Hz.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Software, Formal analysis, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Visualization, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-95802-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study reanalyzing existing public data, so no data have been generated for this manuscript. Modeling code is available on Github and CRAN.</p></sec><ack id="ack"><title>Acknowledgements</title><p>This research was supported by the Intramural Research Program of the National Institute of Mental Health (NIMH), project ZIC-MH002968 and the National Institute on Alcohol Abuse and Alcoholism (NIAAA), project ZIAAA000416. This study utilized the high-performance computational capabilities of the Biowulf Linux cluster at the National Institutes of Health, Bethesda, MD (<ext-link ext-link-type="uri" xlink:href="http://biowulf.nih.gov">http://biowulf.nih.gov</ext-link>). First, we would like to thank the corresponding authors of 'Mesolimbic dopamine release conveys causal associations,' Drs. Vijay Namboodiri and Huijeong Jeong, for sharing their data, helping us conduct analyses, and interpret the results. This work would not have been possible without their generosity, commitment to open science, and scientific rigor. We also thank Dr. Luke Coddington, the corresponding author of and 'Mesolimbic dopamine adapts the rate of learning from action' for generously sharing his data, and being so willing to engage in detailed and open discussions of our analyses. We thank Al Xin for their contributions to the fastFMM package. We would also like to thank Drs. Kauê Costa, Filipe Rodrigues, Guohong Cui, Guillermo Esber, Matthew Gardner, Charles Zheng, Sofia Beas, Adrina Kocharian, Geoff Schoenbaum, Hugo Tejeda, Mario Penzo, Dave Lovinger’s Lab, Joe Cheer’s lab, and the NIMH Machine Learning Core for helpful feedback on our manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aarts</surname><given-names>E</given-names></name><name><surname>Verhage</surname><given-names>M</given-names></name><name><surname>Veenvliet</surname><given-names>JV</given-names></name><name><surname>Dolan</surname><given-names>CV</given-names></name><name><surname>van der Sluis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A solution to dependency: using multilevel analysis to accommodate nested data</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>491</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1038/nn.3648</pub-id><pub-id pub-id-type="pmid">24671065</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adrian</surname><given-names>ED</given-names></name><name><surname>Matthews</surname><given-names>BHC</given-names></name></person-group><year iso-8601-date="1934">1934</year><article-title>The berger rhythm: Potential changes from the occipital lobes in man</article-title><source>Brain</source><volume>57</volume><fpage>355</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1093/brain/57.4.355</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amo</surname><given-names>R</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Yamanaka</surname><given-names>A</given-names></name><name><surname>Tanaka</surname><given-names>KF</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A gradual temporal shift of dopamine responses mirrors the progression of temporal difference error in machine learning</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>1082</fpage><lpage>1092</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01109-2</pub-id><pub-id pub-id-type="pmid">35798979</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baayen</surname><given-names>RH</given-names></name><name><surname>Davidson</surname><given-names>DJ</given-names></name><name><surname>Bates</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mixed-effects modeling with crossed random effects for subjects and items</article-title><source>Journal of Memory and Language</source><volume>59</volume><fpage>390</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2007.12.005</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barr</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Random effects structure for testing interactions in linear mixed-effects models</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>328</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00328</pub-id><pub-id pub-id-type="pmid">23761778</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barr</surname><given-names>DJ</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name><name><surname>Scheepers</surname><given-names>C</given-names></name><name><surname>Tily</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Random effects structure for confirmatory hypothesis testing: Keep it maximal</article-title><source>Journal of Memory and Language</source><volume>68</volume><fpage>255</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2012.11.001</pub-id><pub-id pub-id-type="pmid">24403724</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>lme4: Mixed-Effects Modeling with R</source><publisher-name>ETH Zürich</publisher-name><pub-id pub-id-type="doi">10.1002/9780470061602</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Fitting linear mixed-effects models using lme4</article-title><source>Journal of Statistical Software</source><volume>67</volume><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Burke</surname><given-names>DA</given-names></name><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>SA</given-names></name><name><surname>Floeder</surname><given-names>JR</given-names></name><name><surname>Namboodiri</surname><given-names>VMK</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Few-shot learning: Temporal scaling in behavioral and dopaminergic learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.03.31.535173</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>JY</given-names></name><name><surname>Jang</surname><given-names>HJ</given-names></name><name><surname>Ornelas</surname><given-names>S</given-names></name><name><surname>Fleming</surname><given-names>WT</given-names></name><name><surname>Fürth</surname><given-names>D</given-names></name><name><surname>Au</surname><given-names>J</given-names></name><name><surname>Bandi</surname><given-names>A</given-names></name><name><surname>Engel</surname><given-names>EA</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A comparison of dopaminergic and cholinergic populations reveals unique contributions of vta dopamine neurons to short-term memory</article-title><source>Cell Reports</source><volume>33</volume><elocation-id>108492</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2020.108492</pub-id><pub-id pub-id-type="pmid">33326775</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coddington</surname><given-names>LT</given-names></name><name><surname>Lindo</surname><given-names>SE</given-names></name><name><surname>Dudman</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Mesolimbic dopamine adapts the rate of learning from action</article-title><source>Nature</source><volume>614</volume><fpage>294</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05614-z</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Crainiceanu</surname><given-names>CM</given-names></name><name><surname>Goldsmith</surname><given-names>J</given-names></name><name><surname>Leroux</surname><given-names>A</given-names></name><name><surname>Cui</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2024">2024</year><source>Functional Data Analysis with R</source><publisher-name>Chapman and Hall/CRC</publisher-name><pub-id pub-id-type="doi">10.1201/9781003278726</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname><given-names>G</given-names></name><name><surname>Jun</surname><given-names>SB</given-names></name><name><surname>Jin</surname><given-names>X</given-names></name><name><surname>Pham</surname><given-names>MD</given-names></name><name><surname>Vogel</surname><given-names>SS</given-names></name><name><surname>Lovinger</surname><given-names>DM</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Concurrent activation of striatal direct and indirect pathways during action initiation</article-title><source>Nature</source><volume>494</volume><fpage>238</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1038/nature11846</pub-id><pub-id pub-id-type="pmid">23354054</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname><given-names>E</given-names></name><name><surname>Leroux</surname><given-names>A</given-names></name><name><surname>Smirnova</surname><given-names>E</given-names></name><name><surname>Crainiceanu</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Fast univariate inference for longitudinal functional models</article-title><source>Journal of Computational and Graphical Statistics</source><volume>31</volume><fpage>219</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1080/10618600.2021.1950006</pub-id><pub-id pub-id-type="pmid">35712524</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Functional mixed-effect models for electrophysiological responses</article-title><source>Neurophysiology</source><volume>41</volume><fpage>71</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1007/s11062-009-9079-y</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dudman</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><source>Data and Code Supporting &quot;mesolimbic Dopamine Adapts the Rate of Learning from Action</source><publisher-name>Janelia Research Campus</publisher-name></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fatt</surname><given-names>P</given-names></name><name><surname>Katz</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>Spontaneous subthreshold activity at motor nerve endings</article-title><source>The Journal of Physiology</source><volume>117</volume><fpage>109</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1952.sp004735</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fitzmaurice</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>Longitudinal Data Analysis</source><publisher-name>CRC press</publisher-name><pub-id pub-id-type="doi">10.1201/9781420011579</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greven</surname><given-names>S</given-names></name><name><surname>Crainiceanu</surname><given-names>C</given-names></name><name><surname>Caffo</surname><given-names>B</given-names></name><name><surname>Reich</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Longitudinal functional principal component analysis</article-title><source>Electronic Journal of Statistics</source><volume>4</volume><fpage>1022</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1214/10-EJS575</pub-id><pub-id pub-id-type="pmid">21743825</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gunaydin</surname><given-names>LA</given-names></name><name><surname>Grosenick</surname><given-names>L</given-names></name><name><surname>Finkelstein</surname><given-names>JC</given-names></name><name><surname>Kauvar</surname><given-names>IV</given-names></name><name><surname>Fenno</surname><given-names>LE</given-names></name><name><surname>Adhikari</surname><given-names>A</given-names></name><name><surname>Lammel</surname><given-names>S</given-names></name><name><surname>Mirzabekov</surname><given-names>JJ</given-names></name><name><surname>Airan</surname><given-names>RD</given-names></name><name><surname>Zalocusky</surname><given-names>KA</given-names></name><name><surname>Tye</surname><given-names>KM</given-names></name><name><surname>Anikeeva</surname><given-names>P</given-names></name><name><surname>Malenka</surname><given-names>RC</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Natural neural projection dynamics underlying social behavior</article-title><source>Cell</source><volume>157</volume><fpage>1535</fpage><lpage>1551</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.05.017</pub-id><pub-id pub-id-type="pmid">24949967</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Montanari</surname><given-names>A</given-names></name><name><surname>Rosset</surname><given-names>S</given-names></name><name><surname>Tibshirani</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Surprises in high-dimensional ridgeless least squares interpolation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1903.0856">https://arxiv.org/abs/1903.0856</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heien</surname><given-names>M</given-names></name><name><surname>Khan</surname><given-names>AS</given-names></name><name><surname>Ariansen</surname><given-names>JL</given-names></name><name><surname>Cheer</surname><given-names>JF</given-names></name><name><surname>Phillips</surname><given-names>PEM</given-names></name><name><surname>Wassum</surname><given-names>KM</given-names></name><name><surname>Wightman</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Real-time measurement of dopamine fluctuations after cocaine in the brain of behaving rats</article-title><source>PNAS</source><volume>102</volume><fpage>10023</fpage><lpage>10028</lpage><pub-id pub-id-type="doi">10.1073/pnas.0504657102</pub-id><pub-id pub-id-type="pmid">16006505</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jean-Richard-Dit-Bressel</surname><given-names>P</given-names></name><name><surname>Clifford</surname><given-names>CWG</given-names></name><name><surname>McNally</surname><given-names>GP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Analyzing event-related transients: Confidence intervals, permutation tests, and consecutive thresholds</article-title><source>Frontiers in Molecular Neuroscience</source><volume>13</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fnmol.2020.00014</pub-id><pub-id pub-id-type="pmid">32116547</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Taylor</surname><given-names>A</given-names></name><name><surname>Floeder</surname><given-names>JR</given-names></name><name><surname>Lohmann</surname><given-names>M</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Burke</surname><given-names>DA</given-names></name><name><surname>Namboodiri</surname><given-names>VMK</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mesolimbic dopamine release conveys causal associations</article-title><source>Science</source><volume>378</volume><elocation-id>eabq6740</elocation-id><pub-id pub-id-type="doi">10.1126/science.abq6740</pub-id><pub-id pub-id-type="pmid">36480599</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ju</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Moving block bootstrap for analyzing longitudinal data</article-title><source>Communications in Statistics</source><volume>44</volume><fpage>1130</fpage><lpage>1142</lpage><pub-id pub-id-type="doi">10.1080/03610926.2013.766341</pub-id><pub-id pub-id-type="pmid">26023251</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>S</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Influence of reward delays on responses of dopamine neurons</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>7837</fpage><lpage>7846</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1600-08.2008</pub-id><pub-id pub-id-type="pmid">18667616</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RS</given-names></name><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Parker</surname><given-names>NF</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reward prediction error does not explain movement selectivity in DMS-projecting dopamine neurons</article-title><source>eLife</source><volume>8</volume><elocation-id>e42992</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.42992</pub-id><pub-id pub-id-type="pmid">30946008</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RS</given-names></name><name><surname>Sagiv</surname><given-names>Y</given-names></name><name><surname>Engelhard</surname><given-names>B</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>A feature-specific prediction error model explains dopaminergic heterogeneity</article-title><source>Nature Neuroscience</source><volume>27</volume><fpage>1574</fpage><lpage>1586</lpage><pub-id pub-id-type="doi">10.1038/s41593-024-01689-1</pub-id><pub-id pub-id-type="pmid">38961229</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Loewinger</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Photometry_FLMM</data-title><version designator="swh:1:rev:604093c66985c8f21d26721189b0fe4769b87e6c">swh:1:rev:604093c66985c8f21d26721189b0fe4769b87e6c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:b14f3925ecbd7b03cf5b6b784de970b85f6c1af1;origin=https://github.com/gloewing/photometry_FLMM;visit=swh:1:snp:07209f9b11e3142eb72b6a8e5d7296b1e342cdf6;anchor=swh:1:rev:604093c66985c8f21d26721189b0fe4769b87e6c">https://archive.softwareheritage.org/swh:1:dir:b14f3925ecbd7b03cf5b6b784de970b85f6c1af1;origin=https://github.com/gloewing/photometry_FLMM;visit=swh:1:snp:07209f9b11e3142eb72b6a8e5d7296b1e342cdf6;anchor=swh:1:rev:604093c66985c8f21d26721189b0fe4769b87e6c</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magezi</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Linear mixed-effects models for within-participant psychology experiments: an introductory tutorial and free, graphical user interface (LMMgui)</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>2</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00002</pub-id><pub-id pub-id-type="pmid">25657634</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Gillis</surname><given-names>WF</given-names></name><name><surname>Jay</surname><given-names>M</given-names></name><name><surname>Wood</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>RW</given-names></name><name><surname>Cieszkowski</surname><given-names>R</given-names></name><name><surname>Scott</surname><given-names>R</given-names></name><name><surname>Brann</surname><given-names>D</given-names></name><name><surname>Koveal</surname><given-names>D</given-names></name><name><surname>Kula</surname><given-names>T</given-names></name><name><surname>Weinreb</surname><given-names>C</given-names></name><name><surname>Osman</surname><given-names>MAM</given-names></name><name><surname>Pinto</surname><given-names>SR</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Spontaneous behaviour is structured by reinforcement without explicit reward</article-title><source>Nature</source><volume>614</volume><fpage>108</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05611-2</pub-id><pub-id pub-id-type="pmid">36653449</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>JS</given-names></name><name><surname>Carroll</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Wavelet-based functional mixed models</article-title><source>Journal of the Royal Statistical Society. Series B, Statistical Methodology</source><volume>68</volume><fpage>179</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2006.00539.x</pub-id><pub-id pub-id-type="pmid">19759841</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>PEM</given-names></name><name><surname>Stuber</surname><given-names>GD</given-names></name><name><surname>Heien</surname><given-names>M</given-names></name><name><surname>Wightman</surname><given-names>RM</given-names></name><name><surname>Carelli</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Subsecond dopamine release promotes cocaine seeking</article-title><source>Nature</source><volume>422</volume><fpage>614</fpage><lpage>618</lpage><pub-id pub-id-type="doi">10.1038/nature01476</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Pinheiro</surname><given-names>J</given-names></name><name><surname>Bates</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Linear and nonlinear mixed effectsmodels</data-title><version designator="3.1-162">3.1-162</version><source>Nlme</source><ext-link ext-link-type="uri" xlink:href="https://svn.r-project.org/R-packages/trunk/nlme/">https://svn.r-project.org/R-packages/trunk/nlme/</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>L</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cell-type-specific activity in prefrontal cortex during goal-directed behavior</article-title><source>Neuron</source><volume>87</volume><fpage>437</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.021</pub-id><pub-id pub-id-type="pmid">26143660</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ruppert</surname><given-names>D</given-names></name><name><surname>Wand</surname><given-names>MP</given-names></name><name><surname>Carroll</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Semiparametric Regression</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511755453</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Säfken</surname><given-names>B</given-names></name><name><surname>Rügamer</surname><given-names>D</given-names></name><name><surname>Kneib</surname><given-names>T</given-names></name><name><surname>Greven</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Conditional model selection in mixed-effects models with caic4</article-title><source>Journal of Statistical Software</source><volume>99</volume><fpage>1</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.18637/jss.v099.i08</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheipl</surname><given-names>F</given-names></name><name><surname>Staicu</surname><given-names>A-M</given-names></name><name><surname>Greven</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Functional additive mixed models</article-title><source>Journal of Computational and Graphical Statistics</source><volume>24</volume><fpage>477</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1080/10618600.2014.901914</pub-id><pub-id pub-id-type="pmid">26347592</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheipl</surname><given-names>F</given-names></name><name><surname>Gertheiss</surname><given-names>J</given-names></name><name><surname>Greven</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Generalized functional additive mixed models</article-title><source>Electronic Journal of Statistics</source><volume>10</volume><fpage>1455</fpage><lpage>1492</lpage><pub-id pub-id-type="doi">10.1214/16-EJS1145</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sergazinov</surname><given-names>R</given-names></name><name><surname>Leroux</surname><given-names>A</given-names></name><name><surname>Cui</surname><given-names>E</given-names></name><name><surname>Crainiceanu</surname><given-names>C</given-names></name><name><surname>Aurora</surname><given-names>RN</given-names></name><name><surname>Punjabi</surname><given-names>NM</given-names></name><name><surname>Gaynanova</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A case study of glucose levels during sleep using multilevel fast function on scalar regression inference</article-title><source>Biometrics</source><volume>79</volume><fpage>3873</fpage><lpage>3882</lpage><pub-id pub-id-type="doi">10.1111/biom.13878</pub-id><pub-id pub-id-type="pmid">37189239</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serra</surname><given-names>F</given-names></name><name><surname>Terentjev</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Nonlinear dynamics of absorption and photobleaching of dyes</article-title><source>The Journal of Chemical Physics</source><volume>128</volume><elocation-id>224510</elocation-id><pub-id pub-id-type="doi">10.1063/1.2937455</pub-id><pub-id pub-id-type="pmid">18554032</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simpson</surname><given-names>EH</given-names></name><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Patriarchi</surname><given-names>T</given-names></name><name><surname>Blanco-Pozo</surname><given-names>M</given-names></name><name><surname>Burgeno</surname><given-names>LM</given-names></name><name><surname>Mohebi</surname><given-names>A</given-names></name><name><surname>Cragg</surname><given-names>SJ</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Lights, fiber, action! A primer on in vivo fiber photometry</article-title><source>Neuron</source><volume>112</volume><fpage>718</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.11.016</pub-id><pub-id pub-id-type="pmid">38103545</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wärnberg</surname><given-names>E</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Feasibility of dopamine as a vector-valued feedback signal in the basal ganglia</article-title><source>PNAS</source><volume>120</volume><elocation-id>e2221994120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2221994120</pub-id><pub-id pub-id-type="pmid">37527344</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>L</given-names></name><name><surname>Cameron</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Falkner</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Behavioural and dopaminergic signatures of resilience</article-title><source>Nature</source><volume>611</volume><fpage>124</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05328-2</pub-id><pub-id pub-id-type="pmid">36261520</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Ruppert</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Fast bivariate p-splines: the sandwich smoother</article-title><source>Journal of the Royal Statistical Society. Series B</source><volume>75</volume><fpage>577</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.1111/rssb.12007</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Z</given-names></name><name><surname>Guindani</surname><given-names>M</given-names></name><name><surname>Grieco</surname><given-names>SF</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Holmes</surname><given-names>TC</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Beyond t test and ANOVA: applications of mixed-effects models for more rigorous statistical analysis in neuroscience research</article-title><source>Neuron</source><volume>110</volume><fpage>21</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.10.030</pub-id><pub-id pub-id-type="pmid">34784504</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Photometry citations</title><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Photometry citations by year.</title><p>[Left] Google scholar mentions of the string ‘Fiber Photometry’ by year. There were 549 mentions between January 1, 2023 and June 22, 2023. [Right] Web of Science citations of papers that include the string ‘Fiber Photometry’ by year. There were 50 citations between January 1, 2023 and June 22, 2023. Blue lines indicate fitted values from an exponential fit to the data: <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="monospace">Citations</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>∗</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>exp</mml:mtext></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mi>β</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="monospace">Year</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> were estimated with the <monospace>nls</monospace> package in <monospace>R</monospace>. The 1500 references to photometry described in the main text refers to Google Scholar mentions in the 12 months prior to June 2023.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app1-fig1-v1.tif"/></fig></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>BH correction</title><p>The goal of this appendix is to illustrate the advantages of <italic>FLMM</italic> compared to fitting LMMs to each trial time-point separately, and then applying a multiple-comparisons correction (as proposed in <xref ref-type="bibr" rid="bib27">Lee et al., 2019</xref>). We specifically show that, in contrast to the approach applied in that work, <italic>FLMM</italic> yields far less conservative and more stable inference across different sub-sampling rates. We analyzed the Delay Length experiment (shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>) data sub-sampled at a range of sampling rates (data were sub-sampled at evenly spaced intervals). We fit either a collection of separate LMMs followed by a Benjamini–Hochberg (BH) correction, or <italic>FLMM</italic> with statistical significance determined from both Pointwise and Joint 95% CIs. To avoid introducing more notation, we show the <italic>FLMM</italic> model, with the understanding that the functional notation can be interpreted pointwise for the approach applied in <xref ref-type="bibr" rid="bib27">Lee et al., 2019</xref>:<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Delay</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>As shown in <xref ref-type="table" rid="app2table1 app2table2">Appendix 2—tables 1 and 2</xref> the proportion of time-points, <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, at which <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is statistically significant with <italic>FLMM</italic> Joint CIs is fairly stable across sampling rates. In contrast, the percentage is highly inconsistent with the BH approach. For example, the BH approach identifies ∼30% of time-points as significant at a sampling rate of 20 Hz, less than 1% at 25 Hz, ∼30% at 30 Hz and then drops again to ∼1% at 40 Hz. As the sampling rate grows towards 125 Hz (the sampling rate of the online dataset), the number of statistical comparisons grows, and the proportion of points that are significant drops to below 1%.</p><p>For the <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the BH approach identifies no time-points that achieve statistical significance, even for time-points when the animal is consuming the reward. In contrast, the Pointwise and Joint <italic>FLMM</italic> CIs identify a relatively stable proportion of time-points that are statistically significant. The <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is arguably a far more important point of comparison between these two statistical approaches, as the scientific question that motivated this analysis focuses on <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, rather than on <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Qualitatively speaking, the coefficient estimates, the widths of the 95% CIs, and the time intervals of statistical significance appear stable across the sampling rates, emphasizing how <italic>FLMM</italic> yields consistent inference and is not overly sensitive to the sub-sampling rate. We note that a multiple comparisons correction may yield more stable results if one first smooths regression coefficient point and variance estimates. To the best of our understanding, such smoothing was not conducted by <xref ref-type="bibr" rid="bib27">Lee et al., 2019</xref>. Moreover, such a strategy would essentially be a functional mixed model using a multiple comparisons correction, instead of a joint CI.</p><table-wrap id="app2table1" position="float"><label>Appendix 2—table 1.</label><caption><title>Functional intercept <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</title><p>Percentage of time-points in which the coefficient estimates <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are statistically significant. We compare the Benjamini–Hochberg (BH) correction applied to pointwise linear mixed models (LMM) models, the Functional Linear Mixed Models (<italic>FLMM)</italic> pointwise 95% CIs (Pointwise), and the <italic>FLMM</italic> joint 95% CIs (Joint). The proportion of points that are significant with the Benjamini–Hochberg (BH) approach jump around between 20-40Hz and then dramatically decrease as the sampling rate increases. In contrast, the <italic>FLMM</italic> Pointwise and Joint CIs are relatively stable and show only slight reductions in the proportion of points that are significant as the sampling rate increases.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Sampling rate (Hz)</th><th align="left" valign="bottom">BH</th><th align="left" valign="bottom">Pointwise</th><th align="left" valign="bottom">Joint</th></tr></thead><tbody><tr><td align="left" valign="bottom">20</td><td align="left" valign="bottom">31.00</td><td align="left" valign="bottom">23.58</td><td align="left" valign="bottom">9.17</td></tr><tr><td align="left" valign="bottom">25</td><td align="left" valign="bottom">0.73</td><td align="left" valign="bottom">23.27</td><td align="left" valign="bottom">9.09</td></tr><tr><td align="left" valign="bottom">30</td><td align="left" valign="bottom">30.23</td><td align="left" valign="bottom">22.67</td><td align="left" valign="bottom">8.72</td></tr><tr><td align="left" valign="bottom">40</td><td align="left" valign="bottom">1.09</td><td align="left" valign="bottom">22.66</td><td align="left" valign="bottom">8.50</td></tr><tr><td align="left" valign="bottom">50</td><td align="left" valign="bottom">0.73</td><td align="left" valign="bottom">22.38</td><td align="left" valign="bottom">8.28</td></tr><tr><td align="left" valign="bottom">125</td><td align="left" valign="bottom">0.80</td><td align="left" valign="bottom">22.02</td><td align="left" valign="bottom">7.63</td></tr></tbody></table></table-wrap><table-wrap id="app2table2" position="float"><label>Appendix 2—table 2.</label><caption><title>Functional slope <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</title><p>Percentage of time-points in which the coefficient estimates <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are statistically significant. We compare the Benjamini–Hochberg (BH) correction applied to pointwise linear mixed models (LMM) models, the Functional Linear Mixed Models (<italic>FLMM)</italic> pointwise 95% CIs (Pointwise), and the <italic>FLMM</italic> joint 95% CIs (Joint). The BH identifies no statistically significant effects at any time-points, whereas the the <italic>FLMM</italic> Pointwise and Joint CIs identify significant effects at a relatively consistent proportion of time-points.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Sampling rate (Hz)</th><th align="left" valign="bottom">BH</th><th align="left" valign="bottom">Pointwise</th><th align="left" valign="bottom">Joint</th></tr></thead><tbody><tr><td align="left" valign="bottom">20</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">46.72</td><td align="left" valign="bottom">17.90</td></tr><tr><td align="left" valign="bottom">25</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">44.36</td><td align="left" valign="bottom">16.73</td></tr><tr><td align="left" valign="bottom">30</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">43.31</td><td align="left" valign="bottom">15.41</td></tr><tr><td align="left" valign="bottom">40</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">43.57</td><td align="left" valign="bottom">15.90</td></tr><tr><td align="left" valign="bottom">50</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">43.46</td><td align="left" valign="bottom">14.39</td></tr><tr><td align="left" valign="bottom">125</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">42.15</td><td align="left" valign="bottom">13.66</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s10"><title>Functional mixed models methods</title><p>For a thorough introduction to the Functional Mixed Model fitting framework presented in our manuscript, please refer to <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>. However, for completeness, we provide a short description of the estimation details and then provide a brief derivation of our proposed estimator. The majority of the details presented here can be found in greater depth in section ‘3.1. Analytic Inference for Gaussian Functional Data’ of <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref> but we believe we have included a sufficient description of the statistical estimation scheme to explain the derivation of our proposed estimator (presented in <bold>Appendix 3.2</bold>). For clarity, we focus on functional <italic>linear</italic> mixed models. However, our package also provides the capability to fit the wider class of functional generalized linear mixed models for the distributions supported by the <monospace>R</monospace> <monospace>lme4</monospace> package.</p><sec sec-type="appendix" id="s10-1"><title>3.1 Functional linear mixed models</title><p>We focus on the functional linear mixed model,<disp-formula id="equ10"><label>(4)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> is the vector of observations of functional outcomes at time-point <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> for subject <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of functional observations of subject <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of points on the grid of the functional domain (i.e. the number of trial time-points or trial photometry samples). We take <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> to be the number of subjects (e.g. animals) and <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We denote <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> to be the vector of functional fixed-effects at point <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are vectors of mutually independent random-effects and error terms for time-point <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, with multivariate Gaussian distributions. We denote <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> as (design) matrices of subject <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> containing the covariates for the fixed- and random-effects terms, respectively. Denote <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> to be a block diagonal matrix where the <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>i</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> block entry is <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and let <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> refer to the design matrix from row concatenating the subject-specific matrices <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Estimates for the fixed-effects <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are correlated across trial time-points (i.e. the functional domain), which is incorporated into <italic>joint</italic> inference by assuming <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>q</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We used a two-stage approach for fitting functional linear mixed models. In the first step, we fit pointwise linear mixed models at each time-point <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>, yielding fixed-effect estimates which we denote as <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The pointwise estimator admits the closed form expression, <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are the covariance matrices of <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. The variance of the pointwise estimator is <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The correlation across trial time-points can be seen more explicitly through the following expression<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">W</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">W</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. While estimates of <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are provided by standard mixed modeling software, <inline-formula><mml:math id="inf214"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> must be estimated separately. We detail the estimation scheme below as it plays a critical role in <italic>joint</italic> inference and constitutes the main topic of our statistical contribution. Note that the estimates of <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">H</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> can be smoothed using, for example, fast bivaraite P-splines (<xref ref-type="bibr" rid="bib45">Xiao et al., 2013</xref>) along the functional domain (that is, trial time-points) to reduce variability; and then any negative eigenvalues of the smoothed matrices can be trimmed at 0 to ensure the resulting covariance matrix estimates are positive semi-definite (<xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>; <xref ref-type="bibr" rid="bib19">Greven et al., 2010</xref>).</p><p>Our package provides the option to use an array of smoothing approaches for the regression coefficient estimates, <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, but defaults to using penalized splines (<xref ref-type="bibr" rid="bib36">Ruppert et al., 2003</xref>). When using penalized splines to smooth the pointwise estimates, <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the covariance matrix for the final fixed-effects estimates admits a closed form expression, and allows for fast calculation of <italic>joint</italic> confidence band estimates. To see this, denote <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as the final fixed-effect estimates after smoothing the raw pointwise estimates, <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, directly provided by mixed model software. Then a smoothed estimator for the <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> fixed-effect is <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="double-struck">B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="double-struck">B</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">B</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is a <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-dimensional spline basis matrix, <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>λ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is a smoothing parameter, and <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is a penalty matrix. Typically the number of knots <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo>≪</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, but as long as a sufficient number of knots are specified, the smoother and exact <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> selected appear to have little impact on the final coefficient estimates or 95% CIs in practice. To be conservative, we used a high number of knots (<inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi><mml:mo>≈</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>). In practice, the specific number could be altered depending on the sampling rate of the photometry data analyzed and the specific sensor (since the kinetics of neurochemical signaling systems can vary widely). The results presented in the paper were calculated based upon implementation of our method using thin-plate splines since these performed well in practice.</p><p>Our package leverages the well-known <monospace>lme4</monospace> package in <monospace>R</monospace> for fitting the pointwise mixed effects models (<xref ref-type="bibr" rid="bib7">Bates, 2010</xref>; <xref ref-type="bibr" rid="bib8">Bates, 2014</xref>). This precludes, however, the specification of structure on the covariance matrix of the errors, <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> across trials (for a fixed trial time-point <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) in Gaussian models, functionality provided by, for example, the <monospace>nlme</monospace> in <monospace>R</monospace> (<xref ref-type="bibr" rid="bib34">Pinheiro and Bates, 2023</xref>). Future work could explore functional linear mixed models that allow for this functionality. However, in our experimentation analyzing photometry data summary measures (e.g. AUCs) from a range of experiments that observed multiple trials and sessions per animal, we found that model fit criteria (e.g. AIC, BIC, cAIC <xref ref-type="bibr" rid="bib37">Säfken et al., 2018</xref>) were usually much better when accounting for correlation across trials within-animal through random-effect specifications as opposed to placing structure on the covariance of the errors within-subject across trials. Moreover, we found that models that specified the types of error covariance structures applicable to photometry experiments were often slow to fit which is a drawback in our approach to fitting functional mixed models given that it requires fitting many pointwise models. For those reasons, we set our package’s default to use the <monospace>lme4</monospace> package thereby providing both speed and modeling flexibility to users.</p><p>We now provide a brief description of our proposed estimator for <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The critical role that this plays in inference and in neuroscience applications arises from the nested designs common in the sophisticated behavioral experiments commonly used alongside fiber photometry. We briefly describe this in <bold>Appendix 3.3</bold>.</p></sec><sec sec-type="appendix" id="s10-2"><title>3.2 Covariance estimator</title><p>We begin with a high-level description of the estimator for the covariance matrix, <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and include details below. Denote <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as the functional outcome at time-point <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, for subject <inline-formula><mml:math id="inf240"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf241"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The method of moments estimator proposed by <xref ref-type="bibr" rid="bib19">Greven et al., 2010</xref> and applied to our functional mixed model estimation procedure was presented in section 3.1, equation 4 from <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>,<disp-formula id="equ12"><label>(5)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mstyle></mml:math></inline-formula> are random-effect covariate indices. This expression suggests an estimator that regresses the residual products <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> onto the random-effect covariate products <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>:</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Concatenating these covariate products into a design matrix, <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, and concatenating the residual products into an ‘outcome vector,’ <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, allows us to express the estimator as a solution to the least squares problem for each pair (<inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>)<disp-formula id="equ13"><label>(6)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a matrix of the covariate products, <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, described above. After calculating <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is obtained by re-organizing the elements of the vector <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> into the entries of the matrix <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This estimator is flexible and in principle is agnostic to the random-effect specification, but in practice requires extension for general random-effects specifications, which we derive here.</p><p>Before discussing the extension to this estimator, we provide additional necessary notation. Since a single random-effect can require many columns of <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to encode the corresponding covariate (e.g. if there are many session-specific random intercepts), separate indices are needed to distinguish between a single random-effect distribution and the (potentially many) columns of the random-effect design matrix associated with draws from that random-effect distribution.</p><p>The <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, described above, is a block diagonal matrix where the <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> block entry is <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The number of columns, <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, will vary depending on the random-effect specification. For example, if one specifies a model that includes session-specific random-intercepts, this would require one column in <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (and thus multiple columns in <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>) for each animal and session to encode the corresponding animal-and session-specific covariates associated with these random-intercepts.</p><p>Let <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> be the number of unique random-effects distributions for a given model, where <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Take as an example a model that includes: (1) a subject-specific random intercept, <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, and (2) subject- and session-specific random intercepts, <inline-formula><mml:math id="inf264"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for participant <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> on session <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>l</mml:mi></mml:mstyle></mml:math></inline-formula>. If we assume that <inline-formula><mml:math id="inf267"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">∀</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>, then <inline-formula><mml:math id="inf268"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula>, but <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>q</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula> (and thus <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:math></inline-formula>) since encoding the subject- and session-specific random intercepts, <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> would require many indicator variables (entered as columns of <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>). More generally, for random-effect covariate, <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>, denote <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> as the set of column indices of <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that encode random-effect covariate <inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>. Although we describe <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as if it were drawn from a univariate Gaussian for explanatory purposes, neither the estimator nor the modeling software assume independence between random-effects from <italic>different</italic> covariates.</p><p>Expanding section 3.1, equation 4 from <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref> to be in terms of the column indices of the design matrix <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>υ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:munderover><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Now recall that <inline-formula><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mover><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. Thus, <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> for a covariance value, <inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, that is equal across all <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. It then follows that we can simplify the above as,<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="0.7em 0.3em" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>∗</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This expression suggests an estimation strategy that takes the product <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as a ‘covariate’ in the OLS-based estimator described in <xref ref-type="disp-formula" rid="equ12">Equation 5</xref>. Organizing these products into the columns of <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> does not result, however, in a full rank matrix for all random-effect specifications. In such cases, the solution to the problem described in <xref ref-type="disp-formula" rid="equ13">Equation 6</xref> is not unique. We solve for the ‘ridgeless regression’ solution to the problem described in <xref ref-type="disp-formula" rid="equ13">Equation 6</xref> because it yields the minimum <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> norm estimator and exhibits desirable statistical properties (<xref ref-type="bibr" rid="bib21">Hastie et al., 2019</xref>). This can be expressed as the solution to the optimization problem,<disp-formula id="equ16"> <label>(7)</label><mml:math id="m16"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo form="prefix">min</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are the residual products (i.e. with entries <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) and <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We construct <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> through reorganizing the elements of <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> into the matrix <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> can be estimated with the closed form expression,<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">M</mml:mi></mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> denotes the Moore-Penrose pseudoinverse inverse (<xref ref-type="bibr" rid="bib21">Hastie et al., 2019</xref>). Thus, while the estimator requires one to solve <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> problems of the form above (i.e. one for each unique <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> pair), in practice we only need to calculate <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> once for all <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> pairs. This allows for estimation of <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (i.e. for all unique <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> pairs) for datasets with many observations, complex random-effects specifications, and large <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi></mml:mstyle></mml:math></inline-formula> within a couple of seconds in total. We found that this approach performed well in practice, and it is the default in our package. All results presented in the main text apply to this version.</p><p>The above estimator allows for a fast, flexible extension of the work in <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>, thereby allowing for 95% CI calculation for general random-effects specifications. This statistical contribution is critical for neuroscience, since studies often exhibit nested experimental designs that require sophisticated random-effect models to properly capture the rich information contained in photometry signals.</p><sec sec-type="appendix" id="s10-2-1"><title>Alternative covariance estimators</title><p>We also explored a collection of related strategies. These are motivated by the fact that one can regress the residual products onto <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>:</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, instead of onto the products of the sums (across indices <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), as proposed above. Denote <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> as the coefficient estimate associated with the ‘covariate’ <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in this regression. We exploit the fact that <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for a covariance value, <inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mi>ρ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, common across all <inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">I</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. Solving for the desired quantity then yields the estimator<disp-formula id="equ18"><label>(8)</label><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We found this approach (available in our package by setting the argument <monospace>MoM</monospace> = 2) was slower, more memory intensive, and yielded confidence interval coverage comparable to the approach proposed above. We also explored the performance of two estimators for <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> based on mathematical programs that enforce non-negativity of variance components (before eigenvalue trimming). The first mathematical program is<disp-formula id="equ19"><label>(9)</label><mml:math id="m19"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo form="prefix">min</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>0</mml:mn><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">M</mml:mi></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the set of indices corresponding to variance elements.</p><p><xref ref-type="disp-formula" rid="equ18">Equation 8</xref> suggests that one could alternatively constrain the sum, as opposed to each element in the summation <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus we also explored performance of the estimator based on the mathematical program,<disp-formula id="equ20"><label>(10)</label><mml:math id="m20"><mml:mrow><mml:mtable columnalign="left left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo form="prefix">min</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">I</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">M</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>We found that, in practice, the performance of the estimators based on the mathematical programs <xref ref-type="disp-formula" rid="equ16">Equation 7</xref>, <xref ref-type="disp-formula" rid="equ19">Equation 9</xref>, and <xref ref-type="disp-formula" rid="equ20">Equation 10</xref>, performed comparably in simulations. This may be because the eigenvalue trimming applied to the solutions to the above optimization problems yields similar final estimates. Our software automatically implements estimators based on all approaches described above, but defaults to the first estimator proposed above, which is based on the solution to mathematical program <xref ref-type="disp-formula" rid="equ16">Equation 7</xref>.</p><p>In simulations, we observed that the regression coefficient estimate 95% CIs calculated using our proposed <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">G</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> estimator achieved nearly nominal <italic>joint</italic> coverage even with model specifications that yield reduced rank <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>. Therefore, the above estimator allows for a fast, flexible extension of the work in <xref ref-type="bibr" rid="bib14">Cui et al., 2022</xref>, thereby allowing for 95% CI calculation for general random-effects specifications. This statistical contribution is important for neuroscience since studies often exhibit nested experimental designs that require sophisticated random-effect models to properly capture the rich information contained in photometry signals.</p></sec><sec sec-type="appendix" id="s10-2-2"><title>Reduced rank problem</title><p>We point out that the method of moments estimators applied here (proposed in <xref ref-type="bibr" rid="bib19">Greven et al., 2010</xref>) exhibit a non-identifiability property. While this did not substantially influence the resulting 95% confidence interval coverage in our simulations, we argue that future attention to this issue is warranted. Such settings arise anytime one includes, for example, nested random-effect structures. This is a drawback because these types of random-effect specifications are critical to properly model the sophisticated behavioral designs common in neuroscience which often exhibit multiple layers of nesting. For instance, suppose a collection of animals are trained on multiple sessions, and within each session are trained on multiple trials. Then trial is nested in session, which is nested in animal/subject. Inclusion of animal- and session-specific random intercepts, a common random-effect specification for such experimental designs, will exhibit the challenges above.</p><p>In order to make the issue concrete, we describe perhaps the simplest case where the above problem presents. Specifically, we show how the original estimator requires extension in a simple <italic>FLMM</italic> model that is analogous to the functional analog of the paired samples t-test: a model that includes a single binary covariate such as a treatment indicator (i.e. <inline-formula><mml:math id="inf318"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>), a random subject-specific intercept, and a random subject-specific slope. Suppose we observe a collection of trials indexed by <inline-formula><mml:math id="inf319"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi></mml:mstyle></mml:math></inline-formula>, on a collection of animals indexed by <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>. Then the above model can be expressed as<disp-formula id="equ21"><label>(11)</label><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This model yields the vector of random-effect covariates <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the associated matrix <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is<disp-formula id="equ22"><mml:math id="m22"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The above equality follows trivially because <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> since <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is binary. The columns, therefore, exhibit the linear dependence <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and thus <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> is not full rank. To our knowledge, no re-coding of the binary variable <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> resolves the above linear dependence problem without changing the contrast e.g., one could fit a model without a random intercept and include the covariates <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>More generally, the reduced rank problem presents whenever a subject-specific random intercept is included (because it is encoded by a column of ones, <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, in <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) alongside any binary covariate in <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. As shown in the example above, this is because the column of ones used for random intercepts, when multiplied by binary covariates to construct the matrix <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, will create linear dependence in the columns of <inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>. This is problematic because binary covariates are needed to encode dichotomous and factor covariates (e.g. in the functional analog of t-tests and ANOVAs), as well as, for example, session-specific random intercepts in nested designs. As such, resolving this issue is critical to properly model experiments that regularly arise in neuroscience.</p></sec></sec><sec sec-type="appendix" id="s10-3"><title>3.3 Random-effects structures and interpretations for neuroscience experiments</title><p>Functional random intercepts that are unique to each trial and/or sessions can be used to account for animal-to-animal, session-to-session, and trial-to-trial heterogeneity in the signal and is one example where one might include a covariate in the random-effects (that is, included in <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) but not in the main effects terms, <inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Inclusion of these session- and trial-level random-effects is an example of one way to account for correlation within an animal across trials and sessions (photometry signal on trial <inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi></mml:mstyle></mml:math></inline-formula> for animal <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> is likely similar to the signal on trial <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>). Indeed, many neuroscience studies involving behavioral paradigms that involve multiple sessions and trials (and potentially multiple groups) may require complicated random-effects structures to account for animal-to-animal, session-to-session, and trial-to-trial heterogeneity in the photometry signal. For this reason, we explored the performance of many models to learn how to best model variability in the data.</p><p><italic>FLMM</italic> yields interpretable regression coefficient estimates. Conveniently, they can be interpreted conditional on the functional random-effects or marginally with respect to the random-effects since both interpretations are numerically equivalent in the linear setting. For an intuitive introduction to this equivalence for linear mixed models, we recommend sections 2.2 and 7.4 of <xref ref-type="bibr" rid="bib18">Fitzmaurice, 2008</xref>. We present the marginal interpretation here because it is often more intuitive for photometry analyses. However, the conditional version has its own advantages.</p></sec><sec sec-type="appendix" id="s10-4"><title>3.4 Functional regression model classes</title><table-wrap id="app3table1" position="float"><label>Appendix 3—table 1.</label><caption><title>Regression model classes based on functional vs scalar response variables and for longitudinal (repeated measures) vs cross-sectional data.</title><p>We use ‘functional mixed models’ as a short-hand for function-on-scalar mixed models. We use ‘functional regression’ as a short-hand for single-level function-on-scalar regression.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Longitudinal</th><th align="left" valign="bottom">Cross-sectional</th></tr></thead><tbody><tr><td align="left" valign="bottom">Functional</td><td align="left" valign="bottom">Functional mixed models</td><td align="left" valign="bottom">Functional regression</td></tr><tr><td align="left" valign="bottom">Scalar</td><td align="left" valign="bottom">Generalized linear mixed-effects (GLMM)</td><td align="left" valign="bottom">Generalized linear models (GLM)</td></tr></tbody></table></table-wrap><table-wrap id="app3table2" position="float"><label>Appendix 3—table 2.</label><caption><title>Cross-sectional regression model classes based on functional vs. scalar predictor variables (i.e. covariates) and functional vs. scalar outcome variables.</title><p>We take the FoFR, FoSR, and SoFR to be the single-level (non-longitudinal) versions of these methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Functional outcome</th><th align="left" valign="bottom">Scalar outcome</th></tr></thead><tbody><tr><td align="left" valign="bottom">Functional predictors</td><td align="left" valign="bottom">Function-on-function regression (FoFR)</td><td align="left" valign="bottom">Scalar-onfunction regression (SoFR)</td></tr><tr><td align="left" valign="bottom">Scalar predictors</td><td align="left" valign="bottom">Function-onscalar regression (FoSR)</td><td align="left" valign="bottom">Generalized linear models (GLM)</td></tr></tbody></table></table-wrap></sec></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s11"><title>Reanalysis figures and methods</title><p>In the following sections, we provide additional details for the reanalyses (<xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>) presented in the main text. In cases where it is helpful, we provide quotes from the original paper (<xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>) that informed our modeling decisions. These were taken from the main text of the paper, figure captions, and the supplement. In some places, we omitted minor portions of text to make the quotations more concise (e.g. figure references or citation numbers).</p><sec sec-type="appendix" id="s11-1"><title>4.1 ‘Using <italic>FLMM</italic> to test associations between signal and covariates throughout the trial’ reanalyses</title><fig id="app4fig1" position="float"><label>Appendix 4—figure 1.</label><caption><title><monospace>Reward Number</monospace>–Area Under the Curve (AUC) correlation within-session and across-session.</title><p>(<bold>A</bold>) Color indicates session number, rows denote animal number. <monospace>Trial Number</monospace> is the within-session <monospace>Reward Number</monospace> and ranges from 1 to 100 for each session. The black line that spans across sessions is a <monospace>Reward Number</monospace>–AUC linear regression fit, while the session color lines indicate a within-session <monospace>Trial Number</monospace>–AUC linear regression fit. The large black circles on the left side of each session-specific fit is the intercept, paramterized to yield the interpretation as the ‘expected AUC on the first trial of the corresponding session.’ Dotted horizontal lines are set at the median of the intercepts to facilitate comparison. The intercepts tend to rise across sessions, while few slopes are significantly positive. (<bold>B, C</bold>) Each dot indicates the estimated intercept value (<bold>B</bold>) or slope (<bold>C</bold>) from the fits shown in (<bold>A</bold>). Lines and p-values were calculated in an linear mixed model (LMM) that was fit to the session-specific linear regression slopes, <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and intercepts, <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, shown in (<bold>A</bold>). The LMM included animal-specific random intercepts and slopes. These plots quantify the trend observed in (<bold>A</bold>): the estimated intercepts significantly increase across sessions, but the slopes are mostly negative.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app4-fig1-v1.tif"/></fig><fig id="app4fig2" position="float"><label>Appendix 4—figure 2.</label><caption><title><monospace>Trial Number</monospace>–dopamine (DA) correlation within-session on the random inter-reward interval (<monospace>IRI</monospace>) task.</title><p>Row indicates session number. The intercept is paramterized to yield the interpretation as the ‘expected signal magnitude on the first trial of the corresponding session.’ Effects are aligned to the first lick after reward delivery. This is the session-by-session version of the analysis presented in <xref ref-type="fig" rid="fig5">Figure 5J–K</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app4-fig2-v1.tif"/></fig></sec><sec sec-type="appendix" id="s11-2"><title>4.2 Lick bout correlation</title><p>We present further analyses that illustrate how <italic>FLMM</italic> reveals effects obscured by standard methods. The authors note that the <monospace>Reward Number</monospace>–DA association, reported in their analyses, could arise from <monospace>Lick Rate</monospace> increases that also correlate with <monospace>Reward Number</monospace>. They tested this alternative explanation by applying a Pearson correlation between <monospace>Lick Rate</monospace> and DA. While they reported no significant association, the plot for the <monospace>Lick Rate</monospace> covariate from <italic>FLMM</italic> (<xref ref-type="fig" rid="app4fig3">Appendix 4—figure 3</xref>), shows <monospace>Lick Rate</monospace> exhibits (1) a significant positive association with DA before Lick-bout onset, and (2) a negative association after Lick-bout onset (reaching <italic>joint</italic> significance at ∼1 sec). These results suggest that the correlation analysis of reward period AUC in the paper missed this effect because the AUC summarized a time-window that contained opposing effects. The underlying association was likely diluted by averaging over time-points when <monospace>Lick Rate</monospace> is both positively correlated with DA (the first 0.5 sec of the reward period, visible in positive <italic>FLMM</italic> coefficient estimates) and negatively correlated with DA (the final 1 sec of the 1.5 sec time-window, with negative <italic>FLMM</italic> coefficient estimates). Identifying this effect with a summary measure would have required selecting these time intervals perfectly a priori; this is not necessary with <italic>FLMM</italic>.</p><fig id="app4fig3" position="float"><label>Appendix 4—figure 3.</label><caption><title>Functional Linear Mixed Models (<italic>FLMM</italic>) reveals details occluded by summary measure analyses.</title><p>Coefficient estimates from an <italic>FLMM</italic> analysis of the random inter-trial interval (<monospace>IRI</monospace>) reward delivery experiment. The top row contains the intercept term plots where the title provides an interpretation of the intercept: the average dopamine (DA) signal on trials when <monospace>Lick Rate</monospace> is at its average value. The bottom row shows the coefficient estimate plot of the covariate in the model. The ‘Baseline’ and ‘Reward Period’ bars show the trial period that the original authors used to calculate the summary measure (Area Under the Curve, AUC). Specifically, they quantified DA by a measure of normalized AUC of <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:mstyle></mml:math></inline-formula> during a window 0.5 sec before to 1 sec after the first lick following reward delivery. All plots are aligned to this first lick after reward delivery. The interpretation of the y-value of the bottom plot at any time-point <inline-formula><mml:math id="inf342"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>: the mean change in the dopamine signal at <inline-formula><mml:math id="inf343"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> for a one unit change in <monospace>Lick Rate</monospace>. Association between DA and <monospace>Lick Rate</monospace> aligned to lick bout onset. Time-points when <monospace>Lick Rate</monospace> was negatively associated with DA (negative coefficient estimates in the final 1 sec of the 1.5 sec window) may have diluted time-points when they were positively associated (positive coefficient estimates in the first 0.5 sec of the reward period).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app4-fig3-v1.tif"/></fig><sec sec-type="appendix" id="s11-2-1"><title><monospace>Lick Rate</monospace> model</title><p>We conducted an <italic>FLMM</italic> reanalysis of the analyses shown in Figure S8 C-D of <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. The figure caption notes that “The consummatory lick rate is not correlated with dopamine reward response.” We used the methods described in the quoted paragraphs above, and provide further details in <bold>Appendix 4.2</bold>. We fit the lick rate (<inline-formula><mml:math id="inf344"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext mathvariant="monospace">LR</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) <italic>FLMM</italic><disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">LR</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">TN</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <monospace>Trial Number</monospace> is denoted as <inline-formula><mml:math id="inf345"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">TN</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This can be fit with the code:</p><p><code xml:space="preserve">model_fit = fui(photometry ~ lick_rate + (trial | id/session), 
         data = photometry_data, 
         subj_ID = &quot;id&quot;)</code></p></sec><sec sec-type="appendix" id="s11-2-2"><title>Consummatory lick-bout extraction</title><p>As stated in ‘Data Analysis: Experiment 1’ of the Supplement (p.3–4), consummatory Lick-bouts were defined as follows:</p><p>“To test whether lick rate affects dopamine reward response, we first classified licks into consummatory and non-consummatory licks. A group of licks with less than 1 [sec] interval was defined as a lick bout...Every lick in the first lick bout after reward delivery was defined as a consummatory lick, and all other licks were defined as nonconsummatory licks...To avoid any influence of the previous reward on the dopamine response or behavior to the current reward, we excluded rewards with less than 3 [sec] IRI from the previous reward (22.6 ± 2.4%) for the above analyses. Rewards without lick until the next reward (5.7 ± 0.1%) were also excluded from analyses.”</p><p>For an <monospace>IRI</monospace> that was less than 3 sec, we excluded the trial before and after that <monospace>IRI</monospace> so as to avoid any influence on either trial’s signal. Our code implementations of the above methods can be found on the github page: <ext-link ext-link-type="uri" xlink:href="https://github.com/gloewing/photometry_fLMM">https://github.com/gloewing/photometry_fLMM</ext-link> (copy archived at <xref ref-type="bibr" rid="bib29">Loewinger, 2024</xref>).</p><p>The reward period time-window and methods were described in the Supplement of <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>:</p><p>“Dopamine response to reward was defined as the normalized area under curve (AUC) of ∆F/F during reward period. Reward period was defined as -0.5 to 1 [sec] from the first lick after reward delivery. We defined the window with reference to the first lick time, not reward delivery time, because the response latency to reward differs across trials. Also, we used a window starting from 0.5 [sec] ahead the first lick because dopamine response started to increase even before an animal made the first lick (as they get better at sensing reward delivery in late sessions). The AUC during 1.5 [sec] time window before reward period was subtracted from AUC during reward period to normalize baseline activity.”</p></sec></sec><sec sec-type="appendix" id="s11-3"><title>4.3 Photobleaching</title><p>In main text section <bold>Using FLMM to test associations between signal and covariates throughout the trial</bold>, we conducted analyses that showed DA decreased within-session during the post-lick interval of the reward period ([0, 1] sec from Lick-onset): <monospace>Trial Number</monospace> was negatively correlated with signal magnitude during the post-lick period. We include a portion of main <xref ref-type="fig" rid="fig5">Figure 5</xref> here to assist in the photobleaching discussion.</p><fig id="app4fig4" position="float"><label>Appendix 4—figure 4.</label><caption><title>Random inter-reward interval (<monospace>IRI</monospace>) experiment aligned to Lick-onset: <monospace>Reward Number</monospace>–dopamine (DA) association analyzed as within-session (<monospace>Trial Number</monospace>) and between-session (<monospace>Session Number</monospace>) linear effects.</title><p>The average reward signal shows the average trace with standard error of the mean indicated by the shaded region. The <monospace>Trial Number Session Number</monospace> effects are functional linear mixed model (<italic>FLMM</italic>) coefficient estimate plots.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app4-fig4-v1.tif"/></fig><p>We reasoned that if photobleaching caused the within-session decrease post-lick, indications of photobleaching would also be evident in other time-windows in the trial when the average signal was high (e.g. the pre-lick period). We assumed this would hold even if photobleaching depended on light intensity in a non-linear fashion (<xref ref-type="bibr" rid="bib41">Serra and Terentjev, 2008</xref>), since we reasoned the relationship would still be monotonic. The signal–<monospace>Trial Number</monospace> association is, however, actually slightly positive pre-lick (albeit non-significant), indicating that the signal does not decrease on average within-session pre-lick. Moreover, if the degree of photobleaching scales with signal magnitude, then we would expect to see a negative signal–<monospace>Trial Number</monospace> association only during the [0,0.5] sec time-window when the mean signal is higher than it was during the pre-lick period (since the pre-lick period does not exhibit any negative signal–Trial Number correlation). Instead, the negative correlation is evident in a time-window when the mean signal has returned to close to zero [0, 2.5] sec. In <xref ref-type="fig" rid="app4fig2">Appendix 4—figure 2</xref>, we show the same analyses conducted on each session separately. <italic>FLMM</italic> estimates a large positive <monospace>Trial Number</monospace> effect pre-lick and post-lick on session 4, despite the fact that the average signal was higher than in sessions 1–3 when the <monospace>Trial Number</monospace> is negative. Since there are sessions on which it is possible to detect positive <monospace>Trial Number</monospace> effects, we reasoned that photobleaching would not have occluded any true DA increases within-session. Finally, we repeated the above analyses while adjusting for lick frequency and found that controlling for various behavioral engagement summaries did not impact the <monospace>Trial Number</monospace> effects.</p><p>In <xref ref-type="fig" rid="app4fig5">Appendix 4—figure 5</xref>, we conducted a similar analysis but aligned the signal to reward-delivery. If photobleaching were the main cause of the within-session reductions described above, one might expect the <monospace>Trial Number</monospace> coefficient to be most negative when the average signal is most positive. Instead, these analyses show that the within-session signal decrease (i.e. the negative <monospace>Trial Number</monospace> effect) and the average signal exhibit distinct temporal dynamics. For example, the peak average signal occurs around 0.75 sec post-reward, while the <monospace>Trial Number</monospace> coefficient is most negative around 1.25–1.5 sec. Moreover, while the average signal magnitude has returned to nearly 0 around 1.75 sec, the <monospace>Trial Number</monospace> coefficient remains significantly negative until at least 2.5 sec after reward-delivery.</p><fig id="app4fig5" position="float"><label>Appendix 4—figure 5.</label><caption><title>Random inter-reward interval (<monospace>IRI</monospace>) experiment aligned to reward-delivery: <monospace>Reward Number</monospace>–dopamine (DA) association analyzed as within-session (<monospace>Trial Number</monospace>) and between-session (<monospace>Session Number</monospace>) linear effects.</title><p>The average reward signal shows the average trace with standard error of the mean indicated by the shaded region. The <monospace>Trial Number Session Number</monospace> effects are functional linear mixed model (<italic>FLMM</italic>) coefficient estimate plots.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app4-fig5-v1.tif"/></fig><sec sec-type="appendix" id="s11-3-1"><title>4.3.1 Photobleaching: Delay length experiment</title><p>We next sought to determine whether within-session decreases occurred in a different reward learning task collected from the same mice. We analyzed cue responses on the data from the Delay Length experiments (presented in main text section <bold>Using FLMM to compare signal ‘temporal dynamics’ across conditions</bold>). We used data from the final short-delay session because the animals were well trained on the Pavlovian task at that stage. During personal communications, <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> suggested these analyses because it provided an opportunity to analyze stabilized event-triggered DA responses in a different task from the same animals. <xref ref-type="fig" rid="app4fig6">Appendix 4—figure 6</xref> shows the peak magnitude of the average signal at cue-onset (of the delay length data) is about 4.5 <inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> units (see the Intercept plot), which is about 15% <italic>higher</italic> than the peak magnitude of the average signal during the post-lick period of the experiment descried above (5 shows that the peak magnitude of the average signal was about 4 <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> units). Because the signal is higher and photobleaching is thought to exert a greater effect on larger signals, one would expect photobleaching to have a <italic>larger</italic> effect in this experiment (i.e. a more negative <monospace>Trial Number</monospace> effect). However, <xref ref-type="fig" rid="app4fig6">Appendix 4—figure 6</xref> shows that the within-session signal <italic>increases</italic> significantly over trials during the cue period (i.e. a positive <monospace>Trial Number</monospace> effect). <xref ref-type="fig" rid="app4fig6">Appendix 4—figure 6</xref> also shows that the signal decreases within-session across trials at reward-delivery (3 sec after cue-onset) despite exhibiting a substantially lower average signal than during the Cue Period. This echos the within-session reductions observed around reward-consumption on the random <monospace>IRI</monospace> task above. We note additional analyses that adjusted for anticipatory licking and other indications of behavioral engagement did not noticeably impact the <monospace>Trial Number</monospace> effects. Analyses that pooled together multiple short-delay sessions yielded similar results. Taken together, these analyses of data from the same animals provide additional evidence against a photobleaching explanation as the main contributor for the within-session reduction described in <xref ref-type="fig" rid="fig5">Figure 5</xref>. One potential caveat is that there may be additional unmeasured factors (e.g. change in motivation) that increase across trials within a session, thereby occluding an effect of photobleaching. However, we reason that the parsimonious explanation in this case is that photobleaching is not a significant concern.</p><fig id="app4fig6" position="float"><label>Appendix 4—figure 6.</label><caption><title>Functional Linear Mixed Models (<italic>FLMM</italic>) identifies how the signal increases across trials during the Cue Period and decreases across trials after reward-delivery (3 sec).</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app4-fig6-v1.tif"/></fig></sec><sec sec-type="appendix" id="s11-3-2"><title>4.3.2 Photobleaching: Background reward experiment</title><p>Next, we show results from analyses suggested during personal communications with <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> to further rule out the possibility that the within-session decrease was a result of photobleaching. We specifically reanalyzed ‘Test 7’ in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref> (original analysis results shown in <xref ref-type="fig" rid="fig4">Figure 4M–P</xref> of their manuscript). They described the experiment as follows:</p><p>“To test whether the significant positive dopamine responses ollowing extinction reflect a retrospective association between the cue and reward, we selectively reduced the retrospective association without reducing the prospective association. We maintained the fixed reward following the cue but added unpredictable rewards during the inter-trial interval. In this experiment, not all rewards are preceded by the cue (i.e. retrospective association is weak), but all cues are followed by reward (i.e. prospective association is high). ANCCR predicts a rapid drop in dopamine cue response whereas RPE predicts no change in cue response if TDRL only considers the cue-reward ‘trial period’ (Test 7, fig. S10). The dopamine cue response remained significantly positive but decayed across trials faster than during extinction.”</p><p>This experiment was like long-delay sessions in the Delay Length experiment (i.e. a CS+ followed by reward-delivery 9 sec later), but it also included rewards delivered randomly without any predictive stimuli during the inter-trial interval. These ‘background rewards’ were similar to the reward-delivery schedule in the random <monospace>IRI</monospace> experiment presented in <xref ref-type="fig" rid="fig5">Figure 5</xref> and, therefore, provide a critical point of comparison. Similar to our random <monospace>IRI</monospace> experiment analyses, we analyzed the same ‘reward period’ time window aligned to the first lick after reward-delivery. We removed background reward ‘trials’ that were too close to each other to avoid signal bleed-over from adjacent trials (e.g. trials with inter-reward intervals that were too short), and trials for which no licks occurred between two successive reward-deliveries (background or reward-predicted) to avoid ‘double-counting’ a trial.</p><p><xref ref-type="fig" rid="app4fig7">Appendix 4—figure 7</xref> shows <italic>FLMM</italic> estimates a negative <monospace>Trial Number</monospace> effect. The magnitude of this negative coefficient is comparable to that seen in the random rewards experiments post-lick (i.e. after the first lick following reward-delivery). However, the within-session effect is most negative pre-lick, unlike what we observed in the random <monospace>IRI</monospace> task presented in <xref ref-type="fig" rid="fig5">Figure 5</xref>. It seems unlikely that this would be solely explained through satiation since the animals cannot be consuming the reward in the pre-lick period. Finally, we note additional analyses that adjust for lick frequency and other indications of behavioral engagement did not impact the <monospace>Trial Number</monospace> effects. Thus, while these results might be expected if photobleaching was causing the within-session signal reductions shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>, we argue that the results suggest that it is unlikely that photobleaching is the sole contributor to these within-session reductions.</p><fig id="app4fig7" position="float"><label>Appendix 4—figure 7.</label><caption><title>Background reward experiment analyses.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app4-fig7-v1.tif"/></fig></sec></sec><sec sec-type="appendix" id="s11-4"><title>4.4 Using <italic>FLMM</italic> to test signal changes within- and across-trials</title><p>We show how <italic>FLMM</italic> enables hypothesis testing of signal changes within- and across-trials. In the experiment in section ‘Tests 9–11’ of <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>, the authors test whether, across several sessions of Pavlovian learning, DA activity ‘backpropagates’from reward delivery (3 sec after cue-onset) to the presentation of reward-predictive cues. They analyzed a summary measure defined as the difference between the average signal during pre-reward-delivery (‘Late’) and cue-onset (‘Early’) time-windows.</p><p>We tested the ‘backpropagation’ question with a <italic>FLMM</italic> model with session binary indicators as covariates, similar to a functional repeated measures ANOVA. This yields estimates of mean signal changes, at each time-point, between pairs of sessions. We did not observe significant ‘Late’ period changes, consistent with the authors’ findings (<xref ref-type="fig" rid="app4fig8">Appendix 4—figure 8</xref>; see <xref ref-type="fig" rid="app4fig9">Appendix 4—figure 9</xref> for individual-animal fits). This analysis likely cannot be used to definitively rule out the existence of the ‘backpropagation’ phenomenon, but emphasizes how <italic>FLMM</italic> is well-suited to answer these types of questions. Nevertheless, we identified an additional effect that would be hard to find with summary measure analyses: the average (peak) size of cue-elicited DA, exhibited later in training, is similar to the degree that reward-delivery DA decreased. This is evident by comparing the symmetry between the magnitudes of peak increases during the ‘Early’ period and peak decreases during the post–‘Late’ period on the last session (<xref ref-type="fig" rid="app4fig8">Appendix 4—figure 8</xref>). This illustrates the capability in <italic>FLMM</italic> of directly testing effects visible in graphs, instead of having to perform hypothesis tests on a summary-of-summaries (e.g. a ratio or difference of AUCs).</p><fig id="app4fig8" position="float"><label>Appendix 4—figure 8.</label><caption><title>Functional Linear Mixed Models (<italic>FLMM</italic>) identifies how the signal evolves across trial time-points, and how the temporal location of transients progresses across sessions in a statistically significant manner.</title><p>The panels show coefficient estimates from <italic>FLMM</italic> analyses of the ‘backpropagation’ experiment. Panel (<bold>A</bold>) contains the intercept term plot corresponding to the average signal on the first session of training. The ‘Early’ (0–1 sec), ‘Mid’ (1–2 sec), and ‘Late’ (2–3 sec) bars show the trial time-periods that the original authors used to calculate summary Area Under the Curve (AUC) measures. Trials are aligned to cue onset (cues lasted 2 sec) and rewards were delivered at 3 sec. Panels (<bold>B</bold>-<bold>D</bold>) show the coefficient estimates corresponding to the mean change in signal values from the second, third, or, fourth sessions, respectively, <italic>compared to the first session</italic> (positive values indicate an increase from the first session). Plots are aligned to cue onset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app4-fig8-v1.tif"/></fig><fig id="app4fig9" position="float"><label>Appendix 4—figure 9.</label><caption><title>Individual-level coefficient estimates from functional linear mixed model (<italic>FLMM)</italic> analyses of the ‘backpropagation’ experiment: gold lines indicate the fixed-effect estimates and gray lines indicate animal-specific functional random-effect estimates (Best Linear Unbiased Predictor).</title><p>Panel (<bold>A</bold>) contains the intercept term plot where the title provides an interpretation: the average signal on the first session of training. The ‘Early’ (0–1 sec), ‘Mid’ (1–2 sec), and ‘Late’' (2–3 sec) bars show the trial time-periods that the original authors used to calculate summary Area Under the Curve (AUC) measures. Trials are aligned to cue onset and rewards were delivered at 3 sec. Panels (<bold>B</bold>-<bold>D</bold>) show the coefficient estimates which are interpreted as the change in mean signal from the second, third, or fourth sessions, respectively, compared to the first session (positive values indicate an increase from the first session). Plots are aligned to cue onset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app4-fig9-v1.tif"/></fig><sec sec-type="appendix" id="s11-4-1"><title>4.4.1 Reanalysis methods: Using <italic>FLMM</italic> to test signal changes within- and across-trials</title><p>We reanalyzed data presented in the section ‘Tests 9–11 (Backpropagation within a trial)’ of <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. We sought to evaluate the author’s hypothesis described in the following paragraph in the main text:</p><p>“[in the] TDRL RPE account (...) dopamine responses drive value learning of the immediately preceding state. We tested three predictions of this central postulate that are each inconsistent with ANCCR. The first is that during the acquisition of trace conditioning, dopamine response systematically back propagates from the moment mmediately prior to reward to the cue onset (50) (Test 9, Fig. 6A). Unlike TDRL RPE, ANCCR does not make such a prediction since delay periods are not broken into states in ANCCR (...) Our observations were not consistent with a backpropagating bump of activity and were instead consistent with an increase in cue response over trials of learning (Fig. 6B)”</p><p>We analyzed data from sessions 1–3, and the final session for each animal, as these were the sessions where we noticed the greatest changes. We fit a random slope model using indicator variables of the first, second, third, and subject-specific final sessions as covariates. We discarded data from other sessions and thus the interpretation of the intercept is the mean signal on the first session. Our final random slope model was<disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>+</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">S</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>l</mml:mi></mml:mstyle></mml:math></inline-formula> denotes the session number, <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="monospace">S</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is subject <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>’s final session (which can differ between animals), and <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is an indicator variable for session <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula>. We show the code from our package to fit the above model (note that the period variable below is a factor variable):</p><p><code xml:space="preserve">model_fit = fui(photometry ~ period + (period | id), data=photometry_data)</code></p></sec></sec></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s12"><title>Simulations</title><sec sec-type="appendix" id="s12-1"><title>5.1 Simulation scheme</title><p>As described in the main text, we simulated data from the model<disp-formula id="equ25"><label>(12)</label><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">Delay</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We take the simulated <inline-formula><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> in <bold>Appendix 4.7</bold> to be equal to the estimated coefficients from the model above fit to the real data in <xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>. To ‘simulate’ the covariates (i.e. just the <monospace>Delay</monospace> indicator vector), we randomly draw a subset of animal IDs and concatenate all of their <italic>observed</italic> covariates from the (<xref ref-type="bibr" rid="bib24">Jeong et al., 2022</xref>). That is for each simulation replicate, <inline-formula><mml:math id="inf354"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>, of sample size <inline-formula><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>, we randomly draw a sample of <inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> animal subject IDs, denoted as <inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>n</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> from the set <inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> with uniform probability without replacement. For each animal ID in the sample, <inline-formula><mml:math id="inf359"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>n</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, we concatenate the covariates (i.e. the design matrix) of the corresponding subjects in the <italic>observed</italic> data to be the ‘simulated’ covariates. That is, the design matrix <inline-formula><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> for simulation replicate <inline-formula><mml:math id="inf361"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>, is the row concatenation of each <inline-formula><mml:math id="inf362"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">X</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf363"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s12-2"><title>5.2 Additional simulation results</title><p>To explore how analyzing summary measures can drown-out effects, we compared method performances on the same analyses across different cue period lengths (2 sec, 2.5 sec, or 3 sec from cue onset), which we visualize in <xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1A</xref>. These relatively small adjustments substantially influenced estimation performance (see <xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1B</xref>), and statistical power (see <xref ref-type="fig" rid="app5fig1">Appendix 5—figures 1D</xref> and <xref ref-type="fig" rid="app5fig2">2</xref>). The pointwise 95 % CI coverage (see <xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1C</xref>) of the summary measure approaches (t-test and LMM) is also very sensitive to the specified length of the cue period because they analyze a summary that averages over a time-window that contains heterogeneous effects. However, <italic>FLMM</italic> and <italic>Perm</italic> CI coverages (averaged over the cue period) remain stable to differing time-window lengths, because they evaluate each time-point. Notably, <italic>Perm</italic> exhibits low coverage at smaller sample sizes and the t-test yields poor coverage in all settings tested, likely because of the animal-to-animal variability in the signal magnitude and Delay Length change effect (the data were simulated from a LMM that specifies independence between observations conditional on random-effects. Since the t-test does not include random-effects, it may not achieve the nominal coverage because it relies on a different conditional independence assumption). Finally, <xref ref-type="fig" rid="app5fig3">Appendix 5—figure 3</xref> shows <italic>FLMM</italic> fits in around 10 seconds for datasets with about 800 trials (pooled across animals). Taken together, these data-driven simulations demonstrate that at low sample sizes, and in the presence of individual-differences, the <italic>FLMM</italic> consistently (1) achieves roughly nominal <italic>pointwise</italic> and <italic>joint</italic> coverage, (2) improves statistical power, and (3) can be fit quickly.</p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>Summary measure analyses are highly sensitive to minor changes in the summary time-window.</title><p>(<bold>A</bold>) Average short/long-delay data. Bars show cue period length used in (2 sec) and ‘extended‘ delays analyzed in additional simulations. (<bold>B</bold>) Estimation error (RMSE), <inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>R</mml:mi></mml:msqrt></mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf365"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, associated with a mean difference during the cue periods (panels) and <inline-formula><mml:math id="inf366"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> on the x-axis. Lower numbers indicate more accurate estimates. (<bold>C</bold>) pointwise 95% CI coverage is associated with a mean difference during cue period (panels) and <inline-formula><mml:math id="inf367"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> on the x-axis. Higher values indicate better CI coverage. (<bold>D</bold>) Statistical power during cue period. The linear mixed model (LMM) and t-test were fit on the signal averaged over the cue period and thus each simulation replicate yields a single indicator of CI inclusion or statistical significance, which we represent with a line plot. For other methods, estimates are provided at each time-point, and performance is averaged across the time-points. We summarize these simulation replicate-specific averages with a boxplot.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app5-fig1-v1.tif"/></fig><p>Here, we define the estimation error (RMSE) of statistical methods for the average difference in photometry signal amplitude during the cue period as a function of how long that cue period was (2 sec, 2.5 sec, 3 sec). The error was defined as <inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>R</mml:mi></mml:msqrt></mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf369"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. That is, the average of the coefficients for covariate <inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> across time-points <inline-formula><mml:math id="inf371"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> in a fixed interval (indexed by <inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Since the model only contains a single binary covariate, we can compare the average of functional coefficients for the slope parameter in a <italic>FLMM</italic> model with the slope coefficient estimate in a scalar LMM applied on the outcome: <inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and a paired-samples t-test using the outcome: <inline-formula><mml:math id="inf374"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">J</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the set of trials observed for subject <inline-formula><mml:math id="inf376"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>.</p><fig id="app5fig2" position="float"><label>Appendix 5—figure 2.</label><caption><title>Statistical power associated with mean difference defining the cue period as 2 sec, 2.5 sec, and 3 sec (panels) and sample sizes (numbers of animals) on the x-axis.</title><p>The two panels presents the same data in either violin or boxplot forms. Higher numbers indicate better power. For functional linear mixed model (<italic>FLMM)</italic> and <italic>Perm</italic>, power is averaged across the time-points in the cue period whereas the others assess the power using the average signal (across the cue period) as the outcome. Since each simulation replicate takes the proportion of significant time-points in the cue period for <italic>FLMM</italic> and <italic>Perm</italic>, these are presented as boxplots (or violin plots), whereas the rest are simply presented as the proportion of simulation replicates that identified the mean signal during the cue as statistically significant (either 0 or 1 for each replicate).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app5-fig2-v1.tif"/></fig><fig id="app5fig3" position="float"><label>Appendix 5—figure 3.</label><caption><title>Time to fit functional linear mixed model (<italic>FLMM</italic>) fit with our software (using a closed-form variance calculation) on simulated data (each data point represents one replicate).</title><p><monospace>pffr</monospace> shows the time to fit the functional linear mixed model (with the same model specification) with the <monospace>pffr()</monospace> function in the <monospace>refund</monospace> package. Number of animals in simulations shown in plots ranges from 4 to 8 (i.e. <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mn>100</mml:mn></mml:mstyle></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app5-fig3-v1.tif"/></fig><fig id="app5fig4" position="float"><label>Appendix 5—figure 4.</label><caption><title>Functional Linear Mixed Models (<italic>FLMM</italic>) fit with our software achieves comparable or superior performance to the functional linear mixed model (with the same model specification) fit with the <monospace>pffr()</monospace> function in the <monospace>refund</monospace> package.</title><p>(<bold>A</bold>) <italic>FLMM</italic> achieves <italic>joint</italic> 95% CI coverage at roughly the nominal level. <monospace>pffr</monospace> does not provide <italic>joint</italic> 95% CIs and thus the <italic>pointwise</italic> 95% CIs that it does provide achieve low <italic>joint</italic> coverage. (<bold>B</bold>) <italic>FLMM</italic> achieves <italic>pointwise</italic> 95% CI coverage at or above the nominal level. The <italic>pointwise</italic> 95% CI coverage of <monospace>pffr</monospace> is close to but below the nominal level. <italic>pointwise</italic> 95% CI coverage associated with mean difference during cue period (panels) and <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> on the x-axis. Higher values indicate better CI coverage. (<bold>C</bold>) <italic>FLMM</italic> and <monospace>pffr</monospace> exhibit comparable fixed-effects estimation performance. Estimation error (RMSE), <inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mi>R</mml:mi></mml:msqrt></mml:mfrac><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, associated with mean difference during the cue periods (panels) and <inline-formula><mml:math id="inf381"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> on the x-axis. Lower numbers indicate more accurate estimates. (<bold>D</bold>) <italic>FLMM</italic> exhibits superior statistical power compared to <monospace>pffr</monospace> during the cue period. (<bold>B</bold>-<bold>D</bold>) Since estimates are provided at each timepoint for both methods, pointwise performance is averaged across the time-points. We summarize these simulation replicate-specific averages as one point in a boxplot.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app5-fig4-v1.tif"/></fig></sec></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s13"><title>Additional reanalyses</title><p>We include analyses here conducted on a second recent article <xref ref-type="bibr" rid="bib11">Coddington et al., 2023</xref>; <xref ref-type="bibr" rid="bib16">Dudman, 2023</xref> proposing a new reinforcement learning model for the role of mesolimbic dopamine in learning.</p><sec sec-type="appendix" id="s13-1"><title>6.1 Additional reanalyses results</title><sec sec-type="appendix" id="s13-1-1"><title>6.1.1 Functional methods allow for testing how signal ‘dynamics’ early in training predict behavior later in training</title><p>We next analyze data from a second paper focused on the role of mesolimbic DA in reward learning (<xref ref-type="bibr" rid="bib11">Coddington et al., 2023</xref>). We first examine how between-animal differences in behavior correlate with average nucleus accumbens dopamine neuron calcium (NAc–DA) changes (DAT-Cre::ai32 transgenic mice were injected with a Cre-dependent jRCaMP1b virus across the ventral midbrain enabling the measurement of calcium dynamics in mesolimbic DA cells specifically). In this experiment, mice were exposed to a 0.5 sec stimulus, followed by reward 1 sec after cue-offset. The authors identified significant correlations between average Reward period NAc–DA (trial-averaged on <italic>early</italic> training sessions) with measures of average behavior (trial-averaged on <italic>late</italic> sessions).</p><p>We fit two analogous univariate models with the average behavioral measures as the covariate (cases without repeated observations on the same animal are equivalent to a <italic>functional</italic> linear regression without mixed effects).</p><fig id="app6fig1" position="float"><label>Appendix 6—figure 1.</label><caption><title>Coefficient estimates from Functional Linear Mixed Models (<italic>FLMM)</italic> analyses of <monospace>Final Latency</monospace> and <monospace>Lick Probability</monospace> models.</title><p>The top row contains intercept term plots where the title provides interpretation of the intercept: the average dopamine neuron calcium (NAc-DA) signal on trials when the covariates in the model are at their average value. The bottom row shows the coefficient estimate plot of the covariate in the model. (Left) Association between average NAc-DA (averaged over first 100 trials) and latency to lick <monospace>Final Latency</monospace> (averaged over trials 700–800). (Right) Association between average NAc-DA (averaged over first 100 trials) lick probability (averaged over trials 700–800).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app6-fig1-v1.tif"/></fig><p>Our results reveal details missed in the summary measure analyses. Consistent with the author’s findings, we found Reward period NAc–DA was associated with <monospace>Final Latency</monospace>. However, our analyses also revealed significant associations during the Cue period, an effect hard to see on average traces: while the average signal increases substantially during the Reward period, it exhibits comparatively little change during the Cue period. This demonstrates the difficulty of constructing summary measures: the time-windows when the signal is associated with covariates may not align with time-periods when the average signal exhibits noticeable changes. Remarkably, the association sign differs from the direction of average signal change: <monospace>Final Latency</monospace> is <italic>positively</italic> associated with NAc-DA during both Cue and Reward periods, yet the mean signal <italic>decreases</italic> during the Cue Period and <italic>increases</italic> during the Reward period.</p></sec><sec sec-type="appendix" id="s13-1-2"><title>6.1.2 <italic>FLMM</italic> provides test of how signal differences <italic>between</italic> trial-types change <italic>across</italic> training</title><p>Our final example demonstrates how questions that might otherwise require analysis of summaries of summary measures (e.g. ratios of average AUCs), can be precisely specified in a functional model so as to provide greater detail. Specifically, the authors sought to investigate how the difference in NAc-DA signals between trials with (Lick+) and without (Lick-) preparatory licking changed with learning. They reported a significant correlation between reward collection latency (<monospace>Final Latency</monospace>) and a “ratio of NAc–DA reward signals [AUCs] on Lick- vs Lick+ trials.” We fit an <italic>FLMM</italic> model with an interaction between <monospace>Final Latency</monospace> and <monospace>Lick State</monospace> (a Lick-/Lick+ indicator). The interaction term provides a hypothesis test of their question at <italic>each trial time-point</italic>. The longest portion of joint significance occurs <italic>between</italic> Cue and Reward periods. Our analysis confirms the authors’ results and adds detail obscured by standard methods: the interaction does not reach joint statistical significance until a couple seconds after reward-delivery, well-beyond the time when the average signal has fallen from its peak (shown by the intercept). This example highlights the challenge with constructing an adequate summary measure for this analysis: the period of clearest effect of the <monospace>Lick State</monospace> is before the average signal is largest. This may explain why the authors did not construct a summary measure during the interval between the Cue and Reward periods, when the <monospace>Lick State</monospace> effect was strongest. In sum, this example demonstrates how summary-of-summary measure analyses can be translated into simple <italic>FLMM</italic> regressions that provide greater detail about the time-course and magnitude of the effects throughout the trial.</p><fig id="app6fig2" position="float"><label>Appendix 6—figure 2.</label><caption><title>Coefficient estimates from a single functional linear mixed model (<italic>FLMM</italic>) analysis of <monospace>Final Latency</monospace> × <monospace>Lick State</monospace> interaction model (including main effects).</title><p>A simple <italic>FLMM</italic> model enables characterization of how the association between photometry signals and behavioral responding (<monospace>Final Latency</monospace>) differs between conditions (Lick+/Lick-) at each time-point in the trial. Panel (<bold>A</bold>) contains the intercept term plot where the title provides an interpretation: the average photometry signal on Lick- trials for animals that exhibit average <monospace>Final Latency</monospace> values. Panels (<bold>B</bold>-<bold>D</bold>) show the three covariates of main effects and interaction of <monospace>Lick State</monospace> and<monospace> Final Latency</monospace>. The <monospace>Final Latency</monospace> functional coefficient is interpreted as the effect of <monospace>Final Latency</monospace> on Lick- trials. The <monospace>Lick State</monospace> main effect is interpreted as the difference in average NAc-DA between Lick+ and Lick- trials for an animal with an average <monospace>Final Latency</monospace> value. The interaction is interpreted as a difference in differences: during the Reward period, a 1 standard deviation increase in average <monospace>Final Latency</monospace> is associated with pointwise significantly higher NAc–DA signals on Lick+ than on Lick- trials (with a portion of joint significance) during most of the Reward period.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-95802-app6-fig2-v1.tif"/></fig></sec></sec><sec sec-type="appendix" id="s13-2"><title>6.2 Additional reanalyses methods</title><sec sec-type="appendix" id="s13-2-1"><title>6.2.1 Methods for Appendix 6.1.1</title><p>This paper measured photometry signals in which DAT-Cre::ai32 transgenic mice were injected with a Cre-dependent jRCaMP1b virus across the ventral midbrain enabling the measurement of calcium dynamics in mesolimbic DA cells specifically. We sought to conduct an analysis based upon the following quote:</p><p>“Unexpectedly, initial NAc–DA reward signals were negatively correlated with the amount of preparatory behaviour at the end of raining (NAc–DA reward trials 1-100 versus preparatory index trials 700-800, r=-0.85, P=0.004), as well as the speed of reward collection (NAc-DA reward trials 1-100 versus reward collection latency trials 700-800, r=0.81, P=0.008).”</p><p>Preparatory licking and latency-to-lick were used as indicators of learning.</p><p>Since there is no way to meaningfully pair the neural activity of one trial with behavior on a separate trial, analyzing trial-level data is not appropriate and we therefore modeled the <italic>average</italic> photometry signal (averaged across trials 1–100) as a function of <italic>average</italic> behavior (averaged across trials 700–800). We removed trials with latencies over 1 second as they constituted behavioral outliers comprising less than 1% of trials.</p><p>To conduct a functional regression most analogous to the Pearson correlations the authors conducted, we fit the univariate linear regression models for final latency, <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="monospace">FL</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ26"><mml:math id="m26"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi>Y</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">FL</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Similarly, we fit the lick probability, <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="monospace">LP</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, functional regression model,<disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi>Y</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">LP</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>These models were fit with the <monospace>R</monospace> package <monospace>refund</monospace> with the <monospace>fosr()</monospace> function (where fosr abbreviates function-on-scalar regression used for a functional outcome and scalar covariate). Note that this package function only provides <italic>pointwise</italic> 95% CIs and thus the plots in the main text only have one shade of gray for the 95% CIs. Thus the interpretation for this analysis is confined to a <italic>pointwise</italic> interpretation.</p></sec><sec sec-type="appendix" id="s13-2-2"><title>6.2.2 Methods for Appendix 6.1.2</title><p>We sought to conduct an analysis based upon the following quote: “[the proposed] scheme also predicts that [mesolimbic DA] reward signals should reflect the evolution of reward collection policy across learning…Indeed,…mouse data exhibited differential reward responses on trials with (‘Lick+’) or without (‘Lick-’) preparatory licking as learning progressed” (<xref ref-type="bibr" rid="bib11">Coddington et al., 2023</xref>). The authors reported (see the Figure 8d caption, <xref ref-type="bibr" rid="bib11">Coddington et al., 2023</xref>) a significant Pearson correlation between the final reward collection latency (<monospace>Final Latency</monospace>) and a “ratio of NAc–DA reward signals on Lick- vs Lick+ trials.” They constructed this measure by first summarizing the reward-period NAc-DA (with AUC), averaged across Lick+ and Lick- trials separately, and then calculating a ratio for each animal. Similar to the analysis we presented in the previous section, the authors compared behavior in one set of trials to dopamine activity in a separate set of trials. Because there is no way to meaningfully pair the neural activity of one trial with behavior on a separate trial, we analyzed the <italic>average</italic> photometry signal (Lick+/Lick- separately averaged over the trials specified in the paper) as a function of <italic>average</italic> behavior.</p><p>Specifically, we modeled the trial-averaged signal denoted as <inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mover><mml:mi>Y</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> for animal <inline-formula><mml:math id="inf385"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> and lick state <inline-formula><mml:math id="inf386"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi></mml:mstyle></mml:math></inline-formula>. The signal was trial-averaged separately across Lick+ and Lick- trials. We both indicate lick state with the subscript <inline-formula><mml:math id="inf387"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi></mml:mstyle></mml:math></inline-formula> on <inline-formula><mml:math id="inf388"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mover><mml:mi>Y</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> and also as a covariate in the model where <inline-formula><mml:math id="inf389"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="monospace">LS</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula> for Lick+ trials and <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="monospace">LS</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula> for Lick- trials. We denote the final latency below as <inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mtext mathvariant="monospace">FL</mml:mtext></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. We fit the random slope <italic>FLMM</italic> model,<disp-formula id="equ28"><mml:math id="m28"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi>Y</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">FL</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">LS</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">FL</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="monospace">LS</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Given that the outcome was trial-averaged, the number of observations of the outcome was only twice the number of animals (i.e. one observation of the functional outcome for Lick+ and one for Lick-). We were thus limited in the number of random-effects we could include and still retain an identifiable model. For that reason, our model did not include a random intercept. As for all other analyses in our manuscript, we compared multiple candidate models that we specified based upon the author’s analyses. We then selected the model with the best AIC/BIC model fit criteria.</p><p>We normalized the (average) <monospace>Final Latency</monospace> variable (across trials) to have mean 0 and unit variance. The intercept, therefore, shows that mean NAc–DA activity for a mouse with average <monospace>Final Latency</monospace> on Lick- trials is <italic>jointly</italic> significantly elevated during portions of the Reward period. The <monospace>Final Latency</monospace> functional coefficient is interpreted as the effect of <monospace>Final Latency</monospace> on Lick- trials and is only briefly pointwise significant in the middle of the Reward period. The <monospace>Lick State</monospace> main effect is interpreted as the difference in average NAc–DA between Lick+ and Lick- trials for an animal with an average <monospace>Final Latency</monospace> value. The interaction is interpreted as a difference in differences: during the Reward period, a 1 standard deviation increase in average <monospace>Final Latency</monospace> is associated with pointwise significantly higher NAc–DA signals on Lick+ than on Lick- trials (with a portion of joint significance) during most of the Reward period.</p></sec></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95802.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Iordanova</surname><given-names>Mihaela D</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Concordia University</institution><country>Canada</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study presents a statistical framework for the analysis of photometry signals and provides an open-source implementation. The evidence supporting the benefits of the presented functional mixed-effect modeling analysis as opposed to (1) summary statistics and (2) other pointwise regression models is <bold>convincing</bold> with a thorough comparison with other methods and datasets. This work will be of great interest to researchers using not only fiber photometry, but other time-series data such as calcium imaging or electrophysiology data, and wanting to implement trial-by-trial temporal analysis, taking also into account variability within the dataset.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95802.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Fiber photometry has become a very popular tool in recording neuronal activity in freely behaving animals. Despite the number of papers published with the method, as the authors rightly note, there are currently no standardized ways to analyze the data produced. Moreover, most of the data analyses confine to simple measurements of averaged activity and by doing so, erase valuable information encoded in the data. The authors offer an approach based on functional linear mixed modeling, where beyond changes in overall activity various functions of the data can also be analyzed. More in depth analysis, more variables taken into account, better statistical power all lead to higher quality science.</p><p>Strengths:</p><p>The framework the authors present is solid and well explained. By reanalyzing formerly published data, the authors also further increase the significance of the proposed tool opening new avenues for reinterpreting already collected data. They also made a convincing case showing that the proposed algorithm works on data with different preprocessing backgrounds.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95802.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This work describes a statistical framework that combines functional linear mixed modeling with joint 95% confidence intervals, which improves statistical power and provides less conservative and more robust statistical inferences than in previous studies. Pointwise linear regression analysis has been used extensively to analyze time series signals from a wide range of neuroscience recording techniques, with recent studies applying them to photometry data. The novelty of this study lies in (1) the introduction of joint 95% confidence intervals for statistical testing of functional mixed models with nested random-effects, and (2) providing an open-source R package implementing this framework. This study also highlights how summary statistics as opposed to trial-by-trial analysis can obscure or even change the direction of statistical results by reanalyzing two other studies.</p><p>Strengths:</p><p>The open-source package in R using a similar syntax as lme4 package for the implementation of this framework, the high fitting speed and the low memory footprint, even in complex models, enhance the accessibility and usage by other researchers.</p><p>The reanalysis of two studies using summary statistics on photometry data (Jeong et al., 2022; Coddington et al., 2023) highlights how trial-by-trial analysis at each time-point on the trial can reveal information obscured by averaging across trials. Furthermore, this work also exemplifies how session and subject variability can lead to different conclusions when not considered.</p><p>This study also showcases the statistical robustness of FLMM by comparing this method to fitting pointwise linear mixed models and performing t-test and Benjamini-Hochberg correction as performed by Lee et al. (2019).</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95802.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Loewinger et al. extend a previously described framework (Cui et al., 2021) to provide new methods for statistical analysis of fiber photometry data. The methodology combines functional regression with linear mixed models, allowing inference on complex study designs that are common in photometry studies. To demonstrate its utility, they reanalyze datasets from two recent fiber photometry studies into mesolimbic dopamine. Then, through simulation, they demonstrate the superiority of their approach compared to other common methods.</p><p>Strengths:</p><p>The statistical framework described provides a powerful way to analyze photometry data and potentially other similar signals. The provided package makes this methodology easy to implement and the extensively worked examples of reanalysis provide a useful guide to others on how to correctly specify models.</p><p>Modeling the entire trial (function regression) removes the need to choose appropriate summary statistics, removing the opportunity to introduce bias, for example in searching for optimal windows in which to calculate the AUC. This is demonstrated in the re-analysis of Jeong et al., 2022, in which the AUC measures presented masked important details about how the photometry signal was changing. There is an appropriate level of discussion of the interpretation of the reanalyzed data that highlights the pitfalls of other methods and the usefulness of their methods.</p><p>The authors' use of linear mixed methods, allows for the estimation of random effects, which are an important consideration given the repeated-measures design of most photometry studies.</p><p>The authors provide a useful guide for how to practically use and implement their methods in an easy-to-use package. These methods should have wide applicability to those who use photometry or similar methods. The development of this excellent open-source software is a great service to the wider neuroscience community.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.95802.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Loewinger</surname><given-names>Gabriel</given-names></name><role specific-use="author">Author</role><aff><institution>National Institute of Mental Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Cui</surname><given-names>Erjia</given-names></name><role specific-use="author">Author</role><aff><institution>University of Minnesota</institution><addr-line><named-content content-type="city">Minneapolis</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Lovinger</surname><given-names>David</given-names></name><role specific-use="author">Author</role><aff><institution>National Institute on Alcohol Abuse and Alcoholism</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Pereira</surname><given-names>Francisco</given-names></name><role specific-use="author">Author</role><aff><institution>National Institutes of Health / National Institute of Mental Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife Assessment</bold></p><p>This important work presents a new methodology for the statistical analysis of fiber photometry data, improving statistical power while avoiding the bias inherent in the choices that are necessarily made when summarizing photometry data. The reanalysis of two recent photometry data sets, the simulations, and the mathematical detail provide convincing evidence for the utility of the method and the main conclusions, however, the discussion of the re-analyzed data is incomplete and would be improved by a deeper consideration of the limitations of the original data. In addition, consideration of other data sets and photometry methodologies including non-linear analysis tools, as well as a discussion of the importance of the data normalization are needed.</p></disp-quote><p>Thank you for reviewing our manuscript and giving us the opportunity to respond and improve our paper. In our revision, we have strived to address the points raised in the comments, and implement suggested changes where feasible. We have also improved our package and created an analysis guide (available on our Github - <ext-link ext-link-type="uri" xlink:href="https://github.com/gloewing/fastFMM">https://github.com/gloewing/fastFMM</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/gloewing/photometry_fGLMM">https://github.com/gloewing/photometry_fGLMM</ext-link>), showing users how to apply our methods and interpret their results. Below, we provide a detailed point-by-point response to the reviewers.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1:</bold></p><p>Summary:</p><p>Fiber photometry has become a very popular tool in recording neuronal activity in freely behaving animals. Despite the number of papers published with the method, as the authors rightly note, there are currently no standardized ways to analyze the data produced. Moreover, most of the data analyses confine to simple measurements of averaged activity and by doing so, erase valuable information encoded in the data. The authors offer an approach based on functional linear mixed modeling, where beyond changes in overall activity various functions of the data can also be analyzed. More in-depth analysis, more variables taken into account, and better statistical power all lead to higher quality science.</p><p>Strengths:</p><p>The framework the authors present is solid and well-explained. By reanalyzing formerly published data, the authors also further increase the significance of the proposed tool opening new avenues for reinterpreting already collected data.</p></disp-quote><p>Thank you for your favorable and detailed description of our work!</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>However, this also leads to several questions. The normalization method employed for raw fiber photometry data is different from lab to lab. This imposes a significant challenge to applying a single tool of analysis.</p></disp-quote><p>Thank you for these important suggestions. We agree that many data pre-processing steps will influence the statistical inference from our method. Note, though, that this would also be the case with standard analysis approaches (e.g., t-tests, correlations) applied to summary measures like AUCs. For that reason, we do not believe that variability in pre-processing is an impediment to widespread adoption of a standard analysis procedure. Rather, we would argue that the sensitivity of analysis results to pre-processing choices should motivate the development of statistical techniques that reduce the need for pre-processing, and properly account for structure in the data arising from experimental designs. For example, even without many standard pre-processing steps, <italic>FLMM</italic> provides smooth estimation results across trial timepoints (i.e., the 'functional domain'), has the ability to adjust for betweentrial and -animal heterogeneity, and provides a valid statistical inference framework that quantifies the resulting uncertainty. We appreciate the reviewer’s suggestion to emphasize and further elaborate on our method from this perspective. We have now included the following in the Discussion section:</p><p><italic>“FLMM</italic> can help model signal components unrelated to the scientific question of interest, and provides a systematic framework to quantify the additional uncertainty from those modeling choices. For example, analysts sometimes normalize data with trial-specific baselines because longitudinal experiments can induce correlation patterns across trials that standard techniques (e.g., repeated measures ANOVA) may not adequately account for. Even without many standard data pre-processing steps, <italic>FLMM</italic> provides smooth estimation results across trial time-points (the 'functional domain'), has the ability to adjust for between-trial and -animal heterogeneity, and provides a valid statistical inference approach that quantifies the resulting uncertainty. For instance, session-to-session variability in signal magnitudes or dynamics (e.g., a decreasing baseline within-session from bleaching or satiation) could be accounted for, at least in part, through the inclusion of trial-level fixed or random effects. Similarly, signal heterogeneity due to subject characteristics (e.g., sex, CS+ cue identity) could be incorporated into a model through inclusion of animal-specific random effects. Inclusion of these effects would then influence the width of the confidence intervals. By expressing one’s 'beliefs' in an <italic>FLMM</italic> model specification, one can compare models (e.g., with AIC). Even the level of smoothing in <italic>FLMM</italic> is largely selected as a function of the data, and is accounted for directly in the equations used to construct confidence intervals. This stands in contrast to 'trying to clean up the data' with a pre-processing step that may have an unknown impact on the final statistical inferences.”</p><disp-quote content-type="editor-comment"><p>Does the method that the authors propose work similarly efficiently whether the data are normalized in a running average dF/F as it is described in the cited papers? For example, trace smoothing using running averages (Jeong et al. 2022) in itself may lead to pattern dilution.</p></disp-quote><p>By modeling trial signals as 'functions', the method accounts for and exploits correlation across trial timepoints and, as such, any pre-smoothing of the signals should not negatively affect the validity of the 95% CI coverage. It will, however, change inferential results and the interpretation of the data, but this is not unique to <italic>FLMM</italic>, or many other statistical procedures.</p><disp-quote content-type="editor-comment"><p>The same question applies if the z-score is calculated based on various responses or even baselines. How reliable the method is if the data are non-stationery and the baselines undergo major changes between separate trials?</p></disp-quote><p>Adjustment for trial-to-trial variability in signal magnitudes or dynamics could be accounted for, at least in part, through the inclusion of trial-level random effects. This heterogeneity would then influence the width of the confidence intervals, directly conveying the effect of the variability on the conclusions being drawn from the data. This stands in contrast to 'trying to clean up the data' with a pre-processing step that may have an unknown impact on the final statistical inferences. Indeed, non-stationarity (e.g., a decreasing baseline within-session) due to, for example, measurement artifacts (e.g., bleaching) or behavioral causes (e.g., satiation, learning) should, if possible, be accounted for in the model. As mentioned above, one can often achieve the same goals that motivate pre-processing steps by instead applying specific <italic>FLMM</italic> models (e.g., that include trial-specific intercepts to reflect changes in baseline) to the unprocessed data. One can then compare model criteria in an objective fashion (e.g., with AIC) and quantify the uncertainty associated with those modeling choices. Even the level of smoothing in <italic>FLMM</italic> is largely selected as a function of the data, and is accounted for directly in the equations used to construct confidence intervals. In sum, our method provides both a tool to account for challenges in the data, and a systematic framework to quantify the additional uncertainty that accompanies accounting for those data characteristics.</p><disp-quote content-type="editor-comment"><p>Finally, what is the rationale for not using non-linear analysis methods? Following the paper’s logic, non-linear analysis can capture more information that is diluted by linear methods.</p></disp-quote><p>This is a good question that we imagine many readers will be curious about as well. We have added in notes to the Discussion and Methods Section 4.3 to address this (copied below). We thank the reviewer for raising this point, as your feedback also motivated us to discuss this point in Part 5 of our Analysis Guide.</p><p>Methods</p><p><italic>“FLMM</italic> models each trial’s signal as a function that varies smoothly across trial time-points (i.e., along the 'functional domain'). It is thus a type of non-linear modeling technique over the functional domain, since we do not assume a linear model (straight line). <italic>FLMM</italic> and other functional data analysis methods model data as functions, when there is a natural ordering (e.g., time-series data are ordered by time, imaging data are ordered by x-y coordinates), and are assumed to vary smoothly along the functional domain (e.g., one assumes values of a photometry signal at close time-points in a trial have similar values). Functional data analysis approaches exploit this smoothness and natural ordering to capture more information during estimation and inference.”</p><p>Discussion</p><p>“In this paper, we specified <italic>FLMM</italic> models with linear covariate–signal relationships <italic>at a fixed trial time-point</italic> across trials/sessions, to compare the <italic>FLMM</italic> analogue of the analyses conducted in (Jeong et al., 2022). However, our package allows modeling of covariate–signal relationships with non-linear functions of covariates, using splines or other basis functions. One must consider, however, the tradeoff between flexibility and interpretability when specifying potentially complex models, especially since <italic>FLMM</italic> is designed for statistical inference.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>Summary:</p><p>This work describes a statistical framework that combines functional linear mixed modeling with joint 95% confidence intervals, which improves statistical power and provides less conservative statistical inferences than in previous studies. As recently reviewed by Simpson et al. (2023), linear regression analysis has been used extensively to analyze time series signals from a wide range of neuroscience recording techniques, with recent studies applying them to photometry data. The novelty of this study lies in (1) the introduction of joint 95% confidence intervals for statistical testing of functional mixed models with nested random-effects, and (2) providing an open-source R package implementing this framework. This study also highlights how summary statistics as opposed to trial-by-trial analysis can obscure or even change the direction of statistical results by reanalyzing two other studies.</p><p>Strengths:</p><p>The open-source package in R using a similar syntax as the lme4 package for the implementation of this framework on photometry data enhances the accessibility, and usage by other researchers. Moreover, the decreased fitting time of the model in comparison with a similar package on simulated data, has the potential to be more easily adopted.</p><p>The reanalysis of two studies using summary statistics on photometry data (Jeong et al., 2022; Coddington et al., 2023) highlights how trial-by-trial analysis at each time-point on the trial can reveal information obscured by averaging across trials. Furthermore, this work also exemplifies how session and subject variability can lead to opposite conclusions when not considered.</p></disp-quote><p>We appreciate the in-depth description of our work and, in particular, the R package. This is an area where we put a lot of effort, since our group is very concerned with the practical experience of users.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>Although this work has reanalyzed previous work that used summary statistics, it does not compare with other studies that use trial-by-trial photometry data across time-points in a trial. As described by the authors, fitting pointwise linear mixed models and performing t-test and BenjaminiHochberg correction as performed in Lee et al. (2019) has some caveats. Using joint confidence intervals has the potential to improve statistical robustness, however, this is not directly shown with temporal data in this work. Furthermore, it is unclear how FLMM differs from the pointwise linear mixed modeling used in this work.</p></disp-quote><p>Thank you for making this important point. We agree that this offers an opportunity to showcase the advantages of FLMM over non-functional data analysis methods, such as the approach applied in Lee et al. (2019). As mentioned in the text, fitting entirely separate models at each trial timepoint (without smoothing regression coefficient point and variance estimates across timepoints), and applying multiple comparisons corrections as a function of the number of time points has substantial conceptual drawbacks. To see why, consider that applying this strategy with two different sub-sampling rates requires adjustment for different numbers of comparisons, and could thus lead to very different proportions of timepoints achieving statistical significance. In light of your comments, we decided that it would be useful to provide a demonstration of this. To that effect, we have added Appendix Section 2 comparing FLMM with the method in Lee et al. (2019) on a real dataset, and show that FLMM yields far less conservative and more stable inference across different sub-sampling rates. We conducted this comparison on the delay-length experiment (shown in Figure 6) data, sub-sampled at evenly spaced intervals at a range of sampling rates. We fit either a collection of separate linear mixed models (LMM) followed by a Benjamini–Hochberg (BH) correction, or <italic>FLMM</italic> with statistical significance determined with both Pointwise and Joint 95% CIs. As shown in Appendix Tables 1-2, the proportion of timepoints at which effects are statistically significant with FLMM Joint CIs is fairly stable across sampling rates. In contrast, the percentage is highly inconsistent with the BH approach and is often highly conservative. This illustrates a core advantage of functional data analysis methods: borrowing strength across trial timepoints (i.e., the functional domain), can improve estimation efficiency and lower sensitivity to how the data is sub-sampled. A multiple comparisons correction may, however, yield stable results if one first smooths both regression coefficient point and variance estimates. Because this includes smoothing the coefficient point and variance estimates, this approach would essentially constitute a functional mixed model estimation strategy that uses multiple comparisons correction instead of a joint CI. We have now added in a description of this experiment in Section 2.4 (copied below).</p><p>“We further analyze this dataset in Appendix Section 2, to compare <italic>FLMM</italic> with the approach applied in Lee et al. (2019) of fitting <italic>pointwise</italic> LMMs (without any smoothing) and applying a Benjamini–Hochberg (BH) correction. Our hypothesis was that the Lee et al. (2019) approach would yield substantially different analysis results, depending on the sampling rate of the signal data (since the number of tests being corrected for is determined by the sampling rate). The proportion of timepoints at which effects are deemed statistically significant by <italic>FLMM</italic> joint 95% CIs is fairly stable across sampling rates. In contrast, that proportion is both inconsistent and often low (i.e., highly conservative) across sampling rates with the Lee et al. (2019) approach. These results illustrate the advantages of modeling a trial signal as a function, and conducting estimation and inference in a manner that uses information across the entire trial.”</p><disp-quote content-type="editor-comment"><p>In this work, FLMM usages included only one or two covariates. However, in complex behavioral experiments, where variables are correlated, more than two may be needed (see Simpson et al. (2023), Engelhard et al. (2019); Blanco-Pozo et al. (2024)). It is not clear from this work, how feasible computationally would be to fit such complex models, which would also include more complex random effects.</p></disp-quote><p>Thank you for bringing this up, as we endeavored to create code that is able to scale to complex models and large datasets. We agree that highlighting this capability in the paper will strengthen the work. We now state in the Discussion section that “[T]he package is fast and maintains a low memory footprint even for complex models (see Section 4.6 for an example) and relatively large datasets.” Methods Section 4.6 now includes the following:</p><p>Our fastFMM package scales to the dataset sizes and model specifications common in photometry. The majority of the analyses presented in the Results Section (Section 2) included fairly simple functional fixed and random effect model specifications because we were implementing the <italic>FLMM</italic> versions of the summary measure analyses presented in Jeong et al. (2022). However, we fit the following <italic>FLMM</italic> to demonstrate the scalability of our method with more complex model specifications:</p><p><inline-formula><mml:math id="sa4m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="monospace">S</mml:mi><mml:mi mathvariant="monospace">N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="monospace">T</mml:mi><mml:mi mathvariant="monospace">N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="monospace">I</mml:mi><mml:mi mathvariant="monospace">R</mml:mi><mml:mi mathvariant="monospace">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="monospace">L</mml:mi><mml:mi mathvariant="monospace">i</mml:mi><mml:mi mathvariant="monospace">c</mml:mi><mml:mi mathvariant="monospace">k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="monospace">T</mml:mi><mml:mi mathvariant="monospace">L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>We use the same notation as the Reward Number model in Section 4.5.2, with the additional variable TL_i,j,l_ denoting the Total Licks on trial <italic>j</italic> of session <italic>l</italic> for animal <italic>i</italic>. In a dataset with over 3,200 total trials (pooled across animals), this model took ∼1.2 min to fit on a MacBook Pro with an Apple M1 Max chip with 64GB of RAM. Model fitting had a low memory footprint. This can be fit with the code:</p><p>model_fit = fui(photometry ~ session + trial + iri + lick_time + licks + (session + trial + iri + lick_time + licks | id), parallel = TRUE, data = photometry_data).</p><p>This provides a simple illustration of the scalability of our method. The code (including timing) for this demonstration is now included on our Github repository.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>Summary:</p><p>Loewinger et al., extend a previously described framework (Cui et al., 2021) to provide new methods for statistical analysis of fiber photometry data. The methodology combines functional regression with linear mixed models, allowing inference on complex study designs that are common in photometry studies. To demonstrate its utility, they reanalyze datasets from two recent fiber photometry studies into mesolimbic dopamine. Then, through simulation, they demonstrate the superiority of their approach compared to other common methods.</p><p>Strengths:</p><p>The statistical framework described provides a powerful way to analyze photometry data and potentially other similar signals. The provided package makes this methodology easy to implement and the extensively worked examples of reanalysis provide a useful guide to others on how to correctly specify models.</p><p>Modeling the entire trial (function regression) removes the need to choose appropriate summary statistics, removing the opportunity to introduce bias, for example in searching for optimal windows in which to calculate the AUC. This is demonstrated in the re-analysis of Jeong et al., 2022, in which the AUC measures presented masked important details about how the photometry signal was changing.</p><p>Meanwhile, using linear mixed methods allows for the estimation of random effects, which are an important consideration given the repeated-measures design of most photometry studies.</p></disp-quote><p>We would like to thank the reviewer for the deep reading and understanding of our paper and method, and the thoughtful feedback provided. We agree with this summary, and will respond in detail to all the concerns raised.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>While the availability of the software package (fastFMM), the provided code, and worked examples used in the paper are undoubtedly helpful to those wanting to use these methods, some concepts could be explained more thoroughly for a general neuroscience audience.</p></disp-quote><p>Thank you for this point. While we went to great effort to explain things clearly, our efforts to be concise likely resulted in some lack of clarity. To address this, we have created a series of analysis guides for a more general neuroscience audience, reflecting our experience working with researchers at the NIH and the broader community. These guides walk users through the code, its deployment in typical scenarios, and the interpretation of results.</p><disp-quote content-type="editor-comment"><p>While the methodology is sound and the discussion of its benefits is good, the interpretation and discussion of the re-analyzed results are poor:</p><p>In section 2.3, the authors use FLMM to identify an instance of Simpson’s Paradox in the analysis of Jeong et al. (2022). While this phenomenon is evident in the original authors’ metrics (replotted in Figure 5A), FLMM provides a convenient method to identify these effects while illustrating the deficiencies of the original authors’ approach of concatenating a different number of sessions for each animal and ignoring potential within-session effects.</p></disp-quote><p>Our goal was to demonstrate that <italic>FLMM</italic> provides insight into why the opposing within- and between-session effects occur: the between-session and within-session changes appear to occur at different trial timepoints. Thus, while the AUC metrics applied in Jeong et al. (2022) are enough to show the <italic>presence of</italic> Simpson’s paradox, it is difficult to hypothesize <italic>why</italic> the opposing within-/between-session effects occur. An AUC analysis cannot determine at what trial timepoints (relative to licking) those opposing trends occur.</p><disp-quote content-type="editor-comment"><p>The discussion of this result is muddled. Having identified the paradox, there is some appropriate speculation as to what is causing these opposing effects, particularly the decrease in sessions. In the discussion and appendices, the authors identify (1) changes in satiation/habitation/motivation, (2) the predictability of the rewards (presumably by the click of a solenoid valve) and (3) photobleaching as potential explanations of the decrease within days. Having identified these effects, but without strong evidence to rule all three out, the discussion of whether RPE or ANCCR matches these results is probably moot. In particular, the hypotheses developed by Jeong et al., were for a random (unpredictable) rewards experiment, whereas the evidence points to the rewards being sometimes predictable. The learning of that predictability (e.g. over sessions) and variation in predictability (e.g. by attention level to sounds of each mouse) significantly complicate the analysis. The FLMM analysis reveals the complexity of analyzing what is apparently a straightforward task design.</p></disp-quote><p>While we are disappointed to hear the reviewer felt our initial interpretations and discussion were poor, the reviewer brings up an excellent point re: potential reward predictability that we had not considered. They have convinced us that acknowledging this alternative perspective will strengthen the paper, and we have added it into the Discussion. We agree that the ANCCR/RPE model predictions were made for unpredictable rewards and, as the reviewer rightly points out, there is evidence that the animals may sense the reward delivery. After discussing extensively with the authors of Jeong et al. (2022), it is clear that they went to enormous trouble to prevent the inadvertent generation of a CS+, and it is likely changes in pressure from the solenoid (rather than a sound) that may have served as a cue. Regardless of the learning theory one adopts (RPE, ANCCR or others), we agree that this potential learned predictability could, at least partially, account for the increase in signal magnitude across sessions. As this paper is focused on analysis methods, we feel that we can contribute most thoughtfully to the dopamine–learning theory conversation by presenting this explanation in detail, for consideration in future experiments. We have substantially edited this discussion and, as per the reviewer’s suggestion, have qualified our interpretations to reflect the uncertainty in explaining the observed trends.</p><disp-quote content-type="editor-comment"><p>If this paper is not trying to arbitrate between RPE and ANCCR, as stated in the text, the post hoc reasoning of the authors of Jeong et al 2022 provided in the discussion is not germane. Arbitrating between the models likely requires new experimental designs (removing the sound of the solenoid, satiety controls) or more complex models (e.g. with session effects, measures of predictability) that address the identified issues.</p></disp-quote><p>Thank you for this point. We agree with you that, given the scope of the paper, we should avoid any extensive comparison between the models. To address your comment, we have now removed portions of the Discussion that compared RPE and ANCCR. Overall, we agree with the reviewer, and think that future experiments will be needed for conclusively testing the accuracy of the models’ predictions for random (unpredicted) rewards. While we understand that our description of several conversations with the Jeong et al., 2022 authors could have gone deeper, we hope the reviewer can appreciate that inclusion of these conversations was done with the best of intentions. We wish to emphasize that we also consulted with several other researchers in the field when crafting our discussion. We do commend the authors of Jeong et al., 2022 for their willingness to discuss all these details. They could easily have avoided acknowledging any potential incompleteness of their theory by claiming that our results do not invalidate their predictions for a random reward, because the reward could potentially have been predicted (due to an inadvertent CS+ generated from the solenoid pressure). Instead, they emphasized that they thought their experiment did test a random reward, to the extent they could determine, and that our results suggest components of their theory that should be updated. We think that engagement with re-analyses of one’s data, even when findings are at odds with an initial theoretical framing, is a good demonstration of open science practice. For that reason as well, we feel providing readers with a perspective on the entire discussion will contribute to the scientific discourse in this area.</p><p>Finally, we would like to reiterate that this conversation is happening at least in part because of our method: by analyzing the signal at every trial timepoint, it provides a formal way to test for the presence of a neural signal indicative of reward delivery perception. Ultimately, this was what we set out to do: help researchers ask questions of their data that may have been harder to ask before. We believe that having a demonstration that we can indeed do this for a 'live' scientific issue is the most appropriate way of demonstrating the usefulness of the method.</p><disp-quote content-type="editor-comment"><p>Of the three potential causes of within-session decreases, the photobleaching arguments advanced in the discussion and expanded greatly in the appendices are not convincing. The data being modeled is a processed signal (∆<italic>F/F</italic>) with smoothing and baseline correction and this does not seem to have been considered in the argument. Furthermore, the photometry readout is also a convolution of the actual concentration changes over time, influenced by the on-off kinetics of the sensor, which makes the interpretation of timing effects of photobleaching less obvious than presented here and more complex than the dyes considered in the cited reference used as a foundation for this line of reasoning.</p></disp-quote><p>We appreciate the nuance of this point, and we have made considerable efforts in the Results and Discussion sections to caution that alternative hypotheses (e.g., photobleaching) cannot be definitively ruled out. In response to your criticism, we have consulted with more experts in the field regarding the potential for bleaching in this data, and it is not clear to us why photobleaching would be visible in one time-window of a trial, but not at another (less than a second away), despite high ∆<italic>F/F</italic> magnitudes in both time-windows. We do wish to point out that the Jeong et al. (2022) authors were also concerned about photobleaching as a possible explanation. At their request, we analyzed data from additional experiments, collected from the same animals. In most cases, we did not observe signal patterns that seemed to indicate photobleaching. Given the additional scrutiny, we do not think that photobleaching is more likely to invalidate results in this particular set of experiments than it would be in any other photometry experiment. While the role of photobleaching may be more complicated with this sensor than others in the references, that citation was included primarily as a way of acknowledging that it is possible that non-linearities in photobleaching could occur. Regardless, your point is well taken and we have qualified our description of these analyses to express that photobleaching cannot be ruled out.</p><disp-quote content-type="editor-comment"><p>Within this discussion of photobleaching, the characterization of the background reward experiments used in part to consider photobleaching (appendix 7.3.2) is incorrect. In this experiment (Jeong et al., 2022), background rewards were only delivered in the inter-trial-interval (i.e. not between the CS+ and predicted reward as stated in the text). Both in the authors’ description and in the data, there is a 6s before cue onset where rewards are not delivered and while not described in the text, the data suggests there is a period after a predicted reward when background rewards are not delivered. This complicates the comparison of this data to the random reward experiment.</p></disp-quote><p>Thank you for pointing this out! We removed the parenthetical on page 18 of the appendix that incorrectly stated that rewards can occur between the CS+ and the predicted reward.</p><disp-quote content-type="editor-comment"><p>The discussion of the lack of evidence for backpropagation, taken as evidence for ANCCR over RPE, is also weak.</p></disp-quote><p>Our point was initially included to acknowledge that, although our method yields results that conflict with the conclusions described by Jeong et al., 2022 on data from some experiments, on other experiments our method supports their results. Again, we believe that a critical part of re-analyzing shared datasets is acknowledging both areas where new analyses support the original results, as well as those where they conflict with them. We agree with the reviewer that qualifying our results so as not to emphasize support for/against RPE/ANCCR will strengthen our paper, and we have made those changes. We have qualified the conclusions of our analysis to emphasize they are a demonstration of how <italic>FLMM</italic> can be used to answer a certain style of question with hypothesis testing (how signal dynamics change across sessions), as opposed to providing evidence for/against the backpropagation hypothesis.</p><disp-quote content-type="editor-comment"><p>A more useful exercise than comparing FLMM to the methods and data of Jeong et al., 2022, would be to compare against the approach of Amo et al., 2022, which identifies backpropagation (data publicly available: DOI: 10.5061/dryad.hhmgqnkjw). The replication of a positive result would be more convincing of the sensitivity of the methodology than the replication of a negative result, which could be a result of many factors in the experimental design. Given that the Amo et al. analysis relies on identifying systematic changes in the timing of a signal over time, this would be particularly useful in understanding if the smoothing steps in FLMM obscure such changes.</p></disp-quote><p>Thank you for this suggestion. Your thoughtful review has convinced us that focusing on our statistical contribution will strengthen the paper, and we made changes to further emphasize that we are not seeking to adjudicate between RPE/ANCCR. Given the length of the manuscript as it stands, we could only include a subset of the analyses conducted on Jeong et al., 2022, and had to relegate the results from the Coddington et al., data to an appendix. Realistically, it would be hard for us to justify including analyses from a third dataset, only to have to relegate them to an appendix. We did include numerous examples in our manuscript where we already replicated positive results, in a way that we believe demonstrates the sensitivity of the methodology. We have also been working with many groups at NIH and elsewhere using our approach, in experiments targeting different scientific questions. In fact, one paper that extensively applies our method, and compares the results with those yielded by standard analysis of AUCs, is already published (Beas et al., 2024). Finally, in our analysis guide we describe additional analyses, not included in the manuscript, that replicate positive results. Hence there are numerous demonstrations of <italic>FLMM</italic>’s performance in less controversial settings. We take your point that our description of the data supporting one theory or the other should be qualified, and we have corrected that. Specifically for your suggestion of Amo et al. 2022, we have not had the opportunity to personally reanalyze their data, but we are already in contact with other groups who have conducted preliminary analyses of their data with <italic>FLMM</italic>. We are delighted to see this, in light of your comments and our decision to restrict the scope of our paper. We will help them and other groups working on this question to the extent we can.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the Authors:</bold></p><p><bold>Reviewer #2:</bold></p><p>First, I would like to commend the authors for the clarity of the paper, and for creating an open-source package that will help researchers more easily adopt this type of analysis.</p></disp-quote><p>Thank you for the positive feedback!</p><disp-quote content-type="editor-comment"><p>I would suggest the authors consider adding to the manuscript, either some evidence or some intuition on how feasible would be to use FLMM for very complex model specifications, in terms of computational cost and model convergence.</p></disp-quote><p>Thank you for this suggestion. As we described above in response to Reviewer #2’s Public Reviews, we have added in a demonstration of the scalability of the method. Since our initial manuscript submission, we have further increased the package’s speed (e.g., through further parallelization). We are releasing the updated version of our package on CRAN.</p><disp-quote content-type="editor-comment"><p>From my understanding, this package might potentially be useful not just for photometry data but also for two-photon recordings for example. If so, I would also suggest the authors add to the discussion this potential use.</p></disp-quote><p>This is a great point. Our updated manuscript Discussion includes the following:</p><p>“The <italic>FLMM</italic> framework may also be applicable to techniques like electrophysiology and calcium imaging. For example, our package can fit functional generalized LMMs with a count distribution (e.g., Poisson). Additionally, our method can be extended to model time-varying covariates. This would enable one to estimate how the level of association between signals, simultaneously recorded from different brain regions, fluctuates across trial time-points. This would also enable modeling of trials that differ in length due to, for example, variable behavioral response times (e.g., latency-topress).”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>The authors should define ’function’ in context, as well as provide greater detail of the alternate tests that FLMM is compared to in Figure 7.</p></disp-quote><p>We include a description of the alternate tests in Appendix Section 5.2. We have updated the Methods Section (Section 4) to introduce the reader to how ‘functions’ are conceptualized and modeled in the functional data analysis literature. Specifically, we added the following text:</p><p><italic>“FLMM</italic> models each trial’s signal as a function that varies smoothly across trial time-points (i.e., along the 'functional domain'). It is thus a type of non-linear modeling technique over the functional domain, since we do not assume a linear model (straight line). <italic>FLMM</italic> and other functional data analysis methods model data as functions, when there is a natural ordering (e.g., time-series data are ordered by time, imaging data are ordered by x-y coordinates), and are assumed to vary smoothly along the functional domain (e.g., one assumes values of a photometry signal at close time-points in a trial have similar values). Functional data analysis approaches exploit this smoothness and natural ordering to capture more information during estimation and inference.”</p><p>Given the novelty of estimating joint CIs, the authors should be clearer about how this should be reported and how this differs from pointwise CIs (and how this has been done in the past).</p><p>We appreciate your pointing this out, as the distinction is nuanced. Our manuscript includes a description of how <italic>joint</italic> CIs enable one to interpret effects as statistically significant for time-intervals as opposed to individual timepoints. Unlike <italic>joint</italic> CIs, assessing significance with pointwise CIs suffers from multiple-comparisons problems. As a result of your suggestion, we have included a short discussion of this to our analysis guide (Part 1), entitled 'Pointwise or Joint 95% Confidence Intervals.' The Methods section of our manuscript also includes the following:</p><p>“The construction of <italic>joint</italic> CIs in the context of functional data analysis is an important research question; see Cui et al. (2021) and references therein. Each <italic>point</italic> at which the <italic>pointwise</italic> 95% CI does not contain 0 indicates that the coefficient is <italic>statistically significantly</italic> different from 0 at that point. Compared with <italic>pointwise</italic> CIs, <italic>joint</italic> CIs takes into account the autocorrelation of signal values across trial time-points (the functional domain). Therefore, instead of interpreting results at a specific timepoint, <italic>joint</italic> CIs enable <italic>joint</italic> interpretations at multiple locations along the functional domain. This aligns with interpreting covariate effects on the photometry signals across time-intervals (e.g., a cue period) as opposed to at a single trial time-point. Previous methodological work has provided functional mixed model implementations for either <italic>joint</italic> 95% CIs for simple random-effects models (Cui et al., 2021), or <italic>pointwise</italic> 95% CIs for nested models (Scheipl et al., 2016), but to our knowledge, do not provide explicit formulas or software for computing <italic>joint</italic> 95% CIs in the presence of general random-effects specifications.”</p><disp-quote content-type="editor-comment"><p>The authors identify that many photometry studies are complex nested longitudinal designs, using the cohort of 8 animals used in five task designs of Jeong et al. 2022 as an example. The authors miss the opportunity to illustrate how FLMM might be useful in identifying the effects of subject characteristics (e.g. sex, CS+ cue identity).</p></disp-quote><p>This is a fantastic point and we have added the following into the Discussion:</p><p>“...[S]ignal heterogeneity due to subject characteristics (e.g., sex, CS+ cue identity) could be incorporated into a model through inclusion of animal-specific random effects.”</p><disp-quote content-type="editor-comment"><p>In discussing the delay-length change experiment, it would be more accurate to say that proposed versions of RPE and ANCCR do not predict the specific change.</p></disp-quote><p>Good point. We have made this change.</p><disp-quote content-type="editor-comment"><p>Minor corrections:</p><p>Panels are mislabeled in Figure 5.</p></disp-quote><p>Thank you. We have corrected this.</p><disp-quote content-type="editor-comment"><p>The Crowder (2009) reference is incorrect, being a review of the book with the book presumably being the correct citation.</p></disp-quote><p>Good catch, thank you! Corrected.</p><disp-quote content-type="editor-comment"><p>In Section 5 (first appendix), the authors could include the alternate spelling ’fibre photometry’ to capture any citations that use British English spelling.</p></disp-quote><p>This is a great suggestion, but we did not have time to recreate these figures before re-submission.</p><disp-quote content-type="editor-comment"><p>Section 7.4 is almost all quotation, though unevenly using the block quotation formatting. It is unclear why such a large quotation is included.</p></disp-quote><p>Thank you for pointing this out. We have removed this Appendix section (formerly Section 7.4) as the relevant text was already included in the Methods section.</p><p>References</p><p>Sofia Beas, Isbah Khan, Claire Gao, Gabriel Loewinger, Emma Macdonald, Alison Bashford, Shakira Rodriguez-Gonzalez, Francisco Pereira, and Mario A Penzo. Dissociable encoding of motivated behavior by parallel thalamo-striatal projections. <italic>Current Biology</italic>, 34(7):1549–1560, 2024.</p><p>Erjia Cui, Andrew Leroux, Ekaterina Smirnova, and Ciprian Crainiceanu. Fast univariate inference for longitudinal functional models. <italic>Journal of Computational and Graphical Statistics</italic>, 31:1–27, 07 2021. doi: 10.1080/10618600.2021.1950006.</p><p>Huijeong Jeong, Annie Taylor, Joseph R Floeder, Martin Lohmann, Stefan Mihalas, Brenda Wu, Mingkang Zhou, Dennis A Burke, and Vijay Mohan K Namboodiri. Mesolimbic dopamine release conveys causal associations. <italic>Science</italic>, 378(6626):eabq6740, 2022. doi: 10.1126/science.abq6740. URL https://www. science.org/doi/abs/10.1126/science.abq6740.</p><p>Rachel S Lee, Marcelo G Mattar, Nathan F Parker, Ilana B Witten, and Nathaniel D Daw. Reward prediction error does not explain movement selectivity in dms-projecting dopamine neurons. <italic>eLife</italic>, 8:e42992, apr 2019. ISSN 2050-084X. doi: 10.7554/eLife.42992. URL https://doi.org/10.7554/eLife.42992.</p><p>Fabian Scheipl, Jan Gertheiss, and Sonja Greven. Generalized functional additive mixed models. <italic>Electronic Journal of Statistics</italic>, 10(1):1455 – 1492, 2016. doi: 10.1214/16-EJS1145. URL https://doi.org/10.1214/16-EJS1145.</p></body></sub-article></article>