<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">69884</article-id><article-id pub-id-type="doi">10.7554/eLife.69884</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Presynaptic stochasticity improves energy efficiency and helps alleviate the stability-plasticity dilemma</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-236346"><name><surname>Schug</surname><given-names>Simon</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5305-2547</contrib-id><email>sschug@ethz.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-237321"><name><surname>Benzing</surname><given-names>Frederik</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6580-8690</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-237322"><name><surname>Steger</surname><given-names>Angelika</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Institute of Neuroinformatics, University of Zurich &amp; ETH Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution>Department of Computer Science, ETH Zurich</institution><addr-line><named-content content-type="city">Zurich</named-content></addr-line><country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="editor"><name><surname>O'Leary</surname><given-names>Timothy</given-names></name><role>Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>18</day><month>10</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e69884</elocation-id><history><date date-type="received" iso-8601-date="2021-04-28"><day>28</day><month>04</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-10-18"><day>18</day><month>10</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-05-05"><day>05</day><month>05</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.05.05.442708"/></event></pub-history><permissions><copyright-statement>© 2021, Schug et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Schug et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-69884-v2.pdf"/><abstract><p>When an action potential arrives at a synapse there is a large probability that no neurotransmitter is released. Surprisingly, simple computational models suggest that these synaptic failures enable information processing at lower metabolic costs. However, these models only consider information transmission at single synapses ignoring the remainder of the neural network as well as its overall computational goal. Here, we investigate how synaptic failures affect the energy efficiency of models of entire neural networks that solve a goal-driven task. We find that presynaptic stochasticity and plasticity improve energy efficiency and show that the network allocates most energy to a sparse subset of important synapses. We demonstrate that stabilising these synapses helps to alleviate the stability-plasticity dilemma, thus connecting a presynaptic notion of importance to a computational role in lifelong learning. Overall, our findings present a set of hypotheses for how presynaptic plasticity and stochasticity contribute to sparsity, energy efficiency and improved trade-offs in the stability-plasticity dilemma.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>presynaptic stochasticity</kwd><kwd>presynaptic plasticity</kwd><kwd>energy efficiency</kwd><kwd>lifelong learning</kwd><kwd>sparsity</kwd><kwd>stability-plasticity dilemma</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>PZ00P318602</award-id><principal-award-recipient><name><surname>Schug</surname><given-names>Simon</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>CRSII5_173721</award-id><principal-award-recipient><name><surname>Benzing</surname><given-names>Frederik</given-names></name><name><surname>Steger</surname><given-names>Angelika</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Presynaptic plasticity in simulated neural networks with a simple biologically plausible learning rule improves metabolic efficiency and lifelong learning capabilities.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>It has long been known that synaptic signal transmission is stochastic (<xref ref-type="bibr" rid="bib25">del Castillo and Katz, 1954</xref>). When an action potential arrives at the presynapse, there is a high probability that no neurotransmitter is released – a phenomenon observed across species and brain regions (<xref ref-type="bibr" rid="bib13">Branco and Staras, 2009</xref>). From a computational perspective, synaptic stochasticity seems to place unnecessary burdens on information processing. Large amounts of noise hinder reliable and efficient computation (<xref ref-type="bibr" rid="bib84">Shannon, 1948</xref>; <xref ref-type="bibr" rid="bib26">Faisal et al., 2005</xref>) and synaptic failures appear to contradict the fundamental evolutionary principle of energy-efficient processing (<xref ref-type="bibr" rid="bib68">Niven and Laughlin, 2008</xref>). The brain, and specifically action potential propagation consume a disproportionately large fraction of energy (<xref ref-type="bibr" rid="bib6">Attwell and Laughlin, 2001</xref>; <xref ref-type="bibr" rid="bib35">Harris et al., 2012</xref>) – so why propagate action potentials all the way to the synapse only to ignore the incoming signal there?</p><p>To answer this neurocomputational enigma various theories have been put forward, see <xref ref-type="bibr" rid="bib59">Llera-Montero et al., 2019</xref> for a review. One important line of work proposes that individual synapses do not merely maximise information transmission, but rather take into account metabolic costs, maximising the information transmitted <italic>per unit of energy</italic> (<xref ref-type="bibr" rid="bib55">Levy and Baxter, 1996</xref>). This approach has proven fruitful to explain synaptic failures (<xref ref-type="bibr" rid="bib56">Levy and Baxter, 2002</xref>; <xref ref-type="bibr" rid="bib35">Harris et al., 2012</xref>), low average firing rates (<xref ref-type="bibr" rid="bib55">Levy and Baxter, 1996</xref>) as well as excitation-inhibition balance (<xref ref-type="bibr" rid="bib83">Sengupta et al., 2013</xref>) and is supported by fascinating experimental evidence suggesting that both presynaptic glutamate release (<xref ref-type="bibr" rid="bib80">Savtchenko et al., 2013</xref>) and postsynaptic channel properties (<xref ref-type="bibr" rid="bib36">Harris et al., 2015</xref>; <xref ref-type="bibr" rid="bib37">Harris et al., 2019</xref>) are tuned to maximise information transmission per energy.</p><p>However, so far information-theoretic approaches have been limited to signal transmission at single synapses, ignoring the context and goals in which the larger network operates. As soon as context and goals guide network computation certain pieces of information become more relevant than others. For instance, when reading a news article the textual information is more important than the colourful ad blinking next to it – even when the latter contains more information in a purely information-theoretic sense.</p><p>Here, we study presynaptic stochasticity on the network level rather than on the level of single synapses. We investigate its effect on (1) energy efficiency and (2) the stability-plasticity dilemma in model neural networks that learn to selectively extract information from complex inputs.</p><p>We find that presynaptic stochasticity in combination with presynaptic plasticity allows networks to extract information at lower metabolic cost by sparsely allocating energy to synapses that are important for processing the given stimulus. As a result, presynaptic release probabilities encode synaptic importance. We show that this notion of importance is related to the Fisher information, a theoretical measure for the network’s sensitivity to synaptic changes.</p><p>Building on this finding and previous work (<xref ref-type="bibr" rid="bib48">Kirkpatrick et al., 2017</xref>), we explore a potential role of presynaptic stochasticity in the stability-plasticity dilemma. In line with experimental evidence (<xref ref-type="bibr" rid="bib92">Yang et al., 2009</xref>; <xref ref-type="bibr" rid="bib38">Hayashi-Takagi et al., 2015</xref>), we demonstrate that selectively stabilising important synapses improves lifelong learning. Furthermore, these experiments link presynaptically induced sparsity to improved memory.</p><sec id="s1-1"><title>Model</title><p>Our goal is to understand how information processing and energy consumption are affected by stochasticity in synaptic signal transmission. While there are various sources of stochasticity in synapses, here, we focus on modelling <italic>synaptic failures</italic> where action potentials at the presynapse fail to trigger any postsynaptic depolarisation. The probability of such failures is substantial (<xref ref-type="bibr" rid="bib13">Branco and Staras, 2009</xref>; <xref ref-type="bibr" rid="bib34">Hardingham et al., 2010</xref>; <xref ref-type="bibr" rid="bib79">Sakamoto et al., 2018</xref>) and, arguably, due to its all-or-nothing-characteristic has the largest effect on both energy consumption and information transmission.</p><p>As a growing body of literature suggests, artificial neural networks (ANNs) match several aspects of biological neuronal networks in various goal-driven situations (<xref ref-type="bibr" rid="bib49">Kriegeskorte, 2015</xref>; <xref ref-type="bibr" rid="bib91">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="bib46">Kell et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Banino et al., 2018</xref>; <xref ref-type="bibr" rid="bib22">Cueva and Wei, 2018</xref>; <xref ref-type="bibr" rid="bib63">Mattar and Daw, 2018</xref>). Crucially, they are the only known model to solve complex vision and reinforcement learning tasks comparably well as humans. We therefore choose to extend this class of models by explicitly incorporating synaptic failures and study their properties in a number of complex visual tasks.</p></sec><sec id="s1-2"><title>Model details</title><p>The basic building blocks of ANNs are neurons that combine their inputs <inline-formula><mml:math id="inf1"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> through a weighted sum <inline-formula><mml:math id="inf2"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and apply a nonlinear activation function <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The weights <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> naturally correspond to <italic>synaptic strengths</italic> between presynaptic neuron <inline-formula><mml:math id="inf5"><mml:mi>i</mml:mi></mml:math></inline-formula> and the postsynaptic neuron. Although synaptic transmission is classically described as a binomial process (<xref ref-type="bibr" rid="bib25">del Castillo and Katz, 1954</xref>) most previous modelling studies assume the synaptic strengths to be deterministic. This neglects a key characteristic of synaptic transmission: the possibility of synaptic failures where no communication between pre- and postsynapse occurs at all.</p><p>In the present study, we explicitly model presynaptic stochasticity by introducing a random variable <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, whose outcome corresponds to whether or not neurotransmitter is released. Formally, each synapse <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is activated stochastically according to<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac linethickness="0"><mml:mrow><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">o</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi><mml:mi mathvariant="sans-serif">h</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">l</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:munder><mml:mo>⋅</mml:mo><mml:munder><mml:mrow><mml:munder><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac linethickness="0"><mml:mrow><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">y</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi><mml:mi mathvariant="sans-serif">g</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">h</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:munder><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="sans-serif">w</mml:mi><mml:mi mathvariant="sans-serif">h</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac linethickness="0"><mml:mrow><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">l</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">o</mml:mi><mml:mi mathvariant="sans-serif">b</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">b</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">l</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">y</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:munder></mml:mrow></mml:mstyle></mml:math></disp-formula>so that it has expected synaptic strength <inline-formula><mml:math id="inf8"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The postsynaptic neuron calculates a stochastic weighted sum of its inputs with a nonlinear activation<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mrow><mml:munder><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mfrac linethickness="0"><mml:mrow><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">o</mml:mi><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">y</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">v</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">o</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mrow><mml:munder><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mfrac linethickness="0"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">i</mml:mi></mml:mrow><mml:mtext mathvariant="sans-serif">-</mml:mtext><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">y</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">u</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>During learning, synapses are updated and both synaptic strength and release probability are changed. We resort to standard learning rules to change the expected synaptic strength. For the multilayer perceptron, this update is based on stochastic gradient descent with respect to a loss function <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which in our case is the standard cross-entropy loss. Concretely, we have<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mrow><mml:mi mathvariant="sans-serif">w</mml:mi><mml:mi mathvariant="sans-serif">h</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where the superscript corresponds to time steps. Note that this update is applied to the expected synaptic strength <inline-formula><mml:math id="inf10"><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula>, requiring communication between pre- and postsynape, see also Discussion. For the explicit update rule of the synaptic strength <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> see Materials and methods, <xref ref-type="disp-formula" rid="equ9">Equation (8)</xref>. For the standard perceptron model, <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by its standard learning rule (<xref ref-type="bibr" rid="bib76">Rosenblatt, 1958</xref>). Based on the intuition that synapses which receive larger updates are more important for solving a given task, we update <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> using the update direction <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> according to the following simple scheme<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>up</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">f</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:msub><mml:mi/><mml:mtext>lim</mml:mtext></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>down</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">f</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:msub><mml:mi/><mml:mtext>lim</mml:mtext></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf15"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>up</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mtext>down</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>lim</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> are three metaplasticity parameters shared between all synapses. (We point out that in a noisy learning setting the gradient <inline-formula><mml:math id="inf16"><mml:mi>g</mml:mi></mml:math></inline-formula> does not decay to, so that the learning rule in (4) will maintain network function by keeping certain release probabilities high. See also Materials and methods for a theoretical analysis.) To prevent overfitting and to test robustness, we tune them using one learning scenario and keep them fixed for all other scenarios, see Materials and methods. To avoid inactivated synapses with release probability <inline-formula><mml:math id="inf17"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, we clamp <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to stay above 0.25, which we also use as the initial value of <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> before learning.</p><p>On top of the above intuitive motivation, we give a theoretical justification for this learning rule in Materials and methods, showing that synapses with larger Fisher information obtain high release probabilities, also see Figure 2d.</p><boxed-text id="box1"><label>Box 1.</label><caption><p>Mutual Information.</p><p>The Mutual Information <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>;</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of two jointly distributed random variables <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:math></inline-formula> is a common measure of their dependence (<xref ref-type="bibr" rid="bib84">Shannon, 1948</xref>). Intuitively, mutual information captures how much information about <inline-formula><mml:math id="inf22"><mml:mi>Y</mml:mi></mml:math></inline-formula> can be obtained from <inline-formula><mml:math id="inf23"><mml:mi>Z</mml:mi></mml:math></inline-formula>, or vice versa. Formally, it is defined as<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>;</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≡</mml:mo><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Where <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the entropy of <inline-formula><mml:math id="inf25"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the conditional entropy of <inline-formula><mml:math id="inf27"><mml:mi>Y</mml:mi></mml:math></inline-formula> given <inline-formula><mml:math id="inf28"><mml:mi>Z</mml:mi></mml:math></inline-formula>. In our case, we want to measure how much task-relevant information <inline-formula><mml:math id="inf29"><mml:mi>Y</mml:mi></mml:math></inline-formula> is contained in the neural network output <inline-formula><mml:math id="inf30"><mml:mi>Z</mml:mi></mml:math></inline-formula>. For example, the neural network might receive as input a picture of a digit with the goal of predicting the identity of the digit. Both the ground-truth digit identity <inline-formula><mml:math id="inf31"><mml:mi>Y</mml:mi></mml:math></inline-formula> and the network’s prediction <inline-formula><mml:math id="inf32"><mml:mi>Z</mml:mi></mml:math></inline-formula> are random variables depending on the random image <inline-formula><mml:math id="inf33"><mml:mi>X</mml:mi></mml:math></inline-formula>. The measure <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>;</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> quantifies how much of the behaviourally relevant information <inline-formula><mml:math id="inf35"><mml:mi>Y</mml:mi></mml:math></inline-formula> is contained in the network’s prediction <inline-formula><mml:math id="inf36"><mml:mi>Z</mml:mi></mml:math></inline-formula> ignoring irrelevant information also present in the complex, high-entropy image <inline-formula><mml:math id="inf37"><mml:mi>X</mml:mi></mml:math></inline-formula>.</p></caption></boxed-text></sec><sec id="s1-3"><title>Measuring energy consumption</title><p>For our experiments, we would like to quantify the energy consumption of the neural network. <xref ref-type="bibr" rid="bib35">Harris et al., 2012</xref> find that the main constituent of neural energy demand is synaptic signal transmission and that the cost of synaptic signal transmission is dominated by the energy needed to reverse postsynaptic ion fluxes. In our model, the component most closely matching the size of the postsynaptic current is the expected synaptic strength, which we therefore take as measure for the model’s energy consumption. In the Appendix, we also measure the metabolic cost incurred by the activity of neurons by calculating their average rate of activity.</p></sec><sec id="s1-4"><title>Measuring information transmission</title><p>We would like to measure how well the neural network transmits information relevant to its behavioural goal. In particular, we are interested in the setting where the complexity of the stimulus is high relative to the amount of information that is relevant for the behavioural goal. To this end, we present complex visual inputs with high information content to the network and teach it to recognise the object present in the image. We then measure the mutual information between network output and object identity, see <xref ref-type="box" rid="box1">Box 1</xref>.</p></sec></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Presynaptic stochasticity enables energy-efficient information processing</title><p>We now investigate the energy efficiency of a network that learns to classify digits from the MNIST handwritten digit dataset (<xref ref-type="bibr" rid="bib52">LeCun, 1998</xref>). The inputs are high-dimensional with high entropy, but the relevant information is simply the identity of the digit. We compare the model with plastic, stochastic release to two controls. A standard ANN with deterministic synapses is included to investigate the combined effect of presynaptic stochasticity and plasticity. In addition, to isolate the effect of presynaptic plasticity, we introduce a control which has stochastic release, but with a fixed probability. In this control, the release probability is identical across synapses and chosen to match the average release probability of the model with plastic release after it has learned the task.</p><p>All models are encouraged to find low-energy solutions by penalising large synaptic weights through standard <inline-formula><mml:math id="inf38"><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>-regularisation. <xref ref-type="fig" rid="fig1">Figure 1a</xref> shows that different magnitudes of <inline-formula><mml:math id="inf39"><mml:msub><mml:mi mathvariant="normal">ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>-regularisation induce different information-energy trade-offs for all models, and that the model with plastic, stochastic release finds considerably more energy-efficient solutions than both controls, while the model with non-plastic release requires more energy then the deterministic model. Together, this supports the view that a combination of presynaptic stochasticity and plasticity promotes energy-efficient information extraction.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Energy efficiency of model with stochastic and plastic release.</title><p>(<bold>a</bold>) Different trade-offs between mutual information and energy are achievable in all network models. Generally, stochastic synapses with learned release probabilities are more energy-efficient than deterministic synapses or stochastic synapses with fixed release probability. The fixed release probabilities model was chosen to have the same average release probability as the model with learned probabilities. (<bold>b</bold>) Best achievable ratio of information per energy for the three models from (<bold>a</bold>). Error bars in (<bold>a</bold>) and (<bold>b</bold>) denote the standard error for three repetitions of the experiment.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69884-fig1-v2.tif"/></fig><p>In addition, we investigate how stochastic release helps the network to lower metabolic costs. Intuitively, a natural way to save energy is to assign high release probabilities to synapses that are important to extract relevant information and to keep remaining synapses at a low release probability. <xref ref-type="fig" rid="fig2">Figure 2a</xref> shows that after learning, there are indeed few synapses with high release probabilities, while most release probabilities are kept low. We confirm that this sparsity develops independently of the initial value of release probabilities before learning, see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1d</xref>. To test whether the synapses with high release probabilities are most relevant for solving the task we perform a lesion experiment. We successively remove synapses with low release probability and measure how well the lesioned network still solves the given task, see <xref ref-type="fig" rid="fig2">Figure 2b</xref>. As a control, we remove synapses in a random order independent of their release probability. We find that maintaining synapses with high release probabilities is significantly more important to network function than maintaining random ones. Moreover, we find, as expected, that synapses with high release probabilities consume considerably more energy than synapses with low release probability, see <xref ref-type="fig" rid="fig2">Figure 2c</xref>. This supports the hypothesis that the model identifies important synapses for the task at hand and spends more energy on these synapses while saving energy on irrelevant ones.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Importance of synapses with high release probability for network function.</title><p>(<bold>a</bold>) Histogram of release probabilities before and after learning, showing that the network relies on a sparse subset of synapses to find an energy-efficient solution. Dashed line at <inline-formula><mml:math id="inf40"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> indicates our boundary for defining a release probability as ‘low’ or ‘high’. We confirmed that results are independent of initial value of release probabilities before learning (see <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2d</xref>). (<bold>b</bold>) Accuracy after performing the lesion experiment either removing synapses with low release probabilities first or removing weights randomly, suggesting that synapses with high release probability are most important for solving the task. (<bold>c</bold>) Distribution of synaptic energy demand for high and low release probability synapses. (<bold>d</bold>) Distribution of the Fisher information for high and low release probability synapses. It confirms the theoretical prediction that high release probability corresponds to high Fisher information. All panels show accumulated data for three repetitions of the experiment. Shaded regions in (<bold>b</bold>) show standard error.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69884-fig2-v2.tif"/></fig><p>We have seen that the network relies on a sparse subset of synapses to solve the task efficiently. However, sparsity is usually thought of on a neuronal level, with few neurons rather than few synapses encoding a given stimulus. Therefore, we quantify sparsity of our model on a neuronal level. For each neuron, we count the number of ‘important’ input- and output synapses, where we define ‘important’ to correspond to a release probability of at least <inline-formula><mml:math id="inf41"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>. Note that the findings are robust with respect to the precise value of <inline-formula><mml:math id="inf42"><mml:mi>p</mml:mi></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig2">Figure 2a</xref>. We find that the distribution of important synapses per neuron is inhomogeneous and significantly different from a randomly shuffled baseline with a uniform distribution of active synapses (Kolmogorov-Smirnoff test, <inline-formula><mml:math id="inf43"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0.505</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), see <xref ref-type="fig" rid="fig3">Figure 3a</xref>. Thus, some neurons have disproportionately many important inputs, while others have very few, suggesting sparsity on a neuronal level. As additional quantification of this effect, we count the number of highly important neurons, where we define a neuron to be highly important if its number of active inputs is two standard deviations below or above the mean (mean and standard deviation from shuffled baseline). We find that our model network with presynaptic stochasticity has disproportionate numbers of highly important and unimportant neurons, see <xref ref-type="fig" rid="fig3">Figure 3b</xref>. Moreover, we check whether neurons with many important inputs tend to have many important outputs, indeed finding a correlation of <inline-formula><mml:math id="inf44"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.93</mml:mn></mml:mrow></mml:math></inline-formula>, see <xref ref-type="fig" rid="fig3">Figure 3c</xref>. These analyses all support the claim that the network is sparse not only on a synaptic but also on a neuronal level.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Neuron-level sparsity of network after learning.</title><p>(<bold>a</bold>) Histogram of the fraction of important input synapses per neuron for second layer neurons after learning for true and randomly shuffled connectivity (see <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2a</xref> for other layers). (<bold>b</bold>) Same data as (<bold>a</bold>), showing number of low/medium/high importance neurons, where high/low importance neurons have at least two standard deviations more/less important inputs than the mean of random connectivity. (<bold>c</bold>) Scatter plot of first layer neurons showing the number of important input and output synapses after learning on MNIST, Pearson correlation is <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9390</mml:mn></mml:mrow></mml:math></inline-formula> (see <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2b</xref> for other layers). Data in (<bold>a</bold>) and (<bold>c</bold>) are from one representative run, error bars in (<bold>b</bold>) show standard error over three repetitions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69884-fig3-v2.tif"/></fig><p>Finally, we investigate how release probabilities evolve from a theoretical viewpoint under the proposed learning rule. Note that the evolution of release probabilities is a random process, since it depends on the random input to the network. Under mild assumptions, we show (Materials and methods) that release probabilities are more likely to increase for synapses with large Fisher information (In this context, the Fisher information is a measure of sensitivity of the network to changes in synapses, measuring how important preserving a given synapse is for network function.). Thus, synapses with large release probabilities will tend to have high Fisher information. We validate this theoretical prediction empirically, see <xref ref-type="fig" rid="fig2">Figure 2d</xref>.</p></sec><sec id="s2-2"><title>Presynaptically driven consolidation helps alleviate the stability-plasticity dilemma</title><p>While the biological mechanisms addressing the stability-plasticity dilemma are diverse and not fully understood, it has been demonstrated experimentally that preserving memories requires maintaining the synapses which encode these memories (<xref ref-type="bibr" rid="bib92">Yang et al., 2009</xref>; <xref ref-type="bibr" rid="bib38">Hayashi-Takagi et al., 2015</xref>; <xref ref-type="bibr" rid="bib19">Cichon and Gan, 2015</xref>). In this context, theoretical work suggests that the Fisher information is a useful way to quantify which synapses should be maintained (<xref ref-type="bibr" rid="bib48">Kirkpatrick et al., 2017</xref>). Inspired by these insights, we hypothesise that the synaptic importance encoded in release probabilities can be used to improve the network’s memory retention by selectively stabilising important synapses.</p><p>We formalise this hypothesis in our model by lowering the learning rate (plasticity) of synapses according to their importance (release probability). Concretely, the learning rate <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> used in (3) is scaled as follows<disp-formula id="equ6"><label>(5)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>such that the learning rate is smallest for important synapses with high release probability. <inline-formula><mml:math id="inf47"><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> denotes a base learning rate that is shared by all synapses. We complement this consolidation mechanism by freezing the presynaptic release probabilities <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> once they have surpassed a predefined threshold <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>p</mml:mi><mml:mtext>freeze</mml:mtext></mml:msub></mml:math></inline-formula>. This ensures that a synapse whose presynaptic release probability was high for a previous task retains its release probability even when unused during consecutive tasks. In other words, the effects of presynaptic long-term depression (LTD) are assumed to act on a slower timescale than learning single tasks. Note that the freezing mechanism ensures that all synaptic strengths <inline-formula><mml:math id="inf50"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> retain a small degree of plasticity, since the learning rate modulation factor <inline-formula><mml:math id="inf51"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> remains greater than 0.</p><p>To test our hypothesis that presynaptically driven consolidation allows the network to make improved stability-plasticity trade-offs, we sequentially present a number of tasks and investigate the networks behaviour. We mainly focus our analysis on a variation of the MNIST handwritten digit dataset, in which the network has to successively learn the parity of pairs of digits, see <xref ref-type="fig" rid="fig4">Figure 4a</xref>. Additional experiments are reported in the Appendix, see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Lifelong learning in a model with presynaptically driven consolidation.</title><p>(<bold>a</bold>) Schematic of the lifelong learning task Split MNIST. In the first task the model network is presented 0 s and 1 s, in the second task it is presented 2 s and 3 s, etc. For each task the model has to classify the inputs as even or odd. At the end of learning, it should be able to correctly classify the parity of all digits, even if a digit has been learned in an early task. (<bold>b</bold>) Accuracy of the first task when learning new tasks. Consolidation leads to improved memory preservation. (<bold>c</bold>) Average accuracies of all learned tasks. The presynaptic consolidation model is compared to a model without consolidation and two state-of-the-art machine learning algorithms. Differences to these models are significant in independent t-tests with either <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> (marked with *) or with <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> (marked with **). Dashed line indicates an upper bound for the network’s performance, obtained by training on all tasks simultaneously. Panels (<bold>b</bold>) and (<bold>c</bold>) show accumulated data for three repetitions of the experiment. Shaded regions in (<bold>b</bold>) and error bars in (<bold>c</bold>) show standard error.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69884-fig4-v2.tif"/></fig><p>First, we investigate whether presynaptic consolidation improves the model’s ability to remember old tasks. To this end, we track the accuracy on the first task over the course of learning, see <xref ref-type="fig" rid="fig4">Figure 4b</xref>. As a control, we include a model without consolidation and with deterministic synapses. While both models learn the first task, the model without consolidation forgets more quickly, suggesting that the presynaptic consolidation mechanism does indeed improve memory.</p><p>Next, we ask how increased stability interacts with the network’s ability to remain plastic and learn new tasks. To assess the overall trade-off between stability and plasticity, we report the average accuracy over all five tasks, see <xref ref-type="fig" rid="fig4">Figure 4c</xref>.</p><p>We find that the presynaptic consolidation model performs better than a standard model with deterministic synapses and without consolidation. In addition, we compare performance to two state-of-the art machine learning algorithms: The well-known algorithm Elastic Weight Consolidation (EWC) (<xref ref-type="bibr" rid="bib48">Kirkpatrick et al., 2017</xref>) explicitly relies on the Fisher information and performs a separate consolidation phase after each task. Bayesian Gradient Descent (BGD) (<xref ref-type="bibr" rid="bib94">Zeno et al., 2018</xref>) is a Bayesian approach that models synapses as distributions, but does not capture the discrete nature of synaptic transmission. The presynaptic consolidation mechanism performs better than both these state-of-the-art machine learning algorithms, see <xref ref-type="fig" rid="fig4">Figure 4c</xref>. Additional experiments in the Appendix suggest overall similar performance of Presynaptic Consolidation to BGD and similar or better performance than EWC.</p><p>To determine which components of our model contribute to its lifelong learning capabilities, we perform an ablation study, see <xref ref-type="fig" rid="fig5">Figure 5a</xref>. We aim to separate the effect of (1) consolidation mechanisms and (2) presynaptic plasticity.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Model ablation and lifelong learning in a standard perceptron.</title><p>(<bold>a</bold>) Ablation of the Presynaptic Consolidation model on two different lifelong learning tasks, see full text for detailed description. Both presynaptic plasticity and synaptic stabilisation significantly improve memory. (<bold>b</bold>+<bold>c</bold>) Lifelong Learning in a Standard Perceptron akin to <xref ref-type="fig" rid="fig4">Figure 4b,c</xref>, showing the accuracy of the first task when learning consecutive tasks in (<bold>b</bold>) as well as the average over all five tasks after learning all tasks in (<bold>c</bold>). Error bars and shaded regions show standard error of three respectively ten repetitions, in (<bold>a</bold>), respectively (<bold>b</bold>+<bold>c</bold>). All pair-wise comparisons are significant, independent t-tests with <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> (denoted by **) or with <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> (denoted by *).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69884-fig5-v2.tif"/></fig><p>First, we remove the two consolidation mechanisms, learning rate modulation and freezing release probabilities, from the model with stochastic synapses. This yields a noticeable decrease in performance during lifelong learning, thus supporting the view that stabilising important synapses contributes to addressing the stability-plasticity dilemma.</p><p>Second, we aim to disentangle the effect of presynaptic plasticity from the consolidation mechanisms. We therefore introduce a control in which presynaptic plasticity but not consolidation is blocked. Concretely, the control has ‘ghost release probabilities’ <inline-formula><mml:math id="inf56"><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> evolving according to <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref> and modulating plasticity according to <xref ref-type="disp-formula" rid="equ6">Equation (5)</xref>; but the synaptic release probability is fixed at 0.5. We see that this control performs worse than the original model with a drop in accuracy of 1.4 on Split MNIST (<inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>3.44</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>) and a drop of accuracy of 5.6 on Permuted MNIST (<inline-formula><mml:math id="inf59"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>6.72</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). This suggests that presynaptic plasticity, on top of consolidation, helps to stabilise the network. We believe that this can be attributed to the sparsity induced by the presynaptic plasticity which decreases overlap between different tasks.</p><p>The above experiments rely on a gradient-based learning rule for multilayer perceptrons. To test whether presynaptic consolidation can also alleviate stability-plasticity trade-offs in other settings, we study its effects on learning in a standard perceptron (<xref ref-type="bibr" rid="bib76">Rosenblatt, 1958</xref>). We train the perceptron sequentially on five pattern memorisation tasks, see Materials and methods for full details. We find that the presynaptically consolidated perceptron maintains a more stable memory of the first task, see <xref ref-type="fig" rid="fig5">Figure 5b</xref>. In addition, this leads to an overall improved stability-plasticity trade-off, see <xref ref-type="fig" rid="fig5">Figure 5c</xref> and shows that the effects of presynaptic consolidation in our model extend beyond gradient-based learning.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Main contribution</title><p>Information transmission in synapses is stochastic. While previous work has suggested that stochasticity allows to maximise the amount of information transmitted per unit of energy spent, this analysis has been restricted to single synapses. We argue that the relevant quantity to be considered is task-dependent information transmitted by entire networks. Introducing a simple model of the all-or-nothing nature of synaptic transmission, we show that presynaptic stochasticity enables networks to allocate energy more efficiently. We find theoretically as well as empirically that learned release probabilities encode the importance of weights for network function according to the Fisher information. Based on this finding, we suggest a novel computational role for presynaptic stochasticity in lifelong learning. Our experiments provide evidence that coupling information encoded in the release probabilities with modulated plasticity can help alleviate the stability-plasticity dilemma.</p></sec><sec id="s3-2"><title>Modelling assumptions and biological plausibility</title><sec id="s3-2-1"><title>Stochastic synaptic transmission</title><p>Our model captures the occurrence of synaptic failures by introducing a Bernoulli random variable governing whether or not neurotransmitter is released. Compared to classical models assuming deterministic transmission, this is one step closer to experimentally observed binomial transmission patterns, which are caused by multiple, rather than one, release sites between a given neuron and dendritic branch. Importantly, our simplified model accounts for the event that there is no postsynaptic depolarisation at all. Even in the presence of multiple release sites, this event has non-negligible probability: Data from cultured hippocampal neurons (<xref ref-type="bibr" rid="bib12">Branco et al., 2008</xref>, <xref ref-type="fig" rid="fig2">Figure 2D</xref>) and the neocortex (<xref ref-type="bibr" rid="bib34">Hardingham et al., 2010</xref>, <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2c</xref>) shows that the probability <inline-formula><mml:math id="inf60"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:msup></mml:math></inline-formula> that none of <inline-formula><mml:math id="inf61"><mml:mi>N</mml:mi></mml:math></inline-formula> release sites with release probability <inline-formula><mml:math id="inf62"><mml:mi>p</mml:mi></mml:math></inline-formula> is active, is around 0.3–0.4 even for <inline-formula><mml:math id="inf63"><mml:mi>N</mml:mi></mml:math></inline-formula> as large as 10. More recent evidence suggests an even wider range of values depending on the extracellular calcium concentration (<xref ref-type="bibr" rid="bib79">Sakamoto et al., 2018</xref>).</p></sec><sec id="s3-2-2"><title>Presynaptic long-term plasticity</title><p>A central property of our model builds on the observation that the locus of expression for long-term plasticity can both be presynaptic and postsynaptic (<xref ref-type="bibr" rid="bib51">Larkman et al., 1992</xref>; <xref ref-type="bibr" rid="bib58">Lisman and Raghavachari, 2006</xref>; <xref ref-type="bibr" rid="bib8">Bayazitov et al., 2007</xref>; <xref ref-type="bibr" rid="bib86">Sjöström et al., 2007</xref>; <xref ref-type="bibr" rid="bib11">Bliss and Collingridge, 2013</xref>; <xref ref-type="bibr" rid="bib20">Costa et al., 2017</xref>). The mechanisms to change either are distinct and synapse-specific (<xref ref-type="bibr" rid="bib93">Yang and Calakos, 2013</xref>; <xref ref-type="bibr" rid="bib16">Castillo, 2012</xref>), but how exactly pre- and postsynaptic forms of long-term potentiation (LTP) and long-term depression (LTD) interact is not yet fully understood (<xref ref-type="bibr" rid="bib66">Monday et al., 2018</xref>). The induction of long-term plasticity is thought to be triggered postsynaptically for both presynaptic and postsynaptic changes (<xref ref-type="bibr" rid="bib93">Yang and Calakos, 2013</xref>; <xref ref-type="bibr" rid="bib71">Padamsey and Emptage, 2014</xref>) and several forms of presynaptic plasticity are known to require retrograde signalling (<xref ref-type="bibr" rid="bib66">Monday et al., 2018</xref>), for example through nitric oxide or endocannabinoids (<xref ref-type="bibr" rid="bib40">Heifets and Castillo, 2009</xref>; <xref ref-type="bibr" rid="bib5">Andrade-Talavera et al., 2016</xref>; <xref ref-type="bibr" rid="bib20">Costa et al., 2017</xref>). This interaction between pre- and postsynaptic sites is reflected by our learning rule, in which both pre- and postsynaptic changes are governed by postsynaptic updates and require communication between pre- and postsynapse. The proposed presynaptic updates rely on both presynaptic LTP and presynaptic LTD. At least one form of presynaptic long-term plasticity is known to be bidirectional switching from potentiation to depression depending on endocannabinoid transients (<xref ref-type="bibr" rid="bib23">Cui et al., 2015</xref>; <xref ref-type="bibr" rid="bib24">Cui et al., 2016</xref>).</p></sec><sec id="s3-2-3"><title>Link between presynaptic release and synaptic stability</title><p>Our model suggests that increasing the stability of synapses with large release probability improves memory. Qualitatively, this is in line with observations that presynaptic boutons, which contain stationary mitochondria (<xref ref-type="bibr" rid="bib17">Chang et al., 2006</xref>; <xref ref-type="bibr" rid="bib69">Obashi and Okabe, 2013</xref>), are more stable than those which do not, both on short (<xref ref-type="bibr" rid="bib87">Sun et al., 2013</xref>) and long timescales of at least weeks (<xref ref-type="bibr" rid="bib54">Lees et al., 2019</xref>). Quantitatively, we find evidence for such a link by re-analysing data (Data was made publicly available in <xref ref-type="bibr" rid="bib20">Costa et al., 2017</xref>). from <xref ref-type="bibr" rid="bib85">Sjöström et al., 2001</xref> for a spike-timing-dependent plasticity protocol in the rat primary visual cortex: <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref> shows that synapses with higher initial release probability are more stable than those with low release probabilities for both LTP and LTD.</p></sec><sec id="s3-2-4"><title>Credit assignment</title><p>In our multilayer perceptron model, updates are computed using backpropagated gradients. Whether credit assignment in the brain relies on backpropagation – or more generally gradients – remains an active area of research, but several alternatives aiming to increase biological plausibility exist and are compatible with our model (<xref ref-type="bibr" rid="bib78">Sacramento et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Lillicrap et al., 2016</xref>; <xref ref-type="bibr" rid="bib53">Lee et al., 2015</xref>). To check that the proposed mechanism can also operate without gradient information, we include an experiment with a standard perceptron and its gradient-free learning rule (<xref ref-type="bibr" rid="bib76">Rosenblatt, 1958</xref>), see <xref ref-type="fig" rid="fig5">Figure 5b, c</xref>.</p></sec><sec id="s3-2-5"><title>Correspondence to biological networks</title><p>We study general rate-based neural networks raising the question in which biological networks or contexts one might expect the proposed mechanisms to be at work. Our experiments suggest that improved energy efficiency can at least partly be attributed to the sparsification induced by presynaptic stochasticity (cf. <xref ref-type="bibr" rid="bib70">Olshausen and Field, 2004</xref>). Networks which are known to rely on sparse representations are thus natural candidates for the dynamics investigated here. This includes a wide range of sensory networks (<xref ref-type="bibr" rid="bib73">Perez-Orive et al., 2002</xref>; <xref ref-type="bibr" rid="bib33">Hahnloser et al., 2002</xref>; <xref ref-type="bibr" rid="bib21">Crochet et al., 2011</xref>; <xref ref-type="bibr" rid="bib75">Quiroga et al., 2005</xref>) as well as areas in the hippocampus (<xref ref-type="bibr" rid="bib89">Wixted et al., 2014</xref>; <xref ref-type="bibr" rid="bib60">Lodge and Bischofberger, 2019</xref>).</p><p>In the context of lifelong learning, our learning rule provides a potential mechanism that helps to slowly incorporate new knowledge into a network with preexisting memories. Generally, the introduced consolidation mechanism could benefit the slow part of a complementary learning system as proposed by <xref ref-type="bibr" rid="bib64">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib50">Kumaran et al., 2016</xref>. Sensory networks in particular might utilize such a mechanism as they require to learn new stimuli while retaining the ability to recognise previous ones (<xref ref-type="bibr" rid="bib14">Buonomano and Merzenich, 1998</xref>; <xref ref-type="bibr" rid="bib29">Gilbert et al., 2009</xref>; <xref ref-type="bibr" rid="bib65">Moczulska et al., 2013</xref>). Indeed, in line with the hypothesis that synapses with larger release probability are more stable, it has been observed that larger spines in the mouse barrel cortex are more stable. Moreover, novel experiences lead to the formation of new, stable spines, similar to our findings reported in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3b</xref>.</p></sec></sec><sec id="s3-3"><title>Related synapse models</title><sec id="s3-3-1"><title>Probabilistic synapse models</title><p>The goal of incorporating and interpreting noise in models of neural computation is shared by many computational studies. Inspired by a Bayesian perspective, neural variability is often interpreted as representing uncertainty (<xref ref-type="bibr" rid="bib61">Ma et al., 2006</xref>; <xref ref-type="bibr" rid="bib27">Fiser et al., 2010</xref>; <xref ref-type="bibr" rid="bib45">Kappel et al., 2015</xref>; <xref ref-type="bibr" rid="bib32">Haefner et al., 2016</xref>), or as a means to prevent overfitting (<xref ref-type="bibr" rid="bib88">Wan et al., 2013</xref>). The Bayesian paradigm has been applied directly to variability of individual synapses in neuroscience (<xref ref-type="bibr" rid="bib1">Aitchison et al., 2014</xref>; <xref ref-type="bibr" rid="bib3">Aitchison and Latham, 2015</xref>; <xref ref-type="bibr" rid="bib2">Aitchison et al., 2021</xref>) and machine learning (<xref ref-type="bibr" rid="bib94">Zeno et al., 2018</xref>). It prescribes decreasing the plasticity of synapses with low posterior variance. A similiar relationship can be shown to hold for our model as described in the Material and Methods. In contrast to common Bayesian interpretations (<xref ref-type="bibr" rid="bib94">Zeno et al., 2018</xref>; <xref ref-type="bibr" rid="bib3">Aitchison and Latham, 2015</xref>; <xref ref-type="bibr" rid="bib45">Kappel et al., 2015</xref>) which model release statistics as Gaussians and optimise complex objectives (see also <xref ref-type="bibr" rid="bib59">Llera-Montero et al., 2019</xref>) our simple proposal represents the inherently discrete nature of synaptic transmission more faithfully.</p></sec><sec id="s3-3-2"><title>Complex synapse models</title><p>In the context of lifelong learning, our model’s consolidation mechanism is similar to Elastic Weight Consolidation (EWC) (<xref ref-type="bibr" rid="bib48">Kirkpatrick et al., 2017</xref>), which explicitly relies on the Fisher information to consolidate synapses. Unlike EWC, our learning rule does not require a task switch signal and does not need a separate consolidation phase. Moreover, our model can be interpreted as using distinct states of plasticity to protect memories. This general idea is formalised and analysed thoroughly by theoretical work on cascade models of plasticity (<xref ref-type="bibr" rid="bib28">Fusi et al., 2005</xref>; <xref ref-type="bibr" rid="bib77">Roxin and Fusi, 2013</xref>; <xref ref-type="bibr" rid="bib9">Benna and Fusi, 2016</xref>). The resulting model (<xref ref-type="bibr" rid="bib9">Benna and Fusi, 2016</xref>) has also been shown to be effective in lifelong learning settings (<xref ref-type="bibr" rid="bib44">Kaplanis et al., 2018</xref>).</p></sec></sec><sec id="s3-4"><title>Synaptic importance may govern energy-information trade-offs</title><p>Energy constraints are widely believed to be a main driver of evolution (<xref ref-type="bibr" rid="bib68">Niven and Laughlin, 2008</xref>). From brain size (<xref ref-type="bibr" rid="bib43">Isler and van Schaik, 2009</xref>; <xref ref-type="bibr" rid="bib67">Navarrete et al., 2011</xref>), to wiring cost (<xref ref-type="bibr" rid="bib18">Chen et al., 2006</xref>), down to ion channel properties (<xref ref-type="bibr" rid="bib4">Alle et al., 2009</xref>; <xref ref-type="bibr" rid="bib82">Sengupta et al., 2010</xref>), presynaptic transmitter release (<xref ref-type="bibr" rid="bib80">Savtchenko et al., 2013</xref>) and postsynaptic conductance (<xref ref-type="bibr" rid="bib36">Harris et al., 2015</xref>), various components of the nervous system have been shown to be optimal in terms of their total metabolic cost or their metabolic cost per bit of information transmitted.</p><p>Crucially, there is evidence that the central nervous system operates in varying regimes, making different trade-offs between synaptic energy demand and information transmission: <xref ref-type="bibr" rid="bib74">Perge et al., 2009</xref>; <xref ref-type="bibr" rid="bib15">Carter and Bean, 2009</xref>; <xref ref-type="bibr" rid="bib41">Hu and Jonas, 2014</xref> all find properties of the axon (thickness, sodium channel properties), which are suboptimal in terms of energy per bit of information. They suggest that these inefficiencies occur to ensure fast transmission of highly relevant information.</p><p>We propose that a similar energy/information trade-off could govern network dynamics preferentially allocating more energy to the most relevant synapses for a given task. Our model relies on a simple, theoretically justified learning rule to achieve this goal and leads to overall energy savings. Neither the trade-off nor the overall savings can be accounted for by previous frameworks for energy-efficient information transmission at synapses (<xref ref-type="bibr" rid="bib56">Levy and Baxter, 2002</xref>; <xref ref-type="bibr" rid="bib35">Harris et al., 2012</xref>).</p><p>This view of release probabilities and related metabolic cost provides a way to make the informal notion of ‘synaptic importance’ concrete by measuring how much energy is spent on a synapse. Interestingly, our model suggests that this notion is helpful beyond purely energetic considerations and can in fact help to maintain memories during lifelong learning.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Summary of learning rule</title><p>Our learning rule has two components, an update for the presynaptic release probability <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and an update for the postsynaptic strength <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The update of the synaptic strength <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined implicitly through updating the expected synaptic strength <inline-formula><mml:math id="inf67"><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:math></inline-formula><disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mrow><mml:mi mathvariant="sans-serif">w</mml:mi><mml:mi mathvariant="sans-serif">h</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msubsup><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>and the presynaptic update is given by<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>up</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">f</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:msub><mml:mi/><mml:mtext>lim</mml:mtext></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>down</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">f</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:msub><mml:mi/><mml:mtext>lim</mml:mtext></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This leads to the following explicit update rule for the synaptic strength <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:math></inline-formula><disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula><disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where we used the chain rule to rewrite <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>. For the lifelong learning experiment, we additionally stabilise high release probability synapses by multiplying the learning rate by <inline-formula><mml:math id="inf70"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for each synapse and by freezing release probabilities (but not strengths) when they surpass a predefined threshold <inline-formula><mml:math id="inf71"><mml:msub><mml:mi>p</mml:mi><mml:mtext>freeze</mml:mtext></mml:msub></mml:math></inline-formula>.</p></sec><sec id="s4-2"><title>Theoretical analysis of presynaptic learning rule</title><p>As indicated in the results section the release probability <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is more likely to be large when the Fisher information of the synaptic strength <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is large as well. This provides a theoretical explanation to the intuitive correspondence between release probability and synaptic importance. Here, we formalise this link starting with a brief review of the Fisher information.</p><sec id="s4-2-1"><title>Fisher information</title><p>The Fisher information is a measure for the networks sensitivity to changes in parameters. Under additional assumptions it is equal to the Hessian of the loss function (<xref ref-type="bibr" rid="bib72">Pascanu and Bengio, 2013</xref>; <xref ref-type="bibr" rid="bib62">Martens, 2014</xref>), giving an intuitive reason why synapses with high Fisher information should not be changed much if network function is to be preserved.</p><p>Formally, for a model with parameter vector <inline-formula><mml:math id="inf74"><mml:mi>θ</mml:mi></mml:math></inline-formula> predicting a probability distribution <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for inputs <inline-formula><mml:math id="inf76"><mml:mi>X</mml:mi></mml:math></inline-formula> and labels <inline-formula><mml:math id="inf77"><mml:mi>y</mml:mi></mml:math></inline-formula> drawn from a joint distribution <inline-formula><mml:math id="inf78"><mml:mi class="ltx_font_mathcaligraphic">𝒟</mml:mi></mml:math></inline-formula>, the Fisher information matrix is defined as<disp-formula id="equ11"><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒟</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">[</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Note that this expression is independent of the actual labels <inline-formula><mml:math id="inf79"><mml:mi>y</mml:mi></mml:math></inline-formula> of the dataset and that instead we sample labels from the model’s predictions. If the model makes correct predictions, we can replace the second expectation, which is over <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, by the empirical labels <inline-formula><mml:math id="inf81"><mml:mi>y</mml:mi></mml:math></inline-formula> of the dataset for an approximation called the Empirical Fisher information. If we further only consider the diagonal entries – corresponding to a mean-field approximation – and write <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>ln</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> we obtain the following expression for the <inline-formula><mml:math id="inf83"><mml:mi>i</mml:mi></mml:math></inline-formula>-th entry of the diagonal Empirical Fisher information:<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>𝔼</mml:mi><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒟</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that this version of the Fisher information relies on the same gradients that are used to update the parameters of the multilayer perceptron, see <xref ref-type="disp-formula" rid="equ3 equ4">Equations (3), (4)</xref>.</p><p>Under the assumption that the learned probability distribution <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>∣</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> equals the real probability distribution, the Fisher information equals the Hessian of the cross entropy loss (i.e. the negative log-probabilities) with respect to the model parameters (<xref ref-type="bibr" rid="bib72">Pascanu and Bengio, 2013</xref>; <xref ref-type="bibr" rid="bib62">Martens, 2014</xref>). The Fisher information was previously used in machine learning to enable lifelong learning (<xref ref-type="bibr" rid="bib48">Kirkpatrick et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Huszár, 2018</xref>) and it has been shown that other popular lifelong learning methods implicitly rely on the Fisher information (<xref ref-type="bibr" rid="bib10">Benzing, 2020</xref>).</p></sec><sec id="s4-2-2"><title>Link between release probabilities and Fisher information</title><p>We now explain how our learning rule for the release probability is related to the Fisher information. For simplicity of exposition, we focus our analysis on a particular sampled subnetwork with deterministic synaptic strengths. Recall that update rule (4) for release probabilities increases the release probability, if the gradient magnitude <inline-formula><mml:math id="inf85"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> is above a certain threshold, <inline-formula><mml:math id="inf86"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>lim</mml:mtext></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and decreases them otherwise. Let us denote by <inline-formula><mml:math id="inf87"><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> the probability that the <inline-formula><mml:math id="inf88"><mml:mi>i</mml:mi></mml:math></inline-formula>-th release probability is increased. Then<disp-formula id="equ13"><label>(10)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>:=</mml:mo><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">l</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">l</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where the probability space corresponds to sampling training examples. Note that <inline-formula><mml:math id="inf89"><mml:mrow><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by definition of the Empirical Fisher information <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. So if we assume that <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&gt;</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mtext>lim</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> depends monotonically on <inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> then we already see that <inline-formula><mml:math id="inf93"><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> depends monotonically on <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. This in turn implies that synapses with a larger Fisher information are more likely to have a large release probability, which is what we claimed. We now discuss the assumption made above.</p></sec></sec><sec id="s4-3"><title>Assumption: <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> depends monotonically on <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></title><p>While this assumption is not true for arbitrary distributions of <inline-formula><mml:math id="inf97"><mml:mi>g</mml:mi></mml:math></inline-formula>, it holds for many commonly studied parametric families and seems likely to hold (approximately) for realistic, non-adversarially chosen distributions. For example, if each <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> follows a normal distribution <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒩</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with varying <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf101"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, then<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula>and<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo form="prefix" movablelimits="true">Pr</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">l</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">f</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">l</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>so that <inline-formula><mml:math id="inf102"><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> is indeed monotonically increasing in <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. Similar arguments can be made for example for a Laplace distribution, with scale larger than mean.</p></sec><sec id="s4-4"><title>Link between learning rate modulation and Bayesian updating</title><p>Recall that we multiply the learning rate of each synapse by <inline-formula><mml:math id="inf104"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, see <xref ref-type="disp-formula" rid="equ6">Equation (5)</xref>. This learning rate modulation can be related to the update prescribed by Bayesian modelling. As shown before, synapses with large Fisher information tend to have large release probability, which results in a decrease of the plasticity of synapses with large Fisher information. We can treat the (diagonal) Fisher information as an approximation of the posterior precision based on a Laplace approximation of the posterior likelihood (<xref ref-type="bibr" rid="bib48">Kirkpatrick et al., 2017</xref>) which exploits that the Fisher information approaches the Hessian of the loss as the task gets learned (<xref ref-type="bibr" rid="bib62">Martens, 2014</xref>). Using this relationship, our learning rate modulation tends to lower the learning rate of synapses with low posterior variance as prescribed by Bayesian modelling.</p></sec><sec id="s4-5"><title>Practical approximation</title><p>The derivation above assumes that each gradient <inline-formula><mml:math id="inf105"><mml:mi>g</mml:mi></mml:math></inline-formula> is computed using a single input, so that <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>𝔼</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> equals the Fisher information. While this may be the biologically more plausible setting, in standard artificial neural network (ANN) training the gradient is averaged across several inputs (mini-batches). Despite this modification, <inline-formula><mml:math id="inf107"><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> remains a good, and commonly used, approximation of the Fisher, see for example <xref ref-type="bibr" rid="bib47">Khan et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">Benzing, 2020</xref>.</p></sec><sec id="s4-6"><title>Perceptron for lifelong learning</title><p>To demonstrate that our findings on presynaptic stochasticity and plasticity are applicable to other models and learning rules, we include experiments for the standard perceptron (<xref ref-type="bibr" rid="bib76">Rosenblatt, 1958</xref>) in a lifelong learning setting.</p><sec id="s4-6-1"><title>Model</title><p>The perceptron is a classical model for a neuron with multiple inputs and threshold activation function. It is used to memorise the binary labels of a number of input patterns where input patterns are sampled uniformly from <inline-formula><mml:math id="inf108"><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:msup></mml:math></inline-formula> and their labels are sampled uniformly from <inline-formula><mml:math id="inf109"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. Like in ANNs, the output neuron of a perceptron computes a weighted sum of its inputs followed by nonlinear activation <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ16"><label>(11)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow><mml:munder><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mfrac linethickness="0"><mml:mrow><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">o</mml:mi><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">y</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">v</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">o</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mrow><mml:munder><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mfrac linethickness="0"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">i</mml:mi></mml:mrow><mml:mtext mathvariant="sans-serif">-</mml:mtext><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">h</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">r</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">s</mml:mi><mml:mi mathvariant="sans-serif">y</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi><mml:mi mathvariant="sans-serif">a</mml:mi><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">i</mml:mi><mml:mi mathvariant="sans-serif">n</mml:mi><mml:mi mathvariant="sans-serif">p</mml:mi><mml:mi mathvariant="sans-serif">u</mml:mi><mml:mi mathvariant="sans-serif">t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The only difference to the ANN model is that the nonlinearity is the sign function and that there is only one layer. We model each synapse <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as a Bernoulli variable <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> with synaptic strength <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and release probability <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> just as before, see <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>. The expected strengths <inline-formula><mml:math id="inf115"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are learned according to the standard perceptron learning rule (<xref ref-type="bibr" rid="bib76">Rosenblatt, 1958</xref>). The only modification we make is averaging weight updates across 5 inputs, rather than applying an update after each input. Without this modification, the update size <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for each weight <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> would be constant according to the perceptron learning rule. Consequently, our update rule for <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> would not be applicable. However, after averaging across five patterns we can apply the same update rule for <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> as previously, see <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref>, and also use the same learning rate modification, see <xref ref-type="disp-formula" rid="equ6">Equation (5)</xref>. We clarify that <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> now refers to the update of expected strength <inline-formula><mml:math id="inf121"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>. In the case of ANN this is proportional to the gradient, while in the case of the non-differentiable perceptron it has no additional interpretation.</p></sec><sec id="s4-6-2"><title>Experiments</title><p>For the lifelong learning experiments, we used five tasks, each consisting of 100 randomly sampled and labelled patterns of size <inline-formula><mml:math id="inf122"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>. We compared the perceptron with learned stochastic weights to a standard perceptron. For the standard perceptron, we also averaged updates across five patterns. Both models were sequentially trained on five tasks, using 25 passes through the data for each task.</p><p>We note that for more patterns, when the perceptron gets closer to its maximum capacity of <inline-formula><mml:math id="inf123"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, the average accuracies of the stochastic and standard perceptron become more similar, suggesting that the benefits of stochastic synapses occur when model capacity is not fully used.</p><p>As metaplasticity parameters we used <inline-formula><mml:math id="inf124"><mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mtext>lim</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>up</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mtext>down</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf125"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>min</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>freeze</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. These were coarsely tuned on an analogous experiment with only two tasks instead of five.</p></sec></sec><sec id="s4-7"><title>Experimental setup</title><sec id="s4-7-1"><title>Code availability</title><p>Code for all experiments is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/smonsays/presynaptic-stochasticity">github.com/smonsays/presynaptic-stochasticity</ext-link> (<xref ref-type="bibr" rid="bib81">Schug, 2021</xref>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7d155b25f3b63cb8c296b575c2cbefd2d94f39a0;origin=https://github.com/smonsays/presynaptic-stochasticity;visit=swh:1:snp:21724e4a17b4d4e991980958593e08c91288b836;anchor=swh:1:rev:de0851773cd1375b885dcdb18e711a2fb6eb06a4">swh:1:rev:de0851773cd1375b885dcdb18e711a2fb6eb06a4</ext-link>).</p></sec><sec id="s4-7-2"><title>Metaplasticity parameters</title><p>Our method has a number of metaplasticity parameters, namely <inline-formula><mml:math id="inf126"><mml:msub><mml:mi>p</mml:mi><mml:mtext>up</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf127"><mml:msub><mml:mi>p</mml:mi><mml:mtext>down</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf128"><mml:msub><mml:mi>g</mml:mi><mml:mtext>lim</mml:mtext></mml:msub></mml:math></inline-formula> and the learning rate <inline-formula><mml:math id="inf129"><mml:mi>η</mml:mi></mml:math></inline-formula>. For the lifelong learning experiments, there is an additional parameter <inline-formula><mml:math id="inf130"><mml:msub><mml:mi>p</mml:mi><mml:mtext>freeze</mml:mtext></mml:msub></mml:math></inline-formula>. For the energy experiments, we fix <inline-formula><mml:math id="inf131"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>up</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mtext>down</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.07</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf132"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mtext>lim</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula> and choose <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> based on coarse, manual tuning. For the lifelong learning experiments, we choose <inline-formula><mml:math id="inf134"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>0.001</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and optimise the remaining metaplasticity parameters through a random search on one task, namely Permuted MNIST, resulting in <inline-formula><mml:math id="inf135"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>up</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0516</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf136"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>down</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.0520</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf137"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mtext>lim</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math></inline-formula>. We use the same fixed parametrisation for all other tasks, namely Permuted Fashion MNIST, Split MNIST and Split Fashion MNIST (see below for detailed task descriptions). For the ablation experiment in <xref ref-type="fig" rid="fig5">Figure 5a</xref>, metaplasticity parameters were re-optimised for each ablation in a random search to ensure a fair, meaningful comparison.</p></sec><sec id="s4-7-3"><title>Model robustness</title><p>We confirmed that the model is robust with respect to the exact choice of parameters. For the energy experiments, de- or increasing <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>up</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mtext>down</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> by 25 does not qualitatively change results.</p><p>For the lifelong learning experiment, the chosen tuning method is a strong indicator of robustness: The metaplasticitiy parameters are tuned on one setup (Permuted MNIST) and then transferred to others (Split MNIST, Permuted and Split Fashion MNIST). The results presented in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref> show that the parameters found in one scenario are robust and carry over to several other settings. We emphasise that the differences between these scenarios are considerable. For example, for permuted MNIST consecutive input distributions are essentially uncorrelated by design, while for Split (Fashion) MNIST input distributions are strongly correlated. In addition, from MNIST to Fashion MNIST the number of ‘informative’ pixels changes drastically.</p></sec><sec id="s4-7-4"><title>Lifelong learning tasks</title><p>For the lifelong learning experiments, we tested our method as well as baselines in several scenarios on top of the Split MNIST protocol described in the main text.</p></sec></sec><sec id="s4-8"><title>Permuted MNIST</title><p>In the Permuted MNIST benchmark, each task consists of a random but fixed permutation of the input pixels of all MNIST images (<xref ref-type="bibr" rid="bib31">Goodfellow et al., 2013</xref>). We generate 10 tasks using this procedure and present them sequentially without any indication of task boundaries during training. A main reason to consider the Permuted MNIST protocol is that it generates tasks of equal difficulty.</p></sec><sec id="s4-9"><title>Permuted and split fashion MNIST</title><p>Both the Split and Permuted protocol can be applied to other datasets. We use them on the Fashion MNIST dataset (<xref ref-type="bibr" rid="bib90">Xiao et al., 2017</xref>) consisting of 60,000 greyscale images of 10 different fashion items with <inline-formula><mml:math id="inf139"><mml:mrow><mml:mn>28</mml:mn><mml:mo>×</mml:mo><mml:mn>28</mml:mn></mml:mrow></mml:math></inline-formula> pixels.</p></sec><sec id="s4-10"><title>Continuous permuted MNIST</title><p>We carry out an additional experiment on the continuous Permuted MNIST dataset (<xref ref-type="bibr" rid="bib94">Zeno et al., 2018</xref>). This is a modified version of the Permuted MNIST dataset which introduces a smooth transition period between individual tasks where data from both distributions is mixed. It removes the abrupt change between tasks and allows us to investigate if our method depends on such an implicit task switch signal. We observe a mean accuracy over all tasks of <inline-formula><mml:math id="inf140"><mml:mrow><mml:mn>0.8539</mml:mn><mml:mo>±</mml:mo><mml:mn>0.006</mml:mn></mml:mrow></mml:math></inline-formula> comparable to the non-continuous case suggesting that our method does not require abrupt changes from one task to another.</p><sec id="s4-10-1"><title>Neural network training</title><p>Our neural network architecture consists of two fully connected hidden layers of 200 neurons without biases with rectified linear unit activation functions <inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The final layer uses a softmax and cross-entropy loss. Network weights were initialised according to the PyTorch default for fully connected layers, which is similar to Kaiming uniform initialisation (<xref ref-type="bibr" rid="bib30">Glorot and Bengio, 2010</xref>; <xref ref-type="bibr" rid="bib39">He et al., 2015</xref>) but divides weights by an additional factor of √6. We use standard stochastic gradient descent to update the average weight <inline-formula><mml:math id="inf142"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> only altered by the learning rate modulation described for the lifelong learning experiments. We use a batch size of 100 and train each task for 10 epochs in the lifelong learning setting. In the energy-information experiments we train the model for 50 epochs.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank João Sacramento and Mark van Rossum for stimulating discussions and helpful comments.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Visualization, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>Code for presynaptic stochasticity model and baselines.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-69884-code1-v2.zip"/></supplementary-material><supplementary-material id="sdata1"><label>Source data 1.</label><caption><title>Raw data and code to create figures.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-69884-data1-v2.zip"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-69884-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Code for experiments is part of the submission and is published on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/smonsays/presynaptic-stochasticity">https://github.com/smonsays/presynaptic-stochasticity</ext-link> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:de0851773cd1375b885dcdb18e711a2fb6eb06a4">https://archive.softwareheritage.org/swh:1:rev:de0851773cd1375b885dcdb18e711a2fb6eb06a4</ext-link>).</p><p>The following previously published dataset was used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>R</given-names></name><name><surname>Froemke</surname><given-names>R</given-names></name><name><surname>Sjöström</surname><given-names>P</given-names></name><name><surname>van Rossum</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Data from: Unified pre- and postsynaptic long-term plasticity enables reliable and flexible learning</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.p286g</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Aitchison</surname> <given-names>L</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Probabilistic synapses</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1410.1029">http://arxiv.org/abs/1410.1029</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitchison</surname> <given-names>L</given-names></name><name><surname>Jegminat</surname> <given-names>J</given-names></name><name><surname>Menendez</surname> <given-names>JA</given-names></name><name><surname>Pfister</surname> <given-names>JP</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Synaptic plasticity as bayesian inference</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>565</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00809-5</pub-id><pub-id pub-id-type="pmid">33707754</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Aitchison</surname> <given-names>L</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Synaptic sampling: a connection between PSP variability and uncertainty explains neurophysiological observations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1505.04544">https://arxiv.org/abs/1505.04544</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alle</surname> <given-names>H</given-names></name><name><surname>Roth</surname> <given-names>A</given-names></name><name><surname>Geiger</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Energy-efficient action potentials in hippocampal mossy fibers</article-title><source>Science</source><volume>325</volume><fpage>1405</fpage><lpage>1408</lpage><pub-id pub-id-type="doi">10.1126/science.1174331</pub-id><pub-id pub-id-type="pmid">19745156</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrade-Talavera</surname> <given-names>Y</given-names></name><name><surname>Duque-Feria</surname> <given-names>P</given-names></name><name><surname>Paulsen</surname> <given-names>O</given-names></name><name><surname>Rodríguez-Moreno</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Presynaptic spike Timing-Dependent Long-Term depression in the mouse hippocampus</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>3637</fpage><lpage>3654</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw172</pub-id><pub-id pub-id-type="pmid">27282393</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attwell</surname> <given-names>D</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An energy budget for signaling in the grey matter of the brain</article-title><source>Journal of Cerebral Blood Flow &amp; Metabolism</source><volume>21</volume><fpage>1133</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1097/00004647-200110000-00001</pub-id><pub-id pub-id-type="pmid">11598490</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banino</surname> <given-names>A</given-names></name><name><surname>Barry</surname> <given-names>C</given-names></name><name><surname>Uria</surname> <given-names>B</given-names></name><name><surname>Blundell</surname> <given-names>C</given-names></name><name><surname>Lillicrap</surname> <given-names>T</given-names></name><name><surname>Mirowski</surname> <given-names>P</given-names></name><name><surname>Pritzel</surname> <given-names>A</given-names></name><name><surname>Chadwick</surname> <given-names>MJ</given-names></name><name><surname>Degris</surname> <given-names>T</given-names></name><name><surname>Modayil</surname> <given-names>J</given-names></name><name><surname>Wayne</surname> <given-names>G</given-names></name><name><surname>Soyer</surname> <given-names>H</given-names></name><name><surname>Viola</surname> <given-names>F</given-names></name><name><surname>Zhang</surname> <given-names>B</given-names></name><name><surname>Goroshin</surname> <given-names>R</given-names></name><name><surname>Rabinowitz</surname> <given-names>N</given-names></name><name><surname>Pascanu</surname> <given-names>R</given-names></name><name><surname>Beattie</surname> <given-names>C</given-names></name><name><surname>Petersen</surname> <given-names>S</given-names></name><name><surname>Sadik</surname> <given-names>A</given-names></name><name><surname>Gaffney</surname> <given-names>S</given-names></name><name><surname>King</surname> <given-names>H</given-names></name><name><surname>Kavukcuoglu</surname> <given-names>K</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name><name><surname>Hadsell</surname> <given-names>R</given-names></name><name><surname>Kumaran</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Vector-based navigation using grid-like representations in artificial agents</article-title><source>Nature</source><volume>557</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0102-6</pub-id><pub-id pub-id-type="pmid">29743670</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayazitov</surname> <given-names>IT</given-names></name><name><surname>Richardson</surname> <given-names>RJ</given-names></name><name><surname>Fricke</surname> <given-names>RG</given-names></name><name><surname>Zakharenko</surname> <given-names>SS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Slow presynaptic and fast postsynaptic components of compound long-term potentiation</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>11510</fpage><lpage>11521</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3077-07.2007</pub-id><pub-id pub-id-type="pmid">17959794</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benna</surname> <given-names>MK</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational principles of synaptic memory consolidation</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>1697</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1038/nn.4401</pub-id><pub-id pub-id-type="pmid">27694992</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Benzing</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Understanding regularisation methods for continual learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.06357">https://arxiv.org/abs/2006.06357</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bliss</surname> <given-names>TV</given-names></name><name><surname>Collingridge</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Expression of NMDA receptor-dependent LTP in the hippocampus: bridging the divide</article-title><source>Molecular Brain</source><volume>6</volume><fpage>5</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1186/1756-6606-6-5</pub-id><pub-id pub-id-type="pmid">23339575</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branco</surname> <given-names>T</given-names></name><name><surname>Staras</surname> <given-names>K</given-names></name><name><surname>Darcy</surname> <given-names>KJ</given-names></name><name><surname>Goda</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Local dendritic activity sets release probability at hippocampal synapses</article-title><source>Neuron</source><volume>59</volume><fpage>475</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.07.006</pub-id><pub-id pub-id-type="pmid">18701072</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branco</surname> <given-names>T</given-names></name><name><surname>Staras</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The probability of neurotransmitter release: variability and feedback control at single synapses</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>373</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1038/nrn2634</pub-id><pub-id pub-id-type="pmid">19377502</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname> <given-names>DV</given-names></name><name><surname>Merzenich</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Cortical plasticity: from synapses to maps</article-title><source>Annual Review of Neuroscience</source><volume>21</volume><fpage>149</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.21.1.149</pub-id><pub-id pub-id-type="pmid">9530495</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname> <given-names>BC</given-names></name><name><surname>Bean</surname> <given-names>BP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Sodium entry during action potentials of mammalian neurons: incomplete inactivation and reduced metabolic efficiency in fast-spiking neurons</article-title><source>Neuron</source><volume>64</volume><fpage>898</fpage><lpage>909</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.12.011</pub-id><pub-id pub-id-type="pmid">20064395</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castillo</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>. (2012). Presynaptic ltp and ltd of excitatory and inhibitory synapses</article-title><source>Cold Spring Harbor Perspectives in Biology</source><volume>4</volume><elocation-id>a005728</elocation-id><pub-id pub-id-type="doi">10.1101/cshperspect.a005728</pub-id><pub-id pub-id-type="pmid">22147943</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname> <given-names>DT</given-names></name><name><surname>Honick</surname> <given-names>AS</given-names></name><name><surname>Reynolds</surname> <given-names>IJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Mitochondrial trafficking to synapses in cultured primary cortical neurons</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>7035</fpage><lpage>7045</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1012-06.2006</pub-id><pub-id pub-id-type="pmid">16807333</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>BL</given-names></name><name><surname>Hall</surname> <given-names>DH</given-names></name><name><surname>Chklovskii</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Wiring optimization can relate neuronal structure and function</article-title><source>PNAS</source><volume>103</volume><fpage>4723</fpage><lpage>4728</lpage><pub-id pub-id-type="doi">10.1073/pnas.0506806103</pub-id><pub-id pub-id-type="pmid">16537428</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichon</surname> <given-names>J</given-names></name><name><surname>Gan</surname> <given-names>WB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Branch-specific dendritic ca(2+) spikes cause persistent synaptic plasticity</article-title><source>Nature</source><volume>520</volume><fpage>180</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1038/nature14251</pub-id><pub-id pub-id-type="pmid">25822789</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname> <given-names>RP</given-names></name><name><surname>Padamsey</surname> <given-names>Z</given-names></name><name><surname>D’Amour</surname> <given-names>JA</given-names></name><name><surname>Emptage</surname> <given-names>NJ</given-names></name><name><surname>Froemke</surname> <given-names>RC</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Synaptic transmission optimization predicts expression loci of Long-Term plasticity</article-title><source>Neuron</source><volume>96</volume><fpage>177</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.021</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crochet</surname> <given-names>S</given-names></name><name><surname>Poulet</surname> <given-names>JF</given-names></name><name><surname>Kremer</surname> <given-names>Y</given-names></name><name><surname>Petersen</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Synaptic mechanisms underlying sparse coding of active touch</article-title><source>Neuron</source><volume>69</volume><fpage>1160</fpage><lpage>1175</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.022</pub-id><pub-id pub-id-type="pmid">21435560</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cueva</surname> <given-names>CJ</given-names></name><name><surname>Wei</surname> <given-names>X-X</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.07770">https://arxiv.org/abs/1803.07770</ext-link></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname> <given-names>Y</given-names></name><name><surname>Paillé</surname> <given-names>V</given-names></name><name><surname>Xu</surname> <given-names>H</given-names></name><name><surname>Genet</surname> <given-names>S</given-names></name><name><surname>Delord</surname> <given-names>B</given-names></name><name><surname>Fino</surname> <given-names>E</given-names></name><name><surname>Berry</surname> <given-names>H</given-names></name><name><surname>Venance</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Endocannabinoids mediate bidirectional striatal spike-timing-dependent plasticity</article-title><source>The Journal of Physiology</source><volume>593</volume><fpage>2833</fpage><lpage>2849</lpage><pub-id pub-id-type="doi">10.1113/JP270324</pub-id><pub-id pub-id-type="pmid">25873197</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname> <given-names>Y</given-names></name><name><surname>Prokin</surname> <given-names>I</given-names></name><name><surname>Xu</surname> <given-names>H</given-names></name><name><surname>Delord</surname> <given-names>B</given-names></name><name><surname>Genet</surname> <given-names>S</given-names></name><name><surname>Venance</surname> <given-names>L</given-names></name><name><surname>Berry</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Endocannabinoid dynamics gate spike-timing dependent depression and potentiation</article-title><source>eLife</source><volume>5</volume><elocation-id>e13185</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.13185</pub-id><pub-id pub-id-type="pmid">26920222</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>del Castillo</surname> <given-names>J</given-names></name><name><surname>Katz</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Quantal components of the end-plate potential</article-title><source>The Journal of Physiology</source><volume>124</volume><fpage>560</fpage><lpage>573</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1954.sp005129</pub-id><pub-id pub-id-type="pmid">13175199</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname> <given-names>AA</given-names></name><name><surname>White</surname> <given-names>JA</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Ion-channel noise places limits on the miniaturization of the brain's wiring</article-title><source>Current Biology</source><volume>15</volume><fpage>1143</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2005.05.056</pub-id><pub-id pub-id-type="pmid">15964281</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname> <given-names>J</given-names></name><name><surname>Berkes</surname> <given-names>P</given-names></name><name><surname>Orbán</surname> <given-names>G</given-names></name><name><surname>Lengyel</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.003</pub-id><pub-id pub-id-type="pmid">20153683</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fusi</surname> <given-names>S</given-names></name><name><surname>Drew</surname> <given-names>PJ</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Cascade models of synaptically stored memories</article-title><source>Neuron</source><volume>45</volume><fpage>599</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.02.001</pub-id><pub-id pub-id-type="pmid">15721245</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname> <given-names>CD</given-names></name><name><surname>Li</surname> <given-names>W</given-names></name><name><surname>Piech</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perceptual learning and adult cortical plasticity</article-title><source>The Journal of Physiology</source><volume>587</volume><fpage>2743</fpage><lpage>2751</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2009.171488</pub-id><pub-id pub-id-type="pmid">19525560</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Glorot</surname> <given-names>X</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Understanding the difficulty of training deep feedforward neural networks</article-title><conf-name>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</conf-name><fpage>249</fpage><lpage>256</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Goodfellow</surname> <given-names>IJ</given-names></name><name><surname>Mirza</surname> <given-names>M</given-names></name><name><surname>Xiao</surname> <given-names>D</given-names></name><name><surname>Courville</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>An empirical investigation of catastrophic forgetting in gradient-based neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1312.6211">https://arxiv.org/abs/1312.6211</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haefner</surname> <given-names>RM</given-names></name><name><surname>Berkes</surname> <given-names>P</given-names></name><name><surname>Fiser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual Decision-Making as probabilistic inference by neural sampling</article-title><source>Neuron</source><volume>90</volume><fpage>649</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.020</pub-id><pub-id pub-id-type="pmid">27146267</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahnloser</surname> <given-names>RH</given-names></name><name><surname>Kozhevnikov</surname> <given-names>AA</given-names></name><name><surname>Fee</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>An ultra-sparse code underlies the generation of neural sequences in a songbird</article-title><source>Nature</source><volume>419</volume><fpage>65</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/nature00974</pub-id><pub-id pub-id-type="pmid">12214232</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardingham</surname> <given-names>NR</given-names></name><name><surname>Read</surname> <given-names>JC</given-names></name><name><surname>Trevelyan</surname> <given-names>AJ</given-names></name><name><surname>Nelson</surname> <given-names>JC</given-names></name><name><surname>Jack</surname> <given-names>JJ</given-names></name><name><surname>Bannister</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Quantal analysis reveals a functional correlation between presynaptic and postsynaptic efficacy in excitatory connections from rat neocortex</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>1441</fpage><lpage>1451</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3244-09.2010</pub-id><pub-id pub-id-type="pmid">20107071</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>JJ</given-names></name><name><surname>Jolivet</surname> <given-names>R</given-names></name><name><surname>Attwell</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Synaptic energy use and supply</article-title><source>Neuron</source><volume>75</volume><fpage>762</fpage><lpage>777</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.019</pub-id><pub-id pub-id-type="pmid">22958818</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>JJ</given-names></name><name><surname>Jolivet</surname> <given-names>R</given-names></name><name><surname>Engl</surname> <given-names>E</given-names></name><name><surname>Attwell</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Energy-Efficient information transfer by visual pathway synapses</article-title><source>Current Biology</source><volume>25</volume><fpage>3151</fpage><lpage>3160</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.10.063</pub-id><pub-id pub-id-type="pmid">26671670</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname> <given-names>JJ</given-names></name><name><surname>Engl</surname> <given-names>E</given-names></name><name><surname>Attwell</surname> <given-names>D</given-names></name><name><surname>Jolivet</surname> <given-names>RB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Energy-efficient information transfer at Thalamocortical synapses</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1007226</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007226</pub-id><pub-id pub-id-type="pmid">31381555</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayashi-Takagi</surname> <given-names>A</given-names></name><name><surname>Yagishita</surname> <given-names>S</given-names></name><name><surname>Nakamura</surname> <given-names>M</given-names></name><name><surname>Shirai</surname> <given-names>F</given-names></name><name><surname>Wu</surname> <given-names>YI</given-names></name><name><surname>Loshbaugh</surname> <given-names>AL</given-names></name><name><surname>Kuhlman</surname> <given-names>B</given-names></name><name><surname>Hahn</surname> <given-names>KM</given-names></name><name><surname>Kasai</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Labelling and optical erasure of synaptic memory traces in the motor cortex</article-title><source>Nature</source><volume>525</volume><fpage>333</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1038/nature15257</pub-id><pub-id pub-id-type="pmid">26352471</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Delving Deep Into Rectifiers: Surpassing Human-Level Performance on Imagenet Classification</article-title><conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name><fpage>1026</fpage><lpage>1034</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heifets</surname> <given-names>BD</given-names></name><name><surname>Castillo</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Endocannabinoid signaling and long-term synaptic plasticity</article-title><source>Annual Review of Physiology</source><volume>71</volume><fpage>283</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1146/annurev.physiol.010908.163149</pub-id><pub-id pub-id-type="pmid">19575681</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname> <given-names>H</given-names></name><name><surname>Jonas</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A supercritical density of na(+) channels ensures fast signaling in GABAergic interneuron axons</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>686</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1038/nn.3678</pub-id><pub-id pub-id-type="pmid">24657965</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huszár</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Note on the quadratic penalties in elastic weight consolidation</article-title><source>PNAS</source><volume>115</volume><fpage>E2496</fpage><lpage>E2497</lpage><pub-id pub-id-type="doi">10.1073/pnas.1717042115</pub-id><pub-id pub-id-type="pmid">29463735</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isler</surname> <given-names>K</given-names></name><name><surname>van Schaik</surname> <given-names>CP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The expensive brain: a framework for explaining evolutionary changes in brain size</article-title><source>Journal of Human Evolution</source><volume>57</volume><fpage>392</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1016/j.jhevol.2009.04.009</pub-id><pub-id pub-id-type="pmid">19732937</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kaplanis</surname> <given-names>C</given-names></name><name><surname>Shanahan</surname> <given-names>M</given-names></name><name><surname>Clopath</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Continual reinforcement learning with complex synapses</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>2497</fpage><lpage>2506</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kappel</surname> <given-names>D</given-names></name><name><surname>Habenschuss</surname> <given-names>S</given-names></name><name><surname>Legenstein</surname> <given-names>R</given-names></name><name><surname>Maass</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Network plasticity as bayesian inference</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004485</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004485</pub-id><pub-id pub-id-type="pmid">26545099</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname> <given-names>AJE</given-names></name><name><surname>Yamins</surname> <given-names>DLK</given-names></name><name><surname>Shook</surname> <given-names>EN</given-names></name><name><surname>Norman-Haignere</surname> <given-names>SV</given-names></name><name><surname>McDermott</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A Task-Optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title><source>Neuron</source><volume>98</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Khan</surname> <given-names>ME</given-names></name><name><surname>Nielsen</surname> <given-names>D</given-names></name><name><surname>Tangkaratt</surname> <given-names>V</given-names></name><name><surname>Lin</surname> <given-names>W</given-names></name><name><surname>Gal</surname> <given-names>Y</given-names></name><name><surname>Srivastava</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fast and scalable bayesian deep learning by weight-perturbation in adam</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1806.04854">https://arxiv.org/abs/1806.04854</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirkpatrick</surname> <given-names>J</given-names></name><name><surname>Pascanu</surname> <given-names>R</given-names></name><name><surname>Rabinowitz</surname> <given-names>N</given-names></name><name><surname>Veness</surname> <given-names>J</given-names></name><name><surname>Desjardins</surname> <given-names>G</given-names></name><name><surname>Rusu</surname> <given-names>AA</given-names></name><name><surname>Milan</surname> <given-names>K</given-names></name><name><surname>Quan</surname> <given-names>J</given-names></name><name><surname>Ramalho</surname> <given-names>T</given-names></name><name><surname>Grabska-Barwinska</surname> <given-names>A</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name><name><surname>Clopath</surname> <given-names>C</given-names></name><name><surname>Kumaran</surname> <given-names>D</given-names></name><name><surname>Hadsell</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Overcoming catastrophic forgetting in neural networks</article-title><source>PNAS</source><volume>114</volume><fpage>3521</fpage><lpage>3526</lpage><pub-id pub-id-type="doi">10.1073/pnas.1611835114</pub-id><pub-id pub-id-type="pmid">28292907</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep neural networks: a new framework for modeling biological vision and brain information processing</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>417</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035447</pub-id><pub-id pub-id-type="pmid">28532370</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumaran</surname> <given-names>D</given-names></name><name><surname>Hassabis</surname> <given-names>D</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What learning systems do intelligent agents need? complementary learning systems theory updated</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>512</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.05.004</pub-id><pub-id pub-id-type="pmid">27315762</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkman</surname> <given-names>A</given-names></name><name><surname>Hannay</surname> <given-names>T</given-names></name><name><surname>Stratford</surname> <given-names>K</given-names></name><name><surname>Jack</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Presynaptic release probability influences the locus of long-term potentiation</article-title><source>Nature</source><volume>360</volume><fpage>70</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1038/360070a0</pub-id><pub-id pub-id-type="pmid">1331808</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>LeCun</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="1998">1998</year><data-title>The Mnist Database of Handwritten Digits</data-title><publisher-name>New York University</publisher-name><ext-link ext-link-type="uri" xlink:href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>D-H</given-names></name><name><surname>Zhang</surname> <given-names>S</given-names></name><name><surname>Fischer</surname> <given-names>A</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Difference Target Propagation</article-title><conf-name>Machine Learning and Knowledge Discovery in Databases</conf-name><conf-loc>Porto, Portugal</conf-loc><fpage>498</fpage><lpage>515</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-23528-8_31</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lees</surname> <given-names>RM</given-names></name><name><surname>Johnson</surname> <given-names>JD</given-names></name><name><surname>Ashby</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Presynaptic boutons that contain mitochondria are more stable</article-title><source>Frontiers in Synaptic Neuroscience</source><volume>11</volume><elocation-id>37</elocation-id><pub-id pub-id-type="doi">10.3389/fnsyn.2019.00037</pub-id><pub-id pub-id-type="pmid">31998110</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname> <given-names>WB</given-names></name><name><surname>Baxter</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Energy efficient neural codes</article-title><source>Neural Computation</source><volume>8</volume><fpage>531</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1162/neco.1996.8.3.531</pub-id><pub-id pub-id-type="pmid">8868566</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname> <given-names>WB</given-names></name><name><surname>Baxter</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Energy-efficient neuronal computation via quantal synaptic failures</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>4746</fpage><lpage>4755</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-11-04746.2002</pub-id><pub-id pub-id-type="pmid">12040082</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname> <given-names>TP</given-names></name><name><surname>Cownden</surname> <given-names>D</given-names></name><name><surname>Tweed</surname> <given-names>DB</given-names></name><name><surname>Akerman</surname> <given-names>CJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Random synaptic feedback weights support error backpropagation for deep learning</article-title><source>Nature Communications</source><volume>7</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/ncomms13276</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname> <given-names>J</given-names></name><name><surname>Raghavachari</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A unified model of the presynaptic and postsynaptic changes during LTP at CA1 synapses</article-title><source>Science's STKE</source><volume>2006</volume><elocation-id>re11</elocation-id><pub-id pub-id-type="doi">10.1126/stke.3562006re11</pub-id><pub-id pub-id-type="pmid">17033044</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Llera-Montero</surname> <given-names>M</given-names></name><name><surname>Sacramento</surname> <given-names>J</given-names></name><name><surname>Costa</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Computational roles of plastic probabilistic synapses</article-title><source>Current Opinion in Neurobiology</source><volume>54</volume><fpage>90</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.09.002</pub-id><pub-id pub-id-type="pmid">30308457</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lodge</surname> <given-names>M</given-names></name><name><surname>Bischofberger</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Synaptic properties of newly generated granule cells support sparse coding in the adult hippocampus</article-title><source>Behavioural Brain Research</source><volume>372</volume><elocation-id>112036</elocation-id><pub-id pub-id-type="doi">10.1016/j.bbr.2019.112036</pub-id><pub-id pub-id-type="pmid">31201871</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Beck</surname> <given-names>JM</given-names></name><name><surname>Latham</surname> <given-names>PE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Bayesian inference with probabilistic population codes</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1432</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1038/nn1790</pub-id><pub-id pub-id-type="pmid">17057707</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Martens</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>New insights and perspectives on the natural gradient method</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.1193">https://arxiv.org/abs/1412.1193</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattar</surname> <given-names>MG</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Prioritized memory access explains planning and hippocampal replay</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1609</fpage><lpage>1617</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0232-z</pub-id><pub-id pub-id-type="pmid">30349103</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>O'Reilly</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title><source>Psychological Review</source><volume>102</volume><fpage>419</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.102.3.419</pub-id><pub-id pub-id-type="pmid">7624455</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moczulska</surname> <given-names>KE</given-names></name><name><surname>Tinter-Thiede</surname> <given-names>J</given-names></name><name><surname>Peter</surname> <given-names>M</given-names></name><name><surname>Ushakova</surname> <given-names>L</given-names></name><name><surname>Wernle</surname> <given-names>T</given-names></name><name><surname>Bathellier</surname> <given-names>B</given-names></name><name><surname>Rumpel</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamics of dendritic spines in the mouse auditory cortex during memory formation and memory recall</article-title><source>PNAS</source><volume>110</volume><fpage>18315</fpage><lpage>18320</lpage><pub-id pub-id-type="doi">10.1073/pnas.1312508110</pub-id><pub-id pub-id-type="pmid">24151334</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monday</surname> <given-names>HR</given-names></name><name><surname>Younts</surname> <given-names>TJ</given-names></name><name><surname>Castillo</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Long-Term plasticity of neurotransmitter release: emerging mechanisms and contributions to brain function and disease</article-title><source>Annual Review of Neuroscience</source><volume>41</volume><fpage>299</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-080317-062155</pub-id><pub-id pub-id-type="pmid">29709205</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarrete</surname> <given-names>A</given-names></name><name><surname>van Schaik</surname> <given-names>CP</given-names></name><name><surname>Isler</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Energetics and the evolution of human brain size</article-title><source>Nature</source><volume>480</volume><fpage>91</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1038/nature10629</pub-id><pub-id pub-id-type="pmid">22080949</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niven</surname> <given-names>JE</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Energy limitation as a selective pressure on the evolution of sensory systems</article-title><source>Journal of Experimental Biology</source><volume>211</volume><fpage>1792</fpage><lpage>1804</lpage><pub-id pub-id-type="doi">10.1242/jeb.017574</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obashi</surname> <given-names>K</given-names></name><name><surname>Okabe</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Regulation of mitochondrial dynamics and distribution by synapse position and neuronal activity in the axon</article-title><source>European Journal of Neuroscience</source><volume>38</volume><fpage>2350</fpage><lpage>2363</lpage><pub-id pub-id-type="doi">10.1111/ejn.12263</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname> <given-names>BA</given-names></name><name><surname>Field</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Sparse coding of sensory inputs</article-title><source>Current Opinion in Neurobiology</source><volume>14</volume><fpage>481</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2004.07.007</pub-id><pub-id pub-id-type="pmid">15321069</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padamsey</surname> <given-names>Z</given-names></name><name><surname>Emptage</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Two sides to long-term potentiation: a view towards reconciliation</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><elocation-id>20130154</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0154</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pascanu</surname> <given-names>R</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Revisiting natural gradient for deep networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1301.3584">https://arxiv.org/abs/1301.3584</ext-link></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perez-Orive</surname> <given-names>J</given-names></name><name><surname>Mazor</surname> <given-names>O</given-names></name><name><surname>Turner</surname> <given-names>GC</given-names></name><name><surname>Cassenaer</surname> <given-names>S</given-names></name><name><surname>Wilson</surname> <given-names>RI</given-names></name><name><surname>Laurent</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Oscillations and sparsening of odor representations in the mushroom body</article-title><source>Science</source><volume>297</volume><fpage>359</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1126/science.1070502</pub-id><pub-id pub-id-type="pmid">12130775</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perge</surname> <given-names>JA</given-names></name><name><surname>Koch</surname> <given-names>K</given-names></name><name><surname>Miller</surname> <given-names>R</given-names></name><name><surname>Sterling</surname> <given-names>P</given-names></name><name><surname>Balasubramanian</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How the optic nerve allocates space, energy capacity, and information</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>7917</fpage><lpage>7928</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5200-08.2009</pub-id><pub-id pub-id-type="pmid">19535603</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname> <given-names>RQ</given-names></name><name><surname>Reddy</surname> <given-names>L</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Fried</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Invariant visual representation by single neurons in the human brain</article-title><source>Nature</source><volume>435</volume><fpage>1102</fpage><lpage>1107</lpage><pub-id pub-id-type="doi">10.1038/nature03687</pub-id><pub-id pub-id-type="pmid">15973409</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblatt</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="1958">1958</year><article-title>The perceptron: a probabilistic model for information storage and organization in the brain</article-title><source>Psychological Review</source><volume>65</volume><fpage>386</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1037/h0042519</pub-id><pub-id pub-id-type="pmid">13602029</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roxin</surname> <given-names>A</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Efficient partitioning of memory systems and its importance for memory consolidation</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003146</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003146</pub-id><pub-id pub-id-type="pmid">23935470</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sacramento</surname> <given-names>J</given-names></name><name><surname>Costa</surname> <given-names>RP</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Senn</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dendritic cortical microcircuits approximate the backpropagation algorithm</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>8721</fpage><lpage>8732</lpage></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakamoto</surname> <given-names>H</given-names></name><name><surname>Ariyoshi</surname> <given-names>T</given-names></name><name><surname>Kimpara</surname> <given-names>N</given-names></name><name><surname>Sugao</surname> <given-names>K</given-names></name><name><surname>Taiko</surname> <given-names>I</given-names></name><name><surname>Takikawa</surname> <given-names>K</given-names></name><name><surname>Asanuma</surname> <given-names>D</given-names></name><name><surname>Namiki</surname> <given-names>S</given-names></name><name><surname>Hirose</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synaptic weight set by Munc13-1 supramolecular assemblies</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>41</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0041-9</pub-id><pub-id pub-id-type="pmid">29230050</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savtchenko</surname> <given-names>LP</given-names></name><name><surname>Sylantyev</surname> <given-names>S</given-names></name><name><surname>Rusakov</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Central synapses release a resource-efficient amount of glutamate</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>10</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/nn.3285</pub-id><pub-id pub-id-type="pmid">23242311</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Schug</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Presynaptic Stochasticity</data-title><source>Software Heritage</source><version designator="swh:1:rev:de0851773cd1375b885dcdb18e711a2fb6eb06a4">swh:1:rev:de0851773cd1375b885dcdb18e711a2fb6eb06a4</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7d155b25f3b63cb8c296b575c2cbefd2d94f39a0;origin=https://github.com/smonsays/presynaptic-stochasticity;visit=swh:1:snp:21724e4a17b4d4e991980958593e08c91288b836;anchor=swh:1:rev:de0851773cd1375b885dcdb18e711a2fb6eb06a4">https://archive.softwareheritage.org/swh:1:dir:7d155b25f3b63cb8c296b575c2cbefd2d94f39a0;origin=https://github.com/smonsays/presynaptic-stochasticity;visit=swh:1:snp:21724e4a17b4d4e991980958593e08c91288b836;anchor=swh:1:rev:de0851773cd1375b885dcdb18e711a2fb6eb06a4</ext-link></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sengupta</surname> <given-names>B</given-names></name><name><surname>Stemmler</surname> <given-names>M</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name><name><surname>Niven</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Action potential energy efficiency varies among neuron types in vertebrates and invertebrates</article-title><source>PLO Computational Biology</source><volume>6</volume><elocation-id>e1000840</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000840</pub-id><pub-id pub-id-type="pmid">20617202</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sengupta</surname> <given-names>B</given-names></name><name><surname>Laughlin</surname> <given-names>SB</given-names></name><name><surname>Niven</surname> <given-names>JE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Balanced excitatory and inhibitory synaptic currents promote efficient coding and metabolic efficiency</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003263</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003263</pub-id><pub-id pub-id-type="pmid">24098105</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>A mathematical theory of communication</article-title><source>Bell System Technical Journal</source><volume>27</volume><fpage>379</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname> <given-names>PJ</given-names></name><name><surname>Turrigiano</surname> <given-names>GG</given-names></name><name><surname>Nelson</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title><source>Neuron</source><volume>32</volume><fpage>1149</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00542-6</pub-id><pub-id pub-id-type="pmid">11754844</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname> <given-names>PJ</given-names></name><name><surname>Turrigiano</surname> <given-names>GG</given-names></name><name><surname>Nelson</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Multiple forms of long-term plasticity at unitary neocortical layer 5 synapses</article-title><source>Neuropharmacology</source><volume>52</volume><fpage>176</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.neuropharm.2006.07.021</pub-id><pub-id pub-id-type="pmid">16895733</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname> <given-names>T</given-names></name><name><surname>Qiao</surname> <given-names>H</given-names></name><name><surname>Pan</surname> <given-names>PY</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Sheng</surname> <given-names>ZH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Motile axonal mitochondria contribute to the variability of presynaptic strength</article-title><source>Cell Reports</source><volume>4</volume><fpage>413</fpage><lpage>419</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2013.06.040</pub-id><pub-id pub-id-type="pmid">23891000</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname> <given-names>LI</given-names></name><name><surname>Zeiler</surname> <given-names>M</given-names></name><name><surname>Le</surname> <given-names>ZS</given-names></name><name><surname>Cun</surname> <given-names>Y</given-names></name><name><surname>Fergus</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Regularization of neural networks using dropconnect</article-title><source>Neural Networks</source><volume>110</volume><fpage>82</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2018.09.009</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wixted</surname> <given-names>JT</given-names></name><name><surname>Squire</surname> <given-names>LR</given-names></name><name><surname>Jang</surname> <given-names>Y</given-names></name><name><surname>Papesh</surname> <given-names>MH</given-names></name><name><surname>Goldinger</surname> <given-names>SD</given-names></name><name><surname>Kuhn</surname> <given-names>JR</given-names></name><name><surname>Smith</surname> <given-names>KA</given-names></name><name><surname>Treiman</surname> <given-names>DM</given-names></name><name><surname>Steinmetz</surname> <given-names>PN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sparse and distributed coding of episodic memory in neurons of the human hippocampus</article-title><source>PNAS</source><volume>111</volume><fpage>9621</fpage><lpage>9626</lpage><pub-id pub-id-type="doi">10.1073/pnas.1408365111</pub-id><pub-id pub-id-type="pmid">24979802</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xiao</surname> <given-names>H</given-names></name><name><surname>Rasul</surname> <given-names>K</given-names></name><name><surname>Vollgraf</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fashion-Mnist: a novel image dataset for benchmarking machine learning algorithms</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1708.07747">https://arxiv.org/abs/1708.07747</ext-link></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname> <given-names>DL</given-names></name><name><surname>DiCarlo</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>G</given-names></name><name><surname>Pan</surname> <given-names>F</given-names></name><name><surname>Gan</surname> <given-names>WB</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Stably maintained dendritic spines are associated with lifelong memories</article-title><source>Nature</source><volume>462</volume><fpage>920</fpage><lpage>924</lpage><pub-id pub-id-type="doi">10.1038/nature08577</pub-id><pub-id pub-id-type="pmid">19946265</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname> <given-names>Y</given-names></name><name><surname>Calakos</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Presynaptic long-term plasticity</article-title><source>Frontiers in Synaptic Neuroscience</source><volume>5</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.3389/fnsyn.2013.00008</pub-id><pub-id pub-id-type="pmid">24146648</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zeno</surname> <given-names>C</given-names></name><name><surname>Golan</surname> <given-names>I</given-names></name><name><surname>Hoffer</surname> <given-names>E</given-names></name><name><surname>Soudry</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Task agnostic continual learning using online variational bayes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.10123">https://arxiv.org/abs/1803.10123</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Additional results on energy efficiency of model with stochastic and plastic release.</title><p>(<bold>a</bold>) Mutual information per energy analogous to <xref ref-type="fig" rid="fig1">Figure 1b</xref>, but showing results for different regularisation strengths rather than the best result for each model. As described in the main part, energy is measured via its synaptic contribution. (<bold>b</bold>) Same experiment as in (<bold>a</bold>) but energy is measured as the metabolic cost incurred by the activity of neurons by calculating their average rate of activity. (<bold>c</bold>) Maximum mutual information per energy for a multilayer perceptron with fixed release probability and constant regularisation strength of 0.01. This is the same model as ‘Stochastic Release (Fixed)’ in (<bold>a</bold>), but for a range of different values for the release probability. This is in line with the single synapse analysis in <xref ref-type="bibr" rid="bib35">Harris et al., 2012</xref>. For each model, we searched over different learning rates and report the best result. (<bold>d</bold>) Analogous to <xref ref-type="fig" rid="fig2">Figure 2a</xref>, but release probabilities were initialised independently, uniformly at random in the interval <inline-formula><mml:math id="inf143"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.25</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> rather than with a fixed value of 0.25. Error bars in (<bold>a</bold>) and (<bold>b</bold>) denote the standard error for three repetitions of the experiment. (<bold>c</bold>) shows the best performing model for each release probability after a grid search over the learning rate. (<bold>d</bold>) shows aggregated data over three repetitions of the experiment.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69884-app1-fig1-v2.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Additional results on neuron-level sparsity of network after learning.</title><p>(<bold>a</bold>) Number of important synapses per neuron for all layers after learning on MNIST. The <inline-formula><mml:math id="inf144"><mml:mi>i</mml:mi></mml:math></inline-formula>-th row shows data from the <inline-formula><mml:math id="inf145"><mml:mi>i</mml:mi></mml:math></inline-formula>-th weight matrix of the network and we compare true connectivity to random connectivity. Two-sample Kolmogorov-Smirnov tests comparing the distribution of important synapses in the shuffled and unaltered condition are significant for all layers (<inline-formula><mml:math id="inf146"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>) except for the output neurons in the last layer (lower-left panel) (<inline-formula><mml:math id="inf147"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.41</mml:mn></mml:mrow></mml:math></inline-formula>). This is to be expected as all 10 output neurons in the last layer should be equally active and thus receive similar numbers of active inputs. (<bold>b</bold>) Scatter plot showing the number of important input and output synapses per neuron for both hidden layers after learning on MNIST. First hidden layer (left) has a Pearson correlation coefficient of <inline-formula><mml:math id="inf148"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.9390</mml:mn></mml:mrow></mml:math></inline-formula>. Second hidden layer (right) has a Pearson correlation coefficient of <inline-formula><mml:math id="inf149"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7999</mml:mn></mml:mrow></mml:math></inline-formula>. Data is from one run of the experiment.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69884-app1-fig2-v2.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Additional results on lifelong learning in a model with presynaptically driven consolidation.</title><p>(<bold>a</bold>) Detailed lifelong-learning results of various methods on Split MNIST, same underlying experiment as in <xref ref-type="fig" rid="fig4">Figure 4c</xref>. We report the test accuracy on each task of the final model (after learning all tasks). Error bars denote the standard error for three repetitions of the experiment. (<bold>b</bold>) Mean release probability and percentage of frozen weights over the course of learning ten permuted MNIST tasks. Error bars in (<bold>a</bold>) and shaded regions in (<bold>b</bold>) show standard error over three repetitions of the experiment.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69884-app1-fig3-v2.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Biological evidence for stability of synapses with high release probability.</title><p>To test whether synapses with high release probability are more stable than synapses with low release probability as prescribed by our model, we re-analysed data of <xref ref-type="bibr" rid="bib85">Sjöström et al., 2001</xref> from a set of spike-timing-dependent plasticity protocols. The protocols induce both LTP and LTD depending on their precise timing. The figure shows that synapses with higher release probabilities undergo smaller relative changes in expected strength (Pearson Corr. <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.4416</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>). This suggests that synapses with high release probability are more stable than synapses with low release probability, matching our learning rule.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-69884-app1-fig4-v2.tif"/></fig><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Lifelong learning comparison on additional datasets.</title><p>Average test accuracies (higher is better, average over all sequentially presented tasks) and standard errors for three repetitions of each experiment on four different lifelong learning tasks for the Presynaptic Consolidation mechanism, Bayesian Gradient Descent (BGD) (<xref ref-type="bibr" rid="bib94">Zeno et al., 2018</xref>) and EWC (<xref ref-type="bibr" rid="bib48">Kirkpatrick et al., 2017</xref>). For the control ‘Joint Training’, the network is trained on all tasks simultaneously serving as an upper bound of practically achievable performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Split MNIST</th><th>Split fashion</th><th>Perm. MNIST</th><th>Perm. fashion</th></tr></thead><tbody><tr><td>Presynaptic Consolidation</td><td><inline-formula><mml:math id="inf152"><mml:msup><mml:mn>82.90</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf153"><mml:msup><mml:mn>91.98</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.12</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf154"><mml:msup><mml:mn>86.14</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.67</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf155"><mml:msup><mml:mn>75.92</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.37</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td></tr><tr><td>No Consolidation</td><td><inline-formula><mml:math id="inf156"><mml:msup><mml:mn>77.68</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.31</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf157"><mml:msup><mml:mn>88.76</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.45</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf158"><mml:msup><mml:mn>79.60</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.43</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf159"><mml:msup><mml:mn>72.13</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td></tr><tr><td>Bayesian Gradient Descent</td><td><inline-formula><mml:math id="inf160"><mml:msup><mml:mn>80.44</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.45</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf161"><mml:msup><mml:mn>89.54</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.88</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf162"><mml:msup><mml:mn>89.73</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.52</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf163"><mml:msup><mml:mn>78.45</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td></tr><tr><td>Elastic Weight Consolidation</td><td><inline-formula><mml:math id="inf164"><mml:msup><mml:mn>70.41</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>4.20</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf165"><mml:msup><mml:mn>76.89</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>1.05</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf166"><mml:msup><mml:mn>89.58</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.53</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf167"><mml:msup><mml:mn>77.44</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.41</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td></tr><tr><td>Joint Training</td><td><inline-formula><mml:math id="inf168"><mml:msup><mml:mn>98.55</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.10</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf169"><mml:msup><mml:mn>97.67</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf170"><mml:msup><mml:mn>97.33</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf171"><mml:msup><mml:mn>87.33</mml:mn><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.07</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td></tr></tbody></table></table-wrap></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69884.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>O'Leary</surname><given-names>Timothy</given-names></name><role>Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Pfister</surname><given-names>Jean-Pascal</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.05.05.442708">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.05.05.442708v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>In many nervous systems such as mammalian cortex excitatory synapses are stochastic and the probability of release of neurotransmitter can be modulated by plasticity and neural activity. This paper presents a simple biologically plausible mechanism that regulates the probability of release during learning. Using network simulations the authors show that this can result in more energy efficient processing of learned stimuli by enhancing the reliability of important connections, with lower expected rates of transmission at less important synapses.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Presynaptic Stochasticity Improves Energy Efficiency and Alleviates the Stability-Plasticity Dilemma&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Jean-Pascal Pfister (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Plasticity in p(release) is a nice idea whose simplicity suggests that it is biologically plausible. However, the clear gap in the study is relating the proposed model to the wealth of data on how LTP/D mechanism affect release probabilities. The authors should do more work to survey these mechanisms and, if possible, show how they might relate to the proposed plasticity mechanism or something similar.</p><p>2) The generic nature of the networks and simulations is fine but it would be nice in the discussion to suggest what kinds of networks one might expect such a mechanism to be at work.</p><p>3) The claim that this mechanism fully alleviates the stability/plasticity dilemma should be tempered: it can impact the tradeoff in specific cases and under broad assumptions may improve energy efficiency. The text should be revised appropriately.</p><p>4) Please include a response to the reviewers' comments below with the resubmission and address the specific points raised.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1. There are several freely available datasets on synaptic plasticity (the ones from Sjostrom is just one example). I would encourage the authors to test their novel learning rule on existing plasticity data sets.</p><p>2. I am surprised that the nice work of Fusi is not mentioned here. The cascade model of Fusi precisely proposes a multi-state model for the synapse in order to alleviate the stability-plasticity dilemma. From this perspective, the model proposed by the authors could be seen as a special case of the cascade model.</p><p>3. On Figure 2b, I would add an additional control. What happens if the lesioning targets first the lowest expected synaptic strength? Will it be closer to the blue line (Random Order) or the pink line (lowest probability first)?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69884.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Plasticity in p(release) is a nice idea whose simplicity suggests that it is biologically plausible. However, the clear gap in the study is relating the proposed model to the wealth of data on how LTP/D mechanism affect release probabilities. The authors should do more work to survey these mechanisms and, if possible, show how they might relate to the proposed plasticity mechanism or something similar.</p></disp-quote><p>We made two changes to relate the model to biological data on presynaptic LTP/D mechanisms. Firstly, in the discussion, we more thoroughly review how individual components of our model relate to known plasticity mechanisms, see subsection “Presynaptic Long-Term Plasticity”. Secondly, we analyse a synaptic plasticity dataset by Sjöström et al., showing some evidence for our stabilisation mechanism, see subsection “Link between Presynaptic Release and Stability”, where we also discuss additional qualitative evidence.</p><disp-quote content-type="editor-comment"><p>2) The generic nature of the networks and simulations is fine but it would be nice in the discussion to suggest what kinds of networks one might expect such a mechanism to be at work.</p></disp-quote><p>We expanded the discussion, explaining where and why the proposed mechanisms could contribute to both energy-efficient processing and lifelong learning, see subsection “Correspondence to Biological Networks”.</p><disp-quote content-type="editor-comment"><p>3) The claim that this mechanism fully alleviates the stability/plasticity dilemma should be tempered: it can impact the tradeoff in specific cases and under broad assumptions may improve energy efficiency. The text should be revised appropriately.</p></disp-quote><p>We updated parts of the title and abstract and went over the remainder of the manuscript to make sure that the claims are tempered.</p><disp-quote content-type="editor-comment"><p>4) Please include a response to the reviewers' comments below with the resubmission and address the specific points raised.</p></disp-quote><p>See detailed comments below.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1. There are several freely available datasets on synaptic plasticity (the ones from Sjostrom is just one example). I would encourage the authors to test their novel learning rule on existing plasticity data sets.</p></disp-quote><p>We added an analysis of a synaptic plasticity dataset by Sjöström et al., (2001), see discussion subsection “Link between Presynaptic Release and Stability” and Figure 9. The analysis is consistent with a key property of our model, namely that synapses with high release probability are more stable than ones with low release probability.</p><disp-quote content-type="editor-comment"><p>2. I am surprised that the nice work of Fusi is not mentioned here. The cascade model of Fusi precisely proposes a multi-state model for the synapse in order to alleviate the stability-plasticity dilemma. From this perspective, the model proposed by the authors could be seen as a special case of the cascade model.</p></disp-quote><p>Thanks for pointing out this oversight, we agree that this is relevant work. We updated the discussion accordingly, see subsection “Related Synapse Models”.</p><disp-quote content-type="editor-comment"><p>3. On Figure 2b, I would add an additional control. What happens if the lesioning targets first the lowest expected synaptic strength? Will it be closer to the blue line (Random Order) or the pink line (lowest probability first)?</p></disp-quote><p>We added this control. Keeping “high energy (expected strength)” synapses is even more important for the network than keeping “high release probability synapses”. Nevertheless, the claim that release probabilities are related to importance holds true, and our analysis also shows that release probabilities are more closely related to the Fisher information (which is a more local measure than the lesion experiment) than to the expected strength.</p></body></sub-article></article>