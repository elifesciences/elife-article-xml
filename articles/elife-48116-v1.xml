<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">48116</article-id><article-id pub-id-type="doi">10.7554/eLife.48116</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Advance</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>The visual speech head start improves perception and reduces superior temporal cortex responses to auditory speech</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-140165"><name><surname>Karas</surname><given-names>Patrick J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2605-8820</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140166"><name><surname>Magnotti</surname><given-names>John F</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2093-0603</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140167"><name><surname>Metzger</surname><given-names>Brian A</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140168"><name><surname>Zhu</surname><given-names>Lin L</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-140170"><name><surname>Smith</surname><given-names>Kristen B</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-93527"><name><surname>Yoshor</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-33719"><name><surname>Beauchamp</surname><given-names>Michael S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7599-9934</contrib-id><email>michael.beauchamp@bcm.edu</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Neurosurgery</institution><institution>Baylor College of Medicine</institution><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Pasternak</surname><given-names>Tatiana</given-names></name><role>Reviewing Editor</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>08</day><month>08</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e48116</elocation-id><history><date date-type="received" iso-8601-date="2019-05-02"><day>02</day><month>05</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-07-17"><day>17</day><month>07</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Karas et al</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Karas et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-48116-v1.pdf"/><related-article ext-link-type="doi" id="ra1" related-article-type="article-reference" xlink:href="10.7554/eLife.30387"/><abstract><object-id pub-id-type="doi">10.7554/eLife.48116.001</object-id><p>Visual information about speech content from the talker’s mouth is often available before auditory information from the talker's voice. Here we examined perceptual and neural responses to words with and without this visual head start. For both types of words, perception was enhanced by viewing the talker's face, but the enhancement was significantly greater for words with a head start. Neural responses were measured from electrodes implanted over auditory association cortex in the posterior superior temporal gyrus (pSTG) of epileptic patients. The presence of visual speech suppressed responses to auditory speech, more so for words with a visual head start. We suggest that the head start inhibits representations of incompatible auditory phonemes, increasing perceptual accuracy and decreasing total neural responses. Together with previous work showing visual cortex modulation (Ozker et al., 2018b) these results from pSTG demonstrate that multisensory interactions are a powerful modulator of activity throughout the speech perception network.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>ECoG iEEG</kwd><kwd>posterior superior temporal gyrus pSTG</kwd><kwd>speech</kwd><kwd>audiovisual integration</kwd><kwd>multisensory integration</kwd><kwd>cross-modal suppression</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01NS065395</award-id><principal-award-recipient><name><surname>Beauchamp</surname><given-names>Michael S</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R25NS070694</award-id><principal-award-recipient><name><surname>Karas</surname><given-names>Patrick J</given-names></name><name><surname>Yoshor</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R24MH117529</award-id><principal-award-recipient><name><surname>Beauchamp</surname><given-names>Michael S</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>U01NS098976</award-id><principal-award-recipient><name><surname>Beauchamp</surname><given-names>Michael S</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>National Institute on Deafness and Other Communication Disorders</institution></institution-wrap></funding-source><award-id>F30DC014911</award-id><principal-award-recipient><name><surname>Zhu</surname><given-names>Lin L</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Human perception and brain responses differ between words, in which mouth movements are visible before the voice is heard, and words, for which the reverse is true.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Pairing noisy auditory speech with a video of the talker dramatically improves perception (<xref ref-type="bibr" rid="bib5">Bernstein et al., 2004</xref>; <xref ref-type="bibr" rid="bib23">Grant and Seitz, 2000</xref>; <xref ref-type="bibr" rid="bib42">Munhall et al., 2004</xref>; <xref ref-type="bibr" rid="bib52">Ross et al., 2007</xref>; <xref ref-type="bibr" rid="bib61">Sumby and Pollack, 1954</xref>). Visual speech can provide a perceptual benefit because any particular mouth movement made by the talker is compatible with only a few auditory phonemes (<xref ref-type="bibr" rid="bib11">Cappelletta and Harte, 2012</xref>; <xref ref-type="bibr" rid="bib29">Jeffers and Barley, 1971</xref>; <xref ref-type="bibr" rid="bib43">Neti et al., 2000</xref>). For instance, viewing a rounded open-mouth shape allows the observer to rule out the ~80% of phonemes incompatible with this mouth shape (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Bayesian inference provides a plausible algorithm for how the observer combines the visual mouth shape and auditory phoneme information (<xref ref-type="bibr" rid="bib34">Ma et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Magnotti and Beauchamp, 2017</xref>).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.48116.002</object-id><label>Figure 1.</label><caption><title>Relationship between auditory and visual speech.</title><p>(<bold>A</bold>) Visual speech provides independent information about speech content. Any given visual speech feature, such as the open mouth visible when pronouncing ‘d’, is incompatible with many auditory phonemes (red) and compatible with a few auditory phonemes (green). (<bold>B</bold>) Visual speech provides a processing head start on auditory speech (yellow region) as shown by auditory and visual speech feature asynchrony for the word ‘drive.’ Audiovisual speech is composed of visual mouth movements (green line showing visual mouth area) and auditory speech sounds (purple line showing auditory sound pressure level). Lip and mouth movements (visual speech onset, green bar) occur prior to vocalization (auditory speech onset, purple bar). Time zero is the auditory speech onset. This word is classified as ‘mouth-leading’ as visual mouth movements begin before auditory speech. (<bold>C</bold>) For the word ‘known,’ mouth movements begin after auditory vocalization (green bar comes after purple bar) and there is no visual head start. This word is termed as ‘voice-leading’ because vocalization begins before visible mouth movements.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48116-fig1-v1.tif"/></fig><p>A second potential reason for the perceptual benefit of visual speech has received less attention (<xref ref-type="bibr" rid="bib48">Peelle and Sommers, 2015</xref>): visual speech can provide a head start on perception when mouth movements begin before auditory vocalization. This extra processing time is important because humans must understand speech both quickly (as speech is generated rapidly, at rates of ~5 syllables a second) and accurately (as errors in communication are potentially costly). In order to examine the perceptual and neural consequences of the visual speech head start, we leveraged the natural variability in the temporal relationship between the auditory and visual modalities (<xref ref-type="bibr" rid="bib12">Chandrasekaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib56">Schwartz and Savariaux, 2014</xref>). Although there is a range in the relative onset of auditory and visual speech, most audiovisual words provide a visual head start. We term these words ‘mouth-leading’, as the visual information provided by the talker’s mouth is available before auditory information from the talker’s voice. For instance, in an audiovisual recording of the word ‘drive’ (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) the visual onset of the open mouth required to enunciate the initial ‘d’ of the word preceded auditory vocalization by 400 ms, allowing the observer to rule out incompatible auditory phonemes (and rule in compatible phonemes) well before any auditory speech information is available. We term words that do not provide a visual head start as ‘voice-leading:’ auditory information from the talker's voice is available before visual information provided by the talker's mouth. For instance, in an audiovisual recording of the word ‘known’, auditory voice onset occurred 100 ms before visible changes in the talker's mouth (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). If the visual head start contributes to the perceptual benefits of visual speech, these benefits should be greater for mouth-leading words than for voice-leading words.</p><p>Any influence of the visual head start on speech perception should be reflected in measurements of the activity of the neural circuits important for speech perception. Studies of patients with cortical lesions (<xref ref-type="bibr" rid="bib26">Hickok et al., 2018</xref>; <xref ref-type="bibr" rid="bib59">Stasenko et al., 2015</xref>) and fMRI, MEG, EEG, and electrocorticographic (intracranial EEG) studies have implicated a network of brain areas in occipital, temporal, frontal, and parietal cortex (<xref ref-type="bibr" rid="bib15">Crosse et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Hickok and Poeppel, 2015</xref>; <xref ref-type="bibr" rid="bib44">Okada et al., 2013</xref>; <xref ref-type="bibr" rid="bib53">Salmelin, 2007</xref>; <xref ref-type="bibr" rid="bib57">Shahin et al., 2018</xref>; <xref ref-type="bibr" rid="bib58">Sohoglu and Davis, 2016</xref>; <xref ref-type="bibr" rid="bib62">van Wassenhove et al., 2005</xref>). Within this network, posterior superior temporal gyrus and sulcus (pSTG) are responsive to both unisensory auditory speech (<xref ref-type="bibr" rid="bib4">Belin et al., 2000</xref>; <xref ref-type="bibr" rid="bib21">Formisano et al., 2008</xref>; <xref ref-type="bibr" rid="bib41">Mesgarani et al., 2014</xref>) and unisensory visual speech (<xref ref-type="bibr" rid="bib7">Bernstein et al., 2011</xref>; <xref ref-type="bibr" rid="bib8">Bernstein and Liebenthal, 2014</xref>), with subregions responsive to both auditory and visual speech (<xref ref-type="bibr" rid="bib3">Beauchamp et al., 2004</xref>; <xref ref-type="bibr" rid="bib45">Ozker et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Ozker et al., 2018a</xref>; <xref ref-type="bibr" rid="bib51">Rennig et al., 2018</xref>; <xref ref-type="bibr" rid="bib64">Zhu and Beauchamp, 2017</xref>). We examined the neural differences between the processing of mouth-leading and voice-leading speech in pSTG using intracranial EEG. This technique has the advantage of both high spatial resolution, necessary to measure activity from focal areas within the pSTG, and high temporal resolution, required to capture the small auditory-visual asynchrony differences between mouth-leading and voice-leading words.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Perceptual results</title><p>In the perceptual experiments, participants identified auditory-only and audiovisual words with and without added auditory noise. Perception was very accurate for words without added auditory noise. Adding noise reduced perceptual accuracy below ceiling, revealing differences between conditions. To evaluate these differences, we constructed a generalized linear mixed-effects model (GLMM) with fixed effects of word format (noisy auditory-only <italic>vs</italic>. noisy audiovisual) and word type (mouth-leading <italic>vs.</italic> voice-leading) and their interaction. All models included random effects for participant; exemplar was included as a random effect for the second experiment because of the larger number of stimuli.</p><p>In the first perceptual experiment (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), 40 participants were presented with sixteen different stimuli consisting of four stimulus exemplars (two mouth-leading words and two voice-leading words) in each of the four formats (clear auditory, noisy auditory, clear audiovisual, noisy audiovisual). For mouth-leading words, viewing the face of the talker increased the intelligibility of noisy auditory speech by 23%, from 3% for auditory-only words to 26% for audiovisual words (odds-ratio 0.00 for auditory <italic>vs</italic>. 0.07 for audiovisual; p&lt;10<sup>−16</sup>). For voice-leading words, viewing the face of the talker provided only a 10% accuracy increase, from 14% to 24% (odds-ratio 0.03 vs. 0.09, p=10<sup>−9</sup>). The interaction between format and word type was significant (p=10<sup>−14</sup>). In addition to the interaction, there were significant main effects of format (p&lt;10<sup>−16</sup>) and word type (p=10<sup>−15</sup>) driven by higher accuracy for audiovisual words and for voice-leading words.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.48116.003</object-id><label>Figure 2.</label><caption><title>Perceptual performance on speech-in-noise recognition tasks.</title><p>(<bold>A</bold>) Results of the first perceptual experiment on two mouth-leading words and two voice-leading words. For the mouth-leading words (left plot), the addition of visual speech increased comprehension of noisy auditory words by 23% (small black arrow showing difference between noisy auditory-only speech, orange bar labeled AUD, and noisy audiovisual speech, blue bar labeled AV). In contrast, for voice-leading words (right plot) the addition of visual speech increased accuracy by only 10%. The interaction (difference of the differences) was significant (p=10<sup>−14</sup>). Error bars represent standard error of the mean across subjects. (<bold>B</bold>) Results of the second perceptual experiment on five mouth-leading words and five voice-leading words, all different than those used in the first perceptual experiment. Adding visual speech to noisy auditory speech produced a significantly greater enhancement (p&lt;10<sup>−16</sup>) for mouth-leading words (left plot) than for voice-leading words (right plot).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48116-fig2-v1.tif"/></fig><p>To extend these results to a larger stimulus set, in the second perceptual experiment (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), 46 participants were presented with forty stimuli (different than those used in the first perceptual experiment) consisting of ten stimulus exemplars (five mouth-leading words and five voice-leading words) presented in each of the four formats. For these mouth-leading words, viewing the face of the talker increased the intelligibility of noisy auditory speech by 59%, from 6% for auditory-only words to 65% for audiovisual words (odds-ratio 0.01 <italic>vs</italic>. 2.24, p&lt;10<sup>−16</sup>). For voice-leading words, viewing the face of the talker provided only a 37% accuracy increase, from 23% to 60% (odds-ratio 0.17 <italic>vs</italic>. 1.66, p&lt;10<sup>−16</sup>). The interaction between format and word type was significant (p&lt;10<sup>−16</sup>) driven by the larger benefit of visual speech for mouth-leading words. In addition to the interaction, there was a significant main effect of format (p&lt;10<sup>−16</sup>) and word type (p=0.04) driven by higher accuracy for audiovisual words and for voice-leading words.</p></sec><sec id="s2-2"><title>Neural results</title><p>The perceptual experiments suggest that the visual head start is an important contributor to the benefits of visual speech. To study the neural differences between words with and without a visual head start, in eight epileptic participants we recorded from electrodes implanted bilaterally over the posterior superior temporal gyrus (pSTG) that showed a significant response to auditory-only speech measured as the percent increase in the power of the high-frequency (75 to 150 Hz) electrical activity relative to baseline (<italic>n</italic> = 28 electrodes, locations and auditory-only response magnitude shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>). In contrast to the perceptual studies, where both clear and noisy speech was presented, in the neural experiments only clear speech was presented in order to maximize the size of the neural response. The stimulus exemplars consisted of the two mouth-leading words and the two voice-leading words used in the first perceptual experiment presented in auditory-only, visual-only, and audiovisual formats, resulting in twelve total stimuli.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.48116.004</object-id><label>Figure 3.</label><caption><title>Average broadband high-frequency activity by experimental condition.</title><p>(<bold>A</bold>) The location of 17 left-hemisphere (left panel) and 11 right-hemisphere (right panel) electrodes that met both an anatomical criterion (located over the posterior superior temporal gyrus) and a functional criterion (significant response to auditory-only speech). The color of each electrode shows the significance (corrected for multiple comparisons using p&lt;0.001 Bonferroni-corrected) of each electrode’s response to the auditory-only condition during the period from auditory speech onset to offset. (<bold>B</bold>) For mouth-leading words (left panel), the neural response to auditory-only words (AUD; orange line) was greater than the response to audiovisual words (AV; blue line). For voice-leading words (right panel), the responses were similar. Shaded regions show standard error of the mean across electrodes (<italic>n</italic> = 28) and dashed lines show auditory speech onset (0 s) and offset (0.55 s). (<bold>C</bold>) To quantify the difference between word types, the neural response was averaged within the window defined by auditory speech onset and offset. For mouth-leading words (left panel), the auditory-only format evoked a significantly greater response than the audiovisual format (34% difference, p=10<sup>−5</sup>). For voice-leading words, there was little difference (5%, p=0.41), resulting in a significant interaction between word format and word type (34% <italic>vs.</italic> 5%, p=0.001). Error bars show standard error of the mean across electrodes (<italic>n</italic> = 28).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48116-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48116.005</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Anatomic distribution of multisensory gain by electrode.</title><p>(<bold>A</bold>) The reduction in neural response to audiovisual compared to auditory-only speech for mouth-leading words was measured for each electrode (difference in average BHA to audiovisual stimuli and average BHA to auditory-only stimuli over time window 0 ms to 550 ms). Most electrodes (25 of 28) had a decreased neural response to audiovisual compared to auditory only stimuli. Color on the white-to-blue gradient corresponds to amount of reduction. (<bold>B</bold>) All electrodes from all participants displayed on a template brain (same color scale as A). No consistent organization of multisensory influence was observed.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48116-fig3-figsupp1-v1.tif"/></fig></fig-group><p>As shown in <xref ref-type="fig" rid="fig3">Figure 3B</xref>, the neural response to the single word stimuli in the pSTG began shortly after auditory speech onset at 0 ms, peaked at 180 ms, and returned to baseline after auditory speech offset at 550 ms. To quantify the responses, we entered the mean response across the window from auditory speech onset to offset (0 ms to 550 ms) relative to baseline into a linear mixed-effects model (LME) with fixed effects of word format (auditory-only <italic>vs.</italic> audiovisual), word type (mouth-leading <italic>vs.</italic> voice-leading) and the interaction.</p><p>As shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, the response to audiovisual mouth-leading words was 34% smaller than the response to auditory-only mouth-leading words (118% <italic>vs.</italic> 152%, p=10<sup>−5</sup>). In contrast, the response to audiovisual voice-leading words was not significantly different than the response to auditory-only voice-leading words (129% <italic>vs.</italic> 134%, p=0.4). This interaction between word format and word type was significant (p=0.003). There were also significant main effects of word format (p=10<sup>−6</sup>, driven by larger responses for auditory-only words) and word type (p=0.01, driven by larger responses for mouth-leading words).</p></sec><sec id="s2-3"><title>Multisensory influence of visual speech: Mouth-Leading words</title><p>For the visual speech head start to influence processing in pSTG, the visual speech information must be present in pSTG and it must arrive early enough to exert multisensory influence. To determine if this was the case, we examined the responses to visual-only words. There was a significant positive response to visual-only words, with a mean amplitude of 20% (0 to 200 ms after visual speech onset; significantly greater than baseline, p=10<sup>−7</sup>, single sample <italic>t</italic>-test). The effect was consistent across electrodes, with 27 out of 28 electrodes showing a positive response to visual-only speech, demonstrating that information about visual speech reaches pSTG.</p><p>The earlier that visual information arrives in the pSTG, the more opportunity it has to influence auditory processing. As shown in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, the pSTG response to visual speech features occurred ~100 ms earlier than the response to auditory speech features, sufficient time for substantial multisensory interactions to occur. To further quantify this observation, we calculated the latency of the response (defined as the time of half-maximum response) to visual-only and auditory-only speech in individual electrodes. Responses were aligned to auditory onset (or to the time when auditory onset would have occurred for visual-only stimuli), with the response to visual-only speech occurring a mean of 123 ms earlier than the response to auditory-only speech (paired <italic>t</italic>-test, p=10<sup>−8</sup>). For 25 out of 28 electrodes, the response to visual-only mouth-leading words occurred earlier than the response to auditory-only mouth-leading words.</p><fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.48116.006</object-id><label>Figure 4.</label><caption><title>Influence of Visual Speech on Multisensory Integration.</title><p>(<bold>A</bold>) Responses to the three formats of mouth-leading words (visual-only, gray; auditory-only, orange; audiovisual, blue). Responses were aligned to auditory onset at <italic>t</italic> = 0 (or to the time when auditory onset would have occurred for visual-only stimuli). The left vertical axis contains the scale for visual-only neural responses (0% to 60%), which were much smaller than auditory-only and audiovisual responses (scale given in right-hand vertical axis; 0% to 400%). The visual-only response onset occurred prior to auditory-only response onset, creating a multisensory influence window (yellow region) in which visual speech could influence processing of auditory speech, resulting in a reduced response to audiovisual compared with auditory-only speech. (<bold>B</bold>) The amplitude of the early neural response (BHA) to visual-only stimuli was positively correlated with the difference in the neural response between audiovisual and auditory-only speech (N = 28; <italic>r</italic> = 0.64, p=10<sup>−4</sup>). The early visual-only response (horizontal axis) for each electrode was the average BHA for the 200 ms period following visual speech onset (−100 ms to 100 ms; yellow region in axis inset). The difference between the audiovisual and auditory-only neural response (vertical axis) was calculated as the difference in average BHA during auditory speech (0 ms to 550 ms; red region in axis inset).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48116-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.48116.007</object-id><label>Figure 4—figure supplement 1.</label><caption><title>Influence of Visual Speech for Voice-Leading Words.</title><p>(<bold>A</bold>) Responses to the three formats of voice-leading words (visual-only, gray; auditory-only, orange; audiovisual, blue). Responses were aligned to auditory onset at <italic>t</italic> = 0 (or to the time when auditory onset would have occurred for visual-only stimuli). The left vertical axis contains the scale for visual-only neural responses (0% to 60%), which were much smaller than auditory-only and audiovisual responses (scale given in right-hand vertical axis; 0% to 400%). The neural response to the three word began at similar times. (<bold>B</bold>) Correlation between the amplitude of the early neural response to visual-only words and the difference in the neural response between audiovisual and auditory-only speech. The early visual-only response (horizontal axis) for each electrode was the average BHA for the 200 ms period following visual speech onset (time −100 ms to 100 ms; gray region underneath curve in axis inset). The reduction in neural response to audiovisual <italic>vs.</italic> auditory-only speech (vertical axis) was calculated as the difference in average BHA during the duration of the entire auditory stimulus (0 ms to 550 ms; red region in axis inset).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48116-fig4-figsupp1-v1.tif"/></fig></fig-group><p>If visual and auditory speech processing interacts in pSTG, stronger responses to visual speech might result in more powerful multisensory interactions. To test this idea, we calculated the amplitude of the early neural response to visual-only mouth-leading words (0 to 200 ms following the onset of the first visual mouth movement) and compared it with the multisensory effect, defined as amplitude of the reduction for audiovisual <italic>vs.</italic> auditory-only mouth-leading words. As shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>, there was a significant positive relationship across electrodes between early visual response and multisensory influence (r = 0.64, p=10<sup>−4</sup>).</p></sec><sec id="s2-4"><title>Multisensory influence of visual speech: Voice-Leading words</title><p>For voice-leading words, there was no processing head start, as auditory speech began at the same time or even earlier than visual speech. This suggests that there should be less opportunity for multisensory influence, a prediction verified by examining the neural response to voice-leading words: the neural response to voice-leading words presented in the audiovisual and auditory-only formats were similar (129% <italic>vs.</italic> 134%, p=0.4). This could not be explained by a lack of response to the visual component of the words, as there was a robust neural response to visual-only voice-leading words (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). Instead, it likely reflected less opportunity for visual speech to influence auditory processing due to the relative timing of the two modalities. For voice-leading words, the latency of the visual-only and auditory-only neural responses were similar (mean latency 11 ms later for visual-only <italic>vs.</italic> auditory-only speech, p=0.77) reflecting the similar physical onset times of visual and auditory speech. Consistent with the idea that the later arrival of visual speech information lessened multisensory, across electrodes there was not a significant correlation between visual-only response amplitude and multisensory influence, defined as the difference between the audiovisual <italic>vs</italic>. auditory-only neural responses (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>; <italic>r</italic> = 0.18, p=0.35).</p></sec><sec id="s2-5"><title>Control analysis: Latency</title><p>Our analysis depended on accurate time-locking between stimulus presentation and neural response recording. A photodiode placed on the monitor viewed by the participants was used to measure the actual onset time of visual stimulus presentation, while a splitter duplicated the auditory output from the computer to measure the actual onset time of auditory stimulus presentation. Both signals were recorded by the same amplifier used to record neural data, ensuring accurate synchronization. Latencies were similar for auditory-only mouth-leading <italic>vs.</italic> voice-leading words (128 ms <italic>vs.</italic> 134 ms, p=0.54) demonstrating that the alignment of responses to the physical onset of speech was effective.</p></sec><sec id="s2-6"><title>Control analysis: Anatomical specialization</title><p>Previous studies have described anatomical specialization within pSTG (<xref ref-type="bibr" rid="bib25">Hamilton et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Ozker et al., 2017</xref>). Electrodes were color coded by the amplitude of the multisensory influence, calculated as the difference between auditory-only and audiovisual mouth-leading words. When viewed on the cortical surface, no consistent organization of multisensory influence was observed (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>It has long been known that viewing the talker’s face enhances the intelligibility of noisy auditory speech (<xref ref-type="bibr" rid="bib61">Sumby and Pollack, 1954</xref>). Visual speech can enhance intelligibility in two related ways (<xref ref-type="bibr" rid="bib48">Peelle and Sommers, 2015</xref>). First, independent information about speech content from the visual and auditory modalities allows for the application of multisensory integration to more accurately estimate speech content. Second, the earlier availability of visual speech information allows for a head start on processing. To examine the contributions of the visual head start to perception, we took advantage of the natural variability between mouth-leading words, in which a visual head start is provided by the onset of visual speech before auditory speech, and voice-leading words, for which there is no head start (<xref ref-type="bibr" rid="bib12">Chandrasekaran et al., 2009</xref>; <xref ref-type="bibr" rid="bib56">Schwartz and Savariaux, 2014</xref>).</p><p>The results of our perceptual experiments show that viewing the face of the talker increases intelligibility for both mouth-leading words and voice-leading words, as expected. However, the benefit of visual speech was significantly greater for mouth-leading words, demonstrating that the visual head start enhances perception beyond the mere availability of visual information. Mirroring the perceptual effects, in neural recordings from the pSTG there was a significant difference in the effects of visual speech during presentation of mouth-leading and voice-leading words. Surprisingly, when visual speech was present for mouth-leading words, the neural response <italic>decreased</italic>, the opposite of the <italic>increased</italic> accuracy observed perceptually.</p><p>To better understand these results, we constructed a <italic>post hoc</italic> neural model that builds on the experimental observation that the pSTG contains populations selective for specific phonemes (<xref ref-type="bibr" rid="bib21">Formisano et al., 2008</xref>; <xref ref-type="bibr" rid="bib25">Hamilton et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Mesgarani et al., 2014</xref>). The key feature of the model is that for mouth-leading words (words with a visual head start) the early availability of visual information enhances responses in neurons representing phonemes that are compatible with the visual speech and suppresses neurons representing phonemes that are incompatible with the visual speech. In all published classifications, there are more incompatible than compatible phonemes for any particular viseme (<xref ref-type="bibr" rid="bib11">Cappelletta and Harte, 2012</xref>; <xref ref-type="bibr" rid="bib29">Jeffers and Barley, 1971</xref>; <xref ref-type="bibr" rid="bib43">Neti et al., 2000</xref>). Therefore, the visual head start <italic>decreases</italic> the total response since more neuronal populations are suppressed than are enhanced. This is shown graphically in <xref ref-type="fig" rid="fig5">Figure 5B</xref> for the auditory-only syllable ‘d’ and in <xref ref-type="fig" rid="fig5">Figure 5C</xref> for audiovisual ‘d’. For voice-leading speech such as the audiovisual syllable ‘m’ (<xref ref-type="fig" rid="fig5">Figure 5D</xref>) there is less time for the suppression and enhancement of incompatible and compatible phonemes to manifest, resulting in similar responses to audiovisual and auditory-only voice-leading words.</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.48116.008</object-id><label>Figure 5.</label><caption><title>Model of Audiovisual Interactions in pSTG.</title><p>(<bold>A</bold>) In the pSTG, small populations of neurons are selective for specific speech sounds (phonemes). Each population is shown as an ellipse labeled by its preferred phoneme. The ellipses are shown spatially separated but the model is equally applicable if the neurons are intermixed instead of spatially segregated. Selectivity is only partial, so that for the population of neurons selective for a given phoneme ‘x’ (ellipse containing ‘x’) presentation of the phoneme ‘x’ evokes a larger response than presentation of any other phoneme (‘not x’). (<bold>B</bold>) When an auditory phoneme is presented, all populations of neurons respond, with the highest response in the population of neurons selective for that phoneme. Example shown is for presentation of auditory ‘d’; the amplitude of the response in each population of neurons is shown by the height of the bar inside each ellipse, with highest bar for ‘d’ population. The total response summed across all populations is shown at right. (<bold>C</bold>) For mouth-leading speech, early arriving visual speech provides a head start (yellow region). During this time, activity in neuronal populations representing incompatible phonemes is suppressed (red outlines) and activity in neuronal populations representing compatible phonemes in enhanced (green outlines). Arrival of auditory speech evokes activity in all populations. Because there are more suppressed populations than enhanced populations, the total response across all populations is decreased relative to the auditory-only format (dashed line and red arrow). Example shown is for audiovisual ‘d’, resulting in larger responses in populations representing the compatible phonemes ‘d’ and ‘t’, smaller responses in all other populations. (<bold>D</bold>) For voice-leading speech, visual speech and auditory speech onset at similar times, resulting in no opportunity for suppression or enhancement (dashed outlines; example shown is for audiovisual ‘m’). The total response is similar to the auditory-only format (dashed line). (<bold>E</bold>) For noisy speech, there is a reduction in the amplitude of the response to auditory phonemes for both preferred and non-preferred populations (example shown is for noisy auditory ‘da’; only two neuronal populations are shown for simplicity). The signal-to-noise ratio (SNR) is defined as the ratio the response amplitude of the preferred to the non-preferred population. (<bold>F</bold>) For noisy audiovisual speech that is mouth-leading (example shown is for noisy audiovisual ‘da’) the response to the compatible neuronal populations are enhanced and the response to the incompatible neuronal populations are suppressed (visible as differences in bar height inside green and red outlines), resulting in increased SNR (red arrow).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-48116-fig5-v1.tif"/></fig><p>The <italic>post hoc</italic> neural model provides a qualitative explanation for the decreased neural response to words with a visual head start. The model also provides a qualitative explanation for the perceptual benefit of the visual head start. The key determinant of perceptual accuracy is the signal-to-noise ratio, defined as the ratio between the response of the neurons selective for the presented phoneme and all other populations. Adding noise to auditory stimuli decreases neuronal responses, making it more difficult to determine the population with the strongest response (low signal-to-noise ratio). The visual head start increases the response of neurons responding to the presented auditory phoneme and decreases the response of most other populations because any given visual speech feature is incompatible with most phonemes. This creates a larger response difference between the two populations (higher signal-to-noise ratio) making perception more accurate relative to auditory-only speech. This process is illustrated schematically for noisy auditory ‘da’ and noisy audiovisual ‘da’ in <xref ref-type="fig" rid="fig5">Figure 5E</xref>.</p><sec id="s3-1"><title>Neuroanatomical substrates of the model</title><p>In the neural model, visual speech modulates the activity evoked by auditory speech. This is consistent with perception. For instance, silent viewing of the visual syllable ‘ga’ does not produce an auditory percept, but when paired with the auditory syllable ‘ba’, many participant report hearing ‘da’ (<xref ref-type="bibr" rid="bib38">Mallick et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">McGurk and MacDonald, 1976</xref>). Neurally, this could reflect the fact that although the pSTG responds to visual-only speech, the response is much weaker than the response to auditory speech (note the different scales for the different modalities in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). The weak response could be insufficient to create a conscious percept. Alternately, co-activation in early auditory areas together with pSTG could be required for an auditory percept. In either case, the significant correlation between the amplitude of the visual response and the degree of multisensory modulation bolsters the case that there is an interaction between visual and auditory speech information in pSTG.</p><p>In an earlier study, we demonstrated that audiovisual speech selectively enhances activity in regions of early visual cortex representing the mouth of the talker (<xref ref-type="bibr" rid="bib47">Ozker et al., 2018b</xref>). In the present study, activity in auditory association cortex was differentially modulated by the presence or absence of a visual speech head start. These findings support the idea that interactions between auditory and visual speech can occur both relatively early in processing (‘early integration’) and at both early and late stages (‘multistage integration’) as proposed by <xref ref-type="bibr" rid="bib48">Peelle and Sommers (2015)</xref>. Since cortex in superior temporal gyrus and sulcus receives inputs from earlier stages of the auditory and visual processing hierarchies, it seems probable that information about visual mouth movements arrives in pSTG from more posterior regions of lateral temporal cortex (<xref ref-type="bibr" rid="bib6">Bernstein et al., 2008</xref>; <xref ref-type="bibr" rid="bib64">Zhu and Beauchamp, 2017</xref>), while information about auditory phonemic content arrives in pSTG from posterior belt areas of auditory cortex (<xref ref-type="bibr" rid="bib33">Leaver and Rauschecker, 2016</xref>).</p></sec><sec id="s3-2"><title>Testing the model</title><p>A simple neural model provides a qualitative explanation for the increased perceptual accuracy and decreased neural responses for mouth-leading compared with voice-leading words, many caveats are in order. The neural model assumes enhancement of compatible populations as well as suppression of incompatible populations. However, only suppression was observed in 25/28 pSTG electrodes (the remaining three electrodes showed similar responses to audiovisual and auditory-only speech). This observation is consistent with the fact that there are more incompatible than compatible phonemes for any particular viseme. If we assume that our relatively large recording electrodes recorded the summed responses of a combination of many suppressed populations and a few enhanced populations, the net effect should be suppression.</p><p>Using smaller recording electrodes permits recording from small populations of neurons selective for individual phonemes (<xref ref-type="bibr" rid="bib25">Hamilton et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Mesgarani et al., 2014</xref>). With these recordings, we would expect to observe both enhancement and suppression. For instance, an electrode selective for the phoneme ‘da’ would be expected to show an enhanced audiovisual response (relative to auditory-only ‘da’) when the participant was presented with an auditory ‘da’ paired with the compatible visual open-mouth ‘da’ mouth shape, and a suppressed response when an auditory ‘da’ was paired with an incompatible mouth shape (such as ‘f’).</p><p>While the neural model provides an explanation for how enhancement and suppression could lead to improved perception of noisy speech, we did not directly test this explanation: only clear speech was presented in the neural recording experiments, and since the clear speech was understood nearly perfectly, it was not possible to correlate neural responses with perception. Therefore, a key test of the model would be to record the responses of phoneme-selective populations to noisy speech and correlate them with perception.</p><p>This would allow direct measurement of the signal-to-noise ratio (SNR) in pSTG by comparing the response of the neural populations responsive to the presented phoneme with the response of other populations. The model prediction is that the SNR in the pSTG should be greater for noisy audiovisual words than for noisy auditory-only words, and greater for mouth-leading words with a visual head start than voice-leading words without one.</p><p>In each participant, we recorded the response to only a few stimuli from a few electrodes, precluding meaningful analysis of classification accuracy. With a higher density of smaller electrodes, allowing recording from many pSTG sites in each participant, and a larger stimulus set, it should be possible to train a classifier and then test it on held-out data. The model prediction is that classification accuracy should increase for voice-leading words, due to the increased SNR generated by enhanced activity in neural populations representing the presented phoneme and decreased activity in neural populations representing other phonemes. On a trial-by-trial basis, the pSTG SNR would be expected to predict perceptual accuracy, with higher SNR resulting in more accurate perception. With large recording electrodes, the degree of suppression measured across populations should correlate with pSTG SNR (greater suppression resulting in greater SNR) and perceptual accuracy.</p></sec><sec id="s3-3"><title>Relationship to predictive coding and repetition suppression</title><p>Predictive coding is a well-established principle at all levels of the auditory system (<xref ref-type="bibr" rid="bib18">Denham and Winkler, 2017</xref>; <xref ref-type="bibr" rid="bib48">Peelle and Sommers, 2015</xref>). Cross-modal suppression may result from similar mechanisms as predictive coding, with the difference that the information about the expected auditory stimulus does not come from previously-presented auditory stimuli but from early-arriving visual speech information. This expectancy is generated from humans’ developmental history of exposure to audiovisual speech, possibly through synaptic mechanisms such as spike-timing dependent plasticity (<xref ref-type="bibr" rid="bib17">David et al., 2009</xref>). Over tens of thousands of pairings, circuits in the pSTG could be modified so that particular visual speech features inhibit or excite neuronal populations representing incompatible/compatible phoneme representations. The cross-modal suppression phenomenon we observed in pSTG may also be related to repetition suppression, in which repeated presentation of stimuli (in this case, successive visual and auditory presentations of the same phoneme) leads to sharpened representations and an overall reduction in response (<xref ref-type="bibr" rid="bib24">Grill-Spector et al., 2006</xref>).</p></sec><sec id="s3-4"><title>The role of temporal attention</title><p>The simple neural model assumes that the visual speech head start provides an opportunity to rule in compatible auditory phonemes and rule out incompatible auditory phonemes in advance of the availability of auditory information from the voice. Another possibility is that the visual speech head start provides an alerting signal that auditory speech is expected soon, without being informative about the content of the auditory speech.</p><p>This is related to the idea of ‘temporal attention’ in which observers can use information about the timing of a sensory stimulus to improve detection and discrimination (<xref ref-type="bibr" rid="bib63">Warren et al., 2014</xref>). One key test of the role of temporal attention in audiovisual speech perception is replacing the mouth movements of the talker with a simpler visual cue. In a recent study, Strand and colleagues found that a visually-presented circle that provided information about the onset, offset and acoustic amplitude envelope of the speech did not improve recognition of spoken sentences or words (<xref ref-type="bibr" rid="bib60">Strand et al., 2019</xref>). In the present experiments, we did not present a non-face temporal attention cue, so we cannot disentangle the temporal and content cues provided by the mouth movements present for mouth-leading stimuli. In the neural recording experiments, there were only four stimulus exemplars, so participants could have learned the relative timing of the auditory and visual speech for each individual stimulus, resulting in neural response differences due to temporal attention.</p></sec><sec id="s3-5"><title>Model predictions and summary</title><p>The neural model links the visual head start to an increased perceptual benefit of visual speech. In our experiments, voice-leading and mouth-leading words were different word tokens. Another approach would be to present the same tokens by experimentally manipulating auditory-visual asynchrony. Voice-leading speech could be transformed, advancing the visual portion of the recording and rendering it effectively ‘mouth-leading’ (<xref ref-type="bibr" rid="bib35">Magnotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib54">Sánchez-García et al., 2018</xref>). Conversely, mouth-leading speech could be transformed by retarding the visual speech, rendering it effectively ‘voice-leading’. The model predicts that the transformed voice-leading speech would not exhibit neural cross-modal suppression and the concomitant perceptual benefit, while transformed mouth-leading speech would exhibit both features.</p><p>Our findings contribute to the grown literature of studies showing how visual input can influence the auditory cortex, especially pSTG (<xref ref-type="bibr" rid="bib9">Besle et al., 2008</xref>; <xref ref-type="bibr" rid="bib19">Ferraro et al., 2019</xref>; <xref ref-type="bibr" rid="bib30">Kayser et al., 2008</xref>; <xref ref-type="bibr" rid="bib40">Megevand et al., 2019</xref>; <xref ref-type="bibr" rid="bib65">Zion Golumbic et al., 2013</xref>). Together with previous work showing that visual cortex is modulated by auditory speech (<xref ref-type="bibr" rid="bib47">Ozker et al., 2018b</xref>; <xref ref-type="bibr" rid="bib55">Schepers et al., 2015</xref>), the present results from the pSTG provide another example of how cross-modal interactions are harnessed by all levels of the cortical processing hierarchy in the service of perception and cognition (<xref ref-type="bibr" rid="bib22">Ghazanfar and Schroeder, 2006</xref>).</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Human subject statement</title><p>All experiments were approved by the Committee for the Protection of Human Subjects at Baylor College of Medicine.</p></sec><sec id="s4-2"><title>Overview of perceptual data collection and analysis</title><p>Perceptual data was collected using the on-line testing service Amazon Mechanical Turk (<ext-link ext-link-type="uri" xlink:href="https://www.mturk.com/">https://www.mturk.com/</ext-link>). Previously, we have shown that Mechanical Turk and in-laboratory testing produces similar results for experiments examining multisensory speech perception (<xref ref-type="bibr" rid="bib36">Magnotti et al., 2018</xref>).</p><p>Within each participant, each stimulus exemplar was presented in four different formats: auditory-only (five trials); auditory-only with added auditory noise (10 trials); audiovisual (five trials); audiovisual with added auditory noise (10 trials). The trials were randomly ordered, except that in order to minimize carry over-effects all noisy stimuli were presented before any clear stimuli were presented. To construct the stimuli from the original audiovisual recordings, the auditory track of each exemplar was extracted using Matlab and all tracks were normalized to have similar mean amplitudes. To add auditory noise, white noise was generated with the same mean amplitude; the amplitude of the speech stimuli were reduced by 12 dB; the speech and noise auditory signals were combined; and the modified auditory track was paired with the visual track (for noisy audiovisual format). After each trial, participants responded to the prompt <italic>‘Type an answer in the text box to indicate what you heard. If you are not sure, take you best guess.’</italic> No feedback was given.</p><p>The goal of the perceptual data analysis was to determine if the addition of visual information improved perception differently for mouth-leading and voice-leading words. Statistically, this was determined by testing the interaction (difference of differences) between word format (audiovisual <italic>vs.</italic> auditory-only) and word type (mouth-leading <italic>vs.</italic> voice-leading). While interactions can be tested with ANOVAs, accuracy data are proportional (bounded between 0% and 100%), violating the assumptions of the test. Instead, we applied a generalized linear mixed-effects model (GLMM) using odds-ratios (proportional change in accuracy, defined as the ratio of the probability of a correct response to the probability of an incorrect response) rather than absolute accuracy differences. For instance, an accuracy increase from 5% to 15% (absolute change of 10%) corresponds to a 3.4-fold increase in the odds-ratio (0.05/0.95: 0.15/0.85) while an accuracy increase from 50% to 60% (absolute change of 10%) corresponds to a 1.5-fold odd-ratio increase (0.5/0.5: 0.6/0.4).</p><p>A priori power was calculated using parameters from a previous study with similar methods (<xref ref-type="bibr" rid="bib51">Rennig et al., 2018</xref>). Each participant was assigned a randomly-selected accuracy level for understanding auditory noisy speech, ranging from 0% to 50% across participants; adding visual speech increased accuracy within each participant by 30% for mouth-leading words and by 20% for voice-leading words. We sampled a binomial distribution using the actual experimental design of the first perceptual experiment, with 40 participants and 20 trials for each of the four conditions. The critical test was for the interaction within the GLMM between word type (mouth-leading and voice-leading) and format (auditory-only and audiovisual). In 10,000 boot-strapped replications, the power to detect the simulated 10% interaction effect was 80%.</p><p>Perceptual data and R code used for the data analysis and power calculations are available at <ext-link ext-link-type="uri" xlink:href="https://openwetware.org/wiki/Beauchamp:DataSharing#Cross-modal_Suppression">https://openwetware.org/wiki/Beauchamp:DataSharing#Cross-modal_Suppression</ext-link>.</p></sec><sec id="s4-3"><title>Stimuli</title><p>The stimuli in the first perceptual experiment and the neural experiment consisted of two exemplars of mouth-leading speech (‘drive’ and ‘last’) and two exemplars of voice-leading speech (‘meant’ and ‘known’) presented in clear and noisy auditory and audiovisual formats (16 total stimuli). To estimate the auditory-visual asynchrony for these stimuli, Adobe Premier was used to analyze individual video frames (30 Hz frame rate) and the auditory speech envelope (44.1 kHz). Visual onset was designated as the first video frame containing a visible mouth movement related to speech production.</p><p>Auditory onset was designated as the first increase in the auditory envelope corresponding to the beginning of the speech sound. These values were: ‘drive’ 170 ms/230 ms (visual onset/auditory onset); ‘last’ 170 ms/270 ms; ‘meant’ 170 ms/130 ms; ‘known’ 200 ms/100 ms. In the second perceptual experiment, ten additional stimulus exemplars were presented in clear and noisy auditory-only and audiovisual format (40 total stimuli). The exemplars consisted of five mouth-leading words and five voice-leading words. The visual onset/auditory onset for the five mouth-leading words were: ‘those’ (125 ms/792 ms); ‘chief’ spoken by talker J (42 ms/792 ms); ‘chief’ spoken by talker L (167 ms/875 ms); ‘hash’ (42 ms/750 ms); ‘vacuum’ (42 ms/771 ms). The onsets for the five voice-leading words were: ‘mature’ (708 ms/625 ms); ‘moth’ (917 ms/833 ms); ‘knock’ (708 ms/458 ms) spoken by talker M; ‘knock’ (917 ms/813 ms) spoken by talker P; ‘module’ (833 ms/708 ms). The actual stimuli used in the recordings are available at <ext-link ext-link-type="uri" xlink:href="https://openwetware.org/wiki/Beauchamp:Stimuli">https://openwetware.org/wiki/Beauchamp:Stimuli</ext-link>.</p><p>To visualize the complete time course of auditory and visual speech shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, two of the stimuli were re-recorded at a video frame rate of 240 Hz (these re-recorded stimuli were not used experimentally). The instantaneous mouth size was determined in each video frame using custom software written in R (<xref ref-type="bibr" rid="bib50">R Development Core Team, 2017</xref>) that allowed for manual identification of 4 control points defining the bounding box of the talker's mouth. The area of this polygon was calculated in each frame and plotted over time. Auditory speech was quantified as the volume envelope over time, calculated by extracting the auditory portion of the recording, down-sampling to 240 Hz, and calculating the absolute value of the amplitude at each time step. The visual and auditory speech values were individually smoothed using a cubic spline curve with 30 degrees of freedom.</p></sec><sec id="s4-4"><title>Additional details of perceptual experiments</title><p>In the first perceptual experiment, 46 participants were presented with the first four stimulus exemplars (20 total stimuli) using Amazon Mechanical Turk (<ext-link ext-link-type="uri" xlink:href="https://www.mturk.com/">https://www.mturk.com/</ext-link>). Six participants had very low accuracy rates (from 0% to 75%) for clear words, suggesting that they were not attending to the stimuli. These participants were discarded, resulting in an <italic>n</italic> of 40 (adequate given the power calculation described above). In the second perceptual experiment, 46 MTurk participants were presented with the second set of 10 stimulus exemplars (40 total stimuli).</p><p>Because participant responses were collected using a text-box (free-response) preprocessing was required before analysis. We corrected basic spelling, typographical errors, well as mistaken plurals (e.g., drives was counted correct for drive); for the word ‘last,’ we counted ‘lust’ as an additional correct response because it was the single most frequent answer in both the clear and the noisy conditions. Analyses were conducted in R (<xref ref-type="bibr" rid="bib50">R Development Core Team, 2017</xref>) using the <italic>glmer</italic> function (family set to binomial) from the <italic>lme4</italic> package (<xref ref-type="bibr" rid="bib2">Bates et al., 2015</xref>). The initial GLMM contained fixed effects of word format (auditory-only <italic>vs.</italic> audiovisual) and word type (mouth-leading <italic>vs.</italic> voice-leading), the word format-by-word type interaction, and a random effect for participant (different intercept for each participant) and exemplar. The baseline for the model was set to the response to mouth-leading words in the auditory-only word format. The inclusion of random effects allowed for participant differences but meant that the estimated odds-ratios are different than those calculated from the raw accuracy score.</p><p>For further analysis, separate GLMMs were created for each word type, with a fixed effect of word format (auditory-only <italic>vs.</italic> audiovisual), random effect of participant and baseline set to audiovisual word format. These separate GLMMs were used to calculate the reported odds-ratio differences and the significance within word type.</p></sec><sec id="s4-5"><title>Neural studies</title><p>Eight subjects (5F, mean age 36, 6L hemisphere) who were selected to undergo intracranial electrode grid placement for phase two epilepsy monitoring provided informed consent to participate in this research protocol. Electrode grids and strips were placed based on clinical criteria for epilepsy localization and resection guidance. The research protocol was approved by the Baylor College of Medicine Institutional Review Board. After a short recovery period following electrode implantation, patients were presented with audiovisual stimuli while resting in their hospital bed in the epilepsy monitoring unit. Stimuli were presented with an LCD monitor (Viewsonic VP150, 1024 × 768 pixels) placed 57 cm in front of the subject’s face, and sound was projected through two speakers mounted on the wall behind and above the patient’s head.</p><p>The four stimulus exemplars from the first perceptual experiment were presented in three formats: auditory-only, visual-only, and audiovisual (12 total stimuli). Auditory-only trials were created by replacing the speaker’s face with a blank screen consisting only of a fixation target. Visual-only trials were created by removing the auditory component of the videos. No auditory noise was present in any format. Stimuli were presented in random interval. The behavioral task used a catch trial design. Subjects were instructed to respond only to an audiovisual video of the talker saying ‘press’. No feedback was given. Neural data from catch trials was not analyzed.</p></sec><sec id="s4-6"><title>Neurophysiology recording and data preprocessing</title><p>Implanted electrodes consisted of platinum alloy discs embedded in flexible silastic sheets (Ad-Tech Corporation, Racine, WI). Electrodes with both 2.3 mm and 0.5 mm diameter exposed surfaces were implanted, but only electrodes with 2.3 mm were included in this analysis. After surgery, electrode tails were connected to a 128-channel Cerebus data acquisition system (Blackrock Microsystems, Salt Lake City, UT) and recorded during task performance. A reversed intracranial electrode facing the skull was used as a reference for recording, and all signals were amplified, filtered (high-pass 0.3 Hz first-order Butterworth, low pass 500 Hz fourth-order Butterworth), digitized at 2000 Hz, then converted from Blackrock format to MATLAB (MathWorks Inc, Natick, MA). Stimulus videos were presented using Psychtoolbox software package (<xref ref-type="bibr" rid="bib10">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib49">Pelli, 1997</xref>; <xref ref-type="bibr" rid="bib31">Kleiner et al., 2007</xref>) for MATLAB. The auditory signal from the stimulus videos was recorded on a separate channel simultaneously and synchronously along with the electrocorticography voltage.</p><p>Preprocessing was performed using the R Analysis and Visualization of intracranial Electroencephalography (RAVE) package (<ext-link ext-link-type="uri" xlink:href="https://openwetware.org/wiki/Beauchamp:RAVE">https://openwetware.org/wiki/Beauchamp:RAVE</ext-link>). Data was notch filtered (60 Hz and harmonics), referenced to the average of all valid channels, and converted into frequency and phase domains using a wavelet transform. The number of cycles of the wavelet was increased as a function of frequency, from 3 cycles at 2 Hz to 20 cycles at 200 Hz, to optimize tradeoff between temporal and frequency precision (<xref ref-type="bibr" rid="bib13">Cohen, 2014</xref>). Data was down-sampled to 100 Hz after the wavelet transform. The continuous data was epoched into trials using the auditory speech onset of each stimulus as the reference (<italic>t</italic> = 0). For visual-only trials, <italic>t</italic> = 0 was considered to be the same time at which the auditory speech would have begun, as determined from the corresponding audiovisual stimulus.</p></sec><sec id="s4-7"><title>Calculation of broadband High-Frequency activity (BHA)</title><p>For each trial and frequency, the power data were transformed into percentage signal change from baseline, where baseline was set to the average power of the response from −1.4 to −0.4 s prior to auditory speech onset. This time window consisted of the inter-trial interval, during which participants were shown a gray screen with a white fixation point. The percent signal change from this pre-stimulus baseline was then averaged over frequencies from 75 to 150 Hz to calculate the broadband high-frequency activity (BHA). Trials with median absolute differences more than five standard deviations from the mean were excluded (&lt;2% excluded trials). For visualization in <xref ref-type="fig" rid="fig3">Figure 3A</xref>, the average BHA for auditory-only trials during auditory speech (time window 0.0 to 0.55 s) was calculated for each electrode and compared against baseline (single sample <italic>t</italic>-test). The resulting <italic>p</italic>-value was plotted according to a white to orange color scale (white is p=0.001, Bonferroni-corrected; orange is p&lt;10<sup>−14</sup>).</p></sec><sec id="s4-8"><title>Electrode localization and selection</title><p>FreeSurfer (<xref ref-type="bibr" rid="bib16">Dale et al., 1999</xref>; <xref ref-type="bibr" rid="bib20">Fischl et al., 1999</xref>) was used to construct cortical surface models for each subject from their preoperative structural T1 magnetic resonance image scans. Post-implantation CT brain scans, showing the location of the intracranial electrodes, were then aligned to the preoperative structural MRI brain using the Analysis of Functional Neuroimaging (AFNI; <xref ref-type="bibr" rid="bib14">Cox, 1996</xref>) package and electrode positions were marked manually on the structural surface model. Electrode locations were projected to the surface of the cortical model using AFNI. SUMA (<xref ref-type="bibr" rid="bib1">Argall et al., 2006</xref>) was used to visualize cortical surface models with overlaid electrodes, and positions were confirmed using intraoperative photographs of the electrode grids overlaid on the brain when available. Cortical surface models with overlaid electrodes were mapped to the Colin N27 brain (<xref ref-type="bibr" rid="bib28">Holmes et al., 1998</xref>) using AFNI/SUMA, allowing for visualization of electrodes from all subjects overlaid over one single brain atlas.</p><p>All analyses were performed on electrodes (<italic>n</italic> = 28 from eight participants; <xref ref-type="fig" rid="fig3">Figure 3A</xref>) that met both an anatomical criterion (located over the posterior superior temporal gyrus) and a functional criterion (significant BHA response to auditory-only speech). The anatomical border between anterior and posterior superior temporal gyrus was defined by extending a line inferiorly from the central sulcus to split the superior temporal gyrus into anterior and posterior portions. The functional criterion was a significant (p&lt;0.001, Bonferroni-corrected) BHA response to the auditory-only word format during the period from stimulus onset to stimulus offset (0 s to 0.55 s). Because the functional criterion ignored word type (voice-leading <italic>vs.</italic> mouth-leading) and did not include audiovisual stimuli, the main comparison of interest (the interaction between response amplitude for different word types and stimulus word formats) was independent of the functional criterion and hence unbiased.</p></sec><sec id="s4-9"><title>Statistical analysis of neural data</title><p>Neural responses were collapsed into a single value by averaging the high-frequency activity (BHA) across the time window from stimulus onset to stimulus offset (0 s to 0.55 s) for each trial for each electrode. These values were then analyzed using a linear mixed-effects model (LME) with a baseline of mouth-leading words in the auditory-only word format. The model factors were two fixed effects of word format (auditory-only <italic>vs.</italic> audiovisual) and word type (mouth-leading <italic>vs.</italic> voice-leading), a word format-by-word type interaction was included, as well as random effects of electrode nested within subject (random intercepts and slopes for each subject-by-electrode pair). Because electrodes were selected based on the response to auditory-only speech ignoring word type (regardless of response to audiovisual or visual-only speech), the model was unbiased. All LMEs were performed in R using the <italic>lmer</italic> function in the <italic>lme4</italic> package. Estimated <italic>p</italic>-values were calculated using the Satterthwaite approximation provided by the <italic>lmerTest</italic> package (<xref ref-type="bibr" rid="bib32">Kuznetsova et al., 2017</xref>).</p><p>To further explore the word format-by-word type interaction, we created separate LMEs for each word type (mouth-leading and voice-leading). For each word type, the LME had fixed effect of word format (auditory-only <italic>vs.</italic> audiovisual) and random effects of electrode nested within subject, with baseline set to the auditory-only word format. These separate LMEs were used to calculate the significance and magnitude of the effect of auditory-only and audiovisual word format on BHA in pSTG within word type.</p><p>For the analysis shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, the average BHA for each visual-only word format trial was calculated over the time window from −0.1 to 0.1 s, the time interval when visual speech was present for mouth-leading words but absent for voice-leading words. We then created an LME model with fixed effect of word type and random effects of electrode nested within subject. The neural response onset stim was measured by calculating the average time (across trials) it took the BHA to reach half its maximum value for each word format and word type. Paired <italic>t</italic>-tests were used to compare half-maximum times between specific word types and word formats.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was supported by NIH R01NS065395 and R25NS070694.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Software, Formal analysis, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Visualization, Methodology, Writing—original draft, Writing—review and editing</p></fn><fn fn-type="con" id="con3"><p>Validation, Writing—review and editing</p></fn><fn fn-type="con" id="con4"><p>Formal analysis, Validation</p></fn><fn fn-type="con" id="con5"><p>Online data collection from Amazon MTurk</p></fn><fn fn-type="con" id="con6"><p>Resources, Data curation, Supervision, Funding acquisition, Writing—review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Visualization, Writing—original draft, Project administration, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Informed consent to perform experiments and publish data was obtained from all iEEG participants. Approval for all parts of this study was obtained through the Baylor College of Medicine Institutional Review Board, protocol numbers H-18112, H-9240 and H-36309.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.48116.009</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-48116-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The RAVE Software used for analysis is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/beauchamplab/rave">https://github.com/beauchamplab/rave</ext-link>. Data have been deposited in Dryad: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.v815n58">https://doi.org/10.5061/dryad.v815n58</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Karas</surname><given-names>PK</given-names></name><name><surname>Magnotti</surname><given-names>JF</given-names></name><name><surname>Metzger</surname><given-names>BA</given-names></name><name><surname>Zhu</surname><given-names>LL</given-names></name><name><surname>Smith</surname><given-names>KB</given-names></name><name><surname>Yoshor</surname><given-names>D</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Data from: The Visual Speech Head Start Improves Perception and Reduces Superior Temporal Cortex Responses to Auditory Speech</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.v815n58</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Argall</surname> <given-names>BD</given-names></name><name><surname>Saad</surname> <given-names>ZS</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Simplified intersubject averaging on the cortical surface using SUMA</article-title><source>Human Brain Mapping</source><volume>27</volume><fpage>14</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1002/hbm.20158</pub-id><pub-id pub-id-type="pmid">16035046</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname> <given-names>D</given-names></name><name><surname>Mächler</surname> <given-names>M</given-names></name><name><surname>Bolker</surname> <given-names>B</given-names></name><name><surname>Walker</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fitting linear mixed-effects models using lme4</article-title><source>Journal of Statistical Software</source><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname> <given-names>MS</given-names></name><name><surname>Argall</surname> <given-names>BD</given-names></name><name><surname>Bodurka</surname> <given-names>J</given-names></name><name><surname>Duyn</surname> <given-names>JH</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Unraveling multisensory integration: patchy organization within human STS multisensory cortex</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1190</fpage><lpage>1192</lpage><pub-id pub-id-type="doi">10.1038/nn1333</pub-id><pub-id pub-id-type="pmid">15475952</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname> <given-names>P</given-names></name><name><surname>Zatorre</surname> <given-names>RJ</given-names></name><name><surname>Lafaille</surname> <given-names>P</given-names></name><name><surname>Ahad</surname> <given-names>P</given-names></name><name><surname>Pike</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Voice-selective Areas in human auditory cortex</article-title><source>Nature</source><volume>403</volume><fpage>309</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1038/35002078</pub-id><pub-id pub-id-type="pmid">10659849</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname> <given-names>LE</given-names></name><name><surname>Auer</surname> <given-names>ET</given-names></name><name><surname>Takayanagi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Auditory speech detection in noise enhanced by lipreading</article-title><source>Speech Communication</source><volume>44</volume><fpage>5</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.specom.2004.10.011</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname> <given-names>LE</given-names></name><name><surname>Lu</surname> <given-names>ZL</given-names></name><name><surname>Jiang</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Quantified acoustic-optical speech signal incongruity identifies cortical sites of audiovisual speech processing</article-title><source>Brain Research</source><volume>1242</volume><fpage>172</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2008.04.018</pub-id><pub-id pub-id-type="pmid">18495091</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname> <given-names>LE</given-names></name><name><surname>Jiang</surname> <given-names>J</given-names></name><name><surname>Pantazis</surname> <given-names>D</given-names></name><name><surname>Lu</surname> <given-names>ZL</given-names></name><name><surname>Joshi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual phonetic processing localized using speech and nonspeech face gestures in video and point-light displays</article-title><source>Human Brain Mapping</source><volume>32</volume><fpage>1660</fpage><lpage>1676</lpage><pub-id pub-id-type="doi">10.1002/hbm.21139</pub-id><pub-id pub-id-type="pmid">20853377</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname> <given-names>LE</given-names></name><name><surname>Liebenthal</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural pathways for visual speech perception</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><fpage>386</fpage><pub-id pub-id-type="doi">10.3389/fnins.2014.00386</pub-id><pub-id pub-id-type="pmid">25520611</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Besle</surname> <given-names>J</given-names></name><name><surname>Fischer</surname> <given-names>C</given-names></name><name><surname>Bidet-Caulet</surname> <given-names>A</given-names></name><name><surname>Lecaignard</surname> <given-names>F</given-names></name><name><surname>Bertrand</surname> <given-names>O</given-names></name><name><surname>Giard</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual activation and audiovisual interactions in the auditory cortex during speech perception: intracranial recordings in humans</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>14301</fpage><lpage>14310</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2875-08.2008</pub-id><pub-id pub-id-type="pmid">19109511</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cappelletta</surname> <given-names>L</given-names></name><name><surname>Harte</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Phoneme-To-Viseme mapping for visual speech recognition</article-title><source>Paper Presented At: Proceedings of the 1st International Conference on Pattern Recognition Applications and Methods,</source><conf-name>SciTePress</conf-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname> <given-names>C</given-names></name><name><surname>Trubanova</surname> <given-names>A</given-names></name><name><surname>Stillittano</surname> <given-names>S</given-names></name><name><surname>Caplier</surname> <given-names>A</given-names></name><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The natural statistics of audiovisual speech</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000436</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000436</pub-id><pub-id pub-id-type="pmid">19609344</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname> <given-names>MX</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Analyzing Neural Time Series Data: Theory and Practice</source><publisher-name>MIT press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/9609.001.0001</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname> <given-names>MJ</given-names></name><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Eye can hear clearly now: inverse effectiveness in natural audiovisual speech processing relies on Long-Term crossmodal temporal integration</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>9888</fpage><lpage>9895</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1396-16.2016</pub-id><pub-id pub-id-type="pmid">27656026</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis. I. segmentation and surface reconstruction</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname> <given-names>SV</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Fritz</surname> <given-names>JB</given-names></name><name><surname>Shamma</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Rapid synaptic depression explains nonlinear modulation of spectro-temporal tuning in primary auditory cortex by natural stimuli</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>3374</fpage><lpage>3386</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5249-08.2009</pub-id><pub-id pub-id-type="pmid">19295144</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denham</surname> <given-names>SL</given-names></name><name><surname>Winkler</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Predictive coding in auditory perception: challenges and unresolved questions</article-title><source>The European Journal of Neuroscience</source><pub-id pub-id-type="doi">10.1111/ejn.13802</pub-id><pub-id pub-id-type="pmid">29250827</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ferraro</surname> <given-names>S</given-names></name><name><surname>Van Ackeren</surname> <given-names>MJ</given-names></name><name><surname>Mai</surname> <given-names>R</given-names></name><name><surname>Tassi</surname> <given-names>L</given-names></name><name><surname>Cardinale</surname> <given-names>F</given-names></name><name><surname>Nigri</surname> <given-names>A</given-names></name><name><surname>Bruzzone</surname> <given-names>MG</given-names></name><name><surname>D'Incerti</surname> <given-names>L</given-names></name><name><surname>Hartmann</surname> <given-names>T</given-names></name><name><surname>Weisz</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stereotactic electroencephalography in humans reveals multisensory signal in early visual and auditory cortices</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/549733</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name><name><surname>Dale</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis. II: inflation, flattening, and a surface-based coordinate system</article-title><source>NeuroImage</source><volume>9</volume><fpage>195</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0396</pub-id><pub-id pub-id-type="pmid">9931269</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Formisano</surname> <given-names>E</given-names></name><name><surname>De Martino</surname> <given-names>F</given-names></name><name><surname>Bonte</surname> <given-names>M</given-names></name><name><surname>Goebel</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>&quot;Who&quot; is saying &quot;what&quot;? Brain-based decoding of human voice and speech</article-title><source>Science</source><volume>322</volume><fpage>970</fpage><lpage>973</lpage><pub-id pub-id-type="doi">10.1126/science.1164318</pub-id><pub-id pub-id-type="pmid">18988858</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname> <given-names>AA</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Is neocortex essentially multisensory?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>278</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.04.008</pub-id><pub-id pub-id-type="pmid">16713325</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grant</surname> <given-names>KW</given-names></name><name><surname>Seitz</surname> <given-names>PF</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The use of visible speech cues for improving auditory detection of spoken sentences</article-title><source>The Journal of the Acoustical Society of America</source><volume>108</volume><fpage>1197</fpage><pub-id pub-id-type="doi">10.1121/1.1288668</pub-id><pub-id pub-id-type="pmid">11008820</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname> <given-names>K</given-names></name><name><surname>Henson</surname> <given-names>R</given-names></name><name><surname>Martin</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Repetition and the brain: neural models of stimulus-specific effects</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>14</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.11.006</pub-id><pub-id pub-id-type="pmid">16321563</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname> <given-names>LS</given-names></name><name><surname>Edwards</surname> <given-names>E</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A spatial map of onset and sustained responses to speech in the human superior temporal gyrus</article-title><source>Current Biology</source><volume>28</volume><fpage>1860</fpage><lpage>1871</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.033</pub-id><pub-id pub-id-type="pmid">29861132</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Rogalsky</surname> <given-names>C</given-names></name><name><surname>Matchin</surname> <given-names>W</given-names></name><name><surname>Basilakos</surname> <given-names>A</given-names></name><name><surname>Cai</surname> <given-names>J</given-names></name><name><surname>Pillay</surname> <given-names>S</given-names></name><name><surname>Ferrill</surname> <given-names>M</given-names></name><name><surname>Mickelsen</surname> <given-names>S</given-names></name><name><surname>Anderson</surname> <given-names>SW</given-names></name><name><surname>Love</surname> <given-names>T</given-names></name><name><surname>Binder</surname> <given-names>J</given-names></name><name><surname>Fridriksson</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural networks supporting audiovisual integration for speech: a large-scale lesion study</article-title><source>Cortex</source><volume>103</volume><fpage>360</fpage><lpage>371</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2018.03.030</pub-id><pub-id pub-id-type="pmid">29705718</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname> <given-names>G</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural basis of speech perception</article-title><source>Handbook of Clinical Neurology</source><volume>129</volume><fpage>149</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1016/B978-0-444-62630-1.00008-1</pub-id><pub-id pub-id-type="pmid">25726267</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname> <given-names>CJ</given-names></name><name><surname>Hoge</surname> <given-names>R</given-names></name><name><surname>Collins</surname> <given-names>L</given-names></name><name><surname>Woods</surname> <given-names>R</given-names></name><name><surname>Toga</surname> <given-names>AW</given-names></name><name><surname>Evans</surname> <given-names>AC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Enhancement of MR images using registration for signal averaging</article-title><source>Journal of Computer Assisted Tomography</source><volume>22</volume><fpage>324</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1097/00004728-199803000-00032</pub-id><pub-id pub-id-type="pmid">9530404</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeffers</surname> <given-names>J</given-names></name><name><surname>Barley</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1971">1971</year><source>Speechreading (Lipreading</source><publisher-loc>Springfield</publisher-loc><publisher-name>Thomas</publisher-name></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Petkov</surname> <given-names>CI</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual modulation of neurons in auditory cortex</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>1560</fpage><lpage>1574</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm187</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname> <given-names>M</given-names></name><name><surname>Brainard</surname> <given-names>D</given-names></name><collab>Pelli D</collab></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in Psychtoolbox-3?” Perception 36 ECVP Abstract Supplement</article-title><source>PLOS ONE</source></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuznetsova</surname> <given-names>A</given-names></name><name><surname>Brockhoff</surname> <given-names>PB</given-names></name><name><surname>Christensen</surname> <given-names>RHB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>lmerTest Package: Tests in Linear Mixed Effects Models</article-title><source>Journal of Statistical Software</source><volume>82</volume><pub-id pub-id-type="doi">10.18637/jss.v082.i13</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leaver</surname> <given-names>AM</given-names></name><name><surname>Rauschecker</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional topography of human auditory cortex</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>1416</fpage><lpage>1428</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0226-15.2016</pub-id><pub-id pub-id-type="pmid">26818527</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Zhou</surname> <given-names>X</given-names></name><name><surname>Ross</surname> <given-names>LA</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name><name><surname>Parra</surname> <given-names>LC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Lip-reading aids word recognition most in moderate noise: a bayesian explanation using high-dimensional feature space</article-title><source>PLOS ONE</source><volume>4</volume><elocation-id>e4638</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0004638</pub-id><pub-id pub-id-type="pmid">19259259</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnotti</surname> <given-names>JF</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Causal inference of asynchronous audiovisual speech</article-title><source>Frontiers in Psychology</source><volume>4</volume><fpage>798</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00798</pub-id><pub-id pub-id-type="pmid">24294207</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnotti</surname> <given-names>JF</given-names></name><name><surname>Smith</surname> <given-names>KB</given-names></name><name><surname>Salinas</surname> <given-names>M</given-names></name><name><surname>Mays</surname> <given-names>J</given-names></name><name><surname>Zhu</surname> <given-names>LL</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A causal inference explanation for enhancement of multisensory integration by co-articulation</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>e18032</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-36772-8</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnotti</surname> <given-names>JF</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A causal inference model explains perception of the McGurk effect and other incongruent audiovisual speech</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005229</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005229</pub-id><pub-id pub-id-type="pmid">28207734</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mallick</surname> <given-names>DB</given-names></name><name><surname>Magnotti</surname> <given-names>JF</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Variability and stability in the McGurk effect: contributions of participants, stimuli, time, and response type</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>22</volume><fpage>1299</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0817-4</pub-id><pub-id pub-id-type="pmid">25802068</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGurk</surname> <given-names>H</given-names></name><name><surname>MacDonald</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Hearing lips and seeing voices</article-title><source>Nature</source><volume>264</volume><fpage>746</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1038/264746a0</pub-id><pub-id pub-id-type="pmid">1012311</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Megevand</surname> <given-names>P</given-names></name><name><surname>Mercier</surname> <given-names>MR</given-names></name><name><surname>Groppe</surname> <given-names>DM</given-names></name><name><surname>Zion Golumbic</surname> <given-names>E</given-names></name><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Phase resetting in human auditory cortex to visual speech</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/405597</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname> <given-names>N</given-names></name><name><surname>Cheung</surname> <given-names>C</given-names></name><name><surname>Johnson</surname> <given-names>K</given-names></name><name><surname>Chang</surname> <given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phonetic feature encoding in human superior temporal gyrus</article-title><source>Science</source><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id><pub-id pub-id-type="pmid">24482117</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munhall</surname> <given-names>KG</given-names></name><name><surname>Jones</surname> <given-names>JA</given-names></name><name><surname>Callan</surname> <given-names>DE</given-names></name><name><surname>Kuratate</surname> <given-names>T</given-names></name><name><surname>Vatikiotis-Bateson</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Visual prosody and speech intelligibility: head movement improves auditory speech perception</article-title><source>Psychological Science</source><volume>15</volume><fpage>133</fpage><lpage>137</lpage><pub-id pub-id-type="doi">10.1111/j.0963-7214.2004.01502010.x</pub-id><pub-id pub-id-type="pmid">14738521</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neti</surname> <given-names>C</given-names></name><name><surname>Potamianos</surname> <given-names>G</given-names></name><name><surname>Luettin</surname> <given-names>J</given-names></name><name><surname>Matthews</surname> <given-names>I</given-names></name><name><surname>Glotin</surname> <given-names>H</given-names></name><name><surname>Vergyri</surname> <given-names>D</given-names></name><name><surname>Sison</surname> <given-names>J</given-names></name><name><surname>Mashari</surname> <given-names>A</given-names></name><name><surname>Zhou</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>Audio-Visual Speech Recognition (Center for Language and Speech Processing</source><publisher-loc>Baltimore</publisher-loc><publisher-name>The Johns Hopkins University</publisher-name></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okada</surname> <given-names>K</given-names></name><name><surname>Venezia</surname> <given-names>JH</given-names></name><name><surname>Matchin</surname> <given-names>W</given-names></name><name><surname>Saberi</surname> <given-names>K</given-names></name><name><surname>Hickok</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>An fMRI study of audiovisual speech perception reveals multisensory interactions in auditory cortex</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e68959</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0068959</pub-id><pub-id pub-id-type="pmid">23805332</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozker</surname> <given-names>M</given-names></name><name><surname>Schepers</surname> <given-names>IM</given-names></name><name><surname>Magnotti</surname> <given-names>JF</given-names></name><name><surname>Yoshor</surname> <given-names>D</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A double dissociation between anterior and posterior superior temporal gyrus for processing audiovisual speech demonstrated by electrocorticography</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>1044</fpage><lpage>1060</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01110</pub-id><pub-id pub-id-type="pmid">28253074</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozker</surname> <given-names>M</given-names></name><name><surname>Yoshor</surname> <given-names>D</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Converging evidence from electrocorticography and BOLD fMRI for a sharp functional boundary in superior temporal gyrus related to multisensory speech processing</article-title><source>Frontiers in Human Neuroscience</source><volume>12</volume><fpage>141</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2018.00141</pub-id><pub-id pub-id-type="pmid">29740294</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozker</surname> <given-names>M</given-names></name><name><surname>Yoshor</surname> <given-names>D</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Frontal cortex selects representations of the talker's mouth to aid in speech perception</article-title><source>eLife</source><volume>7</volume><elocation-id>e30387</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.30387</pub-id><pub-id pub-id-type="pmid">29485404</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Sommers</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prediction and constraint in audiovisual speech perception</article-title><source>Cortex</source><volume>68</volume><fpage>169</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.006</pub-id><pub-id pub-id-type="pmid">25890390</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname> <given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Development Core Team</collab></person-group><year iso-8601-date="2017">2017</year><data-title>R: A language and environment for statistical computing</data-title><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name></element-citation></ref><ref id="bib51"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rennig</surname> <given-names>J</given-names></name><name><surname>Wegner-Clemens</surname> <given-names>K</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Face viewing behavior predicts multisensory gain during speech perception</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/331306</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname> <given-names>LA</given-names></name><name><surname>Saint-Amour</surname> <given-names>D</given-names></name><name><surname>Leavitt</surname> <given-names>VM</given-names></name><name><surname>Javitt</surname> <given-names>DC</given-names></name><name><surname>Foxe</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Do you see what I am saying? exploring visual enhancement of speech comprehension in noisy environments</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>1147</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl024</pub-id><pub-id pub-id-type="pmid">16785256</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salmelin</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Clinical neurophysiology of language: the MEG approach</article-title><source>Clinical Neurophysiology</source><volume>118</volume><fpage>237</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1016/j.clinph.2006.07.316</pub-id><pub-id pub-id-type="pmid">17008126</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sánchez-García</surname> <given-names>C</given-names></name><name><surname>Kandel</surname> <given-names>S</given-names></name><name><surname>Savariaux</surname> <given-names>C</given-names></name><name><surname>Soto-Faraco</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The time course of Audio-Visual phoneme identification: a high temporal resolution study</article-title><source>Multisensory Research</source><volume>31</volume><fpage>57</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1163/22134808-00002560</pub-id><pub-id pub-id-type="pmid">31264596</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schepers</surname> <given-names>IM</given-names></name><name><surname>Yoshor</surname> <given-names>D</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Electrocorticography reveals enhanced visual cortex responses to visual speech</article-title><source>Cerebral Cortex</source><volume>25</volume><fpage>4103</fpage><lpage>4110</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu127</pub-id><pub-id pub-id-type="pmid">24904069</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname> <given-names>JL</given-names></name><name><surname>Savariaux</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>No, there is no 150 ms lead of visual speech on auditory speech, but a range of audiovisual asynchronies varying from small audio lead to large audio lag</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003743</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003743</pub-id><pub-id pub-id-type="pmid">25079216</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahin</surname> <given-names>AJ</given-names></name><name><surname>Backer</surname> <given-names>KC</given-names></name><name><surname>Rosenblum</surname> <given-names>LD</given-names></name><name><surname>Kerlin</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural mechanisms underlying Cross-Modal phonetic encoding</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>1835</fpage><lpage>1849</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1566-17.2017</pub-id><pub-id pub-id-type="pmid">29263241</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname> <given-names>E</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual learning of degraded speech by minimizing prediction error</article-title><source>PNAS</source><volume>113</volume><fpage>E1747</fpage><lpage>E1756</lpage><pub-id pub-id-type="doi">10.1073/pnas.1523266113</pub-id><pub-id pub-id-type="pmid">26957596</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stasenko</surname> <given-names>A</given-names></name><name><surname>Bonn</surname> <given-names>C</given-names></name><name><surname>Teghipco</surname> <given-names>A</given-names></name><name><surname>Garcea</surname> <given-names>FE</given-names></name><name><surname>Sweet</surname> <given-names>C</given-names></name><name><surname>Dombovy</surname> <given-names>M</given-names></name><name><surname>McDonough</surname> <given-names>J</given-names></name><name><surname>Mahon</surname> <given-names>BZ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A causal test of the motor theory of speech perception: a case of impaired speech production and spared speech perception</article-title><source>Cognitive Neuropsychology</source><volume>32</volume><fpage>38</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1080/02643294.2015.1035702</pub-id><pub-id pub-id-type="pmid">25951749</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strand</surname> <given-names>JF</given-names></name><name><surname>Brown</surname> <given-names>VA</given-names></name><name><surname>Barbour</surname> <given-names>DL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Talking points: a modulating circle reduces listening effort without improving speech recognition</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>26</volume><fpage>291</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1489-7</pub-id><pub-id pub-id-type="pmid">29790122</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sumby</surname> <given-names>WH</given-names></name><name><surname>Pollack</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1954">1954</year><article-title>Visual contribution to speech intelligibility in noise</article-title><source>The Journal of the Acoustical Society of America</source><volume>26</volume><fpage>212</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1121/1.1907309</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Wassenhove</surname> <given-names>V</given-names></name><name><surname>Grant</surname> <given-names>KW</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Visual speech speeds up the neural processing of auditory speech</article-title><source>PNAS</source><volume>102</volume><fpage>1181</fpage><lpage>1186</lpage><pub-id pub-id-type="doi">10.1073/pnas.0408949102</pub-id><pub-id pub-id-type="pmid">15647358</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname> <given-names>SG</given-names></name><name><surname>Yacoub</surname> <given-names>E</given-names></name><name><surname>Ghose</surname> <given-names>GM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Featural and temporal attention selectively enhance task-appropriate representations in human primary visual cortex</article-title><source>Nature Communications</source><volume>5</volume><fpage>5643</fpage><pub-id pub-id-type="doi">10.1038/ncomms6643</pub-id><pub-id pub-id-type="pmid">25501983</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname> <given-names>LL</given-names></name><name><surname>Beauchamp</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mouth and voice: a relationship between visual and auditory preference in the human superior temporal sulcus</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>2697</fpage><lpage>2708</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2914-16.2017</pub-id><pub-id pub-id-type="pmid">28179553</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname> <given-names>EM</given-names></name><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Bickel</surname> <given-names>S</given-names></name><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Schevon</surname> <given-names>CA</given-names></name><name><surname>McKhann</surname> <given-names>GM</given-names></name><name><surname>Goodman</surname> <given-names>RR</given-names></name><name><surname>Emerson</surname> <given-names>R</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Simon</surname> <given-names>JZ</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Mechanisms underlying selective neuronal tracking of attended speech at a &quot;cocktail party&quot;</article-title><source>Neuron</source><volume>77</volume><fpage>980</fpage><lpage>991</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id><pub-id pub-id-type="pmid">23473326</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48116.014</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Pasternak</surname><given-names>Tatiana</given-names></name><role>Reviewing Editor</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Mahon</surname><given-names>Bradford</given-names> </name><role>Reviewer</role><aff><institution>University of Rochester</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Cross-modal suppression of auditory association cortex by visual speech as a mechanism for audiovisual speech perception&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Andrew King as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Bradford Mahon (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The manuscript was well received by both reviewers who found the work important and informative and the paper well-written. However, they raised a number of issues that you are encouraged to address before the paper can be considered further for publication in <italic>eLife</italic>. These are listed below.</p><p>Essential revisions:</p><p>1) Please address the need for classification of responses to different speech sounds used in the study. Such classification would strengthen the conclusion that multisensory integration actually takes place in pSTG. The reviewers would also like to see more details of the stimuli used in the study.</p><p>2) Please clarify that the assumptions used in the model determine its results (see comments reviewer 1).</p><p>3) Please address the role of attention in your discussion of the results (see comments from reviewer 2).</p><p>4) In the Discussion, please address potential limitations associated with the use of a limited set of stimuli and the use of exclusively clear speech in the study. Also, please comment on early vs. later multisensory effects and cite the Peelle and Sommers, 2015 paper.</p><p><italic>Reviewer #1:</italic> </p><p>This is an interesting and important follow up study on an earlier paper published in <italic>eLife</italic> from the same group. In the current study the authors compare words that are mouth leading versus voice leading (i.e. where visible movement of the mouth either precedes or follows the initiation of auditory signal). The authors found that pSTG shows a response to mouth leading words that is not present for voice leading words (among a number of supporting analyses). This is an interesting and solid contribution and I hope that the comments below are constructive.</p><p>1) The core claim of the paper is that visual information contributes to multisensory prediction, and that this occurs in pSTG. The authors note in the Discussion that they did not do any classification of the speech sounds, but it seems that is the analysis that would be required to really demonstrate that the multisensory integration is actually happening in pSTG (the neural model that is presented does not speak to classification, but just to changes in signal amplitude). For instance, if the visual information in mouth leading words allows for exclusion of 80% of phonemes, then this should be reflected in classification accuracy correct? E.g., comparing classification of phonemes for audio only (ground truth) to the window of time for mouth leading words prior to onset of auditory information.</p><p>To be clear – I think that the paper makes an important contribution already – it’s just that the evidentiary status of the claim that multisensory integration is occurring in pSTG would suggest a more direct test, or at least some discussion about the specific predictions made for a classification analysis by the theory. (I do think if the classification data could be included it would be better, but I don't see this as a condition sin qua non for publication).</p><p>2) The outcome of the neural model seems to be entirely driven by the assumptions used in constructing the model-this should just be clarified that it is a demonstration of principle of the assumptions (to distinguish this from a model where first principles that do not explicitly make assumptions about increases or decreases in signal as a function of modality 'gives rise' to the observed phenomena).</p><p><italic>Reviewer #2:</italic> </p><p>This manuscript presents research aimed at examining differences in neural activity in auditory association cortex between audio and audiovisual speech as a function of the temporal lag between visual speech and audio speech. The authors hypothesize that differences between A and AV responses should be larger when visual speech leads audio speech. This hypothesis appears to be confirmed in their data which shows a suppression of high gamma activity in pSTG for AV words relative to A words when visual speech leads audio speech, but not when audio speech leads visual speech. The authors interpret this – quite reasonably – as evidence for an effect of visual speech on multisensory integration. They further support this interpretation by showing that the strength of AV vs. A suppression is related to the size of the neural response to visual only speech.</p><p>I thought this was a well written manuscript on an interesting topic. I thought the experimental paradigm was somewhat suboptimal, but I then thought that the subsequent analyses were of very good quality and the overall results were very compelling and were interpreted in a very reasonable way with an interesting discussion. I have a few relatively minor comments.</p><p>1) My main concern with the work would be that the experiment involved the use of only a very limited set of stimuli – two mouth-leading and two voice-leading words. Ultimately the results are compelling, but I wonder about how confident we can be that these results would generalize to a broader set of stimuli, including natural speech. For example, I wondered about what effects repeating these stimuli might have on how the subjects are paying attention or otherwise engaging with the stimuli. I mean I can imagine that as soon as the subject sees the mouth leading stimulus, they know what audio word is going to come next and then they pay a bit less attention resulting in smaller responses. They don't get as much forewarning for the voice leading stimuli, so they don't drop their attention. I would have thought that might be worth discussing and would also suggest that the authors are very clear about their stimuli in the main body of the manuscript.</p><p>2) Another limitation – that the authors acknowledge – was the use of just clear speech in the electrophysiology experiments. I guess it might be worth mentioning that a bit more in the Discussion for two reasons: i) it might be worth highlighting that one might expect to see larger suppressive multisensory effects in (somewhat) noisy speech (in the section on model predictions for example), and ii) it casts a slight doubt on the idea that what we are seeing is genuinely multisensory – I mean the V is not very behaviorally very helpful for clear speech. That said, I do appreciate the result in Figure 4B provides evidence for a multisensory effect.</p><p>3) I thought it might be worth mentioning the Peelle and Somers, 2015 review and perhaps speculating about whether the results we are seeing here might reflect early vs. later multisensory effects – or whether you think that's a useful framework at all?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.48116.015</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Please address the need for classification of responses to different speech sounds used in the study. Such classification would strengthen the conclusion that multisensory integration actually takes place in pSTG.</p></disp-quote><p>We now address classification analysis in detail (see response to reviewer 1, comment #1, below, for complete text).</p><disp-quote content-type="editor-comment"><p>The reviewers would also like to see more details of the stimuli used in the study.</p></disp-quote><p>We have edited the Materials and methods section to provide more detail and made the stimuli freely available at https://doi.org/10.5061/dryad.v815n58</p><disp-quote content-type="editor-comment"><p>2) Please clarify that the assumptions used in the model determine its results (see comments reviewer 1).</p></disp-quote><p>We agree completely and have deleted the conceptual model from the manuscript and completely changed our description of the neural model (see response to reviewer 1, comment #2, below, for complete details).</p><disp-quote content-type="editor-comment"><p>3) Please address the role of attention in your discussion of the results (see comments from reviewer 2).</p></disp-quote><p>We now incorporate a discussion of attention (see response to reviewer 2, comment #1, below, for complete details).</p><disp-quote content-type="editor-comment"><p>4) In the Discussion, please address potential limitations associated with the use of a limited set of stimuli and the use of exclusively clear speech in the study.</p></disp-quote><p>We have performed new experiments with a larger stimulus set (see reviewer 2, comment #1) and have added material about the use of clear speech (see reviewer 2, comment #2 for complete text).</p><disp-quote content-type="editor-comment"><p>Also, please comment on early vs. later multisensory effects and cite the Peelle and Sommers, 2015 paper.</p></disp-quote><p>We now comment on early vs. later multisensory effects and cite the Peelle and Sommers, 2015 paper (see reviewer 2, comment #3 for complete text).</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>[…] 1) The core claim of the paper is that visual information contributes to multisensory prediction, and that this occurs in pSTG. The authors note in the Discussion that they did not do any classification of the speech sounds, but it seems that is the analysis that would be required to really demonstrate that the multisensory integration is actually happening in pSTG (the neural model that is presented does not speak to classification, but just to changes in signal amplitude). For instance, if the visual information in mouth leading words allows for exclusion of 80% of phonemes, then this should be reflected in classification accuracy correct? E.g., comparing classification of phonemes for audio only (ground truth) to the window of time for mouth leading words prior to onset of auditory information.</p><p>To be clear – I think that the paper makes an important contribution already – it’s just that the evidentiary status of the claim that multisensory integration is occurring in pSTG would suggest a more direct test, or at least some discussion about the specific predictions made for a classification analysis by the theory. (I do think if the classification data could be included it would be better, but I don't see this as a condition sin qua non for publication).</p></disp-quote><p>We agree with reviewer 1 that classification analysis would be an important test for the neural model presented in the paper. Our existing data does not allow for a classification analysis, so we now present what reviewer 1 suggests as a reasonable alternative, namely &quot;discussion about the specific predictions made for a classification analysis by the theory&quot;. From the Discussion:</p><p>“While the neural model provides an explanation for how enhancement and suppression could lead to improved perception of noisy speech, we did not directly test this explanation: only clear speech was presented in the neural recording experiments, and since the clear speech was understood nearly perfectly, it was not possible to correlate neural responses with perception. […] With large recording electrodes, the degree of suppression measured across populations should correlate with pSTG SNR (greater suppression resulting in greater SNR) and perceptual accuracy.”</p><disp-quote content-type="editor-comment"><p>2) The outcome of the neural model seems to be entirely driven by the assumptions used in constructing the model-this should just be clarified that it is a demonstration of principle of the assumptions (to distinguish this from a model where first principles that do not explicitly make assumptions about increases or decreases in signal as a function of modality 'gives rise' to the observed phenomena).</p></disp-quote><p>We agree completely. We have deleted the &quot;conceptual model&quot; entirely from the manuscript, removed the reference to the neural model from the title of the manuscript and made clear in the Discussionthat the neural model is a post-hoc explanatory model (rather than one derived from first-principles). That said, the neural model makes a number of interesting predictions that are sure to spur further experiments so we feel it is a valuable part of the manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…] 1) My main concern with the work would be that the experiment involved the use of only a very limited set of stimuli – two mouth-leading and two voice-leading words. Ultimately the results are compelling, but I wonder about how confident we can be that these results would generalize to a broader set of stimuli, including natural speech. For example, I wondered about what effects repeating these stimuli might have on how the subjects are paying attention or otherwise engaging with the stimuli. I mean I can imagine that as soon as the subject sees the mouth leading stimulus, they know what audio word is going to come next and then they pay a bit less attention resulting in smaller responses. They don't get as much forewarning for the voice leading stimuli, so they don't drop their attention. I would have thought that might be worth discussing and would also suggest that the authors are very clear about their stimuli in the main body of the manuscript.</p></disp-quote><p>We fully agree with this critique about our limited stimulus set. As suggested by the reviewer, in addition to describing the stimuli in Materials and methodswe now write in the main body of the manuscript (Results section):</p><p>&quot;In the first perceptual experiment, 40 participants were presented with 16 word stimuli consisting of four stimulus exemplars (two mouth-leading words and two voice-leading words) in each of the four formats (clear auditory, noisy auditory, clear audiovisual, noisy audiovisual).&quot;</p><p>and</p><p>&quot;In contrast to the perceptual studies, where both clear and noisy speech was presented, in the neural experiments only clear speech was presented in order to maximize the size of the neural response. The stimulus exemplars consisted of the two mouth-leading words and the two voice-leading words used in the first perceptual experiment presented in auditory-only, visual-only, and audiovisual formats (twelve total stimuli).&quot;</p><p>To help address this concern, we have performed a new experiment using additional stimuli:</p><p>&quot;In the second perceptual experiment, 46 participants were presented with 40 word stimuli different than those used in the first perceptual experiment, consisting of 10 stimulus exemplars (five mouth-leading words and five mouth-leading words) presented in each of the four formats.&quot;</p><p>The results of this new experiment reproduce and extend our findings to a much larger stimulus set:</p><p>&quot;For these mouth-leading words, viewing the face of the talker increased the intelligibility of noisy auditory speech by 53%…For voice-leading words, viewing the face of the talker provided only a 37% accuracy increase…The interaction between format and word type was significant (<italic>p</italic> &lt; 10<sup>-16</sup>) driven by the larger benefit of visual speech for mouth-leading words.&quot;</p><p>The fact that our findings replicate in a different and larger sample is an important confirmation. However, it is true that we cannot rule out alternative explanation. We now include a new section in the Discussion:</p><p>“The Role of Temporal Attention</p><p>The simple neural model assumes that the visual speech head start provides an opportunity to rule in compatible auditory phonemes and rule out incompatible auditory phonemes in advance of the availability of auditory information from the voice. […] In the neural recording experiments, there were only four stimulus exemplars, so participants could have learned the relative timing of the auditory and visual speech for each individual stimulus, resulting in neural response differences due to temporal attention.”</p><disp-quote content-type="editor-comment"><p>2) Another limitation – that the authors acknowledge – was the use of just clear speech in the electrophysiology experiments. I guess it might be worth mentioning that a bit more in the Discussion for two reasons: i) it might be worth highlighting that one might expect to see larger suppressive multisensory effects in (somewhat) noisy speech (in the section on model predictions for example), and ii) it casts a slight doubt on the idea that what we are seeing is genuinely multisensory – I mean the V is not very behaviorally very helpful for clear speech. That said, I do appreciate the result in Figure 4B provides evidence for a multisensory effect.</p></disp-quote><p>We agree that this is a very important point. We have added a new Figure 5E and provide additional material about this in the Discussion:</p><p>&quot;The post hocneural model provides a qualitative explanation for the decreasedneural response to words with a visual head start. […] This process is illustrated schematically for noisy auditory &quot;da&quot; and noisy audiovisual &quot;da&quot; in Figure 5E.&quot;</p><p>In a different section of the Discussion:</p><p>&quot;While the neural model provides an explanation for how enhancement and suppression could lead to improved perception of noisy speech, we did not directly test this explanation: only clear speech was presented in the neural recording experiments, and since the clear speech was understood nearly perfectly, it was not possible to correlate neural responses with perception. […] The model prediction is that the SNR in the pSTG should be greater for noisy audiovisual words than for noisy auditory-only words, and greater for mouth-leading words with a visual head start than voice-leading words without one.&quot;</p><disp-quote content-type="editor-comment"><p>3) I thought it might be worth mentioning the Peelle and Sommers, 2015 review and perhaps speculating about whether the results we are seeing here might reflect early vs. later multisensory effects – or whether you think that's a useful framework at all?</p></disp-quote><p>We cite the Peele and Somers review and now write in the Discussion:</p><p>“In an earlier study, we demonstrated that audiovisual speech selectively enhances activity in regions of early visual cortex representing the mouth of the talker (Ozker et al., 2018b). […] Since cortex in superior temporal gyrus and sulcus receives inputs from earlier stages of the auditory and visual processing hierarchies, it seems probable that information about visual mouth movements arrives in pSTG from more posterior regions of lateral temporal cortex (Bernstein et al., 2008; Zhu and Beauchamp, 2017), while information about auditory phonemic content arrives in pSTG from posterior belt areas of auditory cortex (Leaver and Rauschecker, 2016).”</p></body></sub-article></article>