<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">47869</article-id><article-id pub-id-type="doi">10.7554/eLife.47869</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Statistical learning attenuates visual activity only for attended stimuli</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-139108"><name><surname>Richter</surname><given-names>David</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3404-8374</contrib-id><email>d.richter@donders.ru.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-28130"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6730-1452</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Donders Institute for Brain, Cognition and Behaviour</institution><institution>Radboud University Nijmegen</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University Feinberg School of Medicine</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>23</day><month>08</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>8</volume><elocation-id>e47869</elocation-id><history><date date-type="received" iso-8601-date="2019-04-23"><day>23</day><month>04</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2019-08-21"><day>21</day><month>08</month><year>2019</year></date></history><permissions><copyright-statement>© 2019, Richter and de Lange</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>Richter and de Lange</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-47869-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.47869.001</object-id><p>Perception and behavior can be guided by predictions, which are often based on learned statistical regularities. Neural responses to expected stimuli are frequently found to be attenuated after statistical learning. However, whether this sensory attenuation following statistical learning occurs automatically or depends on attention remains unknown. In the present fMRI study, we exposed human volunteers to sequentially presented object stimuli, in which the first object predicted the identity of the second object. We observed a reliable attenuation of neural activity for expected compared to unexpected stimuli in the ventral visual stream. Crucially, this sensory attenuation was only apparent when stimuli were attended, and vanished when attention was directed away from the predictable objects. These results put important constraints on neurocomputational theories that cast perception as a process of probabilistic integration of prior knowledge and sensory information.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>expectation</kwd><kwd>visual attention</kwd><kwd>perception</kwd><kwd>prediction</kwd><kwd>perceptual inference</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>Vidi Grant 452-13-016</award-id><principal-award-recipient><name><surname>de Lange</surname><given-names>Floris P</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>ERC Starting Grant 678286</award-id><principal-award-recipient><name><surname>de Lange</surname><given-names>Floris P</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Prior expectations, induced by statistical learning, affect sensory processing only for attended stimuli, thereby constraining neurocomputational theories that cast perception as an inferential process integrating prior knowledge and sensory evidence.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Previous experience constitutes a valuable source of information to guide perception and behavior. Extracting statistical regularities from past input in the environment to form expectations about the future has been shown to improve behavior in myriad ways (<xref ref-type="bibr" rid="bib6">Bertels et al., 2012</xref>; <xref ref-type="bibr" rid="bib26">Hunt and Aslin, 2001</xref>; <xref ref-type="bibr" rid="bib32">Kim et al., 2009</xref>). Indeed, the acquisition of statistical regularities is thought to occur automatically (<xref ref-type="bibr" rid="bib58">Turk-Browne et al., 2009</xref>) and affects behavior even in the absence of an intention to learn, or an awareness of, the regularities (<xref ref-type="bibr" rid="bib20">Fiser and Aslin, 2002</xref>; <xref ref-type="bibr" rid="bib8">Brady and Oliva, 2008</xref>). Given the significant behavioral and perceptual relevance of expectations, it is perhaps not surprising that the brain shows a remarkable sensitivity to statistical regularities. Many studies documented attenuated neural responses for expected compared to unexpected object stimuli in ventral visual regions subserving object recognition, both in terms of single unit spiking activity in monkeys (<xref ref-type="bibr" rid="bib43">Meyer and Olson, 2011</xref>; <xref ref-type="bibr" rid="bib31">Kaposvari et al., 2018</xref>) and in terms of non-invasively measured BOLD activity in humans (<xref ref-type="bibr" rid="bib15">den Ouden et al., 2010</xref>; <xref ref-type="bibr" rid="bib17">Egner et al., 2010</xref>; <xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>; for a review see <xref ref-type="bibr" rid="bib13">de Lange et al., 2018</xref>). This reduced response to expected stimuli has frequently been interpreted, within a predictive processing framework (<xref ref-type="bibr" rid="bib21">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib50">Rao, 2005</xref>; <xref ref-type="bibr" rid="bib51">Rao and Ballard, 1999</xref>), as signifying a reduction of prediction errors elicited by the stimulus when sensory input matches prior expectations. However, it remains largely unknown whether this sensory attenuation process to predicted visual stimuli is automatic, as its relation to statistical learning may suggest, or only apparent when the predictable stimuli are attended.</p><p>Indeed, research on visual statistical learning in monkeys has typically not manipulated attention, but only required monkeys to passively fixate in order to obtain reward (<xref ref-type="bibr" rid="bib43">Meyer and Olson, 2011</xref>; <xref ref-type="bibr" rid="bib31">Kaposvari et al., 2018</xref>), thereby precluding conclusions pertaining to the dependence of these predictive processes on attention. Many studies in humans, providing evidence for suppressed responses to expected stimuli, did require participants to attend the predictable stimuli (e.g., <xref ref-type="bibr" rid="bib15">den Ouden et al., 2010</xref>; <xref ref-type="bibr" rid="bib17">Egner et al., 2010</xref>; <xref ref-type="bibr" rid="bib34">Kok et al., 2012a</xref>; <xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>). On the other hand, <xref ref-type="bibr" rid="bib14">den Ouden et al. (2009)</xref> demonstrated attenuated responses to task-irrelevant expected stimuli, suggesting the possibility that the sensory consequences of statistical learning may not depend on attention. Similarly, <xref ref-type="bibr" rid="bib34">Kok et al. (2012a)</xref> showed that the sensory attenuation for grating stimuli with an expected orientation was independent of whether the orientation feature was attended or not. Importantly however, in both these studies the expected or unexpected stimulus was the only stimulus presented on the screen, so even though the stimuli were not relevant, attention was not effectively disengaged by other stimuli. Without competition, it is likely that even a task-irrelevant stimulus will receive some attention.</p><p>Thus, at present it remains unclear whether statistical learning automatically results in altered neural responses to expected compared to unexpected visual stimuli, or whether this process hinges on the stimuli being attended. In order to answer this question, we exposed participants to sequentially presented pairs of object images. The first image predicted the identity of the second image, thereby making an image expected depending on temporal context. We recorded responses to expected and unexpected object images using whole-brain fMRI while participants performed one of two tasks. Either participants categorized the predictable, second object image as (non-)electronic (rendering the object images attended), or they classified a concurrently shown character (letter or symbol), presented within the fixation dot, as (non-)letter (rendering the object images unattended).</p><p>In brief, our results demonstrate strong sensory attenuation for expected object images within the ventral visual stream. Crucially however, expectation suppression was only evident when objects were attended and vanished when participants attended the concurrently presented alphanumeric characters at fixation. This suggests that sensory attenuation induced by statistical learning is not the result of an automatic integration of prior knowledge with incoming information, but hinges on attention, thus constraining neurocomputational theories of perceptual inference.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We exposed participants to statistical regularities by presenting object image pairs in which the leading image predicted the identity of the trailing image. During a learning session, participants performed a detection task of unpredictable upside-down images. On the next day, in the MRI scanner, participants were shown the same object image pairs, however unexpected trailing images were also presented; that is, images which were predicted by a different leading image. Crucially, participants either classified the trailing object as (non-)electronic, thus actively attending the predictable object, or classified a concurrently presented, but unpredictable, trailing character as (non-)letter, thus not attending the predictable object.</p><sec id="s2-1"><title>Attention is a prerequisite for perceptual expectations</title><p>First, we investigated whether the sensory attenuation for expected object stimuli was equally present when participants attended the objects or not, focusing on our a priori defined ROIs (see <xref ref-type="fig" rid="fig1">Figure 1A</xref>): primary visual cortex (V1), object-selective lateral occipital complex (LOC), and temporal occipital fusiform cortex (TOFC). In all three regions, expectation suppression was robustly present when participants attended the objects (V1: <italic>t</italic><sub>(33)</sub> = 3.573, p=0.001, <italic>d</italic><sub>z</sub> = 0.613; LOC: <italic>t</italic><sub>(33)</sub> = 3.860, p=5.0e-4, <italic>d</italic><sub>z</sub> = 0.662; TOFC: <italic>t</italic><sub>(33)</sub> = 5.133, p=1.2e-5, <italic>d</italic><sub>z</sub> = 0.880), but absent when participants attended the characters at fixation; that is, when the predictable objects were unattended (V1: <italic>t</italic><sub>(33)</sub> = −0.216, p=0.830, <italic>d</italic><sub>z</sub> = −0.037; LOC: <italic>t</italic><sub>(33)</sub> = −0.831, p=0.412, <italic>d</italic><sub>z</sub> = −0.143; TOFC: <italic>t</italic><sub>(33)</sub> = 0.072, p=0.943, <italic>d</italic><sub>z</sub> = 0.012). Indeed, Bayesian analyses showed moderate support for the null hypothesis (BF<sub>10</sub> &lt;1/3) of no expectation suppression in all three regions during the character categorization task (V1: BF<sub>10</sub> = 0.188; LOC: BF<sub>10</sub> = 0.253; TOFC: BF<sub>10</sub> = 0.184). The robustness of this distinct pattern of expectation suppression for the two conditions was statistically confirmed by an interaction analysis (expectation by attention interaction, V1:, <italic>F</italic><sub>(1,33)</sub> = 7.706, p=0.009, <italic>η</italic>²=0.189; LOC: <italic>F</italic><sub>(1,33)</sub> = 12.580, p=0.001, <italic>η</italic>²=0.276; TOFC: <italic>F</italic><sub>(1,33)</sub> = 16.955, p=2.4e-4, <italic>η</italic>²=0.339).</p><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.47869.002</object-id><label>Figure 1.</label><caption><title>Expectation suppression within the ventral visual stream depends on attention.</title><p>(<bold>A</bold>) Displayed are parameter estimates + /- within subject SE for responses to expected (blue) and unexpected (green) object stimuli during the objects attended task (attended) and objects unattended task (unattended). In all three ROIs, V1 (left), LOC (middle), and TOFC (right) BOLD responses were significantly suppressed in response to expected stimuli during the objects attended task. No difference was found between BOLD responses to expected and unexpected stimuli during the objects unattended task. The interaction effect between expectation and attention condition was significant in all three ROIs. (<bold>B</bold>) Expectation suppression in primary visual cortex is stimulus unspecific, and specific only in higher visual areas. Displayed is the average expectation suppression effect (BOLD responses, unexpected minus expected) split into stimulus-driven (light gray) and non-stimulus-driven (dark gray) gray matter voxels. Data are shown for the three ROIs, V1 (left bars), LOC (middle bars), and TOFC (right bars). Expectation suppression in LOC and TOFC was significantly larger for stimulus-driven than non-stimulus-driven voxels, while no such difference was evident in V1, indicating that expectation suppression in V1 was stimulus unspecific. Error bars indicate within-subject SE. Note, that the ROI masks in panel A and B differ, for details see: <italic>ROI definition</italic> and <italic>Stimulus specificity analysis</italic> in the Materials and methods section. *p&lt;0.05. **p&lt;0.01. ***p&lt;0.001.</p><p><supplementary-material id="fig1sdata1"><object-id pub-id-type="doi">10.7554/eLife.47869.003</object-id><label>Figure 1—source data 1.</label><caption><title>Expectation suppression within the ventral visual stream depends on attention.</title><p>The source data file contains a separate JASP file per ROI, containing BOLD data for expected and unexpected stimuli for both attention conditions (objects attended and unattended tasks; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). Also contained is a JASP file showing expectation suppression per ROI split into stimulus driven and not stimulus driven voxel (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-47869-fig1-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47869-fig1-v2.tif"/></fig><p>Thus, in V1, LOC, and TOFC, there was a significant suppression of BOLD responses for expected compared to unexpected object stimuli exclusively during the object categorization task. No such modulation of BOLD responses by expectation was observed in the objects unattended condition in any of the three a priori ROIs, and in fact, there was moderate evidence for the absence of such a modulation when objects were unattended. We repeated all ROI analyses within the same ROIs but with different ROI sizes in order to ensure that our results were not dependent on the a priori but arbitrarily defined ROI mask size. Results were highly similar (i.e., the same effects showing statistically significant results) to those mentioned above within all three ROIs (V1, LOC, TOFC) for all tested ROI sizes, ranging from 100 to 400 voxels (800 mm<sup>3</sup> - 3200 mm<sup>3</sup>) in steps of 100 voxels. Thus, our results do not depend on the exact ROI size but represent responses within the respective areas well.</p><p>We also examined how expectation modulated neural activity outside our predefined ROIs by performing a whole-brain analysis. Results of this whole brain analysis are illustrated in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. The upper row in <xref ref-type="fig" rid="fig2">Figure 2A</xref> shows extensive clusters of expectation suppression throughout the ventral visual stream when objects were attended, but no difference when the objects were unattended (middle row), leading to a significant interaction (bottom row). These results complement our ROI-based analysis by showing that the observed expectation suppression effect is not unique to the a priori defined ROIs but evident throughout the ventral visual stream.</p><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.47869.004</object-id><label>Figure 2.</label><caption><title>Expectation suppression across cortex for attended object stimuli only.</title><p>(<bold>A</bold>) Widespread expectation suppression across cortex in the objects attended condition. Displayed are parameter estimates for unexpected minus expected image pairs overlaid onto the MNI152 2 mm template. Color indicates unthresholded parameter estimates: red-yellow clusters represent expectation suppression. Opacity represents the z statistics of the contrasts. Black contours outline statistically significant clusters (GRF cluster corrected). Significant clusters included major parts of the ventral visual stream (early visual cortex, LOC, TOFC), anterior insula, and inferior frontal gyrus during the objects attended condition (upper row). No significant clusters were evident in the objects unattended condition (middle row). The interaction (attended &gt;unattended; bottom row) showed significant clusters similar to those of the attended condition, albeit less extensive. (<bold>B</bold>) Expectation suppression across the ventral visual stream for attended objects, but with task-irrelevant predictions. Displayed are z statistics of the contrast unexpected minus expected of the conjunction: <italic>attended task-relevant predictions</italic> ∪ <italic>task-irrelevant predictions</italic>; data of task-irrelevant predictions from <xref ref-type="bibr" rid="bib54">Richter et al. (2018)</xref>. Exclusively the ventral visual stream clusters showed significant expectation suppression in this conjunction, while all non-sensory area clusters were no longer significant. Thus, only the ventral visual stream clusters displayed a sensitivity to conditional probabilities, irrespective of whether predictions were task-relevant or task-irrelevant, as long as the predictable stimuli were attended.</p><p><supplementary-material id="fig2sdata1"><object-id pub-id-type="doi">10.7554/eLife.47869.005</object-id><label>Figure 2—source data 1.</label><caption><title>Expectation suppression across cortex for attended object stimuli only.</title><p>The source data file contains nifti images for the whole brain contrast unexpected &gt;expected (expectation suppression). Separate files are included for each attention condition, as well as their interaction (attended &gt;unattended), both in terms of unthresholded parameter estimate, z and thresholded z-maps (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The thresholded z map of the conjunction analysis (<xref ref-type="fig" rid="fig2">Figure 2B</xref>) is also included.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-47869-fig2-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47869-fig2-v2.tif"/></fig><p>Outside the ventral visual stream, additional clusters of expectation suppression are evident in anterior insula and the frontal operculum, the precentral and inferior frontal gyrus, superior frontal gyrus and supplementary motor cortex, superior parietal lobule, as well as parts of the cerebellum. All significant clusters are summarized in a table in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>. Again, all these non-sensory clusters showed reduced activity for expected objects only when the object stimuli were attended and categorized. There was no significant modulation of activity by expectation anywhere in the whole brain analysis when the objects were unattended.</p></sec><sec id="s2-2"><title>Expectation suppression requires attention to the stimuli, but not their predictable relationship</title><p>During the object categorization task, the ability to form expectations about the trailing object stimulus was helpful for the participants, and indeed expected object stimuli were categorized more quickly and accurately (see Figure 5A and <italic>Expectations facilitate object classification</italic>). This begs the question whether the expectation suppression effect that we observed throughout multiple brain areas during the object categorization task reflects differences in task engagement. Participants had an incentive to (implicitly or explicitly) use their knowledge of the predictable relationship between the leading and trailing image to prepare their object categorization response. In order to examine which brain regions exhibited expectation suppression irrespective of the relevance of the predictable relationship between stimuli, we performed a conjunction analysis that highlighted regions that showed significant expectation suppression both in the current study (during the object categorization task) and in a similar study that we published previously (<xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>). During this latter study, participants also attended the object stimuli, but were asked to press a button whenever an object appeared that was flipped upside-down. Upside-down images occurred rarely, and importantly, were not related to the (implicitly learned) statistical regularities. <xref ref-type="fig" rid="fig2">Figure 2B</xref> shows the whole-brain results of this conjunction analysis. Significant, bilateral clusters of expectation suppression were evident throughout most of the ventral visual stream. However, none of the non-sensory clusters showed significant expectation suppression during both experiments. Thus, only in the ventral visual stream we found strong and robust evidence for expectation suppression, regardless of whether the predictable relationship was task-relevant or task-irrelevant, as long as the predictable object pairs were attended.</p></sec><sec id="s2-3"><title>Stimulus specificity of the neural modulation by expectation</title><p>Next, we investigated the stimulus specificity of expectation suppression. Stimulus specificity concerns the question whether only stimulus-driven voxels or also voxels that were not (strongly) driven by the object stimuli displayed expectation suppression. The rationale was that an unspecific suppression effect (i.e., expectation suppression that is also evident in not stimulus-driven voxels) may result from global non-sensory effects, such as changes in general arousal or global surprise signals. On the other hand, stimulus-specific suppression effects, being limited to stimulus-driven voxels, are rather suggestive of a more specific suppression mechanism that selectively operates on the neural populations that represent the expected stimulus; for example, the dampening of stimulus-specific prediction errors as a result of a match between prediction and input.</p><p>All three ROIs were split into two populations of gray matter voxels, according to their stimulus responsiveness (stimulus-driven: responding to the object images; not stimulus-driven: not significantly responding to the object images), using independent data from the localizer run. There were strong differences between the ROIs in terms of the stimulus specificity of expectation suppression (<xref ref-type="fig" rid="fig1">Figure 1B</xref>; ROI x drive interaction: <italic>F</italic><sub>(1.245,41.080)</sub> = 7.651, p=0.005, <italic>η</italic>²=0.188). Whereas there was clear evidence for a larger expectation suppression effect in stimulus-driven than not stimulus-driven voxels in higher visual areas (LOC: <italic>t</italic><sub>(33)</sub> = 3.991, p=3.4e-4, <italic>d</italic><sub>z</sub> = 0.684; TOFC: <italic>t</italic><sub>(33)</sub> = 4.654, p=5.1e-5, <italic>d</italic><sub>z</sub> = 0.798), suppression was not significantly different between stimulus-driven and not stimulus-driven voxels in V1 (<italic>t</italic><sub>(33)</sub> = −1.057, p=0.298, <italic>d</italic><sub>z</sub> = −0.181). Indeed, a Bayesian analysis indicated moderate support for the absence of a difference between stimulus-driven and not stimulus-driven voxels in V1 (BF<sub>10</sub> = 0.307). Of note, all sub-populations in all three ROIs showed significant expectation suppression (all p&lt;0.05), suggesting that there is a general suppression of activity for expected stimuli in visual cortex, irrespective of whether the visual cortical area is driven by the stimuli. However, in later visual cortical areas (LOC and TOFC) there was significantly more expectation suppression in neuronal subpopulations that were driven by the stimulus, implying a more selective suppression mechanism in these areas.</p></sec><sec id="s2-4"><title>Surprising stimuli elicit a larger pupil dilation</title><p>In view of the suggestion that a global, stimulus unspecific response modulation may partially account for expectation suppression, we performed an exploratory analysis to examine whether surprising stimuli were associated with a stronger pupil dilation in our task. Pupil responses have been with linked with changes in arousal (<xref ref-type="bibr" rid="bib52">Reimer et al., 2014</xref>; <xref ref-type="bibr" rid="bib60">Vinck et al., 2015</xref>), which in turn may account for the stimulus unspecific suppression component. Moreover, pupil dilation scales with surprise (<xref ref-type="bibr" rid="bib12">Damsma and van Rijn, 2017</xref>; <xref ref-type="bibr" rid="bib33">Kloosterman et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Preuschoff et al., 2011</xref>). Thus, this account would predict enhanced pupil dilation to unexpected compared to expected stimuli when objects were attended.</p><p>There was indeed a larger pupil diameter for unexpected compared to expected trailing images during the objects attended task (<xref ref-type="fig" rid="fig3">Figure 3</xref>, left). This difference emerged gradually starting ~600 ms after the onset of the trailing object image, and was significant between 1.5–2.8 s, as assessed with a cluster permutation test (<italic>p</italic><sub>cluster</sub> = 0.017). When objects were unattended, no significant difference in pupil diameter was found between the expectation conditions, and in fact, no timepoint surpassed the cluster formation threshold (i.e., all timepoints p&gt;0.05 uncorrected; <xref ref-type="fig" rid="fig3">Figure 3</xref>, right). However, the expectation induced difference in pupil diameter was not reliably different between attended and unattended stimuli (<italic>p</italic><sub>cluster</sub> = 0.393). Thus, the data showed that the pupil was significantly more dilated for unexpected than expected objects when the images were attended, mirroring the results of the neural data – albeit, without a reliable difference between attended and unattended stimuli. This tentatively suggests that the enhanced BOLD responses to unexpected stimuli might be partially accounted for by a global mechanism, such as increased arousal in response to surprising stimuli.</p><fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.47869.006</object-id><label>Figure 3.</label><caption><title>Larger pupil dilations in response to unexpected compared to expected stimuli during the objects attended task.</title><p>Displayed are pupil diameter traces over time, relative to trailing image onset. Pupil diameter data for expected (blue) and unexpected (green) image pairs are shown for the objects attended task (left) and objects unattended task (right). The black line on the abscissa denotes statistically significant differences in pupil dilations between expected and unexpected images (cluster permutation test, p&lt;0.05). In the objects attended condition significantly larger pupil dilations in response to unexpected images are evident between 1.52 to 2.88 s after trailing image onset (left). No significant difference is found in the objects unattended condition (right), nor in the interaction between conditions. The first vertical dashed line indicates leading image onset, the second vertical line trailing image onset. Shaded areas denote within-subject SE. Timepoints from −1.0 to −0.5 s served as baseline period.</p><p><supplementary-material id="fig3sdata1"><object-id pub-id-type="doi">10.7554/eLife.47869.010</object-id><label>Figure 3—source data 1.</label><caption><title>Larger pupil dilations in response to unexpected compared to expected stimuli during the objects attended task.</title><p>The source data file contains the preprocessed pupil diameter traces (participants by timepoints) for each of the four experimental conditions separately (two attention by two expectation conditions).</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-47869-fig3-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47869-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47869.007</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Pupil dilation influences BOLD responses in V1.</title><p>Displayed are the parameter estimates of the influence of pupil size on BOLD responses in V1. BOLD responses increase with larger pupil dilations regardless of whether stimuli were attended and expected. However, pupil dilation influenced BOLD responses more when objects were unattended than attended. Whether stimuli were expected or unexpected did not change the association between BOLD and pupil dilation. Error bars indicate within-subject SEM.</p><p>*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47869-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47869.008</object-id><label>Figure 3—figure supplement 2.</label><caption><title>Pupil dilation influences BOLD responses more in non-stimulus-driven than stimulus-driven V1 voxels.</title><p>Displayed are the parameter estimates of the influence of pupil size on BOLD responses in V1. BOLD responses increase with larger pupil dilation. This association was stronger in non-stimulus-driven (left) than stimulus-driven (right) V1 gray matter voxels. Error bars indicate within-subject SEM.</p><p>*p&lt;0.05, **p&lt;0.01, ***p&lt;0.001.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47869-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.47869.009</object-id><label>Figure 3—figure supplement 3.</label><caption><title>No difference in baseline pupil size between attention tasks, nor expectation conditions.</title><p>Displayed are mean pupil sizes during the baseline period in raw units for expected and unexpected trials during the objects attended and unattended task. Pupil sizes during baseline were similar for trials with expected and unexpected object stimuli, as well as during both tasks. Error bars indicate within-subject SEM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47869-fig3-figsupp3-v2.tif"/></fig></fig-group></sec><sec id="s2-5"><title>Expectation suppression and pupil dilations to surprising stimuli are associated</title><p>We explored whether expectation suppression and pupil dilation differences between unexpected and expected objects were associated. In other words, we sought for evidence of an association between the effect of expectations on pupil dilation and the expectation induced neural response attenuation. For this analysis we rank correlated expectation suppression magnitudes with pupil dilation differences for each participant. Results, displayed in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, suggest that, when objects were attended, expectation suppression in V1 was more pronounced for trailing images that also resulted in larger pupil dilation differences (<italic>t</italic><sub>(31)</sub> = 2.464, p=0.019, <italic>d</italic><sub>z</sub> = 0.436). This association was not reliable in LOC (<italic>t</italic><sub>(31)</sub> = 1.413, p=0.167, <italic>d</italic><sub>z</sub> = 0.250; BF<sub>10</sub> = 0.466) or TOFC (<italic>t</italic><sub>(31)</sub> = 1.401, p=0.171, <italic>d</italic><sub>z</sub> = 0.248; BF<sub>10</sub> = 0.458). There was no correlation of pupil dilation differences and expectation suppression when stimuli were unattended in any of the ROIs (V1: <italic>t</italic><sub>(31)</sub> = −0.159, p=0.875, <italic>d</italic><sub>z</sub> = −0.028; BF<sub>10</sub> = 0.191; LOC: <italic>t</italic><sub>(31)</sub> = −0.125, p=0.901, <italic>d</italic><sub>z</sub> = −0.022; BF<sub>10</sub> = 0.190; TOFC: <italic>t</italic><sub>(31)</sub> = 0.177, p=0.861, <italic>d</italic><sub>z</sub> = 0.031; BF<sub>10</sub> = 0.192). There was no significant overall difference in the correlation strength between attended and unattended stimuli (<italic>F</italic><sub>(1,31)</sub> = 1.892, p=0.179, <italic>η</italic>²=0.058), nor between ROIs (<italic>F</italic><sub>(1.558,48.293)</sub> = 0.134, p=0.823, <italic>η</italic>²=0.004), nor their interaction (<italic>F</italic><sub>(2,62)</sub> = 0.482, p=0.603, <italic>η</italic>²=0.015). Thus, when stimuli were attended there was evidence for an association of pupil dilation and expectation suppression in V1.</p><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.47869.011</object-id><label>Figure 4.</label><caption><title>Expectation suppression is associated with pupil dilation differences and behavioral benefits of expectations.</title><p>(<bold>A</bold>) Correlation of expectation suppression magnitude and pupil dilation differences due to expectation. When predictable objects are attended, trailing images that induce larger pupil dilation differences are also showing larger expectation suppression magnitudes in V1. No such association is evident when objects are unattended. (<bold>B</bold>) Correlation of expectation suppression magnitude and RT benefits due to expectation. When predictable objects are attended, larger RT benefits are associated with larger expectation suppression effects in V1 and TOFC. This association is absent when objects are unattended. Error bars indicate within-subject SEM. *p&lt;0.05.</p><p><supplementary-material id="fig4sdata1"><object-id pub-id-type="doi">10.7554/eLife.47869.012</object-id><label>Figure 4—source data 1.</label><caption><title>Neural effects of expectations are associated with pupil dilation differences and reaction time benefits.</title><p>The source data file contains two JASP files of the analyses conducted on the correlation coefficients (Fisher z-transformed Rho), correlating expectation suppression (neural metric) with (<bold>A</bold>) pupil dilation differences due to expectations and (<bold>B</bold>) RT benefits due to expectations (behavioral measure). Correlation coefficients for data from the three ROIs (V1, LOC, TOFC) and both attention tasks are provided.</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-47869-fig4-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47869-fig4-v2.tif"/></fig></sec><sec id="s2-6"><title>Expectations facilitate object classification</title><p>In order to assess whether, concurrent with the neural effects of expectations, behavioral benefits of expectations were evident, we analyzed behavioral responses during MRI scanning in terms of reaction times (RTs) and response accuracy. Overall, the objects attended (classify electronic items) and objects unattended task (classify characters at fixation) showed very similar response accuracies (attended: 94.3 ± 5.4% vs. unattended: 94.0 ± 6.6%, mean ± SD) and only minor differences in RTs (attended: 574 ± 150 ms vs. unattended: 602 ± 131 ms, mean ± SD). This supports the notion that both tasks were of approximately equal difficulty.</p><p>During the object categorization task, participants could benefit from the foreknowledge of the identity of the trailing object image, as they were asked to categorize the trailing image. Such a benefit would however not be expected during the character categorization task, as the participants could fully ignore the object stimuli during this task. This is precisely what we observed, both in terms of accuracy and RTs (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). During the object categorization task, participants were more accurate (<italic>W</italic> = 457, p=3.2e-4, r<sub>B</sub> = 0.536) and faster (<italic>W</italic> = 9, p=3.8e-9, r<sub>B</sub> = −0.970) for expected compared to unexpected trailing object stimuli. Conversely, during the character categorization task, no such benefit was observed in terms of accuracy (<italic>t</italic><sub>(33)</sub> = 1.600, p=0.119, <italic>d</italic><sub>z</sub> = 0.274; BF<sub>10</sub> = 0.582) or RT (<italic>W</italic> = 252, p=0.447, r<sub>B</sub> = −0.153; BF<sub>10</sub> = 0.273). The robustness of this distinct pattern of behavioral advantage for expected stimuli for the two conditions was statistically confirmed by an interaction analysis (accuracy: <italic>F</italic><sub>(1,33)</sub> = 5.203, p=0.029, <italic>η</italic>²=0.136; RT: <italic>F</italic><sub>(1,33)</sub> = 37.543, p=6.6e-7, <italic>η</italic>²=0.532).</p><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.47869.013</object-id><label>Figure 5.</label><caption><title>Behavioral results demonstrate statistical learning.</title><p>(<bold>A</bold>) Behavioral benefits of expectations demonstrate statistical learning. Displayed are mean accuracy (left) and mean reaction time (right) + /- within subject SE. Responses to expected stimuli are significantly more accurate and faster, an effect exclusively observed during the objects attended condition. Thus, object identity expectations benefit behavioral performance during object classification and do not impact letter classification. (<bold>B</bold>) Pairs of both the objects attended image set and the objects unattended image set were classified significantly above chance, indicating a learning of the pairs for both conditions. Displayed are mean accuracy (left) and mean reaction time (right) during the post-scanning pair recognition task, + /- within subject SE. The dashed line indicates chance level. During the pair recognition task, no differences in either classification accuracy (left) or response speed (right) were observed between pairs previously belonging to the objects attended task compared to the objects unattended task. *p&lt;0.05. ***p&lt;0.001.</p><p><supplementary-material id="fig5sdata1"><object-id pub-id-type="doi">10.7554/eLife.47869.014</object-id><label>Figure 5—source data 1.</label><caption><title>Behavioral results demonstrate statistical learning.</title><p>The source data file contains separate JASP files containing the behavioral performance data and conducted analyses for the object and letter classification tasks (both in terms of RTs and response accuracy; <xref ref-type="fig" rid="fig4">Figure 4A</xref>), as well as data from the post-scanning object recognition task (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-47869-fig5-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47869-fig5-v2.tif"/></fig></sec><sec id="s2-7"><title>Neural and behavioral effects of expectations are associated</title><p>In order to explore whether the observed expectation suppression is associated with the behavioral benefits due to expectations, we correlated the magnitude of expectation suppression and the expectation induced RT benefits. Results, illustrated in <xref ref-type="fig" rid="fig4">Figure 4B</xref>, show that when the predictable objects were attended, behaviorally observed expectation RT benefits and neurally observed expectation suppression were associated in both, V1 (<italic>t</italic><sub>(33)</sub> = 2.442, p=0.020, <italic>d</italic><sub>z</sub> = 0.419) and TOFC (<italic>t</italic><sub>(33)</sub> = 2.236, p=0.032, <italic>d</italic><sub>z</sub> = 0.384), but no reliable correlation was found in LOC (<italic>t</italic><sub>(33)</sub> = 1.384, p=0.176, <italic>d</italic><sub>z</sub> = 0.237, BF<sub>10</sub> = 0.439). There was no association in any ROI when objects were unattended (V1: <italic>t</italic><sub>(33)</sub> = −0.418, p=0.679, <italic>d</italic><sub>z</sub> = −0.072, BF<sub>10</sub> = 0.199; LOC: <italic>t</italic><sub>(33)</sub> = −0.374, p=0.711, <italic>d</italic><sub>z</sub> = −0.064, BF<sub>10</sub> = 0.196; TOFC: <italic>t</italic><sub>(33)</sub> = 0.179, p=0.859, <italic>d</italic><sub>z</sub> = 0.031, BF<sub>10</sub> = 0.186). On average correlations were not reliably larger when objects were attended than when they were unattended (attention: <italic>F</italic><sub>(1,33)</sub> = 2.920, p=0.097, <italic>η</italic>²=0.081). The pattern of results was similar in all ROIs (<italic>F</italic><sub>(1.636,53.988)</sub> = 0.615, p=0.513, <italic>η</italic>²=0.018; interaction: <italic>F</italic><sub>(1.461,48.203)</sub> = 0.381, p=0.619, <italic>η</italic>²=0.011). Thus, there is some evidence that when the objects were attended, participants showed larger benefits (faster RTs) for expected trailing images for which they also showed larger magnitudes of expectation suppression in V1 and TOFC. These results suggest that the neural and behavioral effects of expectations are associated.</p></sec><sec id="s2-8"><title>No differences in association strength between attended and unattended object pairs</title><p>An alternative explanation for the absence of sensory attenuation for expected object stimuli during the character categorization task is that statistical regularities for the objects that are presented during this condition have simply not been learned. This explanation may be unlikely, because the vast majority of exposure to the expected pairs takes places in the learning session, during which the same task (upside-down image detection) was used for all image pairs. However, it is nonetheless important to ensure that statistical regularities were learned for the image pair sets of the object and the character categorization task. To empirically address this, we tested whether participants had explicit knowledge of the statistical regularities for all object pairs. During this post-scanning pair recognition task, participants were asked to indicate which one of two trailing images was more likely given the leading image. Participants indicated the correct trailing image with above chance accuracy for both, the set of object pairs that was previously attended (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; performance = 62.1 ± 1.8%, mean ± SE; <italic>t</italic><sub>(33)</sub> = 6.803, p=4.6e-8, <italic>d</italic><sub>z</sub> = 1.167) and the set that was previously unattended (performance = 58.7 ± 2.2%; <italic>t</italic><sub>(33)</sub> = 3.905, p=2.2e-4, <italic>d</italic><sub>z</sub> = 0.670). There was no statistically significant difference in accuracy on the pair recognition task between these sets of objects (<italic>W</italic> = 365, p=0.256, r<sub>B</sub> = 0.227; BF<sub>10</sub> = 0.737). Reaction times were also similar for both sets of objects (objects previously attended: RT = 458.8 ± 25.4 ms; objects previously unattended: RT = 466.5 ± 25.9 ms; <italic>t</italic><sub>(33)</sub> = −1.208, p=0.236, <italic>d</italic><sub>z</sub> = −0.207; BF<sub>10</sub> = 0.358). Thus, the image pairs belonging to both task conditions (objects attended and unattended tasks) were reliably learned, most likely during the extensive behavioral training session, and there was no evidence for a significant difference in the learning of associations for the two sets of object pairs. This strongly suggests that the differences in sensory attenuation between the two attention conditions are unlikely to be explained by differences in the strength of the association between the object pairs.</p></sec><sec id="s2-9"><title>Visual processing continues in the absence of attention</title><p>Finally, one may wonder whether the lack of expectation suppression when objects were unattended is due to the fact that object stimuli simply did not elicit strong activity in the ventral visual stream, as they were not in the focus of attention. Although all three ROIs showed reliable above-baseline activity also when objects were unattended (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), and activity in LOC and TOFC was of similar amplitude during both conditions, the overall activity level may partly represent stimulus-unrelated activity. Therefore, in an explorative analysis, we assessed the strength of stimulus-specific activity in our three ROIs, by means of a decoding analysis of the trailing images. In brief, a multi-class decoder was trained to differentiate between the six trailing images per attention condition. The classifier was trained on data obtained in an independent localizer run, during which participants performed a separate task (detection of dimming of fixation dot). Performance of this decoder was tested on the mean parameter estimates per trailing image for each of the two attention conditions of the main MRI task data. Because each task was comprised of six trailing images, chance performance was 16.7%. One-sample t-tests or Wilcoxon signed rank test (as applicable) showed that in each of the three ROIs (V1, LOC, TOFC) and tasks (objects attended, objects unattended) object identity could be decoded above chance (V1 attended: 81.1%; <italic>W</italic> = 595, p=3.3e-7, r<sub>B</sub> = 1; V1 unattended: 84.8%; <italic>W</italic> = 595, p=3.2e-7, r<sub>B</sub> = 1; LOC attended: 37.3%; <italic>t</italic><sub>(33)</sub> = 6.303, p=4.0e-7, <italic>d</italic><sub>z</sub> = 1.08; LOC unattended: 38.0%; <italic>W</italic> = 583, p=9.7e-7, r<sub>B</sub> = 0.96; TOFC unattended: 25.0%; <italic>W</italic> = 476, p=0.002, r<sub>B</sub> = 0.60), except in TOFC in the attended condition (TOFC attended: 19.6%; <italic>W</italic> = 383, p=0.143, r<sub>B</sub> = 0.287; BF<sub>10</sub> = 0.388).</p><p>Moreover, decoding accuracy was not different between the objects attended and unattended conditions in any of the ROIs (V1: <italic>t</italic><sub>(33)</sub> = −1.197, p=0.240, <italic>d</italic><sub>z</sub> = −0.205, BF<sub>10</sub> = 0.354; LOC: <italic>t</italic><sub>(33)</sub> = −0.214, p=0.832, <italic>d</italic><sub>z</sub> = −0.037, BF<sub>10</sub> = 0.188; TOFC: <italic>t</italic><sub>(33)</sub> = −1.726, p=0.094, <italic>d</italic><sub>z</sub> = −0.296, BF<sub>10</sub> = 0.697). This suggests that the object stimuli evoked a reliable stimulus-specific activity pattern in all three sensory regions, which was not significantly different in strength between the two tasks (object categorization and character categorization). Note, the participants’ task during the localizer run, which we used to train the classifier, was to detect a dimming of the fixation dot. As such, object stimuli were unattended during the localizer run, which may render the training data more similar in terms of attention allocation to the objects unattended task than the objects attended task. This may explain why decoding accuracy is similar, or even higher, for unattended compared to attended objects. More importantly, overall visual processing of the object stimuli was clearly present even when the objects stimuli were not attended, as the identity of the objects could be reliably decoded from neural activity patterns throughout the ventral visual stream when objects were unattended.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In the present study, we set out to investigate how sensory attenuation following visual statistical learning is modulated by attention. In line with previous studies (<xref ref-type="bibr" rid="bib1">Alink et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">den Ouden et al., 2010</xref>; <xref ref-type="bibr" rid="bib34">Kok et al., 2012a</xref>; <xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Summerfield et al., 2008</xref>) we found a significant and wide-spread attenuation of neural responses to expected compared to unexpected stimuli. Crucially, we showed that attending to the predictable stimuli is a prerequisite for this expectation suppression effect to arise. While unattended objects led to reliable and stimulus-specific increases in neural activity, and object pairs were equally learned for these stimuli, there was no differential activity depending on whether the trailing object was expected or unexpected. Additionally, we found that higher visual areas exhibited stimulus-specific expectation suppression, whereas early visual cortex showed a global, stimulus unspecific suppression, possibly arising from a general increase in arousal in response to surprising stimuli.</p><sec id="s3-1"><title>Attention is a prerequisite for expectation suppression</title><p>Our results show that a core neural signature of perceptual expectations, expectation suppression (<xref ref-type="bibr" rid="bib1">Alink et al., 2010</xref>; <xref ref-type="bibr" rid="bib15">den Ouden et al., 2010</xref>; <xref ref-type="bibr" rid="bib34">Kok et al., 2012a</xref>; <xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>), is only evident when attention is directed to the predictable object stimuli. Specifically, when participants engaged in an object categorization task, we found a wide-spread reduction of neural activity for expected compared to unexpected stimuli throughout the ventral visual stream (V1, LOC, TOFC), as well as several non-sensory areas (anterior insula, inferior frontal gyrus, precentral gyrus, and superior parietal lobule). Strikingly, no modulation of neural activity by expectation was found when attention was drawn away from the object stimuli.</p><p>Interestingly, by directly comparing our present data with a previous dataset, in which we used a similar design (reported in <xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>), we established that expectation suppression is present throughout the ventral visual stream irrespective of whether predictions are task-irrelevant, as long as the object stimuli are attended. In contrast, the larger activity for surprising stimuli in non-sensory areas (insular, frontal and parietal cortex) was only observed in the context of task-relevant expectations. This suggests that neural activity in the ventral visual stream is modulated by conditional probabilities, as long as the stimuli are attended, while the modulations in non-sensory regions are probably reflecting differences in task demands, given that unexpected stimuli were more difficult to categorize (reflected by a cost in speed and accuracy). During the object classification task, unexpected objects may require response inhibition, reevaluation of the category, and thus a new response decision. Given that the anterior insula has been associated with task control, action evaluation (<xref ref-type="bibr" rid="bib9">Brass and Haggard, 2010</xref>), as well as general attentional processes (<xref ref-type="bibr" rid="bib47">Nelson et al., 2010</xref>), and inferior frontal gyrus with response inhibition (<xref ref-type="bibr" rid="bib2">Aron et al., 2003</xref>; <xref ref-type="bibr" rid="bib3">Aron et al., 2004</xref>), the interpretation that the expectation modulation in non-sensory clusters may reflect task related aspects, but not conditional probabilities per se, appears well-supported by previous research.</p><p>Finally, our results also demonstrate that larger expectation suppression effects in V1 and TOFC are associated with increased reaction time benefits afforded by expectations when people are judging the predictable objects. This suggests that the observed expectation suppression effect may not merely constitute an epiphenomenon of more resource efficient neural processing. Instead, given the present data, it is plausible that the behavioral advantage of predicting stimuli may partially be rooted in improved and more effective sensory processing already at the early stages of visual processing. Predictions may thus help in converging more rapidly on an interpretation of the current sensory input, thereby contributing to faster reactions to expected than unexpected stimuli.</p></sec><sec id="s3-2"><title>No perceptual predictions without attention</title><p>Our results corroborate and extend earlier work by <xref ref-type="bibr" rid="bib39">Larsson and Smith (2012)</xref>, who observed that stimulus expectation only affected repetition suppression when the stimuli were attended. However, they appear at odds with several previous studies that have reported expectation suppression in the visual system for stimuli that were not task-relevant and thus appeared unattended (<xref ref-type="bibr" rid="bib14">den Ouden et al., 2009</xref>; <xref ref-type="bibr" rid="bib34">Kok et al., 2012a</xref>; <xref ref-type="bibr" rid="bib35">Kok et al., 2012b</xref>). However, in all these studies, while the predictable stimuli were task-irrelevant, attention was not effectively drawn away by a competing stimulus that required attention. While our attention manipulation is also based on task-relevance, we do engage attention elsewhere using a competing task. This is a crucial difference between the present and previous studies, because it is likely that any supraliminal stimulus, in the absence of competition, will be attended to some degree, even if it is not task-relevant, especially if the stimulus is surprising (<xref ref-type="bibr" rid="bib25">Horstmann and Herwig, 2015</xref>). Indeed, synthesizing earlier and current findings, we can conclude that expectation suppression in the visual system occurs irrespective of exact task goals and relevance of the predictable objects and their predictable relationship, but it is abolished by drawing attention away from the stimuli. This suggests that the integration of prior knowledge and sensory input is gated by attention – that is, prior knowledge only exerts an influence on stimuli that are in the current focus of attention, instead of automatically and pre-attentively modulating sensory input as an obligatory component of perceptual processing.</p><p>It is however possible that other, more ‘stubborn’ prior expectations (<xref ref-type="bibr" rid="bib61">Yon et al., 2019</xref>) that are derived over longer (ontogenetic or phylogenetic) time scales may persist even when attention is drawn away, such as perceptual fill-in during the Kanizsa illusion (<xref ref-type="bibr" rid="bib36">Kok et al., 2016</xref>). Therefore, it is crucial to discriminate between different types of predictions, as expectations of different sources may rely on different neural mechanisms and therefore have distinct properties. Similarly, for simple stimuli, such as oriented gratings (<xref ref-type="bibr" rid="bib34">Kok et al., 2012a</xref>; <xref ref-type="bibr" rid="bib35">Kok et al., 2012b</xref>) or simple sequences (<xref ref-type="bibr" rid="bib18">Ekman et al., 2017</xref>), the resolution of expectations may depend less on recurrent processing throughout the visual hierarchy than for complex objects. Thus, it is conceivable that the automaticity of predictive processing partially depends on the complexity of the predictable stimuli and their association, with increasing complexity requiring increasing processing across the hierarchy, and in turn a focus of attention on the predictable stimuli.</p></sec><sec id="s3-3"><title>Specific vs. unspecific surprise responses</title><p>In LOC and TOFC expectation suppression was largest in neural populations that were driven by the stimuli. Surprisingly, this was not the case in V1, where the suppression was uniformly present in the population that was driven by the stimuli and the population that was not. This replicates the results of our previous study (<xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>) and suggests that the expectation suppression we observe in V1 is not the result of a stimulus-specific reduction in prediction error responses of neurons processing the stimulus. Rather, they suggest that the observed expectation suppression effect in V1 may be accounted for by a more general response modulation. Widespread nonperceptual modulations of visual cortical activity have been documented in response to unexpected events (<xref ref-type="bibr" rid="bib28">Jack et al., 2006</xref>; <xref ref-type="bibr" rid="bib16">Donner et al., 2008</xref>) and have been suggested to be linked to the cholinergic or noradrenergic system (<xref ref-type="bibr" rid="bib4">Aston-Jones and Cohen, 2005</xref>; <xref ref-type="bibr" rid="bib62">Yu and Dayan, 2005a</xref>). Interestingly, both the cholinergic and noradrenergic systems have also been associated with fluctuations in pupil dilation (<xref ref-type="bibr" rid="bib53">Reimer et al., 2016</xref>). In line with this, we found a significantly enhanced pupil dilation in response to unexpected stimuli when the objects were attended. This suggests two possible global mechanisms which may partially account for the observed unspecific expectation suppression effect. Given that both pupil dilation (<xref ref-type="bibr" rid="bib52">Reimer et al., 2014</xref>; <xref ref-type="bibr" rid="bib60">Vinck et al., 2015</xref>) and the noradrenergic system (<xref ref-type="bibr" rid="bib5">Berridge et al., 2012</xref>) are associated with arousal changes, it is possible that expectation suppression is partially accounted for by an increased arousal in response to surprising stimuli. A related explanation is that enhanced pupil dilation to surprising stimuli (<xref ref-type="bibr" rid="bib12">Damsma and van Rijn, 2017</xref>; <xref ref-type="bibr" rid="bib33">Kloosterman et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Preuschoff et al., 2011</xref>) results in enhanced retinal illumination, which in turn leads to stronger responses in early visual areas (<xref ref-type="bibr" rid="bib24">Haynes et al., 2004</xref>), which could potentially also contribute to stimulus unspecific expectation suppression in V1. These interpretations are further supported by the fact that expectation suppression and pupil dilation differences between unexpected and expected attended stimuli were associated, with trailing images that elicit larger pupil dilation differences also showing more pronounced expectation suppression in V1.</p><p>It is unlikely however that these explanations can fully account for the observed expectation suppression effect across the visual hierarchy, given the stimulus-specificity of suppression in LOC and TOFC. Also, it is important to bear in mind that earlier studies, using different stimuli and paradigms, did observe stimulus-specific expectation effects in V1 (<xref ref-type="bibr" rid="bib34">Kok et al., 2012a</xref>; <xref ref-type="bibr" rid="bib22">Gavornik and Bear, 2014</xref>). Combined, the evidence suggests that the resolution of prediction errors crucially depends on the visual areas that are specifically coding the feature that is diagnostic of an expectation confirmation or violation, while areas below this level may only witness an unspecific, global modulation in their response, signifying the binary expectation confirmation or violation.</p></sec><sec id="s3-4"><title>Attention and prediction errors</title><p>Within the predictive coding framework, it has been suggested that attention modulates the gain of prediction error units (<xref ref-type="bibr" rid="bib19">Feldman and Friston, 2010</xref>). On first glance, our results may not appear compatible with the suggestion that attention modulates the gain of prediction errors, because we observe a stimulus-specific bottom-up signal (prediction error) when stimuli are unattended, but no difference in the size of this prediction error between expected and unexpected stimuli. However, it is conceivable that the gain modulation of activity in prediction error units only occurs after the initial feedforward activity sweep, once the object predictions are strongly activated and start exerting an effect on the resolution of the prediction error. In particular, the response to unexpected attended stimuli may be upregulated by attention, while prediction errors for expected attended stimuli are rapidly resolved, thus resulting in the difference in activity for attended objects. On the other hand, when attention is drawn away from the object stimuli, a reduced gain on prediction error units results in the observed attenuation of overall BOLD responses, and an absence of a reliable difference between expected and unexpected stimuli. A closely related, but conceptually distinct, interpretation is that attention constitutes a (modulation of the) prior itself (<xref ref-type="bibr" rid="bib50">Rao, 2005</xref>; <xref ref-type="bibr" rid="bib63">Yu and Dayan, 2005b</xref>). On this account, attention boosts relevant predictions, as during the object classification task, thus leading to wide-spread expectation suppression, due to larger prediction errors for unexpected compared to expected stimuli. However, when attention is disengaged from the object stimuli, object predictions are not generated, and thus do not exert an effect on sensory processing.</p></sec><sec id="s3-5"><title>Interpretational limitations</title><p>One may wonder whether the character categorization task at fixation may have drawn attention away from the objects so forcefully that the object stimuli were no longer processed by sensory cortex. It is important to note here that, although attention was engaged at fixation by the character categorization task, this task was of trivial difficulty. Thus, it seems unlikely that attentional resources were exhaustively engaged by the task, preventing any processing of the surrounding object stimuli, thereby causing the absence of predictive processing. Indeed, behavioral performance was at ceiling during both tasks. Furthermore, even when objects were unattended reliable visual processing took place, as evident by strong responses and object-specific neural patterns in the visual ventral stream. This suggests that in-depth visual processing of object stimuli did occur in the absence of attention, but predictive processes in particular ceased.</p><p>Another alternative explanation of the present results could be that predictive relationships were not learned for the set of objects that were used during the character categorization task, thereby accounting for the absence of a prediction effect. The pair recognition task at the end of the experiment however showed that associations were learned for both image pair sets. Thus, a lack of visual processing or absence of learning cannot account for the observed results. Also, it is worth noting that initially the used probabilistic associations (P(expected|cue)=0.5) may appear less strong than in some previous studies; for example, <xref ref-type="bibr" rid="bib17">Egner et al. (2010)</xref>, <xref ref-type="bibr" rid="bib34">Kok et al. (2012a)</xref>, and <xref ref-type="bibr" rid="bib57">Summerfield et al. (2008)</xref> used P(expected|cue)=0.75. However, the likelihood ratio of expected/unexpected stimuli (0.5/0.1 = 5) used here is actually larger (i.e., each unexpected image is more surprising) than in the cited studies (0.75/0.25 = 3). Moreover, similar probabilistic associations have been successfully employed in studies investigating neural effects of statistical learning in both non-human primates (<xref ref-type="bibr" rid="bib43">Meyer and Olson, 2011</xref>) and humans (<xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>). In short, the utilized conditional probabilities are comparable to previous studies investigating statistical learning. Finally, it is worth emphasizing that neither adaptation nor familiarity effects can account for the observed results, because all trailing objects served both as expected and unexpected images, depending only on temporal context (i.e., the leading image).</p></sec><sec id="s3-6"><title>Conclusion</title><p>In sum, our results suggest that visual statistical learning results in attenuated sensory processing for predicted input, but only when this input is attentively processed. Thus, attention seems to gate the integration of prior knowledge and sensory input. This places important constraints on neurocomputational theories that cast perceptual inference as a process of automatic integration of prior and sensory information.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Preregistration and data availability</title><p>The present study was preregistered at Open Science Framework (OSF) before any data were acquired. The preregistration document is available at DOI: <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.17605/OSF.IO/36TE7">10.17605/OSF.IO/36TE7</ext-link>. All procedures and criteria outlined in the preregistration document were followed, unless explicitly specified in the Materials and method section below. In this manuscript, only research question 1 of the preregistration document is addressed. All data analyzed in the present paper are available here: <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/11633/aacg3rkw">http://hdl.handle.net/11633/aacg3rkw.</ext-link></p></sec><sec id="s4-2"><title>Participants and data exclusion</title><p>Our target sample size was n = 34. This sample size was chosen to ensure 80% power for detecting at least a medium effect size (Cohen’s d ≥ 0.5) with a two-sided paired t-test at an alpha level of 0.05. In total, 38 healthy, right-handed participants were recruited from the Radboud University research participation system. The study followed institutional guidelines of the local ethics committee (CMO region Arnhem-Nijmegen, The Netherlands). We excluded four participants, following our exclusion criteria (see preregistration document and <italic>Data Exclusion</italic>) resulting in the desired sample size of n = 34 participants (25 females, age 24.9 ± 4.8 years, mean ± SD) for data analysis. Of these four exclusions, three exhibited excessive motion during scanning, and one was caused by the participant falling asleep, thus resulting in an incomplete data set.</p></sec><sec id="s4-3"><title>Data exclusion</title><p>The following preregistered criteria were utilized for the rejection of data. If any of the following criteria applied, data from that participant were excluded from all analyses. (1) Subpar fixation behavior during scanning, indicative by a total duration of closed eyes exceeding 3 SD above the group mean – only trials with stimuli were considered in this analysis; that is, null events and instruction or performance screens were not included. (2) Excessive relative motion larger than ½ voxel size (i.e., 1 mm) during MRI scanning, as indexed by the total number of these motion events exceeding 2 SD above the group mean. (3) Task performance during MRI scanning indicating frequent attentional lapses, as indicated by a mean error rate 3 SD above the group mean.</p><p>A fourth rejection criterion, outlined in the preregistration document, based on chance level performance during the post-scan pair recognition task (see: <italic>Pair recognition task</italic> and <italic>2AFC task</italic> in the preregistration document), was not enforced. This decision was based on feedback by participants, indicating that the short ITI during this task made it very challenging, even for participants who reported to have learned most of the associations. Thus, the preregistered pair recognition task based exclusion criterion would not fulfill the desired function of reliably indicating which participants did not explicitly learn the associations, as participants struggled with the task due to its fast pace. Indeed, the enforcement of the criterion would have resulted in the rejection of an additional nine participants (~26% of participants) from data analysis, which was deemed too stringent.</p></sec><sec id="s4-4"><title>Stimuli and experimental paradigm</title><sec id="s4-4-1"><title>Experimental paradigm</title><p>The experiment consisted of two sessions on two consecutive days. On each day the same stimuli were used for each participant, but different tasks were employed.</p><p><italic>Learning session – day one.</italic> On each trial participants were exposed to two images of objects in quick succession (see <xref ref-type="fig" rid="fig6">Figure 6A</xref> for a single trial). Each stimulus was presented for 500 ms without an interstimulus interval and an intertrial interval between 1000–2000 ms. Each participant saw 24 different object images, 12 of which only occurred as leading images (i.e., as the first image on a trial), while the remaining 12 occurred only as trailing images (i.e., as the second image on a trial). Importantly, during the learning session the leading image was perfectly predictive of the identity of the trailing image [P(trailing|leading)=1]. In other words, there were 12 image pairs during learning. While participants were made aware of the existence of such regularities, the regularities were not task-relevant. On 20% of trials, one of the two object images was presented upside-down – either the leading or the trailing image could be flipped upside-down. Crucially, whether an image was upside-down could not be predicted and was completely randomized. Participants were instructed to press a button as soon as an upside-down image occurred. Both speed and accuracy were emphasized. On trials without an upside-down image, no response was required. Throughout the entire trial, a fixation bull’s-eye (outer circle 0.7° visual angle) was superimposed at the center of the screen. Within the inner circle of the fixation bull’s-eye (0.6° visual angle) alphanumeric characters (letters or symbols) were presented (~0.4° visual angle). The characters were presented at the same time and for the same duration as the object stimuli – that is, two characters per trial, each for 500 ms. As with the object images, there were 12 leading characters and 12 trailing characters. However, unlike the objects, the identity of the characters, including whether a letter or symbol occurred, was randomized and thus unpredictable. Participants were instructed that they could ignore these characters, but to maintain fixation on the fixation bulls-eye. In total each participant performed 960 trials during the learning session split into four runs, with a brief break in between runs. Thus, each of the image pairs occurred 80 times during the learning session. The learning session took approximately 60 min. <italic>fMRI session – day two.</italic> Day two of the experiment took place one day after the learning session. First, participants performed an additional 240 trials of the same upside-down task as during the learning session in order to refresh the learned associations. Then participants performed two new tasks in the MRI scanner. During MRI scanning, trials were similar to the learning session, using the same stimulus presentation durations, except for longer intertrial intervals (4000–6000 ms, randomly sampled from a uniform distribution). Another change to the paradigm during MRI scanning was a reduction of the probability of the trailing image given the leading image; P(trailing_expected|leading)=0.5. Thus, now only in 50% of trials a leading image was followed by its expected trailing image. In the remaining 50% of trials, one of the other five trailing images would occur, making these images unexpected given that particular leading image (i.e., each unexpected trailing image had P(trailing_unexpected|leading)=0.1). This was achieved by splitting the original 12 × 12 transition matrix from day one into two 6 × 6 matrices (see <xref ref-type="fig" rid="fig6">Figure 6B</xref>). One 6 × 6 matrix was used for each of the two tasks participants performed in the MRI (object categorization and character categorization tasks; see below). Thus, each expected trailing image was five times more likely given its leading image than any of the unexpected trailing images. Furthermore, each trailing image was only (un-)expected by virtue of the leading image it followed, which in turn also ensured that all images occurred equally often throughout the experiment, excluding confounds due to stimulus frequency or familiarity. During MRI scanning, an infrared eye tracker (SensoMotoric Instruments, Berlin, Germany) was used to monitor and record the position and pupil size of the left eye, at 50 Hz. Finally, after MRI scanning, a brief pair recognition task was performed – for details see <italic>Pair recognition task</italic> below.</p><fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.47869.015</object-id><label>Figure 6.</label><caption><title>Experimental paradigm.</title><p>(<bold>A</bold>) A single trial is displayed, starting with a 500 ms presentation of the leading object and the leading character superimposed at fixation. Next, without ISI, the trailing object and trailing character are shown for 500 ms. Each trial ends with a 4000–6000 ms ITI (MRI session; 1000–2000 ms ITI learning session), showing only a fixation dot. (<bold>B</bold>) Statistical regularities depicted as image transition matrix with object pairs and trial numbers during MRI scanning. L1 to L12 represent leading objects, while T1 to T12 represent the trailing objects. Leading and trailing objects were randomly selected per participant from a larger pool of images - that is, leading images of one participant may occur as trailing objects of another participant, in a different task, or not at all. Blue cells denote expected object pairs of the objects attended (object categorization) task, while green indicates unexpected object pairs of the objects attended task. Red denotes expected objects of the objects unattended (character categorization) task, and orange indicates unexpected objects of the objects unattended task. Each participant was also assigned 12 leading and 12 trailing characters (six letters, six symbols each). Unlike the object images, there was no association between leading and trailing characters – that is, the identity of the leading and trailing character was unpredictable. White numbers represent the total number of trials of that cell during MRI scanning. In total 120 trials of each of the four conditions were shown during MRI scanning per participant. In the behavioral learning session, participants performed an orthogonal oddball detection task, during which only expected pairs were shown (i.e., only the diagonal of the matrix), for a total of 80 trials per expected pair (960 trials total).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-47869-fig6-v2.tif"/></fig><p><italic>Object categorization task.</italic> During the object categorization task participants were required to categorize, as quickly and accurately as possible, the trailing object on each trial as either electronic or non-electronic. Thus, during this task it was beneficial to be able to predict the identity of the trailing object using the learned associations. Failing to respond, or responding later than 1500 ms after trailing image onset, was considered a miss. Because the 12 × 12 transition matrix was split into two 6 × 6 matrices, one for this task, one for the character categorization task, it was ensured that both 6 × 6 matrices contained three electronic and three non-electronic objects as trailing and leading images, ensuring an equal base rate of both categories. Before performing this task, it was explained that ‘electronic’ would be any object that contains any electronic components or requires electricity to be used. Furthermore, it was ensured that participants could correctly classify each object by displaying all objects on screen and requesting participants to verbally categorize and name each object before entering the MRI.</p><p><italic>Character categorization task.</italic> Trials of the character categorization task were identical to the object categorization task, except that participants were instructed to categorize the trailing character on each trial as a letter (of the standard Latin alphabet: A, B, D, E, G, H, J, K, M, N, R, S) or non-letter (i.e., a symbol or letter of a non-Latin alphabet: €, $, =, +, ɸ, Ͽ, £, ‡, Ӵ, ל, !, ?). While the presentation onset and duration of the characters coincided with the presentation of the object images, the identity of the trailing character was not predictable. As with the object images, six characters (three letters, three non-letters) were assigned as leading characters and six were assigned as trailing characters (three letters, three non-letters) for each of the two tasks (object and character categorization task). This was done to ensure that the character categorization task was as similar as possible to the object categorization task, and that exposure to the individual characters was as frequent as to the objects. Thus, in short, the rationale of the character categorization task was to draw attention away from the object stimuli and towards the characters, without imposing a heavy load on attentional or cognitive resources. Indeed, both tasks were designed to yield task performance at ceiling level. For both the object and character categorization tasks, feedback on behavioral performance was provided at the end of each run.</p><p><italic>Procedure, MRI session.</italic> First, participants performed a brief practice run consisting of 50 trials (~5 min in duration) of either the object or character categorization task in the MRI. However, during the practice run, no unexpected trailing images occurred in order to retain the strong expectations built up during the learning session. Additionally, during the practice run, an anatomical image was acquired. After the practice run, two runs of the object or character categorization task were performed. Each run (~14 min) consisted of 120 trials and seven null events of 12 s. Next, a practice run of the other task followed – that is, if the object categorization task was performed first, the character categorization task would now follow, or vice versa. The task order was counter-balanced across participants. The practice run was again followed by two runs of the second task. After this, participants performed one functional localizer run (see: <italic>localizer</italic>). Finally, participants did a pair recognition task (see: <italic>Pair recognition task</italic>), assessing the learning of the object pairs. Once finished, participants were fully debriefed, and any remaining questions were addressed.</p><p><italic>Localizer.</italic> We included a localizer session to define object-selective LOC for each participant and to constrain region of interest (ROI) masks to the most informative voxels using data from an independent, context-neutral run (i.e., without expectations). The functional localizer consisted of a repeated presentation of the previously seen trailing images and their phase-scrambled version. Images were presented for 12 s at a time, flashing at 2 Hz (300 ms on, 200 ms off). At some point during stimulus presentation, the middle circle of the fixation dot would dim. Participants were instructed to press a button, as fast as possible, once they detected the dimming of the fixation dot. Each trailing image was presented six times. Additionally, a phase-scrambled version of each trailing image was presented three times. Furthermore, 12 null events, each with a duration of 12 s were presented. The presentation order was fully randomized, except for excluding direct repetitions of the same image and ensuring that each trailing image once preceded and once followed a null event in order to optimize the design.</p><p><italic>Pair recognition task.</italic> The rationale of this task was to assess the learning of the object pairs (i.e, statistical regularities) and to compare whether participants learned the regularities during the objects attended task better than during the character categorization task. The pair recognition task followed the MRI session and consisted of the presentation of a leading image followed by two trailing images, one on the left and one on the right of the fixation dot. Participants were instructed to indicate, by button press, which of the two trailing images was more likely given the leading image. In order to prevent extensive learning during this task, a few trials with only unexpected trailing images were shown. Furthermore, participants were instructed that a response was required on each trial, even when they were unsure. Stimulus durations and intertrial intervals were identical to the learning session, that is, 500 ms leading image, 500 ms trailing images, and a variable intertrial interval (1000–2000 ms randomly sampled from a uniform distribution). A response had to be provided within 1500 ms after trailing image onset, or otherwise the trial was counted as a miss. Participants performed one block of this task, consisting of 240 trials.</p></sec></sec><sec id="s4-5"><title>Stimuli</title><p>Sixty-four full color object stimuli were used during the experiment. The object images were a selection of stimuli from <xref ref-type="bibr" rid="bib7">Brady et al. (2008)</xref> comprising typical object stimuli which were clearly electronic or non-electronic in nature (stimuli can be found here, DOI: <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.17605/OSF.IO/36TE7">10.17605/OSF.IO/36TE7</ext-link>). Of these 64 object stimuli, 24 were randomly selected per participant, of which 12 were randomly assigned as leading images, while the other 12 served as trailing images. Thus, each specific image could occur as leading image for one participant, as trailing image for another participant, and not at all for a third participant, thereby minimizing the impact of any particular image’s features. Images spanned approximately 5° x 5° visual angle on a mid-gray background, both during the learning session and MRI scanning. During the learning session stimuli were presented on an LCD screen (BenQ XL2420T, 1920 × 1080 pixel resolution, 60 Hz refresh rate). During MRI scanning, stimuli were back-projected (EIKI LC-XL100 projector, 1024 × 768 pixel resolution, 60 Hz refresh rate) on an MRI-compatible screen, visible using an adjustable mirror mounted on the head coil.</p><p>We calculated the average relative luminance of the object stimuli by converting the stimulus images from sRGB to linear RGB, then calculated the relative luminance for all pixels (where relative luminance Y = 0.2126*R + 0.7152*G + 0.0722*B; <xref ref-type="bibr" rid="bib56">Stokes et al., 1996</xref>), and finally averaged the obtained luminance values, thereby obtaining the mean relative luminance per image. On this relative luminance scale, 0 would be a completely black image, while one would be a white image. The average relative luminance of the stimulus set was 0.225, while the relative luminance of the mid gray background, presented during the ITI, was 0.216.</p></sec><sec id="s4-6"><title>fMRI data acquisition</title><p>Anatomical and functional images were acquired on a 3T Prisma scanner (Siemens, Erlangen, Germany), using a 32-channel head coil. Anatomical images were acquired using a T1-weighted magnetization prepared rapid gradient echo sequence (MP-RAGE; GRAPPA acceleration factor = 2, TR/TE = 2300/3.03 ms, voxel size 1 mm isotropic, 8° flip angle). Functional images were acquired using a whole-brain T2*-weighted multiband-6 sequence (time repetition [TR]/time echo [TE]=1000/34.0 ms, 66 slices, voxel size 2 mm isotropic, 75° flip angle, A/P phase encoding direction, FOV = 210 mm, BW = 2090 Hz/Px). To allow for signal stabilization, the first five volumes of each run were discarded.</p></sec><sec id="s4-7"><title>Data analysis</title><sec id="s4-7-1"><title>Behavioral data analysis</title><p>Behavioral data from the main task MRI runs were analyzed in terms of reaction time (RT) and response accuracy. Trials with RT &lt;200 ms, RT &gt;1500 ms, or no response were rejected as outliers from RT analysis (1.56% of trials). The two factors of interest were expectation status (expected vs. unexpected) and attention (objects attended vs. objects unattended task). Thus, a 2 × 2 repeated measures analysis of variance (RM ANOVA) was used to analyze behavioral data, with the additional planned simple main effects analyses of expected vs. unexpected within each task condition using two-sided paired t-tests. For these tests, RT and accuracy data per participant were averaged across trials and subjected to the analyses. For all paired t-tests, the effect size was calculated in terms of Cohen’s <italic>d</italic><sub>z</sub> (<xref ref-type="bibr" rid="bib38">Lakens, 2013</xref>), while partial eta-squared (<italic>η</italic><sup>2</sup>), as implemented in JASP (<xref ref-type="bibr" rid="bib29">JASP Team, 2018</xref>), was used as a measure of effect size for the RM ANOVA. Standard errors of the mean were calculated as the within-subject normalized standard error of the mean (<xref ref-type="bibr" rid="bib10">Cousineau, 2005</xref>) with bias correction (<xref ref-type="bibr" rid="bib44">Morey, 2008</xref>). Data from the pair recognition task were analyzed by means of two-sided paired t-tests or Wilcoxon signed rank test, if the normality assumption was violated, comparing RTs and response accuracies between image pairs belonging to the attended vs. unattended conditions. Effect size for Wilcoxon signed rank test was calculated as the matched rank biserial correlation (r<sub>b</sub>; <xref ref-type="bibr" rid="bib29">JASP Team, 2018</xref>).</p></sec><sec id="s4-7-2"><title>fMRI data preprocessing</title><p>fMRI data were preprocessed using FSL 5.0.11 (FMRIB Software Library; Oxford, UK; <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl">www.fmrib.ox.ac.uk/fsl</ext-link>; <xref ref-type="bibr" rid="bib55">Smith et al., 2004</xref>, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002823">SCR_002823</ext-link>). The preprocessing pipeline consisted of the following steps: brain extraction (BET), motion correction (MCFLIRT), grand mean scaling, temporal high-pass filtering (128 s). For univariate analyses, data were spatially smoothed (Gaussian kernel with full-width at half-maximum of 5 mm), while for multivariate analyses no spatial smoothing was applied. FSL FLIRT was used to register functional images to the anatomical image (BBR) and the anatomical image to the MNI152 T1 2 mm template brain using linear registration (12 degrees of freedom). Registration to the MNI152 standard brain was only applied for whole-brain analyses, while all ROI analyses were performed in each participant’s native space in order to minimize data interpolation.</p></sec><sec id="s4-7-3"><title>fMRI data analysis</title><p>FSL FEAT was used to fit voxel-wise general linear models (GLM) to each participant’s run data in an event-related approach. In these first-level GLMs, expected and unexpected image pair events were modeled as two separate regressors with a duration of one second (the combined duration of leading and trailing image) and convolved with a double gamma haemodynamic response function. An additional regressor of no interest was added to the GLM, modeling the instruction and performance summary screens. Moreover, the first temporal derivatives of these three regressors were added to the GLM. Finally, 24 motion regressors (FSL’s standard + extended set of motion parameters) were added to account for head motion, comprised of the six standard motion parameters, the squares of the six motion parameters, the derivatives of the standard motion parameters and the squares of the derivatives. The contrast of interest, expectation suppression, was defined as the BOLD response to unexpected minus expected images. FSL’s fixed effects analysis was used to combine data across runs. Because each run either used the objects attended or objects unattended (character categorization) task, two separate regressors were used in the fixed effects analysis, one for the objects attended task, one for the objects unattended task. Finally, across participants, data were combined using FSL’s mixed effects analysis (FLAME 1). Gaussian random-field cluster thresholding was used to correct for multiple comparisons, using the updated default settings of FSL 5.0.11, with a cluster formation threshold of p&lt;0.001 (one-sided; that is, <italic>z</italic> ≥ 3.1) and cluster significance threshold of p&lt;0.05.</p></sec><sec id="s4-7-4"><title>Region of interest (ROI) analysis</title><p>ROI analyses were conducted in each participant’s native space. The three a priori defined and preregistered ROIs were V1, object-selective LOC and TOFC. The choice of these ROIs was based on our previous study (<xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>), in which we found significant expectation suppression in these cortical areas. For each ROI the mean parameter estimate was extracted from the participant’s parameter estimate maps, representing the expected and unexpected images. This was done separately for the objects attended and objects unattended tasks, thus resulting in four parameter of interest. The parameter estimates were divided by 100 to yield percent signal change relative to baseline (<xref ref-type="bibr" rid="bib45">Mumford, 2007</xref>). For each ROI, these data were submitted to a 2 × 2 RM ANOVA with expectation (expected, unexpected) and attention (objects attended, objects unattended) as factors. Simple main effects were calculated for the expectation effect in each of the attention conditions using two-sided paired t-tests. As applicable, Cohen’s <italic>d</italic><sub>z</sub> or partial eta-squared (<italic>η</italic><sup>2</sup>) were calculated as measures of effect size. Again, the within-subject normalized standard error of the mean (<xref ref-type="bibr" rid="bib10">Cousineau, 2005</xref>) with bias correction (<xref ref-type="bibr" rid="bib44">Morey, 2008</xref>) was calculated as an indicator of the standard error.</p><p><italic>ROI definition.</italic> All ROIs were preregistered and defined a priori, based on previous results, and refined using independent data. The three ROIs were V1, object selective LOC, and TOFC. V1 was defined based on each participant’s anatomical image, using Freesurfer 6.0 for cortex segmentation (recon-all; <xref ref-type="bibr" rid="bib11">Dale et al., 1999</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001847">SCR_001847</ext-link>). The resulting V1 labels were transformed into native volume space using ‘mri_label2vol’ and merged into one bilateral mask. LOC masks were created in each participant’s native space using data from the functional localizer. Object selective LOC was defined as bilateral clusters, within anatomical LOC, showing a significant preference for intact compared to scrambled object stimuli (<xref ref-type="bibr" rid="bib37">Kourtzi and Kanwisher, 2001</xref>; <xref ref-type="bibr" rid="bib23">Haushofer et al., 2008</xref>). To this end, one regressor modeling intact objects and one regressor modeling scrambled objects were fit to each participant’s localizer data. Additional regressors of no interest were added to the model, with one regressor modeling instruction and performance screens, the temporal derivatives of all regressors, and the 24 motion regressor as also described above (see: <italic>fMRI data analysis</italic>). The contrast of interest, objects minus scrambles, was constrained to anatomical LOC, and the largest contiguous clusters in each hemisphere were extracted per participant. By default, the contrast was thresholded at <italic>z</italic> &gt;= 5 (uncorrected; that is, p&lt;1e-6). The threshold was lowered on a per participant basis if the resulting LOC clusters were too small; that is, bilateral mask with less than 400 voxels in native volume space. The TOFC ROI mask was created using an anatomical temporal-occipital fusiform cortex mask from the Harvard-Oxford cortical atlas (RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001476">SCR_001476</ext-link>), as distributed with FSL. This mask was further constrained to voxels showing a significant expectation suppression effect on the group level in our previous study, using an independent data set (Figure 2A in <xref ref-type="bibr" rid="bib54">Richter et al., 2018</xref>). The resulting mask was transformed from MNI space to each participant’s native space using FSL FLIRT.</p><p>Finally, each of the three ROI masks were constrained to the 300 voxels forming the most informative neighborhoods concerning object identity decoding. This was done by performing a multi-voxel pattern analysis (see: <italic>Multi-voxel pattern analysis (MVPA)</italic>) on the localizer data set per participant, decoding object identity. This ensured that the final masks contained the voxels that were from the most informative neighborhoods in each respective mask. It was not required that the final mask formed one contiguous cluster. In order to verify that our results did not depend on the a priori defined but arbitrary number of voxels in the ROI masks, we repeated all ROI analyses with masks ranging from 100 to 400 voxels (i.e., 800 mm<sup>3</sup> to 3200 mm<sup>3</sup>) in steps of 100 voxels.</p></sec><sec id="s4-7-5"><title>Multi-voxel pattern analysis (MVPA)</title><p>A decoding analysis was performed on each participant’s localizer data. For this analysis, not spatially smoothed mean parameter estimate maps were obtained per localizer trial by fitting a GLM with only one trial as regressor of interest and all remaining trials as one regressor of no interest (<xref ref-type="bibr" rid="bib46">Mumford et al., 2012</xref>). Subsequently, these parameter estimate maps were used in a multi-class, linear SVM-based decoding analysis (SVC function, Scikit-learn; <xref ref-type="bibr" rid="bib48">Pedregosa et al., 2011</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002577">SCR_002577</ext-link>), with the 12 trailing images as classes. The analysis was performed on the localizer data across the whole brain using a searchlight approach (6 mm radius) and stratified 4-fold cross-validation. Finally, the resulting decoding accuracy maps were used to constrain the ROI masks (see <italic>ROI definition</italic>).</p><p>We employed a similar decoding analysis to determine whether object-specific neural activity in the visual ventral stream was equally present during both the objects attended and unattended tasks. As above, a multi-class decoder with linear SVMs was used to decode object images. The per trial parameter estimates of the localizer run served as training data. For each main task run voxel-wise GLMs were fit with a regressor for each trailing image per expectation condition. As in the other fMRI analyses, the 24 motion regressors and temporal derivatives were added to the model (see <italic>fMRI data analysis</italic>). Finally, the decoder was tested on the obtained trailing image parameter estimates per run. As each attention condition consisted of six trailing images, chance performance of this decoder was at 16.7% (1/6).</p></sec><sec id="s4-7-6"><title>Stimulus specificity analysis</title><p>In an effort to further explore the nature of expectation suppression throughout the ventral visual stream, we investigated the stimulus specificity of the suppression effect. The key question here was if expectation suppression was primarily present in stimulus-driven voxels within a given area, or whether most voxels in an area showed the effect, regardless of whether or not they were stimulus-driven.</p><p>In order to investigate specificity, we obtained anatomically defined masks of our three ROIs (V1, LOC, TOFC). For V1 the unconstrained, anatomically defined Freesurfer V1 mask was used (see <italic>ROI definition</italic>). Anatomical LOC and TOFC were defined using the Harvard-Oxford cortical atlas. FSL FAST was used to obtain a gray matter mask for each participant based on their anatomical scan. Masks were transformed to the participant’s native EPI space. Next, the three ROI masks were constrained to the participant’s gray matter voxels. Within the resulting ROI masks, using the contrast object stimuli compared to baseline from the functional localizer run, voxels were split into two categories, stimulus-driven (<italic>z</italic> &gt; 1.96; that is, p&lt;0.05, two-sided), and not stimulus-driven, but also not deactivated, voxels (−1.96 &lt; <italic>z</italic> &lt; 1.96). Average expectation suppression was compared between ROIs split into stimulus-driven vs. not stimulus-driven voxels. Thus, a 3 × 2 RM ANOVA with ROI (V1, LOC, TOFC) and stimulus-driven (stimulus-driven vs not stimulus-driven) as factors was used for analysis. Greenhouse-Geisser correction was applied, if Mauchly's sphericity test indicated a violation of the sphericity assumption. Furthermore, the simple main effect of stimulus-driven vs. not stimulus-driven was assessed within each ROI. Additionally, to test for the presence of any expectation suppression, the amount of suppression was compared against zero using one sample t-tests.</p></sec><sec id="s4-7-7"><title>Pupillometry</title><p>In order to investigate whether pupil dilation effects accompany expectation suppression, we analyzed the pupil diameter data recorded during MRI scanning. A priori, two participants were rejected from this analysis, as the experiment log book indicated that pupil diameter data were unreliable for these two participants, leaving 32 participants for pupillometry. First, blinks were detected using a velocity based method, following the procedure outlined by <xref ref-type="bibr" rid="bib41">Mathôt (2013)</xref>. A blink was defined as a negative velocity peak (eyes closing), followed by a positive velocity peak (eyes opening) within a time period of 500 ms. The velocity threshold was set to 5 (arbitrary units). An additional 100 ms were added as padding before and after the detected blink onset and offset. If padding resulted in overlapping blink windows, consecutive blinks were considered as one long blink. Linear interpolation was used to replace missing data during blinks (18.05% of data). Note, this number includes the padding, and all time periods of no interest, such as null events, instruction and performance screens, as well as recording periods before and after MRI run onset; that is, periods during which participants were free to close their eyes. Remaining missing data, not following a typical blink profile, were excluded from analysis, again adding a padding of 100 ms (3.07% of data). Similarly, outlier data with implausible velocity profiles were also rejected from the analysis, using the same velocity-based threshold as for blink detection but without the criterion of a negative peak followed by a positive peak (5.30% of data). Thus, data interpolation was only applied for short time intervals, which represent a clear blink, in order to avoid interpolation based on artifacts or over exceedingly long time periods. Finally, pupil data were smoothed using a Hanning window of 200 ms, and epoched into trials from 1 s before trailing image onset to 4 s after trailing image onset. The data of each trial were baseline corrected by diving the pupil diameter estimates by the mean diameter during the baseline period, 0.5 to 0 s before leading image onset. As a final data quality check, all trials exceeding pupil diameter values 7 SDs above the mean pupil diameter were rejected (3.01% trials). Trials with expected trailing images and unexpected trailing images were averaged separately for each participant. The difference between unexpected minus expected was subjected to a cluster-based permutation test (100,000 permutations; two-sided p&lt;0.05; cluster formation threshold p&lt;0.05) in order to assess statistical significance. Data from the objects attended and the objects unattended tasks were analyzed separately.</p></sec></sec><sec id="s4-8"><title>Linking pupil and neural measures</title><p>In an exploratory analysis we sought to provide additional evidence for an association between pupil dilation and expectation suppression. To this end, we correlated expectation suppression with pupil dilation differences between expected and unexpected objects per trailing image. First, we obtained per trailing image parameter estimates by fitting a voxel-wise GLM to the fMRI data for each run, following the same procedure as for the main fMRI data analysis, outlined in <italic>fMRI data analysis</italic> and <italic>Region of interest (ROI) analysis</italic>. The only difference was that a separate regressor per trailing image and expectation condition was fit, thus resulting in a model with 12 regressors of interest (six trailing images * two expectation conditions). As before, data were combined across runs using FSL’s fixed effect analysis. The resulting parameter estimate maps were extracted for each ROI (V1, LOC, TOFC) and converted to percent signal change. Finally, for each participant we calculated expectation suppression for each trailing image (expectation suppression = BOLD<sub>unexpected</sub> – BOLD<sub>expected</sub>). Similarly, we calculated the difference in pupil dilation between unexpected and expected occurrences of each trailing image. For this we extracted the preprocessed (see: <italic>Pupillometry</italic>) pupil size estimates for each trial and calculated the mean pupil size within the time window that showed a significant difference in pupil dilation between unexpected compared to expected attended stimuli on the group level (<xref ref-type="fig" rid="fig3">Figure 3</xref>, left panel); that is, 1.52 to 2.88 s after trailing image onset. Next, we calculated the average difference in pupil size for each trailing image for unexpected compared to expected occurrences, thus yielding six pupil size difference scores (unexpected – expected) for both attention tasks per participant. Spearman’s rank correlation was then used to estimate the correlation between the pupil dilation differences and expectation suppression magnitudes for each participant. Therefore, this correlation expresses the correlation in ranks of pupil dilation differences and expectation suppression magnitude for the trailing images, with positive correlations indicating that trailing images with large expectation suppression effects are also associated with larger pupil dilation differences. The obtained correlation coefficients were Fisher z-transformed and compared against zero (no correlation) using one-sample t-tests for each ROI and attention condition. We also submitted the coefficients to a repeated measures ANOVA with ROI and attention as factors.</p></sec><sec id="s4-9"><title>Linking behavioral and neural measures</title><p>In another exploratory analysis we investigated the relationship between behavioral and neural benefits of expectations by correlating expectation suppression with the behavioral RT benefit for expected stimuli observed during MRI scanning. First, we calculated the RT benefit for each trailing image during the main fMRI task (RT<sub>benefit</sub> = RT<sub>unexpected</sub> – RT<sub>expected</sub>, per trailing image). Within each ROI we then correlated expectation suppression per trailing image (see: <italic>Linking pupil and neural measures</italic>) with RT benefit per trailing image using Spearman's rank correlation. Thus, this correlation coefficient indicates the rank correlation of expectation induced RT benefits and expectation suppression magnitude for the different trailing images. For statistical inference across participants, we Fisher z-transformed the correlation coefficients, and tested whether the observed correlation coefficients differ from zero (no correlation) in each condition by performing one-sample t-tests for each ROI and attention task separately. Finally, we also compared the magnitude of the correlations between ROIs and attention tasks using a 3 × 2 repeated measures ANOVA with ROI and attention condition (task) as factors.</p></sec><sec id="s4-10"><title>Bayesian analyses</title><p>In order to further evaluate any non-significant tests, in particular simple main effects, we performed the Bayesian equivalents of the above outlined analyses. JASP 0.9.0.1 (<xref ref-type="bibr" rid="bib29">JASP Team, 2018</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_015823">SCR_015823</ext-link>) was used to perform all Bayesian analyses, using default settings. Thus, for Bayesian t-tests a Cauchy prior width of 0.707 was chosen. Qualitative interpretations of Bayes Factors are based on criteria by <xref ref-type="bibr" rid="bib40">Lee and Wagenmakers (2013)</xref>.</p></sec><sec id="s4-11"><title>Software</title><p>MRI data preprocessing and analysis was performed using FSL 5.0.11 (FMRIB Software Library; Oxford, UK; <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl">www.fmrib.ox.ac.uk/fsl</ext-link>; <xref ref-type="bibr" rid="bib55">Smith et al., 2004</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002823">SCR_002823</ext-link>). Custom Python 2.7.13 (Python Software Foundation, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008394">SCR_008394</ext-link>) scripts were used for additional analyses, data handling, statistical tests and data visualization. The following Python libraries and toolboxes were used: NumPy 1.12.1 (<xref ref-type="bibr" rid="bib59">van der Walt et al., 2011</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008633">SCR_008633</ext-link>), SciPy 0.19.0 (<xref ref-type="bibr" rid="bib30">Jones et al., 2001</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008058">SCR_008058</ext-link>), Matplotlib 1.5.1 (<xref ref-type="bibr" rid="bib27">Hunter, 2007</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_008624">SCR_008624</ext-link>), Statsmodels 0.8.0 (<ext-link ext-link-type="uri" xlink:href="http://www.statsmodels.org">www.statsmodels.org</ext-link>) and Scikit-learn 0.18.1 (<xref ref-type="bibr" rid="bib48">Pedregosa et al., 2011</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002577">SCR_002577</ext-link>). Additionally, Slice Display (<xref ref-type="bibr" rid="bib64">Zandbelt, 2017</xref>), a MATLAB 2017a (The MathWorks, Inc, Natick, Massachusetts, United States, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_001622">SCR_001622</ext-link>) data visualization toolbox, was used for displaying whole-brain results. JASP 0.9.0.1 (<xref ref-type="bibr" rid="bib29">JASP Team, 2018</xref>; RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_015823">SCR_015823</ext-link>) was used for Bayesian analyses and RM ANOVAs. Stimuli were presented using Presentation software (version 18.3, Neurobehavioral Systems, Inc, Berkeley, CA, RRID:<ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/SCR_002521">SCR_002521</ext-link>).</p></sec><sec id="s4-12"><title>Supplemental analyses</title><sec id="s4-12-1"><title>Pupil dilation is associated with larger BOLD responses</title><p>In order to provide additional support for the hypothesis that pupil dilation differences may partially underlie expectation suppression in V1, we examined the relationship between pupil dilation and the BOLD response. First, we extracted per trial pupil size data and parameter estimate maps from the fMRI main task data for V1. Pupil size data were preprocessed as described in <italic>Pupillometry</italic>, and extracted from a three-second time window, starting with trailing image onset and ending 2.5 s after trailing image offset; thus, the time window covered the full duration shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> after trailing image onset. fMRI data were preprocessed as outlined in <italic>fMRI data preprocessing</italic>. Next, for each trial we fitted a GLM with only one trial as regressor of interest and all remaining trials as regressors of no interest (<xref ref-type="bibr" rid="bib46">Mumford et al., 2012</xref>). Per participant, we extracted the per trial parameter estimate maps, averaged within the V1 ROI, and z scored the mean parameter estimates per condition separately in order to remove potential effects of mean differences between the conditions. We also z scored the pupil size estimates per condition for the same reason. Next, we fitted per participant a GLM with the mean BOLD parameter estimates (one per trial) as predicted variable and a regressor with pupil size for each expectation and attention condition combination (i.e., four regressors of interest) as predictors. Statistical inference across subjects was performed by subjecting the thus obtained parameter estimates of the four regressors of interest to a 2 × 2 repeated measures ANOVA, as with our main ROI analysis; that is, with attention and expectation as factors. Furthermore, in order to assess whether the BOLD response was influenced by pupil dilation at all we performed one-sample t-tests comparing the obtained parameter estimates against zero for each condition separately. Additionally, we performed a similar analysis, but split the fMRI data into stimulus-driven vs. non-stimulus-driven V1 gray matter voxels (see <italic>Stimulus specificity analysis</italic> for details on the ROI mask creation). This analysis thus results in a 2 × 2 × 2 repeated measures ANOVA with expectation, attention and stimulus-responsiveness as factors.</p><p>Increased pupil dilations were associated with larger BOLD responses regardless of whether stimuli were attended and expected (attended expected: <italic>t</italic><sub>(31)</sub> = 3.006, p=0.005, <italic>d</italic><sub>z</sub> = 0.531; attended unexpected: <italic>t</italic><sub>(31)</sub> = 4.392, p=1.2e-4, <italic>d</italic><sub>z</sub> = 0.776; unattended expected: <italic>t</italic><sub>(31)</sub> = 5.228, p=1.1e-5, <italic>d</italic><sub>z</sub> = 0.924; unattended unexpected: <italic>W</italic> = 452, p=2.1e-4, r<sub>B</sub> = 0.712). Results are shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. Pupil dilation led to slightly stronger BOLD increases when objects were unattended than attended (<italic>F</italic><sub>(1,31)</sub> = 5.563, p=0.025, <italic>η</italic>²=0.152), but independent of whether stimuli were expected or unexpected (<italic>F</italic><sub>(1,31)</sub> = 0.054, p=0.817, <italic>η</italic>²=0.002; interaction: <italic>F</italic><sub>(1,31)</sub> = 2.261, p=0.143, <italic>η</italic>²=0.068). Thus, pupil dilation had a positive effect on overall BOLD responses in V1.</p></sec><sec id="s4-12-2"><title>Pupil dilation influences BOLD responses more in non-stimulus-driven V1 voxels</title><p>Next, we assessed whether the same association would hold in stimulus-driven and non-stimulus driven V1 voxels. <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> shows that there was indeed a reliable, positive association between BOLD responses and pupil dilation within both stimulus-driven and non-stimulus-driven voxels. Again, larger pupil dilations were predictive of enhanced BOLD responses when object stimuli were attended and unattended, as well as for expected and unexpected objects (all t-tests p&lt;0.05). Interestingly, this association was somewhat larger in non-stimulus-driven than stimulus-driven voxels (<italic>F</italic><sub>(1,31)</sub> = 9.267, p=0.005, <italic>η</italic>²=0.230), suggesting that the association between BOLD and pupil dilation is particularly strong for those neural populations that are not driven by our object stimuli. This is in line with earlier observations that non-stimulus-driven activations (possibly reflecting neuromodulation) are greater in regions that represent more peripheral parts of the visual field (<xref ref-type="bibr" rid="bib28">Jack et al., 2006</xref>). There was also a stronger association of pupil dilation and BOLD responses when objects were unattended (<italic>F</italic><sub>(1,31)</sub> = 5.042, p=0.032, <italic>η</italic>²=0.140), but the magnitude of the association was not affected by whether a stimulus was expected or not (<italic>F</italic><sub>(1,31)</sub> = 0.008, p=0.928, <italic>η</italic>²=2.6e-4). Moreover, no interaction effect was observed (all interactions p&gt;0.1).</p><p>Thus, to summarize, our results show that pupil dilation has a substantial, positive association with V1 BOLD responses, regardless of whether stimuli were attended and expected, for both stimulus-driven and non-stimulus-driven neural populations. This result is expected, given that pupil dilation has been related to other processes known to correlate with BOLD responses such as mental effort, arousal and attention (for a review see: <xref ref-type="bibr" rid="bib42">Mathôt, 2018</xref>). Moreover, increases in retinal illumination due to larger pupil dilation can also result in increased BOLD activity (<xref ref-type="bibr" rid="bib24">Haynes et al., 2004</xref>). These results support our suggestion that larger pupil dilations in response to unexpected stimuli, possibly reflecting general arousal mechanisms, may partially account for expectation suppression in V1. However, it should also be noted that the association between pupil dilation and the BOLD response is not solely observed when objects were attended, as pupil dilation is likely a general reflection of vigilance and arousal (<xref ref-type="bibr" rid="bib52">Reimer et al., 2014</xref>; <xref ref-type="bibr" rid="bib60">Vinck et al., 2015</xref>), which is expected to fluctuate also when the objects are not attended. That this association is more pronounced in non-stimulus-driven voxels, further supports the possibility that expectation suppression in V1, including the suppression observed in non-stimulus-driven voxels, may partially reflect non-perceptual effects such as arousal changes, which are reflected by larger pupil dilations in response to surprising stimuli.</p></sec><sec id="s4-12-3"><title>No differences in pupil dilation during baseline</title><p>We assessed pupil size during baseline to ensure that differences in pupil dilation between expectation conditions or attention tasks do not simply reflect difference in baseline (e.g., pre-stimulus arousal differences). Pupil data were preprocessed using the same pipeline as described in <italic>Pupillometry</italic>, except for that pupil size was extracted in raw units during the baseline period. Per participant, pupil size was then averaged for each attention and expectation condition separately. Mean pupil estimates were then compared between conditions using a 2 × 2 repeated measures ANOVA, with expectation and attention as factors. Additionally, a Bayesian repeated measure ANOVA was conducted to quantify the evidence for the absence of a difference in pupil size during baseline.</p><p>Results showed that there was no difference in baseline pupil size before attended compared to unattended stimuli (<italic>F</italic><sub>(1,31)</sub> = 5.226, p=0.484, <italic>η</italic>²=0.016, BF<sub>inclusion</sub> = 0.254), nor before expected compared to unexpected stimuli (<italic>F</italic><sub>(1,31)</sub> = 0.001, p=0.926, <italic>η</italic>²=2.8e-4, BF<sub>inclusion</sub> = 0.136; interaction: <italic>F</italic><sub>(1,31)</sub> = 6.2e-4, p=0.955, <italic>η</italic>²=1.0e-4, BF<sub>inclusion</sub> = 0.042). <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref> shows the pupil size in raw units during the baseline period. Thus, data suggest that pupil size, and thereby likely arousal, during baseline was of a similar magnitude during both attention tasks and expectation conditions, thereby rendering an explanation of the observed phasic differences in pupil size based on differences in baseline pupil size unlikely.</p></sec></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Matthias Ekman, Mariya Manahova, Peter Kok and Karl Friston for helpful comments and discussions of the manuscript and results. We thank José Marques for assistance with MR sequence questions.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing—original draft, Project administration, Writing—review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Supervision, Funding acquisition, Writing—review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The study followed institutional guidelines of the local ethics committee (CMO region Arnhem-Nijmegen, The Netherlands; Protocol CMO2014/288), including informed consent of all participants.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><object-id pub-id-type="doi">10.7554/eLife.47869.016</object-id><label>Supplementary file 1.</label><caption><title>Overview of expectation suppression across cortex.</title><p>Brain areas showing significant expectation suppression (GRF cluster corrected). Listed are significant clusters with their respective area label, MNI coordinate of the peak z value, the number of voxels in the cluster, as well as the p value of the cluster and its max z statistic. For large clusters (n voxels &gt;700) additional local z maxima (<italic>z</italic> &gt; 3.72; that is, p&lt;0.0001, one-sided) are also shown with area label, MNI coordinates and max z statistic. Unexp. = unexpected image pairs; Exp. = expected image pairs; Att. = objects attended task; Unatt. = objects unattended (characters attended) task.</p></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-47869-supp1-v2.docx"/></supplementary-material><supplementary-material id="transrepform"><object-id pub-id-type="doi">10.7554/eLife.47869.017</object-id><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-47869-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data and code necessary to replicate the reported results are available via the following URL: <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/11633/aacg3rkw">http://hdl.handle.net/11633/aacg3rkw</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Richter</surname><given-names>D</given-names></name><name><surname>de</surname><given-names>Lange FP</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Attentional modulation of perceptual predictions</data-title><source>Donders Institute</source><pub-id assigning-authority="other" pub-id-type="archive" xlink:href="http://hdl.handle.net/11633/aacg3rkw">11633/aacg3rkw</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alink</surname> <given-names>A</given-names></name><name><surname>Schwiedrzik</surname> <given-names>CM</given-names></name><name><surname>Kohler</surname> <given-names>A</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name><name><surname>Muckli</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Stimulus predictability reduces responses in primary visual cortex</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>2960</fpage><lpage>2966</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3730-10.2010</pub-id><pub-id pub-id-type="pmid">20181593</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aron</surname> <given-names>AR</given-names></name><name><surname>Fletcher</surname> <given-names>PC</given-names></name><name><surname>Bullmore</surname> <given-names>ET</given-names></name><name><surname>Sahakian</surname> <given-names>BJ</given-names></name><name><surname>Robbins</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Stop-signal inhibition disrupted by damage to right inferior frontal gyrus in humans</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>115</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/nn1003</pub-id><pub-id pub-id-type="pmid">12536210</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aron</surname> <given-names>AR</given-names></name><name><surname>Robbins</surname> <given-names>TW</given-names></name><name><surname>Poldrack</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Inhibition and the right inferior frontal cortex</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>170</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.02.010</pub-id><pub-id pub-id-type="pmid">15050513</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aston-Jones</surname> <given-names>G</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>An integrative theory of locus coeruleus-norepinephrine function: adaptive gain and optimal performance</article-title><source>Annual Review of Neuroscience</source><volume>28</volume><fpage>403</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.28.061604.135709</pub-id><pub-id pub-id-type="pmid">16022602</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berridge</surname> <given-names>CW</given-names></name><name><surname>Schmeichel</surname> <given-names>BE</given-names></name><name><surname>España</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Noradrenergic modulation of wakefulness/arousal</article-title><source>Sleep Medicine Reviews</source><volume>16</volume><fpage>187</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1016/j.smrv.2011.12.003</pub-id><pub-id pub-id-type="pmid">22296742</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertels</surname> <given-names>J</given-names></name><name><surname>Franco</surname> <given-names>A</given-names></name><name><surname>Destrebecqz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How implicit is visual statistical learning?</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>38</volume><fpage>1425</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1037/a0027210</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname> <given-names>TF</given-names></name><name><surname>Konkle</surname> <given-names>T</given-names></name><name><surname>Alvarez</surname> <given-names>GA</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual long-term memory has a massive storage capacity for object details</article-title><source>PNAS</source><volume>105</volume><fpage>14325</fpage><lpage>14329</lpage><pub-id pub-id-type="doi">10.1073/pnas.0803390105</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname> <given-names>TF</given-names></name><name><surname>Oliva</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Statistical learning using real-world scenes: extracting categorical regularities without conscious intent</article-title><source>Psychological Science</source><volume>19</volume><fpage>678</fpage><lpage>685</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brass</surname> <given-names>M</given-names></name><name><surname>Haggard</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The hidden side of intentional action: the role of the anterior insular cortex</article-title><source>Brain Structure and Function</source><volume>214</volume><fpage>603</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1007/s00429-010-0269-6</pub-id><pub-id pub-id-type="pmid">20512363</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cousineau</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Confidence intervals in within-subject designs: a simpler solution to Loftus and Masson's method</article-title><source>Tutorials in Quantitative Methods for Psychology</source><volume>1</volume><fpage>42</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.20982/tqmp.01.1.p042</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname> <given-names>AM</given-names></name><name><surname>Fischl</surname> <given-names>B</given-names></name><name><surname>Sereno</surname> <given-names>MI</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Cortical surface-based analysis. I. segmentation and surface reconstruction</article-title><source>NeuroImage</source><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Damsma</surname> <given-names>A</given-names></name><name><surname>van Rijn</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pupillary response indexes the metrical hierarchy of unattended rhythmic violations</article-title><source>Brain and Cognition</source><volume>111</volume><fpage>95</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.bandc.2016.10.004</pub-id><pub-id pub-id-type="pmid">27816784</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname> <given-names>FP</given-names></name><name><surname>Heilbron</surname> <given-names>M</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How do expectations shape perception?</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.06.002</pub-id><pub-id pub-id-type="pmid">30122170</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>den Ouden</surname> <given-names>HE</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>McIntosh</surname> <given-names>AR</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A dual role for prediction error in associative learning</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>1175</fpage><lpage>1185</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn161</pub-id><pub-id pub-id-type="pmid">18820290</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>den Ouden</surname> <given-names>HE</given-names></name><name><surname>Daunizeau</surname> <given-names>J</given-names></name><name><surname>Roiser</surname> <given-names>J</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name><name><surname>Stephan</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Striatal prediction error modulates cortical coupling</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>3210</fpage><lpage>3219</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4458-09.2010</pub-id><pub-id pub-id-type="pmid">20203180</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donner</surname> <given-names>TH</given-names></name><name><surname>Sagi</surname> <given-names>D</given-names></name><name><surname>Bonneh</surname> <given-names>YS</given-names></name><name><surname>Heeger</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Opposite neural signatures of motion-induced blindness in human dorsal and ventral visual cortex</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>10298</fpage><lpage>10310</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2371-08.2008</pub-id><pub-id pub-id-type="pmid">18842889</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egner</surname> <given-names>T</given-names></name><name><surname>Monti</surname> <given-names>JM</given-names></name><name><surname>Summerfield</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Expectation and surprise determine neural population responses in the ventral visual stream</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>16601</fpage><lpage>16608</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2770-10.2010</pub-id><pub-id pub-id-type="pmid">21147999</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekman</surname> <given-names>M</given-names></name><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Time-compressed preplay of anticipated events in human primary visual cortex</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>15276</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15276</pub-id><pub-id pub-id-type="pmid">28534870</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname> <given-names>H</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attention, uncertainty, and free-energy</article-title><source>Frontiers in Human Neuroscience</source><volume>4</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.3389/fnhum.2010.00215</pub-id><pub-id pub-id-type="pmid">21160551</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname> <given-names>J</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Statistical learning of higher-order temporal structure from visual shape sequences</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>28</volume><fpage>458</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.28.3.458</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gavornik</surname> <given-names>JP</given-names></name><name><surname>Bear</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learned spatiotemporal sequence recognition and prediction in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>732</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.1038/nn.3683</pub-id><pub-id pub-id-type="pmid">24657967</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haushofer</surname> <given-names>J</given-names></name><name><surname>Livingstone</surname> <given-names>MS</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multivariate patterns in object-selective cortex dissociate perceptual and physical shape similarity</article-title><source>PLOS Biology</source><volume>6</volume><elocation-id>e187</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060187</pub-id><pub-id pub-id-type="pmid">18666833</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname> <given-names>JD</given-names></name><name><surname>Lotto</surname> <given-names>RB</given-names></name><name><surname>Rees</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Responses of human visual cortex to uniform surfaces</article-title><source>PNAS</source><volume>101</volume><fpage>4286</fpage><lpage>4291</lpage><pub-id pub-id-type="doi">10.1073/pnas.0307948101</pub-id><pub-id pub-id-type="pmid">15010538</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horstmann</surname> <given-names>G</given-names></name><name><surname>Herwig</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Surprise attracts the eyes and binds the gaze</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>22</volume><fpage>743</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.3758/s13423-014-0723-1</pub-id><pub-id pub-id-type="pmid">25199467</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname> <given-names>RH</given-names></name><name><surname>Aslin</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Statistical learning in a serial reaction time task: access to separable statistical cues by individual learners</article-title><source>Journal of Experimental Psychology: General</source><volume>130</volume><fpage>658</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.130.4.658</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Matplotlib: a 2D graphics environment</article-title><source>Computing in Science &amp; Engineering</source><volume>9</volume><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jack</surname> <given-names>AI</given-names></name><name><surname>Shulman</surname> <given-names>GL</given-names></name><name><surname>Snyder</surname> <given-names>AZ</given-names></name><name><surname>McAvoy</surname> <given-names>M</given-names></name><name><surname>Corbetta</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Separate modulations of human V1 associated with spatial attention and task structure</article-title><source>Neuron</source><volume>51</volume><fpage>135</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.003</pub-id><pub-id pub-id-type="pmid">16815338</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="software"><person-group person-group-type="author"><collab>JASP Team</collab></person-group><year iso-8601-date="2018">2018</year><data-title>JASP</data-title><source>Computer Software</source><version designator="0.9.0.1">0.9.0.1</version></element-citation></ref><ref id="bib30"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>E</given-names></name><name><surname>Oliphant</surname> <given-names>E</given-names></name><name><surname>Peterson</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>SciPy Open Source Scientific Tools for Python</source><ext-link ext-link-type="uri" xlink:href="https://www.scipy.org/">https://www.scipy.org/</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaposvari</surname> <given-names>P</given-names></name><name><surname>Kumar</surname> <given-names>S</given-names></name><name><surname>Vogels</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Statistical learning signals in macaque inferior temporal cortex</article-title><source>Cerebral Cortex</source><volume>28</volume><fpage>250</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw374</pub-id><pub-id pub-id-type="pmid">27909007</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname> <given-names>R</given-names></name><name><surname>Seitz</surname> <given-names>A</given-names></name><name><surname>Feenstra</surname> <given-names>H</given-names></name><name><surname>Shams</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Testing assumptions of statistical learning: is it long-term and implicit?</article-title><source>Neuroscience Letters</source><volume>461</volume><fpage>145</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2009.06.030</pub-id><pub-id pub-id-type="pmid">19539701</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kloosterman</surname> <given-names>NA</given-names></name><name><surname>Meindertsma</surname> <given-names>T</given-names></name><name><surname>van Loon</surname> <given-names>AM</given-names></name><name><surname>Lamme</surname> <given-names>VA</given-names></name><name><surname>Bonneh</surname> <given-names>YS</given-names></name><name><surname>Donner</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Pupil size tracks perceptual content and surprise</article-title><source>European Journal of Neuroscience</source><volume>41</volume><fpage>1068</fpage><lpage>1078</lpage><pub-id pub-id-type="doi">10.1111/ejn.12859</pub-id><pub-id pub-id-type="pmid">25754528</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><volume>75</volume><fpage>265</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Rahnev</surname> <given-names>D</given-names></name><name><surname>Jehee</surname> <given-names>JF</given-names></name><name><surname>Lau</surname> <given-names>HC</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Attention reverses the effect of prediction in silencing sensory signals</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>2197</fpage><lpage>2206</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr310</pub-id><pub-id pub-id-type="pmid">22047964</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname> <given-names>P</given-names></name><name><surname>Bains</surname> <given-names>LJ</given-names></name><name><surname>van Mourik</surname> <given-names>T</given-names></name><name><surname>Norris</surname> <given-names>DG</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Selective activation of the deep layers of the human primary visual cortex by Top-Down feedback</article-title><source>Current Biology</source><volume>26</volume><fpage>371</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.12.038</pub-id><pub-id pub-id-type="pmid">26832438</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname> <given-names>Z</given-names></name><name><surname>Kanwisher</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Representation of perceived object shape by the human lateral occipital complex</article-title><source>Science</source><volume>293</volume><fpage>1506</fpage><lpage>1509</lpage><pub-id pub-id-type="doi">10.1126/science.1061133</pub-id><pub-id pub-id-type="pmid">11520991</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakens</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs</article-title><source>Frontiers in Psychology</source><volume>4</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00863</pub-id><pub-id pub-id-type="pmid">24324449</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsson</surname> <given-names>J</given-names></name><name><surname>Smith</surname> <given-names>AT</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>fMRI repetition suppression: neuronal adaptation or stimulus expectation?</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>567</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr119</pub-id><pub-id pub-id-type="pmid">21690262</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lee</surname> <given-names>MD</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Bayesian Cognitive Modeling: A Practical Course</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781139087759</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="data"><person-group person-group-type="author"><name><surname>Mathôt</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><data-title>A simple way to reconstruct pupil size during eye blinks</data-title><source>Figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.688002</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathôt</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pupillometry: psychology, physiology, and function</article-title><source>Journal of Cognition</source><volume>1</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.5334/joc.18</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>T</given-names></name><name><surname>Olson</surname> <given-names>CR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Statistical learning of visual transitions in monkey inferotemporal cortex</article-title><source>PNAS</source><volume>108</volume><fpage>19401</fpage><lpage>19406</lpage><pub-id pub-id-type="doi">10.1073/pnas.1112895108</pub-id><pub-id pub-id-type="pmid">22084090</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morey</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Confidence intervals from normalized data: a correction to Cousineau (2005)</article-title><source>Tutorials in Quantitative Methods for Psychology</source><volume>4</volume><fpage>61</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.20982/tqmp.04.2.p061</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Mumford</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A guide to calculating percent change with featquery</article-title><ext-link ext-link-type="uri" xlink:href="http://mumford.fmripower.org/perchange_guide.pdf">http://mumford.fmripower.org/perchange_guide.pdf</ext-link><date-in-citation iso-8601-date="2017-08-21">August 21, 2017</date-in-citation></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mumford</surname> <given-names>JA</given-names></name><name><surname>Turner</surname> <given-names>BO</given-names></name><name><surname>Ashby</surname> <given-names>FG</given-names></name><name><surname>Poldrack</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Deconvolving BOLD activation in event-related designs for multivoxel pattern classification analyses</article-title><source>NeuroImage</source><volume>59</volume><fpage>2636</fpage><lpage>2643</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.08.076</pub-id><pub-id pub-id-type="pmid">21924359</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname> <given-names>SM</given-names></name><name><surname>Dosenbach</surname> <given-names>NU</given-names></name><name><surname>Cohen</surname> <given-names>AL</given-names></name><name><surname>Wheeler</surname> <given-names>ME</given-names></name><name><surname>Schlaggar</surname> <given-names>BL</given-names></name><name><surname>Petersen</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Role of the anterior insula in task-level control and focal attention</article-title><source>Brain Structure and Function</source><volume>214</volume><fpage>669</fpage><lpage>680</lpage><pub-id pub-id-type="doi">10.1007/s00429-010-0260-2</pub-id><pub-id pub-id-type="pmid">20512372</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname> <given-names>F</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name><name><surname>Gramfort</surname> <given-names>A</given-names></name><name><surname>Michel</surname> <given-names>V</given-names></name><name><surname>Thirion</surname> <given-names>B</given-names></name><name><surname>Grisel</surname> <given-names>O</given-names></name><name><surname>Blondel</surname> <given-names>M</given-names></name><name><surname>Prettenhofer</surname> <given-names>P</given-names></name><name><surname>Weiss</surname> <given-names>R</given-names></name><name><surname>Dubourg</surname> <given-names>V</given-names></name><name><surname>Vanderplas</surname> <given-names>J</given-names></name><name><surname>Passos</surname> <given-names>A</given-names></name><name><surname>Cournapeau</surname> <given-names>D</given-names></name><name><surname>Brucher</surname> <given-names>M</given-names></name><name><surname>Perrot</surname> <given-names>M</given-names></name><name><surname>É</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in Python</article-title><source>Journal of Machine Learning Research : JMLR</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preuschoff</surname> <given-names>K</given-names></name><name><surname>'t Hart</surname> <given-names>BM</given-names></name><name><surname>Einhäuser</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pupil dilation signals surprise: evidence for noradrenaline's Role in Decision Making</article-title><source>Frontiers in Neuroscience</source><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.3389/fnins.2011.00115</pub-id><pub-id pub-id-type="pmid">21994487</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RPN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Bayesian inference and attentional modulation in the visual cortex</article-title><source>Neuroreport</source><volume>16</volume><fpage>3</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1097/01.wnr.0000183900.92901.fc</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname> <given-names>RP</given-names></name><name><surname>Ballard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reimer</surname> <given-names>J</given-names></name><name><surname>Froudarakis</surname> <given-names>E</given-names></name><name><surname>Cadwell</surname> <given-names>CR</given-names></name><name><surname>Yatsenko</surname> <given-names>D</given-names></name><name><surname>Denfield</surname> <given-names>GH</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pupil fluctuations track fast switching of cortical states during quiet wakefulness</article-title><source>Neuron</source><volume>84</volume><fpage>355</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.033</pub-id><pub-id pub-id-type="pmid">25374359</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reimer</surname> <given-names>J</given-names></name><name><surname>McGinley</surname> <given-names>MJ</given-names></name><name><surname>Liu</surname> <given-names>Y</given-names></name><name><surname>Rodenkirch</surname> <given-names>C</given-names></name><name><surname>Wang</surname> <given-names>Q</given-names></name><name><surname>McCormick</surname> <given-names>DA</given-names></name><name><surname>Tolias</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Pupil fluctuations track rapid changes in adrenergic and cholinergic activity in cortex</article-title><source>Nature Communications</source><volume>7</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/ncomms13289</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richter</surname> <given-names>D</given-names></name><name><surname>Ekman</surname> <given-names>M</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Suppressed sensory response to predictable object stimuli throughout the ventral visual stream</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7452</fpage><lpage>7461</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3421-17.2018</pub-id><pub-id pub-id-type="pmid">30030402</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>SM</given-names></name><name><surname>Jenkinson</surname> <given-names>M</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Beckmann</surname> <given-names>CF</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Johansen-Berg</surname> <given-names>H</given-names></name><name><surname>Bannister</surname> <given-names>PR</given-names></name><name><surname>De Luca</surname> <given-names>M</given-names></name><name><surname>Drobnjak</surname> <given-names>I</given-names></name><name><surname>Flitney</surname> <given-names>DE</given-names></name><name><surname>Niazy</surname> <given-names>RK</given-names></name><name><surname>Saunders</surname> <given-names>J</given-names></name><name><surname>Vickers</surname> <given-names>J</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>De Stefano</surname> <given-names>N</given-names></name><name><surname>Brady</surname> <given-names>JM</given-names></name><name><surname>Matthews</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><volume>23</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Stokes</surname> <given-names>M</given-names></name><name><surname>Anderson</surname> <given-names>M</given-names></name><name><surname>Chandrasekar</surname> <given-names>S</given-names></name><name><surname>Motta</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A standard default color space for the internet – sRGB</article-title><source>w3.org</source><ext-link ext-link-type="uri" xlink:href="https://www.w3.org/Graphics/Color/sRGB">https://www.w3.org/Graphics/Color/sRGB</ext-link><date-in-citation iso-8601-date="2019-07-26">July 26, 2019</date-in-citation></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname> <given-names>C</given-names></name><name><surname>Trittschuh</surname> <given-names>EH</given-names></name><name><surname>Monti</surname> <given-names>JM</given-names></name><name><surname>Mesulam</surname> <given-names>MM</given-names></name><name><surname>Egner</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural repetition suppression reflects fulfilled perceptual expectations</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1004</fpage><lpage>1006</lpage><pub-id pub-id-type="doi">10.1038/nn.2163</pub-id><pub-id pub-id-type="pmid">19160497</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk-Browne</surname> <given-names>NB</given-names></name><name><surname>Scholl</surname> <given-names>BJ</given-names></name><name><surname>Chun</surname> <given-names>MM</given-names></name><name><surname>Johnson</surname> <given-names>MK</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neural evidence of statistical learning: efficient detection of visual regularities without awareness</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>1934</fpage><lpage>1945</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21131</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname> <given-names>S</given-names></name><name><surname>Colbert</surname> <given-names>SC</given-names></name><name><surname>Varoquaux</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The NumPy array: a structure for efficient numerical computation</article-title><source>Computing in Science &amp; Engineering</source><volume>13</volume><fpage>22</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2011.37</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinck</surname> <given-names>M</given-names></name><name><surname>Batista-Brito</surname> <given-names>R</given-names></name><name><surname>Knoblich</surname> <given-names>U</given-names></name><name><surname>Cardin</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Arousal and locomotion make distinct contributions to cortical activity patterns and visual encoding</article-title><source>Neuron</source><volume>86</volume><fpage>740</fpage><lpage>754</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.028</pub-id><pub-id pub-id-type="pmid">25892300</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yon</surname> <given-names>D</given-names></name><name><surname>de Lange</surname> <given-names>FP</given-names></name><name><surname>Press</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The predictive brain as a stubborn scientist</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>6</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.10.003</pub-id><pub-id pub-id-type="pmid">30429054</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>AJ</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005a</year><article-title>Uncertainty, neuromodulation, and attention</article-title><source>Neuron</source><volume>46</volume><fpage>681</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.04.026</pub-id><pub-id pub-id-type="pmid">15944135</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>AJ</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005b</year><article-title>Inference, attention, and decision in a bayesian neural architecture</article-title><source>Advances in Neural Information Processing Systems</source><volume>17</volume><fpage>1577</fpage><lpage>1584</lpage></element-citation></ref><ref id="bib64"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Zandbelt</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Slice display</data-title><publisher-name>Figshare</publisher-name><ext-link ext-link-type="uri" xlink:href="http://doi.org/10.6084/m9.figshare.4742866.v1">http://doi.org/10.6084/m9.figshare.4742866.v1</ext-link></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47869.021</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University Feinberg School of Medicine</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Wyart</surname><given-names>Valentin</given-names> </name><role>Reviewer</role><aff><institution>Ecole normale supérieure</institution><country>France</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Heinzle</surname><given-names>Jakob</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife includes the editorial decision letter and accompanying author responses. A lightly edited version of the letter sent to the authors after peer review is shown, indicating the most substantive concerns; minor comments are not usually included.</p></boxed-text><p>Thank you for submitting your article &quot;Statistical learning attenuates visual activity only for attended stimuli&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Valentin Wyart (Reviewer #1); Jakob Heinzle (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>The study described in this manuscript provides evidence that statistical learning does not (automatically) lead to suppression of expected stimuli when stimuli are not attended (i.e. not task relevant). This finding is supported by a series of clever experiments (fMRI, behavior, pupillometry) and analysis (classical GLM based brain mapping, ROI based analysis, MVPA, behavioral tests) that rule out several potential confounds. The key result is that sensory attenuation disappears when attention is diverted to a second task at fixation. The authors interpret their findings in light of existing theories of predictive processing.</p><p>All reviewers agreed that your paper addresses a novel, interesting and important question, and that the results provide important evidence informing models of predictive coding. Reviewers also raised a few points that should be addressed in a revision. These include additional analyses liking pupil and fMRI data, additional information about the stimulus material and pupil response, and the interpretation of the results.</p><p>Essential revisions:</p><p>1) The authors suggest, based on effects found in pupil dilation (larger phasic dilation for surprising object stimuli when attended), that some of the neural effects found (in particular in V1) may be caused by an increased arousal for surprising stimuli. This relationship between arousal and the neural effects observed in the fMRI data occupies a central space in the Discussion section, but it is currently not well supported by analyses that directly link the pupil and fMRI data. It would be important to test more directly the relationship between (pupil-linked) arousal and the attenuation of sensory responses for expected stimuli (or the enhancement of sensory responses for surprising stimuli). For instance, the authors could perform a mediation analysis by using the phasic pupil dilation presented in Figure 3 as a modulator of the difference between expected and unexpected trailing object stimuli in the BOLD signals. One would predict that a larger pupil dilation may increase overall sensory responses to both expected and surprising stimuli, or maybe even only to surprising stimuli. Another prediction would be that this relationship between pupil dilation and BOLD effects is reduced (or even absent) when the object stimuli are not attended. The authors further propose that stimulus-specific and stimulus-unspecific attenuations may rely on different mechanisms (the stimulus-unspecific attenuation being driven primarily by arousal). Using the mediation analysis suggested above, the authors could directly test such interpretation. Alternatively, they could bin the trials based on pupil dilation response and test whether the BOLD suppression effects depend on pupil-linked neuromodulation. In addition, it could be informative to know whether the neural (and pupil) effects predict behavioral performance (although this may be difficult because behavioral performance was at ceiling in the attended condition). In any case, additional analyses linking pupil data to behavioral data and/or fMRI would be helpful for supporting the conclusions.</p><p>2) Reviewers were surprised that no constriction (pupil light reflex) was found at stimulus onset, which presumably involves increases in contrast and light. Were the stimuli and ITI period matched in luminance? They for sure differ in contrast, to which the pupil also responds. Was the screen was quite dim in the scanner, so no transient constriction was elicited? Or was it perhaps that due to the quite narrow ITI range trial onset was quite predictable? Also, baseline pupil should be tested for the 2 tasks (e.g. the raw units) to see whether baseline arousal is different between conditions. This might help elucidate the phasic differences as well, since a lower baseline might be associated with a higher phasic response. Also, the increased dilation comes very late, when the pupil is returning to baseline, suggesting prolonged processing for unexpected stimuli. The baseline was taken from -1 s, giving the pupil response only 4 s to die out for the shortest ITI, which is quite short. The authors could control for expectedness in the previous trial to take into account the supposedly higher baseline after an unexpected trial.</p><p>3) There are several points that need a more thorough discussion.</p><p>a) While the authors frame their study as an attention study, one could also consider the finding an effect of task (task relevant vs. task irrelevant). Of course, the two concepts overlap, but this should be discussed.</p><p>b) Is the explanation for the discrepancy with other studies related to differently controlled attention (as suggested by the authors) or, possibly, to differences in the nature of the stimuli and differences in how they are processed? Importantly, the statistical learning in this study was not automatic (even if being engaged in a different task participants were informed about the regularities) and thus it is likely that they paid attention to the regularities (the other task was easy – performance at ceiling), which might be a crucial difference to other settings. For example, there are auditory, but also visual, mismatch negativity studies which show mismatch effects when the stimuli are not attended. It seems highly unlikely that all of these studies did not control correctly for attention.</p><p>c) The implications of the current findings for models of predictive coding need a more balanced discussion. While reviewers agree that the current evidence is not strongly supportive of the idea that attention provides gain control on prediction errors, their main argument in support of this may not be fully correct: It is well conceivable that there is very strong gain control on prediction error neurons (possibly a small subset of all neurons) even if there is strong baseline activity in response to all stimuli. Why should gain control affect all neurons in a region and not be neuron specific?</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.47869.022</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The authors suggest, based on effects found in pupil dilation (larger phasic dilation for surprising object stimuli when attended), that some of the neural effects found (in particular in V1) may be caused by an increased arousal for surprising stimuli. This relationship between arousal and the neural effects observed in the fMRI data occupies a central space in the Discussion section, but it is currently not well supported by analyses that directly link the pupil and fMRI data. It would be important to test more directly the relationship between (pupil-linked) arousal and the attenuation of sensory responses for expected stimuli (or the enhancement of sensory responses for surprising stimuli).</p><p>For instance, the authors could perform a mediation analysis by using the phasic pupil dilation presented in Figure 3 as a modulator of the difference between expected and unexpected trailing object stimuli in the BOLD signals. One would predict that a larger pupil dilation may increase overall sensory responses to both expected and surprising stimuli, or maybe even only to surprising stimuli. Another prediction would be that this relationship between pupil dilation and BOLD effects is reduced (or even absent) when the object stimuli are not attended. The authors further propose that stimulus-specific and stimulus-unspecific attenuations may rely on different mechanisms (the stimulus-unspecific attenuation being driven primarily by arousal). Using the mediation analysis suggested above, the authors could directly test such interpretation. Alternatively, they could bin the trials based on pupil dilation response and test whether the BOLD suppression effects depend on pupil-linked neuromodulation.</p></disp-quote><p>We agree that establishing an association between pupil dilation and BOLD responses is important for our interpretation that pupil dilation differences may underlie expectation suppression in V1. We followed the reviewer’s suggestions and explored the relationship between pupil dilation and BOLD responses more directly. Specifically, we would expect larger BOLD responses to be associated with larger pupil dilations. We indeed find a positive association between pupil dilation and the BOLD response in V1, regardless of whether stimuli were attended and expected (all <italic>p</italic>&lt;0.005). We also considered the reviewer’s suggestion of investigating the association of pupil dilation and BOLD responses within stimulus-driven compared to non-stimulus-driven voxels. One would expect the general relationship between pupil dilation and BOLD to be similar in these neural populations, given that arousal-related mechanisms are assumed to effect neural populations in a retinotopically unspecific manner. We do indeed find a positive association between BOLD and pupil dilation in both neural populations with a larger effect in non-stimulus-driven populations.</p><p>We added the following subsections to a new “Supplemental analyses” section in the manuscript (“Pupil dilation is associated with larger BOLD responses” and “Pupil dilation influences BOLD responses more in non-stimulus-driven V1 voxels”</p><p>After establishing a reliable association between pupil dilation and BOLD responses, we investigated whether there is evidence for an association between pupil dilation differences between expected compared to unexpected stimuli and expectation suppression (i.e. BOLD<sub>unexpected</sub> – BOLD<sub>expected</sub>). We now describe this additional analyses in the Materials and methods subsection “Linking pupil and neural measures”.</p><p>The results of this analysis, and a new Figure 4A, are now presented in the Results subsection “Expectation suppression and pupil dilations to surprising stimuli are associated”.</p><p>Finally, the new results are briefly discussed in the paragraph discussing the pupil dilation differences in the Discussion section:</p><p>“[…], it is possible that expectation suppression is partially accounted for by an increased arousal in response to surprising stimuli. A related explanation is that enhanced pupil dilation to surprising stimuli (Damsma and van Rijn, 2017; Kloosterman et al., 2015; Preuschoff et al., 2011) results in enhanced retinal illumination, which in turn leads to stronger responses in early visual areas (Haynes et al., 2004), which could potentially also contribute to stimulus unspecific expectation suppression in V1. These interpretations are further supported by the fact that expectation suppression and pupil dilation differences between unexpected and expected attended stimuli were associated, with trailing images that elicit larger pupil dilation differences also showing more pronounced expectation suppression in V1.”</p><disp-quote content-type="editor-comment"><p>In addition, it could be informative to know whether the neural (and pupil) effects predict behavioral performance (although this may be difficult because behavioral performance was at ceiling in the attended condition). In any case, additional analyses linking pupil data to behavioral data and/or fMRI would be helpful for supporting the conclusions.</p></disp-quote><p>We agree with the reviewers that linking the neural effects of expectation with its behavioral benefits is useful. A particularly interesting question is whether the reaction time (RT) benefits due to valid expectations are linked to the neural expectation suppression effect. A prediction would be that stronger expectation suppression correlates with larger RT benefits due to valid expectations. Testing this prediction is challenging in the present dataset, given that expectation suppression is a difference between expectation conditions (i.e. not calculable on single trial level), thus precluding trial-wise analysis. However, it is possible to assess whether within subjects there is a correlation between the magnitude of expectation suppression and the expectation RT benefit per trailing image. We added the following analysis’s description to the Materials and methods subsection “Linking behavioral and neural measures”.</p><p>The corresponding results are presented in the manuscript’s Results subsection “Neural and behavioral effects of expectations are associated”., including Figure 4B.</p><p>The new results are briefly discussed in the last paragraph of the Discussion section:</p><p>“Finally, our results also demonstrate that larger expectation suppression effects in V1 and TOFC are associated with increased reaction time benefits afforded by expectations when people are judging the predictable objects. […] Predictions may thus help in converging more rapidly on an interpretation of the current sensory input, thereby contributing to faster reactions to expected than unexpected stimuli.”</p><disp-quote content-type="editor-comment"><p>2) Reviewers were surprised that no constriction (pupil light reflex) was found at stimulus onset, which presumably involves increases in contrast and light. Were the stimuli and ITI period matched in luminance? They for sure differ in contrast, to which the pupil also responds. Was the screen was quite dim in the scanner, so no transient constriction was elicited? Or was it perhaps that due to the quite narrow ITI range trial onset was quite predictable?</p></disp-quote><p>We agree with the reviewers that the absence of a pupil light response may initially seem surprising. However, as speculated by the reviewer, the stimuli and ITI period were indeed very similar in terms of their luminance. We think that this is important information for the reader and thus added the following paragraph to the “Stimuli” subsection of the Materials and methods: “We calculated the average relative luminance of the object stimuli by converting the stimulus images from sRGB to linear RGB, then calculated the relative luminance for all pixels (where relative luminance Y = 0.2126*R + 0.7152*G + 0.0722*B; Stokes et al., 1996), and finally averaged the obtained luminance values, thereby obtaining the mean relative luminance per image. On this relative luminance scale, 0 would be a completely black image, while 1 would be a white image. The average relative luminance of the stimulus set was 0.225, while the relative luminance of the mid gray background, presented during the ITI, was 0.216.” Thus, when considering the mean relative luminance of the stimuli compared to the ITI (background), one would not expect a reliable pupil light response.</p><disp-quote content-type="editor-comment"><p>Also, baseline pupil should be tested for the 2 tasks (e.g. the raw units) to see whether baseline arousal is different between conditions. This might help elucidate the phasic differences as well, since a lower baseline might be associated with a higher phasic response. Also, the increased dilation comes very late, when the pupil is returning to baseline, suggesting prolonged processing for unexpected stimuli. The baseline was taken from -1 s, giving the pupil response only 4 s to die out for the shortest ITI, which is quite short. The authors could control for expectedness in the previous trial to take into account the supposedly higher baseline after an unexpected trial.</p></disp-quote><p>We followed the reviewers suggestion and compared pupil size during the baseline periods of the two tasks and expectation conditions in raw units. The following subsection was added to the Supplemental analyses section (“No differences in pupil dilation during baseline”.</p><disp-quote content-type="editor-comment"><p>3) There are several points that need a more thorough discussion.</p><p>a) While the authors frame their study as an attention study, one could also consider the finding an effect of task (task relevant vs. task irrelevant). Of course, the two concepts overlap, but this should be discussed.</p></disp-quote><p>We agree with the reviewer that we withdrew attention from the object stimuli by manipulating task-relevance, and as such, our findings, if considered in isolation, can be interpreted as an effect of attention or task-relevance. It is however by considering our results in the context of the relevant literature that we believe our results are better accounted for by a manipulation of attention than task-relevance. We fully agree that a discussion of the overlap and distinction between attention and task-relevance is crucial. Thus, we introduce the two concepts early in the manuscript (Introduction section): “On the other hand, den Ouden et al., 2009, demonstrated attenuated responses to task-irrelevant expected stimuli, suggesting the possibility that the sensory consequences of statistical learning may not depend on attention. et al.[…] Without competition, it is likely that even a task-irrelevant stimulus will receive some attention”.</p><p>We later repeat this argument in the Discussion, and now also explicitly point out the overlap between task-relevance and attention with respect to our design. “However, in all these studies, while the predictable stimuli were task-irrelevant, attention was not effectively drawn away by a competing stimulus that required attention. […] This is a crucial difference between the present and previous studies, because it is likely that any supraliminal stimulus, in the absence of competition, will be attended to some degree, even if it is not task-relevant, especially if the stimulus is surprising (Horstmann et al., 2015)”. Thus, previous studies have shown that task-irrelevant stimuli can result in a modulation of sensory responses by expectations (den Ouden et al., 2009; Kok et al., 2012a), while in our manipulation expectation effects vanish. Therefore, we believe that attention, rather than task-relevance, is likely to be the key manipulation that abolishes expectation effects.</p><p>Additionally, we show that task relevance of the predictive relationship itself does not seem to modulate expectation suppression either. Thus, synthesizing our results with those of previous studies, we conclude “that expectation suppression in the visual system occurs irrespective of exact task goals and relevance of the predictable objects and their predictable relationship, but it is abolished by drawing attention away from the stimuli”.</p><disp-quote content-type="editor-comment"><p>b) Is the explanation for the discrepancy with other studies related to differently controlled attention (as suggested by the authors) or, possibly, to differences in the nature of the stimuli and differences in how they are processed? Importantly, the statistical learning in this study was not automatic (even if being engaged in a different task participants were informed about the regularities) and thus it is likely that they paid attention to the regularities (the other task was easy – performance at ceiling), which might be a crucial difference to other settings. For example, there are auditory, but also visual, mismatch negativity studies which show mismatch effects when the stimuli are not attended. It seems highly unlikely that all of these studies did not control correctly for attention.</p></disp-quote><p>We think that the reviewer addresses an important point when referring to studies which show mismatch effects when stimuli were not attended. Indeed, we do believe that there are cases in which prediction (mismatch) effects can occur in the absence of (top-down) attention, as for example in classical mismatch negativity studies. However, it is important to keep in mind that these studies usually do not differentiate between expectation and adaptation (repetition), and thus do not allow for inferences with respect to the automaticity of either mechanism, just that at least one can induce a mismatch effect without attention. Indeed, there is evidence that adaptation (repetition) effects can persist with minimal attention (Chee and Tan, 2007; Murray and Wojciulik, 2004) or are unmodulated by attention (Bentley et al., 2003; although also different results have been reported, e.g. Eger et al., 2004). Thus, it is difficult to compare our results to studies that did not independently manipulate or control expectation and adaptation. However, with respect to studies that did investigate expectation effects while controlling for stimulus repetition, such as den Ouden et al., 2009, or Kok et al., 2012a; 2012b, a crucial distinction is the attention compared to a task-relevance manipulation. As explained above, and in our Discussion: “However, in all these studies, while the predictable stimuli were task-irrelevant, attention was not effectively drawn away by a competing stimulus that required attention. […] This is a crucial difference between the present and previous studies, because it is likely that any supraliminal stimulus, in the absence of competition, will be attended to some degree, even if it is not task-relevant, especially if the stimulus is surprising (Horstmann et al., 2015)”.</p><p>That said, we do agree with the reviewer that this should not be taken to suggest that all expectation effects depend on attention, but that is important to clarify the type of expectation that is being investigated. As we note in our Discussion: “[…] other, more ‘stubborn’ prior expectations (Yon et al., 2019) that are derived over longer (ontogenetic or phylogenetic) time scales may persist even when attention is drawn away, such as perceptual fill-in during the Kanizsa illusion (Kok et al., 2016)”. Indeed, classical mismatch negativity studies may depend on such more fundamental priors and the above mentioned repetition effects. The other suggestion made by the reviewer that differences in stimuli, and how they are processed, could account for differences in results between the present and previous studies, is important to consider in this context as well. It is indeed conceivable that for complex associations and stimuli (such as the object stimuli used here) the resolution of the expectation status may depend more on recurrent processing across the ventral visual stream, than for simpler associations and stimuli (e.g. grating stimuli, or dot and sound sequences). Expectations of a simpler type (simple associations and stimuli) may be resolved mainly within lower sensory areas. If this is the case, it does also seem plausible to suggest that the automaticity of expectation suppression may differ between these scenarios.</p><p>We now also discuss this possibility in the manuscript: “Similarly, for simple stimuli, such as oriented gratings (Kok et al., 2012a; Kok et al., 2012b) or simple sequences (Ekman et al., 2017), the resolution of expectations may depend less on recurrent processing throughout the visual hierarchy than for complex objects. Thus, it is conceivable that the automaticity of predictive processing partially depends on the complexity of the predictable stimuli and their association, with increasing complexity requiring increasing processing across the hierarchy, and in turn a focus of attention on the predictable stimuli.”.</p><p>Finally, it should be noted that while the task performance was indeed at ceiling during MRI scanning, participants frequently reported not noticing any associations (not even during behavioral training), and consequently also stated that they did not pay attention to the regularities during MRI scanning. This is also reflect by the fact that performance during the post-scanning recognition test (Figure 5B) is far from perfect. Combined this data suggest that participants were less aware of the associations than one may expect given the ceiling performance during MRI scanning, which in turn implies that the present study may not differ substantially in terms of the automaticity of the statistical learning from other studies of statistical learning.</p><disp-quote content-type="editor-comment"><p>c) The implications of the current findings for models of predictive coding need a more balanced discussion. While reviewers agree that the current evidence is not strongly supportive of the idea that attention provides gain control on prediction errors, their main argument in support of this may not be fully correct: It is well conceivable that there is very strong gain control on prediction error neurons (possibly a small subset of all neurons) even if there is strong baseline activity in response to all stimuli. Why should gain control affect all neurons in a region and not be neuron specific?</p></disp-quote><p>We agree with the reviewer that our discussion could have been better balanced. We reexamined the discussion of our results in the context of predictive coding, and have revised the corresponding subsection, “Attention and prediction errors”.</p><p>References:</p><p>Bentley, P., Vuilleumier, P., Thiel, C. M., Driver, J., &amp; Dolan, R. J. (2003). Effects of attention and emotion on repetition priming and their modulation by cholinergic enhancement. Journal of Neurophysiology, 90(2), 1171–1181.</p><p>Chee, M. W. L., &amp; Tan, J. C. (2007). Inter-relationships between attention, activation, fMR adaptation and longterm memory. NeuroImage, 37(4), 1487–1495.</p><p>Eger, E., Henson, R.N.A, Driver, J., &amp; Dolan, R.J. (2004). BOLD Repetition Decreases in Object-Responsive Ventral Visual Areas Depend on Spatial Attention. J Neurophysiol. 92(2), 1241-7</p><p>Murray, S.O. &amp; Wojciulik, E. (2004). Attention increases neural selectivity in the human lateral occipital complex. Nat. Neurosci., 7, 70–74.</p></body></sub-article></article>