<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">65894</article-id><article-id pub-id-type="doi">10.7554/eLife.65894</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Cell Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>CEM500K, a large-scale heterogeneous unlabeled cellular electron microscopy image dataset for deep learning</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-220308"><name><surname>Conrad</surname><given-names>Ryan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-222096"><name><surname>Narayan</surname><given-names>Kedar</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7982-6494</contrib-id><email>kedar.narayan@nih.gov</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Center for Molecular Microscopy, Center for Cancer Research, National Cancer Institute, National Institutes of Health</institution><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Cancer Research Technology Program, Frederick National Laboratory for Cancer Research</institution><addr-line><named-content content-type="city">Frederick</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Grigorieff</surname><given-names>Nikolaus</given-names></name><role>Reviewing Editor</role><aff><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Akhmanova</surname><given-names>Anna</given-names></name><role>Senior Editor</role><aff><institution>Utrecht University</institution><country>Netherlands</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>08</day><month>04</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e65894</elocation-id><history><date date-type="received" iso-8601-date="2020-12-18"><day>18</day><month>12</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-03-13"><day>13</day><month>03</month><year>2021</year></date></history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-65894-v1.pdf"/><abstract><p>Automated segmentation of cellular electron microscopy (EM) datasets remains a challenge. Supervised deep learning (DL) methods that rely on region-of-interest (ROI) annotations yield models that fail to generalize to unrelated datasets. Newer unsupervised DL algorithms require relevant pre-training images, however, pre-training on currently available EM datasets is computationally expensive and shows little value for unseen biological contexts, as these datasets are large and homogeneous. To address this issue, we present CEM500K, a nimble 25 GB dataset of 0.5 × 10<sup>6</sup> unique 2D cellular EM images curated from nearly 600 three-dimensional (3D) and 10,000 two-dimensional (2D) images from &gt;100 unrelated imaging projects. We show that models pre-trained on CEM500K learn features that are biologically relevant and resilient to meaningful image augmentations. Critically, we evaluate transfer learning from these pre-trained models on six publicly available and one newly derived benchmark segmentation task and report state-of-the-art results on each. We release the CEM500K dataset, pre-trained models and curation pipeline for model building and further expansion by the EM community. Data and code are available at <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pdbe/emdb/empiar/entry/10592/">https://www.ebi.ac.uk/pdbe/emdb/empiar/entry/10592/</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://git.io/JLLTz">https://git.io/JLLTz</ext-link>.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>electron microscopy</kwd><kwd>deep learning</kwd><kwd>segmentation</kwd><kwd>vEM</kwd><kwd>neural network</kwd><kwd>image dataset</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000054</institution-id><institution>National Cancer Institute</institution></institution-wrap></funding-source><award-id>Contract No. 75N91019D00024</award-id><principal-award-recipient><name><surname>Conrad</surname><given-names>Ryan</given-names></name><name><surname>Narayan</surname><given-names>Kedar</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>'Cellular Electron Microscopy 500,000 images' (CEM500K) is a highly heterogeneous, information-rich, non-redundant, unlabeled EM dataset curated to pre-train DL algorithms for better model generalization on EM segmentation tasks.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Accurate image segmentation is essential for analyzing the structure of organelles and cells in electron microscopy (EM) image datasets. Segmentation of volume EM (vEM) data has enabled researchers to address questions of fundamental biological interest, including the organization of neural circuits (<xref ref-type="bibr" rid="bib50">Takemura et al., 2015</xref>; <xref ref-type="bibr" rid="bib32">Kasthuri et al., 2015</xref>) and the structure of various organelles (<xref ref-type="bibr" rid="bib54">Vincent et al., 2017</xref>; <xref ref-type="bibr" rid="bib55">Vincent et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Hoffman et al., 2020</xref>). Truly automated EM image segmentation methods hold the promise of significantly accelerating the rate of discovery by enabling researchers to extract and analyze information from their datasets without months or years of tedious manual labeling. While supervised deep learning (DL) models are effective at the segmentation of objects in natural images (e.g. of people, cars, furniture, and landscapes) (<xref ref-type="bibr" rid="bib57">Wang et al., 2020</xref>; <xref ref-type="bibr" rid="bib51">Tao et al., 2020</xref>; <xref ref-type="bibr" rid="bib5">Carion et al., 2020</xref>; <xref ref-type="bibr" rid="bib23">He et al., 2020</xref>), they require significant human oversight and correction when applied to the organelles and cellular structures captured by EM (<xref ref-type="bibr" rid="bib35">Lichtman et al., 2014</xref>; <xref ref-type="bibr" rid="bib43">Plaza and Funke, 2018</xref>).</p><p>Many of the limitations of supervised DL segmentation models for cellular EM data result from a lack of large and, importantly, diverse training datasets (<xref ref-type="bibr" rid="bib18">Goodfellow, 2016</xref>; <xref ref-type="bibr" rid="bib41">Pereira et al., 2009</xref>; <xref ref-type="bibr" rid="bib49">Sun et al., 2017</xref>). Although several annotated image datasets for cell and organelle segmentation are publicly available, these often exclusively consist of images from a single experiment or tissue type, and a single imaging approach (<xref ref-type="bibr" rid="bib19">Guay et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Žerovnik Mekuč et al., 2020</xref>; <xref ref-type="bibr" rid="bib6">Casser et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Perez et al., 2014</xref>; <xref ref-type="bibr" rid="bib1">Berning et al., 2015</xref>). The homogeneity of such datasets often means that they are ineffective for training DL models to accurately segment images from unseen experiments. Instead, when confronted with new data, the norm is to extract and annotate small regions-of-interest (ROIs) from the EM image, train a model on the ROIs, and then apply the model to infer segmentations for the remaining unlabeled data (<xref ref-type="bibr" rid="bib19">Guay et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Žerovnik Mekuč et al., 2020</xref>; <xref ref-type="bibr" rid="bib6">Casser et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Perez et al., 2014</xref>; <xref ref-type="bibr" rid="bib1">Berning et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Januszewski et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Funke et al., 2019</xref>). Often, not only are these models dataset-specialized, reducing their utility, they often fail to generalize even to parts of the same dataset that are spatially distant from the training ROIs (<xref ref-type="bibr" rid="bib61">Žerovnik Mekuč et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Buhmann, 2019</xref>).</p><p>There are two main methods to address this challenge. First, gathering more annotated data for model training from disparate sources could certainly improve a model’s ability to generalize to unseen images, yet it is rarely feasible for typical research laboratories to generate truly novel datasets; most have expertise in a particular imaging technique, organism, or tissue type. Beyond collecting the EM data, manual segmentation is time-consuming and, unlike for natural images, difficult to crowdsource because of the extensive domain knowledge required to identify objects in novel cellular contexts. Promising work is being done in the area of citizen science as it pertains to EM data, but it is clear that there are limitations to the range of structures that can be accurately segmented by volunteers (<xref ref-type="bibr" rid="bib48">Spiers, 2020</xref>; <xref ref-type="bibr" rid="bib15">EyeWirers et al., 2014</xref>). Moreover, structure-specific annotations will not solve the generalization problem for all possible EM segmentation targets; for example, thousands of hours spent labeling neurites is unlikely to buy any gains for mitochondrial segmentation.</p><p>The second and less costly method is to use transfer learning. In transfer learning, a DL model is pre-trained on a general task and its parameters are reused for more specialized downstream tasks. A well-known example is to transfer parameters learned from the ImageNet classification task (<xref ref-type="bibr" rid="bib11">Deng et al., 2015</xref>) to other classification or object detection tasks which have fewer training examples (<xref ref-type="bibr" rid="bib45">Ren et al., 2017</xref>). Transfer learning, when relevant pre-trained parameters are available, is the default approach for extracting the best performance out of small training datasets (<xref ref-type="bibr" rid="bib26">Huh et al., 2016</xref>; <xref ref-type="bibr" rid="bib13">Devlin et al., 2018</xref>). While ImageNet pre-trained models are sometimes used for cellular EM segmentation tasks (<xref ref-type="bibr" rid="bib31">Karabağ et al., 2020</xref>; <xref ref-type="bibr" rid="bib12">Devan et al., 2019</xref>), high-level features learned from ImageNet may not be applicable to biological imaging domains (<xref ref-type="bibr" rid="bib44">Raghu et al., 2019</xref>). Building a more domain-specific annotated dataset large enough for pre-training would be a significant bottleneck, and indeed, it required multiple years to annotate the 3.2 × 10<sup>6</sup> images that form the basis of ImageNet. Fortunately, recent advances in unsupervised learning algorithms have now enabled effective pre-training and transfer learning without the need for any up-front annotations; in fact, on many tested benchmarks, unsupervised pre-training leads to better transfer learning performance (<xref ref-type="bibr" rid="bib52">Tian et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Chen et al., 2020a</xref>; <xref ref-type="bibr" rid="bib22">He et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Donahue and Simonyan, 2019</xref>; <xref ref-type="bibr" rid="bib29">Ji et al., 2018</xref>; <xref ref-type="bibr" rid="bib58">Wu et al., 2018</xref>; <xref ref-type="bibr" rid="bib34">Kolesnikov, 2019</xref>).</p><p>To provide a resource for the EM community to explore these exciting advances, we constructed an unlabeled cellular EM dataset which we call CEMraw, containing images from 101 unrelated biological projects. The image data superset, comprising 591 3D image volumes and 9,626 2D images, is collated from a collection of experiments conducted in our own laboratory as well as data from publicly available sources. After gathering this set of heterogeneous images, we create a pipeline where we first remove many nearly identical images and then filter out low-quality and low-information images. This results in a highly information-rich, relevant, and non-redundant 25 GB 2D image dataset comprising 0.5 × 10<sup>6</sup> images. As a proof of concept for its potential applications, we pre-trained a DL model on CEM500K using an unsupervised algorithm, MoCoV2 (<xref ref-type="bibr" rid="bib8">Chen et al., 2020b</xref>), and evaluated the results for transfer learning on six publicly available benchmarks: CREMI Synaptic Clefts (<xref ref-type="bibr" rid="bib10">CREMI, 2016</xref>), Guay (<xref ref-type="bibr" rid="bib19">Guay et al., 2020</xref>), Kasthuri++ and Lucchi++ (<xref ref-type="bibr" rid="bib6">Casser et al., 2018</xref>), Perez (<xref ref-type="bibr" rid="bib42">Perez et al., 2014</xref>) and UroCell (<xref ref-type="bibr" rid="bib61">Žerovnik Mekuč et al., 2020</xref>).and one newly derived benchmark that we introduce in this work. CEM500K pre-trained models significantly outperformed randomly initialized and ImageNet pre-trained models, as well as previous baseline results from benchmark-associated publications.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Creation of CEM500K</title><p>In order to create an image dataset that is relevant to cellular EM and yet general enough to be applicable to a variety of biological studies and experimental approaches, we collected 2D and 3D cellular EM images from both our own experiments and publicly available sources. These included images from a variety of imaging modalities and their corresponding sample preparation protocols, resolutions reported, and cell types imaged (<xref ref-type="fig" rid="fig1">Figure 1a–c</xref>). We selected 'in-house' datasets corresponding to 251 reconstructed FIB-SEM volumes from 33 unrelated experiments and 2975 transmission EM (TEM) images from 35 additional experiments. Other data was sourced externally; as there is currently no central hub for accessing publicly available datasets, we manually searched through databases (Cell Image Library, Open Connectome Project [<xref ref-type="bibr" rid="bib56">Vogelstein et al., 2018</xref>], EMPIAR [<xref ref-type="bibr" rid="bib27">Iudin et al., 2016</xref>]), GitHub repositories, and publications. A complete accounting of the datasets with relevant attribution is detailed in the <bold>Supplementary Materials</bold>. Included in this batch of data were 340 EM image volumes (some derived from video data) from 26 experiments and 9792 2D images from 14 other experiments. Among the externally gathered datasets, there were disparate file types (avi, mp4, png, tiff, jpeg, mrc, nii.gz) and pixel/voxel data types (signed and unsigned, 32-bit float, 8-bit and 16-bit integer) as well as a mixture of image volumes with isotropic or anisotropic voxels, and regular or inverted intensities. These data were standardized into 2D tiff images, or patches, of 224 × 224 8-bit unsigned pixels (see 'Materials and methods'); the resulting set of 5.3 × 10<sup>6</sup> images constitutes what we term CEMraw (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, top).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Preparation of a deep learning appropriate 2D EM image dataset rich with relevant and unique features.</title><p>(<bold>a</bold>) Percent distribution of collated experiments grouped by imaging technique: TEM, transmission electron microscopy; SEM, scanning electron microscopy. (<bold>b</bold>) Distribution of imaging plane pixel spacings in nm for volumes in the 3D corpus. (<bold>c</bold>) Percent distribution of collated experiments by organism and tissue origin. (<bold>d</bold>) Schematic of our workflow: 2D electron microscopy (EM) image stacks (top left) or 3D EM image volumes sliced into 2D cross-sections (top right) were cropped into patches of 224 × 224 pixels, comprising CEMraw. Nearly identical patches excepting a single exemplar were eliminated to generate CEMdedup. Uninformative patches were culled to form CEM500K.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>Details of imaging technique, organism, tissue type and imaging plane pixel spacing in collated imaging experiments.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-65894-fig1-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-fig1-v1.tif"/></fig><p>Within CEMraw, however, most images were redundant. Nearly identical patches existed because of the similarity between adjacent cross-sections in high-resolution 3D volumes as well as in patches cropped from uniform intensity regions like empty resin. Duplicates are not only memory and computationally inefficient, but they may also induce undesirable biases toward the most frequently sampled features in the dataset. Therefore, we aggressively removed duplicates using an automated algorithm: we calculated and compared image hashes for each patch in CEMraw and then kept a single, randomly chosen exemplar image from each group of near-duplicates (see 'Materials and methods'). As a result of this operation, we obtained an 80% decrease in the number of patches when compared to CEMraw; this ‘deduplicated’ subset of 1.1 × 10<sup>6</sup> image patches we refer to as CEMdedup (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, middle).</p><p>Deduplication ensures that each image will make a unique contribution to our dataset, but it is agnostic to the content of the image, which may or may not be relevant to downstream tasks. Upon visual inspection, it was clear that many of the images in CEMdedup contained little information useful to the segmentation of organelles or cellular structures, for example, images dominated by empty resin, background padding, or homogeneously stained interiors of nuclei or cytoplasm (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1a</xref>). However, while these images were uninformative for our purposes, they also represented a wide variety of image features, making them challenging to identify with simple image statistics. Instead, we separated an arbitrary subset of 12,000 images from CEMdedup into informative and uninformative classes and trained a DL model to perform binary classification on the entire dataset. Uninformative images were characterized by poor contrast, large areas of uniform intensity, artifacts, and the presence of non-cellular objects. Detailed criteria are given in 'Materials and methods'. The classifier achieved an area under the receiver operating characteristic (AUROC) score of 0.962 on a holdout test set of 2000 images, as shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1b</xref>, suggesting that it could reliably distinguish between the informative and uninformative image classes. Classification of the remaining unlabeled images with this model yielded 0.5 × 10<sup>6</sup> patches with a visibly higher density of views containing organelles and cellular structures. We refer to this final subset of uniquely informative 2D cellular EM images as CEM500K (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, bottom). Representative patches from the three datasets (CEMraw, CEMdedup, and CEM500K) are shown in <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>.</p></sec><sec id="s2-2"><title>Test of pre-training by CEM500K</title><p>We then decided to test CEM500K for unsupervised pre-training of a DL model, using the MoCoV2 algorithm, a relatively new and computationally efficient approach (<xref ref-type="bibr" rid="bib22">He et al., 2019</xref>). The algorithm works by training a DL model to match differently augmented (e.g., cropped, rotated, zoomed in, and brightened) pairs of images. The first batch of augmented images is called the query and the batch of their differently augmented counterparts is called the key. Before matching, the encoded images in the key are added to a continuously updated queue containing tens of thousands of recently seen images (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3a</xref>). To be useful for other tasks, it is assumed that the model will learn features that correspond to relevant objects within the training images. Recently, models pre-trained on ImageNet with the MoCoV2 algorithm have shown superior transfer learning performance over supervised methods when applied to a variety of tasks including segmentation (<xref ref-type="bibr" rid="bib8">Chen et al., 2020b</xref>). Before we were able to evaluate the MoCoV2 algorithm on CEM500K, it was necessary to define a set of downstream tasks to quantify and compare performance. We chose six publicly available benchmark datasets: CREMI Synaptic Clefts (<xref ref-type="bibr" rid="bib10">CREMI, 2016</xref>), Guay (<xref ref-type="bibr" rid="bib19">Guay et al., 2020</xref>), Kasthuri++ and Lucchi++ (<xref ref-type="bibr" rid="bib6">Casser et al., 2018</xref>), Perez (<xref ref-type="bibr" rid="bib42">Perez et al., 2014</xref>), and UroCell (<xref ref-type="bibr" rid="bib61">Žerovnik Mekuč et al., 2020</xref>). The benchmarks included a total of eight organelles or subcellular structures for segmentation (mitochondria, lysosomes, nuclei, nucleoli, canalicular channels, alpha granules, dense granules, dense granule cores, and synaptic clefts). In <xref ref-type="fig" rid="fig2">Figure 2a</xref>, we show representative images and label maps from the benchmarks. Additional information about the benchmarks, including imaging techniques and sizes of the training and test sets, is given in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>CEM500K pre-training improves the transferability of learned features.</title><p>(<bold>a</bold>) Example images and colored label maps from each of the six publicly available benchmark datasets: clockwise from top left: Kasthuri++, UroCell, CREMI Synaptic Clefts, Guay, Perez, and Lucchi++. The All Mitochondria benchmark is a superset of these benchmarks and is not depicted. (<bold>b</bold>) Schematic of our pre-training, transfer, and evaluation workflow. Gray blocks denote trainable models with randomly initialized parameters; blue block denotes a model with frozen pre-trained parameters. (<bold>c</bold>) Baseline Intersection-over-Union (IoU) scores for each benchmark achieved by skipping MoCoV2 pre-training. Randomly initialized parameters in ResNet50 layers were transferred directly to UNet-ResNet50 and frozen during training. (<bold>d</bold>) Measured percent difference in IoU scores between models pre-trained on CEMraw vs. CEM500K (red) and on CEMdedup vs. CEM500K (blue). (<bold>e</bold>) Measured percent difference in IoU scores between a model pre-trained on CEM500K over the mouse brain (Bloss) pre-training dataset. Benchmark datasets comprised exclusively of electron microscopy (EM) images of mouse brain tissue are highlighted.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>IoU scores achieved with different datasets used for pre-training.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-65894-fig2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-fig2-v1.tif"/></fig><p>Performance on each benchmark was measured using the standard Intersection-over-Union (IoU) score. Considered on their own, many of these benchmark datasets are not difficult enough to expose the gap in performance between different models: they only require the segmentation of a single organelle within a test set that is often from the same image volume as the training set. At the same time, they are an accurate reflection of common use cases for deep learning in EM laboratories where the goal is to segment data from a single experiment in order to support biological, not computational, research. To address the lack of variety within the benchmark training and test sets, we derived an additional benchmark that we call All Mitochondria, which is a combination of the training and test sets from each of the five benchmarks that contain label maps for mitochondria (Guay, Perez, UroCell, Lucchi++, and Kasthuri++; the labels for all other objects were removed). Although this benchmark is specific to a single organelle, it is challenging in that it requires a model to learn features that are general for mitochondria from image volumes generated independently and from unrelated experiments and imaging parameters.</p><p>Our overall pre-training, transfer, and evaluation workflow is shown in a schematic in <xref ref-type="fig" rid="fig2">Figure 2b</xref>. Pre-training was performed by applying the MoCoV2 algorithm to learn parameters for a ResNet50 (<xref ref-type="bibr" rid="bib20">He et al., 2016</xref>) before transferring the parameters into the encoder of a U-net (<xref ref-type="bibr" rid="bib46">Ronneberger et al., 2015</xref>). A detailed schematic of the UNet-ResNet50 architecture is shown in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3b</xref>. For this section, once transferred, the parameters were frozen such that no updates were made during training on the benchmark tasks; this enabled us to isolate the effects of pre-training. As a simple baseline reference for calibrating later results, we started by measuring the performance of the proposed segmentation model with randomly initialized and frozen encoder parameters (i.e., we skipped the pre-training step in the workflow); the results for each benchmark are shown <xref ref-type="fig" rid="fig2">Figure 2c</xref>. Given that in our architecture, the encoder includes approximately 23 × 10<sup>6</sup> parameters and the decoder approximately 9 × 10<sup>6</sup> parameters, some 70% of the model’s parameters were never updated during training. Still, some benchmarks permit strikingly good performance, with IoU scores of over 0.75 on both Lucchi++ and Kasthuri++. These results emphasize the necessity of evaluating deep learning algorithms and pre-training datasets on multiple benchmarks before drawing conclusions about their quality.</p><p>We next tested the influence of our curation pipeline on the quality of pre-trained parameters. We pre-trained models on CEMraw, CEMdedup, and CEM500K with an abbreviated training schedule (see 'Materials and methods') and compare the IoU scores achieved on the benchmarks in <xref ref-type="fig" rid="fig2">Figure 2d</xref> (the actual IoU scores are shown in <xref ref-type="table" rid="table1">Table 1</xref>). We observed that pre-training on CEM500K gave better or equivalent results than the CEMraw superset and CEMdedup subset for every benchmark. The average increase in performance of CEM500K over CEMraw was 4.5%, and CEM500K over CEMdedup was 2.0%, with a maximum increase of 12.3% and 4.1%, respectively, on the UroCell benchmark (IoU scores increased from 0.652 and 0.699 to 0.729). These increases are significant. As a comparison, a 2% increase in model performance is similar in magnitude to what might be expected from using an ensemble of a few models (<xref ref-type="bibr" rid="bib30">Ju et al., 2017</xref>). Besides these gains, curation is valuable for reducing the computational cost of using CEM500K: the final filtered subset is 90% smaller than the raw superset (25 GB compared to 250 GB). Deduplication and filtering likely contributed to the performance gain by enabling both faster convergence and the learning of more relevant feature detectors. Duplicate images consume training iterations without presumably transmitting any new information, resulting in slower learning. Uninformative images, on the other hand, may guide a model to discover discriminative features that are useless for most segmentation tasks. For example, a model must learn feature detectors that can distinguish between images of empty resin in order to succeed on the pre-training task, but those feature detectors are unlikely to help with a common task like mitochondrial segmentation. Therefore, eliminating uninformative images may reduce the learning of irrelevant details during pre-training.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Comparison of segmentation Intersection-over-Union (IoU) results for benchmark datasets from models randomly initialized and pre-trained with MoCoV2 on the Bloss dataset, and CEMraw, CEMdedup, and CEM500K.</title><p>* denotes benchmarks that exclusively contain electron microscopy (EM) images from mouse brain tissue. The best result for each benchmark is highlighted in bold and underlined.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Benchmark</th><th>Random Init. <break/>(No Pre-training)</th><th><xref ref-type="bibr" rid="bib2">Bloss et al., 2018</xref></th><th>CEMraw</th><th>CEMdedup</th><th>CEM500K</th></tr></thead><tbody><tr><td>All Mitochondria</td><td>0.306</td><td>0.694</td><td>0.719</td><td>0.722</td><td><bold><underline>0.745</underline></bold></td></tr><tr><td>CREMI Synaptic Clefts</td><td>0.000</td><td>0.242</td><td>0.254</td><td>0.259</td><td><bold><underline>0.265</underline></bold></td></tr><tr><td>Guay</td><td>0.349</td><td>0.380</td><td>0.372</td><td>0.391</td><td><bold><underline>0.404</underline></bold></td></tr><tr><td>*Kasthuri++</td><td>0.855</td><td>0.907</td><td>0.913</td><td>0.913</td><td><bold><underline>0.915</underline></bold></td></tr><tr><td>*Lucchi++</td><td>0.788</td><td><bold><underline>0.899</underline></bold></td><td>0.880</td><td>0.890</td><td>0.894</td></tr><tr><td>*Perez</td><td>0.547</td><td><bold><underline>0.874</underline></bold></td><td>0.854</td><td>0.866</td><td>0.869</td></tr><tr><td>UroCell</td><td>0.208</td><td>0.638</td><td>0.652</td><td>0.699</td><td><bold><underline>0.729</underline></bold></td></tr><tr><td>*Average Mouse Brain</td><td>0.730</td><td><bold><underline>0.893</underline></bold></td><td>0.883</td><td>0.890</td><td><bold><underline>0.893</underline></bold></td></tr><tr><td>Average Other</td><td>0.216</td><td>0.489</td><td>0.499</td><td>0.518</td><td><bold><underline>0.536</underline></bold></td></tr></tbody></table></table-wrap><p>We also posited that, in addition to the benefits of curation, the heterogeneity of examples in CEM500K would be essential for achieving good segmentation performance across disparate biological contexts. To test this, we considered an alternative pre-training dataset consisting exclusively of 1 × 10<sup>6</sup> images from a single large connectomics volume of mouse brain tissue (<xref ref-type="bibr" rid="bib2">Bloss et al., 2018</xref>). Coming from a single volume of a highly homogeneous tissue type, images in this dataset show much less variation in cellular features than those in CEM500K (a random sampling of images is shown in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). The size of the volume and the density of its content allowed us to sparsely sample patches without the need for deduplication and filtering.</p><p>Compared to the Bloss pre-training dataset, CEMraw, CEMdedup, and CEM500K all demonstrated significantly higher performance on four of the seven benchmarks, as shown in <xref ref-type="fig" rid="fig2">Figure 2e</xref> (the actual IoU scores are shown in <xref ref-type="table" rid="table1">Table 1</xref>). The average increase in IoU scores from the Bloss baseline to CEM500K over these four benchmarks was 9.1%, with a maximum of 13.8% for the UroCell benchmark (increase in IoU score from 0.638 to 0.729). Tellingly, the three benchmarks on which Bloss pre-trained models performed comparably well (Kasthuri++, Lucchi++, and Perez) were the only benchmarks that exclusively contained images from mouse brain tissue, like the Bloss dataset itself. This apparent specificity for images from the same organism and tissue type may indicate that the models learn to represent elements of the underlying biology or tissue architecture. Alternatively, it may reflect similarities in the image acquisition and sample preparation protocols, though the plausibility of this explanation is unlikely, given that each benchmark dataset was imaged with different, albeit broadly similar, technologies (Bloss with serial section TEM; Kasthuri++ with ATUM-SEM; Lucchi++ with FIB-SEM; Perez with SBF-SEM). It is clear that pre-training on large but biological narrow datasets is insufficient for learning general-purpose features that apply equally well across a broad spectrum of contexts. To guard against potential biases, our results instead suggest that the pre-training dataset ought to include image examples from as many different tissues, organisms, sample preparation protocols, and EM techniques as possible. Furthermore, a set of diverse benchmark datasets is essential for identifying such biases when they do arise.</p></sec><sec id="s2-3"><title>CEM500K models are largely impervious to meaningful image augmentations</title><p>Having established CEM500K as the EM dataset for pre-training and transfer learning, we investigated the qualities of the model pre-trained by the MoCoV2 algorithm on CEM500K and compare it to a model pre-trained by the MoCoV2 algorithm on ImageNet (IN-moco). We note that unlike the abbreviated training used to evaluate pre-training on various subsets of cellular electron microscopy (CEM), here we trained the model for the complete schedule, and henceforth refer to the fully trained model as CEM500K-moco. In general, good DL models have neurons that are both robust to distortions and are selective for particular features (<xref ref-type="bibr" rid="bib17">Goodfellow et al., 2009</xref>). In the context of EM images, for example, a good model must be able to recognize a mitochondrion as such irrespective of its orientation in space, its size, or some reasonable variation in resolution of its membrane. On the other hand, the same model must also be able to discern mitochondria, no matter how heterogeneous, from a variety of other organelles or cellular features. First, we attempted to evaluate the robustness of CEM500K-moco neurons by measuring their invariances to transformations of input images. Specifically, we considered the average activations of the 2048 neurons in the last layer of the ResNet50s’ encoders, pre-trained by either CEM500k-moco or IN-moco, to input images. Broadly following the approach detailed in <xref ref-type="bibr" rid="bib17">Goodfellow et al., 2009</xref>, we defined invariance based on the mean firing rates of neurons in response to distortions of their inputs. Plots showing changes in mean firing rates with respect to rotation, Gaussian blur and noise, brightness, contrast, and scale are shown in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. These six transforms that we choose account for much of the variation observed experimentally in cellular EM datasets, and we expect that models in which many neurons are invariant to these differences would be better suited to cellular EM segmentation tasks.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Features learned from CEM500K pre-training are more robust to image transformations and encode for semantically meaningful objects with greater selectivity.</title><p>(<bold>a</bold>) Mean firing rates calculated between feature vectors of images distorted by (i) rotation, (ii) Gaussian blur, (iii) Gaussian noise, (iv) brightness v. contrast, (vi) scale. Dashed black lines show the range of augmentations used for CEM500K + MoCoV2 during pre-training. For transforms in the top row, the undistorted images occur at x = 0; bottom row, at x = 1. (<bold>b</bold>) Evaluation of features corresponding to ER (left), mitochondria (middle), and nucleus (right). For each organelle, the panels show: input image and ground truth label map (top row), heatmap of CEM500K-moco activations of the 32 filters most correlated with the organelle and CEM500K-moco binary mask created by thresholding the mean response at 0.3 (middle row), IN-moco activations and IN-moco binary mask (bottom row). Also included are Point-Biserial correlation coefficients (<italic>r<sub>pb</sub></italic>) values and Intersection-over-Union scores (IoUs) for each response and segmentation. All feature responses are rescaled to range [0, 1]. (<bold>c</bold>) Heatmap of occlusion analysis showing the region in each occluded image most important for forming a match with a corresponding reference image. All magnitudes are rescaled to range [0, 1].</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-fig3-v1.tif"/></fig><p>We observed that neurons in CEM500K-moco models had consistently stronger invariance to all tested transformations (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). The two exceptions were a reduction in invariance when contrast was very high and a smaller reduction when scale factors were very large (<xref ref-type="fig" rid="fig3">Figure 3a,v and vi</xref>, respectively). First, with regard to rotation, virtually all the neurons in the CEM500K-moco model were remarkably invariant to rotation compared to about 70% of the neurons in the IN-moco model, reflecting the fact that orientation matters for representing images in ImageNet but, appropriately, not for CEM500K. This rotation invariance likely arises from the range of augmentations applied during pre-training. Next, neurons in the CEM500K-moco model fire more consistently when presented with increasingly blurry and noisy images, in both cases falling off significantly later as compared to IN-moco, when, presumably, meaningful information in the images has been lost. Further, while both of the tested pre-trained models responded comparably to increasing image brightness, the CEM500K-moco model had a noticeably greater invariance to both more brightened and more darkened images. For contrast adjustments, there was a similar robustness to decreased contrast. This was indicative of the distribution of images in CEM500K, and cellular EM data more broadly: very low-contrast images are common, very high-contrast images are not. On the other hand, the gap between CEM500K-moco and IN-moco pre-trained models in the high-contrast regime not only reinforces this observation but also suggests more relevant learning by the former. CEM500K-moco neurons show an invariance to a transformation only insofar as that transformation mimics real variance in the data distribution, and the firing rate decreases when the high contrast becomes no longer plausible. Similarly, there is some evidence that the results for scale invariance follow the same logic. In CEM500K, the most common reported image pixel sampling was 15–20 nm and the highest was 2 nm. Extreme scaling transformations (greater than 5x) would exceed the limits of features commonly sampled in CEM500K, rendering invariance to such transformations useless. We expect that the superior robustness to variations in cellular EM data baked into CEM500K-moco should simplify the process of adjusting to new tasks. For example, when training a U-Net on a segmentation task, the parameters in the decoder will receive a consistent signal from the pre-trained encoder regardless of the orientation and other typical variations of the input image, presumably easing the learning burden on the decoder. For the same reason, we expect models to gain robustness to rare and random events such as experimental artifacts generated during sample preparation or image acquisition.</p></sec><sec id="s2-4"><title>CEM500K models learn biologically relevant features</title><p>Next, we assessed selectivity for objects of interest, that is, do these models learn something meaningful from cellular EM images? We created feature maps by appropriately upsampling the activations of each of the 2048 neurons in the last layer of the pre-trained ResNet50 and correlated these maps to the ground truth segmentations for three different organelles. In <xref ref-type="fig" rid="fig3">Figure 3b</xref>, activations of the 32 neurons most positively correlated with the presence of the corresponding organelle were averaged, scaled from 0 to 1 (displayed as a heatmap), and then binarized with a threshold of 0.3 (displayed as a binary mask). We observed that these derived heatmaps from the CEM500K-moco model shared a higher correlation with the presence of an organelle than features from the equivalent IN-moco model, irrespective of whether the organelle interrogated was ER, mitochondria, or nucleus. For the CEM500K-moco model, Point-Biserial correlation coefficients were 0.418, 0.680, and 0.888 for ER, mitochondria, and nucleus compared to 0.329, 0.608, and 0.803 for the IN-moco model. The segmentations created by binarizing the mean responses also have a greater IoU with ground truth segmentations (CEM500K-moco: 0.284, 0.517, and 0.887 for ER, mitochondria, and nucleus; IN-moco: 0.208, 0.325, and 0.790, respectively) for the model. Unexpectedly, features learned from ImageNet displayed some selectivity for mitochondria and nuclei, emphasizing the surprising transferability of features to domains that are seemingly unrelated to a model’s training dataset. Nevertheless, it is clear that relevant pre-training, as is the case with CEM500K-moco, results in the model learning features that are meaningful in a cell biological context. The link between these results and the subsequent model’s performance on downstream segmentation tasks is self-evident.</p><p>Pre-training on CEM500K encouraged the learning of representations that encode information about organelles. We analyzed how the model completed the MoCoV2 training task of matching differently transformed views of the same image. We first generated two different views of the same image by taking random crops and then randomly rescaling them. Then, we took one of the images in the pair and sequentially masked out small squares of data and measured the dot product similarity between the model’s output on this occluded image and its output on the other image in the pair. Using this technique, called occlusion analysis, we were able to detect the areas in each image that were the most important for making a positive match (<xref ref-type="bibr" rid="bib60">Zeiler and Fergus, 2014</xref>). Results are displayed as heatmaps overlaid on the occluded image (<xref ref-type="fig" rid="fig3">Figure 3c</xref>) and show, importantly, that without any guidance the model spontaneously learned to use organelles as ‘landmarks’ in the images, visible as ‘hot spots’ around such features. This behavior mirrors how a human annotator would likely approach the same problem: identify a prominent object in the first image and look for it in the second image. That these prominent objects should happen to be organelles is not coincidental as sample preparation protocols for electron microscopy are explicitly designed to accentuate organelles and membranes relative to other content. Thus, representations learned by CEM500K-moco pre-training display robustness to EM-specific image variations and selectivity for objects of interest, demonstrating that they should be well-suited to any downstream segmentation tasks.</p><p>With this understanding for how a model pre-trained with MoCoV2 on an EM-specific dataset might confer an advantage for EM segmentation tasks as compared to similar pre-training on a natural image dataset (ImageNet), we quantified this advantage by evaluating IoU improvements across the benchmark datasets. In addition to the CEM500K-moco and IN-moco pre-trained encoders, we also considered two alternative parameter initializations: ImageNet Supervised (IN-super)(<xref ref-type="bibr" rid="bib22">He et al., 2019</xref>) and, as a baseline, random initialization. In contrast to results in <xref ref-type="fig" rid="fig2">Figure 2c</xref>, all encoder parameters for randomly initialized models were updated during training. Pre-trained models, as before, had their encoder parameters frozen to assess their transferability.</p></sec><sec id="s2-5"><title>Fully trained CEM500K models achieve state-of-the-art results on EM benchmarks</title><p>Results showing the measured percent difference in IoU scores against random initialization are shown in <xref ref-type="fig" rid="fig4">Figure 4a</xref>. For each benchmark, we applied the number of training iterations that gave the best performance for CEM500K-moco pre-trained models and averaged the results from five independent runs (see <xref ref-type="table" rid="table2">Table 2</xref>). Across the board, CEM500K-moco was the best initialization method with performance increases over random initialization ranging from 0.2% on the Lucchi++ benchmark to a massive 73% on UroCell; the mean improvement (excluding CREMI Synaptic Clefts) was 30%. For the short training schedule used, the baseline random initialization IoU score on the CREMI Synaptic Clefts benchmark was 0.000, making any % measurements of performance improvements meaningless. (The large foreground-to-background imbalance for synaptic clefts necessitates longer training schedules for the combination of randomly initialized models and unweighted binary cross-entropy loss). For ease of visualization, we assigned an IoU score of 0.2 for this dataset and calculated improvements based off of this score. Example 2D and 3D segmentations on the UroCell benchmark test set are shown in <xref ref-type="fig" rid="fig4">Figure 4b</xref>; we also display representative segmentations for selected labelmaps from all of the 2D-only benchmarks in <xref ref-type="fig" rid="fig4">Figure 4c</xref>. On the UroCell test set, all of the initialization methods except CEM500K-moco failed to accurately segment mitochondria in an anomalously bright and low-contrast region (example marked by a black arrow in <xref ref-type="fig" rid="fig4">Figure 4b</xref>). Indeed, CEM500K-moco also correctly identified features that the human annotator appears to have missed (example of missed mitochondrion, red arrow in <xref ref-type="fig" rid="fig4">Figure 4c</xref>). On average, IN-super and IN-moco achieved 11% and 14% higher IoU scores than random initialization, respectively. Parameters pre-trained with the unsupervised MoCoV2 algorithm thus appear to transfer better to new tasks than parameters pre-trained on the ImageNet supervised classification task (<xref ref-type="bibr" rid="bib22">He et al., 2019</xref>). Crucially, the 14% average increase in IoU scores from CEM500K-moco over IN-moco reveals the advantage of pre-training on a domain-specific dataset. Thus, while it is clear that some of CEM500K-moco’s improvement over random initialization is explained by pre-training with the MoCoV2 algorithm in general, most of the improvement comes from the characteristics of the pre-training data.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Models pre-trained on CEM500K yield superior segmentation quality and training speed on all segmentation benchmarks.</title><p>(<bold>a</bold>) Plot of percent difference in segmentation performance between pre-trained models and a randomly initialized model. (<bold>b</bold>) Example segmentations on the UroCell benchmark in 3D (top) and 2D (bottom). The black arrows show the location of the same mitochondrion in 2D and in 3D. (<bold>c</bold>) Example segmentations from all 2D-only benchmark datasets. The red arrow marks a false negative in ground truth segmentation detected by the CEM500K-moco pre-trained model. (<bold>d</bold>) Top, average IoU scores as a percent of the average IoU after 10,000 training iterations, bottom, absolute average IoU scores over a range of training iteration lengths.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>IoU scores for different pre-training protocols.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-65894-fig4-data1-v1.xlsx"/></supplementary-material></p><p><supplementary-material id="fig4sdata2"><label>Figure 4—source data 2.</label><caption><title>IoU scores for different training iterations by pre-training protocol .</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-65894-fig4-data2-v1.xlsx"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-fig4-v1.tif"/></fig><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Comparison of segmentation IoU scores for different weight initialization methods versus the best results on each benchmark as reported in the publication presenting the segmentation task.</title><p>All IoU scores are the average of five independent runs. References listed after the benchmark names indicate the sources for Reported IoU scores.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Benchmark</th><th>Training Iterations</th><th>Random Init.</th><th>IN-super</th><th>IN-moco</th><th>CEM500K-moco</th><th>Reported</th></tr></thead><tbody><tr><td>All Mitochondria</td><td>10000</td><td>0.587</td><td>0.653</td><td>0.653</td><td><bold><underline>0.770</underline></bold></td><td>–</td></tr><tr><td>CREMI Synaptic Clefts</td><td>5000</td><td>0.000</td><td>0.196</td><td>0.226</td><td><bold><underline>0.254</underline></bold></td><td>–</td></tr><tr><td>Guay (<xref ref-type="bibr" rid="bib19">Guay et al., 2020</xref>)</td><td>1000</td><td>0.308</td><td>0.275</td><td>0.300</td><td><bold><underline>0.429</underline></bold></td><td>0.417</td></tr><tr><td>Kasthuri++ (<xref ref-type="bibr" rid="bib6">Casser et al., 2018</xref>)</td><td>10000</td><td>0.905</td><td>0.908</td><td>0.911</td><td><bold><underline>0.915</underline></bold></td><td>0.845</td></tr><tr><td>Lucchi++ (<xref ref-type="bibr" rid="bib6">Casser et al., 2018</xref>)</td><td>10000</td><td>0.894</td><td>0.865</td><td>0.892</td><td><bold><underline>0.895</underline></bold></td><td>0.888</td></tr><tr><td>Perez (<xref ref-type="bibr" rid="bib42">Perez et al., 2014</xref>)</td><td>2500</td><td>0.672</td><td>0.886</td><td>0.883</td><td><bold><underline>0.901</underline></bold></td><td>0.821</td></tr><tr><td>Lysosomes</td><td>–</td><td>0.842</td><td>0.838</td><td>0.816</td><td><bold><underline>0.849</underline></bold></td><td>0.726</td></tr><tr><td>Mitochondria</td><td>–</td><td>0.130</td><td>0.860</td><td>0.866</td><td><bold><underline>0.884</underline></bold></td><td>0.780</td></tr><tr><td>Nuclei</td><td>–</td><td>0.984</td><td>0.987</td><td>0.986</td><td><bold><underline>0.988</underline></bold></td><td>0.942</td></tr><tr><td>Nucleoli</td><td>–</td><td>0.731</td><td>0.859</td><td>0.865</td><td><bold><underline>0.885</underline></bold></td><td>0.835</td></tr><tr><td>UroCell</td><td>2500</td><td>0.424</td><td>0.584</td><td>0.618</td><td><bold><underline>0.734</underline></bold></td><td>–</td></tr></tbody></table></table-wrap><p>In addition to better IoU performance, pre-trained models converged more quickly. We found that models pre-trained with the MoCoV2 algorithm converged the fastest (<xref ref-type="fig" rid="fig4">Figure 4d</xref>, top). Within just 500 iterations, these models reach over 90% of their performance at 10,000 training iterations, and within only 100 iterations, they achieve over 80%. In some cases, 100 iterations required less than 45 s of training on our hardware, which included an Nvidia P100 GPU, making this approach more feasible for resource-limited work. We posit that the faster training associated with the MoCoV2 algorithm stems from the much lower magnitudes of feature activations, as observed in <xref ref-type="bibr" rid="bib52">Tian et al., 2019</xref>, which facilitates training with higher learning rates. CEM500K-moco models trained marginally faster than IN-moco models. This speedup may have stemmed from CEM500K-moco’s better robustness to the chosen data augmentations, reducing variance in the feature maps received by the trainable U-Net decoder. Overall, these results suggest a suitability of CEM500K-moco models for applications where rapid turnarounds for, say, a roughly accurate segmentation may be desired. In cases where more accurate segmentations are required, faster training as we see in <xref ref-type="fig" rid="fig4">Figure 4d</xref> reduces the amount of time needed for hyperparameter optimization.</p><p>Finally, the plot of average IoU scores over a range of training iterations showed that the performance of randomly initialized models leveled off after 5000 iterations, see <xref ref-type="fig" rid="fig4">Figure 4d</xref>, bottom. Previously, it has been observed that granted enough time to converge, randomly initialized models can often achieve comparable results to pre-trained models (<xref ref-type="bibr" rid="bib21">He et al., 2018</xref>), and we did observe this for the easiest benchmarks (Perez, Lucchi++, and Kasthuri++, data not shown). After 30,000 iterations of training on these benchmarks, the performance of randomly initialized models effectively reached parity with CEM500K-moco models. However, for the hard benchmarks, randomly initialized models never reached the average IoU scores measured at even just 500 training iterations for CEM500K-moco models. ImageNet pre-trained models, on the other hand, had the lowest average IoUs on easy benchmarks, but were better than random initialization for hard benchmarks. All of these observations align with expectations. Pre-trained models with frozen encoders only have 9 × 10<sup>6</sup> parameters to fit to the data. On easy benchmarks where overfitting is not a concern, this reduction in trainable parameters hurt ImageNet pre-trained models, but not CEM500K-moco models, since the latter were already pre-trained to EM data. On hard benchmarks, the regularization effects of having fewer trainable parameters are an advantage. Randomly initialized models continued to decrease training loss on hard benchmarks, yet those gains did not translate to increases in test set IoU, a signature of overfitting (data not shown). Overfitting may be avoided by smaller models with fewer trainable parameters, similar to the pre-trained models; however, this would require costly and slow additional engineering and hyperparameter optimization for each benchmark. Our results show that regardless of whether benchmarks are easy or hard, CEM500K-moco pre-trained models trained the fastest and achieved the best IoU scores. Indeed, these models outperformed the customized algorithms and training schemes presented as baselines for four of the benchmarks that we tested (by 3.0% on Guay, 8.6% on Kasthuri++, 1.2% on Lucchi++, and 10% on Perez), see <xref ref-type="table" rid="table2">Table 2</xref>. The All Mitochondria benchmark is a newly derived dataset and therefore has not been previously evaluated, but we show that it is a relatively challenging benchmark and suggest its use as a baseline for future comparisons. The remaining two benchmarks (CREMI Synaptic Clefts and UroCell) used special evaluation methods that were incompatible with our work (see 'Materials and methods'); instead, we present a representative visual comparison of our best results with those from the UroCell publication (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>) showing a marked improvement in mitochondria (blue) and lysosome (red) 3D reconstructions. While ImageNet pre-trained models are broadly useful, our results show that for some EM segmentation tasks they perform worse than random initialization. For all the available benchmarks and the newly derived All Mitochondria benchmark, CEM500K-moco pre-training uniformly performed better than the current alternatives and we demonstrate here its reliability and effectiveness for EM-specific transfer learning.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>CEM500K is a diverse, relevant, information-rich, and non-redundant dataset of unlabeled cellular EM images designed expressly to aid in the development of more robust and general DL models. Above all, two features distinguish CEM500K from other larger, publicly available EM datasets that make it superior for DL applications. First, it is derived from a far greater variety of tissue types, experimental conditions, and imaging techniques, resulting in models with less bias toward such specific variables. Second, it is condensed by aggressively deleting redundant and uninformative images; this improves model performance and renders CEM500K more accessible to users. By evaluating on seven benchmarks that represent different segmentation tasks and biological contexts, we demonstrate that, on average, models pre-trained on CEM500K performed better than those pre-trained on a dataset extracted from a single large EM volume (Bloss). Remarkably, the targeted removal of 90% of the images from the original corpus of data to generate CEM500K returned a significant increase in the quality of pre-trained parameters as measured by segmentation IoU scores.</p><p>This raises the question of what the nature and extent of dataset curation should be: If a target segmentation task contains data from a particular biological context, should the pre-training dataset be curated specifically for that context? And would pre-training on the task data alone result in adequate models? Our results suggest that the benefits from curating the pre-training dataset for a particular context are minimal. Pre-training exclusively on images of mouse brain tissue (Bloss) did not improve performance over CEM500K on benchmarks from that same tissue (see <xref ref-type="fig" rid="fig2">Figure 2e</xref>). Pre-training exclusively on images from a target dataset (say, for a segmentation task) is possible but would have limited utility as it nullifies the advantages introduced by dataset heterogeneity. Further, we speculate that as dataset size decreases, it becomes more likely that a model will overfit to the pre-training task and learn image features that are irrelevant for other downstream tasks (<xref ref-type="bibr" rid="bib53">Tian and Sun, 2020</xref>; <xref ref-type="bibr" rid="bib38">Minderer et al., 2020</xref>). Other unsupervised pre-training algorithms that work for smaller datasets and/or larger benchmark datasets would be needed to determine the appropriate curation approach.</p><p>Regardless, we have shown here that parameters trained on CEM500K are a strong and general-purpose starting point for improving downstream segmentation models. U-Nets pre-trained on CEM500K significantly outperformed randomly initialized U-Nets on all of the segmentation benchmarks that we tested, with the largest improvements corresponding to the most difficult benchmarks. Impressively, such pre-trained models achieved state-of-the-art IoU scores on all benchmarks for which comparison with previous results was possible. The only variables tuned were the number of training iterations and data augmentations. Use of CEM500K pre-trained models by the EM community may reveal that further tuning of hyperparameters or unfreezing of the U-Net’s encoder parameters could further boost performance.</p><p>Our work focused on the application of CEM500K for transfer learning. This decision was informed by the current status of DL research for cellular EM, where, typically, segmentation tasks are performed by models trained on a few labeled examples (<xref ref-type="bibr" rid="bib16">Funke et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Guay et al., 2020</xref>; <xref ref-type="bibr" rid="bib42">Perez et al., 2014</xref>; <xref ref-type="bibr" rid="bib61">Žerovnik Mekuč et al., 2020</xref>; <xref ref-type="bibr" rid="bib6">Casser et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">CREMI, 2016</xref>). In general, pre-trained parameters have been shown to guide downstream models to converge to more general optima than they would from random initialization (<xref ref-type="bibr" rid="bib26">Huh et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Yosinski et al., 2014</xref>; <xref ref-type="bibr" rid="bib39">Neyshabur et al., 2020</xref>). As the number of examples in the training dataset increases the generalization benefits from transfer learning start to diminish (gains in training speed are retained)(<xref ref-type="bibr" rid="bib62">Zoph, 2020</xref>; <xref ref-type="bibr" rid="bib21">He et al., 2018</xref>). Therefore, while unsupervised pre-training on CEM500K for transfer learning has demonstrably high utility for the common paradigm of ‘train on labeled ROIs/infer labels for the whole dataset’, currently it cannot solve the problem of creating general segmentation models that reliably segment features of interest for data generated by novel experiments. However, using CEM500K as seed data provides a path forward for tackling this much more difficult challenge. With 0.5 × 10<sup>6</sup> uniquely informative images representing approximately six hundred 3D and ten thousand 2D images corresponding to more than 100 completely unrelated biological projects, CEM500K is to our knowledge the most comprehensive and diversified resource of cellular EM images. Annotating images from CEM500K (or identifying them as negative examples) will enable the creation of new task-specific training datasets with substantially more variety than previously available. Models trained on such datasets will likely be better equipped to handle data from new microscopes, biological contexts, and sample preparation protocols. Moreover, each image chosen for annotation from CEM500K is likely to be uniquely informative for a model because of the extensive deduplication and filtering pipeline that we have created and used here, and which we share for future work by the community.</p><p>The available benchmark datasets that we chose are a reflection of common applications of DL to cellular EM data, but they do not cover the full scope of possible segmentation tasks. In particular, all but one of the benchmarks involved the annotation of mitochondria and three of the seven were from mouse brain tissue. We observed that benchmark variety is essential to identify biases in pre-trained parameters and that difficult tasks are a necessary and stringent test of pre-training algorithms or datasets. For example, visual inspection of the label maps in <xref ref-type="fig" rid="fig4">Figure 4c</xref> makes it obvious that our results leave little room for improvement on relatively easy (and 2D only) benchmarks like Lucchi++, Kasthuri++, and Perez, suggesting that going forward, new and more challenging benchmarks will be required.</p><p>Additionally, we only tested semantic and not instance segmentation (i.e. all objects from one class share one label). We made this decision in order to avoid the more complex model architectures, postprocessing, and hyperparameters that usually accompany instance segmentation (<xref ref-type="bibr" rid="bib24">Heinrich et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Funke et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Januszewski et al., 2018</xref>). Focusing on simple end-to-end semantic segmentation tasks emphasizes the effects of pre-training and eliminates the possibility that non-DL algorithms could confound the interpretation of our results. 2D models work well for semantic segmentation in both 2D and 3D (our 2D models beat the state-of-the-art results set by 3D models on some of the benchmarks, see <xref ref-type="table" rid="table2">Table 2</xref>) and from a computational standpoint, running training and inference on 2D data typically requires fewer resources. Therefore, we believe that 2D pre-trained parameters are broadly useful for cellular EM researchers, especially for laboratories with limited access to high-performance computing. For some complex tasks like 3D instance segmentation, an important and common task in cellular EM connectomics research, 2D models may perform poorly. Unsupervised pre-training on 3D data is currently an underexplored research area and algorithms like MoCoV2 should work for pre-training 3D models. The creation of a 3D image dataset similar to the resource reported here is likely to be of great use to the nascent volume EM community.</p><p>The goal of this work is to begin the process of creating a data ecosystem for cellular EM images and datasets. CEM500K will be a valuable resource for experimenting with and taking advantage of the latest developments in DL research, where access to troves of image data is usually taken for granted. To further increase its utility, more data from uncommon organisms, tissue and cell types, sample preparation protocols, and acquisition parameters will be needed. In the current state, the dataset is still heavily skewed to a few common organisms like mice and tissues like brain, and it is clear that there is much room for greater sampling and heterogeneity (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>). We hope that other researchers will consider using the curation tools that we developed in this work to contribute to CEM500K. The dataset has been made available for download on EMPIAR (ID 10592). The massive reduction in dataset size from curation makes the sharing of data relatively quick and easy; moreover, the elimination of 3D context from volume EM datasets ensures that the shared data can only reasonably be used for DL applications. Similar to pre-training on natural images, we expect that the quality of the pre-trained parameters for transfer learning will improve logarithmically as CEM500K grows (<xref ref-type="bibr" rid="bib37">Mahajan, 2018</xref>). In the meantime, the pre-trained parameters that we release here can serve as the foundation for rapidly prototyping and building more general segmentation models for cellular EM data.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Dataset standardization</title><p>Datasets generated from microscopes in our lab were already in the desired standardized format: 8-bit unsigned volumes or 2D tiff images. Publicly available EM data are in a variety of file formats and data types; these datasets were individually reformatted as needed to match the formatting of our internal datasets. Importantly, data from each of the seven benchmarks we tested were included as well but comprised less than 0.1% of the dataset. To reduce the memory requirements of large 3D volumes, datasets were downsampled such that no individual dataset was larger than 5 GB (affecting only seven of the total 591 image volumes). The majority of 3D datasets included metadata of their image resolutions; isotropic and anisotropic volumes were thus automatically identified and processed differently. For all isotropic voxel data and for any anisotropic voxel data in which the z resolution was less than 20% different from the x and y resolutions, 2D cross-sections from the xy, xz, and yz planes were sequentially extracted. Anisotropic voxel data with a greater than 20% difference in axial versus lateral resolutions were only sliced into cross-sections in the xy plane. At this point, all of the gathered image data was in the format of 2D tiff images, though with variable heights and widths. These images were cropped into 224 × 224 patches without any overlap. If the image’s width or height was not a multiple of 224, then crops from the remaining area were discarded if either of their dimensions were less than 112.</p><p>Separately, additional 2D images available through the Open Connectome Project were collected. As these volumes were too large to reasonably download and store (tens of TB), Cloud Volume API was used to randomly sample 1000 2D patches from the xy planes of each available dataset. These extracted patches were already of the correct size and format, therefore no further processing was required. This corpus of 5.3 × 10<sup>6</sup> 2D patches constitutes ‘CEMraw’. Certain datasets were not accessible with this method and were therefore not included in the final version of CEMraw (see Supplementary Materials). The ‘Bloss baseline’ dataset (<xref ref-type="bibr" rid="bib2">Bloss et al., 2018</xref>) was also extracted and generated with this method; however, 1 × 10<sup>6</sup> patches were collected from that single data volume to roughly match the number of images in CEMraw (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>).</p></sec><sec id="s4-2"><title>Deduplication</title><p>To remove duplicate patches, image hashes for all 5.3 × 10<sup>6</sup> images in CEMraw were calculated. Difference hashes gave the best results of all the hashing algorithms tested (<xref ref-type="bibr" rid="bib33">Kind of Like That, 2013</xref>). A hash size of 8 results in a 64-bit array to encode each 224 × 224 image. The similarity of two images was then measured by the Hamming distance between their hashes. A pairwise comparison of all 5.3 × 10<sup>6</sup> hashes was not computationally feasible or meaningful. Instead, hashes belonging to the same 2D or 3D source dataset were compared. For a 64-bit hash, distances range from 0 to 64. Sets of hashes with a distance &lt;12 (distance cutoff chosen by visual inspection of groups) between them were considered a group of near-duplicates. All but one randomly chosen image from each group were dropped (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Together, the resulting 1.1 × 10<sup>6</sup> images constitute a deduplicated dataset or ‘CEMdedup’.</p></sec><sec id="s4-3"><title>Uninformative patch filtering</title><p>A random subset of 14,000 images from CEMdedup were manually labeled either informative or uninformative. The criteria for this classification process were informed by the hyperparameters of the MoCoV2 pre-training algorithm, which takes random crops as small as 20% of an area of an image. For an image that is only 20% informative, there is a 30% chance that such a randomly drawn crop will be completely uninformative, and this fraction increases exponentially for images less than 20% informative (<xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7</xref>). Therefore, 20% was chosen as the cutoff for manual labeling. Concretely, this means that images with 80% or more of their area occupied by uniform intensity structures like nuclei, cytoplasm, or resin are classified as uninformative. Other criteria included whether the image was low-contrast, displayed reconstruction artifacts (not sample preparation or image acquisition artifacts), or contained non-cellular objects as determined by a human annotator. A breakdown of the frequency of traits present in a subset of uninformative patches is shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1a</xref>.</p><p>2000 labeled images were set aside as a test set and the remaining 12,000 were used as training data for a model classifier: a ResNet34 pre-trained on ImageNet. The fourth layer of residual blocks and the classification head of the model were fine-tuned for 30 epochs on a P100 GPU with the Adam optimizer and a learning rate of 0.001. A Random Forest classifier trained on four image-level statistics (the standard deviations of the local binary pattern [<xref ref-type="bibr" rid="bib40">Ojala et al., 2002</xref>] and image entropy, the median of the geometric mean, and the mean value of a canny edge detector [<xref ref-type="bibr" rid="bib4">Canny, 1986</xref>]) was also tested. These features were chosen from a larger superset based on their measured importance. The performance for the two classifiers is shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1b</xref>. The DL model was used to create CEM500K with a confidence threshold set at 0.5. Since the benchmark data themselves were put through the curation pipeline, approximately 5% of images in CEM500K are from one of the six benchmark datasets; we show that the presence of this small fraction has a marginal effect on performance (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p></sec><sec id="s4-4"><title>Momentum contrast pre-training</title><p>For unsupervised pre-training, the Momentum Contrast (MoCoV2) algorithm (<xref ref-type="bibr" rid="bib44">Raghu et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Tian et al., 2019</xref>) was used. A schematic of a single step in the algorithm is shown in <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3a</xref>. Pre-training was completed on a machine with four Nvidia V100 GPUs using a batch size of 128 and queue length of 65,536. The initial learning rate was set to 0.015 and divided by 10 at epochs 120 and 160. In addition, 360° rotations and Gaussian noise with a standard deviation range of 1 × 10<sup>−5</sup> to 1 × 10<sup>−4</sup> were added to the data augmentations. All other hyperparameters and data augmentations were left as the defaults presented in <xref ref-type="bibr" rid="bib52">Tian et al., 2019</xref>. For pre-training comparisons between different EM datasets, that is the three subsets of CEM plus Bloss (<xref ref-type="fig" rid="fig2">Figure 2d,e</xref>), 4.5 × 10<sup>5</sup> total parameter updates (iterations) were run for each model, which is equivalent to 120 passes (epochs) through all the images in CEM500K. The average training time for each of these models was 2.5 days. The final pre-trained parameters generated for results shown in <xref ref-type="fig" rid="fig4">Figure 4b,c</xref> were trained on CEM500K for an additional 80 epochs: a total of 200 epochs and 4 days of training.</p></sec><sec id="s4-5"><title>U-Net segmentation architecture</title><p>Our implementation was similar to the original implementation of the U-Net, except that the encoder was replaced with a ResNet50 model (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3b</xref>). When using pre-trained models in these experiments, all parameters in the encoder were frozen such that no updates were made during training. Randomly initialized encoders were tested with both frozen and unfrozen parameters. The random number generator seed was fixed at 42 such that any randomly initialized parameters in either the U-Net encoder or decoder would be the same in every experiment.</p></sec><sec id="s4-6"><title>Benchmark segmentation tasks</title><p>The One Cycle Policy and AdamW optimizer with maximum learning rate 0.003, weight decay 0.1, batch size 16, and (binary) cross-entropy loss were used for all benchmarks (<xref ref-type="bibr" rid="bib36">Loshchilov and Hutter, 2017</xref>; <xref ref-type="bibr" rid="bib47">Smith, 2018</xref>). For the Guay and Urocell benchmarks, which required multiclass segmentation, the cross-entropy loss was weighted by the prevalence of each class; we observed that this yielded better IoU scores on the Guay validation dataset (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). Classes that accounted for less than 10% of all pixels in the dataset were given a weight of 3, those that accounted for more than 10% were given a weight of 1, and all background classes were given a weight of 0.1. Data augmentations included randomly resized crops with scaling from 0.08 to 1 and aspect ratio from 0.5 to 1.5, 360° rotations, random 30% brightness and contrast adjustments, and horizontal and vertical flips. For the Guay benchmark, and consequently the All Mitochondria benchmark, Gaussian noise with a variance limit of 400–1200 and Gaussian blur with a maximum standard deviation of 7 were also added. The decision to add more data augmentations for these benchmarks was made in response to observed overfitting on the Guay benchmark validation dataset (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). Lastly, different crop sizes were used for each benchmark: 512 × 512 for Guay, CREMI, Synaptic Cleft, Kasthuri++, and Lucchi++; 480 × 480 for Perez; and 224 × 224 for UroCell and All Mitochondria.</p><p>To create 3D segmentations for the UroCell, Guay, and CREMI Synaptic Cleft, test sets we used either orthoplane or 2D stack inference following <xref ref-type="bibr" rid="bib9">Conrad et al., 2020</xref>. Briefly, in 2D stack inference the model only makes predictions on xy cross-sections; in orthoplane inference, the model makes predictions on xy, yz, and xz cross-sections and the confidence scores are averaged together. Orthoplane inference was used for the UroCell test set because its test volume has isotropic voxels. Because both the Guay and CREMI Synaptic Cleft test volumes are anisotropic, we used 2D stack inference instead.</p><p>Evaluation generally followed the details given in the publication that accompanied the benchmark. First, test images in the Perez datasets did not have labels for all instances of an object; for example, only one nucleus was labeled in an image containing two nuclei. To circumvent this problem, we ignored areas in the predicted segmentations that did not coincide with a labeled instance in the ground truth. Second, the UroCell benchmark was evaluated in previous work by averaging K-Fold cross-validation results on five unique splits of the five training volumes such that each training volume was used as the test set once. The authors also excluded pixels on the boundary of object instances both when training and when calculating the prediction’s IoU with ground truth. Here, a simpler evaluation was run on a single split of the data with four volumes used for training and one volume used for testing. To eliminate small regions of missing data, we cropped two of the five volumes along the y axis (fib1-0-0-0.nii.gz, the test volume, by 12 pixels and fib1-1-0-3.nii.gz by 54 pixels). Third, the test data for the CREMI Synaptic Cleft benchmark is not publicly available and the challenge uses a different evaluation metric than IoU. Therefore, volumes A and B were used exclusively for training and IoU scores were evaluated on volume C.</p></sec><sec id="s4-7"><title>Mean firing rate</title><p>Following <xref ref-type="bibr" rid="bib17">Goodfellow et al., 2009</xref>, neuron firing thresholds were determined by passing 1000 images of randomly sampled noise through each pre-trained ResNet50 model and calculating the 99th percentile of responses. In our experiments, only the neurons in the output of the global average pooling layer were considered such that there were 2048. Responses to 100 randomly selected images from CEM500K were then recorded over a range of distortion strengths. For each neuron, the set of undistorted images that activated the neuron near maximally (over the 90th percentile), called Z, was determined. A set containing versions of all images in Z with a particular distortion applied is called Z’. Any neuron that responded to images in Z less strongly than the neuron’s firing threshold were ignored as they are not selective for features observed in the test images. However, for all remaining neurons, the firing rate at a particular distortion strength is calculated as the number of images in Z’ that activate the neuron over its firing threshold divided by the number of images in Z. The mean firing rate to a particular distortion is then the average of firing rates for any of the 2048 neurons that were selective enough to be considered.</p></sec><sec id="s4-8"><title>Feature selectivity</title><p>To measure feature selectivity, we first manually segmented three organelles (ER, mitochondria, nucleus) in three images. By construction, the ResNet50 architecture downsamples an input image by 32. For thin and small organelles like ER, the final feature maps were too coarse to accurately show the localization of responses. Therefore, we eliminated the last four downsampling operations such that the output feature map was only 2x smaller than the input. Following similar logic, we eliminated the last two downsampling operations for mitochondria and the last downsampling operation for nuclei – 8x and 16x smaller than the input images, respectively. For all organelles, these differently downsampled feature maps were resized to match the dimensions of the input image (224 × 224) and then each feature map was compared against the ground truth labelmap by Point Biserial correlation. A simple average of the 32 most correlated feature maps was then overlaid on the original image as the mean response. Drawing a threshold at 0.3 yielded the binary segmentations.</p></sec><sec id="s4-9"><title>Occlusion analysis</title><p>Typically, occlusion analysis measures the importance of regions in an image to a classification task (<xref ref-type="bibr" rid="bib60">Zeiler and Fergus, 2014</xref>). In our experiments, importance was measured as a function of the dot product similarity between the feature vectors output by the global average pooling layer of a ResNet50 for an image and its occluded copy. Sequential regions of 61 × 61 pixels spaced every 30 pixels (in both x and y dimensions) were zeroed out in each image. Region importance to the similarity measurement was then normalized to fall in the range 0–1 and overlaid on the original image.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank the creators of the benchmarks and other datasets for making the image data freely available to the community. We thank Patrick Friday for help with running some of the models described herein, members of the CMM for contributing EM images, and FNL and NCI colleagues for critical comments on this manuscript. This work utilized the computational resources of the NIH HPC Biowulf cluster (<ext-link ext-link-type="uri" xlink:href="http://hpc.nih.gov">http://hpc.nih.gov</ext-link>). This project has been funded in whole or in part with Federal funds from the National Cancer Institute, National Institutes of Health, under contract no. 75N91019D00024. The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products, or organizations imply endorsement by the U.S. Government.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Visualization, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="sdata1"><label>Source data 1.</label><caption><title>Details of image datasets acquired from external sources.</title></caption><media mime-subtype="xlsx" mimetype="application" xlink:href="elife-65894-data1-v1.xlsx"/></supplementary-material><supplementary-material id="sdata2"><label>Source data 2.</label><caption><title>Zipped folder containing .xl files for <xref ref-type="fig" rid="fig1">Figure 1</xref>, <xref ref-type="fig" rid="fig2">2</xref> and <xref ref-type="fig" rid="fig4">4</xref> source data.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-65894-data2-v1.zip"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Details of benchmarks used in this paper.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-65894-supp1-v1.docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>IoU scores for pre-training with CEM500K after removing benchmark data from pre-training dataset .</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-65894-supp2-v1.docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>IoU scores on Guay benchmark using different hyperparameter choices.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-65894-supp3-v1.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-65894-transrepform-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The CEM500K dataset, metadata, and pre-trained weights available at: <ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/pdbe/emdb/empiar/entry/10592/">https://www.ebi.ac.uk/pdbe/emdb/empiar/entry/10592/</ext-link>. The full code for curation pipeline is available at <ext-link ext-link-type="uri" xlink:href="https://git.io/JLLTz">https://git.io/JLLTz</ext-link>. Links to publicly available benchmark datasets are provided in Supplementary File 1.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Conrad</surname><given-names>R</given-names></name><name><surname>Narayan</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>CEM500K</data-title><source>Electron Microscopy Public Image Archive</source><pub-id assigning-authority="EBI" pub-id-type="accession" xlink:href="https://www.ebi.ac.uk/pdbe/emdb/empiar/entry/10592/">EMPIAR-10592</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berning</surname> <given-names>M</given-names></name><name><surname>Boergens</surname> <given-names>KM</given-names></name><name><surname>Helmstaedter</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>SegEM: efficient image analysis for High-Resolution connectomics</article-title><source>Neuron</source><volume>87</volume><fpage>1193</fpage><lpage>1206</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.003</pub-id><pub-id pub-id-type="pmid">26402603</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bloss</surname> <given-names>EB</given-names></name><name><surname>Cembrowski</surname> <given-names>MS</given-names></name><name><surname>Karsh</surname> <given-names>B</given-names></name><name><surname>Colonell</surname> <given-names>J</given-names></name><name><surname>Fetter</surname> <given-names>RD</given-names></name><name><surname>Spruston</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Single excitatory axons form clustered synapses onto CA1 pyramidal cell dendrites</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>353</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0084-6</pub-id><pub-id pub-id-type="pmid">29459763</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Buhmann</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Automatic detection of synaptic partners in a Whole-Brain <italic>Drosophila</italic> EM Dataset</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2019.12.12.874172</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canny</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>A computational approach to edge detection</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>8</volume><fpage>679</fpage><lpage>698</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.1986.4767851</pub-id><pub-id pub-id-type="pmid">21869365</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Carion</surname> <given-names>N</given-names></name><name><surname>Massa</surname> <given-names>F</given-names></name><name><surname>Synnaeve</surname> <given-names>G</given-names></name><name><surname>Usunier</surname> <given-names>N</given-names></name><name><surname>Kirillov</surname> <given-names>A</given-names></name><name><surname>Zagoruyko</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>End-to-End object detection with transformers</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2005.12872">https://arxiv.org/abs/2005.12872</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Casser</surname> <given-names>V</given-names></name><name><surname>Kang</surname> <given-names>K</given-names></name><name><surname>Pfister</surname> <given-names>H</given-names></name><name><surname>Haehn</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fast mitochondria segmentation for connectomics</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1812.06024">https://arxiv.org/abs/1812.06024</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>T</given-names></name><name><surname>Kornblith</surname> <given-names>S</given-names></name><name><surname>Norouzi</surname> <given-names>M</given-names></name><name><surname>Hinton</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>A simple framework for contrastive learning of visual representations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>X</given-names></name><name><surname>Fan</surname> <given-names>H</given-names></name><name><surname>Girshick</surname> <given-names>R</given-names></name><name><surname>He</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Improved baselines with momentum contrastive learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2003.04297">https://arxiv.org/abs/2003.04297</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conrad</surname> <given-names>R</given-names></name><name><surname>Lee</surname> <given-names>H</given-names></name><name><surname>Narayan</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Enforcing prediction consistency across orthogonal Planes significantly improves segmentation of FIB-SEM image volumes by 2D neural networks</article-title><source>Microscopy and Microanalysis</source><volume>26</volume><fpage>2128</fpage><lpage>2130</lpage><pub-id pub-id-type="doi">10.1017/S143192762002053X</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="web"><person-group person-group-type="author"><collab>CREMI</collab></person-group><year iso-8601-date="2016">2016</year><article-title>Miccai challenge on circuit reconstruction from electron microscopy images</article-title><ext-link ext-link-type="uri" xlink:href="https://cremi.org/">https://cremi.org/</ext-link><date-in-citation iso-8601-date="2020-10-27">October 27, 2020</date-in-citation></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname> <given-names>J</given-names></name><name><surname>Dong</surname> <given-names>W</given-names></name><name><surname>Socher</surname> <given-names>R</given-names></name><name><surname>L.-J.</surname> <given-names>L</given-names></name><name><surname>Li</surname> <given-names>K</given-names></name><name><surname>Fei-Fei</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ImageNet: a Large-Scale hierarchical image database</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devan</surname> <given-names>KS</given-names></name><name><surname>Walther</surname> <given-names>P</given-names></name><name><surname>von Einem</surname> <given-names>J</given-names></name><name><surname>Ropinski</surname> <given-names>T</given-names></name><name><surname>Kestler</surname> <given-names>HA</given-names></name><name><surname>Read</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Detection of herpesvirus capsids in transmission electron microscopy images using transfer learning</article-title><source>Histochemistry and Cell Biology</source><volume>151</volume><fpage>101</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1007/s00418-018-1759-5</pub-id><pub-id pub-id-type="pmid">30488339</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Devlin</surname> <given-names>J</given-names></name><name><surname>Chang</surname> <given-names>M-W</given-names></name><name><surname>Lee</surname> <given-names>K</given-names></name><name><surname>Toutanova</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>BERT: pre-training of deep bidirectional transformers for language understanding</article-title><conf-name>NAACL HLT 2019 - 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf</conf-name><fpage>4171</fpage><lpage>4186</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Donahue</surname> <given-names>J</given-names></name><name><surname>Simonyan</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Large scale adversarial representation learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1907.02544">https://arxiv.org/abs/1907.02544</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>EyeWirers</collab><name><surname>Kim</surname> <given-names>JS</given-names></name><name><surname>Greene</surname> <given-names>MJ</given-names></name><name><surname>Zlateski</surname> <given-names>A</given-names></name><name><surname>Lee</surname> <given-names>K</given-names></name><name><surname>Richardson</surname> <given-names>M</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name><name><surname>Purcaro</surname> <given-names>M</given-names></name><name><surname>Balkam</surname> <given-names>M</given-names></name><name><surname>Robinson</surname> <given-names>A</given-names></name><name><surname>Behabadi</surname> <given-names>BF</given-names></name><name><surname>Campos</surname> <given-names>M</given-names></name><name><surname>Denk</surname> <given-names>W</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Space-time wiring specificity supports direction selectivity in the retina</article-title><source>Nature</source><volume>509</volume><fpage>331</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1038/nature13240</pub-id><pub-id pub-id-type="pmid">24805243</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funke</surname> <given-names>J</given-names></name><name><surname>Tschopp</surname> <given-names>F</given-names></name><name><surname>Grisaitis</surname> <given-names>W</given-names></name><name><surname>Sheridan</surname> <given-names>A</given-names></name><name><surname>Singh</surname> <given-names>C</given-names></name><name><surname>Saalfeld</surname> <given-names>S</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Large scale image segmentation with structured loss based deep learning for connectome reconstruction</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>41</volume><fpage>1669</fpage><lpage>1680</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2018.2835450</pub-id><pub-id pub-id-type="pmid">29993708</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodfellow</surname> <given-names>IJ</given-names></name><name><surname>Le</surname> <given-names>QV</given-names></name><name><surname>Saxe</surname> <given-names>AM</given-names></name><name><surname>Lee</surname> <given-names>H</given-names></name><name><surname>Ng</surname> <given-names>AY</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title><italic>Measuring invariances in deep networks</italic></article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>646</fpage><lpage>654</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Deep Learning</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Guay</surname> <given-names>M</given-names></name><name><surname>Emam</surname> <given-names>Z</given-names></name><name><surname>Anderson</surname> <given-names>A</given-names></name><name><surname>Aronova</surname> <given-names>M</given-names></name><name><surname>Leapman</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dense cellular segmentation using 2D-3D neural network ensembles for electron microscopy</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.05.895003</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep residual learning for image recognition</article-title><conf-name>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Girshick</surname> <given-names>R</given-names></name><name><surname>Dollár</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rethinking ImageNet Pre-Training</article-title><conf-name>Proc. IEEE Int. Conf. Comput. Vis</conf-name><fpage>4917</fpage><lpage>4926</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2019.00502</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Fan</surname> <given-names>H</given-names></name><name><surname>Wu</surname> <given-names>Y</given-names></name><name><surname>Xie</surname> <given-names>S</given-names></name><name><surname>Girshick</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Momentum contrast for unsupervised visual representation learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1911.05722">https://arxiv.org/abs/1911.05722</ext-link></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Gkioxari</surname> <given-names>G</given-names></name><name><surname>Dollár</surname> <given-names>P</given-names></name><name><surname>Girshick</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mask R-CNN</article-title><conf-name>IEEE Transactions on Pattern Analysis and Machine Intelligence</conf-name><fpage>386</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2018.2844175</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Heinrich</surname> <given-names>L</given-names></name><name><surname>Funke</surname> <given-names>J</given-names></name><name><surname>Pape</surname> <given-names>C</given-names></name><name><surname>Nunez-Iglesias</surname> <given-names>J</given-names></name><name><surname>Saalfeld</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Synaptic cleft segmentation in Non-Isotropic volume electron microscopy of the Complete <italic>Drosophila</italic> Brain</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1805.02718">https://arxiv.org/abs/1805.02718</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname> <given-names>DP</given-names></name><name><surname>Shtengel</surname> <given-names>G</given-names></name><name><surname>Xu</surname> <given-names>CS</given-names></name><name><surname>Campbell</surname> <given-names>KR</given-names></name><name><surname>Freeman</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>L</given-names></name><name><surname>Milkie</surname> <given-names>DE</given-names></name><name><surname>Pasolli</surname> <given-names>HA</given-names></name><name><surname>Iyer</surname> <given-names>N</given-names></name><name><surname>Bogovic</surname> <given-names>JA</given-names></name><name><surname>Stabley</surname> <given-names>DR</given-names></name><name><surname>Shirinifard</surname> <given-names>A</given-names></name><name><surname>Pang</surname> <given-names>S</given-names></name><name><surname>Peale</surname> <given-names>D</given-names></name><name><surname>Schaefer</surname> <given-names>K</given-names></name><name><surname>Pomp</surname> <given-names>W</given-names></name><name><surname>Chang</surname> <given-names>CL</given-names></name><name><surname>Lippincott-Schwartz</surname> <given-names>J</given-names></name><name><surname>Kirchhausen</surname> <given-names>T</given-names></name><name><surname>Solecki</surname> <given-names>DJ</given-names></name><name><surname>Betzig</surname> <given-names>E</given-names></name><name><surname>Hess</surname> <given-names>HF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Correlative three-dimensional super-resolution and block-face electron microscopy of whole vitreously frozen cells</article-title><source>Science</source><volume>367</volume><elocation-id>eaaz5357</elocation-id><pub-id pub-id-type="doi">10.1126/science.aaz5357</pub-id><pub-id pub-id-type="pmid">31949053</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Huh</surname> <given-names>M</given-names></name><name><surname>Agrawal</surname> <given-names>P</given-names></name><name><surname>Efros</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What makes ImageNet good for transfer learning?</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1608.08614">https://arxiv.org/abs/1608.08614</ext-link></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iudin</surname> <given-names>A</given-names></name><name><surname>Korir</surname> <given-names>PK</given-names></name><name><surname>Salavert-Torres</surname> <given-names>J</given-names></name><name><surname>Kleywegt</surname> <given-names>GJ</given-names></name><name><surname>Patwardhan</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>EMPIAR: a public archive for raw electron microscopy image data</article-title><source>Nature Methods</source><volume>13</volume><fpage>387</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3806</pub-id><pub-id pub-id-type="pmid">27067018</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Januszewski</surname> <given-names>M</given-names></name><name><surname>Kornfeld</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>PH</given-names></name><name><surname>Pope</surname> <given-names>A</given-names></name><name><surname>Blakely</surname> <given-names>T</given-names></name><name><surname>Lindsey</surname> <given-names>L</given-names></name><name><surname>Maitin-Shepard</surname> <given-names>J</given-names></name><name><surname>Tyka</surname> <given-names>M</given-names></name><name><surname>Denk</surname> <given-names>W</given-names></name><name><surname>Jain</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High-precision automated reconstruction of neurons with flood-filling networks</article-title><source>Nature Methods</source><volume>15</volume><fpage>605</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0049-4</pub-id><pub-id pub-id-type="pmid">30013046</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ji</surname> <given-names>X</given-names></name><name><surname>Henriques</surname> <given-names>JF</given-names></name><name><surname>Vedaldi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Invariant information clustering for unsupervised image classification and segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.06653">https://arxiv.org/abs/1807.06653</ext-link></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ju</surname> <given-names>C</given-names></name><name><surname>Bibaut</surname> <given-names>A</given-names></name><name><surname>Van Der Laan</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The relative performance of ensemble methods with deep convolutional neural networks for image classification</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1704.01664">https://arxiv.org/abs/1704.01664</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karabağ</surname> <given-names>C</given-names></name><name><surname>Jones</surname> <given-names>ML</given-names></name><name><surname>Peddie</surname> <given-names>CJ</given-names></name><name><surname>Weston</surname> <given-names>AE</given-names></name><name><surname>Collinson</surname> <given-names>LM</given-names></name><name><surname>Reyes-Aldasoro</surname> <given-names>CC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Semantic segmentation of HeLa cells: an objective comparison between one traditional algorithm and four deep-learning architectures</article-title><source>PLOS ONE</source><volume>15</volume><elocation-id>e0230605</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0230605</pub-id><pub-id pub-id-type="pmid">33006963</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kasthuri</surname> <given-names>N</given-names></name><name><surname>Hayworth</surname> <given-names>KJ</given-names></name><name><surname>Berger</surname> <given-names>DR</given-names></name><name><surname>Schalek</surname> <given-names>RL</given-names></name><name><surname>Conchello</surname> <given-names>JA</given-names></name><name><surname>Knowles-Barley</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Vázquez-Reina</surname> <given-names>A</given-names></name><name><surname>Kaynig</surname> <given-names>V</given-names></name><name><surname>Jones</surname> <given-names>TR</given-names></name><name><surname>Roberts</surname> <given-names>M</given-names></name><name><surname>Morgan</surname> <given-names>JL</given-names></name><name><surname>Tapia</surname> <given-names>JC</given-names></name><name><surname>Seung</surname> <given-names>HS</given-names></name><name><surname>Roncal</surname> <given-names>WG</given-names></name><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Burns</surname> <given-names>R</given-names></name><name><surname>Sussman</surname> <given-names>DL</given-names></name><name><surname>Priebe</surname> <given-names>CE</given-names></name><name><surname>Pfister</surname> <given-names>H</given-names></name><name><surname>Lichtman</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Saturated reconstruction of a volume of neocortex</article-title><source>Cell</source><volume>162</volume><fpage>648</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2015.06.054</pub-id><pub-id pub-id-type="pmid">26232230</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Kind of Like That</collab></person-group><year iso-8601-date="2013">2013</year><article-title>The hacker factor blog</article-title><ext-link ext-link-type="uri" xlink:href="http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html">http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html</ext-link><date-in-citation iso-8601-date="2020-10-28">October 28, 2020</date-in-citation></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kolesnikov</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Large scale learning of general visual representations for transfer</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.11370">https://arxiv.org/abs/1912.11370</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lichtman</surname> <given-names>JW</given-names></name><name><surname>Pfister</surname> <given-names>H</given-names></name><name><surname>Shavit</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The big data challenges of connectomics</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1448</fpage><lpage>1454</lpage><pub-id pub-id-type="doi">10.1038/nn.3837</pub-id><pub-id pub-id-type="pmid">25349911</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Loshchilov</surname> <given-names>I</given-names></name><name><surname>Hutter</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Decoupled weight decay regularization</article-title><conf-name>7th Int. Conf. Learn. Represent. ICLR</conf-name></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mahajan</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Exploring the limits of weakly supervised pretraining</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1805.00932">https://arxiv.org/abs/1805.00932</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Minderer</surname> <given-names>M</given-names></name><name><surname>Bachem</surname> <given-names>O</given-names></name><name><surname>Houlsby</surname> <given-names>N</given-names></name><name><surname>Tschannen</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Automatic shortcut removal for Self-Supervised representation learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2002.08822">https://arxiv.org/abs/2002.08822</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Neyshabur</surname> <given-names>B</given-names></name><name><surname>Sedghi</surname> <given-names>H</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>What is being transferred in transfer learning?</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2008.11687">https://arxiv.org/abs/2008.11687</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ojala</surname> <given-names>T</given-names></name><name><surname>Pietikainen</surname> <given-names>M</given-names></name><name><surname>Maenpaa</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>24</volume><fpage>971</fpage><lpage>987</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2002.1017623</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>F</given-names></name><name><surname>Norvig</surname> <given-names>P</given-names></name><name><surname>Halev</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The unreasonable effectiveness of data</article-title><conf-name>IEEE Intell. Syst</conf-name><pub-id pub-id-type="doi">10.1109/MIS.2009.36</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perez</surname> <given-names>AJ</given-names></name><name><surname>Seyedhosseini</surname> <given-names>M</given-names></name><name><surname>Deerinck</surname> <given-names>TJ</given-names></name><name><surname>Bushong</surname> <given-names>EA</given-names></name><name><surname>Panda</surname> <given-names>S</given-names></name><name><surname>Tasdizen</surname> <given-names>T</given-names></name><name><surname>Ellisman</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A workflow for the automatic segmentation of organelles in electron microscopy image stacks</article-title><source>Frontiers in Neuroanatomy</source><volume>8</volume><elocation-id>126</elocation-id><pub-id pub-id-type="doi">10.3389/fnana.2014.00126</pub-id><pub-id pub-id-type="pmid">25426032</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plaza</surname> <given-names>SM</given-names></name><name><surname>Funke</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Analyzing image segmentation for connectomics</article-title><source>Frontiers in Neural Circuits</source><volume>12</volume><elocation-id>102</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2018.00102</pub-id><pub-id pub-id-type="pmid">30483069</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Raghu</surname> <given-names>M</given-names></name><name><surname>Zhang</surname> <given-names>C</given-names></name><name><surname>Kleinberg</surname> <given-names>J</given-names></name><name><surname>Bengio</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Transfusion: understanding transfer learning for medical imaging</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1902.07208">https://arxiv.org/abs/1902.07208</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname> <given-names>S</given-names></name><name><surname>He</surname> <given-names>K</given-names></name><name><surname>Girshick</surname> <given-names>R</given-names></name><name><surname>Sun</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Faster R-CNN: towards Real-Time object detection with region proposal networks</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>39</volume><fpage>1137</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id><pub-id pub-id-type="pmid">27295650</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ronneberger</surname> <given-names>O</given-names></name><name><surname>Fischer</surname> <given-names>P</given-names></name><name><surname>Brox</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>“U-net: Convolutional networks for biomedical image segmentation,”</article-title><source>In Lecture Notes in Computer Science</source><volume>9351</volume><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>LN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A disciplined approach to neural network hyper-parameters: part 1 - learning rate, batch size, momentum, and weight decay</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.09820">https://arxiv.org/abs/1803.09820</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Spiers</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Citizen science, cells and CNNs – deep learning for automatic segmentation of the nuclear envelope in electron microscopy data, trained with volunteer segmentations</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.28.223024</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sun</surname> <given-names>C</given-names></name><name><surname>Shrivastava</surname> <given-names>A</given-names></name><name><surname>Singh</surname> <given-names>S</given-names></name><name><surname>Gupta</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Revisiting unreasonable effectiveness of data in deep learning era</article-title><conf-name>Proceedings</conf-name><fpage>843</fpage><lpage>852</lpage></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takemura</surname> <given-names>SY</given-names></name><name><surname>Xu</surname> <given-names>CS</given-names></name><name><surname>Lu</surname> <given-names>Z</given-names></name><name><surname>Rivlin</surname> <given-names>PK</given-names></name><name><surname>Parag</surname> <given-names>T</given-names></name><name><surname>Olbris</surname> <given-names>DJ</given-names></name><name><surname>Plaza</surname> <given-names>S</given-names></name><name><surname>Zhao</surname> <given-names>T</given-names></name><name><surname>Katz</surname> <given-names>WT</given-names></name><name><surname>Umayam</surname> <given-names>L</given-names></name><name><surname>Weaver</surname> <given-names>C</given-names></name><name><surname>Hess</surname> <given-names>HF</given-names></name><name><surname>Horne</surname> <given-names>JA</given-names></name><name><surname>Nunez-Iglesias</surname> <given-names>J</given-names></name><name><surname>Aniceto</surname> <given-names>R</given-names></name><name><surname>Chang</surname> <given-names>LA</given-names></name><name><surname>Lauchie</surname> <given-names>S</given-names></name><name><surname>Nasca</surname> <given-names>A</given-names></name><name><surname>Ogundeyi</surname> <given-names>O</given-names></name><name><surname>Sigmund</surname> <given-names>C</given-names></name><name><surname>Takemura</surname> <given-names>S</given-names></name><name><surname>Tran</surname> <given-names>J</given-names></name><name><surname>Langille</surname> <given-names>C</given-names></name><name><surname>Le Lacheur</surname> <given-names>K</given-names></name><name><surname>McLin</surname> <given-names>S</given-names></name><name><surname>Shinomiya</surname> <given-names>A</given-names></name><name><surname>Chklovskii</surname> <given-names>DB</given-names></name><name><surname>Meinertzhagen</surname> <given-names>IA</given-names></name><name><surname>Scheffer</surname> <given-names>LK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Synaptic circuits and their variations within different columns in the visual system of <italic>Drosophila</italic></article-title><source>PNAS</source><volume>112</volume><fpage>13711</fpage><lpage>13716</lpage><pub-id pub-id-type="doi">10.1073/pnas.1509820112</pub-id><pub-id pub-id-type="pmid">26483464</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tao</surname> <given-names>A</given-names></name><name><surname>Sapra</surname> <given-names>K</given-names></name><name><surname>Catanzaro</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hierarchical Multi-Scale attention for semantic segmentation</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2005.10821">https://arxiv.org/abs/2005.10821</ext-link></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>Y</given-names></name><name><surname>Krishnan</surname> <given-names>D</given-names></name><name><surname>Isola</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Contrastive multiview coding</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1906.05849">https://arxiv.org/abs/1906.05849</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tian</surname> <given-names>Y</given-names></name><name><surname>Sun</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>What makes for good views for contrastive learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2005.10243">https://arxiv.org/abs/2005.10243</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vincent</surname> <given-names>AE</given-names></name><name><surname>Turnbull</surname> <given-names>DM</given-names></name><name><surname>Eisner</surname> <given-names>V</given-names></name><name><surname>Hajnóczky</surname> <given-names>G</given-names></name><name><surname>Picard</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mitochondrial nanotunnels</article-title><source>Trends in Cell Biology</source><volume>11</volume><fpage>787</fpage><lpage>799</lpage></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vincent</surname> <given-names>AE</given-names></name><name><surname>White</surname> <given-names>K</given-names></name><name><surname>Davey</surname> <given-names>T</given-names></name><name><surname>Philips</surname> <given-names>J</given-names></name><name><surname>Ogden</surname> <given-names>RT</given-names></name><name><surname>Lawless</surname> <given-names>C</given-names></name><name><surname>Warren</surname> <given-names>C</given-names></name><name><surname>Hall</surname> <given-names>MG</given-names></name><name><surname>Ng</surname> <given-names>YS</given-names></name><name><surname>Falkous</surname> <given-names>G</given-names></name><name><surname>Holden</surname> <given-names>T</given-names></name><name><surname>Deehan</surname> <given-names>D</given-names></name><name><surname>Taylor</surname> <given-names>RW</given-names></name><name><surname>Turnbull</surname> <given-names>DM</given-names></name><name><surname>Picard</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Quantitative 3D mapping of the human skeletal muscle mitochondrial network</article-title><source>Cell Reports</source><volume>26</volume><fpage>996</fpage><lpage>1009</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2019.01.010</pub-id><pub-id pub-id-type="pmid">30655224</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogelstein</surname> <given-names>JT</given-names></name><name><surname>Perlman</surname> <given-names>E</given-names></name><name><surname>Falk</surname> <given-names>B</given-names></name><name><surname>Baden</surname> <given-names>A</given-names></name><name><surname>Gray Roncal</surname> <given-names>W</given-names></name><name><surname>Chandrashekhar</surname> <given-names>V</given-names></name><name><surname>Collman</surname> <given-names>F</given-names></name><name><surname>Seshamani</surname> <given-names>S</given-names></name><name><surname>Patsolic</surname> <given-names>JL</given-names></name><name><surname>Lillaney</surname> <given-names>K</given-names></name><name><surname>Kazhdan</surname> <given-names>M</given-names></name><name><surname>Hider</surname> <given-names>R</given-names></name><name><surname>Pryor</surname> <given-names>D</given-names></name><name><surname>Matelsky</surname> <given-names>J</given-names></name><name><surname>Gion</surname> <given-names>T</given-names></name><name><surname>Manavalan</surname> <given-names>P</given-names></name><name><surname>Wester</surname> <given-names>B</given-names></name><name><surname>Chevillet</surname> <given-names>M</given-names></name><name><surname>Trautman</surname> <given-names>ET</given-names></name><name><surname>Khairy</surname> <given-names>K</given-names></name><name><surname>Bridgeford</surname> <given-names>E</given-names></name><name><surname>Kleissas</surname> <given-names>DM</given-names></name><name><surname>Tward</surname> <given-names>DJ</given-names></name><name><surname>Crow</surname> <given-names>AK</given-names></name><name><surname>Hsueh</surname> <given-names>B</given-names></name><name><surname>Wright</surname> <given-names>MA</given-names></name><name><surname>Miller</surname> <given-names>MI</given-names></name><name><surname>Smith</surname> <given-names>SJ</given-names></name><name><surname>Vogelstein</surname> <given-names>RJ</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name><name><surname>Burns</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A community-developed open-source computational ecosystem for big neuro data</article-title><source>Nature Methods</source><volume>15</volume><fpage>846</fpage><lpage>847</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0181-1</pub-id><pub-id pub-id-type="pmid">30377345</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>CY</given-names></name><name><surname>Mark Liao</surname> <given-names>HY</given-names></name><name><surname>Wu</surname> <given-names>YH</given-names></name><name><surname>Chen</surname> <given-names>PY</given-names></name><name><surname>Hsieh</surname> <given-names>JW</given-names></name><name><surname>Yeh</surname> <given-names>IH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>CSPNet: a new backbone that can enhance learning capability of CNN,</article-title><conf-name>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</conf-name><pub-id pub-id-type="doi">10.1109/CVPRW50498.2020.00203</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wu</surname> <given-names>Z</given-names></name><name><surname>Xiong</surname> <given-names>Y</given-names></name><name><surname>Yu</surname> <given-names>SX</given-names></name><name><surname>Lin</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unsupervised feature learning via Non-Parametric instance discrimination</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1805.01978">https://arxiv.org/abs/1805.01978</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Yosinski</surname> <given-names>J</given-names></name><name><surname>Clune</surname> <given-names>J</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Lipson</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>How transferable are features in deep neural networks?</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1411.1792">https://arxiv.org/abs/1411.1792</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zeiler</surname> <given-names>MD</given-names></name><name><surname>Fergus</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visualizing and understanding convolutional networks</article-title><conf-name>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</conf-name><fpage>818</fpage><lpage>833</lpage></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Žerovnik Mekuč</surname> <given-names>M</given-names></name><name><surname>Bohak</surname> <given-names>C</given-names></name><name><surname>Hudoklin</surname> <given-names>S</given-names></name><name><surname>Kim</surname> <given-names>BH</given-names></name><name><surname>Romih</surname> <given-names>R</given-names></name><name><surname>Kim</surname> <given-names>MY</given-names></name><name><surname>Marolt</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Automatic segmentation of mitochondria and endolysosomes in volumetric electron microscopy data</article-title><source>Computers in Biology and Medicine</source><volume>119</volume><elocation-id>103693</elocation-id><pub-id pub-id-type="doi">10.1016/j.compbiomed.2020.103693</pub-id><pub-id pub-id-type="pmid">32339123</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zoph</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rethinking Pre-training and Self-training</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.06882">https://arxiv.org/abs/2006.06882</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Deduplication and image filtering.</title><p>(<bold>a</bold>) Breakdown of fractions (top) and representative examples (bottom) of patches labeled ‘uninformative’ by a trained deep learning (DL) model based on defect (as determined by a human annotator). (<bold>b</bold>) Receiver operating characteristic curve for the DL model classifier and a Random Forest classifier evaluated on a holdout test set of 2000 manually labeled patches (1000 informative and 1000 uninformative).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-app1-fig1-v1.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Randomly selected images from CEMraw, CEMdedup, and CEM500K.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-app1-fig2-v1.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Schematics of the MoCoV2 algorithm and UNet-ResNet50 model architecture.</title><p>(<bold>a</bold>) Shows a single step in the MoCoV2 algorithm. A batch of images is copied; images in each copy of the batch are independently and randomly transformed and then shuffled into a random order (the first batch is called the <italic>query</italic> and the second is called the <italic>key</italic>). <italic>Query</italic> and <italic>key</italic> are encoded by two different models, the <italic>encoder</italic> and <italic>momentum encoder,</italic> respectively. The encoded <italic>key</italic> is appended to the <italic>queue.</italic> Dot products of every image in the <italic>query</italic> with every image in the <italic>queue</italic> measure similarity. The similarity between an image in the <italic>query</italic> and its match from the <italic>key</italic> is the signal that informs parameter updates. More details in <xref ref-type="bibr" rid="bib22">He et al., 2019</xref>. (<bold>b</bold>) Detailed schematic of the UNet-ResNet50 architecture.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-app1-fig3-v1.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Randomly selected images from the <xref ref-type="bibr" rid="bib2">Bloss et al., 2018</xref> pre-training dataset.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-app1-fig4-v1.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Visual comparison of results on the UroCell benchmark.</title><p>The ground truth and Authors’ Best Results are taken from the original UroCell publication (<xref ref-type="bibr" rid="bib61">Žerovnik Mekuč et al., 2020</xref>). The results from the CEM500K-moco pre-trained model have been colorized to approximately match the originals; 2D label maps were not included in the UroCell paper.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-app1-fig5-v1.tif"/></fig><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Images from source electron microscopy (EM) volumes are unequally represented in the subsets of CEM.</title><p>The line at 45° shows the expected curve for perfect equality between all source volumes (i.e. each volume would contribute the same number of images to CEMraw, CEMdedup, or CEM500K). Gini coefficients measure the area between the Lorenz Curves and the line of perfect equality, with 0 meaning perfect equality and 1 meaning perfect inequality. For each subset of cellular electron microscopy (CEM), approximately 20% of the source 3D volumes account for 80% of all the 2D patches.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-app1-fig6-v1.tif"/></fig><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Plot showing the percent of random crops from an image that will be 100% uninformative based on the percent of the image that is informative.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65894-app1-fig7-v1.tif"/></fig></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65894.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Grigorieff</surname><given-names>Nikolaus</given-names></name><role>Reviewing Editor</role><aff><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Grigorieff</surname><given-names>Nikolaus</given-names> </name><role>Reviewer</role><aff><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Saalfeld</surname><given-names>Stephan</given-names> </name><role>Reviewer</role><aff><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2020.12.11.421792">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.12.11.421792v2">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This manuscript describes the curation of a training dataset that will be an important resource for developers of new segmentation and deep-learning algorithms for electron microscopy data. The small size of the dataset makes it easy to use, and its broad range of image modalities ensures that the model will be applicable in many situations, making it very useful for the community.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;CEM500K – A large-scale heterogeneous unlabeled cellular electron microscopy image dataset for deep learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Nikolaus Grigorieff as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Anna Akhmanova as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Stephan Saalfeld (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential Revisions:</p><p>1) This is an excellent manuscript. The authors establish a broadly usable dataset, and designed and conducted a strict and sound evaluation study. The paper is well written, easy to follow and overall well balanced in how it discusses technical details and the wider impact of this study. However, the work focuses on pixel classification on 2D images. This is not clear in the beginning and only discussed fairly late in the manuscript where 3D approaches are generally dismissed for no other reason than that they are not applicable to 2D images and more expensive. This dismissal is invalid and unnecessary. The authors should mention their focus on 2D images earlier in the manuscript, explain why this approach is taken, and what would have to be changed to include 3D data.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1) The authors should clarify that none of the benchmark data have been included in the CEM500K training set.</p><p>2) The authors should include a supplementary table for the cross-validation scores they obtained to validate their trained CEM500K-moco model.</p><p>3) Given the relatively small size of the CEM500K dataset, how much of a concern is overfitting?</p><p>4) Can the authors comment on the resolution of features that are likely relevant in their CEM500K-moco model? What are the prospects of using MoCoV2 to recognize higher-resolution features in cryo-EM images?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1) This work clearly targets pixel classification on 2D images, however the 2D focus is not clear in the beginning and only discussed late (Discussion) where 3D approaches are generally dismissed for no other reason than that they are not applicable to 2D images and more expensive. This dismissal is invalid and also unnecessary and should be removed. The 2D focus should become obvious in the Abstract and Introduction of the paper since the term &quot;image&quot; is used for nD data in the community.</p><p>2) Introduction, third paragraph: add paragraph.</p><p>3) The hashing algorithm used for identifying near duplicates (https://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) may not be optimal but this probably doesn't do much harm as its main purpose is to remove nearby sections from 3D volumes.</p><p>4) Could CEMdedup be useful for tasks that contain empty resin or nuclei?</p><p>5) Conclusion: Pretraining dataset should include data relevant for the benchmark task. Does unrelated data necessarily make the benchmark performance better? How would CEM500K + Bloss pre-training perform for the mouse data?</p><p>6) CEM500K MoCoV2 training included 360° rotations, i.e. the increased rotation invariance compared to ImageNet is by design of the training, not necessarily the data.</p><p>7) Subsection “CEM500K models are largely impervious to meaningful image augmentations”: Why would models gain robustness to rare and random events such as artifacts? Weren't such images excluded from the dataset?</p><p>8) Subsection “CEM500K models learn biologically relevant features”: Could this also be just high contrast/high gradient regions which makes sense for the given training task? I.e. is the focus on objects of interest may be a trivial consequence of objects of interest being the only thing present/distinguishable in the image?</p><p>9) Subsection “Fully trained CEM500K models achieve state-of-the-art results on EM benchmarks”: The random initialization score for CREMI of 0.0 is surprising. Is this an indication that the training setup or this particular network architecture have other issues that are unrelated to the pre-training? Others have successfully trained synapse detection networks without pre-training.</p><p>10) Discussion, second paragraph: This is not correct for CREMI which is data from the large FAFB dataset, and also itself contains more data than necessary in the padded volumes available for download. I do not find it necessary to perform this experiment but it is ok to simply say that this hasn't been done without falsely asserting that it wasn't possible.</p><p>11) I do not understand this statement: &quot;[…] moreover, the elimination of 3D context from volume EM datasets ensures that the shared data can only reasonably be used for DL.&quot; Is it necessary?</p><p>12) Materials and methods: The CREMI synaptic cleft evaluation metric is public including code to execute it. Ground truth for test data, however, is secret. Did you submit your results for evaluation? Like above, it is not necessary to do this but the statement &quot;[…] the training and test datasets did not have an official evaluation metric, and the ground truth segmentations were not publicly available[…]&quot; is not correct. May be replace with &quot;The test data for the CREMI Synaptic Cleft benchmark is not publicly available and the challenge uses a different evaluation metric than IoU. Therefore[…]&quot;.</p><p>Thank you very much for this wonderful work. It's been a pleasure to read this manuscript.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>– Would you please clarify the statement “a model's ability to generalize to unseen images”? I would like to see more clarification regarding this statement as it seems that it ignores the definition of transfer learning problems. Transfer learning techniques are applied where there exists lack of data. Therefore, providing datasets with large numbers of data does not seem to be a good solution for these problems though it is still a valuable dataset for many problems.</p><p>– In the first part of the paper, it is referred to &quot;six publicly available dataset&quot;. I know that you name them in the later part of the paper but it is a question for the reader. It would be better to name them in these sections as well.</p><p>– It would be better if the minimum amount of outperformance is defined when it is presented &quot;significantly outperformed randomly initialized&quot;.</p><p>– Is there any reference/proof for the following claim: &quot;Although it is currently impossible to determine a priori what data will be useful for a model, we expect that this removal of significant redundancies in the image dataset is unlikely to result in the loss of meaningful information for DL model training.&quot;</p><p>– What would happen if you keep the duplicate images and use MoCoV2 algorithm?</p><p>– How about not applying the MoCoV2 algorithm and using other algorithms? Have you considered ablation studies in this regard?</p><p>– The following statements are unclear and I appreciate it if you clarify them more.</p><p>1) &quot;These results emphasize the necessity of evaluating deep learning algorithms and pre-training datasets on multiple benchmarks before drawing conclusions about their quality.&quot;</p><p>2) “We expect that the superior robustness to variations in cellular EM data baked into CEM500K-moco should simplify the process of adjusting to new tasks.”</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65894.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential Revisions:</p><p>1) This is an excellent manuscript. The authors establish a broadly usable dataset, and designed and conducted a strict and sound evaluation study. The paper is well written, easy to follow and overall well balanced in how it discusses technical details and the wider impact of this study. However, the work focuses on pixel classification on 2D images. This is not clear in the beginning and only discussed fairly late in the manuscript where 3D approaches are generally dismissed for no other reason than that they are not applicable to 2D images and more expensive. This dismissal is invalid and unnecessary. The authors should mention their focus on 2D images earlier in the manuscript, explain why this approach is taken, and what would have to be changed to include 3D data.</p></disp-quote><p>Thank you for your positive comments, we now explicitly define CEM500K as a 2D image dataset in the Abstract and Introduction. We have also edited and clarified the “2D vs. 3D” argument in the Discussion.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1) The authors should clarify that none of the benchmark data have been included in the CEM500K training set.</p></disp-quote><p>– The benchmark data, which represented ~5% of the total data was indeed included in the CEM500K training set for the results reported in Figures 2, 4. We did this to avoid the confusion of reporting results pre-trained without the benchmark data and releasing weights pre-trained with the benchmark data. The manuscript has been edited to make this point clear.</p><p>– However, the reviewer does bring up a fair point. To test whether including benchmark data misrepresents the quality of the pre-trained weights for unseen data, we had previously run experiments in which benchmark data was excluded; we saw both increases and decreases in IoU scores (average 0.6% decrease) depending on the dataset. Importantly, this comparison is not perfect: CEM500K without the benchmark data had ~24,000 fewer images, the same 200 epochs of training therefore included 37,000 fewer parameter updates (iterations).</p><p>– We have included a new table of these results, Supplementary file 2.</p><disp-quote content-type="editor-comment"><p>2) The authors should include a supplementary table for the cross-validation scores they obtained to validate their trained CEM500K-moco model.</p></disp-quote><p>– Thank you for noting this omission. We used cross-validation for the Guay benchmark dataset; none of the other benchmark datasets included predefined validation data splits and the accompanying publications did not report cross-validation results. Note this lack of data splits also informed our choice to use a fixed set of hyperparameters for all other benchmarks. We only optimized hyperparameters for the Guay benchmark – we added a Gaussian Noise data augmentation – and without a validation dataset this would have run the risk of overfitting to the test set.</p><p>– We have added Supplementary file 3 to show cross-validation results for the Guay benchmarks.</p><disp-quote content-type="editor-comment"><p>3) Given the relatively small size of the CEM500K dataset, how much of a concern is overfitting?</p></disp-quote><p>– We agree that overfitting is always a concern. This is exactly why having multiple segmentation benchmarks was essential – indeed we hope that one of the take-aways from our paper is the necessity of testing against multiple benchmarks.</p><p>– Our results show clearly that heterogeneity is more important than size, and this mitigates overfitting. Pre-training on the Bloss, 2018 dataset, a larger but less heterogenous image set, led to models that performed well on segmentation tasks drawn from the same organism and tissue (mouse brain) but underperformed when tasked with segmenting data from other contexts (Figure 2E). In the same figure, we showed that, in contrast, models pre-trained on CEM500K performed well across the board.</p><p>– That said, we do recognize that there are biases in the current dataset – Figure 1C, C and Appendix—figure 6 show that certain organisms, tissues, and imaging experiments are overrepresented in CEM500K. As more diverse images are added to CEM500K, we expect these disparities will be reduced.</p><disp-quote content-type="editor-comment"><p>4) Can the authors comment on the resolution of features that are likely relevant in their CEM500K-moco model? What are the prospects of using MoCoV2 to recognize higher-resolution features in cryo-EM images?</p></disp-quote><p>– We anticipate that the CEM500K model should be relevant for typical volume EM experiments, where data is acquired at pixel spacing greater than 2 nm (see Figure 1B, and results for Kasthuri++, pixel spacing of 3nm). Relevance will likely taper off at resolutions higher than this. However, it should be possible to finetune the CEM500K pre-trained weights on higher resolution TEM images, following approaches reported for other imaging modalities (Touvron, Vedaldi, Douze, and Jégou, 2019).</p><p>– Re. MoCoV2 for cryo EM. For cryotomograms of cells, it is possible that MoCoV2 pre-training can be used for cellular features captured at higher resolutions; of course, this will require a different pre-training dataset, a possible resource in the future. For CryoEM SPA, we think this would be more challenging. The main requirement of MoCoV2 is that images are distinguishable from each other; fields of many near-identical particles but distributed randomly from image to image would therefore present a daunting challenge to MoCoV2 but may find use for detection of rare orientations.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1) This work clearly targets pixel classification on 2D images, however the 2D focus is not clear in the beginning and only discussed late (Discussion) where 3D approaches are generally dismissed for no other reason than that they are not applicable to 2D images and more expensive. This dismissal is invalid and also unnecessary and should be removed. The 2D focus should become obvious in the Abstract and Introduction of the paper since the term &quot;image&quot; is used for nD data in the community.</p></disp-quote><p>Thank you for your positive comments, we have addressed this point at the start of the rebuttal.</p><disp-quote content-type="editor-comment"><p>2) Introduction, third paragraph: add paragraph.</p></disp-quote><p>Thank you, the large paragraph has been split to create a new one.</p><disp-quote content-type="editor-comment"><p>3) The hashing algorithm used for identifying near duplicates (https://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) may not be optimal but this probably doesn't do much harm as its main purpose is to remove nearby sections from 3D volumes.</p></disp-quote><p>We agree, hashing is a somewhat coarse method for deduplication, however it is simple and fast. Importantly, the algorithm reduces ~80% of redundant images and visual inspection of the remaining images show satisfactory dissimilarities.</p><disp-quote content-type="editor-comment"><p>4) Could CEMdedup be useful for tasks that contain empty resin or nuclei?</p></disp-quote><p>It is true that in the filtering step we remove images from CEMdedup that exclusively or predominantly consisted of uniform intensity content like resin and the interior of nuclei. However, there are still many images in the final dataset CEM500K that contain patches of nuclei and empty resin, and indeed we show that for a nuclei segmentation task (Perez), models pre-trained on CEMdedup performed similarly to those pre-trained on CEM500K (nuclei IoU of 0.9890 vs. 0.9891 respectively, see source data file). Therefore, we think that the final dataset should suffice for these tasks.</p><disp-quote content-type="editor-comment"><p>5) Conclusion: Pretraining dataset should include data relevant for the benchmark task. Does unrelated data necessarily make the benchmark performance better? How would CEM500K + Bloss pre-training perform for the mouse data?</p></disp-quote><p>– This is a nuanced and excellent point. A key challenge is that “relatedness” of the concepts is some complex (and unknown) function of organism, tissue, sample prep, imaging approach, resolution etc., which is impossible to determine a priori. Therefore in some ways one could think of providing this highly heterogenous image data as hedging our bets, i.e., not to maximize performance on any one benchmark task but rather on the set of all possible cellular EM segmentation tasks. Results comparing CEM500K to Bloss (see Figure 2E) suggest that the improved generality that comes from pre-training on many unrelated datasets does not hinder the model from performing well on specialized downstream tasks such as the mouse brain benchmarks.</p><p>– CEM500K includes &gt;15% of brain tissue (see Figure 1C) so the combination of CEM500K and Bloss for pre-training may improve performance on mouse brain data; indeed one possible use-case of CEM500K would be as an add-on to boost performance of specific or homogenous pre-training data sets. We note however that this would heavily skew the pre-training dataset toward a particular biological context, likely decreasing the general utility of resulting models while also increasing computational costs.</p><disp-quote content-type="editor-comment"><p>6) CEM500K MoCoV2 training included 360° rotations, i.e. the increased rotation invariance compared to ImageNet is by design of the training, not necessarily the data.</p></disp-quote><p>Thank you for pointing this out, our wording was ambiguous. Yes, CEM500K-moco’s better invariance to rotation is due to the training augmentations. Pre-training on ImageNet, where object orientation matters, never includes 360° rotations. We have added a note to the section in the manuscript to reflect this point.</p><disp-quote content-type="editor-comment"><p>7) Subsection “CEM500K models are largely impervious to meaningful image augmentations”: Why would models gain robustness to rare and random events such as artifacts? Weren't such images excluded from the dataset?</p></disp-quote><p>Thank you for pointing this out, we were imprecise in our definition of “artifacts”. The artifacts that we filtered out were related to reconstruction artifacts, usually resulting from improper reporting of pixel data. We did NOT filter out experimental artifacts such as those arising from heavy metal stains, FIB milling artifacts, brightness/contrast changes, and we did not exclude misalignments/missing slices from 3-D reconstructions. We relabel the artifact images as “reconstruction artifacts” to be more precise, and we explicitly mention this in the Materials and methods. Since these artifacts occur occasionally and at random with respect to meaningful cellular features, we expect that models will learn to ignore these features.</p><disp-quote content-type="editor-comment"><p>8) Subsection “CEM500K models learn biologically relevant features”: Could this also be just high contrast/high gradient regions which makes sense for the given training task? I.e. is the focus on objects of interest may be a trivial consequence of objects of interest being the only thing present/distinguishable in the image?</p></disp-quote><p>Yes, this is correct. However, sample preparation protocols in EM are designed to specifically accentuate objects of interest (organelles etc.), while other high-contrast features (sample prep artifacts for example) show no consistent patterns. Therefore, the model learns that there are “things” in an image that are distinguishable from each other and that the rest of the image is “background”, we exploit this knowledge for accurate object detection and segmentation of EM images.</p><disp-quote content-type="editor-comment"><p>9) Subsection “Fully trained CEM500K models achieve state-of-the-art results on EM benchmarks”: The random initialization score for CREMI of 0.0 is surprising. Is this an indication that the training setup or this particular network architecture have other issues that are unrelated to the pre-training? Others have successfully trained synapse detection networks without pre-training.</p></disp-quote><p>The 0.0 score results from the shortened training schedule used for pre-trained models. The CREMI Synaptic Cleft benchmark uniquely has a large foreground-background imbalance (very few pixels in the image correspond to the small feature that is the synaptic cleft) and we trained our models using a binary cross entropy loss, which does correct for that imbalance. As a result, randomly initialized models take a much longer time to achieve non-zero results. In the source files released with this manuscript, we include results for 30,000 training iterations for this unique case (compared to the 5,000 reported in the paper).</p><disp-quote content-type="editor-comment"><p>10) Discussion, second paragraph: This is not correct for CREMI which is data from the large FAFB dataset, and also itself contains more data than necessary in the padded volumes available for download. I do not find it necessary to perform this experiment but it is ok to simply say that this hasn't been done without falsely asserting that it wasn't possible.</p></disp-quote><p>Thank you for pointing this out. The lack of heterogeneity of such a dataset is more of a factor than size, and we have updated the manuscript accordingly.</p><disp-quote content-type="editor-comment"><p>11) I do not understand this statement: &quot;[…] moreover, the elimination of 3D context from volume EM datasets ensures that the shared data can only reasonably be used for DL.&quot; Is it necessary?</p></disp-quote><p>Our goal with this statement was to assuage researchers who may be reticent to share unpublished data that contributions to CEM500K could only be used for DL work.</p><disp-quote content-type="editor-comment"><p>12) Materials and methods: The CREMI synaptic cleft evaluation metric is public including code to execute it. Ground truth for test data, however, is secret. Did you submit your results for evaluation? Like above, it is not necessary to do this but the statement &quot;[…] the training and test datasets did not have an official evaluation metric, and the ground truth segmentations were not publicly available[…]&quot; is not correct. May be replace with &quot;The test data for the CREMI Synaptic Cleft benchmark is not publicly available and the challenge uses a different evaluation metric than IoU. Therefore[…]&quot;.</p></disp-quote><p>Yes, we did submit results which achieved very good F-scores but scored lower on the CREMI metric. As the CREMI leaderboard itself states, “We are not including the F-Score for now, as our current way of computing it can lead to unfair comparison.” So we side-step this point and we replace the text in the manuscript with the suggested wording.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>– Would you please clarify the statement “a model's ability to generalize to unseen images”? I would like to see more clarification regarding this statement as it seems that it ignores the definition of transfer learning problems. Transfer learning techniques are applied where there exists lack of data. Therefore, providing datasets with large numbers of data does not seem to be a good solution for these problems though it is still a valuable dataset for many problems.</p></disp-quote><p>In our manuscript, we write that there are two ways by which more robust models can be generated: either a massive amount of annotated data is provided (possible but impractical for a typical biological lab), or transfer learning can be used. Here, by pre-training on existing heterogenous unlabeled data, a model would require far less labeled data to prevent overfitting and perform well on unseen data. We now add a note clarifying this point in the paragraph.</p><disp-quote content-type="editor-comment"><p>– In the first part of the paper, it is referred to &quot;six publicly available dataset&quot;. I know that you name them in the later part of the paper but it is a question for the reader. It would be better to name them in these sections as well.</p></disp-quote><p>We have updated the manuscript and now name the benchmarks in the Introduction.</p><disp-quote content-type="editor-comment"><p>– It would be better if the minimum amount of outperformance is defined when it is presented &quot;significantly outperformed randomly initialized&quot;.</p></disp-quote><p>Thank you, we have removed the word “significantly” from the sentence.</p><disp-quote content-type="editor-comment"><p>– Is there any reference/ proof for the following claim: &quot;Although it is currently impossible to determine a priori what data will be useful for a model, we expect that this removal of significant redundancies in the image dataset is unlikely to result in the loss of meaningful information for DL model training.&quot;</p></disp-quote><p>The determination of usefulness of specific data for a model is a point of intense work in the DL field, but rather than segueing into this discussion, we simply remove the line for brevity. We note however that current state-of-the-art supports our assertion here.</p><disp-quote content-type="editor-comment"><p>– What would happen if you keep the duplicate images and use MoCoV2 algorithm?</p></disp-quote><p>When duplicates are not removed (i.e., the pre-training dataset is CEMraw), we show that performance suffers, we report these results in Figure 2D. It is possible that the main impact of duplicates is to slow down model training as iterations are consumed relearning the same features already seen multiple times previously in the dataset.</p><disp-quote content-type="editor-comment"><p>– How about not applying the MoCoV2 algorithm and using other algorithms? Have you considered ablation studies in this regard?</p></disp-quote><p>Our goal is this work was to prove the usefulness of the CEM500K dataset and demonstrate its application for unsupervised pre-training. To this end we are agnostic to the actual algorithm used (and indeed we consider this an advantage of our approach), making ablation studies unnecessary. We report high performances with our choices, but we have also open-sourced the dataset and our code to encourage the EM and DL communities to investigate other algorithms that may perform better.</p><disp-quote content-type="editor-comment"><p>– The following statements are unclear and I appreciate it if you clarify them more.</p><p>1) &quot;These results emphasize the necessity of evaluating deep learning algorithms and pre-training datasets on multiple benchmarks before drawing conclusions about their quality.&quot;</p></disp-quote><p>By “quality” in this context we mean a model’s biases. If one were to use only a single benchmark from any one organism and tissue (e.g., mouse brain), models showing a good performance may or may not perform well on segmentation tasks from random unrelated EM data – a point that could never be tested unless there were multiple disparate benchmarks. Therefore, multiple benchmarks are needed to determine if a model will be generally useful for an arbitrary EM segmentation task.</p><disp-quote content-type="editor-comment"><p>2) “We expect that the superior robustness to variations in cellular EM data baked into CEM500K-moco should simplify the process of adjusting to new tasks.”</p></disp-quote><p>What we mean here is that the heterogeneity of CEM500K combined with the MoCoV2 algorithm results in the sampling of a wide variety of possible cellular EM features and also robustness to such variations in tasks. To give a concrete example, consider image rotations. When presented with a new EM segmentation task, CEM500K-moco pre-trained models (as opposed to say ImageNet pre-trained models) will not need to learn rotation invariance because, as we demonstrated, they are already largely invariant to image rotation (see Figure 3A). In short, the invariances in CEM500K-moco models put it in a better starting position for adjusting to a new EM image task resulting in faster learning and better performances (Figure 4).</p></body></sub-article></article>