<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">57458</article-id><article-id pub-id-type="doi">10.7554/eLife.57458</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Dynamics of gaze control during prey capture in freely moving mice</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-182026"><name><surname>Michaiel</surname><given-names>Angie M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5312-8329</contrib-id><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-182025"><name><surname>Abe</surname><given-names>Elliott TT</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-21688"><name><surname>Niell</surname><given-names>Cristopher M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6283-3540</contrib-id><email>cniell@uoregon.edu</email><xref ref-type="aff" rid="aff1"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution>Institute of Neuroscience and Department of Biology, University of Oregon</institution><addr-line><named-content content-type="city">Eugene</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Spering</surname><given-names>Miriam</given-names></name><role>Reviewing Editor</role><aff><institution>The University of British Columbia</institution><country>Canada</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution>University of Texas at Austin</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>24</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e57458</elocation-id><history><date date-type="received" iso-8601-date="2020-04-01"><day>01</day><month>04</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-07-23"><day>23</day><month>07</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Michaiel et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Michaiel et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-57458-v2.pdf"/><abstract><p>Many studies of visual processing are conducted in constrained conditions such as head- and gaze-fixation, and therefore less is known about how animals actively acquire visual information in natural contexts. To determine how mice target their gaze during natural behavior, we measured head and bilateral eye movements in mice performing prey capture, an ethological behavior that engages vision. We found that the majority of eye movements are compensatory for head movements, thereby serving to stabilize the visual scene. During movement, however, periods of stabilization are interspersed with non-compensatory saccades that abruptly shift gaze position. Notably, these saccades do not preferentially target the prey location. Rather, orienting movements are driven by the head, with the eyes following in coordination to sequentially stabilize and recenter the gaze. These findings relate eye movements in the mouse to other species, and provide a foundation for studying active vision during ethological behaviors in the mouse.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>As you read this sentence, your eyes will move automatically from one word to the next, while your head remains still. Moving your eyes enables you to view each word using your central – as opposed to peripheral – vision. Central vision allows you to see objects in fine detail. It relies on a specialized area of the retina called the fovea. When you move your eyes across a page, you keep the images of the words you are currently reading on the fovea. This provides the detailed vision required for reading.</p><p>The same process works for tracking moving objects. When watching a bird fly across the sky, you can track its progress by moving your eyes to keep the bird in the center of your visual field, over the fovea. But the majority of mammals do not have a fovea, and yet are still able to track moving targets. Think of a lion hunting a gazelle, for instance, or a cat stalking a mouse. Even mice themselves can track and capture insect prey such as crickets, despite not having a fovea. And yet, exactly how they do this is unknown. This is particularly surprising given that mice have long been used to study the neural basis of vision.</p><p>By fitting mice with miniature head-mounted cameras, Michaiel et al. now reveal how the rodents track and capture moving crickets. It turns out that unlike animals with a fovea, mice do not use eye movements to track moving objects. Instead, when a mouse wants to look at something new, it moves its head to point at the target. The eyes then follow and ‘land’ on the target. In essence, head movements lead the way and the eyes catch up afterwards.</p><p>These findings are consistent with the idea that mammals with large heads evolved eye movements to overcome the energy costs of turning the head whenever they want to look at something new. For small animals, moving the head is less energetically expensive. As a result, being able to move the eyes independent of the head is unnecessary. Future work could use a combination of behavioral experiments and brain recordings to reveal how visual areas of the brain process what an animal is seeing in real time.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>vision</kwd><kwd>active sensing</kwd><kwd>eye movements</kwd><kwd>ethology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R34NS111669</award-id><principal-award-recipient><name><surname>Niell</surname><given-names>Cristopher M</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100011348</institution-id><institution>University of Oregon</institution></institution-wrap></funding-source><award-id>Promising Scholar Award</award-id><principal-award-recipient><name><surname>Michaiel</surname><given-names>Angie M</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>During natural visual behavior in mice, orienting towards a target is driven by head movements, during which the eyes stabilize and shift the visual input.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Across animal species, eye movements are used to acquire information about the world and vary based on the particular goal (<xref ref-type="bibr" rid="bib41">Yarbus, 1967</xref>). Mice, a common model system to study visual processing due to their genetic accessibility, depend on visual cues to successfully achieve goal-directed tasks in both artificial and ethological freely-moving behavioral paradigms, such as the Morris water maze, nest building, and prey capture; (<xref ref-type="bibr" rid="bib29">Morris, 1981</xref>; <xref ref-type="bibr" rid="bib6">Clark et al., 2006</xref>; <xref ref-type="bibr" rid="bib14">Hoy et al., 2016</xref>). It is unclear, however, how mice regulate their gaze to accomplish these visually mediated goals. Previous studies in both freely moving rats and mice have shown that eye movements largely serve to compensate for head movements (<xref ref-type="bibr" rid="bib38">Wallace et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Payne and Raymond, 2017</xref>; <xref ref-type="bibr" rid="bib26">Meyer et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Meyer et al., 2020</xref>), consistent with the vestibulo-ocular reflex (VOR) present in nearly all species (<xref ref-type="bibr" rid="bib37">Straka et al., 2016</xref>). While such compensation can serve to stabilize the visual scene during movement, it is not clear how this stabilization is integrated with the potential need to shift the gaze for behavioral goals, particularly because mice lack a specialized central fovea in the retina, and also have laterally facing eyes resulting in a relatively limited binocular field (roughly 40° as opposed to 135° in humans [<xref ref-type="bibr" rid="bib8">Dräger, 1978</xref>]). In addition, because eye movements are altered in head-fixed configurations due to the lack of head movement (<xref ref-type="bibr" rid="bib31">Payne and Raymond, 2017</xref>; <xref ref-type="bibr" rid="bib27">Meyer et al., 2020</xref>), understanding the mechanisms of gaze control and active visual search benefits from studies in freely moving behaviors.</p><p>Prey capture can serve as a useful paradigm for investigating visually guided behavior. Recent studies have shown that mice use vision to accurately orient towards and pursue cricket prey (<xref ref-type="bibr" rid="bib14">Hoy et al., 2016</xref>), and have begun to uncover neural circuit mechanisms that mediate both the associated sensory processing and motor output (<xref ref-type="bibr" rid="bib15">Hoy et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Shang et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Zhao et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Han et al., 2017</xref>). Importantly, prey capture also provides a context to investigate how mice actively acquire visual information, as it entails identifying and tracking a localized and ethological sensory input during freely moving behavior. Here, we asked whether mice utilize specific eye movement strategies, such as regulating their gaze to maximize binocular overlap, or actively targeting and tracking prey. Alternatively, or in addition, mice may use directed head movements to target prey, with eye movements primarily serving a compensatory role to stabilize the visual scene.</p><p>Predators typically have front-facing eyes which create a wide binocular field through the overlap of the two monocular fields, allowing for depth perception and accurate estimation of prey location (<xref ref-type="bibr" rid="bib5">Cartmill, 1974</xref>). Prey species, in contrast, typically have laterally facing eyes, and as a result, have large monocular fields spanning the periphery, which allow for reliable detection of approaching predators. Though mice possess these characteristics of prey animals, they also act as predators in pursuing cricket prey (<xref ref-type="bibr" rid="bib14">Hoy et al., 2016</xref>). How then do animals with laterally placed eyes target prey directly in front of them, especially when these targets can rapidly move in and out of the narrow binocular field? This could require the modulation of the amount of binocular overlap, through directed lateral eye movements, to generate a wider binocular field, such as in the case of cuttlefish (<xref ref-type="bibr" rid="bib9">Feord et al., 2020</xref>), fish (<xref ref-type="bibr" rid="bib3">Bianco et al., 2011</xref>), many birds (<xref ref-type="bibr" rid="bib23">Martin, 2009</xref>), and chameleons (<xref ref-type="bibr" rid="bib17">Katz et al., 2015</xref>). In fact, these animals specifically rotate their eyes nasally before striking prey, thereby creating a larger binocular zone. However, it is unknown whether mice use a similar strategy during prey capture. Alternatively, they may use coordinated head and eye movements to stabilize a fixed size binocular field over the visual target.</p><p>Foveate species make eye movements that center objects of interest over the retinal fovea, in order to use high acuity vision for complex visual search functions including identifying and analyzing behaviorally relevant stimuli (<xref ref-type="bibr" rid="bib12">Hayhoe and Ballard, 2005</xref>). Importantly, afoveate animals (those lacking a fovea) represent a majority of vertebrate species, with only some species of birds, reptiles, and fish possessing distinct foveae (<xref ref-type="bibr" rid="bib11">Harkness and Bennet-Clark, 1978</xref>), and among mammals, only simian primates possessing foveae (<xref ref-type="bibr" rid="bib39">Walls, 1942</xref>). It remains unclear whether mice, an afoveate mammalian species, actively control their gaze to target and track moving visual targets using directed eye movements, or whether object localization is driven by head movements. We therefore aimed to determine the oculomotor strategies that allow for effective targeting of a discrete object, cricket prey, within the context of a natural behavior.</p><p>Recent studies have demonstrated the use of head-mounted cameras to measure eye movements in freely moving rodents (<xref ref-type="bibr" rid="bib38">Wallace et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Meyer et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Meyer et al., 2020</xref>). Here, we used miniature cameras and an inertial measurement unit (IMU) to record head and bilateral eye movements while unrestrained mice performed a visually guided and goal-directed task, approach and capture of live insect prey. We compared the coordination of eye and head movements, as well as measurements of gaze relative to the cricket prey during approach and non-approach epochs, to determine the oculomotor strategies that mice use when localizing moving prey.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Tracking eye and head movements during prey capture</title><p>Food-restricted mice were habituated to hunt crickets in an experimental arena, following the paradigm of <xref ref-type="bibr" rid="bib14">Hoy et al., 2016</xref>. To measure eye and head rotations in all dimensions, mice were equipped with two reversibly attached, lateral head-mounted cameras and an inertial measurement unit (IMU) board with an integrated 3-dimensional accelerometer and gyroscope (<xref ref-type="fig" rid="fig1">Figure 1A and B</xref>; <xref ref-type="video" rid="video1">Video 1</xref>). The estimated error in measurement of head and eye angle were both less than one degree (see Materials and methods). In addition, we recorded the behavior of experimental animals and the cricket prey with an overhead camera to compute the relative position of the mouse and cricket, as well as orientation of the head relative to the cricket. Following our previous studies (<xref ref-type="bibr" rid="bib14">Hoy et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Hoy et al., 2019</xref>), we defined approaches based on the kinematic criteria that the mouse was oriented towards the cricket and actively moving towards it (see Materials and methods). Together, these recordings and analyses allowed us to synchronously measure eye and head rotations along with cricket and mouse kinematics throughout prey capture behavior (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; <xref ref-type="video" rid="video1">Video 1</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Tracking eye and head movements during prey capture.</title><p>(<bold>A</bold>) Unrestrained mice hunted live crickets in a rectangular plexiglass arena (45 × 38 × 30 cm). Using an overhead camera, we tracked the movement of the mouse and cricket. Example image with overlaid tracks of the mouse (cyan). (<bold>B</bold>) 3D printed holders house a miniature camera, collimating lens, an IR LED, and an IMU, and are reversibly attached to implants on the mouse’s head, with one camera aimed at each eye. (<bold>C</bold>) Synchronized recordings of measurements related to bilateral eye position and velocity, mouse position relative to cricket (distance and azimuth, as measured relative to the center of the head), mouse speed, and head rotation in multiple dimensions (analysis here focuses on yaw and pitch). (<bold>D</bold>) Average mouse locomotor speed did not differ across experimental and control experiments (no camera and IMU) for both non-approach and approach periods. Individual dots represent the average velocity per trial. (<bold>E</bold>) Average number of captures per 10 min session did not differ between experimental and control sessions (control N = 7 animals, 210 trials; cameras N = 7 animals, 105 trials; two-sample t-test, p=0.075).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57458-fig1-v2.tif"/></fig><media id="video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-57458-video1.mp4"><label>Video 1.</label><caption><title>Video of mouse performing prey capture with reversibly attached eye cameras, demonstrating synchronized measurement of bilateral eye positions and mouse/cricket behavior.</title><p>The direction of each eye is superimposed on the head (purple and light blue lines) based on calculated eye position and head angle.</p></caption></media><p>The cameras and IMU did not affect overall mouse locomotor speed in the arena or total number of crickets caught per 10 min session (paired t-test, p=0.075; <xref ref-type="fig" rid="fig1">Figure 1D/E</xref>), suggesting that placement of the cameras and IMU did not significantly impede movement or occlude segments of the visual field required for successful prey capture behavior.</p></sec><sec id="s2-2"><title>Eye vergence is stabilized during approach periods</title><p>To determine whether mice make convergent eye movements to enhance binocular overlap during approaches, we first characterized the coordination of bilateral eye movements. We defined central eye position, that is 0°, as the average pupil location for each eye, across the recording duration. Measurement of eye position revealed that freely moving mice nearly constantly move their eyes, typically within a ± 20 degree range (<xref ref-type="fig" rid="fig1">Figures 1C</xref>, <xref ref-type="fig" rid="fig2">2A and B</xref>), as shown previously (<xref ref-type="bibr" rid="bib27">Meyer et al., 2020</xref>; <xref ref-type="bibr" rid="bib32">Sakatani and Isa, 2007</xref>). <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows example traces of the horizontal position of the two eyes (top), along with running speed of the mouse (bottom). As described previously (<xref ref-type="bibr" rid="bib38">Wallace et al., 2013</xref>; <xref ref-type="bibr" rid="bib31">Payne and Raymond, 2017</xref>; <xref ref-type="bibr" rid="bib26">Meyer et al., 2018</xref>) and analyzed below (<xref ref-type="fig" rid="fig3">Figure 3D</xref>), the eyes are generally stable when the mouse is not moving. In addition, the raw traces reveal a pattern of eye movement wherein rapid correlated movements of the two eyes are superimposed on slower anti-correlated movements. The pattern of rapid congruent movements and slower incongruent movements was also reflected in the time-lagged cross-correlation of the change in horizontal position across the two eyes (<xref ref-type="fig" rid="fig2">Figure 2E</xref>), which was positive at short time lags and negative at longer time lags.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Eye position is more aligned across the two eyes during approach periods.</title><p>(<bold>A</bold>) Example eye movement trajectory for right and left eyes for a 20 s segment, with points color-coded for time. (<bold>B</bold>) Horizontal and vertical position for right and left eyes during approach and non-approach times. N = 7 animals, 105 trials, 792 time pts (non-approach), 110 time pts (approach), representing a random sample of 0.22% of non-approach and 0.52% of approach time points. (<bold>C</bold>) Example trace of horizontal eye positions (top) and running speed (bottom) for a 30 s segment. (<bold>D</bold>) Schematic demonstrating vergence eye movements. (<bold>E</bold>) Cross correlation of horizontal eye position across the two eyes for non-approach and approach periods. (<bold>F</bold>) Histogram of vergence during non-approach and approach. (<bold>G</bold>) Example trace of horizontal eye position (top) and head pitch (bottom) before, during, and after an approach. (<bold>H</bold>) Scatter plot of head pitch and eye vergence. As head pitch tilts downwards, the eyes move temporally to compensate (as in schematic). N = 7 animals, 105 trials, 1240 time points (non-approach), 132 time points (approach), representing a sample of 0.35% of non-approach and 0.63% of approach time points. (<bold>I</bold>) Histogram of head pitch during approach and non-approach periods, across all experiments.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57458-fig2-v2.tif"/></fig><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Horizontal eye movements are mostly compensatory for yaw head rotations.</title><p>(<bold>A</bold>) To remove the effect of non-conjugate changes in eye position (i.e. vergence shifts), we compute the average angular position of the two eyes. (<bold>B</bold>) Cross-correlation of change in head yaw and horizontal eye position. (<bold>C</bold>) Scatter plot of horizontal rotational head velocity and horizontal eye velocity. N = 7 animals, 105 trials, 3565 (non-approach) and 211 (approach) timepoints, representing 1% of non-approach and 1% of approach timepoints. (<bold>D</bold>) Distribution of horizontal eye position during stationary and running periods (defined as times when mouse speed is greater than 1 cm/sec; Kolmogorov-Smirnov test, p=0.032). (<bold>E</bold>) Distribution of head angle velocity (paired t-test, p=0.938). (<bold>F</bold>) Distribution of mean absolute eye position (paired t-test, p=0.156). (<bold>G</bold>) Distribution of horizontal eye velocity (paired t-test, p=0.155) and distribution of eye velocity when head yaw is not changing (change in head yaw between ±15 deg/sec; paired t-test, p=0.229; N = 7 animals, 105 trials).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57458-fig3-v2.tif"/></fig><p>We next calculated the vergence angle, which is the difference in the horizontal position of the two eyes (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). The range of vergence angles was broadly distributed across negative (converged) and positive (diverged) values during non-approach periods, but became more closely distributed around zero (neutral vergence) during approaches (<xref ref-type="fig" rid="fig2">Figure 2F</xref>; paired t-test, p=0.024). This can be observed in the individual trace of eye movements before, during, and after an approach (<xref ref-type="fig" rid="fig2">Figure 2G</xref>, top), showing that while the eyes converge and diverge outside of approach periods, they move in a more coordinated fashion during the approaches. Thus, mice do not converge their eyes nasally to create a wider binocular field during approaches; rather the eyes are more tightly aligned, but at a neutral vergence, during approaches relative to non-approach periods.</p><p>Previous studies have demonstrated that eye vergence varies with head pitch (<xref ref-type="bibr" rid="bib38">Wallace et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Meyer et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Meyer et al., 2020</xref>). As the head tilts downwards, the eyes move outwards; based on the lateral position of the eyes, this serves to vertically stabilize the visual scene relative to changes in head pitch (<xref ref-type="bibr" rid="bib38">Wallace et al., 2013</xref>). We therefore sought to determine whether the stabilization of horizontal eye vergence we observed during approaches reflects corresponding changes in head pitch. Consistent with previous studies, we also found eye vergence to covary with head pitch (<xref ref-type="fig" rid="fig2">Figure 2H</xref>), such that when the head was vertically centered, the eyes no longer converged or diverged, but were aligned at a neutral vergence (i.e., no difference between the angular positions across the two eyes, see schematic in <xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>Strikingly, we found that while the relationship between head pitch and vergence was maintained during approaches (<xref ref-type="fig" rid="fig2">Figure 2H</xref>), the distribution of head pitch was more centered during approach periods (<xref ref-type="fig" rid="fig2">Figure 2H and I</xref>; paired t-test, p=0.0250), indicating a stabilization of the head in the vertical dimension. This can also be seen in the example trace in <xref ref-type="fig" rid="fig2">Figure 2G</xref>, where the head pitch becomes maintained around zero during approach. These data show that the increased alignment of the two eyes observed during approaches largely represents the stabilization of up/down head rotation, consequently reducing the need for compensatory vergence movements.</p></sec><sec id="s2-3"><title>Coordinated horizontal eye movements are primarily compensatory for horizontal head rotations</title><p>Next, we aimed to understand the relationship between horizontal head movements (yaw) and horizontal eye movements during approach behavior. In order to isolate the coordinated movement of the two eyes, removing the compensatory changes in vergence described above, we averaged the horizontal position of the two eyes for the remaining analyses (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Changes in head yaw and mean horizontal eye position were strongly negatively correlated at zero time lag (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), suggesting rapid compensation of head movements by eye movements, as expected for VOR-stabilization of the visual scene. The distribution of head and eye movements at zero lag (<xref ref-type="fig" rid="fig3">Figure 3C</xref>) shows that indeed changes in head yaw were generally accompanied by opposing changes in horizontal eye position, represented by the points along the negative diagonal axis. However, there was also a distinct distribution of off-axis points, representing a proportion of non-compensatory eye movements in which the eyes and head moved in the same direction (<xref ref-type="fig" rid="fig3">Figure 3C</xref>).</p><p>Many studies have reported a limited range of infrequent eye movements in head restrained mice (<xref ref-type="bibr" rid="bib31">Payne and Raymond, 2017</xref>; <xref ref-type="bibr" rid="bib30">Niell and Stryker, 2010</xref>; <xref ref-type="bibr" rid="bib33">Samonds et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Stahl, 2004</xref>), consistent with the idea that eye movements are generally driven by head movement. Correspondingly in the freely moving context of the prey capture paradigm, we found greatly reduced eye movements when the animals were stationary versus when the animals were running (<xref ref-type="fig" rid="fig3">Figure 3D</xref>; Kolmogorov-Smirnov test, p=0.032).</p><p>We next compared the distribution of mean eye position during approaches and non-approach periods. In contrast to the stabilization of head pitch described above, the distribution of head yaw velocities was not reduced during approaches as shown (<xref ref-type="fig" rid="fig3">Figure 3E</xref>; paired t-test p=0.938), consistent with the fact that mice must move their heads horizontally as they continuously orient to pursue prey. For both non-approach and approach periods, eye position generally remained within a range less than the size of the binocular zone (±20 degrees; <xref ref-type="fig" rid="fig3">Figure 3F</xref>, paired t-test, p=0.156), suggesting that the magnitude of eye movements would not shift the binocular zone to an entirely new location. Comparison of horizontal eye velocity between non-approach and approach epochs revealed that the eyes move with similar dynamics across both behavioral periods (<xref ref-type="fig" rid="fig3">Figure 3G</xref>, panel 1; paired t-test, p=0.155). Additionally, at times when head yaw was not changing, horizontal eye position also did not change (<xref ref-type="fig" rid="fig3">Figure 3G</xref>, panel 2; paired t-test, p=0.229). Together, these observations suggest that most coordinated eye movements in the horizontal axis correspond to changes in head yaw, and that the eyes do not scan the visual environment independent of head movements or when stationary.</p></sec><sec id="s2-4"><title>Non-compensatory saccades shift gaze position</title><p>Gaze position - the location the eyes are looking in the world - is a combination of the position of the eyes and the orientation of the head. Compensatory eye movements serve to prevent a shift in gaze, whereas non-compensatory eye movements (i.e., saccades) shift gaze to a new position. Although the vast majority of eye movements are compensatory for head movements, as demonstrated by strong negative correlation in <xref ref-type="fig" rid="fig3">Figure 3B/C</xref>, a significant number of movements are not compensatory, as seen by the distribution of off-axis points in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. These eye movements will therefore shift the direction of the animal’s gaze relative to the environment. We next examined how eye movements, and particularly non-compensatory movements, contribute to the direction of gaze during free exploration and prey capture. In particular, are these gaze shifts directed at the target prey?</p><p>We segregated eye movements into compensatory versus gaze-shifting by setting a fixed gaze velocity threshold of ±180 °/sec, based on the gaze velocity distribution (<xref ref-type="fig" rid="fig4">Figure 4A</xref>), which shows a transition between a large distribution around zero (stabilized gaze) and a long tail of higher velocities (rapid gaze shifts). This also provides a clear segregation in the joint distribution of eye and head velocity (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), with a large number of compensatory gaze-stabilizing movements (black points) where eye and head motion are anti-correlated, and much smaller population of gaze shifts (red). This classification approach provides an alternative to standard primate saccade detection (<xref ref-type="bibr" rid="bib1">Andersson et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Stahl, 2004</xref>; <xref ref-type="bibr" rid="bib25">Matthis et al., 2018</xref>), which is often based on eye velocity rather than gaze velocity, since in the freely moving condition, particularly in afoveate species, rapid gaze shifts (saccades) often result from a combination of head and non-compensatory eye movements, rather than eye movements alone (<xref ref-type="bibr" rid="bib19">Land, 2006</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Compensatory and non-compensatory eye movements generate a saccade-and-fixate gaze pattern during head turns.</title><p>(<bold>A</bold>) Distribution of gaze velocity (N = 377459 time points) showing segregation of non-compensatory and compensatory movements with thresholds at ±180°/sec. (<bold>B</bold>) Joint distributions of head yaw and horizontal eye velocity colored by their type as defined in A. Black points represent compensatory movements and red represent non-compensatory saccadic movements. Points shown are a random sample of 2105 approach timepoints, 10% of total approach time points. (<bold>C</bold>) Example traces of horizontal eye position, head yaw, and gaze demonstrate a saccade-and-fixate pattern in gaze. (<bold>D</bold>) Histogram of fixation duration; fixations N = 9730, 105 trials. (<bold>E</bold>) Root Mean Squared (RMS) stabilization histograms for head yaw and gaze. (<bold>F</bold>) Bar graphs are medians of RMS stabilization distributions (median head = 3.87 deg; median gaze = 1.5 deg; paired t-test, p=0).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57458-fig4-v2.tif"/></fig><p>We next determined how compensatory and non-compensatory eye movements contribute to the dynamics of gaze during ongoing behavior, by computing the direction of gaze as the sum of eye position and head position. Strikingly, the combination of compensatory and non-compensatory eye movements (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, top) with continuous change in head orientation (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, middle) results in a series of stable gaze positions interspersed with abrupt shifts (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, bottom). This pattern of gaze stabilization interspersed with rapid gaze-shifting movements, known as ‘saccade-and-fixate,’ is present across the animal kingdom and likely represents a fundamental mechanism to facilitate visual processing during movement (<xref ref-type="bibr" rid="bib18">Land, 1999</xref>). These results demonstrate that the mouse oculomotor system also engages this fundamental mechanism.</p><p>Durations of fixations between saccades showed wide variation, with a median of 220 ms (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). To quantify the degree of stabilization achieved, we compared the root mean square (RMS) deviation of gaze position and head yaw during stabilization periods (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). This revealed that the gaze is nearly three times less variable than the head (<xref ref-type="fig" rid="fig4">Figure 4F</xref>; median head = 3.87 deg; median gaze = 1.58 deg; p=0), resulting in stabilization to within nearly one degree over extended periods, even during active approach toward the cricket.</p></sec><sec id="s2-5"><title>Targeting of gaze relative to cricket during approach</title><p>Saccade-and-fixate serves as an oculomotor strategy to sample and stabilize the visual world during free movement. In primates, saccades are directed towards specific targets of interest in the visual field. Is this true of the non-compensatory movements in the mouse? In other words, do saccades directly target the cricket? To address this, we next analyzed the dynamics of head and gaze movements relative to the cricket position during hunting periods, to compare how accurately the direction of the gaze and the head targeted the cricket during saccades.</p><p><xref ref-type="fig" rid="fig5">Figure 5A</xref> shows example traces of head and eye dynamics across an approach period (see also <xref ref-type="video" rid="video2">Video 2</xref>). Immediately before approaching the cricket, the animal begins a large head turn towards the target, thereby reducing the azimuth angle (center of the head relative to cricket). This head turn is accompanied by a non-compensatory eye movement in the same direction (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, 3rd panel, see mean trace in black) that accelerates the shift in gaze. Then during the approach, the eyes convert the continuous tracking of the head into a series of stable locations of the gaze (black sections in <xref ref-type="fig" rid="fig5">Figure 5A</xref>, bottom). Note also the locking of the relative position of the two eyes (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, 3rd panel, blue and purple), as described above in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Head angle tracks cricket position more accurately than gaze position.</title><p>(<bold>A</bold>) Example traces of horizontal eye position, azimuth to cricket, head yaw, and gaze demonstrate a saccade-and-fixate pattern in gaze before and during an approach period. The head is pointed directly at the cricket when azimuth is 0°. Note the rapid decrease in azimuth, head yaw, and mean horizontal eye position creating a saccade immediately preceding the start of approach. (<bold>B</bold>) Average head yaw and gaze around the time of saccade as a function of azimuth to the cricket. Time = 0 is the saccade onset. (<bold>C</bold>) Histograms of head yaw and gaze position before and after saccades occur. (<bold>D</bold>) Medians of yaw and gaze distributions from C (paired t-test, p<sub>pre saccade</sub>=8.48x10<sup>−9</sup>; p<sub>post saccade</sub>=0.979). (<bold>E</bold>) Cross correlation of azimuth and change in head yaw for non-approach and approach periods. (<bold>F</bold>) Cross correlation of azimuth and change in gaze for non-approach and approach periods. N = 105 trials, 7 animals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-57458-fig5-v2.tif"/></fig><media id="video2" mime-subtype="mp4" mimetype="video" xlink:href="elife-57458-video2.mp4"><label>Video 2.</label><caption><title>Video of mouse performing prey capture, demonstrating dynamics of head orienting (dark blue) and gaze direction (cyan).</title><p>Note that during head turns the gaze is transiently offset from the head angle vector, due to compensatory eye movements, creating a stable image for the animal. Then, non-compensatory saccades shift the gaze position such that it aligns with the head to accurately target the cricket.</p></caption></media><p>To determine how head and eye movements target the prey, we computed absolute value traces of head and gaze angle relative to cricket (head and gaze azimuth), and aligned these to the onset of each non-compensatory saccadic eye movement. The average of all traces during approaches revealed that saccades are associated with a head turn towards the cricket, as shown by a decrease in the azimuth angle (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Immediately preceding a saccade, the gaze is stabilized while the head turns, and the saccade then abruptly shifts the gaze. Notably, following the saccade, the azimuth of gaze is the same as the azimuth of the head, suggesting that eye movements are not targeting the cricket more precisely, but simply ‘catching up’ with the head, by re-centering following a period of stabilization.</p><p>To further quantify this, we assessed the accuracy of the head and gaze at targeting cricket position before and after saccades. Preceding saccades, the distribution of head angles was centered around the cricket, while the gaze less accurately targeted and was offset from the cricket to the left or right (<xref ref-type="fig" rid="fig5">Figure 5C</xref>/5D top; paired t-test; p=8.48×10<sup>−9</sup> p=2×10<sup>−5</sup>), due to compensatory stabilization. After the saccade, however, gaze and head were equally targeted towards the cricket (<xref ref-type="fig" rid="fig5">Figure 5C/D</xref> bottom; p=0.979p=0.4), as the saccade recentered the eyes relative to the head and thereby the cricket. This pattern of stabilizing the gaze and then saccading to recenter the gaze repeats whenever the head turns until capture is successful (see <xref ref-type="video" rid="video2">Video 2</xref>).</p><p>Further supporting a strategy where the head guides targeting, with the eyes following to compensate, we examined how both head and eye movements are correlated with the cricket’s position. At short latencies, the change in head angle relative to the location of the cricket was highly correlated (<xref ref-type="fig" rid="fig5">Figure 5E</xref>), indicating that during approach the animal is rapidly reorienting its head towards the cricket. However, the change in gaze with the azimuth instead showed only a weak correlation because the eyes themselves are not always aligned with the azimuth due to stabilization periods (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). Together, these results suggest that in mice, tracking of visual objects in freely moving contexts is mediated through directed head movements, and corresponding eye movements that stabilize the gaze and periodically update to recenter with the head as it turns.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here we investigated the coordination of eye and head movements in mice during a visually guided ethological behavior, prey capture, that requires the localization of a specific point in the visual field. This work demonstrates that general principles of coordinated eye and head movements, observed across species, are present in the mouse. Additionally, we address the potential targeting of eye movements towards behaviorally relevant visual stimuli, specifically the moving cricket prey. We find that tracking is achieved through directed head movements that accurately target the cricket prey, rather than directed, independent eye movements. Together, these findings define how mice move their eyes to achieve an ethological behavior and provide a foundation for studying active visually-guided behaviors in the mouse.</p><p>One potential limitation of our eye tracking system is the 60 Hz framerate of the miniature cameras. This temporal resolution is significantly lower than traditional eye tracking paradigms using videography or eye-coil systems in head-restrained humans, non-human primates, and rodents (<xref ref-type="bibr" rid="bib31">Payne and Raymond, 2017</xref>; <xref ref-type="bibr" rid="bib32">Sakatani and Isa, 2007</xref>), though similar to recent video-based tracking in freely moving rodents (<xref ref-type="bibr" rid="bib26">Meyer et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Meyer et al., 2020</xref>) and humans (<xref ref-type="bibr" rid="bib25">Matthis et al., 2018</xref>). We do not expect that this would significantly alter our findings, as the basic parameters of eye movements (amplitude and speed) that we found (<xref ref-type="fig" rid="fig2">Figures 2B</xref>, <xref ref-type="fig" rid="fig3">3C and F</xref>) were similar to measurements made in both head-fixed mice with high-speed videography (<xref ref-type="bibr" rid="bib32">Sakatani and Isa, 2007</xref>) and freely moving mice with a magnetic sensor (<xref ref-type="bibr" rid="bib31">Payne and Raymond, 2017</xref>). However, although we are able to detect peak velocities over 300°/sec, we may still be under-estimating the peak velocity during saccades. Therefore increasing the temporal resolution further could lead to more robust detection of rapid gaze shifts and would potentially enhance classification of saccadic eye movements.</p><p>We found a pattern of gaze stabilization interspersed with abrupt, gaze-shifting saccades during both non-approach and approach epochs. This oculomotor strategy has been termed ‘saccade-and-fixate’ (reviewed in <xref ref-type="bibr" rid="bib18">Land, 1999</xref>), and is present in most visual animal species, from insects to primates, and was recently demonstrated in mice (<xref ref-type="bibr" rid="bib27">Meyer et al., 2020</xref>). In primates, gaze shifts can be purely driven by eye movements, but in other species saccades generally correspond to non-compensatory eye movements during head rotation, suggesting transient disengagement of VOR mechanisms. These saccadic movements are present in invertebrates and both foveate and non-foveate vertebrates (reviewed in <xref ref-type="bibr" rid="bib18">Land, 1999</xref>), and work to both recenter the eyes and relocate the position of gaze as animals turn. We found that these brief congruent head and eye movements are interspersed with longer duration (median ~200 ms) periods of compensatory movements, which stabilize the gaze to within nearly 1° as the head continues to rotate. Together these eye movements function to create a stable series of images on the retina even during rapid tracking of prey.</p><p>However, the saccade-and-fixate strategy raises the question of whether mice actively target a specific relevant location with saccadic eye movements. We examined this during periods of active approach toward the cricket to determine whether the eyes specifically target the cricket, relative to head orientation. During approaches, most saccades occur during corrective head turns toward the cricket location. While saccades do bring the gaze closer to the cricket, they do not do so more accurately than the head direction. In fact, prior to the saccade, mice sacrifice centering of the gaze on the target to instead achieve visual scene stability. The eyes then ‘catch up’ to the head as it is rotating (<xref ref-type="fig" rid="fig5">Figure 5B/C</xref>). Thus, these eye movements serve to reset eye position with the head, rather than targeting the cricket specifically. Combined with the fact that mice do not make significant eye movements in the absence of head movements (<xref ref-type="fig" rid="fig3">Figure 3F</xref>), this suggests that mice do not perform either directed eye saccades or smooth pursuit, which are prominent features of primate vision. On the other hand, the fact that mice use a saccade-and-fixate strategy makes it clear that they are still actively controlling their retinal input, despite low visual acuity. Indeed, the saccade-and-fixate strategy makes mouse vision consistent with the vast majority of species across the animal kingdom.</p><p>We also examined whether mice make specific vergence eye movements that could serve to modulate the binocular zone, as in some other species with eyes located laterally on the head. We find that rather than moving the eyes nasally to expand the binocular zone, during approach toward the cricket the two eyes become stably aligned, but at a neutral vergence angle that is neither converged or diverged (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). While several species with laterally-placed eyes use convergent eye movements during prey capture to create a wider binocular field (<xref ref-type="bibr" rid="bib9">Feord et al., 2020</xref>; <xref ref-type="bibr" rid="bib3">Bianco et al., 2011</xref>; <xref ref-type="bibr" rid="bib23">Martin, 2009</xref>; <xref ref-type="bibr" rid="bib17">Katz et al., 2015</xref>), our results show that mice do not utilize this strategy during prey capture. However, vergence eye movements in rodents have previously been shown to compensate for head tilt (<xref ref-type="bibr" rid="bib38">Wallace et al., 2013</xref>), and correspondingly we find that during approach periods mice stabilize head tilt. Thus, the stable relative alignment of the two eyes during approach likely reflects stabilization of the head itself. These results suggest that the 40 degree binocular zone is sufficient for tracking centrally located objects, as the eyes to not move to expand this during approaches. This is consistent with previous work showing that during active approach the mouse’s head is oriented within ±15 degrees relative to the cricket (<xref ref-type="bibr" rid="bib14">Hoy et al., 2016</xref>), meaning that even the resting binocular zone would encompass the cricket. However, it remains to be determined whether mice actually use binocular disparity for depth estimation during prey capture. A recent study demonstrated that mouse V1 encodes binocular disparities spanning a range of 3–25 cm from the mouse’s head (<xref ref-type="bibr" rid="bib20">Land, 2018</xref>), suggesting that disparity cues are available at the typical distances during approach (interquartile range 14.6 cm to 27.6 cm). Alternatively, mice may use retinal image size or other distance cues, or may simply orient to the azimuthal position of the cricket regardless of distance.</p><p>The finding that mice do not specifically move their eyes to target a location does not preclude the possibility that different regions of retinal space are specialized for certain processing. In fact, as a result of targeting head movements, the cricket prey is generally within the binocular zone during approach, so any mechanisms of enhanced processing in the binocular zone or lateral retina would still be behaviorally relevant. Anatomically, there is a gradient in density of different retinal ganglion cell types from medial to lateral retina (<xref ref-type="bibr" rid="bib4">Bleckert et al., 2014</xref>). Likewise behavioral studies have shown enhanced contrast detection when visual stimuli are located in the binocular field, rather than the monocular fields (<xref ref-type="bibr" rid="bib35">Speed et al., 2019</xref>). Based on the results presented here, in mice these specializations are likely to be engaged by head movements that localize stimuli in the binocular zone in front of the head, as opposed to primates, which make directed eye movements to localize stimuli on the fovea.</p><p>Together, the present findings suggest that orienting relative to visual cues is driven by head movements rather than eye movements in the mouse. This is consistent with the general finding that for animals with small heads it is more efficient to move the head, whereas animals with large heads have adapted eye movements for rapid shifts to overcome the inertia of the head (<xref ref-type="bibr" rid="bib20">Land, 2018</xref>). From the experimental perspective, this suggests that head angle alone is an appropriate measure to determine which visual cues are important during study of visually guided, goal-directed behaviors in the mouse. However, measurements of eye movements will be essential for computing the precise visual input animals receive (i.e., the retinal image) during ongoing freely moving behaviors, and how this visual input is processed within visual areas of the brain.</p><p>The saccade-and-fixate strategy generates a series of stable visual images separated by abrupt changes in gaze that shift the visual scene and location of objects on the retina. How then are these images, interleaved with periods of motion blur, converted into a continuous coherent percept that allows successful natural behaviors to occur? Anticipatory shifts in receptive field location during saccades, as well as gaze position-tuned neural populations, have been proposed as mechanisms in primates to maintain coherent percepts during saccades, while corollary discharge, saccadic suppression, and visual masking have been proposed to inhibit perception of motion blur during rapid eye movements (<xref ref-type="bibr" rid="bib13">Higgins and Rayner, 2015</xref>; <xref ref-type="bibr" rid="bib40">Wurtz, 2008</xref>). However, the mechanisms that might mediate these, at the level of specific cell types and neural circuits, are poorly understood. Studying these processes in the mouse will allow for investigation of the neural circuit basis of these perceptual mechanisms through the application of genetic tools and other circuit dissection approaches (<xref ref-type="bibr" rid="bib16">Huberman and Niell, 2011</xref>; <xref ref-type="bibr" rid="bib22">Luo et al., 2008</xref>). Importantly, most of our visual perception occurs during active exploration of the environment, where the combined dynamics of head and eye movements create a dramatically different image processing challenge than typical studies in stationary subjects viewing stimuli on a computer monitor. Examination of these neural mechanisms will extend our understanding of how the brain performs sensory processing in real-world conditions.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th valign="top">Reagent type <break/>(species) or resource</th><th valign="top">Designation</th><th valign="top">Source or reference</th><th valign="top">Identifiers</th><th valign="top">Additional <break/>information</th></tr></thead><tbody><tr><td valign="top">Strain, strain background (<italic>Mus musculus</italic>)</td><td valign="top">C57Bl/6J</td><td valign="top">JAX</td><td valign="top">JAX: 000664</td><td valign="top">Wild type animals</td></tr><tr><td valign="top">Software, algorithm</td><td valign="top">Matlab</td><td valign="top">Matlab</td><td valign="top">Matlab R2020a</td><td valign="top"/></tr><tr><td valign="top">Software, algorithm</td><td valign="top">DeepLabCut</td><td valign="top"><xref ref-type="bibr" rid="bib24">Mathis et al., 2018</xref></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top">Software, <break/>algorithm</td><td valign="top">Bonsai</td><td valign="top"><xref ref-type="bibr" rid="bib21">Lopes et al., 2015</xref></td><td valign="top"/><td valign="top"/></tr></tbody></table></table-wrap><sec id="s4-1"><title>Animals</title><p>All procedures were conducted in accordance with the guidelines of the National Institutes of Health and were approved by the University of Oregon Institutional Animal Care and Use Committee (protocol number 17–27). Animals used for this study were wild-type (C57 Bl/6J) males and females (3 males and four females) aged 2–6 months.</p></sec><sec id="s4-2"><title>Prey capture behavior</title><p>Prey capture experiments were performed following the general paradigm of <xref ref-type="bibr" rid="bib14">Hoy et al., 2016</xref>. Mice readily catch crickets in the homecage without any training or habituation, even on the first exposure to crickets. However, we perform a standard habituation process to acclimate the mice to being handled by the experimenters, hunting within the experimental arena, and wearing cameras and an IMU while hunting. Following six 3 min sessions (over 1–2 days) of handling, the animals were placed in the prey capture arena to explore with their cagemates. The duration of this group habituation was at least six 10 min sessions over 1–2 days. One cricket (Rainbow mealworms, 5 week old) per mouse was placed in the arena with the mice for the last half of the habituation sessions. For the subsequent habituation step, the mice were placed in the arena alone with one cricket for 7–10 min. This step was repeated for 2–3 training days (6–9 sessions) until most mice successfully caught crickets within the 10 min period.</p><p>Animals were then habituated to head-fixation above a spherical Styrofoam treadmill (<xref ref-type="bibr" rid="bib7">Dombeck et al., 2007</xref>). Head fixation was only used to fit, calibrate, and attach cameras before experiments. Cameras were then fitted to each mouse (described below) and mice were habituated to wearing the cameras while walking freely in the arena, which took 1–2 sessions lasting 10 min. After the animals were comfortable with free locomotion with cameras, they were habituated to hunting with cameras attached. This took roughly one to two e hunting sessions of 10 min duration for each mouse. The animals were then food deprived for a period of ~12–18 hr and then run in the prey capture assay for three 10 min sessions per data collection day. Although animals will hunt crickets without food restriction, this allowed for more trials within a defined experimental period.</p><p>The rectangular prey capture arena was a white arena of dimensions 38 × 45×30 cm (<xref ref-type="bibr" rid="bib14">Hoy et al., 2016</xref>). The arena was illuminated with a 15 Watt, 100 lumen incandescent light bulb placed roughly one meter above the center of the arena to mimic lux during dawn and dusk, times at which mice naturally hunt (<xref ref-type="bibr" rid="bib2">Bailey and Sperry, 1929</xref>). Video signal was recorded from above the arena using a CMOS camera (Basler Ace, acA2000–165 umNIR, 30 Hz acquisition).</p><p>Following the habituation process, cameras were attached and mice were placed in the prey capture arena with one cricket. Experimental animals captured and consumed the cricket before a new cricket was placed in the arena. The experimenters removed any residual cricket pieces in the arena before the addition of the next cricket. A typical mouse catches and consumes between 3–5 crickets per 10 min session. Control experiments were performed using the same methods, but with no cameras or IMU attached.</p></sec><sec id="s4-3"><title>Surgical procedure</title><p>To allow for head-fixation during initial eye camera alignment, before the habituation process mice were surgically implanted with a steel headplate, following <xref ref-type="bibr" rid="bib30">Niell and Stryker, 2010</xref>. Animals were anesthetized with isoflurane (3% induction, 1.5–2% maintenance, in O<sub>2</sub>) and body temperature was maintained at 37.5°C using a feedback-controlled heating pad. Fascia was cleared from the surface of the skull following scalp incision and a custom steel headplate was attached to the skull using Vetbond (3M) and dental acrylic. The headplate was placed near the back of the skull, roughly 1 mm anterior of Lambda. A flat layer of dental acrylic was placed in front of the headplate to allow for attachment of the camera connectors. Carprofen (10 mg/kg) and lactated Ringer’s solution were administered subcutaneously and animals were monitored for three days following surgery.</p></sec><sec id="s4-4"><title>Camera assembly and head-mounting</title><p>To measure eye position, we used miniature cameras that could be reversibly attached to the mouse’s head via a chronically implanted Millmax connector. The cameras (1000 TVL Mini CCTV Camera; iSecurity101) were 5 × 6 × 6mm with a resolution of 480 × 640 pixels and a 78 degree viewing angle, and images were acquired at 30 Hz. Some of the cameras were supplied with a built in NIR blocking filter. For these cameras, the lens was unscrewed and the glass IR filter removed with fine forceps. A 200 Ohm resistor and 3 mm IR LED were integrated onto the cameras for uniform illumination of the eyes. Power, ground, and video cables were soldered with lightweight 36 gauge FEP hookup wire (Cooner Wire; CZ 1174). A 6 mm diameter collimating lens with a focal distance of 12 mm (Lilly Electronics) was inserted into custom 3D printed housing and the cameras were then inserted and glued behind this (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for schematic of design). The inner side of the arm of the camera holder housed a male Mill-Max connector (Mill-Max Manufacturing Corp. 853-93-100-10-001000) cut to 5 mm (2 rows of 4 columns), used for reversible attachment of the cameras to the implants of experimental animals. A custom IMU board with integrated 3-dimensional accelerometer and gyroscopes (Rosco Technologies) was attached to the top of one of the camera holders (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). The total weight of the two cameras together, with the lenses, connectors, 3D printed holders, and IMU was 2.6 grams. Camera assemblies were fitted onto the head by attaching them to corresponding female Mill-Max connectors. Cameras were located in the far lateral periphery of the mouse’s visual field, roughly 100° lateral of the head midline and 40 degrees above the horizontal axis, and covered roughly 25 × 25° of the visual field. When the camera was appropriately focused on the eye, the female connectors were glued onto the acrylic implant using cyanoacrylate adhesive (Loctite). Because the connectors were each positioned during this initial procedure and permanently fixed in place, no adjustment of camera alignment was needed for subsequent experimental days. With this system, the average magnitude of camera shake jitter across experiments was 0.49 + / - 0.33 pixels (mean + / - s.d., N = 7 animals), as measured by computing the RMS frame-to-frame jitter of stationary points on the animal’s head (base of the implant) in the recorded videos.</p></sec><sec id="s4-5"><title>Mouse and cricket tracking</title><p>Video data with timestamps for the overhead camera were acquired at 30 frames per second (fps) using Bonsai (<xref ref-type="bibr" rid="bib21">Lopes et al., 2015</xref>). We used DeepLabCut (<xref ref-type="bibr" rid="bib24">Mathis et al., 2018</xref>) for markerless estimation of mouse and cricket position from overhead videos. For network training, we selected eight points on the mouse head (nose, two camera connectors, two IR LEDs, two ears, and center of the head between the two ears), and two points for the cricket (head and body). Following estimation of the selected points, analysis was performed with custom MATLAB scripts, available at <xref ref-type="bibr" rid="bib28">Michaiel et al., 2020</xref>.</p><p>To determine periods when the animal was moving versus stationary, head movement speed was median filtered across a window of 500 ms and a threshold of 1 cm/sec was applied. Position and angle of the head were computed by fitting the eight measured points on the head for each video frame to a defined mean geometry plus an x-y translation and horizontal rotation. The head direction was defined as the angle of this rotation, referenced to the line between the nose and center of the head. We also used this head-centered reference to compute the azimuth, which is the angle of the mouse relative to the cricket. Following <xref ref-type="bibr" rid="bib14">Hoy et al., 2016</xref>, we defined approaches as times at which the velocity of the mouse was greater than 1 cm/sec, the azimuth of the mouse was between −45 and 45 degrees relative to cricket location, and the distance to the cricket was decreasing at a rate greater than 10 cm/sec. Although mice eventually catch and consume the cricket in each trial, and are motivated to hunt due to food restriction, we cannot rule out the possibility that some approach periods may represent tracking or chasing without the intent to capture.</p><p>Analog voltage signals from the IMU were recorded using a LabJack U6 at 50 Hz sampling rate. Voltages from the accelerometer channels were median filtered with a window of 266.7 ms to remove rapid transients and converted to m/sec<sup>2</sup>, providing angular head orientation. Voltages from the gyroscope channels were converted to radians/sec without filtering, providing head rotation velocity.</p></sec><sec id="s4-6"><title>Eye tracking and eye camera calibration</title><p>Video data with timestamps for the two eyes were acquired at 30fps using Bonsai. The video data are delivered by the camera in NTSC format, an interlaced video format in which two sequential images (acquired at 60fps) are interdigitated into each frame on alternate horizontal lines. We therefore de-interlaced the video in order to restore the native 60fps resolution by separating out alternate lines of each image. We then linearly downsampled the resolution along the horizontal axis by a factor of two, to match spatial resolution in horizontal and vertical dimensions.</p><p>To track eye position, we used DeepLabCut (<xref ref-type="bibr" rid="bib24">Mathis et al., 2018</xref>) to track eight points along the edge of the pupil. The eight points were then fit to an ellipse using the least-squares criterion. In order to convert pupil displacement into angular rotation, which cannot be calibrated by directed fixation as in primates, we followed the methods used in <xref ref-type="bibr" rid="bib38">Wallace et al., 2013</xref>. This approach is based on the principle that when the eye is pointed directly at the camera axis, the pupil is circular, and as the eye rotates, the circular shape flattens into an ellipse depending on the direction and degree of angular rotation from the center of the camera axis. To calculate the transformation of a circle along the camera axis to the ellipse fit, two pieces of information are needed: the camera axis center position and the scale factor relating pixels of displacement to angular rotation. To find the camera axis, we used the constraint that the major axis of the pupil ellipse is perpendicular to the vector from the pupil center to the camera axis center. This defines a set of linear equations for all of the pupil observations with significant ellipticity, which are solved directly with a least-squares solution. Next, the scale factor was estimated based on the equation defining how the ellipticity of the pupil changes with the corresponding shift from the camera center in each video frame. Based on the camera center and scale factor for each video, we calculated the affine transformation needed to transform the circle to the ellipse fit of the pupil in each frame, and the angular displacement from the camera axis was then used for subsequent analyses. Mathematical details of this method are presented in <xref ref-type="bibr" rid="bib38">Wallace et al., 2013</xref>.</p><p>Following computation of kinematic variables (mouse, cricket, and eye position/rotation), these values were linearly interpolated to a standard 60 Hz timestamp to account for differences in acquisition timing across the multiple cameras and the IMU.</p><p>To characterize the robustness of the tracking system, we estimated the error in eye and head position measurements. As there is no ground truth measurement for eye position to compare to, we estimated an upper bound based on the stability of the eye when the head was stationary, as this is when eye movements are expected to be minimal. Specifically, we computed the standard deviation of horizontal eye position between frames during times when mouse speed was less than 1 cm/sec and head rotation &lt;1 degree. We computed the error to be 0.51 + / - 0.25 degrees (mean + / - s.d., n = 105 trials). Similarly, to estimate the error of head angle measurements, we compared the independent estimates of head yaw rotation between frames as measured by both the IMU and DeepLabCut tracking. These measures have an RMS difference of 0.95 + / - 0.25 (mean+/-s.d., n = 105 trials), which represents an upper bound as it is based on the combined error of these two measurements separately. Thus, we infer that errors in estimating eye and head position are both less than one degree.</p></sec><sec id="s4-7"><title>Quantification and statistical analyses</title><p>Two-tailed paired t-tests or Wilcoxon Rank sum tests were used to compare data between non-approach and approach epochs. For comparisons between experimental and control groups, two-sample tests (Kolmogorov-Smirnov or two-sample two-tailed t-test) were used. Significance was defined as p&lt;0.05, although p-values are presented throughout. In all figures, error bars represent ±the standard error of the mean or median, as appropriate.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Yichen Fan and Alyssa Fuentez for their help with behavioral data collection; the Wehr lab for assistance in implementing head-mounted cameras; members of the Niell lab for helpful discussions; and Dr. Marina Garrett, Dr. Clifford Keller, Dr. Jude Mitchell, and Dr. Matthew Smear for feedback on the manuscript. This work was supported by NIH R34NS111669 (CMN) and the University of Oregon Promising Scholar Award (AMM).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceived the project, Developed methodology and designed experiments, Analyzed data, Wrote the manuscript, Created figures</p></fn><fn fn-type="con" id="con2"><p>Wrote camera calibration software, Contributed to data analysis, Manuscript preparation</p></fn><fn fn-type="con" id="con3"><p>Conceived the project, Designed experiments, Analyzed data, Wrote manuscript, Provided resources</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Animal experimentation: All procedures were conducted in accordance with the guidelines of the National Institutes of Health and were approved by the University of Oregon Institutional Animal Care and Use Committee (Protocol number: 17-27).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-57458-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Behavioral data has been submitted to Dryad with DOI <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.8cz8w9gmw">https://doi.org/10.5061/dryad.8cz8w9gmw</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Michaiel</surname><given-names>AM</given-names></name><name><surname>Abe</surname><given-names>ETT</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Data from: Dynamics of gaze control during prey capture in freely moving mice</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.8cz8w9gmw</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname> <given-names>R</given-names></name><name><surname>Larsson</surname> <given-names>L</given-names></name><name><surname>Holmqvist</surname> <given-names>K</given-names></name><name><surname>Stridh</surname> <given-names>M</given-names></name><name><surname>Nyström</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>One algorithm to rule them all? an evaluation and discussion of ten eye movement event-detection algorithms</article-title><source>Behavior Research Methods</source><volume>49</volume><fpage>616</fpage><lpage>637</lpage><pub-id pub-id-type="doi">10.3758/s13428-016-0738-9</pub-id><pub-id pub-id-type="pmid">27193160</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Bailey</surname> <given-names>V</given-names></name><name><surname>Sperry</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1929">1929</year><article-title>Life history and habits of grasshopper mice</article-title><source>Genus Onychomys</source><ext-link ext-link-type="uri" xlink:href="https://ageconsearch.umn.edu/record/157954?ln=en">https://ageconsearch.umn.edu/record/157954?ln=en</ext-link><date-in-citation iso-8601-date="2020-03-25">March 25, 2020</date-in-citation></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname> <given-names>IH</given-names></name><name><surname>Kampff</surname> <given-names>AR</given-names></name><name><surname>Engert</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Prey capture behavior evoked by simple visual stimuli in larval zebrafish</article-title><source>Frontiers in Systems Neuroscience</source><volume>5</volume><elocation-id>101</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2011.00101</pub-id><pub-id pub-id-type="pmid">22203793</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bleckert</surname> <given-names>A</given-names></name><name><surname>Schwartz</surname> <given-names>GW</given-names></name><name><surname>Turner</surname> <given-names>MH</given-names></name><name><surname>Rieke</surname> <given-names>F</given-names></name><name><surname>Wong</surname> <given-names>RO</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Visual space is represented by nonmatching topographies of distinct mouse retinal ganglion cell types</article-title><source>Current Biology</source><volume>24</volume><fpage>310</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.12.020</pub-id><pub-id pub-id-type="pmid">24440397</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cartmill</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Rethinking primate origins</article-title><source>Science</source><volume>184</volume><fpage>436</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1126/science.184.4135.436</pub-id><pub-id pub-id-type="pmid">4819676</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname> <given-names>BJ</given-names></name><name><surname>Hamilton</surname> <given-names>DA</given-names></name><name><surname>Whishaw</surname> <given-names>IQ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Motor activity (exploration) and formation of home bases in mice (C57BL/6) influenced by visual and tactile cues: modification of movement distribution, distance, location, and speed</article-title><source>Physiology &amp; Behavior</source><volume>87</volume><fpage>805</fpage><lpage>816</lpage><pub-id pub-id-type="doi">10.1016/j.physbeh.2006.01.026</pub-id><pub-id pub-id-type="pmid">16530235</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dombeck</surname> <given-names>DA</given-names></name><name><surname>Khabbaz</surname> <given-names>AN</given-names></name><name><surname>Collman</surname> <given-names>F</given-names></name><name><surname>Adelman</surname> <given-names>TL</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Imaging large-scale neural activity with cellular resolution in awake, mobile mice</article-title><source>Neuron</source><volume>56</volume><fpage>43</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.08.003</pub-id><pub-id pub-id-type="pmid">17920014</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dräger</surname> <given-names>UC</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Observations on monocular deprivation in mice</article-title><source>Journal of Neurophysiology</source><volume>41</volume><fpage>28</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1152/jn.1978.41.1.28</pub-id><pub-id pub-id-type="pmid">621544</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feord</surname> <given-names>RC</given-names></name><name><surname>Sumner</surname> <given-names>ME</given-names></name><name><surname>Pusdekar</surname> <given-names>S</given-names></name><name><surname>Kalra</surname> <given-names>L</given-names></name><name><surname>Gonzalez-Bellido</surname> <given-names>PT</given-names></name><name><surname>Wardill</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cuttlefish use Stereopsis to strike at prey</article-title><source>Science Advances</source><volume>6</volume><elocation-id>eaay6036</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aay6036</pub-id><pub-id pub-id-type="pmid">31934631</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname> <given-names>W</given-names></name><name><surname>Tellez</surname> <given-names>LA</given-names></name><name><surname>Rangel</surname> <given-names>MJ</given-names></name><name><surname>Motta</surname> <given-names>SC</given-names></name><name><surname>Zhang</surname> <given-names>X</given-names></name><name><surname>Perez</surname> <given-names>IO</given-names></name><name><surname>Canteras</surname> <given-names>NS</given-names></name><name><surname>Shammah-Lagnado</surname> <given-names>SJ</given-names></name><name><surname>van den Pol</surname> <given-names>AN</given-names></name><name><surname>de Araujo</surname> <given-names>IE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Integrated control of predatory hunting by the central nucleus of the amygdala</article-title><source>Cell</source><volume>168</volume><fpage>311</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.12.027</pub-id><pub-id pub-id-type="pmid">28086095</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harkness</surname> <given-names>L</given-names></name><name><surname>Bennet-Clark</surname> <given-names>HC</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>The deep fovea as a focus Indicator</article-title><source>Nature</source><volume>272</volume><fpage>814</fpage><lpage>816</lpage><pub-id pub-id-type="doi">10.1038/272814a0</pub-id><pub-id pub-id-type="pmid">643070</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname> <given-names>M</given-names></name><name><surname>Ballard</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Eye movements in natural behavior</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>188</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.02.009</pub-id><pub-id pub-id-type="pmid">15808501</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname> <given-names>E</given-names></name><name><surname>Rayner</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Transsaccadic processing: stability, integration, and the potential role of remapping</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>77</volume><fpage>3</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.3758/s13414-014-0751-y</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname> <given-names>JL</given-names></name><name><surname>Yavorska</surname> <given-names>I</given-names></name><name><surname>Wehr</surname> <given-names>M</given-names></name><name><surname>Niell</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vision Drives Accurate Approach Behavior during Prey Capture in Laboratory Mice</article-title><source>Current Biology</source><volume>26</volume><fpage>3046</fpage><lpage>3052</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.09.009</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname> <given-names>JL</given-names></name><name><surname>Bishop</surname> <given-names>HI</given-names></name><name><surname>Niell</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Defined Cell Types in Superior Colliculus Make Distinct Contributions to Prey Capture Behavior in the Mouse</article-title><source>Current Biology</source><volume>29</volume><fpage>4130</fpage><lpage>4138</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.10.017</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huberman</surname> <given-names>AD</given-names></name><name><surname>Niell</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>What can mice tell Us about how vision works?</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>464</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2011.07.002</pub-id><pub-id pub-id-type="pmid">21840069</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname> <given-names>HK</given-names></name><name><surname>Lustig</surname> <given-names>A</given-names></name><name><surname>Lev-Ari</surname> <given-names>T</given-names></name><name><surname>Nov</surname> <given-names>Y</given-names></name><name><surname>Rivlin</surname> <given-names>E</given-names></name><name><surname>Katzir</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Eye movements in chameleons are not truly independent - evidence from simultaneous monocular tracking of two targets</article-title><source>Journal of Experimental Biology</source><volume>218</volume><fpage>2097</fpage><lpage>2105</lpage><pub-id pub-id-type="doi">10.1242/jeb.113084</pub-id><pub-id pub-id-type="pmid">26157161</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Motion and vision: why animals move their eyes</article-title><source>Journal of Comparative Physiology A: Sensory, Neural, and Behavioral Physiology</source><volume>185</volume><fpage>341</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1007/s003590050393</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Eye movements and the control of actions in everyday life</article-title><source>Progress in Retinal and Eye Research</source><volume>25</volume><fpage>296</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.preteyeres.2006.01.002</pub-id><pub-id pub-id-type="pmid">16516530</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The evolution of gaze shifting eye movements</article-title><source>Processes of Visuospatial Attention and Working Memory</source><volume>41</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1007/7854_2018_60</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname> <given-names>G</given-names></name><name><surname>Bonacchi</surname> <given-names>N</given-names></name><name><surname>Frazão</surname> <given-names>J</given-names></name><name><surname>Neto</surname> <given-names>JP</given-names></name><name><surname>Atallah</surname> <given-names>BV</given-names></name><name><surname>Soares</surname> <given-names>S</given-names></name><name><surname>Moreira</surname> <given-names>L</given-names></name><name><surname>Matias</surname> <given-names>S</given-names></name><name><surname>Itskov</surname> <given-names>PM</given-names></name><name><surname>Correia</surname> <given-names>PA</given-names></name><name><surname>Medina</surname> <given-names>RE</given-names></name><name><surname>Calcaterra</surname> <given-names>L</given-names></name><name><surname>Dreosti</surname> <given-names>E</given-names></name><name><surname>Paton</surname> <given-names>JJ</given-names></name><name><surname>Kampff</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><pub-id pub-id-type="pmid">25904861</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>L</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Genetic dissection of neural circuits</article-title><source>Neuron</source><volume>57</volume><fpage>634</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.01.002</pub-id><pub-id pub-id-type="pmid">18341986</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>GR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>What is binocular vision for? A birds' eye view</article-title><source>Journal of Vision</source><volume>9</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/9.11.14</pub-id><pub-id pub-id-type="pmid">20053077</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthis</surname> <given-names>JS</given-names></name><name><surname>Yates</surname> <given-names>JL</given-names></name><name><surname>Hayhoe</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gaze and the control of foot placement when walking in natural terrain</article-title><source>Current Biology</source><volume>28</volume><fpage>1224</fpage><lpage>1233</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.03.008</pub-id><pub-id pub-id-type="pmid">29657116</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>AF</given-names></name><name><surname>Poort</surname> <given-names>J</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Linden</surname> <given-names>JF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A Head-Mounted Camera system integrates detailed behavioral monitoring with multichannel electrophysiology in freely moving mice</article-title><source>Neuron</source><volume>100</volume><fpage>46</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.020</pub-id><pub-id pub-id-type="pmid">30308171</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>AF</given-names></name><name><surname>O’Keefe</surname> <given-names>J</given-names></name><name><surname>Poort</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Two distinct types of eye-head coupling in freely moving mice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.02.20.957712</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Michaiel</surname> <given-names>AM</given-names></name><name><surname>Abe</surname> <given-names>ET</given-names></name><name><surname>Niell</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>2020_Head-EyeTracking</data-title><source>Github</source><version designator="c392559">c392559</version><ext-link ext-link-type="uri" xlink:href="https://github.com/nielllab/Michaiel-et-al.-2020_Head-EyeTracking">https://github.com/nielllab/Michaiel-et-al.-2020_Head-EyeTracking</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname> <given-names>RGM</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Spatial localization does not require the presence of local cues</article-title><source>Learning and Motivation</source><volume>12</volume><fpage>239</fpage><lpage>260</lpage><pub-id pub-id-type="doi">10.1016/0023-9690(81)90020-5</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname> <given-names>CM</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payne</surname> <given-names>HL</given-names></name><name><surname>Raymond</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Magnetic eye tracking in mice</article-title><source>eLife</source><volume>6</volume><elocation-id>e29222</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.29222</pub-id><pub-id pub-id-type="pmid">28872455</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakatani</surname> <given-names>T</given-names></name><name><surname>Isa</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Quantitative analysis of spontaneous saccade-like rapid eye movements in C57BL/6 mice</article-title><source>Neuroscience Research</source><volume>58</volume><fpage>324</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1016/j.neures.2007.04.003</pub-id><pub-id pub-id-type="pmid">17482700</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samonds</surname> <given-names>JM</given-names></name><name><surname>Geisler</surname> <given-names>WS</given-names></name><name><surname>Priebe</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Natural image and receptive field statistics predict saccade sizes</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1591</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0255-5</pub-id><pub-id pub-id-type="pmid">30349110</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shang</surname> <given-names>C</given-names></name><name><surname>Liu</surname> <given-names>A</given-names></name><name><surname>Li</surname> <given-names>D</given-names></name><name><surname>Xie</surname> <given-names>Z</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Huang</surname> <given-names>M</given-names></name><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Shen</surname> <given-names>WL</given-names></name><name><surname>Cao</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A subcortical excitatory circuit for sensory-triggered predatory hunting in mice</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>909</fpage><lpage>920</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0405-4</pub-id><pub-id pub-id-type="pmid">31127260</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Speed</surname> <given-names>A</given-names></name><name><surname>Del Rosario</surname> <given-names>J</given-names></name><name><surname>Burgess</surname> <given-names>CP</given-names></name><name><surname>Haider</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cortical state fluctuations across layers of V1 during visual spatial perception</article-title><source>Cell Reports</source><volume>26</volume><fpage>2868</fpage><lpage>2874</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2019.02.045</pub-id><pub-id pub-id-type="pmid">30865879</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stahl</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Using eye movements to assess brain function in mice</article-title><source>Vision Research</source><volume>44</volume><fpage>3401</fpage><lpage>3410</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.09.011</pub-id><pub-id pub-id-type="pmid">15536008</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Straka</surname> <given-names>H</given-names></name><name><surname>Zwergal</surname> <given-names>A</given-names></name><name><surname>Cullen</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vestibular animal models: contributions to understanding physiology and disease</article-title><source>Journal of Neurology</source><volume>263</volume><fpage>10</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1007/s00415-015-7909-y</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname> <given-names>DJ</given-names></name><name><surname>Greenberg</surname> <given-names>DS</given-names></name><name><surname>Sawinski</surname> <given-names>J</given-names></name><name><surname>Rulla</surname> <given-names>S</given-names></name><name><surname>Notaro</surname> <given-names>G</given-names></name><name><surname>Kerr</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rats maintain an overhead binocular field at the expense of constant fusion</article-title><source>Nature</source><volume>498</volume><fpage>65</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1038/nature12153</pub-id><pub-id pub-id-type="pmid">23708965</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Walls</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="1942">1942</year><source>The Vertebrate Eye and Its Adaptive Radiation</source><publisher-name>Gordon Lynn Walls</publisher-name></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurtz</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neuronal mechanisms of visual stability</article-title><source>Vision Research</source><volume>48</volume><fpage>2070</fpage><lpage>2089</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2008.03.021</pub-id><pub-id pub-id-type="pmid">18513781</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarbus</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>Human saccadic eye movements</article-title><source>Eye Movements and Vision</source><volume>1</volume><fpage>7</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1068/i0382</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname> <given-names>ZD</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Xiang</surname> <given-names>X</given-names></name><name><surname>Hu</surname> <given-names>M</given-names></name><name><surname>Xie</surname> <given-names>H</given-names></name><name><surname>Jia</surname> <given-names>X</given-names></name><name><surname>Cai</surname> <given-names>F</given-names></name><name><surname>Cui</surname> <given-names>Y</given-names></name><name><surname>Chen</surname> <given-names>Z</given-names></name><name><surname>Qian</surname> <given-names>L</given-names></name><name><surname>Liu</surname> <given-names>J</given-names></name><name><surname>Shang</surname> <given-names>C</given-names></name><name><surname>Yang</surname> <given-names>Y</given-names></name><name><surname>Ni</surname> <given-names>X</given-names></name><name><surname>Sun</surname> <given-names>W</given-names></name><name><surname>Hu</surname> <given-names>J</given-names></name><name><surname>Cao</surname> <given-names>P</given-names></name><name><surname>Li</surname> <given-names>H</given-names></name><name><surname>Shen</surname> <given-names>WL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Zona incerta GABAergic neurons integrate prey-related sensory signals and induce an appetitive drive to promote hunting</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>921</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0404-5</pub-id><pub-id pub-id-type="pmid">31127258</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.57458.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Spering</surname><given-names>Miriam</given-names></name><role>Reviewing Editor</role><aff><institution>The University of British Columbia</institution><country>Canada</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Isa</surname><given-names>Tadashi</given-names> </name><role>Reviewer</role><aff><institution>National Institute for Physiological Sciences</institution><country>Japan</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Leonardo</surname><given-names>Anthony</given-names> </name><role>Reviewer</role><aff><institution>HHMI, Janelia Farm</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Churchland</surname><given-names>Anne K</given-names></name><role>Reviewer</role><aff><institution>Cold Spring Harbor Laboratory</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper provides novel insights into how mice actively acquire visual information in natural contexts by moving their eyes and head. Findings provide a foundation for studying active vision and movement during ethological behaviors such as prey catching.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Dynamics of gaze control during prey capture in freely moving mice&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Laura Colgin as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Tadashi Isa (Reviewer #1); Anthony Leonardo (Reviewer #2); Anne K. Churchland (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require a modest amount of additional new data, as they do with your paper, we are asking that the manuscript be revised to either limit claims to those supported by data in hand, or to explicitly state that the relevant conclusions require additional supporting data.</p><p>Our expectation is that the authors will eventually carry out the additional experiments and report on how they affect the relevant conclusions either in a preprint on bioRxiv or medRxiv, or if appropriate, as a Research Advance in <italic>eLife</italic>, either of which would be linked to the original paper.</p><p>Summary:</p><p>This paper presents a creative task and utilizes an elegant, wearable two-camera measurement device along with a 3-d accelerometer and gyroscope to investigate eye and head movements in free-moving mice during general exploration and natural prey capture. The authors show that the majority of eye movements are compensatory for head movements which stabilizes the visual scene. At the same time, they also recorded non-compensatory saccades. However, head movements are targeting the prey more precisely than the eyes. In a nutshell, head movements are the primary gatherer of directed visual information, and eye movements are used in a compensatory fashion. This is broadly consistent with studies on other vertebrates that lack a fovea and overall what one might expect for a fovea-less organism (no need to orient eyes except for keeping visual target of interest broadly within them; see for example Borghuis' study on amphibians which shows much the same). The authors also measured vergence. They found that during approach of the prey, the animal's head tends to be at a neutral pitch, and neutral pitches are associated with neither convergent nor divergent eye movements. Taken together, the authors argue that these observations constrain the mouse's oculomotor strategy during prey capture, and that this strategy can be summarized as &quot;saccade and fixate.&quot;</p><p>The reviewers and I agree that the experimental study and data analysis are technically rigorous and the paper is rich with detailed statistical information. Whereas there is no fundamentally new result here about gaze control on a general computational level, this work will be of great use to the mouse vision and behavior community. There is also considerable current interest in measuring neural activity during free-moving behavior making this an important paper in terms of its rigorous characterization of the accompanying head and eye movements. Further, the authors selected a really nice behavior, cricket hunting, that highlights the dual role of mice as not only prey but predator as well. This dual role means that the lateral positioning of the eyes on the mouse's head is somewhat mysterious, raising questions about what its oculomotor strategy might be during prey capture.</p><p>Revisions for this paper:</p><p>All reviewers emphasize the importance of discussing in more detail the limitations of a measurement device that samples eye movements at 30 Hz. I endorse these comments and would like to see them addressed in detail in a revision.</p><p>1) The authors recorded the eye movements with the time resolution of 30 Hz. This is obviously slow to record the rapid eye movements in general. In case of primate and human studies, the standard is 1000Hz. In case of rodents, some older studies recorded at 240 Hz and the results showed that under the head-fixed condition, the peak velocity of saccades in mice was several hundred degree/s (Sakatani and Isa, 2004, 2007). Reviewer #1 is concerned that the authors missed the very fast component of saccades from their analysis and concluded the dominance of compensatory eye movements, which are slow.</p><p>2) The authors' system for measuring eye position is well-suited for freely moving animals. Reviewer #3 would have liked to see a bit more data validating that system. For instance, the authors might compute the gain of the VOR and see how its mean and variance measure up to other studies. This would be somewhat tricky since most other work is in head-fixed animals, but it would at least give the reader a sense as to how the measures here compare to others in the field.</p><p>3) Along the same lines, more plots of eye velocity and not just position (e.g., in Figure 1) would be helpful. I recognize that with a 30Hz camera sampling rate, the velocity traces will be noisy; nonetheless, it would help give readers a sense of how the measurements here square up with others. Because this system will likely be used by other groups, due to its many advantages, having a sense of any pitfalls will be helpful.</p><p>Other methodological questions:</p><p>4) The authors need more explanation about the GMM clustering and how they divided compensatory eye movements and saccades in the Materials and methods section. In Figure 4A, it seems some red dots and blue dots are mixed with each other.</p><p>Terminology / Discussion:</p><p>5) Please provide clarification regarding the pursuit terminology. Whereas the term &quot;pursuit&quot; might be standard in interception and behavior studies of predator/prey interactions, with many decades of use, there is some ambiguity with smooth pursuit eye movements – slow, continuous rotations of the eyes to keep small moving objects close to the fovea. Reviewer #2 suggests additional editing and clear definitions at the beginning of the manuscript (e.g. prey pursuit vs smooth pursuit).</p><p>6) Discussion: Reviewer #3 was surprised that the authors described mice as, &quot;not a highly visual species.&quot; Their own work highlights that mice use vision for prey capture. Further, as the authors, know, mice have multiple (~7) independent maps of visual space in the brain. Why would the brain go to the trouble of generating all these maps if mice didn't make much use their visual systems? Finally, describing an animal as &quot;highly visual&quot; or &quot;not highly visual&quot; ignores the fact that all animals use multiple sensory systems to navigate the world, and tradeoff which sensory modality is weighted most heavily in a dynamic way (e.g., relying on vision during daytime hunting/foraging and somatosensation at night).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.57458.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Revisions for this paper:</p><p>All reviewers emphasize the importance of discussing in more detail the limitations of a measurement device that samples eye movements at 30 Hz. I endorse these comments and would like to see them addressed in detail in a revision.</p><p>1) The authors recorded the eye movements with the time resolution of 30 Hz. This is obviously slow to record the rapid eye movements in general. In case of primate and human studies, the standard is 1000Hz. In case of rodents, some older studies recorded at 240 Hz and the results showed that under the head-fixed condition, the peak velocity of saccades in mice was several hundred degree/s (Sakatani and Isa, 2004, 2007). Reviewer #1 is concerned that the authors missed the very fast component of saccades from their analysis and concluded the dominance of compensatory eye movements, which are slow.</p></disp-quote><p>We greatly appreciate this concern, which spurred us to re-analyze our data at higher framerate, an option we had not pursued previously. In order to achieve this, we have taken advantage of the fact that our 30Hz video was in fact in interlaced format, meaning that there were actually two sequential images in each video frame, interleaved together in alternate horizontal rows. By de-interlacing these two images, we are able to restore the full 60Hz video rate, albeit at a 2x loss of spatial resolution along the y-axis. This loss of spatial resolution does not impede our ability to resolve pupil position (estimated at &lt;=0.5deg as discussed below), and therefore <italic>we</italic> now present all our results at 60Hz temporal resolution, which has been standard in several recent studies of eye movements in freely moving rat and mouse. While this provides improved segregation of non-compensatory movements (Figure 4B) and clearer demonstration of the saccade-and-fixate pattern (Figure 4C), our overall findings are unchanged. We greatly appreciate the motivation to improve our data quality in this manner. We have also added a discussion of the temporal resolution and comparison to studies using various methods.</p><disp-quote content-type="editor-comment"><p>2) The authors' system for measuring eye position is well-suited for freely moving animals. Reviewer #3 would have liked to see a bit more data validating that system. For instance, the authors might compute the gain of the VOR and see how its mean and variance measure up to other studies. This would be somewhat tricky since most other work is in head-fixed animals, but it would at least give the reader a sense as to how the measures here compare to others in the field.</p></disp-quote><p>We appreciate the need to provide benchmarks for validating our system, and we now include assessment of the accuracy of the resulting measures as discussed below in specific points raised by the reviewers. In particular, we provide estimates of error in measurement of eye and head positions, as well as camera jitter, in the Materials and methods. These factors are all &lt;1deg, similar to other recent studies in freely moving rodent vision. This also supports the robustness of results presented, though it represents a limitation in the smallest movements we might detect, such as micro-saccades. Furthermore, these benchmarks provide a direct measurement of the performance of the system and do not require comparison to head-fixed recordings such as for VOR gain. Finally, we note that the basic parameters of eye movements, amplitude and velocity, agree with recent comparable studies in freely moving animals.</p><disp-quote content-type="editor-comment"><p>3) Along the same lines, more plots of eye velocity and not just position (e.g., in Figure 1) would be helpful. I recognize that with a 30Hz camera sampling rate, the velocity traces will be noisy; nonetheless, it would help give readers a sense of how the measurements here square up with others. Because this system will likely be used by other groups, due to its many advantages, having a sense of any pitfalls will be helpful.</p></disp-quote><p>We have now included eye velocity traces in Figures 1, 4, and 5. These are not as noisy as one might have expected, particularly with 60Hz resolution, and certainly do help demonstrate the pattern of eye movements more clearly.</p><disp-quote content-type="editor-comment"><p>Other methodological questions:</p><p>4) The authors need more explanation about the GMM clustering and how they divided compensatory eye movements and saccades in the Materials and methods section. In Figure 4A, it seems some red dots and blue dots are mixed with each other.</p></disp-quote><p>Based on the concern about the clarity of the GMM clustering, as well as the improved temporal resolution resulting from 60Hz video data, we have simplified our segregation of compensatory and non-compensatory eye movements. We now use a threshold on gaze velocity, similar to that frequently applied to eye velocity in primate studies, which is supported by the distribution of eye/head movements in Figures 4A,B.</p><disp-quote content-type="editor-comment"><p>Terminology / Discussion:</p><p>5) Please provide clarification regarding the pursuit terminology. Whereas the term &quot;pursuit&quot; might be standard in interception and behavior studies of predator/prey interactions, with many decades of use, there is some ambiguity with smooth pursuit eye movements – slow, continuous rotations of the eyes to keep small moving objects close to the fovea. Reviewer #2 suggests additional editing and clear definitions at the beginning of the manuscript (e.g. prey pursuit vs smooth pursuit).</p></disp-quote><p>We recognize that we were unclear in using the terms pursuit and approach interchangeably, particularly given the potential confusion with “smooth pursuit”. We now use the term approach throughout.</p><disp-quote content-type="editor-comment"><p>6) Discussion: Reviewer #3 was surprised that the authors described mice as, &quot;not a highly visual species.&quot; Their own work highlights that mice use vision for prey capture. Further, as the authors, know, mice have multiple (~7) independent maps of visual space in the brain. Why would the brain go to the trouble of generating all these maps if mice didn't make much use their visual systems? Finally, describing an animal as &quot;highly visual&quot; or &quot;not highly visual&quot; ignores the fact that all animals use multiple sensory systems to navigate the world, and tradeoff which sensory modality is weighted most heavily in a dynamic way (e.g., relying on vision during daytime hunting/foraging and somatosensation at night).</p></disp-quote><p>We agree completely, as the reviewer infers from our previous work. We simply meant to refer to the lower visual acuity relative to some other species that are commonly studied in visual neuroscience. We have now removed this phrasing.</p></body></sub-article></article>