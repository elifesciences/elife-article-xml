<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">72056</article-id><article-id pub-id-type="doi">10.7554/eLife.72056</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Parallel processing in speech perception with local and global representations of linguistic context</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-177109"><name><surname>Brodbeck</surname><given-names>Christian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8380-639X</contrib-id><email>christianbrodbeck@me.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-246285"><name><surname>Bhattasali</surname><given-names>Shohini</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6767-6529</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-246286"><name><surname>Cruz Heredia</surname><given-names>Aura AL</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-246287"><name><surname>Resnik</surname><given-names>Philip</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6130-8602</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-23965"><name><surname>Simon</surname><given-names>Jonathan Z</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0858-0698</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-102527"><name><surname>Lau</surname><given-names>Ellen</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02der9h97</institution-id><institution>Department of Psychological Sciences, University of Connecticut</institution></institution-wrap><addr-line><named-content content-type="city">Storrs</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>Institute for Systems Research, University of Maryland</institution></institution-wrap><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>Department of Linguistics, University of Maryland</institution></institution-wrap><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>Institute for Advanced Computer Studies, University of Maryland</institution></institution-wrap><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>Department of Psychology, University of Pennsylvania</institution></institution-wrap><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>Department of Electrical and Computer Engineering, University of Maryland</institution></institution-wrap><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/047s2c258</institution-id><institution>Department of Biology, University of Maryland</institution></institution-wrap><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>van Wassenhove</surname><given-names>Virginie</given-names></name><role>Reviewing Editor</role><aff><institution>CEA, DRF/I2BM, NeuroSpin; INSERM, U992, Cognitive Neuroimaging Unit</institution><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution>Carnegie Mellon University</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>21</day><month>01</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e72056</elocation-id><history><date date-type="received" iso-8601-date="2021-07-08"><day>08</day><month>07</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-01-16"><day>16</day><month>01</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-07-03"><day>03</day><month>07</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.07.03.450698"/></event></pub-history><permissions><copyright-statement>© 2022, Brodbeck et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Brodbeck et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-72056-v2.pdf"/><abstract><p>Speech processing is highly incremental. It is widely accepted that human listeners continuously use the linguistic context to anticipate upcoming concepts, words, and phonemes. However, previous evidence supports two seemingly contradictory models of how a predictive context is integrated with the bottom-up sensory input: Classic psycholinguistic paradigms suggest a two-stage process, in which acoustic input initially leads to local, context-independent representations, which are then quickly integrated with contextual constraints. This contrasts with the view that the brain constructs a single coherent, unified interpretation of the input, which fully integrates available information across representational hierarchies, and thus uses contextual constraints to modulate even the earliest sensory representations. To distinguish these hypotheses, we tested magnetoencephalography responses to continuous narrative speech for signatures of local and unified predictive models. Results provide evidence that listeners employ both types of models in parallel. Two local context models uniquely predict some part of early neural responses, one based on sublexical phoneme sequences, and one based on the phonemes in the current word alone; at the same time, even early responses to phonemes also reflect a unified model that incorporates sentence-level constraints to predict upcoming phonemes. Neural source localization places the anatomical origins of the different predictive models in nonidentical parts of the superior temporal lobes bilaterally, with the right hemisphere showing a relative preference for more local models. These results suggest that speech processing recruits both local and unified predictive models in parallel, reconciling previous disparate findings. Parallel models might make the perceptual system more robust, facilitate processing of unexpected inputs, and serve a function in language acquisition.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>temporal response functions</kwd><kwd>entropy</kwd><kwd>surprisal</kwd><kwd>MEG</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100008510</institution-id><institution>University of Maryland</institution></institution-wrap></funding-source><award-id>BBI Seed Grant</award-id><principal-award-recipient><name><surname>Simon</surname><given-names>Jonathan Z</given-names></name><name><surname>Lau</surname><given-names>Ellen</given-names></name><name><surname>Brodbeck</surname><given-names>Christian</given-names></name><name><surname>Cruz Heredia</surname><given-names>Aura AL</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>BCS-1749407</award-id><principal-award-recipient><name><surname>Lau</surname><given-names>Ellen</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01DC014085</award-id><principal-award-recipient><name><surname>Simon</surname><given-names>Jonathan Z</given-names></name><name><surname>Brodbeck</surname><given-names>Christian</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>SMA-1734892</award-id><principal-award-recipient><name><surname>Simon</surname><given-names>Jonathan Z</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>MURI Award N00014-18-1-2670</award-id><principal-award-recipient><name><surname>Resnik</surname><given-names>Philip</given-names></name><name><surname>Bhattasali</surname><given-names>Shohini</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008982</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>BCS-1754284</award-id><principal-award-recipient><name><surname>Brodbeck</surname><given-names>Christian</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Speech processing engages multiple predictive models, using sublexical, word- and sentence contexts in parallel to anticipate upcoming phonemes.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Acoustic events in continuous speech occur at a rapid pace, and listeners face pressure to process the speech signal rapidly and incrementally (<xref ref-type="bibr" rid="bib19">Christiansen and Chater, 2016</xref>). One strategy that listeners employ to achieve this is to organize internal representations in such a way as to minimize the processing cost of future language input (<xref ref-type="bibr" rid="bib36">Ferreira and Chantavarin, 2018</xref>). Different accounts have been proposed for how listeners do this, many centered on the notion that they actively predict future input (<xref ref-type="bibr" rid="bib43">Gagnepain et al., 2012</xref>), for instance using internalized generative models (<xref ref-type="bibr" rid="bib54">Halle and Stevens, 1962</xref>). Such predictive strategies manifest in a variety of measures that suggest that more predictable words are easier to process (<xref ref-type="bibr" rid="bib52">Hale, 2003</xref>; <xref ref-type="bibr" rid="bib70">Levy, 2008</xref>; <xref ref-type="bibr" rid="bib102">Smith and Levy, 2013</xref>). For instance, spoken words are recognized more quickly when they are heard in a meaningful context (<xref ref-type="bibr" rid="bib76">Marslen-Wilson and Tyler, 1975</xref>), and words that are made more likely by the context are associated with reduced neural responses, compared to less expected words (<xref ref-type="bibr" rid="bib62">Holcomb and Neville, 2013</xref>; <xref ref-type="bibr" rid="bib21">Connolly and Phillips, 1994</xref>; <xref ref-type="bibr" rid="bib111">Van Petten et al., 1999</xref>; <xref ref-type="bibr" rid="bib28">Diaz and Swaab, 2007</xref>; <xref ref-type="bibr" rid="bib14">Broderick et al., 2018</xref>). This contextual facilitation is pervasive, and is sensitive to language statistics (<xref ref-type="bibr" rid="bib116">Willems et al., 2016</xref>; <xref ref-type="bibr" rid="bib115">Weissbart et al., 2020</xref>; <xref ref-type="bibr" rid="bib99">Schmitt et al., 2020</xref>), as well as the discourse level meaning of the language input for the listeners (<xref ref-type="bibr" rid="bib109">van Berkum et al., 2003</xref>; <xref ref-type="bibr" rid="bib85">Nieuwland and Van Berkum, 2006</xref>).</p><p>In speech, words are often predictable because they occur in sequences that form meaningful messages. Similarly, phonemes are predictable because they occur in sequences that form words. For example, after hearing the beginning /ɹıv/, /ɝ/ would be a likely continuation forming <italic>river</italic>; /i/ would be more surprising, because <italic>riviera</italic> is a less frequent word, whereas /ʊ/ would be highly surprising because there are no common English words starting with that sequence. Phonemes that are thus inconsistent with known word forms elicit an electrophysiological mismatch response (<xref ref-type="bibr" rid="bib43">Gagnepain et al., 2012</xref>), and responses to valid phonemes are proportionately larger the more surprising the phonemes are (<xref ref-type="bibr" rid="bib30">Ettinger et al., 2014</xref>; <xref ref-type="bibr" rid="bib51">Gwilliams and Marantz, 2015</xref>; <xref ref-type="bibr" rid="bib45">Gaston and Marantz, 2017</xref>). Predictive processing is not restricted to linguistic representations, as even responses to acoustic features in early auditory cortex reflect expectations based on the acoustic context (<xref ref-type="bibr" rid="bib100">Singer et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Forseth et al., 2020</xref>).</p><p>Thus, there is little doubt that the brain uses context to facilitate processing of upcoming input, at multiple levels of representation. Here, we investigate a fundamental question about the underlying cognitive organization: Does the brain develop a single, unified representation of the input? In other words, one representation that is consistent across hierarchical levels, effectively propagating information from the sentence context across hierarchical levels to anticipate even low-level features of the sensory input such as phonemes? Or do cognitive subsystems differ in the extent and kind of context they use to interpret their input? This question has appeared in different forms, for example in early debates about whether sensory systems are modular (<xref ref-type="bibr" rid="bib38">Fodor, 1985</xref>), or whether sensory input and contextual constraints are combined immediately in speech perception (<xref ref-type="bibr" rid="bib76">Marslen-Wilson and Tyler, 1975</xref>; <xref ref-type="bibr" rid="bib106">Tanenhaus et al., 1995</xref>). A similar distinction has also surfaced more recently between the local and global architectures of predictive coding (<xref ref-type="bibr" rid="bib105">Tabas and Kriegstein, 2021</xref>).</p><p>A strong argument for a unified, globally consistent model comes from Bayesian frameworks, which suggest that, for optimal interpretation of imperfect sensory signals, listeners ought to use the maximum amount of information available to them to compute a prior expectation for upcoming sensory input (<xref ref-type="bibr" rid="bib66">Jurafsky, 1996</xref>; <xref ref-type="bibr" rid="bib90">Norris and McQueen, 2008</xref>). An implication is that speech processing is truly incremental, with a unified linguistic representation that is updated at the phoneme (or an even lower) time scale (<xref ref-type="bibr" rid="bib102">Smith and Levy, 2013</xref>). Such a unified representation is consistent with empirical evidence for top-down modulation of sensory representations, for example, suggesting that recognizing a word can bias subsequent phonetic representations (<xref ref-type="bibr" rid="bib73">Luthra et al., 2021</xref>), that listeners weight cues like a Bayes-optimal observer during speech perception (<xref ref-type="bibr" rid="bib5">Bejjanki et al., 2011</xref>; <xref ref-type="bibr" rid="bib35">Feldman et al., 2009</xref>), and that they immediately interpret incoming speech with regard to communicative goals (<xref ref-type="bibr" rid="bib18">Chambers et al., 2004</xref>; <xref ref-type="bibr" rid="bib59">Heller et al., 2016</xref>). A recent implementation proposed for such a model is the global variant of hierarchical predictive coding, which assumes a cascade of generative models predicting sensory input from higher-level expectations (<xref ref-type="bibr" rid="bib20">Clark, 2013</xref>; <xref ref-type="bibr" rid="bib41">Friston, 2010</xref>; <xref ref-type="bibr" rid="bib105">Tabas and Kriegstein, 2021</xref>). A unified model is also assumed by classical interactive models of speech processing, which rely on cross-hierarchy interactions to generate a globally consistent interpretation of the input (<xref ref-type="bibr" rid="bib81">McClelland and Rumelhart, 1981</xref>; <xref ref-type="bibr" rid="bib82">McClelland and Elman, 1986</xref>; <xref ref-type="bibr" rid="bib75">Magnuson et al., 2018</xref>).</p><p>However, there is also evidence for incomplete use of context in speech perception. Results from cross-modal semantic priming suggest that, during perception of a word, initially multiple meanings are activated regardless of whether they are consistent with the sentence context or not, and contextually appropriate meanings only come to dominate at a later stage (<xref ref-type="bibr" rid="bib104">Swinney, 1979</xref>; <xref ref-type="bibr" rid="bib119">Zwitserlood, 1989</xref>). Similarly, listeners’ eye movements suggest that they initially consider word meanings that are impossible given the syntactic context (<xref ref-type="bibr" rid="bib46">Gaston et al., 2020</xref>). Such findings can be interpreted as evidence for a two-stage model of word recognition, in which an earlier retrieval process operates without taking into account the wider sentence context, and only a secondary process of selection determines the best fit with context (<xref ref-type="bibr" rid="bib1">Altmann and Steedman, 1988</xref>). Similarly, at the sublexical level, experiments with nonwords suggest that phoneme sequence probabilities can have effects that are decoupled from the word recognition process (<xref ref-type="bibr" rid="bib112">Vitevitch and Luce, 1999</xref>; <xref ref-type="bibr" rid="bib113">Vitevitch and Luce, 2016</xref>). However, it is also possible that such effects occur only due to the unnaturalness of experimental tasks. For example, in the cross-modal priming task, listeners might come to expect a visual target which is not subject to sentence context constraints, and thus change their reliance on that context.</p><p>Finally, a third possibility is that a unified model coexists with more local models of context, and that they operate in a parallel fashion. For example, it has been suggested that the two hemispheres differ with respect to their use of context, with the left hemisphere relying heavily on top-down predictions, and the right hemisphere processing language in a more bottom-up manner (<xref ref-type="bibr" rid="bib33">Federmeier, 2007</xref>).</p><p>Distinguishing among these possibilities requires a task that encourages naturalistic engagement with the context, and a nonintrusive measure of linguistic processing. To achieve this, we analyzed magnetoencephalography (MEG) responses to continuous narrative speech, using methods that have previously shown electrophysiological brain responses related to predictive language models. Previous work, however, has tested either only for a local, or only for a unified context model, by either using only the current word up to the current phoneme as context (<xref ref-type="bibr" rid="bib9">Brodbeck et al., 2018a</xref>; <xref ref-type="bibr" rid="bib48">Gillis et al., 2021</xref>) or by using predictions from a complete history of phonemes and words (<xref ref-type="bibr" rid="bib29">Donhauser and Baillet, 2020</xref>). Because these two context models include overlapping sets of constraints, their predictions for neural responses are correlated, and thus need to be assessed jointly. Furthermore, some architectures predict that both kinds of context model should affect brain responses separately. For example, a two-stage architecture predicts an earlier stage of lexical processing that is sensitive to lexical statistics only, and a later stage that is sensitive to the global sentence context. Here, we directly test such possibilities by comparing the ability of different context models to jointly predict brain responses.</p><sec id="s1-1"><title>Expressing the use of context through information theory</title><p>The sensitivity of speech processing to different definitions of context is formalized through conditional probability distributions (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Each distribution reflects an interpretation of ongoing speech input, at a given level of representation. We here use word forms and phonemes as units of representation (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), and all our predictors reflect information-theoretic quantities at the rate of phonemes; however, this is a matter of methodological convenience, and similar models could be formulated using a different granularity (<xref ref-type="bibr" rid="bib102">Smith and Levy, 2013</xref>). <xref ref-type="fig" rid="fig1">Figure 1B</xref> shows an architecture in which each level uses local information from that level, but information from higher levels does not affect beliefs at lower levels. In this architecture, phonemes are classified at the sublexical level based on the acoustic input and possibly a local phoneme history. The word level decodes the current word from the incoming phonemes, but without access to the multiword context. Finally, the sentence-level updates the sentence representation from the incoming word candidates, and thus selects those candidates that are consistent with the sentence context. In such a model, apparent top-down effects such as perceptual restoration of noisy input (<xref ref-type="bibr" rid="bib44">Ganong, 1980</xref>; <xref ref-type="bibr" rid="bib69">Leonard et al., 2016</xref>) are generated at higher-level decision stages rather than at the initial perceptual representations (<xref ref-type="bibr" rid="bib88">Norris, 1994</xref>). In contrast, <xref ref-type="fig" rid="fig1">Figure 1C</xref> illustrates the hypothesis of a unified, global context model, in which priors at lower levels take advantage of information available at the higher levels. Here, the sentence context is used in decoding the current word by directly altering the prior expectation of the word candidates, and this sentence-appropriate expectation is in turn used to alter expectations for upcoming phonemes.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Information flow in local and unified architectures for speech processing.</title><p>(<bold>A</bold>) Schematic characterization of the linguistic units used to characterize speech. The same phoneme can be invoked as part of a sublexical phoneme sequence, <italic>ph<sub>k</sub></italic>, or as part of <italic>word<sub>j</sub></italic>, <italic>ph<sub>j</sub></italic><sub>,</sub><italic><sub>i</sub></italic>. (<bold>B</bold>) Each box stands for a level of representation, characterized by its output and a probability distribution describing the level’s use of context. For example, the sublexical level’s output is an estimate of the current phoneme, <italic>ph<sub>k</sub></italic>, and the distribution for <italic>ph<sub>k</sub></italic> is estimated as probability for different phonemes based on the sound input and a sublexical phoneme history. At the sentence level, <italic>sentence<sub>j</sub></italic><sub>,</sub><italic><sub>i</sub></italic> stands for a temporary representation of the sentence at time <italic>j</italic>,<italic>i</italic>. Boxes represent functional organization rather than specific brain regions. Arrows reflect the flow of information: each level of representation is updated incrementally, combining information from the same level at the previous time step (horizontal arrows) and the level below (bottom-up arrows). (<bold>C</bold>) The unified architecture implements a unified, global context model through information flowing down the hierarchy, such that expectations at lower levels incorporate information accumulated at the sentence level. Relevant differences from the local context model are in red. Note that while the arrows only cross one level at a time, the information is propagated in steps and eventually crosses all levels.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72056-fig1-v2.tif"/></fig><p>These hypotheses make different predictions for brain responses sensitive to language statistics. Probabilistic speech representations, as in <xref ref-type="fig" rid="fig1">Figure 1</xref>, are linked to brain activity through information-theoretic complexity metrics (<xref ref-type="bibr" rid="bib53">Hale, 2016</xref>). The most common linking variable is <italic>surprisal</italic>, which is equivalent to the difficulty incurred in updating an incremental representation of the input (<xref ref-type="bibr" rid="bib70">Levy, 2008</xref>). Formally, the surprisal experienced at phoneme <italic>k</italic> is inversely related to the likelihood of that phoneme in its context:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A second information-theoretic measure that has been found to independently predict brain activity is entropy (<xref ref-type="bibr" rid="bib9">Brodbeck et al., 2018a</xref>; <xref ref-type="bibr" rid="bib29">Donhauser and Baillet, 2020</xref>), a measure of the uncertainty in a probability distribution. Phoneme entropy is defined as the expected (average) surprisal for the next phoneme:<disp-formula id="equ2"><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In contrast to surprisal, which is a function of the expected probability of the current event only, entropy is a function of the whole distribution of expectations. This makes it possible to distinguish between phoneme entropy, the uncertainty about the next phoneme, and cohort entropy, the uncertainty about the complete word form that matches the current partial input (for more details see <italic>Lexical context model</italic> in <italic>Methods</italic>):<disp-formula id="equ3"><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Entropy might relate to neuronal processes in at least two ways. First, the amount of uncertainty might reflect the amount of competition among different representations, which might play out through a neural process such as lateral inhibition (<xref ref-type="bibr" rid="bib82">McClelland and Elman, 1986</xref>). Second, uncertainty might also be associated with increased sensitivity to bottom-up input, because the input is expected to be more informative (<xref ref-type="bibr" rid="bib64">Jaramillo and Zador, 2011</xref>; <xref ref-type="bibr" rid="bib3">Auksztulewicz et al., 2019</xref>).</p></sec><sec id="s1-2"><title>Models for responses to continuous speech</title><p>To test how context is used in continuous speech processing, we compared the ability of three different context models to predict MEG responses, corresponding to the three levels in <xref ref-type="fig" rid="fig1">Figure 1B</xref> (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). The context models all incrementally estimate a probability distribution at each phoneme position, but they differ in the amount and kind of context they incorporate. Throughout, we used <italic>n</italic>-gram models to estimate sequential dependencies because they are powerful language models that can capture effects of language statistics in a transparent manner, with minimal assumptions about the underlying cognitive architecture (<xref ref-type="bibr" rid="bib42">Futrell et al., 2020</xref>; <xref ref-type="bibr" rid="bib70">Levy, 2008</xref>; <xref ref-type="bibr" rid="bib102">Smith and Levy, 2013</xref>). An example of the complete set of predictors is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Models for predictive speech processing based on the sentence, word, and sublexical context, used to predict magnetoencephalography (MEG) data.</title><p>(<bold>A</bold>) Example of word-by-word surprisal. The sentence (5 gram) context generally leads to a reduction of word surprisal, but the magnitude of the reduction differs substantially between words (across all stimuli, mean ± standard deviation, unigram surprisal: 10.76 ± 5.15; 5 gram surprisal: 7.43 ± 5.98; <italic>t</italic><sub>8172</sub> = 76.63, p &lt; 0.001). (<bold>B</bold>) Sentence-level predictions propagate to phoneme surprisal, but not in a linear fashion. For example, in the word <italic>happened</italic>, the phoneme surprisal based on all three models is relatively low for the second phoneme /æ/ due to the high likelihood of word candidates like <italic>have</italic> and <italic>had</italic>. However, the next phoneme is /p/ and phoneme surprisal is high across all three models. On the other hand, for words like <italic>find</italic>, <italic>on</italic>, and <italic>Ohio</italic>, the sentence-constrained phoneme surprisal is disproportionately low for subsequent phonemes, reflecting successful combination of the sentence constraint with the first phoneme. (<bold>C</bold>) Phoneme-by-phoneme estimates of information processing demands, based on different context models, were used to predict MEG responses through multivariate temporal response functions (mTRFs) (<xref ref-type="bibr" rid="bib10">Brodbeck et al., 2018b</xref>). An mTRF consists of multiple TRFs estimated jointly such that each predictor, convolved with the corresponding TRF, predicts a partial response, and the pointwise sum of partial responses constitutes the predicted MEG response. The dependent measure (measured response) was the fixed orientation, distributed minimum norm source current estimate of the continuous MEG response. The blue arrow illustrates a single virtual current source dipole. Estimated signals at current dipoles across the brain were analyzed using a mass-univariate approach. See Methods for details. TRFs: temporal response functions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72056-fig2-v2.tif"/></fig><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Stimulus excerpt with all 26 predictors used to model brain responses.</title><p>From the top: phoneme-level information-theoretic variables, based on different context definitions: sentence, word, and sublexical context; intercepts for word- and phoneme-related brain activity, that is, a predictor to control for brain activity that does not scale with the variables under consideration; and an auditory processing model, consisting of an acoustic spectrogram (sound envelopes) and an onset spectrogram (sound onsets), each represented by eight predictors for eight frequency bands.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72056-fig3-v2.tif"/></fig><sec id="s1-2-1"><title>Sublexical context model</title><p>A 5 gram model estimates the prior probability for the next phoneme given the four preceding phonemes. This model reflects simple phoneme sequence statistics (<xref ref-type="bibr" rid="bib112">Vitevitch and Luce, 1999</xref>; <xref ref-type="bibr" rid="bib113">Vitevitch and Luce, 2016</xref>) and is unaware of word boundaries. Such a model is thought to play an important role in language acquisition (<xref ref-type="bibr" rid="bib16">Cairns et al., 1997</xref>; <xref ref-type="bibr" rid="bib17">Chambers et al., 2003</xref>; <xref ref-type="bibr" rid="bib97">Saffran et al., 1996</xref>), but it is unknown whether it has a functional role in adult speech processing. The sublexical model predicted brain responses via the phoneme surprisal and entropy linking variables.</p></sec><sec id="s1-2-2"><title>Word context model</title><p>This model implements the cohort model of word perception (<xref ref-type="bibr" rid="bib77">Marslen-Wilson, 1987</xref>), applied to each word in isolation. The first phoneme of the word generates a probability distribution over the lexicon, including all words starting with the given phoneme, and each word’s probability proportional to the word’s relative unigram frequency. Each subsequent phoneme trims this distribution by removing words that are inconsistent with that phoneme. Like the sublexical model, the lexical model can be used as a predictive model for upcoming phonemes, yielding phoneme surprisal and entropy variables. In addition, the lexical model generates a probability distribution over the lexicon, which yields a cohort entropy variable.</p></sec><sec id="s1-2-3"><title>Sentence context model</title><p>The sentence model is closely related to the word context model, but each word’s prior probability is estimated from a lexical 5 gram model. While a 5 gram model misses longer-range linguistic dependencies, we use it here as a conservative approximation of sentence-level linguistic and interpretive constraints (<xref ref-type="bibr" rid="bib102">Smith and Levy, 2013</xref>). The sentence model implements cross-hierarchy predictions by using the sentence context in concert with the partial current word to predict upcoming phonemes. Brain activity is predicted from the same three variables as from the word context model.</p><p>We evaluated these different context models in terms of their ability to explain held-out MEG responses, and the latency of the brain responses associated with each model. An architecture based on local context models, as in <xref ref-type="fig" rid="fig1">Figure 1B</xref>, predicts a temporal sequence of responses as information passes up the hierarchy, with earlier responses reflecting lower order context models. In contrast, a unified architecture, as in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, predicts that the sentence context model should exhaustively explain brain responses, because all representational levels use priors derived from the sentence context. Finally, architectures that entail multiple kinds of models predict that different context models might explain response components, possibly in different anatomical areas.</p></sec><sec id="s1-2-4"><title>Acoustic controls</title><p>In order to dissociate effects of linguistic processing from responses to acoustic properties of the speech stimulus, all models controlled for a gammatone spectrogram and an acoustic onset spectrogram (<xref ref-type="bibr" rid="bib11">Brodbeck et al., 2020</xref>), as well as word onsets and phoneme onsets (<xref ref-type="bibr" rid="bib9">Brodbeck et al., 2018a</xref>).</p></sec></sec></sec><sec id="s2" sec-type="results"><title>Results</title><p>Twelve participants listened to ~47 min of a nonfiction audiobook. Multivariate temporal response functions (mTRFs) were used to jointly predict held-out, source localized MEG responses (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). To test whether each context model is represented neurally, the predictive power of the full model including all predictors was compared with the predictive power of a model that was estimated without the predictor variables belonging to this specific context model. Besides 11 right-handers, our sample included a single left-hander. While this participant’s brain responses were more right-lateralized than average, excluding them did not change the conclusions from any of the reported lateralization significance tests. We thus report results from the total sample, but identify the left-hander in plots and source data.</p><sec id="s2-1"><title>Phoneme-, word-, and sentence-constrained models coexist in the brain</title><p>Each context model significantly improves the prediction of held-out data, even after controlling for acoustic features and the other two context models (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Each of the three context models’ source localization is consistent with sources in the superior temporal gyrus (STG), thought to support phonetic and phonological processing (<xref ref-type="bibr" rid="bib83">Mesgarani et al., 2014</xref>). In addition, the sentence-constrained model also extends to more ventral parts of the temporal lobe, consistent with higher-level language processing (<xref ref-type="bibr" rid="bib60">Hickok and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib117">Wilson et al., 2018</xref>). In comparison, the predictive power of the acoustic features is highest in closer vicinity of Heschl’s gyrus (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). At each level of context, surprisal and entropy contribute about equally to the model’s predictive power (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, <xref ref-type="table" rid="table1">Table 1</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>All context models significantly contribute to predictions of brain responses.</title><p>(<bold>A</bold>) Each context model significantly improves predictions of held-out magnetoencephalography (MEG) data in both hemispheres (<italic>t</italic><sub>max</sub> ≥ 6.16, p ≤ 0.005). Black bars below anatomical plots indicate a significant difference between hemispheres. The white outline indicates a region of interest (ROI) used for measures shown in (<bold>B</bold>), (<bold>C</bold>), and (<bold>E</bold>). Brain regions excluded from analysis are darkened (occipital lobe and insula). (<bold>B</bold>) Surprisal and entropy have similar predictive power in each context model. Each dot represents the difference in predictive power between the full and a reduced model for one subject, averaged in the ROI. Cohort- and phoneme entropy are combined here because the predictors are highly correlated and hence share a large portion of their explanatory power. Corresponding statistics and effect size are given in <xref ref-type="table" rid="table1">Table 1</xref>. A single left-handed participant is highlighted throughout with an unfilled circle. LH: left hemisphere; RH: right hemisphere. (<bold>C</bold>) Even when tested individually, excluding variability that is shared between the two, cohort- and phoneme entropy at each level significantly improve predictions. A significant effect of sentence-constrained phoneme entropy is evidence for cross-hierarchy integration, as it suggests that sentence-level information is used to predict upcoming phonemes. (<bold>D</bold>) Predictive power of the acoustic feature representations. (<bold>E</bold>) The lateralization index <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates that the sublexical context model is more right-lateralized than the sentence context model. Left: LI = 0; right: LI = 1. Significance levels: *p ≤ 0.05; **p ≤ 0.01; ***p ≤ 0.001.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Mass-univariate statistics results for Panels A &amp; D.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-72056-fig4-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata2"><label>Figure 4—source data 2.</label><caption><title>Predictive power in the mid/posterior superior temporal gyrus ROI, data used in Panels B, C &amp; E.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-72056-fig4-data2-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72056-fig4-v2.tif"/></fig><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Predictive power in mid/posterior superior temporal gyrus region of interest for individual predictors.</title><p>One-tailed <italic>t</italic>-tests and Cohen’s <italic>d</italic> for the predictive power uniquely attributable to the respective predictors. Data underlying these values are the same as for the swarm plots in <xref ref-type="fig" rid="fig4">Figure 4B, C</xref> (<xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref>). *p ≤ 0.05; **p ≤ 0.01; ***p ≤ 0.001.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" colspan="4" valign="top">Left hemisphere</th><th align="left" valign="top"/><th align="left" colspan="4" valign="top">Right hemisphere</th></tr></thead><tbody><tr><td align="left" valign="top"/><td align="left" valign="top">∆‰</td><td align="left" valign="top"><italic>t</italic>(11)</td><td align="left" valign="top">p</td><td align="left" valign="top"><italic>d</italic></td><td align="left" valign="top"/><td align="left" valign="top">∆‰</td><td align="left" valign="top"><italic>t</italic>(11)</td><td align="left" valign="top">p</td><td align="left" valign="top"><italic>d</italic></td></tr><tr><td align="left" valign="top"><bold>Sentence context</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Surprisal</td><td align="char" char="." valign="top">3.77</td><td align="char" char="." valign="top">4.40**</td><td align="char" char="." valign="top">0.001</td><td align="char" char="." valign="top">1.27</td><td align="left" valign="top"/><td align="char" char="." valign="top">5.51</td><td align="char" char="." valign="top">4.14**</td><td align="char" char="." valign="top">0.002</td><td align="char" char="." valign="top">1.19</td></tr><tr><td align="left" valign="top">Entropy</td><td align="char" char="." valign="top">3.40</td><td align="char" char="." valign="top">5.96***</td><td align="char" char="." valign="top">&lt;0.001</td><td align="char" char="." valign="top">1.72</td><td align="left" valign="top"/><td align="char" char="." valign="top">1.94</td><td align="char" char="." valign="top">4.11**</td><td align="char" char="." valign="top">0.002</td><td align="char" char="." valign="top">1.19</td></tr><tr><td align="left" valign="top"> Cohort</td><td align="char" char="." valign="top">0.83</td><td align="char" char="." valign="top">3.41**</td><td align="char" char="." valign="top">0.006</td><td align="char" char="." valign="top">0.98</td><td align="left" valign="top"/><td align="char" char="." valign="top">0.39</td><td align="char" char="." valign="top">2.45*</td><td align="char" char="." valign="top">0.032</td><td align="char" char="." valign="top">0.71</td></tr><tr><td align="left" valign="top"> Phoneme</td><td align="char" char="." valign="top">0.85</td><td align="char" char="." valign="top">5.18***</td><td align="char" char="." valign="top">&lt;0.001</td><td align="char" char="." valign="top">1.50</td><td align="left" valign="top"/><td align="char" char="." valign="top">0.79</td><td align="char" char="." valign="top">3.85**</td><td align="char" char="." valign="top">0.003</td><td align="char" char="." valign="top">1.11</td></tr><tr><td align="left" valign="top"><bold>Word context</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Surprisal</td><td align="char" char="." valign="top">0.78</td><td align="char" char="." valign="top">3.62**</td><td align="char" char="." valign="top">0.004</td><td align="char" char="." valign="top">1.04</td><td align="left" valign="top"/><td align="char" char="." valign="top">1.71</td><td align="char" char="." valign="top">5.76***</td><td align="char" char="." valign="top">&lt;0.001</td><td align="char" char="." valign="top">1.66</td></tr><tr><td align="left" valign="top">Entropy</td><td align="char" char="." valign="top">1.26</td><td align="char" char="." valign="top">4.43**</td><td align="char" char="." valign="top">0.001</td><td align="char" char="." valign="top">1.28</td><td align="left" valign="top"/><td align="char" char="." valign="top">1.31</td><td align="char" char="." valign="top">4.39**</td><td align="char" char="." valign="top">0.001</td><td align="char" char="." valign="top">1.27</td></tr><tr><td align="left" valign="top"> Cohort</td><td align="char" char="." valign="top">0.25</td><td align="char" char="." valign="top">3.29**</td><td align="char" char="." valign="top">0.007</td><td align="char" char="." valign="top">0.95</td><td align="left" valign="top"/><td align="char" char="." valign="top">0.36</td><td align="char" char="." valign="top">3.99**</td><td align="char" char="." valign="top">0.002</td><td align="char" char="." valign="top">1.15</td></tr><tr><td align="left" valign="top"> Phoneme</td><td align="char" char="." valign="top">0.51</td><td align="char" char="." valign="top">4.59***</td><td align="char" char="." valign="top">&lt;0.001</td><td align="char" char="." valign="top">1.32</td><td align="left" valign="top"/><td align="char" char="." valign="top">0.66</td><td align="char" char="." valign="top">3.61**</td><td align="char" char="." valign="top">0.004</td><td align="char" char="." valign="top">1.04</td></tr><tr><td align="left" valign="top"><bold>Sublexical context</bold></td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Surprisal</td><td align="char" char="." valign="top">0.64</td><td align="char" char="." valign="top">4.88***</td><td align="char" char="." valign="top">&lt;0.001</td><td align="char" char="." valign="top">1.41</td><td align="left" valign="top"/><td align="char" char="." valign="top">1.29</td><td align="char" char="." valign="top">4.59***</td><td align="char" char="." valign="top">&lt;0.001</td><td align="char" char="." valign="top">1.33</td></tr><tr><td align="left" valign="top">Entropy</td><td align="char" char="." valign="top">0.66</td><td align="char" char="." valign="top">2.53*</td><td align="char" char="." valign="top">0.028</td><td align="char" char="." valign="top">0.73</td><td align="left" valign="top"/><td align="char" char="." valign="top">1.91</td><td align="char" char="." valign="top">5.32***</td><td align="char" char="." valign="top">&lt;0.001</td><td align="char" char="." valign="top">1.54</td></tr></tbody></table></table-wrap><p>Overall, the acoustic features explain more of the variability in brain responses than the linguistic features (compare scales in <xref ref-type="fig" rid="fig4">Figure 4A–D</xref>). This is likely because speech is an acoustically rich stimulus, driving many kinds of auditory receptive fields. In contrast, the linguistic predictors represent very specific computations, likely represented in a small and specialized neural territory. For the present purpose, what is critical is the consistency of the linguistic effects across subjects: <xref ref-type="fig" rid="fig4">Figure 4B,C</xref>, as well as the effect sized shown in <xref ref-type="table" rid="table1">Table 1</xref> suggests that the linguistic modulation of brain responses can be detected very reliably across subjects.</p><p>The significant predictive power of the local context models is inconsistent with the hypothesis of a single, unified context model (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Instead, it suggests that different neural representations incorporate different kinds of context. We next pursued the question of how these different representations are organized hierarchically. Phoneme surprisal depends on the conditional probability of the current phoneme, and thus does not distinguish between whether what is predicted is a single phoneme or the whole lexical completion (<xref ref-type="bibr" rid="bib70">Levy, 2008</xref>; <xref ref-type="bibr" rid="bib102">Smith and Levy, 2013</xref>). Entropy, on the other hand, depends on the units over which probabilities are calculated, and can thus potentially distinguish between whether brain responses reflect uncertainty over the next phoneme alone, or uncertainty over the word currently being perceived, that is, over the lexical completion (see <italic>Lexical context model</italic> in <italic>Methods</italic>). This distinction is particularly interesting for the sentence context model: if predictions are constrained to using context <italic>within</italic> a hierarchical level, as in <xref ref-type="fig" rid="fig1">Figure 1B</xref>, then the sentence context should affect uncertainty about the upcoming word, but not uncertainty about the upcoming phoneme. On the other hand, a brain response related to sentence-conditional phoneme entropy would constitute evidence for cross-hierarchy predictions, with sentence-level context informing predictions of upcoming phonemes.</p><p>Even though phoneme and cohort entropy were highly correlated (sentence context: <italic>r</italic> = 0.92; word context: <italic>r</italic> = 0.90), each of the four representations was able to explain unique variability in the MEG responses that could not be attributed to any of the other representations (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, <xref ref-type="table" rid="table1">Table 1</xref>). This suggests that the sentence context model is not restricted to predicting upcoming words, but also generates expectations for upcoming phonemes. This is thus evidence for cross-hierarchy top-down information flow, indicative of a unified language model that aligns representations across hierarchical levels. Together, these results thus indicate that the brain does maintain a unified context model, but that it <italic>also</italic> maintains more local context models.</p></sec><sec id="s2-2"><title>Different context models affect different neural processes</title><p>All three context models individually contribute to neural representations, but are these representations functionally separable? While all three context models improve predictions in both hemispheres, the sentence-constrained model does so symmetrically, whereas the lexical and sublexical models are both more strongly represented in the right hemisphere than in the left hemisphere (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This pattern might suggest an overall right lateralization of linguistic processing; however, the predictive power of the joint linguistic model (predictors from all three levels combined) is not significantly lateralized (<italic>t</italic><sub>max</sub> = 4.11, p = 0.134). These results thus suggest that linguistic processing is bilateral, but that the hemispheres differ in what context models they rely on. Consistent with this, the context models differ in their relative lateralization (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). The sublexical context model is significantly more right-lateralized than the sentence model (<italic>t</italic><sub>11</sub> = 4.41, p = 0.001), while the word model is only numerically more right-lateralized than the sentence model (<italic>t</italic><sub>11</sub> = 1.53, p = 0.154). These lateralization patterns suggest an anatomical differentiation in the representations of different context models, with the left hemisphere primarily relying on a unified model of the sentence context, and the right hemisphere more broadly keeping track of different context levels.</p><p>Given that all three context models are represented in the STG, especially in the right hemisphere, a separate question concerns whether, within a hemisphere, the different context models predict activity in the same or different neural sources. While MEG source localization does not allow precisely separating different sources in close proximity, it does allow statistically testing whether two effects originate from the same or from a different configuration of neural sources (<xref ref-type="bibr" rid="bib74">Lütkenhöner, 2003</xref>). The null hypothesis of such a test (<xref ref-type="bibr" rid="bib80">McCarthy and Wood, 1985</xref>) is that a single neural process, corresponding to a fixed configuration of current sources, generates activity that is correlated with all three context models. The alternative hypothesis suggests some differentiation between the configuration of sources recruited by the different models. Results indicate that, in the right hemisphere, all three context models, as well as the two acoustic models, originate from different source configurations (<italic>F</italic><sub>(175, 1925)</sub> ≥ 1.25, p ≤ 0.017). In the left hemisphere, the sentence-constrained model is localized differently from all other models (<italic>F</italic><sub>(179, 1969)</sub> ≥ 1.38, p &lt; 0.001), whereas there is no significant distinction among the other models (possibly due to lower power due to the weaker effects in the left hemisphere for all but the sentence model). In sum, these results suggest that the different context models are maintained by at least partially separable neural processes.</p></sec><sec id="s2-3"><title>Sentence context affects early responses and dominates late responses</title><p>The temporal response functions (TRFs) estimated for the full model quantify the influence of each predictor variable on brain responses over a range of latencies (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). <xref ref-type="fig" rid="fig5">Figure 5</xref> shows the response magnitude to each predictor variable as a function of time, relative to phoneme onset. For an even comparison between predictors, TRFs were summed in the anatomical region in which any context model significantly improved predictions. Note that responses prior to 0 ms are plausible due to coarticulation, by which information about a phoneme’s identity can already be present in the acoustic signal prior to the conventional phoneme onset (<xref ref-type="bibr" rid="bib4">Beddor et al., 2013</xref>; <xref ref-type="bibr" rid="bib98">Salverda et al., 2003</xref>). <xref ref-type="fig" rid="fig5">Figure 5G</xref> shows the anatomical distribution of responses related to the different levels of context.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Early responses reflect parallel activation of all context models, later responses selectively reflect activity in the sentence-constrained model.</title><p>(<bold>A</bold>) Current magnitude of temporal response functions (TRFs) to phoneme surprisal for each level of context (mean and within-subject standard error [<xref ref-type="bibr" rid="bib71">Loftus and Masson, 1994</xref>]; <italic>y</italic>-axis scale identical in all panels of the figure). To allow fair comparison, all TRFs shown are from the same symmetric region of interest, including all current dipoles for which at least one of the three context models significantly improved the response predictions. Bars indicate time windows corresponding to source localizations shown in panel G. (<bold>B</bold>) When plotted separately for each hemisphere, relative lateralization of the TRFs is consistent with the lateralization of predictive power (<xref ref-type="fig" rid="fig4">Figure 4</xref>). (<bold>C, D</bold>) TRFs to lexical cohort entropy are dominated by the sentence context model. (<bold>E, F</bold>) TRFs to phoneme entropy are similar between context models, consistent with parallel use of different contexts in predictive models for upcoming speech. (<bold>G</bold>) All context models engage the superior temporal gyrus at early responses, midlatency responses incorporating the sentence context also engage more ventral temporal areas. Anatomical plots reflect total current magnitude associated with different levels of context representing early (−50 to 150 ms), midlatency (150–350 ms), and late (350–550 ms) responses. The color scale is adjusted for different predictors to avoid images dominated by the spatial dispersion characteristic of magnetoencephalography source estimates.</p><p><supplementary-material id="fig5sdata1"><label>Figure 5—source data 1.</label><caption><title>Temporal response function peak latencies in the early time window.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-72056-fig5-data1-v2.zip"/></supplementary-material></p><p><supplementary-material id="fig5sdata2"><label>Figure 5—source data 2.</label><caption><title>Pairwise tests of temporal response function time courses.</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-72056-fig5-data2-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72056-fig5-v2.tif"/></fig><p>Surprisal quantifies the incremental update to a context model due to new input. A brain response related to surprisal therefore indicates that the sensory input is brought to bear on a neural representation that uses the corresponding context model. Consequently, the latencies of brain responses related to surprisal at different context models are indicative of the underlying processing architecture. In an architecture in which information is sequentially passed to higher-level representations with broadening context models (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), responses should form a temporal sequence from narrower to broader contexts. However, in contrast to this prediction, the observed responses to surprisal suggest that bottom-up information reaches representations that use the sentence- and word-level contexts <italic>simultaneously</italic>, at an early response peak (<xref ref-type="fig" rid="fig5">Figure 5A</xref>; peak in the early time window for sentence context: 78 ms, standard deviation (SD) = 24 ms; word context: 76 ms, SD = 11 ms). Sublexical surprisal is associated with a lower response magnitude overall, but also exhibits an early peak at 94 ms (SD = 26 ms). None of these three peak latencies differ significantly (all pairwise <italic>t</italic><sub>11</sub> ≤ 2.01, p ≥ 0.065). This suggests a parallel processing architecture in which different context representations are activated simultaneously by new input. Later in the timecourse the responses dissociate more strongly, with a large, extended response reflecting the sentence context, but not the word context starting at around 205 ms (<italic>t</italic><sub>max</sub> = 5.27, p = 0.007). The lateralization of the TRFs is consistent with the trend observed for predictive power: a symmetric response reflecting the unified sentence context, and more right-lateralized responses reflecting the more local contexts (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><p>The TRFs, convolved with the corresponding predictors generate partial predicted responses (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). This reconstruction thus allows decomposing brain responses into component responses corresponding to different predictors. <xref ref-type="fig" rid="fig6">Figure 6</xref> uses this to simulate the responses corresponding to the different context models, illustrating several of the observations made above. As the sentence level has the most predictive power, so it also corresponds to higher amplitude responses than the other levels. Furthermore, the subsentence levels exhibit small modulations close to surprising phonemes, corresponding to the mainly brief, low latency TRFs. In contrast, the response corresponding to the sentence level is dominated by larger waves, lagging surprising phonemes by several hundred milliseconds, corresponding to the sustained, higher TRF amplitudes at later latencies.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title><bold>Decomposition of brain responses into context levels</bold>.</title><p>Predicted brain responses related to processing different levels of context (combining surprisal and entropy predictors; mean and within-subject standard error). Stem plots correspond to surprisal for the given level. Slow fluctuations in brain responses are dominated by the sentence level, with responses occurring several hundred milliseconds after surprising phonemes, consistent with the high amplitude at late latencies in temporal response functions (TRFs). Partial predicted responses were generated for each context model by convolving the TRFs with the corresponding predictors; summing the predicted responses for the predictors corresponding to the same level; and extracting the magnitude (sum of absolute values) in the superior temporal gyrus region of interest.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72056-fig6-v2.tif"/></fig></sec><sec id="s2-4"><title>Sentence context dominates word recognition, all contexts drive phoneme predictions</title><p>Brain responses related to entropy indicate that neural processes are sensitive to uncertainty or competition in the interpretation of the speech input. Like surprisal, such a response suggests that the information has reached a representation that has incorporated the corresponding context. In addition, because entropy measures uncertainty regarding a categorization decision, the response to entropy can distinguish between different levels of categorization: uncertainty about the current word (cohort entropy) versus uncertainty about the next phoneme (phoneme entropy).</p><p>The TRFs to cohort entropy suggest a similar pattern as those to surprisal (<xref ref-type="fig" rid="fig5">Figure 5C, D</xref>). Both cohort representations are associated with an early peak (sentence context: 56 ms, SD = 28 ms; word context: 80 ms, SD = 23 ms), followed only in the sentence-constrained cohort by a later sustained effect. In contrast to surprisal, however, even early responses to cohort entropy are dominated by the sentence context (<italic>t</italic><sub>max</sub> = 5.35, p = 0.004 at 43 ms; later responses: <italic>t</italic><sub>max</sub> = 7.85, p &lt; 0.001 at 461 ms). This suggests that lexical representations are overall most strongly activated in a model that incorporates the sentence context.</p><p>In contrast to surprisal and cohort entropy, the responses to phoneme entropy are similar for all levels of context, dominated by an early and somewhat broader peak (<xref ref-type="fig" rid="fig5">Figure 5E, F</xref>). There is still some indication of a second, later peak in the response to sentence-constrained phoneme entropy, but this might be due to the high correlation between cohort and phoneme entropy. A direct comparison of sentence-constrained cohort and phoneme entropy indicates that early processing is biased toward phoneme entropy (though not significantly) while later processing is biased toward cohort entropy (<italic>t</italic><sub>max</sub> = 4.74, p = 0.017 at 231 ms).</p><p>In sum, the entropy results suggest that all context representations drive a predictive model for upcoming phonemes. This is reflected in a short-lived response in STG, consistent with the fast rate of phonetic information. Simultaneously, the incoming information is used to constrain the cohort of word candidates matching the current input, with lexical activations primarily driven by a unified model that incorporates the sentence context.</p></sec><sec id="s2-5"><title>Midlatency, sentence-constrained processing engages larger parts of the temporal lobe</title><p>Source localization suggests that early activity originates from the vicinity of the auditory cortex in the upper STG, regardless of context (<xref ref-type="fig" rid="fig5">Figure 5G</xref>). The precise source configuration in the right STG nevertheless differs between contexts in the early time window (sentence vs word: <italic>F</italic><sub>(175, 1925)</sub> = 2.08, p &lt; 0.001; word vs sublexical: <italic>F</italic><sub>(175, 1925)</sub> = 5.99, p &lt; 0.001). More notably, the sentence-based responses in the midlatency window recruits more sources, localized to the middle and inferior temporal lobe. Accordingly, the sentence-based responses in the midlatency window differs significantly from the early window (left hemisphere: <italic>F</italic><sub>(179, 1969)</sub> = 1.72, p &lt; 0.001; right hemisphere: <italic>F</italic><sub>(175, 1925)</sub> = 5.48, p &lt; 0.001). These results suggest that phonetic information initially engages a set of sources in the STG, while a secondary stage then engages more ventral sources that specifically represent the sentence context.</p></sec><sec id="s2-6"><title>No evidence for a trade-off between contexts</title><p>We interpret our results as evidence that different context models are maintained in parallel. An alternative possibility is that there is some trade-off between contexts used, and it only appears in the averaged data as if all models were operating simultaneously. This alternative predicts a negative correlation between the context models, reflecting the trade-off in their activation. No evidence was found for such a trade-off, as correlation between context models were generally neutral or positive across subjects and across time (<xref ref-type="fig" rid="fig7">Figure 7</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>No evidence for a trade-off between context models.</title><p>(<bold>A</bold>) Trade-off across subjects: testing the hypothesis that subjects differ in which context model they rely on. Each plot compares the predictive power of two context models in the mid/posterior superior temporal gyrus region of interest, each dot representing % explained for one subject. The line represents a linear regression with 95% bootstrap confidence interval (<xref ref-type="bibr" rid="bib114">Waskom, 2021</xref>). None of the pairwise comparisons exhibits a negative correlation that would be evidence for a trade-off between reliance on different context models. Data from <xref ref-type="supplementary-material" rid="fig4sdata2">Figure 4—source data 2</xref>. (<bold>B</bold>) Trade-off over time: testing the hypothesis that subjects alternate over time in which context model they rely on. Each dot represents the partial correlation over time between the predictive power of two context models for one subject, controlling for predictive power of the full model. Correlations are shown separately for the left and the right hemisphere (L/R). Stars correspond to a one-sample <italic>t</italic>-tests of the null hypothesis that the average <italic>r</italic> across subjects is 0, that is, that the two context models are unrelated over time. None of the context models exhibited a significant negative correlation that would be evidence for a trade-off over time.</p><p><supplementary-material id="fig7sdata1"><label>Figure 7—source data 1.</label><caption><title>Partial correlations over time for each subject (data for Panel B).</title></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-72056-fig7-data1-v2.zip"/></supplementary-material></p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72056-fig7-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The present MEG data provide clear evidence for the existence of a neural representation of speech that is unified across representational hierarchies. This representation incrementally integrates phonetic input with information from the multiword context within about 100 ms. However, in addition to this globally unified representation, brain responses also show evidence of separate neural representations that use more local contexts to process the same input.</p><sec id="s3-1"><title>Parallel representations of speech using different levels of context</title><p>The evidence for a unified global model suggests that there is a functional brain system that processes incoming phonemes while building a representation that incorporates constraints from the multiword context. A possible architecture for such a system is the unified global architecture shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, in which a probabilistic representation of the lexical cohort mediates between sentence- and phoneme-level representations: the sentence context modifies the prior expectation for each word, which is in turn used to make low-level predictions about the phonetic input. While there are different possible implementations for such a system, the key feature is that the global sentence context is used to make predictions for and interpret low-level phonetic, possibly even acoustic (<xref ref-type="bibr" rid="bib103">Sohoglu and Davis, 2020</xref>) input.</p><p>A second key result from this study, however, is evidence that this unified model is not the only representation of speech. Brain responses also exhibited evidence for two other, separate functional systems that process incoming phonemes while building representations that incorporate different, more constrained kinds of context: one based on a local word context, processing the current word with a prior based on context-independent lexical frequencies, and another based on the local phoneme sequence regardless of word boundaries. Each of these three functional systems generates its own predictions for upcoming phonemes, resulting in parallel responses to phoneme entropy. Each system is updated incrementally at the phoneme rate, reflected in early responses to surprisal. However, each system engages an at least partially different configuration of neural sources, as evidenced by the localization results.</p><p>Together, these results suggest that multiple predictive models process speech input in parallel. An architecture consistent with these observations is sketched in <xref ref-type="fig" rid="fig8">Figure 8</xref>: three different neural systems receive the speech input in parallel. Each representation is updated incrementally by arriving phonemes. However, the three systems differ in the extent and kind of context that they incorporate, each generating its own probabilistic beliefs about the current word and/or future phonemes. For instance, the sublexical model uses the local phoneme history to predict upcoming phonemes. The updates are incremental because the state of the model at time <italic>k</italic> + 1 is determined by the state of the model at time <italic>k</italic> and the phoneme input from time <italic>k</italic>. The same incremental update pattern applies to the sublexical, word, and sentence models.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>An architecture for speech perception with multiple parallel context models.</title><p>A model of information flow, consistent with brain signals reported here. Brain responses associated with Information-theoretic variables provide separate evidence for each of the probability distributions in the colored boxes. From left to right, the three different context models (sentence, word, and sublexical) update incrementally as each phoneme arrives. The cost of these updates is reflected in the brain response related to surprisal. Representations also include probabilistic representations of words and upcoming phonemes, reflected in brain responses related to entropy.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72056-fig8-v2.tif"/></fig><p>A listener whose goal is comprehending a discourse-level message might be expected to rely primarily on the unified, sentence-constrained context model. Consistent with this, there is some evidence that this model has a privileged status. Among the linguistic models, the unified model has the most explanatory power and clearly bilateral representation (<xref ref-type="fig" rid="fig4">Figure 4</xref>). In addition, while activity in local models was short lived, the unified model was associated with extended activation for up to 600 ms and recruitment of more ventral regions of the temporal lobe (<xref ref-type="fig" rid="fig5">Figure 5</xref>). This suggests that the update in the unified model is normally more extensive than the local models, and could indicate that the unified model most commonly drives semantic as well as form representations, while the short-lived local models might be restricted to form-based representations.</p></sec><sec id="s3-2"><title>Implications for speech processing</title><p>A longstanding puzzle in the comprehension literature has been why activation of candidates not supported by context is sometimes reported (<xref ref-type="bibr" rid="bib104">Swinney, 1979</xref>; <xref ref-type="bibr" rid="bib119">Zwitserlood, 1989</xref>), if top-down sentence context rapidly feeds down to early levels of speech perception. Parallel activation of lexical candidates based on sentence and word context models can explain these findings. Short-lived brain responses (up to 150 ms after phoneme onset) show evidence of parallel activation of sentence-constrained as well as sentence-independent word candidates. The coexistence of these two candidate sets can explain short-lived priming of sentence-inappropriate candidates. Whereas brain responses related to sentence-independent candidates are transient, brain responses related to sentence-appropriate candidates exhibit a secondary, sustained response (150–550 ms), explaining selective priming of sentence-appropriate candidates at longer delays.</p><p>If context-constrained candidates are immediately available, then why maintain alternative, sentence-independent candidates or even sublexical probabilistic phoneme sequences? One functional advantage might be faster recovery when sentence-based predictions turn out to be misleading. Such an effect has been described in reading, where contextually unexpected continuations are not associated with a proportional increase in processing cost (<xref ref-type="bibr" rid="bib40">Frisson et al., 2017</xref>; <xref ref-type="bibr" rid="bib72">Luke and Christianson, 2016</xref>).</p><p>Similarly, a representation of sublexical phoneme sequences might be functionally relevant when encountering input that is noisy or not yet part of the lexicon. Phoneme transition probabilities are generally higher within words than between words, such that low probability phoneme transitions are cues to word boundaries (<xref ref-type="bibr" rid="bib16">Cairns et al., 1997</xref>; <xref ref-type="bibr" rid="bib56">Harris, 1955</xref>). Statistical phoneme sequence models might thus play an important role in language acquisition by bootstrapping lexical segmentation of continuous speech (<xref ref-type="bibr" rid="bib16">Cairns et al., 1997</xref>; <xref ref-type="bibr" rid="bib17">Chambers et al., 2003</xref>; <xref ref-type="bibr" rid="bib97">Saffran et al., 1996</xref>). Even in adult speech perception, they might have a similar function when encountering novel words, such as domain-specific vocabularies or personal names (<xref ref-type="bibr" rid="bib90">Norris and McQueen, 2008</xref>). Finally, the linguistic context can be highly informative for phoneme recognition (<xref ref-type="bibr" rid="bib61">Hitczenko et al., 2020</xref>), and different levels of context might make complementary contributions.</p><p>Our results suggest that the different context models operate in parallel, without an apparent trade-off, between subjects, or over time. However, our listening condition was also relatively uniform – listening to an audiobook in quiet background. It is conceivable that the different models play more differentiated roles in different listening conditions, for example in unpredictable conversations or speech in noise.</p></sec><sec id="s3-3"><title>Implications for word recognition</title><p>Perhaps the most surprising implication of our results is that multiple probabilistic cohort representations seem to be maintained in parallel. This is most directly implied by the result that two different cohort entropy predictors both explain unique variability in the data, one based on the lexical frequency of each candidate, and another based on the contextual likelihood of each candidate. This is inconsistent with models in which each lexical candidate is assigned a single ‘activation’ value (e.g., <xref ref-type="bibr" rid="bib84">Morton, 1969</xref>). It might be more readily reconciled with models that distinguish between a lexical item’s entry in long-term memory, and its instantiation as a token for parsing a speech signal, since such a mechanism allows for multiple tokens corresponding to the same lexical item (<xref ref-type="bibr" rid="bib82">McClelland and Elman, 1986</xref>; <xref ref-type="bibr" rid="bib88">Norris, 1994</xref>; <xref ref-type="bibr" rid="bib90">Norris and McQueen, 2008</xref>). Yet, existing models generally are restricted to a single ‘arena’ for lexical competition, whereas our results imply the possibility that the competition plays out in parallel in at least partially different brain systems.</p><p>A second implication is that feedback from the sentence-level context can and does affect phoneme processing. The observed phoneme entropy effects indicate that phoneme-level expectations are modulated by the sentence context. This is inconsistent with some models of word recognition that assume a pure bottom-up process (e.g., <xref ref-type="bibr" rid="bib89">Norris et al., 2000</xref>). However, at the same time, the parallel architecture we propose (<xref ref-type="fig" rid="fig8">Figure 8</xref>) addresses a central theoretical problem associated with feedback in multistage architectures: Bayesian accounts of perception suggest that listeners generate a prior, reflecting an estimate of future input, and compare this prior to the actual input to compute a posterior probability, or interpretation of the sensory percept. In multistage architectures that allow different priors at sequential hierarchical levels (such as <xref ref-type="fig" rid="fig1">Figure 1B</xref>), higher levels receive the posterior interpretation of the input from the lower levels, rather than the unbiased input itself. This is suboptimal when considering a Bayesian model of perception, because the prior of lower-level systems is allowed to distort the bottom-up evidence before it is compared to the prior generated by higher levels (<xref ref-type="bibr" rid="bib91">Norris et al., 2016</xref>). In contrast, the parallel representations favored by the evidence presented here allow undistorted bottom-up information to be directly compared with the context model for each definition of context. The parallel model can thus accommodate empirical evidence for feedback while avoiding this theoretical problem associated with sequential models.</p></sec><sec id="s3-4"><title>Evidence for graded linguistic predictions</title><p>There is broad agreement that language processing involves prediction, but the exact nature of these predictions is more controversial (<xref ref-type="bibr" rid="bib26">DeLong et al., 2005</xref>; <xref ref-type="bibr" rid="bib63">Huettig, 2015</xref>; <xref ref-type="bibr" rid="bib87">Nieuwland et al., 2020</xref>; <xref ref-type="bibr" rid="bib86">Nieuwland et al., 2018</xref>; <xref ref-type="bibr" rid="bib93">Pickering and Gambi, 2018</xref>). Much of the debate is about whether humans can represent distributions over many likely items, or just predict specific items. Previous research showing an early influence of sentence context on speech processing has typically relied on specifically designed, highly constraining contexts which are highly predictive of a specific lexical item (<xref ref-type="bibr" rid="bib62">Holcomb and Neville, 2013</xref>; <xref ref-type="bibr" rid="bib21">Connolly and Phillips, 1994</xref>; <xref ref-type="bibr" rid="bib111">Van Petten et al., 1999</xref>; <xref ref-type="bibr" rid="bib96">Rommers et al., 2013</xref>). In such highly predictive contexts, listeners might indeed predict specific items, and such predictions might be linked to the left-lateralized speech productions system (<xref ref-type="bibr" rid="bib33">Federmeier, 2007</xref>; <xref ref-type="bibr" rid="bib93">Pickering and Gambi, 2018</xref>). However, such a mechanism would be less useful in more representative language samples, in which highly predictable words are rare (<xref ref-type="bibr" rid="bib72">Luke and Christianson, 2016</xref>). In such situations of limited predictability, reading time data suggest that readers instead make graded predictions, over a large number of possible continuations (<xref ref-type="bibr" rid="bib72">Luke and Christianson, 2016</xref>; <xref ref-type="bibr" rid="bib102">Smith and Levy, 2013</xref>). Alternatively, it has also been suggested that what looks like graded predictions could actually be preactivation of specific higher-level semantic and syntactic features shared among the likely items (<xref ref-type="bibr" rid="bib2">Altmann and Kamide, 1999</xref>; <xref ref-type="bibr" rid="bib72">Luke and Christianson, 2016</xref>; <xref ref-type="bibr" rid="bib78">Matchin et al., 2019</xref>; <xref ref-type="bibr" rid="bib93">Pickering and Gambi, 2018</xref>; <xref ref-type="bibr" rid="bib110">Van Berkum et al., 2005</xref>), without involving prediction of form-based representations. The present results, showing brain responses reflecting sentence-constrained cohort- and phoneme entropy, provide a new kind of evidence in favor of graded probabilistic and form-based predictions at least down to the phoneme level.</p></sec><sec id="s3-5"><title>Bilateral pathways to speech comprehension</title><p>Our results suggest that lexical/phonetic processing is largely bilateral. This is consistent with extensive clinical evidence for bilateral receptive language ability (<xref ref-type="bibr" rid="bib47">Gazzaniga and Sperry, 1967</xref>; <xref ref-type="bibr" rid="bib67">Kutas et al., 1988</xref>; <xref ref-type="bibr" rid="bib94">Poeppel, 2001</xref>; <xref ref-type="bibr" rid="bib60">Hickok and Poeppel, 2007</xref>), and suggestions that the right hemisphere might even play a distinct role in complex, real-world language processing (<xref ref-type="bibr" rid="bib34">Federmeier et al., 2008</xref>; <xref ref-type="bibr" rid="bib65">Jung-Beeman, 2005</xref>). In healthy participants, functional lateralization of sentence processing has been studied in reading using visual half-field presentation (<xref ref-type="bibr" rid="bib31">Federmeier and Kutas, 1999</xref>). Overwhelmingly, results from these studies suggest that lexical processing in both hemispheres is dominated by sentence meaning (<xref ref-type="bibr" rid="bib22">Coulson et al., 2005</xref>; <xref ref-type="bibr" rid="bib32">Federmeier et al., 2005</xref>; <xref ref-type="bibr" rid="bib31">Federmeier and Kutas, 1999</xref>; <xref ref-type="bibr" rid="bib118">Wlotko and Federmeier, 2007</xref>). This is consistent with the strong bilateral representation of the unified model of speech found here. As in the visual studies, the similarity of the response latencies in the two hemispheres implies that right-hemispheric effects are unlikely to be due to interhemispheric transfer from the left hemisphere (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><p>In addition to bilateral representation, however, our results also suggest that the two hemispheres differ with respect to the context models they entertain. Visual half-field reading studies have indicated a pattern of hemispheric differences, which has been interpreted as indicating that the left hemisphere processes language in a maximally context-sensitive manner, whereas the right hemisphere processes the sensory input in a bottom-up manner, unbiased by the linguistic context (<xref ref-type="bibr" rid="bib33">Federmeier, 2007</xref>). Our results suggest a modification of this proposal, indicating that both hemispheres rely on sentence-based, graded predictions, but that the right hemisphere <italic>additionally</italic> maintains stronger representations of local contexts. Finally, lateralization might also depend on task characteristics such as stimulus familiarity (<xref ref-type="bibr" rid="bib9">Brodbeck et al., 2018a</xref>), and in highly constraining contexts the left hemisphere might engage the left-lateralized language production system to make specific predictions (<xref ref-type="bibr" rid="bib33">Federmeier, 2007</xref>; <xref ref-type="bibr" rid="bib93">Pickering and Gambi, 2018</xref>).</p></sec><sec id="s3-6"><title>Limitations of the sentence context model</title><p>We approximated the sentence context with a 5 gram model. This model provides an accurate estimate of the sum of local constraints, based on a context of the four preceding words only. However, it misses the more subtle influences of the larger context, both semantic constraints and syntactic long-range dependencies, which might make the sentence level even more different from the local context models. Furthermore, lexical <italic>N</italic> gram models conflate the influence of syntactic, semantic, and associative constraints (e.g., idioms). It is thus possible that work with more sophisticated language models can reveal an even more complex relationship between global and local contexts.</p></sec><sec id="s3-7"><title>Conclusions</title><p>Prior research on the use of context during language processing has often focused on binary distinctions, such as asking whether context is or is not used to predict future input. Such questions assumed a single serial or cascaded processing stream. Here, we show that this assumption might have been misleading, because different predictive models are maintained in parallel. Our results suggest that robust speech processing is based on probabilistic predictions using different context models in parallel, and cutting across hierarchical levels of representations.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>Twelve native speakers of English were recruited from the University of Maryland community (six female, six male, age mean = 21 years, range 19–23). None reported any neurological or hearing impairment. According to self-report using the Edinburgh Handedness Inventory (<xref ref-type="bibr" rid="bib92">Oldfield, 1971</xref>), 11 were right handed and 1 left handed. All subjects provided written informed consent in accordance with the University of Maryland Institutional Review Board. Subjects either received course credit (<italic>n</italic> = 4) or were paid for their participation (<italic>n</italic> = 8). This sample size is comparable to the most directly relatable previous research which either had a similar number of subjects, <italic>N</italic> = 11 (<xref ref-type="bibr" rid="bib29">Donhauser and Baillet, 2020</xref>) or a larger number of subjects but substantially less data per subject, <italic>N</italic> = 28 with single talker stimulus duration 8 min (<xref ref-type="bibr" rid="bib9">Brodbeck et al., 2018a</xref>).</p></sec><sec id="s4-2"><title>Stimuli</title><p>Stimuli consisted of 11 excerpts from the audiobook version of <italic>The Botany of Desire</italic> by <italic>Michael Pollan</italic> (<xref ref-type="bibr" rid="bib95">Pollan, 2001</xref>). Each excerpt was between 210 and 332 s long, for a total of 46 min and 44 s. Excerpts were selected to create a coherent narrative and were presented in chronological order to maximize deep processing for meaning.</p></sec><sec id="s4-3"><title>Procedure</title><p>During MEG data acquisition, participants lay in a supine position. They were allowed to keep their eyes open or closed to maximize subjective comfort and allow them to focus on the primary task of listening to the audiobook. Stimuli were delivered through foam pad earphones inserted into the ear canal at a comfortably loud listening level. After each segment, participants answered two to three questions relating to its content and had an opportunity to take a short break.</p></sec><sec id="s4-4"><title>Data acquisition and preprocessing</title><p>Brain responses were recorded with a 157 axial gradiometer whole head MEG system (KIT, Kanazawa, Japan) inside a magnetically shielded room (Vacuumschmelze GmbH &amp; Co. KG, Hanau, Germany) at the University of Maryland, College Park. Sensors (15.5 mm diameter) are uniformly distributed inside a liquid-He dewar, spaced ~25 mm apart, and conﬁgured as ﬁrst-order axial gradiometers with 50 mm separation and sensitivity better than 5 fT Hz<sup>−1/2</sup> in the white noise region (&gt;1 kHz). Data were recorded with an online 200 Hz low-pass ﬁlter and a 60 Hz notch ﬁlter at a sampling rate of 1 kHz.</p><p>Recordings were preprocessed using mne-python (<xref ref-type="bibr" rid="bib49">Gramfort et al., 2014</xref>). Flat channels were automatically detected and excluded. Extraneous artifacts were removed with temporal signal space separation (<xref ref-type="bibr" rid="bib107">Taulu and Simola, 2006</xref>). Data were ﬁltered between 1 and 40 Hz with a zero-phase FIR ﬁlter (mne-python 0.20 default settings). Extended infomax independent component analysis (<xref ref-type="bibr" rid="bib6">Bell and Sejnowski, 1995</xref>) was then used to remove ocular and cardiac artifacts. Responses time locked to the speech stimuli were extracted, low pass filtered at 20 Hz and resampled to 100 Hz.</p><p>Five marker coils attached to participants’ head served to localize the head position with respect to the MEG sensors. Head position was measured at the beginning and at the end of the recording session and the two measurements were averaged. The FreeSurfer (<xref ref-type="bibr" rid="bib37">Fischl, 2012</xref>) ‘fsaverage’ template brain was coregistered to each participant’s digitized head shape (Polhemus 3SPACE FASTRAK) using rotation, translation, and uniform scaling. A source space was generated using fourfold icosahedral subdivision of the white matter surface, with source dipoles oriented perpendicularly to the cortical surface. Regularized minimum l2 norm current estimates (<xref ref-type="bibr" rid="bib23">Dale and Sereno, 1993</xref>; <xref ref-type="bibr" rid="bib55">Hämäläinen and Ilmoniemi, 1994</xref>) were computed for all data using an empty room noise covariance (<italic>λ</italic> = 1/6). The TRF analysis was restricted to brain areas of interest by excluding the occipital lobe, insula, and midline structures based on the ‘aparc’ FreeSurfer parcellation (<xref ref-type="bibr" rid="bib27">Desikan et al., 2006</xref>). Excluded areas are shaded gray in <xref ref-type="fig" rid="fig4">Figure 4</xref>. A preliminary analysis (see below) was restricted to the temporal lobe (superior, middle, and inferior temporal gyri, Heschl’s gyrus, and superior temporal sulcus).</p></sec><sec id="s4-5"><title>Predictor variables</title><sec id="s4-5-1"><title>Acoustic model</title><p>To control for brain responses to acoustic features, all models included an eight band gammatone spectrogram and an eight band acoustic onset spectrogram (<xref ref-type="bibr" rid="bib11">Brodbeck et al., 2020</xref>), both covering frequencies from 20 to 5000 Hz in equivalent rectangular bandwidth space (<xref ref-type="bibr" rid="bib58">Heeris, 2018</xref>) and scaled with exponent 0.6 (<xref ref-type="bibr" rid="bib7">Biesmans et al., 2017</xref>).</p></sec><sec id="s4-5-2"><title>Word- and phoneme segmentation</title><p>A pronunciation dictionary was generated by combining the Carnegie-Mellon University pronunciation dictionary with the Montreal Forced Aligner (<xref ref-type="bibr" rid="bib79">McAuliffe et al., 2017</xref>) dictionary and adding any additional words that occurred in the stimuli. Transcripts were then aligned to the acoustic stimuli using the Montreal Forced Aligner (<xref ref-type="bibr" rid="bib79">McAuliffe et al., 2017</xref>) version 1.0.1. All models included control predictors for word onsets (equal value impulse at the onset of each word) and phoneme onsets (equal value impulse at the onset of each non word-initial phoneme).</p></sec><sec id="s4-5-3"><title>Context-based predictors</title><p>All experimental predictor variables consist of one value for each phoneme and were represented as a sequence of impulses at all phoneme onsets. The specific values were derived from three different linguistic context models.</p></sec><sec id="s4-5-4"><title>Sublexical context model</title><p>The complete SUBTLEX-US corpus (<xref ref-type="bibr" rid="bib15">Brysbaert and New, 2009</xref>) was transcribed by substituting the pronunciation for each word and concatenating those pronunciations across word boundaries (i.e., no silence between words). Each line was kept separate since lines are unordered in the SUBTLEX corpus. The resulting phoneme sequences were then used to train a 5 gram model using KenLM (<xref ref-type="bibr" rid="bib57">Heafield, 2011</xref>). This 5 gram model was then used to derive phoneme surprisal and entropy.</p><p>The surprisal of experiencing phoneme <italic>ph<sub>k</sub></italic> at time point <italic>k</italic> is inversely related to the likelihood of that phoneme, conditional on the context (measured in bits): <inline-formula><mml:math id="inf2"><mml:mi>I</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula> . In the case of the 5 phone model this context consists of the preceding four phonemes, <italic>ph<sub>k−</sub></italic><sub>4;…</sub><italic><sub>k−</sub></italic><sub>1</sub>.</p><p>The entropy <italic>H</italic> (Greek Eta) at phoneme position <italic>ph<sub>k</sub></italic> reflects the uncertainty of what the next phoneme, <italic>ph<sub>k</sub></italic><sub>+1</sub> will be. It is defined as the expected (average) surprisal at the next phoneme, <disp-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>Based on the 5 phone model, the context here is <italic>ph<sub>k−</sub></italic><sub>3;…</sub><italic><sub>k</sub></italic>.</p></sec><sec id="s4-5-5"><title>Word context model</title><p>The word context model takes into account information from all phonemes that are in the same word as, and precede the current phoneme (<xref ref-type="bibr" rid="bib9">Brodbeck et al., 2018a</xref>) and is based on the cohort model of word perception (<xref ref-type="bibr" rid="bib77">Marslen-Wilson, 1987</xref>). At word onset, the prior for each word is proportional to its frequency in the Corpus of Contemporary American English (COCA; <xref ref-type="bibr" rid="bib25">Davies, 2015</xref>). With each subsequent phoneme, the probability for words that are inconsistent with that phoneme is set to 0, and the remaining distribution is renormalized. Phoneme surprisal and entropy are then calculated as above, but with the context being all phonemes in the current word so far. In addition, lexical entropy is calculated at each phoneme position as the entropy in the distribution of the cohort <disp-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>j</italic> is the index of the word, <italic>i</italic> is the index of the current phoneme within word <italic>j</italic>, and the context consists of phonemes <italic>ph<sub>j</sub></italic><sub>,</sub><italic><sub>1</sub></italic><sub>;…<italic>j</italic>,</sub><italic><sub>i−</sub></italic><sub>1</sub>.</p><p>This context model thus allows two different levels of representation, phonemes and words, and two corresponding entropy values, phoneme entropy and lexical entropy. Yet, we only include one version of surprisal. The reason for this is that calculating surprisal over phonemes or over words leads to identical results. This is because the <italic>k</italic>th phoneme of a word, together with the cohort at phoneme <italic>k</italic> − 1 exhaustively defines the cohort at phoneme <italic>k</italic>: <inline-formula><mml:math id="inf5"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>≡</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>.</p></sec><sec id="s4-5-6"><title>Sentence context model</title><p>The sentence context model was implemented like the lexical context model, but with the addition of lexical priors based on the 5 gram word context. A 5 gram model was trained on COCA (<xref ref-type="bibr" rid="bib25">Davies, 2015</xref>) with KenLM (<xref ref-type="bibr" rid="bib57">Heafield, 2011</xref>). Then, at the onset of each word, the cohort was initialized with each word’s prior set to its probability given the four preceding words in the 5 gram model.</p></sec></sec><sec id="s4-6"><title>Deconvolution</title><p>Deconvolution and statistical analysis were performed with Eelbrain (<xref ref-type="bibr" rid="bib12">Brodbeck et al., 2021</xref>) and additional scripts available at <ext-link ext-link-type="uri" xlink:href="https://github.com/christianbrodbeck/TRF-Tools">https://github.com/christianbrodbeck/TRF-Tools</ext-link> (<xref ref-type="bibr" rid="bib13">Brodbeck, 2021</xref>).</p><p>mTRFs were computed independently for each subject and each virtual current source (<xref ref-type="bibr" rid="bib10">Brodbeck et al., 2018b</xref>; <xref ref-type="bibr" rid="bib68">Lalor et al., 2009</xref>). The neural response at time <italic>t</italic>, <italic>y<sub>t</sub></italic> was predicted jointly from <italic>N</italic> predictor time series <italic>x<sub>i</sub></italic><sub>,</sub><italic><sub>t</sub></italic> convolved with a corresponding mTRF <italic>h<sub>iτ</sub></italic> of length <italic>T</italic>:<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>mTRFs were generated from a basis of 50 ms wide Hamming windows centered at <inline-formula><mml:math id="inf6"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mn>1000</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> ms. For estimating mTRFs, all responses and predictors were standardized by centering and dividing by the mean absolute value.</p><p>For estimation using fourfold cross-validation, each subject’s data were concatenated along the time axis and split into four contiguous segments of equal length. The mTRFs for predicting the responses in each segment were trained using coordinate descent (<xref ref-type="bibr" rid="bib24">David et al., 2007</xref>) to minimize the l1 error in the three remaining segments. For each test segment there were three training runs, with each of the remaining segments serving as the validation segment once. In each training run, the mTRF was iteratively modified based on the maximum error reduction in two training segments (the steepest coordinate descent) and validated based on the error in the validation segment. Whenever a training step caused an increase of error in the validation segment, the TRF for the predictor responsible for the increase was frozen. Training continued until the whole mTRF was frozen. The three mTRFs from the three training runs were then averaged to predict responses in the left-out testing segment.</p></sec><sec id="s4-7"><title>Model comparisons</title><p>Model quality was quantified through the l1 norm of the residuals. For this purpose, the predicted responses for the four test segments, each based on mTRFs estimated on the other three segments, were concatenated again. To compare the predictive power of two models, the difference in the residuals of the two models was calculated at each virtual source dipole. This difference map was smoothed (Gaussian window, SD = 5 mm) and tested for significance using a mass-univariate one-sample <italic>t</italic>-test with threshold-free cluster enhancement (TFCE) (<xref ref-type="bibr" rid="bib101">Smith and Nichols, 2009</xref>) and a null distribution based on the full set of 4095 possible permutations of the 12 difference maps. For effect size comparison we report <italic>t<sub>max</sub></italic>, the largest <italic>t</italic>-value in the significant (p ≤ 0.05) area.</p><p>The full model consisted of the following predictors: acoustic spectrogram (eight bands); acoustic onset spectrogram (eight bands); word onsets; phoneme onsets; sublexical context model (phoneme surprisal and phoneme entropy); lexical context model (phoneme surprisal, phoneme entropy, and word entropy); sentence context model (phoneme surprisal, phoneme entropy, and word entropy).</p><p>For each of the tests reported in <xref ref-type="fig" rid="fig4">Figure 4</xref>, mTRFs were reestimated using a corresponding subset of the predictors in the full model. For instance, to calculate the predictive power for a given level of context, the model was refit using all predictors except the predictors of the level under investigation. Each plot thus reflects the variability that can <italic>only</italic> be explained by the level in question. This is generally a conservative estimate for the predictive power because it discounts any explanatory power based on variability that is shared with other predictors. In order to determine the predictive power of linguistic processing in general, we also fit a model excluding all eight information-theoretic predictors from the three levels combined.</p><p>To express model fits in a meaningful unit, the explainable variability was estimated through the largest possible explanatory power of the full model (maximum across the brain of the measured response minus residuals, averaged across subjects). All model fits were then expressed as % of this value. For visualization, brain maps are not masked by significance to accurately portray the continuous nature of MEG source estimates.</p><sec id="s4-7-1"><title>Tests of lateralization</title><p>For spatiotemporal tests of lateralization (<xref ref-type="fig" rid="fig4">Figure 4A, D</xref>), the difference map was first morphed to the symmetric ‘fsaverage_sym’ brain (<xref ref-type="bibr" rid="bib50">Greve et al., 2013</xref>), and the data from the right hemisphere were morphed to the left hemisphere. Once in this common space, a mass-univariate repeated measures <italic>t</italic>-test with TFCE was used to compare the difference map from the left and right hemisphere.</p></sec><sec id="s4-7-2"><title>Region of interest analysis</title><p>To allow for univariate analyses of predictive power, a ROI was used including a region responsive to all context models (white outline in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). This ROI was defined as the posterior 2/3 of the combined Heschl’s gyrus and STG ‘aparc’ label, separately in each hemisphere.</p><p>To compare relative lateralization in this ROI (<xref ref-type="fig" rid="fig4">Figure 4E</xref>), the predictive power in each hemisphere’s ROI was rectified (values smaller than 0 were set to 0). The lateralization index (LI) was then computed as <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-7-3"><title>Tests of localization difference</title><p>A direct comparison of two localization maps can have misleading results due to cancellation between different current sources (<xref ref-type="bibr" rid="bib74">Lütkenhöner, 2003</xref>) as well as the spatially continuous nature of MEG source estimates (<xref ref-type="bibr" rid="bib8">Bourguignon et al., 2018</xref>). However, a test of localization difference is possible due to the additive nature of current sources (<xref ref-type="bibr" rid="bib80">McCarthy and Wood, 1985</xref>). Specifically, for a linear inverse solver as used here, if the relative amplitude of a configuration of current sources is held constant, the topography of the resulting source localization is also unchanged. Consequently, we employed a test of localization difference that has the null hypothesis that the topography of two effect in source space is the same (<xref ref-type="bibr" rid="bib80">McCarthy and Wood, 1985</xref>). Localization tests were generally restricted to an area encompassing the major activation seen in <xref ref-type="fig" rid="fig4">Figure 4</xref>, based on ‘aparc’ labels (<xref ref-type="bibr" rid="bib27">Desikan et al., 2006</xref>): the posterior 2/3 of the superior temporal gyrus and Heschl’s gyrus combined, the superior temporal sulcus, and the middle 3/5 of the middle temporal gyrus. For each map, the values in this area were extracted and <italic>z</italic>-scored (separately for each hemisphere). For each comparison, the two <italic>z</italic>-scored maps were subtracted, and the resulting difference map was analyzed with a one-way repeated measures ANOVA with factor source location (left hemisphere: 180 sources; right hemisphere: 176 sources). According to the null hypothesis, the two maps should be (statistically) equal, and the difference map should only contain noise. In contrast, a significant effect of source location would indicate that the difference map reflects a difference in topography that is systematic between subjects.</p></sec></sec><sec id="s4-8"><title>TRF analysis</title><p>For the analysis of the TRFs, all 12 mTRFs estimated for each subject were averaged (four test segments × three training runs). TRFs were analyzed in the normalized scale that was used for model estimation.</p><sec id="s4-8-1"><title>TRF time course</title><p>To extract the time course of response functions, an ROI was generated including all virtual current sources for which at least one of the three context models significantly improved the response predictions. To allow a fair comparison between hemispheres, the ROI was made symmetric by morphing it to the ‘fsaverage_sym’ brain (<xref ref-type="bibr" rid="bib50">Greve et al., 2013</xref>) and taking the union of the two hemispheres. With this ROI, the magnitude of the TRFs at each time point was then extracted as the sum of the absolute current values across source dipoles. These time courses were resampled from 100 Hz, used for the deconvolution, to 1000 Hz for visualization and for more accurate peak time extraction. Peak times were determined by finding the maximum value within the early time window (−50 to 150 ms) for each subject. Time courses were statistically compared using mass-univariate related measures <italic>t</italic>-tests, with a null distribution based on the maximum statistic in the 4095 permutations (no cluster enhancement).</p></sec><sec id="s4-8-2"><title>TRF localization</title><p>To analyze TRF localization, TRF magnitude was quantified as the summed absolute current values in three time windows, representing early (−50 to 150 ms), midlatency (150 to 350 ms), and late (350–550 ms) responses (see <xref ref-type="fig" rid="fig5">Figure 5</xref>). Maps were smoothed (Gaussian window, SD = 5 mm) and tested for localization differences with the same procedure as described above (tests of localization difference).</p></sec></sec><sec id="s4-9"><title>Analysis of trade-off between context models</title><p>Several analyses were performed to detect a trade-off between the use of the different context models.</p><sec id="s4-9-1"><title>Trade-off by subject</title><p>One possible trade-off is between subjects: some subjects might rely on sentence context more than local models, whereas other subjects might rely more on local models. For example, for lexical processing, this hypothesis would predict that for a subject for whom the sentence context model is more predictive, the lexical context model should be less and vice versa. According to this hypothesis, the predictive power of the different context models should be negatively correlated across subjects. To evaluate this, we computed correlations between the predictive power of the different models in the mid/posterior STG ROI (see <xref ref-type="fig" rid="fig7">Figure 7A</xref> ).</p></sec><sec id="s4-9-2"><title>Trade-off over time</title><p>A second possible trade-off is across time: subjects might change their response characteristics over time to change the extent to which they rely on the lower- or higher-level context. For example, the depth of processing of meaningful speech might fluctuate with the mental state of alertness. According to this hypothesis, the predictive power of the different context models should be anticorrelated over time. To evaluate this, we calculated the residuals for the different model fits for each time point, <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, aggregating by taking the mean in the mid/posterior STG ROI (separately or each subject). The predictive power was calculated for each model by subtracting the residuals of the model from the absolute values of the measured data (i.e., the residuals of a null model without any predictor). The predictive power for each level of context was then computed by subtracting the predictive power of a corresponding reduced model, lacking the given level of context, from the predictive power of the full model. Finally, to reduce the number of data points the predictive power was summed in 1 s bins.</p><p>For each subject, the trade-off between each pair of contexts was quantified as the partial correlation (<xref ref-type="bibr" rid="bib108">Vallat, 2018</xref>) between the predictive power of the two contexts, controlling for the predictive power of the full model (to control for MEG signal quality fluctuations over time). To test for a significant trade-off, a one-sample <italic>t</italic>-test was used for the correlation between each pair of contexts in each hemisphere, with the null hypothesis that the correlation over time is 0 (see <xref ref-type="fig" rid="fig7">Figure 7B</xref> ).</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Resources, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Methodology, Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Investigation</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Methodology, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Supervision, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The study was approved by the IRB of the University of Maryland under the protocol titled 'MEG Studies of Speech and Language Processing' (reference # 01153), on August 22, 2018 and September 9, 2019 (approval duration: 1 year). All participants provided written informed consent prior to the start of the experiment.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-72056-transrepform1-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The raw data and predictors used in this study are available for download from Dryad at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.nvx0k6dv0">https://doi.org/10.5061/dryad.nvx0k6dv0</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Bhattasali</surname><given-names>S</given-names></name><name><surname>Cruz Heredia</surname><given-names>A</given-names></name><name><surname>Resnik</surname><given-names>P</given-names></name><name><surname>Simon</surname><given-names>J</given-names></name><name><surname>Lau</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Data from: Parallel processing in speech perception with local and global representations of linguistic context</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.nvx0k6dv0</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altmann</surname><given-names>G</given-names></name><name><surname>Steedman</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Interaction with context during human sentence processing</article-title><source>Cognition</source><volume>30</volume><fpage>191</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(88)90020-0</pub-id><pub-id pub-id-type="pmid">3215002</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altmann</surname><given-names>GTM</given-names></name><name><surname>Kamide</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Incremental interpretation at verbs: restricting the domain of subsequent reference</article-title><source>Cognition</source><volume>73</volume><fpage>247</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(99)00059-1</pub-id><pub-id pub-id-type="pmid">10585516</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auksztulewicz</surname><given-names>R</given-names></name><name><surname>Myers</surname><given-names>NE</given-names></name><name><surname>Schnupp</surname><given-names>JW</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Rhythmic Temporal Expectation Boosts Neural Activity by Increasing Neural Gain</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>9806</fpage><lpage>9817</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0925-19.2019</pub-id><pub-id pub-id-type="pmid">31662425</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beddor</surname><given-names>PS</given-names></name><name><surname>McGowan</surname><given-names>KB</given-names></name><name><surname>Boland</surname><given-names>JE</given-names></name><name><surname>Coetzee</surname><given-names>AW</given-names></name><name><surname>Brasher</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The time course of perception of coarticulation</article-title><source>The Journal of the Acoustical Society of America</source><volume>133</volume><fpage>2350</fpage><lpage>2366</lpage><pub-id pub-id-type="doi">10.1121/1.4794366</pub-id><pub-id pub-id-type="pmid">23556601</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Clayards</surname><given-names>M</given-names></name><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cue integration in categorical tasks: insights from audio-visual speech perception</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e19812</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0019812</pub-id><pub-id pub-id-type="pmid">21637344</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AJ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>An information-maximization approach to blind separation and blind deconvolution</article-title><source>Neural Computation</source><volume>7</volume><fpage>1129</fpage><lpage>1159</lpage><pub-id pub-id-type="doi">10.1162/neco.1995.7.6.1129</pub-id><pub-id pub-id-type="pmid">7584893</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biesmans</surname><given-names>W</given-names></name><name><surname>Das</surname><given-names>N</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name><name><surname>Bertrand</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Auditory-Inspired Speech Envelope Extraction Methods for Improved EEG-Based Auditory Attention Detection in a Cocktail Party Scenario</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>25</volume><fpage>402</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2016.2571900</pub-id><pub-id pub-id-type="pmid">27244743</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourguignon</surname><given-names>M</given-names></name><name><surname>Molinaro</surname><given-names>N</given-names></name><name><surname>Wens</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Contrasting functional imaging parametric maps: The mislocation problem and alternative solutions</article-title><source>NeuroImage</source><volume>169</volume><fpage>200</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.12.033</pub-id><pub-id pub-id-type="pmid">29247806</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Hong</surname><given-names>LE</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Rapid Transformation from Auditory to Linguistic Representations of Continuous Speech</article-title><source>Current Biology</source><volume>28</volume><fpage>3976</fpage><lpage>3983</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.042</pub-id><pub-id pub-id-type="pmid">30503620</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Presacco</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Neural source dynamics of brain responses to continuous stimuli: Speech processing from acoustics to comprehension</article-title><source>NeuroImage</source><volume>172</volume><fpage>162</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.042</pub-id><pub-id pub-id-type="pmid">29366698</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Jiao</surname><given-names>A</given-names></name><name><surname>Hong</surname><given-names>LE</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural speech restoration at the cocktail party: Auditory cortex recovers masked speech of both attended and ignored speakers</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3000883</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000883</pub-id><pub-id pub-id-type="pmid">33091003</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Brooks</surname><given-names>TL</given-names></name><name><surname>Das</surname><given-names>P</given-names></name><name><surname>Reddigari</surname><given-names>S</given-names></name><name><surname>Kulasingham</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Eelbrain</data-title><version designator="0.35">0.35</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.4650416">https://doi.org/10.5281/zenodo.4650416</ext-link></element-citation></ref><ref id="bib13"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>TRF-Tools</data-title><version designator="0928036">0928036</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/christianbrodbeck/TRF-Tools">https://github.com/christianbrodbeck/TRF-Tools</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname><given-names>MP</given-names></name><name><surname>Anderson</surname><given-names>AJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Electrophysiological Correlates of Semantic Dissimilarity Reflect the Comprehension of Natural, Narrative Speech</article-title><source>Current Biology</source><volume>28</volume><fpage>803</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.080</pub-id><pub-id pub-id-type="pmid">29478856</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>New</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Moving beyond Kucera and Francis: a critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English</article-title><source>Behavior Research Methods</source><volume>41</volume><fpage>977</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.3758/BRM.41.4.977</pub-id><pub-id pub-id-type="pmid">19897807</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cairns</surname><given-names>P</given-names></name><name><surname>Shillcock</surname><given-names>R</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Levy</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Bootstrapping Word Boundaries: A Bottom-up Corpus-Based Approach to Speech Segmentation</article-title><source>Cognitive Psychology</source><volume>33</volume><fpage>111</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1006/cogp.1997.0649</pub-id><pub-id pub-id-type="pmid">9245468</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>KE</given-names></name><name><surname>Onishi</surname><given-names>KH</given-names></name><name><surname>Fisher</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Infants learn phonotactic regularities from brief auditory experience</article-title><source>Cognition</source><volume>87</volume><fpage>B69</fpage><lpage>B77</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(02)00233-0</pub-id><pub-id pub-id-type="pmid">12590043</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>CG</given-names></name><name><surname>Tanenhaus</surname><given-names>MK</given-names></name><name><surname>Magnuson</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Actions and affordances in syntactic ambiguity resolution</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>30</volume><fpage>687</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.30.3.687</pub-id><pub-id pub-id-type="pmid">15099136</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christiansen</surname><given-names>MH</given-names></name><name><surname>Chater</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The Now-or-Never bottleneck: A fundamental constraint on language</article-title><source>The Behavioral and Brain Sciences</source><volume>39</volume><elocation-id>e62</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X1500031X</pub-id><pub-id pub-id-type="pmid">25869618</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title><source>The Behavioral and Brain Sciences</source><volume>36</volume><fpage>181</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id><pub-id pub-id-type="pmid">23663408</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connolly</surname><given-names>JF</given-names></name><name><surname>Phillips</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Event-related potential components reflect phonological and semantic processing of the terminal word of spoken sentences</article-title><source>Journal of Cognitive Neuroscience</source><volume>6</volume><fpage>256</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1162/jocn.1994.6.3.256</pub-id><pub-id pub-id-type="pmid">23964975</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coulson</surname><given-names>S</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Van Petten</surname><given-names>C</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Right hemisphere sensitivity to word- and sentence-level context: evidence from event-related brain potentials</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>31</volume><fpage>129</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.31.1.129</pub-id><pub-id pub-id-type="pmid">15641911</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Improved Localizadon of Cortical Activity by Combining EEG and MEG with MRI Cortical Surface Reconstruction: A Linear Approach</article-title><source>Journal of Cognitive Neuroscience</source><volume>5</volume><fpage>162</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1162/jocn.1993.5.2.162</pub-id><pub-id pub-id-type="pmid">23972151</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Estimating sparse spectro-temporal receptive fields with natural stimuli</article-title><source>Network</source><volume>18</volume><fpage>191</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1080/09548980701609235</pub-id><pub-id pub-id-type="pmid">17852750</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Corpus of Contemporary American English</source><publisher-name>MIT Libraries Dataverse</publisher-name><pub-id pub-id-type="doi">10.7910/DVN/AMUDUW</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeLong</surname><given-names>KA</given-names></name><name><surname>Urbach</surname><given-names>TP</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Probabilistic word pre-activation during language comprehension inferred from electrical brain activity</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>1117</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.1038/nn1504</pub-id><pub-id pub-id-type="pmid">16007080</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Blacker</surname><given-names>D</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Maguire</surname><given-names>RP</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name><name><surname>Albert</surname><given-names>MS</given-names></name><name><surname>Killiany</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diaz</surname><given-names>MT</given-names></name><name><surname>Swaab</surname><given-names>TY</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Electrophysiological differentiation of phonological and semantic integration in word and sentence contexts</article-title><source>Brain Research</source><volume>1146</volume><fpage>85</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2006.07.034</pub-id><pub-id pub-id-type="pmid">16952338</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donhauser</surname><given-names>PW</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Two Distinct Neural Timescales for Predictive Speech Processing</article-title><source>Neuron</source><volume>105</volume><fpage>385</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.019</pub-id><pub-id pub-id-type="pmid">31806493</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ettinger</surname><given-names>A</given-names></name><name><surname>Linzen</surname><given-names>T</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The role of morphology in phoneme prediction: evidence from MEG</article-title><source>Brain and Language</source><volume>129</volume><fpage>14</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2013.11.004</pub-id><pub-id pub-id-type="pmid">24486600</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Right words and left words: electrophysiological evidence for hemispheric differences in meaning processing</article-title><source>Brain Research. Cognitive Brain Research</source><volume>8</volume><fpage>373</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1016/s0926-6410(99)00036-1</pub-id><pub-id pub-id-type="pmid">10556614</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Mai</surname><given-names>H</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Both sides get the point: hemispheric sensitivities to sentential constraint</article-title><source>Memory &amp; Cognition</source><volume>33</volume><fpage>871</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.3758/bf03193082</pub-id><pub-id pub-id-type="pmid">16383175</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Thinking ahead: the role and roots of prediction in language comprehension</article-title><source>Psychophysiology</source><volume>44</volume><fpage>491</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.2007.00531.x</pub-id><pub-id pub-id-type="pmid">17521377</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Wlotko</surname><given-names>EW</given-names></name><name><surname>Meyer</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>What’s ‘Right’ in Language Comprehension: Event-Related Potentials Reveal Right Hemisphere Language Capabilities</article-title><source>Language and Linguistics Compass</source><volume>2</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1111/j.1749-818X.2007.00042.x</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>NH</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Morgan</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The influence of categories on perception: explaining the perceptual magnet effect as optimal statistical inference</article-title><source>Psychological Review</source><volume>116</volume><fpage>752</fpage><lpage>782</lpage><pub-id pub-id-type="doi">10.1037/a0017196</pub-id><pub-id pub-id-type="pmid">19839683</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferreira</surname><given-names>F</given-names></name><name><surname>Chantavarin</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integration and Prediction in Language Processing: A Synthesis of Old and New</article-title><source>Current Directions in Psychological Science</source><volume>27</volume><fpage>443</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1177/0963721418794491</pub-id><pub-id pub-id-type="pmid">31130781</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fodor</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Précis of The Modularity of Mind</article-title><source>The Behavioral and Brain Sciences</source><volume>8</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1017/S0140525X0001921X</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forseth</surname><given-names>KJ</given-names></name><name><surname>Hickok</surname><given-names>G</given-names></name><name><surname>Rollo</surname><given-names>PS</given-names></name><name><surname>Tandon</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Language prediction mechanisms in human auditory cortex</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>5240</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-19010-6</pub-id><pub-id pub-id-type="pmid">33067457</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Harvey</surname><given-names>DR</given-names></name><name><surname>Staub</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>No prediction error cost in reading: Evidence from eye movements</article-title><source>Journal of Memory and Language</source><volume>95</volume><fpage>200</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2017.04.007</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The free-energy principle: a unified brain theory?</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nrn2787</pub-id><pub-id pub-id-type="pmid">20068583</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Futrell</surname><given-names>R</given-names></name><name><surname>Gibson</surname><given-names>E</given-names></name><name><surname>Levy</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Lossy-Context Surprisal: An Information-Theoretic Model of Memory Effects in Sentence Processing</article-title><source>Cognitive Science</source><volume>44</volume><elocation-id>e12814</elocation-id><pub-id pub-id-type="doi">10.1111/cogs.12814</pub-id><pub-id pub-id-type="pmid">32100918</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagnepain</surname><given-names>P</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Temporal predictive codes for spoken words in auditory cortex</article-title><source>Current Biology</source><volume>22</volume><fpage>615</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.02.015</pub-id><pub-id pub-id-type="pmid">22425155</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganong</surname><given-names>WF</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Phonetic categorization in auditory word perception</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>6</volume><fpage>110</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1037//0096-1523.6.1.110</pub-id><pub-id pub-id-type="pmid">6444985</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaston</surname><given-names>P</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The time course of contextual cohort effects in auditory processing of category-ambiguous words: MEG evidence for a single “clash” as noun or verb</article-title><source>Language, Cognition and Neuroscience</source><volume>33</volume><fpage>402</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1080/23273798.2017.1395466</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gaston</surname><given-names>P</given-names></name><name><surname>Lau</surname><given-names>E</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>How Does(n’t) Syntactic Context Guide Auditory Word Recognition?</article-title><source>PsyArXiv</source><ext-link ext-link-type="uri" xlink:href="https://psyarxiv.com/sbxpn/">https://psyarxiv.com/sbxpn/</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gazzaniga</surname><given-names>MS</given-names></name><name><surname>Sperry</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>Language after section of the cerebral commissures</article-title><source>Brain</source><volume>90</volume><fpage>131</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1093/brain/90.1.131</pub-id><pub-id pub-id-type="pmid">6023071</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillis</surname><given-names>M</given-names></name><name><surname>Vanthornhout</surname><given-names>J</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural Markers of Speech Comprehension: Measuring EEG Tracking of Linguistic Speech Representations, Controlling the Speech Acoustics</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>10316</fpage><lpage>10329</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0812-21.2021</pub-id><pub-id pub-id-type="pmid">34732519</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>MNE software for processing MEG and EEG data</article-title><source>NeuroImage</source><volume>86</volume><fpage>446</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.027</pub-id><pub-id pub-id-type="pmid">24161808</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Van der Haegen</surname><given-names>L</given-names></name><name><surname>Cai</surname><given-names>Q</given-names></name><name><surname>Stufflebeam</surname><given-names>S</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A surface-based analysis of language lateralization and cortical asymmetry</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1477</fpage><lpage>1492</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00405</pub-id><pub-id pub-id-type="pmid">23701459</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Non-linear processing of a linear speech stream: The influence of morphological structure on the recognition of spoken Arabic words</article-title><source>Brain and Language</source><volume>147</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.bandl.2015.04.006</pub-id><pub-id pub-id-type="pmid">25997171</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hale</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The information conveyed by words in sentences</article-title><source>Journal of Psycholinguistic Research</source><volume>32</volume><fpage>101</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1023/a:1022492123056</pub-id><pub-id pub-id-type="pmid">12690827</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hale</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Information‐theoretical Complexity Metrics</article-title><source>Language and Linguistics Compass</source><volume>10</volume><fpage>397</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1111/lnc3.12196</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halle</surname><given-names>M</given-names></name><name><surname>Stevens</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Speech recognition: A model and a program for research</article-title><source>IEEE Transactions on Information Theory</source><volume>8</volume><fpage>155</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1109/TIT.1962.1057686</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hämäläinen</surname><given-names>MS</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Interpreting magnetic fields of the brain: minimum norm estimates</article-title><source>Medical &amp; Biological Engineering &amp; Computing</source><volume>32</volume><fpage>35</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1007/BF02512476</pub-id><pub-id pub-id-type="pmid">8182960</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>ZS</given-names></name></person-group><year iso-8601-date="1955">1955</year><article-title>From Phoneme to Morpheme</article-title><source>Language</source><volume>31</volume><elocation-id>190</elocation-id><pub-id pub-id-type="doi">10.2307/411036</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Heafield</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><conf-name>KenLM: Faster and Smaller Language Model QueriesProceedings of the 6th Workshop on Statistical Machine Translation</conf-name><article-title>Proceedings of the Sixth Workshop on Statistical Machine Translation</article-title><fpage>187</fpage><lpage>197</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Heeris</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Gammatone Filterbank Toolkit</data-title><version designator="0626328">0626328</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/detly/gammatone">https://github.com/detly/gammatone</ext-link></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heller</surname><given-names>D</given-names></name><name><surname>Parisien</surname><given-names>C</given-names></name><name><surname>Stevenson</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perspective-taking behavior as the probabilistic weighing of multiple domains</article-title><source>Cognition</source><volume>149</volume><fpage>104</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2015.12.008</pub-id><pub-id pub-id-type="pmid">26836401</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1038/nrn2113</pub-id><pub-id pub-id-type="pmid">17431404</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hitczenko</surname><given-names>K</given-names></name><name><surname>Mazuka</surname><given-names>R</given-names></name><name><surname>Elsner</surname><given-names>M</given-names></name><name><surname>Feldman</surname><given-names>NH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>When context is and isn’t helpful: A corpus study of naturalistic speech</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>27</volume><fpage>640</fpage><lpage>676</lpage><pub-id pub-id-type="doi">10.3758/s13423-019-01687-6</pub-id><pub-id pub-id-type="pmid">32166605</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holcomb</surname><given-names>PJ</given-names></name><name><surname>Neville</surname><given-names>HJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Natural speech processing: An analysis using event-related brain potentials</article-title><source>Psychobiology</source><volume>19</volume><fpage>286</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.3758/BF03332082</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huettig</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Four central questions about prediction in language processing</article-title><source>Brain Research</source><volume>1626</volume><fpage>118</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2015.02.014</pub-id><pub-id pub-id-type="pmid">25708148</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaramillo</surname><given-names>S</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The auditory cortex mediates the perceptual effects of acoustic temporal expectation</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>246</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1038/nn.2688</pub-id><pub-id pub-id-type="pmid">21170056</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung-Beeman</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Bilateral brain processes for comprehending natural language</article-title><source>Trends in Cognitive Sciences</source><volume>9</volume><fpage>512</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.09.009</pub-id><pub-id pub-id-type="pmid">16214387</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jurafsky</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A Probabilistic Model of Lexical and Syntactic Access and Disambiguation</article-title><source>Cognitive Science</source><volume>20</volume><fpage>137</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog2002_1</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Gazzaniga</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Processing of semantic anomaly by right and left hemispheres of commissurotomy patients. Evidence from event-related brain potentials</article-title><source>Brain</source><volume>111 (Pt 3)</volume><fpage>553</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1093/brain/111.3.553</pub-id><pub-id pub-id-type="pmid">3382912</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Power</surname><given-names>AJ</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Resolving precise temporal processing properties of the auditory system using continuous stimuli</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>349</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1152/jn.90896.2008</pub-id><pub-id pub-id-type="pmid">19439675</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonard</surname><given-names>MK</given-names></name><name><surname>Baud</surname><given-names>MO</given-names></name><name><surname>Sjerps</surname><given-names>MJ</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Perceptual restoration of masked speech in human cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13619</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13619</pub-id><pub-id pub-id-type="pmid">27996973</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Expectation-based syntactic comprehension</article-title><source>Cognition</source><volume>106</volume><fpage>1126</fpage><lpage>1177</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2007.05.006</pub-id><pub-id pub-id-type="pmid">17662975</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loftus</surname><given-names>GR</given-names></name><name><surname>Masson</surname><given-names>MEJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Using confidence intervals in within-subject designs</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>1</volume><fpage>476</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.3758/BF03210951</pub-id><pub-id pub-id-type="pmid">24203555</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luke</surname><given-names>SG</given-names></name><name><surname>Christianson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Limits on lexical prediction during reading</article-title><source>Cognitive Psychology</source><volume>88</volume><fpage>22</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2016.06.002</pub-id><pub-id pub-id-type="pmid">27376659</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luthra</surname><given-names>S</given-names></name><name><surname>Peraza-Santiago</surname><given-names>G</given-names></name><name><surname>Beeson</surname><given-names>K</given-names></name><name><surname>Saltzman</surname><given-names>D</given-names></name><name><surname>Crinnion</surname><given-names>AM</given-names></name><name><surname>Magnuson</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Robust Lexically Mediated Compensation for Coarticulation: Christmash Time Is Here Again</article-title><source>Cognitive Science</source><volume>45</volume><elocation-id>e12962</elocation-id><pub-id pub-id-type="doi">10.1111/cogs.12962</pub-id><pub-id pub-id-type="pmid">33877697</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lütkenhöner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Magnetoencephalography and its Achilles’ heel</article-title><source>Journal of Physiology, Paris</source><volume>97</volume><fpage>641</fpage><lpage>658</lpage><pub-id pub-id-type="doi">10.1016/j.jphysparis.2004.01.020</pub-id><pub-id pub-id-type="pmid">15242672</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnuson</surname><given-names>JS</given-names></name><name><surname>Mirman</surname><given-names>D</given-names></name><name><surname>Luthra</surname><given-names>S</given-names></name><name><surname>Strauss</surname><given-names>T</given-names></name><name><surname>Harris</surname><given-names>HD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Interaction in Spoken Word Recognition Models: Feedback Helps</article-title><source>Frontiers in Psychology</source><volume>9</volume><elocation-id>369</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2018.00369</pub-id><pub-id pub-id-type="pmid">29666593</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Processing structure of sentence perception</article-title><source>Nature</source><volume>257</volume><fpage>784</fpage><lpage>786</lpage><pub-id pub-id-type="doi">10.1038/257784a0</pub-id><pub-id pub-id-type="pmid">1186856</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Functional parallelism in spoken word-recognition</article-title><source>Cognition</source><volume>25</volume><fpage>71</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(87)90005-9</pub-id><pub-id pub-id-type="pmid">3581730</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matchin</surname><given-names>W</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Hammerly</surname><given-names>C</given-names></name><name><surname>Lau</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The temporal dynamics of structure and content in sentence comprehension: Evidence from fMRI-constrained MEG</article-title><source>Human Brain Mapping</source><volume>40</volume><fpage>663</fpage><lpage>678</lpage><pub-id pub-id-type="doi">10.1002/hbm.24403</pub-id><pub-id pub-id-type="pmid">30259599</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McAuliffe</surname><given-names>M</given-names></name><name><surname>Socolof</surname><given-names>M</given-names></name><name><surname>Mihuc</surname><given-names>S</given-names></name><name><surname>Wagner</surname><given-names>M</given-names></name><name><surname>Sonderegger</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><conf-name>Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi</conf-name><article-title>Interspeech 2017</article-title><fpage>498</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.21437/Interspeech.2017-1386</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCarthy</surname><given-names>G</given-names></name><name><surname>Wood</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Scalp distributions of event-related potentials: An ambiguity associated with analysis of variance models</article-title><source>Electroencephalography and Clinical Neurophysiology</source><volume>61</volume><fpage>S226</fpage><lpage>S227</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(85)90858-2</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Rumelhart</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>An interactive activation model of context effects in letter perception: I. An account of basic findings</article-title><source>Psychological Review</source><volume>88</volume><fpage>375</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.88.5.375</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>The TRACE model of speech perception</article-title><source>Cognitive Psychology</source><volume>18</volume><fpage>1</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(86)90015-0</pub-id><pub-id pub-id-type="pmid">3753912</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Cheung</surname><given-names>C</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phonetic feature encoding in human superior temporal gyrus</article-title><source>Science</source><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id><pub-id pub-id-type="pmid">24482117</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morton</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Interaction of information in word recognition</article-title><source>Psychological Review</source><volume>76</volume><fpage>165</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1037/h0027366</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieuwland</surname><given-names>MS</given-names></name><name><surname>Van Berkum</surname><given-names>JJA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>When peanuts fall in love: N400 evidence for the power of discourse</article-title><source>Journal of Cognitive Neuroscience</source><volume>18</volume><fpage>1098</fpage><lpage>1111</lpage><pub-id pub-id-type="doi">10.1162/jocn.2006.18.7.1098</pub-id><pub-id pub-id-type="pmid">16839284</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieuwland</surname><given-names>MS</given-names></name><name><surname>Politzer-Ahles</surname><given-names>S</given-names></name><name><surname>Heyselaar</surname><given-names>E</given-names></name><name><surname>Segaert</surname><given-names>K</given-names></name><name><surname>Darley</surname><given-names>E</given-names></name><name><surname>Kazanina</surname><given-names>N</given-names></name><name><surname>Von Grebmer Zu Wolfsthurn</surname><given-names>S</given-names></name><name><surname>Bartolozzi</surname><given-names>F</given-names></name><name><surname>Kogan</surname><given-names>V</given-names></name><name><surname>Ito</surname><given-names>A</given-names></name><name><surname>Mézière</surname><given-names>D</given-names></name><name><surname>Barr</surname><given-names>DJ</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Ferguson</surname><given-names>HJ</given-names></name><name><surname>Busch-Moreno</surname><given-names>S</given-names></name><name><surname>Fu</surname><given-names>X</given-names></name><name><surname>Tuomainen</surname><given-names>J</given-names></name><name><surname>Kulakova</surname><given-names>E</given-names></name><name><surname>Husband</surname><given-names>EM</given-names></name><name><surname>Donaldson</surname><given-names>DI</given-names></name><name><surname>Kohút</surname><given-names>Z</given-names></name><name><surname>Rueschemeyer</surname><given-names>SA</given-names></name><name><surname>Huettig</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Large-scale replication study reveals a limit on probabilistic prediction in language comprehension</article-title><source>eLife</source><volume>7</volume><elocation-id>e33468</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.33468</pub-id><pub-id pub-id-type="pmid">29631695</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieuwland</surname><given-names>MS</given-names></name><name><surname>Barr</surname><given-names>DJ</given-names></name><name><surname>Bartolozzi</surname><given-names>F</given-names></name><name><surname>Busch-Moreno</surname><given-names>S</given-names></name><name><surname>Darley</surname><given-names>E</given-names></name><name><surname>Donaldson</surname><given-names>DI</given-names></name><name><surname>Ferguson</surname><given-names>HJ</given-names></name><name><surname>Fu</surname><given-names>X</given-names></name><name><surname>Heyselaar</surname><given-names>E</given-names></name><name><surname>Huettig</surname><given-names>F</given-names></name><name><surname>Matthew Husband</surname><given-names>E</given-names></name><name><surname>Ito</surname><given-names>A</given-names></name><name><surname>Kazanina</surname><given-names>N</given-names></name><name><surname>Kogan</surname><given-names>V</given-names></name><name><surname>Kohút</surname><given-names>Z</given-names></name><name><surname>Kulakova</surname><given-names>E</given-names></name><name><surname>Mézière</surname><given-names>D</given-names></name><name><surname>Politzer-Ahles</surname><given-names>S</given-names></name><name><surname>Rousselet</surname><given-names>G</given-names></name><name><surname>Rueschemeyer</surname><given-names>SA</given-names></name><name><surname>Segaert</surname><given-names>K</given-names></name><name><surname>Tuomainen</surname><given-names>J</given-names></name><name><surname>Von Grebmer Zu Wolfsthurn</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dissociable effects of prediction and integration during language comprehension: evidence from a large-scale study using brain potentials</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>375</volume><elocation-id>20180522</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2018.0522</pub-id><pub-id pub-id-type="pmid">31840593</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Shortlist: a connectionist model of continuous speech recognition</article-title><source>Cognition</source><volume>52</volume><fpage>189</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(94)90043-4</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name><name><surname>Cutler</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Merging information in speech recognition: feedback is never necessary</article-title><source>The Behavioral and Brain Sciences</source><volume>23</volume><fpage>299</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1017/s0140525x00003241</pub-id><pub-id pub-id-type="pmid">11301575</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Shortlist B: a Bayesian model of continuous speech recognition</article-title><source>Psychological Review</source><volume>115</volume><fpage>357</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.115.2.357</pub-id><pub-id pub-id-type="pmid">18426294</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name><name><surname>Cutler</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prediction, Bayesian inference and feedback in speech recognition</article-title><source>Language, Cognition and Neuroscience</source><volume>31</volume><fpage>4</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1080/23273798.2015.1081703</pub-id><pub-id pub-id-type="pmid">26740960</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title><source>Neuropsychologia</source><volume>9</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id><pub-id pub-id-type="pmid">5146491</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pickering</surname><given-names>MJ</given-names></name><name><surname>Gambi</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predicting while comprehending language: A theory and review</article-title><source>Psychological Bulletin</source><volume>144</volume><fpage>1002</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1037/bul0000158</pub-id><pub-id pub-id-type="pmid">29952584</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Pure word deafness and the bilateral processing of the speech code</article-title><source>Cognitive Science</source><volume>25</volume><fpage>679</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog2505_3</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pollan</surname><given-names>M.</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>The Botany of Desire: A Plant’s-Eye View of the World</source><publisher-name>Random House Publishing Group</publisher-name></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rommers</surname><given-names>J</given-names></name><name><surname>Meyer</surname><given-names>AS</given-names></name><name><surname>Praamstra</surname><given-names>P</given-names></name><name><surname>Huettig</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The contents of predictions in sentence comprehension: activation of the shape of objects before they are referred to</article-title><source>Neuropsychologia</source><volume>51</volume><fpage>437</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.12.002</pub-id><pub-id pub-id-type="pmid">23238371</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname><given-names>JR</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Statistical learning by 8-month-old infants</article-title><source>Science</source><volume>274</volume><fpage>1926</fpage><lpage>1928</lpage><pub-id pub-id-type="doi">10.1126/science.274.5294.1926</pub-id><pub-id pub-id-type="pmid">8943209</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salverda</surname><given-names>AP</given-names></name><name><surname>Dahan</surname><given-names>D</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The role of prosodic boundaries in the resolution of lexical embedding in speech comprehension</article-title><source>Cognition</source><volume>90</volume><fpage>51</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(03)00139-2</pub-id><pub-id pub-id-type="pmid">14597270</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schmitt</surname><given-names>LM</given-names></name><name><surname>Erb</surname><given-names>J</given-names></name><name><surname>Tune</surname><given-names>S</given-names></name><name><surname>Rysop</surname><given-names>A</given-names></name><name><surname>Hartwigsen</surname><given-names>G</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Predicting Speech from a Cortical Hierarchy of Event-Based Timescales</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.12.19.423616</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Teramoto</surname><given-names>Y</given-names></name><name><surname>Willmore</surname><given-names>BD</given-names></name><name><surname>Schnupp</surname><given-names>JW</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sensory cortex is optimized for prediction of future input</article-title><source>eLife</source><volume>7</volume><elocation-id>e31557</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31557</pub-id><pub-id pub-id-type="pmid">29911971</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Threshold-free cluster enhancement: addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><volume>44</volume><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.061</pub-id><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The effect of word predictability on reading time is logarithmic</article-title><source>Cognition</source><volume>128</volume><fpage>302</fpage><lpage>319</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2013.02.013</pub-id><pub-id pub-id-type="pmid">23747651</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rapid computations of spectrotemporal prediction error support perception of degraded speech</article-title><source>eLife</source><volume>9</volume><elocation-id>e58077</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.58077</pub-id><pub-id pub-id-type="pmid">33147138</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swinney</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Lexical access during sentence comprehension: (Re)consideration of context effects</article-title><source>Journal of Verbal Learning and Verbal Behavior</source><volume>18</volume><fpage>645</fpage><lpage>659</lpage><pub-id pub-id-type="doi">10.1016/S0022-5371(79)90355-4</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabas</surname><given-names>A</given-names></name><name><surname>Kriegstein</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Adjudicating Between Local and Global Architectures of Predictive Processing in the Subcortical Auditory Pathway</article-title><source>Frontiers in Neural Circuits</source><volume>15</volume><elocation-id>644743</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2021.644743</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanenhaus</surname><given-names>MK</given-names></name><name><surname>Spivey-Knowlton</surname><given-names>MJ</given-names></name><name><surname>Eberhard</surname><given-names>KM</given-names></name><name><surname>Sedivy</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Integration of visual and linguistic information in spoken language comprehension</article-title><source>Science</source><volume>268</volume><fpage>1632</fpage><lpage>1634</lpage><pub-id pub-id-type="doi">10.1126/science.7777863</pub-id><pub-id pub-id-type="pmid">7777863</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname><given-names>S</given-names></name><name><surname>Simola</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements</article-title><source>Physics in Medicine and Biology</source><volume>51</volume><fpage>1759</fpage><lpage>1768</lpage><pub-id pub-id-type="doi">10.1088/0031-9155/51/7/008</pub-id><pub-id pub-id-type="pmid">16552102</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallat</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pingouin: statistics in Python</article-title><source>Journal of Open Source Software</source><volume>3</volume><elocation-id>1026</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01026</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Berkum</surname><given-names>JJA</given-names></name><name><surname>Zwitserlood</surname><given-names>P</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>Brown</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>When and how do listeners relate a sentence to the wider discourse? Evidence from the N400 effect</article-title><source>Brain Research. Cognitive Brain Research</source><volume>17</volume><fpage>701</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1016/s0926-6410(03)00196-4</pub-id><pub-id pub-id-type="pmid">14561457</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Berkum</surname><given-names>JJA</given-names></name><name><surname>Brown</surname><given-names>CM</given-names></name><name><surname>Zwitserlood</surname><given-names>P</given-names></name><name><surname>Kooijman</surname><given-names>V</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Anticipating upcoming words in discourse: evidence from ERPs and reading times</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>31</volume><fpage>443</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.31.3.443</pub-id><pub-id pub-id-type="pmid">15910130</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Petten</surname><given-names>C</given-names></name><name><surname>Coulson</surname><given-names>S</given-names></name><name><surname>Rubin</surname><given-names>S</given-names></name><name><surname>Plante</surname><given-names>E</given-names></name><name><surname>Parks</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Time course of word identification and semantic integration in spoken language</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>25</volume><fpage>394</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1037//0278-7393.25.2.394</pub-id><pub-id pub-id-type="pmid">10093207</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vitevitch</surname><given-names>MS</given-names></name><name><surname>Luce</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Probabilistic Phonotactics and Neighborhood Activation in Spoken Word Recognition</article-title><source>Journal of Memory and Language</source><volume>40</volume><fpage>374</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1006/jmla.1998.2618</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vitevitch</surname><given-names>MS</given-names></name><name><surname>Luce</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>When Words Compete: Levels of Processing in Perception of Spoken Words</article-title><source>Psychological Science</source><volume>9</volume><fpage>325</fpage><lpage>329</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00064</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waskom</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>seaborn: statistical data visualization</article-title><source>Journal of Open Source Software</source><volume>6</volume><elocation-id>3021</elocation-id><pub-id pub-id-type="doi">10.21105/joss.03021</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weissbart</surname><given-names>H</given-names></name><name><surname>Kandylaki</surname><given-names>KD</given-names></name><name><surname>Reichenbach</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical Tracking of Surprisal during Continuous Speech Comprehension</article-title><source>Journal of Cognitive Neuroscience</source><volume>32</volume><fpage>155</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01467</pub-id><pub-id pub-id-type="pmid">31479349</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willems</surname><given-names>RM</given-names></name><name><surname>Frank</surname><given-names>SL</given-names></name><name><surname>Nijhof</surname><given-names>AD</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>Bosch</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prediction During Natural Language Comprehension</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>2506</fpage><lpage>2516</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhv075</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>SM</given-names></name><name><surname>Bautista</surname><given-names>A</given-names></name><name><surname>McCarron</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Convergence of spoken and written language processing in the superior temporal sulcus</article-title><source>NeuroImage</source><volume>171</volume><fpage>62</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.12.068</pub-id><pub-id pub-id-type="pmid">29277646</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wlotko</surname><given-names>EW</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Finding the right word: hemispheric asymmetries in the use of sentence context information</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>3001</fpage><lpage>3014</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.05.013</pub-id><pub-id pub-id-type="pmid">17659309</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zwitserlood</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>The locus of the effects of sentential-semantic context in spoken-word processing</article-title><source>Cognition</source><volume>32</volume><fpage>25</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(89)90013-9</pub-id><pub-id pub-id-type="pmid">2752705</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72056.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>van Wassenhove</surname><given-names>Virginie</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>CEA, DRF/I2BM, NeuroSpin; INSERM, U992, Cognitive Neuroimaging Unit</institution><country>France</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.07.03.450698" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.03.450698"/></front-stub><body><p>To comprehend speech efficiently, the brain predicts what comes next as sentences unfold. In this study, Brodbeck and colleagues asked at which scale predictive processing helps the analysis of speech. The authors combined magnetoencephalography with state-of-the-art analyses (multivariate Temporal Response Functions) and information-theoretic measures (entropy, surprisal) to test distinct contextual speech models at three hierarchical processing levels. The authors report evidence for the coexistence of hierarchical and parallel speech processing supporting the independent contribution of local (e.g. sublexical) and global (e.g. sentences) contextual probabilities to the analysis of speech.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72056.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>van Wassenhove</surname><given-names>Virginie</given-names></name><role>Reviewing Editor</role><aff><institution>CEA, DRF/I2BM, NeuroSpin; INSERM, U992, Cognitive Neuroimaging Unit</institution><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Brennan</surname><given-names>Jonathan</given-names></name><role>Reviewer</role><aff><institution>University of Michigan</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.07.03.450698">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.07.03.450698v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Parallel processing in speech perception: Local and global representations of linguistic context&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Barbara Shinn-Cunningham as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Jonathan Brennan (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>The sample size (N=12) appears like a low number, and authors should rationalize their sample choice with a power analysis, eventually illustrate with single-participant level data or explain why, in light of the paradigmatic strategy and analyses performed, this sample size is reasonable.</p><p>Reviewer 2 suggested that yuo contextualize a bit better how outcomes of Figure 7 fits other models (e.g. TRACE vs. RACE) and how the authors' novel observations update or modify existing architectures in the field.</p><p>Reviewer 3 questioned in his first main comment the choice of number of terms in the models being tested. The authors may wish to carefully address possible shortcomings of how more global models may leave room for local models to capture variance.</p><p>Reviewer 1 would like to see some justifications about why the analysis of phase-locked activity vs. induced responses is informative, and whether the latter could reveal additional insights if at all.</p><p>Additional suggestions made by all Reviewers should help clarify and streamline the manuscript further. Please keep in mind that the audience in <italic>eLife</italic> is diverse and readers may not necessarily be expert in (neuro)linguistics or technically versed with MEG. The overall flow of the manuscript can be streamlined a bit so as to clarify the complexity of some analyses that Reviewers 2 and 3 pointed out (some snippets are provided by Reviewer 1).</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I had a few comments that I hope might help to make this paper even more impactful.</p><p>First, Figure 7 offers a boxology of the parallel processing architecture the authors believe is consistent with the data. Overall I'm pretty sympathetic to this view, but I would have liked to see the Discussion section better connect these conclusions with the existing literature. As presented, the reader might take the Figure 7 architecture to be a totally new model. I think it would be more appropriate to see how this updates or refines existing models. Specifically, I found myself reading the Discussion section through the lens of the late 90s debate on lexical access, specifically the TRACE model with fully interactive access, as compared to the RACE model of fully bottom-up access. I think the existing model can be recast as an extension of TRACE, but perhaps with the addition of &quot;outputs&quot; at each intermediate level (not just at the top?) I may not be exactly right here, but the upshot is I'd appreciate some extra handholding here for the reader to see how this architecture updates existing theories.</p><p>Second, the right lateralization of lower level effects seems to warrant further discussion. The interpretatin of these seems to emphasize the bilateral nature of speech perception – no arguments there – but the data actually favor a right-hemisphere bias which is unexpected to me (cf. the Giraud and Poeppel model for speech perception placed phoneme-level analysis predominantly in the left hemisphere).</p><p>Third, at N=12 the sample size is relatively low for 2021, and some key statistics are only reported as t_max. Together, I'm a little concerned that this may be a bit anti-conservative. At the least, I would like to see the statistics for reliable effects reproted as ranges (t_min – t_max). Increasing the N of the study would be great, but I understand if it is not feasible.</p><p>Figure 1: Where does meaning(j,i) come from? The red coloring seems to indicate it is the output of the sentence-level box, but that isn't clear to me from the sentence(i,j) notation.</p><p>ln. 255-256 – &quot;While surprisal depends on the conditional probability of a discrete event and is agnostic to the underlying unit of representation&quot;. I don't understand this point. Both surprisal and entropy are calculated over distributions of some particular representation (P(phoneme_i|phoneme_i-1) ! = P(phoneme_i|word_j)… P(phoneme|…) ! = P(word|…)) I'm afraid I'm missing the intended point.</p><p>ln 702-704: I'm having trouble understanding the test for localization differences. I gather that the analysis takes source amplitude differences (180 or 176) per participant, and subjected these to a one-way anova, which was repeated for each pair of conditions. If so, shouldn't the DF for the F-test be (179, 11) or (175,11)? Instead, ln. 294-295 gives F(175,1925) and F(179 , 1969). I don't understand where that residual DF is coming from.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72056.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The sample size (N=12) appears like a low number, and authors should rationalize their sample choice with a power analysis, eventually illustrate with single-participant level data or explain why, in light of the paradigmatic strategy and analyses performed, this sample size is reasonable.</p></disp-quote><p>We appreciate and share the reviewers’ concern with statistical power and have made several modifications to better explain and rationalize our choices.</p><p>First, to contextualize our study: The sample size is similar to the most comparable published study, which had 11 participants <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?hOTWkZ">(Donhauser</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?hOTWkZ">and</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?hOTWkZ">Baillet,</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?hOTWkZ">2020)</ext-link>. Our own previous study <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?RI2Lpp">(Brodbeck</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?RI2Lpp">et</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?RI2Lpp">al.,</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?RI2Lpp">2018)</ext-link> had more participants (28) but only a fraction of the data per subject (8 minutes of speech in quiet, vs. 47 minutes in the present dataset). We added this consideration to the Methods/Participants section.</p><p>We also added a table with effect-sizes for all the main predictors to make that information more accessible (Table 1). This suggests that the most relevant effects have Cohen’s <italic>d</italic> &gt; 1. With our sample size 12, we had 94% power to detect an effect with <italic>d</italic> = 1, and 99% power to detect an effect with <italic>d</italic> = 1.2. This post-hoc analysis suggests that our sample was adequately powered for the intended purpose.</p><p>Finally, all crucial model comparisons are accompanied by swarm-plots that show each subject as a separate dot, thus showing that these comparisons are highly reproducible across participants (note that there rarely are participants with model difference below 0, indicating that the effects are all seen in most subjects).</p><disp-quote content-type="editor-comment"><p>Reviewer 2 suggested that yuo contextualize a bit better how outcomes of Figure 7 fits other models (e.g. TRACE vs. RACE) and how the authors' novel observations update or modify existing architectures in the field.</p></disp-quote><p>Please see the corresponding section for Reviewer #2 (Recommendations for the authors).</p><disp-quote content-type="editor-comment"><p>Reviewer 3 questioned in his first main comment the choice of number of terms in the models being tested. The authors may wish to carefully address possible shortcomings of how more global models may leave room for local models to capture variance.</p></disp-quote><p>Please see the corresponding section for Reviewer #3 (Public Review).</p><disp-quote content-type="editor-comment"><p>Reviewer 1 would like to see some justifications about why the analysis of phase-locked activity vs. induced responses is informative, and whether the latter could reveal additional insights if at all.</p></disp-quote><p>Please see the corresponding section for Reviewer #1 (Public Review).</p><disp-quote content-type="editor-comment"><p>Additional suggestions made by all Reviewers should help clarify and streamline the manuscript further. Please keep in mind that the audience in eLife is diverse and readers may not necessarily be expert in (neuro)linguistics or technically versed with MEG. The overall flow of the manuscript can be streamlined a bit so as to clarify the complexity of some analyses that Reviewers 2 and 3 pointed out (some snippets are provided by Reviewer 1).</p></disp-quote><p>We thank the reviewers for many great suggestions to make the manuscript more accessible. We have revised the manuscript to reduce reliance on terminology, and incorporated all the other suggestions (see the responses to the many individual suggestions below).</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I had a few comments that I hope might help to make this paper even more impactful.</p><p>First, Figure 7 offers a boxology of the parallel processing architecture the authors believe is consistent with the data. Overall I'm pretty sympathetic to this view, but I would have liked to see the Discussion section better connect these conclusions with the existing literature. As presented, the reader might take the Figure 7 architecture to be a totally new model. I think it would be more appropriate to see how this updates or refines existing models. Specifically, I found myself reading the Discussion section through the lens of the late 90s debate on lexical access, specifically the TRACE model with fully interactive access, as compared to the RACE model of fully bottom-up access. I think the existing model can be recast as an extension of TRACE, but perhaps with the addition of &quot;outputs&quot; at each intermediate level (not just at the top?) I may not be exactly right here, but the upshot is I'd appreciate some extra handholding here for the reader to see how this architecture updates existing theories.</p></disp-quote><p>We have added a <italic>Discussion section</italic> called <italic>Implications for word recognition</italic> to discuss implications for existing models more explicitly. We are reluctant to draw stronger comparisons with computational models such as TRACE, because such models might have emergent properties that are not straightforward to deduce from their architecture, and instead require careful analysis of model behavior <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?cGVYCR">(e.g.</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?cGVYCR">Luthra</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?cGVYCR">et</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?cGVYCR">al.,</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?cGVYCR">2021)</ext-link>. However, we share the reviewer’s interest and such work is underway <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?MsibuE">(e.g. Brodbeck</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?MsibuE">et al., 2021)</ext-link><bold>.</bold></p><disp-quote content-type="editor-comment"><p>Second, the right lateralization of lower level effects seems to warrant further discussion. The interpretation of these seems to emphasize the bilateral nature of speech perception – no arguments there – but the data actually favor a right-hemisphere bias which is unexpected to me (cf. the Giraud and Poeppel model for speech perception placed phoneme-level analysis predominantly in the left hemisphere).</p></disp-quote><p>You are right to point this out. In order to verify this observation we performed an additional model test of linguistic processing in general (test the predictive power of the full model compared with a model excluding all linguistic predictors). It turns out that overall, linguistic processing is not significantly lateralized. This might seem counterintuitive given the significant lateralization of two out of three context models. However, it is important that the tests of the individual models partial out variability that can <italic>only</italic> be explained by the respective model, and so the test for the combined linguistic model thus likely explains more than the sum of the three individual comparisons, because it also includes variability that is shared between two or more of the individual models. We have added this test to the relevant Results section (Different context models affect different neural processes).</p><p>An additional consideration is that our method for estimating speech tracking is disproportionately sensitive to slow cortical frequencies below 10 Hz <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?0DONFy">(Ding</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?0DONFy">et</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?0DONFy">al.,</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?0DONFy">2014)</ext-link> and such low frequencies might be inherently stronger in the right hemisphere <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?WeD3ez">(Giraud</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?WeD3ez">et</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?WeD3ez">al.,</ext-link> <ext-link ext-link-type="uri" xlink:href="https://www.zotero.org/google-docs/?WeD3ez">2007)</ext-link>. In our interpretation we thus emphasize relative patterns of lateralization, and are cautious about interpreting the absolute lateralization. Most importantly, our results suggest that speech perception is bilateral, but with different properties in each hemisphere, in a way that is consistent with findings based on a different methodology (as discussed under <italic>Bilateral pathways to speech comprehension</italic>).</p><disp-quote content-type="editor-comment"><p>Third, at N=12 the sample size is relatively low for 2021, and some key statistics are only reported as t_max. Together, I'm a little concerned that this may be a bit anti-conservative. At the least, I would like to see the statistics for reliable effects reported as ranges (t_min – t_max). Increasing the N of the study would be great, but I understand if it is not feasible.</p></disp-quote><p>Please see response in Essential revisions.</p><disp-quote content-type="editor-comment"><p>Figure 1: Where does meaning(j,i) come from? The red coloring seems to indicate it is the output of the sentence-level box, but that isn't clear to me from the sentence(i,j) notation.</p></disp-quote><p>Thank you for noticing this inconsistency, it should have said sentence(j-1) to invoke the last state of the higher level.</p><disp-quote content-type="editor-comment"><p>ln. 255-256 – &quot;While surprisal depends on the conditional probability of a discrete event and is agnostic to the underlying unit of representation&quot;. I don't understand this point. Both surprisal and entropy are calculated over distributions of some particular representation (P(phoneme_i|phoneme_i-1) ! = P(phoneme_i|word_j)… P(phoneme|…) ! = P(word|…)) I'm afraid I'm missing the intended point.</p></disp-quote><p>This is indeed a tricky point to explain without equations, and we did not do it justice. We have made the relevant section more explicit, and we have also added the details with equations to the Methods section (<italic>Lexical context model</italic> subsection) and point the reader to this section from the main text. Based on a suggestion from Reviewer 3 we have also added the formal definitions to the Introduction, which further clarifies the distinction between phoneme and cohort entropy.</p><disp-quote content-type="editor-comment"><p>ln 702-704: I'm having trouble understanding the test for localization differences. I gather that the analysis takes source amplitude differences (180 or 176) per participant, and subjected these to a one-way anova, which was repeated for each pair of conditions. If so, shouldn't the DF for the F-test be (179, 11) or (175,11)? Instead, ln. 294-295 gives F(175,1925) and F(179 , 1969). I don't understand where that residual DF is coming from.</p></disp-quote><p>Please note that these are within-subject ANOVAs, so the denominator df is (n_subjects – 1) • (n_treatments – 1) = 11 • 175 = 1925 and 11 • 179 = 1969, respectively (e.g. Rutherford, 2001, p. 71).</p><p>References</p><p>Brodbeck C, Gaston P, Luthra S, Magnuson JS. 2021. Discovering computational principles in models and brains.</p><p>Brodbeck C, Hong LE, Simon JZ. 2018. Rapid Transformation from Auditory to Linguistic Representations of Continuous Speech. Curr Biol 28:3976-3983.e5.</p><p>doi:10.1016/j.cub.2018.10.042</p><p>Ding N, Chatterjee M, Simon JZ. 2014. Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure. NeuroImage 88:41–46.</p><p>doi:10.1016/j.neuroimage.2013.10.054</p><p>Donhauser PW, Baillet S. 2020. Two Distinct Neural Timescales for Predictive Speech Processing. Neuron 105:385-393.e9. doi:10.1016/j.neuron.2019.10.019</p><p>Giraud A-L, Kleinschmidt A, Poeppel D, Lund TE, Frackowiak RSJ, Laufs H. 2007. Endogenous Cortical Rhythms Determine Cerebral Specialization for Speech Perception and Production. Neuron 56:1127–1134. doi:10.1016/j.neuron.2007.09.038</p><p>Gulordava K, Bojanowski P, Grave E, Linzen T, Baroni M. 2018. Colorless Green Recurrent Networks Dream HierarchicallyProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Presented at the NAACL-HLT 2018. New Orleans, Louisiana: Association for Computational Linguistics. pp. 1195–1205. doi:10.18653/v1/N18-1108</p><p>Luthra S, Li MYC, You H, Brodbeck C, Magnuson JS. 2021. Does signal reduction imply predictive coding in models of spoken word recognition? Psychon Bull Rev.</p><p>doi:10.3758/s13423-021-01924-x</p><p>Rutherford A. 2001. Introducing ANOVA and ANCOVA: a GLM approach, Introducing statistical methods. London ; Thousand Oaks, Calif.: SAGE.</p><p>Shain C, Blank IA, van Schijndel M, Schuler W, Fedorenko E. 2020. fMRI reveals language-specific predictive coding during naturalistic sentence comprehension. Neuropsychologia 138:107307. doi:10.1016/j.neuropsychologia.2019.107307</p><p>Willems RM, Van der Haegen L, Fisher SE, Francks C. 2014. On the other hand: including left-handers in cognitive neuroscience and neurogenetics. Nat Rev Neurosci 15:193–201. doi:10.1038/nrn3679</p></body></sub-article></article>