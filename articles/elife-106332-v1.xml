<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">106332</article-id><article-id pub-id-type="doi">10.7554/eLife.106332</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Active vision of bees in a simple pattern discrimination task</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7612-6465</contrib-id><email>maboudi@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Richter</surname><given-names>Jasmin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0003-9534-7903</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Guiraud</surname><given-names>Marie-Geneviève</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5843-9188</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Roper</surname><given-names>Mark</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1135-6187</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author"><name><surname>Marshall</surname><given-names>James AR</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1506-167X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Chittka</surname><given-names>Lars</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8153-1732</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/026zzn846</institution-id><institution>School of Biological and Behavioural Sciences, Queen Mary University of London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>Department of Computer Science, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>School of Biosciences, University of Sheffield</institution></institution-wrap><addr-line><named-content content-type="city">Sheffield</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sf06y89</institution-id><institution>School of Natural Sciences, Macquarie University</institution></institution-wrap><addr-line><named-content content-type="city">Sydney</named-content></addr-line><country>Australia</country></aff><aff id="aff5"><label>5</label><institution>Drone Development Lab, Ben Thorns Ltd</institution><addr-line><named-content content-type="city">Colchester</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Desplan</surname><given-names>Claude</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Desplan</surname><given-names>Claude</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>10</day><month>04</month><year>2025</year></pub-date><volume>14</volume><elocation-id>e106332</elocation-id><history><date date-type="received" iso-8601-date="2025-02-07"><day>07</day><month>02</month><year>2025</year></date><date date-type="accepted" iso-8601-date="2025-02-24"><day>24</day><month>02</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-03-10"><day>10</day><month>03</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.03.09.434580"/></event></pub-history><permissions><copyright-statement>© 2025, MaBouDi et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>MaBouDi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-106332-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-106332-figures-v1.pdf"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/elife.89929" id="ra1"/><abstract><p>Active vision, the sensory-motor process through which animals dynamically adjust visual input to sample and prioritise relevant information via photoreceptors, eyes, head, and body movements, is well-documented across species. In small-brained animals like insects, where parallel processing may be limited, active vision for sequential acquisition of visual features might be even more important. We investigated how bumblebees use active vision to distinguish between two visual patterns: a multiplication sign and its 45°-rotated variant, a plus sign. By allowing bees to freely inspect patterns, we analysed their flight paths, inspection times, velocities and regions of focus through high-speed videography. We observed that bees tended to inspect only a small region of each pattern, with a preference for lower and left-side sections, before accurately accepting target or rejecting distractor patterns. The specific pattern areas scanned differed between the plus and multiplication signs, yet flight behaviour remained consistent and specific to each pattern, regardless of whether the pattern was rewarding or punishing. Transfer tests showed that bees could generalise their pattern recognition to partial cues, maintaining scanning strategies and selective attention to learned regions. These findings highlight active vision as a crucial aspect of bumblebees' visual processing, where selective scanning behaviours during flight enhance discrimination accuracy and enable efficient environmental analysis and visual encoding.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>scanning behaviour</kwd><kwd>visual recognition</kwd><kwd>Bombus terrestris</kwd><kwd>lateralisation</kwd><kwd>flight analysis</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000854</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGP0022/2014</award-id><principal-award-recipient><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><name><surname>Guiraud</surname><given-names>Marie-Geneviève</given-names></name><name><surname>Roper</surname><given-names>Mark</given-names></name><name><surname>Chittka</surname><given-names>Lars</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>Programme Grant Brains on Board (EP/P006094/1)</award-id><principal-award-recipient><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><name><surname>Marshall</surname><given-names>James AR</given-names></name><name><surname>Chittka</surname><given-names>Lars</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100018693</institution-id><institution>HORIZON EUROPE Framework Programme</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Chittka</surname><given-names>Lars</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Bumblebees refine visual discrimination through selective and structured scanning, demonstrating a dynamic interplay between movement and perception in pattern recognition.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Active vision refers to a closed loop of sensory-motor process wherein animals actively manipulate their visual input, optimising perception and decision-making. In some vertebrates, the repertoire of such active vision strategies is well researched (<xref ref-type="bibr" rid="bib43">Land, 1999</xref>; <xref ref-type="bibr" rid="bib44">Land and Nilsson, 2012</xref>; <xref ref-type="bibr" rid="bib89">Yarbus, 2013</xref>). To scan visual targets, there can be large-scale movement by the body or head, or smaller scale movements of the eyes (saccades) or photoreceptors (<xref ref-type="bibr" rid="bib36">Juusola and Song, 2017</xref>; <xref ref-type="bibr" rid="bib59">Najemnik and Geisler, 2005</xref>; <xref ref-type="bibr" rid="bib88">Yang and Chiao, 2016</xref>) as well as subtle head movements or head-bobbing, which induce motion parallax to assess distances (<xref ref-type="bibr" rid="bib16">Dutta et al., 2020</xref>; <xref ref-type="bibr" rid="bib15">Dutta and Maor, 2007</xref>; <xref ref-type="bibr" rid="bib41">Kral, 2003</xref>; <xref ref-type="bibr" rid="bib40">Kral, 1998</xref>; <xref ref-type="bibr" rid="bib71">Sobel, 1990</xref>). Such active vision is essential for obtaining an accurate three-dimensional representation of the surrounding environment (<xref ref-type="bibr" rid="bib38">Kagan, 2012</xref>; <xref ref-type="bibr" rid="bib39">Kemppainen et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Martinez-Conde et al., 2013</xref>; <xref ref-type="bibr" rid="bib57">Martinez-Conde and Macknik, 2008</xref>; <xref ref-type="bibr" rid="bib87">Werner et al., 2016</xref>). In certain vertebrates, eye movements are also used as a sampling strategy, enhancing the encoding of high spatial frequency of natural stimuli and providing fine spatial information (<xref ref-type="bibr" rid="bib1">Anderson et al., 2020</xref>; ; <xref ref-type="bibr" rid="bib42">Kuang et al., 2012</xref>; <xref ref-type="bibr" rid="bib67">Rucci and Poletti, 2015</xref>). Some animals adopt characteristic routes during visual tasks to facilitate target recognition (<xref ref-type="bibr" rid="bib70">Skorupski and Chittka, 2017</xref>; <xref ref-type="bibr" rid="bib12">Dawkins and Woodington, 2000</xref>; <xref ref-type="bibr" rid="bib45">Langridge et al., 2021</xref>; <xref ref-type="bibr" rid="bib46">Lehrer et al., 1985</xref>). For instance, individual chickens exhibit stereotyped approach paths when learning to discriminate visual patterns, consistently using the same path and side relative to the object during each trial (<xref ref-type="bibr" rid="bib12">Dawkins and Woodington, 2000</xref>). However, these paths are not identical across all chickens, as each bird develops its own distinct approach strategy. Interestingly, they fail at these tasks when their developed route is disrupted. Additionally, characteristic head movements have been observed in pigeons during image stabilisation for forward locomotion (<xref ref-type="bibr" rid="bib82">Theunissen and Troje, 2017</xref>).</p><p>In insects, which possess miniature brains and, thus possibly more limited parallel processing, there appears to be a heightened necessity for sequential scanning to acquire spatial information compared to animals with larger brains (<xref ref-type="bibr" rid="bib7">Chittka and Niven, 2009</xref>; <xref ref-type="bibr" rid="bib9">Chittka and Skorupski, 2011</xref>; <xref ref-type="bibr" rid="bib37">Juusola et al., 2025</xref>; <xref ref-type="bibr" rid="bib52">MaBouDi et al., 2020a</xref>; <xref ref-type="bibr" rid="bib73">Spaethe et al., 2006</xref>). Indeed, in bumblebees, there is evidence that complex patterns cannot be discriminated when they are only briefly flashed on a screen, preventing bees from sampling in a continuous scan (<xref ref-type="bibr" rid="bib61">Nityananda et al., 2014</xref>). <xref ref-type="bibr" rid="bib45">Langridge et al., 2021</xref> explored how bumblebees' approach direction prior to landing affects their colour learning, showing that bees tend to remember the colour they predominantly face during their approach, which influences their learned preferences for floral patterns. Furthermore, bees exhibit defined sequences of body movements in response to particular visual stimuli (<xref ref-type="bibr" rid="bib10">Collett et al., 1993</xref>; <xref ref-type="bibr" rid="bib24">Guiraud et al., 2018</xref>; <xref ref-type="bibr" rid="bib47">Lehrer, 1994</xref>; <xref ref-type="bibr" rid="bib52">MaBouDi et al., 2020a</xref>; <xref ref-type="bibr" rid="bib53">MaBouDi et al., 2020b</xref>; <xref ref-type="bibr" rid="bib87">Werner et al., 2016</xref>). This deterministic relationship between movement and perception suggests that active vision allows movement to shape the sensory information acquired and influences perception.</p><p>Extensive studies have demonstrated that bees are capable of memorising and discriminating a wide variety of visual patterns, including complex ones that, for example, include different stripe orientations in each of four quadrants (<xref ref-type="bibr" rid="bib3">Avarguès-Weber et al., 2011</xref>; <xref ref-type="bibr" rid="bib5">Benard et al., 2006</xref>; <xref ref-type="bibr" rid="bib17">Dyer et al., 2005</xref>; <xref ref-type="bibr" rid="bib26">Guiraud et al., 2025a</xref>; <xref ref-type="bibr" rid="bib78">Srinivasan, 2010</xref>; <xref ref-type="bibr" rid="bib76">Srinivasan, 1994</xref>; <xref ref-type="bibr" rid="bib79">Stach et al., 2004</xref>; <xref ref-type="bibr" rid="bib83">Turner, 1911</xref>; <xref ref-type="bibr" rid="bib19">Frisch, 1914</xref>; <xref ref-type="bibr" rid="bib86">Wehner, 1967</xref>). On the other hand, there is a long history of claims that bees are incapable of discriminating some relatively simple patterns (<xref ref-type="bibr" rid="bib3">Avarguès-Weber et al., 2011</xref>; <xref ref-type="bibr" rid="bib25">Guiraud et al., 2022</xref>; <xref ref-type="bibr" rid="bib31">Hertz, 1935</xref>; <xref ref-type="bibr" rid="bib30">Hertz, 1929</xref>; <xref ref-type="bibr" rid="bib33">Horridge, 1996</xref>; <xref ref-type="bibr" rid="bib76">Srinivasan, 1994</xref>; <xref ref-type="bibr" rid="bib19">Frisch, 1914</xref>). As one example, it was reported that honeybees (<italic>Apis mellifera</italic>) were not able to distinguish a “plus pattern”, made up of a vertical and horizontal bar, from the same pattern rotated through 45° that is multiplication sign pattern (<xref ref-type="bibr" rid="bib33">Horridge, 1996</xref>; <xref ref-type="bibr" rid="bib77">Srinivasan et al., 1994</xref>) in a Y-maze setup where the patterns were displayed at a fixed distance from the bees’ decision point. However, there is evidence that the successes and failures of bees in discriminating visual patterns are not strictly related to pattern complexity. Factors such as the angle subtended on the visual field of a stimulus from a distance (<xref ref-type="bibr" rid="bib33">Horridge, 1996</xref>; <xref ref-type="bibr" rid="bib78">Srinivasan, 2010</xref>) and the particular sensitivity of orientation and colour selective neurons in the bee visual system (<xref ref-type="bibr" rid="bib64">Paulk et al., 2009</xref>; <xref ref-type="bibr" rid="bib63">Paulk et al., 2008</xref>; <xref ref-type="bibr" rid="bib66">Roper et al., 2017</xref>; <xref ref-type="bibr" rid="bib55">Maddess and Yang, 1997</xref>) may limit the ability of bees to solve a specific task. Indeed, the visual scanning procedures that bees use when examining and memorising the patterns can also affect performance (<xref ref-type="bibr" rid="bib21">Giurfa et al., 1999</xref>; <xref ref-type="bibr" rid="bib27">Guiraud et al., 2025b</xref>; <xref ref-type="bibr" rid="bib47">Lehrer, 1994</xref>; <xref ref-type="bibr" rid="bib54">MaBouDi et al., 2023</xref>; <xref ref-type="bibr" rid="bib80">Stach and Giurfa, 2005</xref>).</p><p>In this study, we revisit one of the pattern discrimination tasks that has been proven highly challenging for honeybees (<xref ref-type="bibr" rid="bib77">Srinivasan et al., 1994</xref>), namely the plus versus multiplication sign discrimination task (these experiments required bees to make a decision from a distance). Here, we aimed to explore whether and, more importantly, how bumblebees can successfully solve this discrimination task. Hence, by recording the bees’ flight trajectories, and analysing their scanning movements, we aimed to determine the strategies employed in solving this visual task, specifically to investigate whether they are able to develop an active sampling strategy.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>The main objective of this study was to examine whether bees develop specific scanning behaviours that enable them to solve pattern discrimination tasks, effectively. We conducted a visual discrimination task using a plus sign and a 45°-rotated version of the same pattern (multiplication sign). Forty bumblebees (<italic>Bombus terrestris audax</italic>) from six colonies were utilised and housed in wooden nest boxes connected to a flight arena (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Two groups of bees (n=20 each) were trained using a differential conditioning protocol: one group was trained to associate the plus pattern with a reward (sucrose solution) while avoiding the multiplication pattern associated with punishment (quinine solution), and the other group was trained on the reciprocal arrangement (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). After five bouts of training or upon reaching 80% performance in the last 10 choices, the bees underwent one learning test and three transfer tests.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Bees’ performance in a pattern recognition task.</title><p>(<bold>A</bold>) Illustration of the flight arena, equipped with two cameras: a side camera above the entrance viewing the rear wall and a top camera above the rear wall recording from above. (<bold>B</bold>) Training and testing protocol: Bees were trained with four plus and four multiplication patterns on white discs (10 cm diameter, 2 mm margin), attached to the arena’s rear wall <italic>via</italic> small Eppendorf tubes. Group 1 of bumblebees (n=20) was rewarded with the plus sign (sucrose solution) and punished with the multiplication sign (quinine), while Group 2 (n=20) received the opposite reinforcement. After training, four tests with refresher interval bouts were conducted: (1) a learning test using the original patterns from training, (2) a top-half test showing only the upper half of the patterns, (3) a bottom-half test displaying only the lower half of the patterns, and (4) a single-bar test presenting individual bars in different orientations. (<bold>C</bold>) Learning curves for Group 1 (blue) and Group 2 (orange) show both groups learned the pattern discrimination task. (<bold>D</bold>) Unrewarded learning test performance, with both groups distinguishing patterns (p&lt;4.8e-3). (<bold>E</bold>) Initial inspections of the plus symbol upon arena entry showed no significant preference from a distance (&gt;5 cm), with 8 out of 13 bees in group 1 and 5 out of 12 bees in group 2 making initial visits to the plus symbol in the learning test (p=0.55).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106332-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Bees’ flight paths upon first entering the arena during learning tests.</title><p>In 10 instances the bees initially inspected the correct stimulus, scanned the pattern and visited the feeder. In the remaining nine flights the bees initially inspected the incorrect pattern, then rejected the pattern and flew to another, usually adjacent pattern. One video is missing where the footage was only recorded at 30 fps; this bee initially inspected the incorrect pattern, and again rejected the stimulus. The bees’ first inspection appears to be random with 50/50 correct pattern selections from the arena entrance; this suggests bees have to scan the stimuli before making decisions. Line colour from blue to yellow: flight speed 0.0–0.7 m/s.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106332-fig1-figsupp1-v1.tif"/></fig></fig-group><p>The learning test used the same multiplication and plus patterns as during training to assess whether the bees developed specific manoeuvres for inspecting patterns before making choices. Each of the transfer tests presented novel stimuli showing fragments of both training patterns. In the bottom-half test, only the bottom half of the multiplication and plus patterns was shown, while in the top-half test, only the top half was presented. These tests aimed to assess whether the bees' recognition was based solely on the top or bottom halves of the patterns. Additionally, a single-bar orientation test was conducted to determine the bees' preferences for each component bar of the training stimuli (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><p>In all four tests, correct and incorrect stimuli were randomly positioned on the rear arena wall, with feeding tubes containing sterilised water (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The bees' flight trajectories during the tests were recorded using two high-speed cameras, capturing their flight behaviour from different angles. Advanced video analysis was then performed, using a custom algorithm developed to automatically detect and track the bees' movements. The algorithms extracted various behavioural parameters, including flight speeds, distances from the patterns, flight orientations, inspection times, and areas of interest (see Materials and methods section).</p><sec id="s2-1"><title>Bumblebees’ performance in a visual recognition task</title><p>We first confirmed that bumblebees, when allowed to fly as close to the patterns as they desired, could perform the simple visual discrimination task of identifying and associating either a plus or a multiplication sign pattern with a reward (sucrose solution) and the other with quinine solution (penalty, bitter taste).</p><p>Bees in Group 1 recognised the plus patterns as rewarding above chance after just 20 choices (<xref ref-type="fig" rid="fig1">Figure 1C</xref>; Wilcoxon signed rank test; z=1.95, n=20, p=0.04; mean = 57%). Conversely, Group 2 achieved the same performance on the multiplication patterns after 30 choices (Wilcoxon signed rank test; z=2.34 n=20, p=3.6e-3). Nevertheless, there was no notable difference in the learning rate between the two groups after 30 choices (p=0.77), and the bees’ performance was not affected by colony membership (p=0.25; <xref ref-type="fig" rid="fig1">Figure 1C</xref>; see <xref ref-type="table" rid="table1">Table 1</xref>). The bees continued to increase in performance during the 70 choices of the training (per block of 10 choices, see <xref ref-type="fig" rid="fig1">Figure 1C</xref>); whereupon all bees achieved ≥92% (±7.8 SEM) correct choice performance. The results of a generalised linear mixed model (GLMM) analysis confirmed that both groups of bees had learned to select the rewarding patterns significantly above chance (&gt;50%) after training (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, p=4.8e-9). Additionally, the bees’ performance in the learning tests indicated that both groups of trained bees successfully learned to discriminate the plus from the multiplication sign, and vice versa (<xref ref-type="fig" rid="fig1">Figure 1D</xref>; Wilcoxon signed rank test; z=3.21, n=13, p=1.3e-3 for Group 1; z=3.08, n=12, p=2.0e-3 for Group 2); again there was no significant difference between the performance of two groups in the learning test (Wilcoxon rank sum test; z=0.45, n=25, p=0.64). The bees’ performance in the learning test was similar to that seen during the last block of 10 choices of the training phase (Wilcoxon signed rank test z=1.09, n=25, p=0.27).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Summary of the Generalised Linear Mixed Model (GLMM) examining factors in relation to proportion of correct choices during the training<bold>.</bold></title><p>The dependent variable was the number of correct choices from the block of 10 choices. Fixed factors, Colony, Group and Trial were examined in the model. Bee index was calculated in the model as a random factor. Model fit statistics: AIC = 32.91; BIC = 337.62; Log-Likelihood=−156.45; Deviance = 312.91.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Fixed factors</th><th align="left" valign="bottom">Estimate</th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom">tStat</th><th align="left" valign="bottom">DF</th><th align="left" valign="bottom">p Value</th><th align="left" valign="bottom">Lower</th><th align="left" valign="bottom">Upper</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold><italic>Intercept</italic></bold></td><td align="char" char="." valign="bottom">–0.11</td><td align="char" char="." valign="bottom">0.50</td><td align="char" char="." valign="bottom">–0.33</td><td align="char" char="." valign="bottom">266</td><td align="char" char="." valign="bottom">0.71</td><td align="char" char="." valign="bottom">–1.17</td><td align="char" char="." valign="bottom">0.83</td></tr><tr><td align="left" valign="bottom"><bold><italic>Colony</italic></bold></td><td align="char" char="." valign="bottom">0.19</td><td align="char" char="." valign="bottom">0.17</td><td align="char" char="." valign="bottom">1.04</td><td align="char" char="." valign="bottom">266</td><td align="char" char="." valign="bottom">0.25</td><td align="char" char="." valign="bottom">–0.13</td><td align="char" char="." valign="bottom">0.42</td></tr><tr><td align="left" valign="bottom"><bold><italic>Group</italic></bold></td><td align="char" char="." valign="bottom">–0.06</td><td align="char" char="." valign="bottom">0.27</td><td align="char" char="." valign="bottom">–0.24</td><td align="char" char="." valign="bottom">266</td><td align="char" char="." valign="bottom">0.82</td><td align="char" char="." valign="bottom">–0.61</td><td align="char" char="." valign="bottom">0.36</td></tr><tr><td align="left" valign="bottom"><bold><italic>Trials</italic></bold></td><td align="char" char="." valign="bottom">0.21</td><td align="char" char="." valign="bottom">0.03</td><td align="char" char="." valign="bottom">6.75</td><td align="char" char="." valign="bottom">266</td><td align="char" char="hyphen" valign="bottom"><bold>4.8e-9</bold></td><td align="char" char="." valign="bottom">0.14</td><td align="char" char="." valign="bottom">0.27</td></tr></tbody></table></table-wrap><p>During the learning tests, for 8 out of the 13 bees in Group 1 (trained to plus) the first pattern to be inspected (flight within 12 cm diameter of centre of pattern and 10 cm out from rear stimulus wall) was the rewarding plus pattern (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). However, 5 out of the 12 bees in Group 2 also inspected one of the plus patterns first, in their case the incorrect pattern. Therefore, as a whole, the bees’ pattern selection from a distance (i.e. from arena entrance to stimuli wall) was no different to chance (50% correct initial pattern inspections; <xref ref-type="fig" rid="fig1">Figure 1E</xref>; <italic>χ</italic><sup>2</sup> test, Chi-square statistics <italic>= 0.35</italic>, p=0.55). In addition, during all the correct initial inspections of both groups, the bees still scanned the pattern before flying to the feeder tube, further suggesting that the bees do not receive sufficient evidence for a choice from a distance (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p></sec><sec id="s2-2"><title>Bees' discrimination of partial and component visual cues</title><p>To control for the possibility that the partial cues of patterns may have also influenced the bees’ decisions, we carried out three transfer tests (see Methods section). <xref ref-type="fig" rid="fig2">Figure 2</xref> illustrates bumblebees’ performance in the transfer testing phase, where they were presented with specific sections or components of the trained visual patterns (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). In the bottom-half test, bees demonstrated a statistically significant ability to discriminate between the bottom portions of the correct and incorrect patterns (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; Wilcoxon signed rank test; z=2.81, n=10, p=4.9e-3 for Group 1; z=2.66, n=10, p=7.7e-3 for Group 2), suggesting that they could generalise their learned associations to these bottom cues. In contrast, the top-half test revealed that bees exposed only to the upper sections of the patterns showed no significant preference for the correct pattern (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; Wilcoxon signed rank test; z=0.17, n=10, p=0.83 for Group 1; z=0.17, n=10, p=0.85 for Group 2), indicating that these cues alone were insufficient for discrimination. Panel C displays the bees' responses to single bars from the training patterns. The single-bar orientation test showed a significantly higher response to bars matching the rewarded orientation across both groups (<xref ref-type="fig" rid="fig2">Figure 2C</xref>; Wilcoxon signed rank test; z=2.70, n=10, p=6.8e-3 for Group 1; z=2.6, n=10, p=9.3e-3 for Group 2). Furthermore, bees trained to the plus pattern favoured the vertical bar, while those trained to the multiplication pattern showed an equal preference for the 45° and –45° bars, indicating orientation-specific associations (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>; Kruskal-Wallis test; Chi-sq=25.72; df = 39, p=1.0e-5 for Group 1; Chi-sq=19.0; df = 39, p=3.0e-4 for Group 1).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Bumblebees' performance in the transfer testing phase.</title><p>This figure shows bumblebee performance in unrewarded tests where different sections of the training patterns were presented. (<bold>A</bold>) Bees successfully discriminated the bottom halves of the patterns, with statistically significant results (p &lt; 7.7e-3). However, in (<bold>B</bold>), bees exposed only to the top halves did not show significant recognition of the correct patterns (p=0.83), suggesting these cues were insufficient for accurate discrimination. (<bold>C</bold>) shows that bees exhibited a significantly higher response to individual bars matching the rewarded orientations in the training (p&lt;6.8e-4), indicating effective recognition based on specific visual cues learned during training.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106332-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Bumblebees' preference in the single-bar orientation test.</title><p>The bar plot shows the percentage of choices made by bumblebees in the single-bar test, highlighting bees' differential responses to isolated bar cues. Bees trained to the plus pattern displayed a strong preference for the vertical bar, whereas bees trained to the multiplication pattern showed significantly higher selection of both the 45° and –45° bars (p&lt;3.0e-4), with no notable difference in their choices between these two orientations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106332-fig2-figsupp1-v1.tif"/></fig></fig-group><p>These results suggest that bumblebees rely on specific parts of the visual patterns—particularly the lower sections—and individual bar orientations to recognise the overall pattern. This finding implies that bees may focus on a discriminable local feature of a pattern that suffices for recognition, while disregarding other elements. This highlights the role of partial and component-based cues in bumblebees' visual processing and learning.</p></sec><sec id="s2-3"><title>Bumblebee flight speeds and trajectories during the learning tests</title><p>To explore how bees choose the correct patterns and reject the incorrect ones, we analysed the bees’ inspection behaviours, employing a custom algorithm to track bee locations and body orientations within each frame of the videos (<xref ref-type="fig" rid="fig3">Figure 3A and B</xref> and <xref ref-type="video" rid="video1">Video 1</xref>; see Video Analysis in Methods section).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Scanning behaviour of bumblebees in a simple pattern discrimination task (A, B).</title><p>An example of a flight path showing the activity of a bee during part of the learning test; presented bee trained to select the 'plus' and avoid the 'multiplication' patterns. Each point on the flight path corresponds to a single video frame with an interval of 4ms between frames which was recorded from the front camera (<bold>A</bold>) and top camera (<bold>B</bold>). The bee sequentially inspected each pattern, correctly landed on the multiplication sign and avoided the plus sign. The colour map changes from blue to yellow with increasing time (See <xref ref-type="video" rid="video1">Video 1</xref>). In (A) the plus patterns may appear slightly misaligned vertically due to visual distortion caused by the camera perspective and edge warping. In the actual experimental setup, the patterns were presented to the bees with a precise vertical alignment. In (B) the black lines in the right panel exhibit bee’s body yaw orientations. (<bold>C</bold>) Distribution of average entrance flight speed toward the wall in the learning tests (See <xref ref-type="fig" rid="fig1">Figure 1</xref>). Filled dots: speed of each individual bee. (<bold>D</bold>) Probability distribution of the bees’ speed in two conditions; when they were inspecting patterns (Red), and when they were flying between patterns (Yellow). This indicates that they slow down to scan patterns before accepting while they accelerate when they fly to another pattern. (<bold>E</bold>) Probability distribution of the bees’ distance from the stimuli wall whilst inspecting patterns. (<bold>F</bold>) From video analysis, the proportion of scanned regions (mean ± SEM) of bees’ inspections before the correct accept or correct rejection (x-axis: regions of interest are highlighted in grey). Triangles: inspection proportion (mean ± SEM) of Group 1 bees (trained to plus sign rewarding), squares: Group 2 bees (trained to multiplication sign), green error bars: correct accepts; red error bars: correct rejections.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106332-fig3-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106332-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Example video of a bee’s scanning behaviour in the learning test.</title><p>The bee was trained to find the reward from the multiplication. The bee inspected lower region of the patterns and rejected the plus and accepted several multiplication signs. The video was recorded by the front camera at 240 fps (frames per second).</p></caption></media><p>The bees’ initial flight speed upon entering the flight arena and approaching the first inspected stimulus was on median 0.20 (±0.13 SEM) m/s (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The speed reduced to a median of 0.11 (±0.10 SEM) m/s whilst inspecting the stimulus; the highest proportion of flight speeds was less than 0.1 m/s (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The bees’ speed increased to a median of 0.20 (±0.24 SEM) m/s whilst traversing between the presented patterns (i.e. flights between the ⌀12cm x 10cm cylinder regions surrounding stimuli). Bees typically scanned the patterns (inspections with the lower flight speed of 0.20 (±0.13SEM) m/s) from a distance of 10 mm to 50mm from the stimuli (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). The bees spent approximately 1.51 (±0.50 SEM) seconds inspecting a stimulus, irrespective of whether this was a plus sign or multiplication sign, or the correct or incorrect pattern (<xref ref-type="fig" rid="fig4">Figure 4H</xref>). The flight speed during inspection when rejecting a pattern was on average three times that of when the bee accepted a pattern and flew to the feeder (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). However, analysis of the flight trajectories (<xref ref-type="fig" rid="fig4">Figure 4A, C and E</xref>) shows this was due to the bee accelerating away from the current pattern to the next one. Interestingly, the bees showed an overall tendency to scan the patterns with their bodies oriented at ~±30° relative to the rear stimuli wall, keeping one or other eye predominantly aligned to the stimuli during the scans (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). Conversely, when flying between the patterns, they mostly looked forward in the direction of their motion with a much wider range of flight directions relative to the rear wall (see Discussion section).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Bee scanning strategy in a pattern recognition task.</title><p>(<bold>A</bold>) The flight paths of one example of acceptance and two examples of rejection behaviours of a bee trained to plus; the bee accepted the plus pattern after scanning the lower half of the vertical bar, while she rejected the multiplication pattern after scanning one or both diagonal bars. Line colour: flight speed 0.0–0.4 ms<sup>–1</sup> (see <xref ref-type="video" rid="video2">Video 2</xref>). (<bold>B</bold>) Group 1 (trained to plus) probability maps (heat-maps) of bees’ locations per frame in front of plus and multiplication type stimuli during all learning tests. The yellow colours show the most visited regions. (<bold>C</bold> &amp; <bold>D</bold>) same analysis as A, B for Group 2 bees trained to discriminate the multiplication sign from the plus sign. This indicates that bees typically scanned the lower half of the pattern with a lower speed to Group 1 bees, prior to their decisions (see <xref ref-type="video" rid="video3">Videos 3</xref> and <xref ref-type="video" rid="video4">4</xref>). (<bold>E</bold>) Three examples of bees’ flight paths shown from the top camera; black lines show bees’ body orientation during the flight, and arrows designate the start and ending time of scanning. (<bold>F</bold>) Probability distribution of the bees’ body yaw orientation perpendicular to the rear stimuli wall in two conditions: when they were inspecting patterns (red) and when they were flying between patterns (yellow). Inset figure exhibits one example of a bee’s orientation (viewed from the top) with +45° to the orientation of the wall on which stimuli were displayed. Bees viewed the patterns at a median ~±30° whilst scanning, with one or other eye having a predominant view. On the other hand, when they transitioned between patterns, the body orientation was more parallel to the flight direction with a wider distribution of orientations relative to the stimuli wall, resulting in a median of ~±50° to the stimuli wall. The dashed lines show the Gaussian mixture distribution models were fitted to each distribution (flights within patterns:<inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>+</mml:mo><mml:mn>27</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>33</mml:mn></mml:math></inline-formula>; flights between patterns:<inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>+</mml:mo><mml:mn>51</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>55</mml:mn></mml:math></inline-formula>). (<bold>G</bold>) Mean flight speed (±SEM) of scanning flight prior to decisions (accept and rejection) for both groups of bees. Blue: Group 1 (trained to plus sign); orange: Group 2 (trained to multiplication sign). (<bold>H</bold>) Inspection time (i.e., the time spent hovering in front of a pattern) for each symbol type for both groups of bees; inspection times of bees in front of both pattern types were equal regardless of their decision or training protocol.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106332-fig4-v1.tif"/></fig></sec><sec id="s2-4"><title>Bumblebees scanned specific regions of the patterns prior to making a decision</title><p>As the bees did not appear to be making pattern selections from a distance (<xref ref-type="fig" rid="fig1">Figure 1E</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> and <xref ref-type="fig" rid="fig3">Figure 3E</xref>), we further analysed the movements of the bees whilst directly in front of the patterns. In most instances (Group 1 trained to ⊕: 89.2%, Group 2 trained to ⊗: 87%), the bees first traversed to, and then scanned, the lower part of the patterns regardless of whether the target was rewarding or aversive (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Each scan led to either a landing on the feeding tube (an acceptance) or the bee flying to another stimulus without landing (a rejection). The selection proportions of bees for each region of the patterns before making acceptance or rejection decisions are illustrated in <xref ref-type="fig" rid="fig3">Figure 3F</xref> for each group of trained bees. The bottom centre of the patterns attracted the highest proportion of interest among the five analysed regions. In Group 1, the bees scanned the bottom centre before deciding to land in 54% of all correct choices. In Group 2, the bees accurately rejected the patterns, and the bottom centre was scanned prior to their rejection in 39.7% of instances. Importantly, this proportion was comparable to the combined instances of scanning the lower left corner, the lower right corner, or both lower corners (totalling 47.5% for correct choices in Group 2 and 35% for correct rejections in Group 1). It is crucial to emphasise that the bees consistently displayed a preference for the lower left corner, which will be further discussed in the subsequent analysis. These preferences can be clearly seen on the heat map representation of the accumulated bee positions during scanning (<xref ref-type="fig" rid="fig4">Figure 4B and D</xref>). Bees trained on the protocol with the plus pattern as rewarding (Group 1) would typically approach the lower half of the stimulus (89% of inspections). They would scan the lower centre of the pattern (containing the vertical bar) and then fly directly to the pattern centre to access the feeding tube (<xref ref-type="fig" rid="fig4">Figure 4A and E</xref>, see <xref ref-type="video" rid="video2">Video 2</xref>). However, if the bees observed a multiplication sign, they would usually scan the lower left corner of the pattern, containing the +45°-angled bar of the multiplication sign (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Of these trials, over half consisted of a single corner scan before the bees rejected the patterns. A scan of the whole pattern was clearly not required: the inspection of a single diagonal pattern element was sufficient to ascertain that the pattern was not a plus sign. In the remaining cases the bees would traverse to the opposite lower corner, then scan the remaining oriented bar before rejection (<xref ref-type="fig" rid="fig3">Figures 3F</xref> and <xref ref-type="fig" rid="fig4">4A</xref>). On average, in only 4.5% of such inspections did the bees only scan the right corner. Bees trained on the multiplication pattern (Group 2) showed a slightly different behaviour. If the bees were inspecting a multiplication sign stimulus, they would first approach the left or right lower section of the pattern (<xref ref-type="fig" rid="fig4">Figure 4C and E</xref>, see <xref ref-type="video" rid="video3">Videos 3</xref> and <xref ref-type="video" rid="video4">4</xref>). We still observed the same preference for the left-side inspections, double that of the lower right-side scans. However, there were far fewer instances of bees inspecting both corners before flying to the feeding tube (<xref ref-type="fig" rid="fig3">Figure 3F</xref>). When Group 2 bees (trained to the multiplication pattern) encountered a plus pattern they would again scan the lower centre at the base of the vertical bar (see <xref ref-type="video" rid="video3">Video 3</xref>). In contrast to the Group 1 bees accepting the multiplication symbol, these bees would also, on occasion, scan the lower left corner where no oriented bar was present (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). However, the observed scanning behaviour is consistent with bees’ performance in the transfer tests with having higher performance of availability of cue of the positive patterns in the lower part of the stimuli.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106332-video2.mp4" id="video2"><label>Video 2.</label><caption><title><bold>Example video of rejecting the multiplication and accept the plus</bold>.</title><p>The video was recorded at 240 fps (frames per second).</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106332-video3.mp4" id="video3"><label>Video 3.</label><caption><title>Example video of rejecting two plus signs and accept the multiplication sign.</title><p>The video was recorded at 240 fps (frames per second).</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106332-video4.mp4" id="video4"><label>Video 4.</label><caption><title>Example video of from the top camera.</title><p>The video is the same flight track as <xref ref-type="video" rid="video3">Video 3</xref> but recorded from the top camera. The video was recorded at 120 fps (frames per second).</p></caption></media></sec><sec id="s2-5"><title>Selective scanning and pattern recognition</title><p><xref ref-type="fig" rid="fig5">Figure 5</xref> provides a detailed illustration of bumblebee scanning behaviour during the top-half and bottom-half tests, showing that bees focused their attention on specific pattern areas that they had learned to identify as minimal discriminative visual cues during training. Panels A and B display heatmaps of flight paths, with warmer colours indicating regions of heightened attention and prolonged hovering. In the bottom-half test (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), bees trained to the plus pattern predominantly focused on the lower section of the vertical bar, while those trained to the multiplication sign concentrated on the space between the two diagonal bars in the bottom-half stimuli. These results indicate that bees rely heavily on the lower regions of the patterns as their primary source of visual information, and their scanning strategy remains consistent with the behaviour developed during training, even in the absence of the top sections of the stimuli (see comparison with <xref ref-type="fig" rid="fig3">Figures 3F</xref>, <xref ref-type="fig" rid="fig4">4B and D</xref>). Conversely, in the top-half test (<xref ref-type="fig" rid="fig5">Figure 5B</xref>), despite the bees’ inability to statistically discriminate between the presented patterns (as shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref>), they continued to prioritise scanning the lower sections of the stimuli, even though these areas lacked any visual cues. Some bees expanded their search to the top half of the patterns and occasionally made incorrect acceptances or rejections after encountering vertical or diagonal visual cues. This behaviour highlights a strong bias toward learned lower regions for pattern discrimination and underscores the influence of pre-learned scanning strategies and task-relevant visual features in guiding bumblebee decision-making, even under conditions where critical cues are absent.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Bumblebee scanning behaviour when inspecting partial and component visual cues.</title><p>(<bold>A &amp; B</bold>) Heatmaps of bumblebee flight paths overlaid on the visual stimuli during the bottom-half test (<bold>A</bold>) and top-half test (<bold>B</bold>). Each panel is grouped based on the bees' decisions to correctly or incorrectly accept or reject the presented test patterns. The heatmaps illustrate areas where bees spent the most time hovering, with warmer colours indicating prolonged hovering and increased attention to specific regions of the pattern. The contour lines further highlight the spatial distribution of bee activity over the stimuli. (<bold>C</bold>) presents bar plots comparing hovering times for correct accept and correct reject responses. The results reveal a strong focus on the bottom sections with longer hovering time when bees correctly accepted the positive pattern (p&lt;2.8e-4), suggesting a scanning strategy that prioritises these areas for pattern discrimination. However, there was no significant difference in hovering time when bees correctly rejected the negative patterns (p&gt;0.05). This preferential focus on specific regions implies that bees adopt scanning behaviours centred on discriminative visual cues, particularly in the lower half of the patterns, as an effective strategy for pattern recognition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106332-fig5-v1.tif"/></fig><p>As shown in <xref ref-type="fig" rid="fig5">Figure 5C</xref>, bees spent more time inspecting the lower parts of the patterns when accepting the visual cues associated with the correct patterns (Wilcoxon rank sum test; z=4.06, n=10, p=4.7e-5 for Group 1; z=3.64, n=10, p=2.7e-4 for Group 2). In contrast, when rejecting incorrect patterns, bees exhibited shorter hovering times in the bottom sections of both the bottom-half and top-half stimuli (Wilcoxon rank sum test; z=9.66, n=10, p=4.1e-22 for Group 1; z=9.19, n=10, p=3.6e-20 for Group 2), suggesting they rejected the stimuli quicker when they could not locate positive visual cues in the lower section. However, there was no significant difference in hovering time when bees correctly rejected the negative patterns (Wilcoxon rank sum test; z=1.89, n=10, p=0.057 for Group 1; z=0.83, n=10, p=0.4 for Group 2). This behaviour aligns with the equal selection of both types of stimuli in the top-half test (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), where bees were unable to find familiar visual cues in the lower sections, leading to random choices or rejections. These findings highlight a scanning strategy in which bees rely on the lower regions of patterns for cue-based decision-making, a behaviour developed during training in response to positive and negative patterns, regardless of the cue’s actual location within the stimuli during inspection.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we explored the flight characteristics and active vision strategies used by bumblebees (<italic>Bombus terrestris audax</italic>) to solve a simple yet challenging visual discrimination task, in which bees were required to distinguish between a multiplication sign and its 45°-rotated variant (a plus sign). Previous studies showed that honeybees (<italic>Apis mellifera</italic>) were unable to discriminate these patterns when prevented from viewing them up close (<xref ref-type="bibr" rid="bib76">Srinivasan, 1994</xref>; <xref ref-type="bibr" rid="bib77">Srinivasan et al., 1994</xref>). However, our results demonstrate that bumblebees, within our flight arena, could successfully learn to distinguish between these patterns while they were allowed to freely inspect the patterns. Bumblebees in our study preferred to inspect both rewarding and aversive stimuli from a close distance (1–5 cm), with the patterns subtending a visual angle of approximately 90° to 160°, in contrast to the ~50° angle used in honeybee experiments (<xref ref-type="bibr" rid="bib77">Srinivasan et al., 1994</xref>). Interestingly, upon entering the arena, bees did not show a preference for initial stimulus selection from a distance, with approximately 50% of initial inspections directed toward incorrect patterns (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), a behaviour consistent with previous reports on bee visual concept learning (<xref ref-type="bibr" rid="bib24">Guiraud et al., 2018</xref>). Even when they chose correctly, bees consistently performed a close-range scan of pattern elements before landing on the feeder (<xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>). These findings suggest that bumblebees rely on close inspection of specific features within a pattern to make accurate acceptance or rejection decisions. For this experimental paradigm, it appears that bees do not make stimulus selection decisions from a distance but instead choose to approach and inspect patterns closely before committing to a choice. This behaviour supports the hypothesis that bumblebees employ a sequential feature-scanning strategy, enabled by active vision, to efficiently distinguish between complex visual stimuli in a controlled environment.</p><p>Our experimental paradigm cannot confirm with certainty that bumblebees are unable to discriminate these simple patterns from a distance; for that we would need to control for distance as done with the honeybee experiments (<xref ref-type="bibr" rid="bib33">Horridge, 1996</xref>; <xref ref-type="bibr" rid="bib77">Srinivasan et al., 1994</xref>). However, our experiment allowed us to carefully analyse the bees’ scanning behaviour of visual features and to extract useful insights into the active vision of bees.</p><p>In brief, our bumblebees had no difficulty in learning to identify and associate either the plus or multiplication signs with reward, with all bees achieving over 90% accuracy after 70 trials (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). This performance was preserved during the unrewarded learning tests (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Our bespoke video analysis toolkit allowed us to track the bee positions and body yaw orientations for every frame of each learning test. The most notable, and consistent, characteristics observed were:</p><sec id="s3-1"><title>Partial pattern inspection</title><p>The bees primarily flew to, and scanned, the lower half of the patterns (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). This suggests that the lower half was all the bees learned. Indeed, when exposed to a transfer test with only the top half of the pattern available, bees failed to identify the correct halves of the training patterns (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). A previous study showed that honeybees (<italic>Apis mellifera</italic>) trained in a Y-Maze using absolute conditioning (where only the positive pattern and a secondary blank stimulus is provided) assigned more importance to the lower half of the pattern to that of the top half (<xref ref-type="bibr" rid="bib6">Chittka et al., 1988</xref>; <xref ref-type="bibr" rid="bib21">Giurfa et al., 1999</xref>). During tests with only the top half of the training pattern and a novel pattern they failed to select the correct pattern half. Conversely, if bees were presented with the lower half of the training pattern and again a novel pattern, they could identify the correct stimulus. In contrast, when trained using differential conditioning (using both rewarded and unrewarded patterns), the honeybees learned the whole pattern; correctly identifying both bottom and top half patterns during tests. However, in this instance, unlike in our study, the bees’ choices were recorded from a distance (for apparatus details, see <xref ref-type="bibr" rid="bib33">Horridge, 1996</xref>) and bees’ flights were not analysed systematically. However, in our study, we did not test bumblebees with just the lower half of the pattern. It is possible that just presenting the lower section of the pattern would also decrease performance. Future work is required to address this limitation. In addition, it would be useful to see the bee performance if only the left or right side of the pattern is presented (see below).</p><p>In a more recent study, in which the flight path of bees was also analysed, <xref ref-type="bibr" rid="bib24">Guiraud et al., 2018</xref> showed how honeybees (<italic>Apis mellifera</italic>) can solve a conceptual learning task of ‘above and below’ by scanning the lower of two pattern elements presented on the stimuli; this provided sufficient information for the bees to make a decision without needing to understand, or inspect, the relationship between the top and bottom pattern elements.</p><p>The reason why bees primarily focus on the lower half of the pattern instead of the top half remains unclear. However, several factors may contribute to this behavioural preference. Flowers positioned at the lower end of an inflorescence are typically older, larger, and contain more nectar compared to those located higher up (<xref ref-type="bibr" rid="bib11">Davies et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Heinrich, 1979</xref>; <xref ref-type="bibr" rid="bib29">Heinrich, 1979</xref>; <xref ref-type="bibr" rid="bib28">Heinrich, 1975</xref>; <xref ref-type="bibr" rid="bib35">Johnson and Anderson, 2010</xref>). As a result, bees can optimise their nectar collection efficiency by starting from the bottom and gradually moving upward. Additionally, studies have indicated functional differences in bee eye regions (<xref ref-type="bibr" rid="bib49">Lehrer, 1998</xref>; <xref ref-type="bibr" rid="bib72">Spaethe and Chittka, 2003</xref>; <xref ref-type="bibr" rid="bib85">Wakakuwa et al., 2005</xref>), suggesting specialised neural mechanisms in different eye regions that serve the specific needs of bees. If the ventral eye region provides a neural advantage, bees can still scan the top half of patterns by flying above the stimuli and examining them with their ventral eye regions. Another possibility is that the lower part of the pattern may contain distinguishing visual features or fractures that provide sufficient information for bees to solve the visual task. By scanning the lower part of patterns, bees can capture the key information necessary for successful pattern recognition, thereby enhancing their speed in solving the task (<xref ref-type="bibr" rid="bib8">Chittka et al., 2009</xref>; <xref ref-type="bibr" rid="bib26">Guiraud et al., 2025a</xref>; <xref ref-type="bibr" rid="bib54">MaBouDi et al., 2023</xref>). Additionally, inspecting from the lower part of the pattern and moving upward through flight may be easier for the motor system of bees.</p></sec><sec id="s3-2"><title>Initial side preference</title><p>The bees had a significant preference for initially scanning the left side of the multiplication pattern (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). This left side preference for visual objects, known as pseudoneglect, is also seen in humans (<xref ref-type="bibr" rid="bib34">Jewell and McCourt, 2000</xref>), and birds (<xref ref-type="bibr" rid="bib13">Diekamp et al., 2005</xref>; <xref ref-type="bibr" rid="bib68">Rugani et al., 2015</xref>). This preference may allow an individual to always start its inspection of a stimulus at the same location, allowing for consistent learning and recognition of natural stimuli; but it remains a curiosity as to why the left preference was so prevalent amongst the bees tested (<xref ref-type="bibr" rid="bib2">Anfora et al., 2011</xref>; <xref ref-type="bibr" rid="bib22">Giurfa et al., 2022</xref>; <xref ref-type="bibr" rid="bib51">Letzkus et al., 2008</xref>; <xref ref-type="fig" rid="fig2">Figure 2F</xref>). In humans and birds, this lateralisation of spatial attention may have evolved once in a common ancestor (<xref ref-type="bibr" rid="bib13">Diekamp et al., 2005</xref>). However, since the visual system of insects evolved largely independently from that of vertebrates, the left-side bias must have emerged by convergent evolution. Its computational neural advantage in bees or vertebrates (if any), is not known.</p></sec><sec id="s3-3"><title>Common body orientation during scans</title><p>The yaw orientation of the bees’ bodies was most often at ~±30° to the stimuli during pattern inspections. In this manner, one or other of the bees’ eyes would face the pattern, with only a small proportion of the opposite eye having visual access to the pattern. There was no overall preference for the left or right eye (with median orientations at ~–33° and ~+27°, respectively) during scans (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). In previous modelling work, <xref ref-type="bibr" rid="bib66">Roper et al., 2017</xref> showed that lateral connections from both the left and right lobula to the bee mushroom bodies allowed for better pattern recognition during partial occlusion of stimuli. However, this came at the expense of fine detail recognition. Therefore, and counterintuitively, having one eye mostly obscured from the pattern may provide the mushroom bodies (learning centres of the bee brain) with more distinct neural inputs. It may also allow bees to learn both the pattern and location cues simultaneously whilst scanning a resource. Future work will be needed to see if this behaviour is particular to the patterns used in this experiment, or a stereotypical behaviour.</p></sec><sec id="s3-4"><title>Commonality in scan strategies is based on stimuli, not protocol</title><p>It may seem sensible, from the bees’ perspective, if trained on plus signs, only to inspect the lower centre of the pattern for the vertical bar. However, both groups of trained bees initially approached and scanned the plus and multiplication signs in the same manner, typically checking the lower left corner of the multiplication sign and the vertical bar of the plus sign. This might suggest that the bees did not learn the relative position of the cues and simply searched for the first visual item at the lower left of the pattern. However, the flight tracking analysis conflicts with this hypothesis. Bees initial scan of the left 45° bar of the multiplication symbol was occasionally followed by a scan of the adjacent 45° bar on the right of the multiplication pattern. Additionally, with the group trained on multiplication as rewarding, after scanning the vertical bar of the plus they occasionally flew to the lower left corner; presumably checking for the multiplication sign-oriented bar. We therefore assume that the stimulus directs the scanning behaviour of the bee, and in turn the bee is learning both rewarding and aversive pattern features during training. Previous works have shown that bees will typically follow the contours of a stimulus during inspections, but can also learn to suppress this strategy if needed to solve a given task (<xref ref-type="bibr" rid="bib47">Lehrer, 1994</xref>; <xref ref-type="bibr" rid="bib48">Lehrer and Srinivasan, 1994</xref>). Further experiments will be required to ascertain the particular rules which dictate the bees’ flight manoeuvres based on the stimuli provided.</p><p>In the pioneering works of Karl von Frisch, 1914, free-flying bees were trained to find sugar reward on certain black or coloured patterns placed horizontally on a white background. Later studies showed that bees only used local cues corresponding to their approach direction when the stimuli were presented to them horizontally (<xref ref-type="bibr" rid="bib86">Wehner, 1967</xref>). Since bees were not able to capture global shapes in this position, this might be the reason bees could only recognise some simple patterns in the early studies. However, vertical presentation of stimuli was developed to examine what diversity of visual features bees may use, such as orientation (<xref ref-type="bibr" rid="bib84">van Hateren et al., 1990</xref>), radial, bilateral symmetry (<xref ref-type="bibr" rid="bib20">Giurfa et al., 1996</xref>; <xref ref-type="bibr" rid="bib33">Horridge, 1996</xref>), or spatial frequency and ring-like structures (<xref ref-type="bibr" rid="bib32">Horridge and Zhang, 1995</xref>). To control the decision distance and understand which cues were utilised by bees to recognise the target pattern, Y-mazes were used (<xref ref-type="bibr" rid="bib75">Srinivasan and Lehrer, 1988</xref>). Although such mazes enabled researchers to control the cues that bees could see when making decisions about visual patterns from a distance, it is a less useful paradigm to inspect the scanning strategies used by bees.</p><p>Analysing rejection responses provides valuable insights into the decision-making processes and factors that influence precise visual assessments. This type of analysis not only reveals the criteria and thresholds that contribute to decision-making strategies but also helps uncover the main visual features that bees rely on to enhance their performance (<xref ref-type="bibr" rid="bib8">Chittka et al., 2009</xref>; <xref ref-type="bibr" rid="bib23">Green and Swets, 1966</xref>; <xref ref-type="bibr" rid="bib54">MaBouDi et al., 2023</xref>). Surprisingly, rejection responses have been largely ignored in the existing literature of insect visual learning. In this study, we aimed to bridge this gap by analysing both acceptance and rejection behaviours in bees. Our results uncovered distinctive responses exhibited by bees when accepting or rejecting a pattern, indicating that both positive and negative patterns play a role in shaping their scanning behaviour. These findings shed light on the importance of negative patterns in understanding visual discrimination and provide substantial contributions into the comprehensive nature of bees' visual processing capabilities.</p><p>In our study, we observed that bees employed shorter scanning times in rejecting patterns when presented with only the bottom-half or top-half of the stimuli, compared to their scanning behaviour during pattern acceptance (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). However, the hovering times for acceptance and rejection behaviours were approximately the same when the observed pattern matched their expectations from the learning tests (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). This behaviour aligns with findings from <xref ref-type="bibr" rid="bib54">MaBouDi et al., 2023</xref>, who reported that bees make rapid rejections when learned evidence is reduced. These observations suggest that bees dynamically utilise both sampling and decision-making strategies to optimise foraging efficiency, actively integrating sensory information with higher-level processes, such as decision-making, in visual discrimination tasks.</p><p>The results of this study demonstrate that bumblebees exhibit a persistent, refined scanning strategy developed through training, focusing primarily on the lower sections of patterns for effective cue-based decision-making (<xref ref-type="fig" rid="fig2">Figures 2F</xref> and <xref ref-type="fig" rid="fig3">3</xref>). This preference for specific cues persists even when bees are presented with altered patterns (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), as bees trained to associate distinct visual cues with positive or negative outcomes consistently prioritised the lower regions of both familiar and modified patterns during inspection. For example, in the bottom-half test, bees successfully generalised their learned associations to cues in the lower sections, achieving statistically significant discrimination (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Interestingly, even in the top-half test, bees continued to focus on the lower sections despite the lack of any visual cues in those areas (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). This inability to generalise their learned scanning strategy to the top-half test reveals critical limitations in their decision-making process, as bees failed to adapt their scanning behaviour to identify discriminative features in the top halves of the patterns. This suggests a strong bias towards pre-learned spatial regions, reflecting a reliance on fixed scanning strategies that prioritise previously reinforced visual cues in the lower sections, even when these cues are absent. By reducing their focus to minimal, stable visual cues, bees appear to adopt a strategy that minimises cognitive load, simplifying decision-making in complex visual environments (<xref ref-type="bibr" rid="bib26">Guiraud et al., 2025a</xref>). These findings indicate that bumblebees' pattern recognition relies on a spatially selective scanning approach that becomes robust against changes in pattern structure, while also emphasising the importance of localised, learned visual features in guiding their decision-making processes. However, the observed limitations highlight that their scanning strategy is highly experience-dependent, restricting adaptability when visual information is presented outside familiar regions. Nevertheless, this result supports the hypothesis that bees actively seek out stable, minimal cues to streamline recognition and maximise efficiency in complex visual environments.</p><p>In recent years the availability of high-speed camera equipment and computer processing power has enabled closer examination of the flight trajectories of insects (<xref ref-type="bibr" rid="bib14">Doussot et al., 2020</xref>; <xref ref-type="bibr" rid="bib50">Lent et al., 2013</xref>; <xref ref-type="bibr" rid="bib54">MaBouDi et al., 2023</xref>; <xref ref-type="bibr" rid="bib56">Mamiya et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Mamiya et al., 2011</xref>; <xref ref-type="bibr" rid="bib62">Odenthal et al., 2020</xref>; <xref ref-type="bibr" rid="bib65">Philippides et al., 2013</xref>; <xref ref-type="bibr" rid="bib81">Stowers et al., 2017</xref>). In this study, we showcase a suite of tools for automatic video tracking of bees in free flight and during their scanning manoeuvres, as well the algorithms needed to analyse and visualise the large amount of positional and orientation data these tracking produces. In our previous work on ‘above and below’ conceptual learning (<xref ref-type="bibr" rid="bib24">Guiraud et al., 2018</xref>) we had to manually view and annotate 368 hr of video footage (46 hr of video footage taken at 120 fps, watched at 1/8th speed). In contrast, here the only manual process was providing a mask frame (without the bee present) per test and marking the feeder positions within that frame. Notably, the introduced algorithm can automatically track bees and analyse their flight without requiring prior training on the data and operates with almost no free parameters. This results in a faster, energy-efficient algorithm for tracking animal behaviour using affordable, standard, or high-frame-rate cameras. With only a small number of test videos to process this was not an issue, but even here, recent advances in making convoluted neural networks for pattern recognition accessible to non-programmers (<ext-link ext-link-type="uri" xlink:href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.14706&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">playground.tensorflow.org</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://runwayml.com/">https://runwayml.com/</ext-link>), as well as the more research programmer-centric DeepLabCut (<xref ref-type="bibr" rid="bib60">Nath et al., 2019</xref>), allows researchers to provide a few dozen labelled mask frames and have these systems process thousands of mask images for all the other videos (<xref ref-type="bibr" rid="bib18">Egnor and Branson, 2016</xref>). Future work is proposed that would record the flight paths of the bees throughout the whole training paradigm; this would allow us to determine, if, and how, the bees’ strategy and flight trajectories evolve over time. This would produce substantially more flight recording data than analysed here, but our suite of tools presented here will vastly reduce the workload. Similarly, the ability to visualise either individual flight paths (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) or combined heat maps of positional data (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) allowed us to quickly identify behavioural aspects of interest. Histograms of velocity, distance and orientation can be quickly generated, but more importantly the parameters defining the areas of interest can be modified and processed in a matter of minutes. Previous studies have relied upon binary fixed decision lines (<xref ref-type="bibr" rid="bib4">Avarguès-Weber et al., 2012</xref>; <xref ref-type="bibr" rid="bib33">Horridge, 1996</xref>; <xref ref-type="bibr" rid="bib32">Horridge and Zhang, 1995</xref>; <xref ref-type="bibr" rid="bib48">Lehrer and Srinivasan, 1994</xref>), with experimenters manually recording these limited behavioural data. Our in-depth analysis on such a straightforward pattern recognition task highlighted key behavioural characteristics, which can now influence future work on active vision, this simply would not have been viable without these automated tools.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals and experimental setup</title><p>In total, forty bees from six colonies of bumblebees (<italic>Bombus terrestris audax,</italic> purchased from Agralan Ltd., Swindon, UK) were used during this study. Colonies were housed in wooden nest boxes (28 x 16 x 11 cm) connected to a wooden flight arena (60 x 60 x 40 cm) via an acrylic tunnel (25 x 3.5 x 3.5 cm). The arena was covered with a UV-transparent Plexiglas ceiling (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Illumination was provided from directly the above the arena via high frequency fluorescent lighting (TMS 24 F lamps with HF-B 236 TLD ballasts, Phillips, Netherland; fitted with Activa daylight fluorescent tubes, Osram, Germany) enclosed within a light diffuser box; the flicker frequency of the lights was ~42 kHz, which is well above the flicker fusion frequency for bees (<xref ref-type="bibr" rid="bib69">Skorupski and Chittka, 2010</xref>; <xref ref-type="bibr" rid="bib74">Srinivasan and Lehrer, 1984</xref>). The walls of the arena were covered with a Gaussian white and pink pattern (MATLAB generated); this provided good contrast between the colour of the bees and the background, required for the video analysis. Sugar water was provided at night through a mass gravity feeder and removed during the day when bees were performing experiments to ensure motivation. Pollen was provided every 2 days into the colonies.</p></sec><sec id="s4-2"><title>Stimuli</title><p>The stimulus patterns were printed on laminated white discs (10 cm in diameter) to allow for easy cleaning with a 70% ethanol solution between training bouts and tests. The training patterns consisted of two black bars (1x10 cm) presented in two configurations: (1) <italic>Plus pattern:</italic> one vertical and one horizontal bar aligned at their centre (⊕); and (2) <italic>Multiplication pattern:</italic> the same configuration as the plus pattern but rotated by 45° to form an ‘X’ shape (⊗). Additional patterns were created for transfer tests, which presented only the top half (top-half test) or the bottom half (bottom-half test) of the training stimuli to assess partial shape recognition. Furthermore, single-bar patterns were included to test orientation discrimination, with bars oriented vertically, horizontally, at 45°, or at –45°. Each pattern included a 2 mm black margin around the outer edge of the disc to ensure consistent visual boundaries.</p><p>Each disc was attached at its centre to the back wall of the arena via a feeder made from a small 0.5 ml Eppendorf tube without a cap (5 mm in diameter). The feeder held 10 µl of one of three solutions: 50% sucrose solution (w/w) as a reward, saturated quinine solution (0.12%) as an aversive stimulus, or sterilised water as a neutral control. This setup allowed for controlled delivery of reinforcement during training and testing, enabling us to systematically investigate the bees' ability to distinguish between complete, partial, and orientation-specific stimuli and to analyse their visual scanning behaviour in making acceptance or rejection decisions.</p></sec><sec id="s4-3"><title>Training and test protocol</title><p>Prior to the experiments, bumblebees could freely fly between the colony and a gravity feeder providing 30% sucrose solution (w/w) placed in the centre of the flight arena. Successful foragers were individually marked on the thorax with number labels (Opalithplättchen, Warnholz &amp; Bienenvoigt, Germany) for identification during the experiment. Every day of experiment, marked bees were randomly selected and pre-trained to receive 50% sucrose solution from eight white discs presented on the rear wall of the area. These pre-training stimuli were 10 cm in diameter with 2 mm wide black margins at the edges. After several bouts of pre-training, a forager that learned to take the sucrose from the feeder at the centre of the white pattern was selected for the individual experiment. During training, only the selected bee was allowed to enter the flight arena.</p><p>To improve the accuracy and the speed of learning, a differential conditioning protocol was used. Four multiplication and four plus pattern stimuli were randomly affixed to set positions on the rear wall of the arena. Each stimulus was 3–6 cm horizontally, and 5 cm vertically separated from the next stimulus, or arena wall/floor/ceiling (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). One group of bees (n=10) was trained to receive 10 µl 50% sucrose solution (w/w) from the feeding tubes at the centre of the plus pattern stimuli, and to avoid the multiplication patterns that contained 10 µl saturated quinine solution. The second group (n=10) was trained on the reciprocal arrangement, that is associate the multiplication pattern with a reward and avoided the plus pattern.</p><p>Bees were allowed to freely choose and feed from multiple stimuli, until they were satiated and returned to their hive; empty tubes were refilled with 10 µl of sucrose solution after the bee had left the correct stimulus and made its next choice. A bout of training was completed once the bee returned to the hive. After each bout, all feeding tubes were cleaned with soap and 70% ethanol and then rinsed with water. The patterns were separately washed with 70% ethanol. Both tubes and patterns were air-dried in the lab before reuse. The position of stimuli on the wall were randomly varied for each bout to prevent bees from using the location of the reward when solving the task.</p><p>After five bouts of training, or upon reaching 80% performance in the last 10 choices, the bees were subjected to four tests to evaluate whether and how they could recognise and select the correct pattern (<xref ref-type="fig" rid="fig1">Figure 1B</xref>).</p><list list-type="order"><list-item><p><italic>Learning test:</italic> Bees were presented with the same multiplication and plus patterns used during training. This test aimed to verify that the bees had learned to associate the correct pattern with the reward and to control for any possible olfactory cues that may have been used during training.</p></list-item><list-item><p><italic>Bottom-half transfer test:</italic> Bees were exposed to stimuli presenting only the bottom half of the multiplication and plus patterns. This addition aimed to evaluate whether the bees could recognise the correct pattern using only the lower part of the shapes.</p></list-item><list-item><p><italic>Top-half transfer test:</italic> Bees were exposed to novel stimuli that presented only the top half of the multiplication and plus patterns (see <xref ref-type="fig" rid="fig1">Figure 1B</xref>). This test was designed to determine if the bees could still recognise the correct pattern based solely on the top half.</p></list-item><list-item><p><italic>Single-bar orientation test:</italic> The fourth test introduced single-bar stimuli in four different orientations: vertical, horizontal, 45°, and –45°. This test assessed whether the bees could distinguish the pattern based on a single bar orientation rather than the full or partial pattern configurations.</p></list-item></list><p>For the first three tests, as during training, both correct and incorrect stimuli were presented, with four instances of each pattern randomly positioned on the rear arena wall. In the single-bar orientation test, two patterns of each of the four orientations were presented to the bees. Each stimulus feeding tube was filled with 10 µl of sterilised water (i.e. no reward or punishment) to prevent reinforcement effects during testing. One to two refresher bouts of training (with reward and punishment), identical to the training bouts, were conducted between tests to maintain the bees’ motivation. The sequence of the tests was randomised for each bee to minimise potential order effects. The first 20 bees were tested only with the first two tests. Additionally, some bees did not complete all tests due to a loss of motivation to return to the flight arena. The entire pre-training, training, and testing process was conducted continuously, taking approximately 3–5 hr per bee.</p><sec id="s4-3-1"><title>Video analysis</title><p>The arena was equipped with two cameras to record all activity of bees during tests. An iPhone 6 (Apple, Cupertino, USA) with 1280x720 pixels and 240 fps (frames per second) was positioned above the arena entrance tunnel viewing the rear stimuli wall, filming the bee’s flight in front of the stimuli wall and patterns. The second camera, a Yi sport camera (Xiaomi Inc China) with 1280x720 pixels at 120 fps, was placed on the top of the rear wall orientated downward to view the stimuli. The first 120 s of each test were recorded and analysed.</p><p>To analyse bees’ scanning behaviours in front of the stimuli, prior to their choices, a MATLAB algorithm was developed that detected the bees automatically and then tracked the centroid of the bee bodies within each frame as they flew through the arena. For each frame, the algorithm subtracted a background mask image to find new candidate positions of the bee using MATLAB’s blob detection function. The parameters of this function were set to detect the blob with the same approximate size of a bee. In addition, an elliptic filter was used in the frames from the top camera to extract the bees’ body orientations. We utilised the MATLAB smoothing function (‘filter’) to exclude any erroneous data points and correct trajectories. Examples of the annotated flight paths and corresponding video recordings are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="video" rid="video1">Video 1</xref>.</p><p>Using the first frame of each video recording, we manually specified the x, y pixel position of each of the eight pattern centres (i.e. entrances to the feeding tubes). The speed of the bees was calculated using the calibration data, the distance of the camera to the stimuli, and the pixel positions of the bee’s trajectory, converted into real-world units based on the known dimensions of the experimental setup. After calculating the speed of each bee at each point of the trajectory, a threshold rule was applied to the trajectories close to the feeding tube positions to identify if the bees had landed, labelling the decision as either a correct/incorrect accept or rejection. This ‘landing’ threshold was determined by K-means clustering (<xref ref-type="bibr" rid="bib54">MaBouDi et al., 2023</xref>; <xref ref-type="bibr" rid="bib52">MaBouDi et al., 2020a</xref>) of all bee speeds within the specified region of the feeding tubes. For further analysis of stimuli inspections, the flight speeds, distances from wall, orientation, inspection times, areas of interest, and heat maps, we extracted the bees’ trajectory data (using the above procedure) from a cylindrical region in front of each stimulus, with a diameter 12 cm around the pattern centre and 10 cm out from the stimuli wall. Bespoke MATLAB algorithms were developed to calculate and plot the required datasets for each of these individual stimuli analyses (see examples: <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>). Unfortunately, one of the learning test videos from Group 1 (trained to plus) was accidently recorded at just 30 fps; we therefore excluded it from the above flight analysis. This video was sufficient, however, for the behavioural results of the choices and rejections of the bee to be extracted.</p></sec><sec id="s4-3-2"><title>Statistical analysis</title><p>A generalised linear mixed model (GLMM) with Binomial distribution and link function ‘logit’ was applied to the bees’ choices recorded during the training phase to evaluate the effect of colony and group on bees’ performance and compare the learning rate between two groups of bees. To assess the bees’ performances in the tests, we analysed the proportion of correct choices for each individual bee. The proportion of correct choices was calculated by the number of correct choices divided by the bee’s total choices during the first 120 s of the test. A choice was defined as when a bee touched a microcentrifuge feeding tube with her antennae or when she landed on a feeding tube. We then applied the Wilcoxon signed rank or Wilcoxon rank-sum tests to compare the bees’ responses to the learning test and the transfer test.</p></sec></sec><sec id="s4-4"><title>Computing environment</title><p>All data analysis and visualisation were performed using MATLAB (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_001622">SCR_001622</ext-link>) and Python (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_008394">SCR_008394</ext-link>). Both Python and MATLAB were used for image processing, video analysis, and custom script development. MATLAB was also used for statistical analysis and behavioural data analysis.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Employee of Ben Thorns Ltd</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Investigation, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Investigation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Validation, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Supervision, Funding acquisition, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-106332-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The behavioural data and codes for analysing trajectory are available in the public repository figshare at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.15131/shef.data.14185865.v1">https://doi.org/10.15131/shef.data.14185865.v1</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>HD</given-names></name><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Guiraud</surname><given-names>M</given-names></name><name><surname>Marshall</surname><given-names>J</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Data for &quot;Automated video tracking and flight analysis show how bumblebees solve a pattern discrimination task using active vision&quot;</data-title><source>figshare</source><pub-id pub-id-type="doi">10.15131/shef.data.14185865.v1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Olivia Brookes for her help in collecting the preliminary data. This study was supported by the Human Frontier Science Program (HFSP) grant (RGP0022/2014), the Engineering and Physical Sciences Research (EPSRC) Programme Grant Brains on Board (EP/P006094/1) and Horizon Europe Framework Programme grant NimbleAI - Ultra energy-efficient and secure neuromorphic sensing and processing at the endpoint. MGG was supported by ARC Discovery Projects (DP230100006 and DP210100740) and Templeton World Charity Foundation Project Grant (TWCF-2020–20539).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>AG</given-names></name><name><surname>Ratnam</surname><given-names>K</given-names></name><name><surname>Roorda</surname><given-names>A</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>High-acuity vision from retinal image motion</article-title><source>Journal of Vision</source><volume>20</volume><elocation-id>34</elocation-id><pub-id pub-id-type="doi">10.1167/jov.20.7.34</pub-id><pub-id pub-id-type="pmid">32735342</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anfora</surname><given-names>G</given-names></name><name><surname>Rigosi</surname><given-names>E</given-names></name><name><surname>Frasnelli</surname><given-names>E</given-names></name><name><surname>Ruga</surname><given-names>V</given-names></name><name><surname>Trona</surname><given-names>F</given-names></name><name><surname>Vallortigara</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Lateralization in the invertebrate brain: left-right asymmetry of olfaction in bumble bee, Bombus terrestris</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e18903</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0018903</pub-id><pub-id pub-id-type="pmid">21556150</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avarguès-Weber</surname><given-names>A</given-names></name><name><surname>Deisig</surname><given-names>N</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Visual cognition in social insects</article-title><source>Annual Review of Entomology</source><volume>56</volume><fpage>423</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1146/annurev-ento-120709-144855</pub-id><pub-id pub-id-type="pmid">20868283</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avarguès-Weber</surname><given-names>A</given-names></name><name><surname>Mota</surname><given-names>T</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>New vistas on honey bee vision</article-title><source>Apidologie</source><volume>43</volume><fpage>244</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1007/s13592-012-0124-2</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benard</surname><given-names>J</given-names></name><name><surname>Stach</surname><given-names>S</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Categorization of visual stimuli in the honeybee Apis mellifera</article-title><source>Animal Cognition</source><volume>9</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1007/s10071-006-0032-9</pub-id><pub-id pub-id-type="pmid">16909238</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Hoffmann</surname><given-names>M</given-names></name><name><surname>Menzel</surname><given-names>R</given-names></name><name><surname>Elsner</surname><given-names>N</given-names></name><name><surname>Barth</surname><given-names>FG</given-names></name></person-group><year iso-8601-date="1988">1988</year><chapter-title>Discrimination of UV-green patterns in honey bees</chapter-title><person-group person-group-type="editor"><name><surname>Elsner</surname><given-names>N</given-names></name><name><surname>Barth</surname><given-names>FG</given-names></name></person-group><source>Sense Organs</source><publisher-loc>Stuttgart</publisher-loc><publisher-name>Thieme Verlag</publisher-name><fpage>1</fpage><lpage>218</lpage></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Niven</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Are Bigger Brains Better?</article-title><source>Current Biology</source><volume>19</volume><fpage>R995</fpage><lpage>R1008</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2009.08.023</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Raine</surname><given-names>NE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Speed-accuracy tradeoffs in animal decision making</article-title><source>Trends in Ecology &amp; Evolution</source><volume>24</volume><fpage>400</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2009.02.010</pub-id><pub-id pub-id-type="pmid">19409649</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Information processing in miniature brains</article-title><source>Proceedings. Biological Sciences</source><volume>278</volume><fpage>885</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1098/rspb.2010.2699</pub-id><pub-id pub-id-type="pmid">21227971</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collett</surname><given-names>TS</given-names></name><name><surname>Fry</surname><given-names>SN</given-names></name><name><surname>Wehner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Sequence learning by honeybees</article-title><source>Journal of Comparative Physiology A</source><volume>172</volume><fpage>693</fpage><lpage>706</lpage><pub-id pub-id-type="doi">10.1007/BF00195395</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>NB</given-names></name><name><surname>Krebs</surname><given-names>JR</given-names></name><name><surname>West</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>An Introduction to Behavioural Ecology</source><publisher-name>John Wiley &amp; Sons</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dawkins</surname><given-names>MS</given-names></name><name><surname>Woodington</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Pattern recognition and active vision in chickens</article-title><source>Nature</source><volume>403</volume><fpage>652</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1038/35001064</pub-id><pub-id pub-id-type="pmid">10688200</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diekamp</surname><given-names>B</given-names></name><name><surname>Regolin</surname><given-names>L</given-names></name><name><surname>Güntürkün</surname><given-names>O</given-names></name><name><surname>Vallortigara</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A left-sided visuospatial bias in birds</article-title><source>Current Biology</source><volume>15</volume><fpage>R372</fpage><lpage>R373</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2005.05.017</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doussot</surname><given-names>C</given-names></name><name><surname>Bertrand</surname><given-names>OJN</given-names></name><name><surname>Egelhaaf</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The critical role of head movements for spatial representation during bumblebees learning flight</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>14</volume><elocation-id>606590</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2020.606590</pub-id><pub-id pub-id-type="pmid">33542681</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dutta</surname><given-names>S</given-names></name><name><surname>Maor</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Voids of dark energy</article-title><source>Physical Review D</source><volume>75</volume><elocation-id>063507</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevD.75.063507</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dutta</surname><given-names>A</given-names></name><name><surname>Mondal</surname><given-names>A</given-names></name><name><surname>Dey</surname><given-names>N</given-names></name><name><surname>Sen</surname><given-names>S</given-names></name><name><surname>Moraru</surname><given-names>L</given-names></name><name><surname>Hassanien</surname><given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Vision tracking: a survey of the state-of-the-art</article-title><source>SN Computer Science</source><volume>1</volume><elocation-id>57</elocation-id><pub-id pub-id-type="doi">10.1007/s42979-019-0059-z</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyer</surname><given-names>AG</given-names></name><name><surname>Neumeyer</surname><given-names>C</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Honeybee (Apis mellifera) vision can discriminate between and recognise images of human faces</article-title><source>The Journal of Experimental Biology</source><volume>208</volume><fpage>4709</fpage><lpage>4714</lpage><pub-id pub-id-type="doi">10.1242/jeb.01929</pub-id><pub-id pub-id-type="pmid">16326952</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egnor</surname><given-names>SER</given-names></name><name><surname>Branson</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Computational Analysis of Behavior</article-title><source>Annual Review of Neuroscience</source><volume>39</volume><fpage>217</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013845</pub-id><pub-id pub-id-type="pmid">27090952</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Frisch</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1914">1914</year><source>Der Farbensinn Und Formensinn Der Biene</source><publisher-name>Bio Diversity Library</publisher-name></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Eichmann</surname><given-names>B</given-names></name><name><surname>Menzel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Symmetry perception in an insect</article-title><source>Nature</source><volume>382</volume><fpage>458</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1038/382458a0</pub-id><pub-id pub-id-type="pmid">18610516</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Hammer</surname><given-names>M</given-names></name><name><surname>Stach</surname><given-names>S</given-names></name><name><surname>Stollhoff</surname><given-names>N</given-names></name><name><surname>Müller-deisig</surname><given-names>N</given-names></name><name><surname>Mizyrycki</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Pattern learning by honeybees: conditioning procedure and recognition strategy</article-title><source>Animal Behaviour</source><volume>57</volume><fpage>315</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1006/anbe.1998.0957</pub-id><pub-id pub-id-type="pmid">10049470</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Marcout</surname><given-names>C</given-names></name><name><surname>Hilpert</surname><given-names>P</given-names></name><name><surname>Thevenot</surname><given-names>C</given-names></name><name><surname>Rugani</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>An insect brain organizes numbers on a left-to-right mental number line</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2203584119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2203584119</pub-id><pub-id pub-id-type="pmid">36252101</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Green</surname><given-names>DM</given-names></name><name><surname>Swets</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1966">1966</year><source>Signal Detection Theory and Psychophysics</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guiraud</surname><given-names>M</given-names></name><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High-speed videography reveals how honeybees can turn a spatial concept learning task into a simple discrimination task by stereotyped flight movements and sequential inspection of pattern elements</article-title><source>Frontiers in Psychology</source><volume>9</volume><elocation-id>1347</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2018.01347</pub-id><pub-id pub-id-type="pmid">30123157</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guiraud</surname><given-names>M</given-names></name><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Wolf</surname><given-names>S</given-names></name><name><surname>Woodgate</surname><given-names>JL</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Discrimination of edge orientation by bumblebees</article-title><source>PLOS ONE</source><volume>17</volume><elocation-id>e0263198</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0263198</pub-id><pub-id pub-id-type="pmid">35709295</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guiraud</surname><given-names>MG</given-names></name><name><surname>Gallo</surname><given-names>V</given-names></name><name><surname>Quinsal-Keel</surname><given-names>E</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2025">2025a</year><article-title>Bumble bee visual learning: simple solutions for complex stimuli</article-title><source>Animal Behaviour</source><volume>221</volume><elocation-id>123070</elocation-id><pub-id pub-id-type="doi">10.1016/j.anbehav.2024.123070</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guiraud</surname><given-names>MG</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Woodgate</surname><given-names>J</given-names></name><name><surname>Bates</surname><given-names>OK</given-names></name><name><surname>Rodriguez</surname><given-names>OR</given-names></name><name><surname>Gallo</surname><given-names>V</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2025">2025b</year><article-title>How bumblebees manage conflicting information seen on arrival and departure from flowers</article-title><source>Animal Cognition</source><volume>28</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.1007/s10071-024-01926-x</pub-id><pub-id pub-id-type="pmid">39909894</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinrich</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>Bee flowers: a hypothesis on flower variety and blooming times</article-title><source>Evolution; International Journal of Organic Evolution</source><volume>29</volume><fpage>325</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1111/j.1558-5646.1975.tb00212.x</pub-id><pub-id pub-id-type="pmid">28555846</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinrich</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Resource heterogeneity and patterns of movement in foraging bumblebees</article-title><source>Oecologia</source><volume>40</volume><fpage>235</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1007/BF00345321</pub-id><pub-id pub-id-type="pmid">28309608</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertz</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1929">1929</year><article-title>Die Organisation des optischen Feldes bei der Biene. I</article-title><source>Zeitschrift Für Vergleichende Physiologie</source><volume>8</volume><fpage>693</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1007/BF00340937</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertz</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1935">1935</year><article-title>Die Untersuchungen über den Formensinn der Honigbiene</article-title><source>The Science of Nature</source><volume>23</volume><fpage>618</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1007/BF01493245</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horridge</surname><given-names>GA</given-names></name><name><surname>Zhang</surname><given-names>SW</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Pattern vision in honeybees (Apis mellifera): flower-like patterns with no predominant orientation</article-title><source>Journal of Insect Physiology</source><volume>41</volume><fpage>681</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.1016/0022-1910(95)00021-L</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horridge</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Pattern vision of the honeybee (Apis mellifera): the significance of the angle subtended by the target</article-title><source>Journal of Insect Physiology</source><volume>42</volume><fpage>693</fpage><lpage>703</lpage><pub-id pub-id-type="doi">10.1016/0022-1910(96)00004-2</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jewell</surname><given-names>G</given-names></name><name><surname>McCourt</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Pseudoneglect: a review and meta-analysis of performance factors in line bisection tasks</article-title><source>Neuropsychologia</source><volume>38</volume><fpage>93</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/s0028-3932(99)00045-7</pub-id><pub-id pub-id-type="pmid">10617294</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>SD</given-names></name><name><surname>Anderson</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Coevolution between food-rewarding flowers and their pollinators</article-title><source>Evolution</source><volume>3</volume><fpage>32</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1007/s12052-009-0192-6</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How a fly photoreceptor samples light information in time</article-title><source>The Journal of Physiology</source><volume>595</volume><fpage>5427</fpage><lpage>5437</lpage><pub-id pub-id-type="doi">10.1113/JP273645</pub-id><pub-id pub-id-type="pmid">28233315</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Takalo</surname><given-names>J</given-names></name><name><surname>Kemppainen</surname><given-names>J</given-names></name><name><surname>Haghighi</surname><given-names>KR</given-names></name><name><surname>Scales</surname><given-names>B</given-names></name><name><surname>McManus</surname><given-names>J</given-names></name><name><surname>Bridges</surname><given-names>A</given-names></name><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Theory of morphodynamic information processing: Linking sensing to behaviour</article-title><source>Vision Research</source><volume>227</volume><elocation-id>108537</elocation-id><pub-id pub-id-type="doi">10.1016/j.visres.2024.108537</pub-id><pub-id pub-id-type="pmid">39755072</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kagan</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Active vision: fixational eye movements help seeing space in time</article-title><source>Current Biology</source><volume>22</volume><fpage>R186</fpage><lpage>R188</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.02.009</pub-id><pub-id pub-id-type="pmid">22440800</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemppainen</surname><given-names>J</given-names></name><name><surname>Scales</surname><given-names>B</given-names></name><name><surname>Razban Haghighi</surname><given-names>K</given-names></name><name><surname>Takalo</surname><given-names>J</given-names></name><name><surname>Mansour</surname><given-names>N</given-names></name><name><surname>McManus</surname><given-names>J</given-names></name><name><surname>Leko</surname><given-names>G</given-names></name><name><surname>Saari</surname><given-names>P</given-names></name><name><surname>Hurcomb</surname><given-names>J</given-names></name><name><surname>Antohi</surname><given-names>A</given-names></name><name><surname>Suuronen</surname><given-names>JP</given-names></name><name><surname>Blanchard</surname><given-names>F</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name><name><surname>Song</surname><given-names>Z</given-names></name><name><surname>Hampton</surname><given-names>M</given-names></name><name><surname>Eckermann</surname><given-names>M</given-names></name><name><surname>Westermeier</surname><given-names>F</given-names></name><name><surname>Frohn</surname><given-names>J</given-names></name><name><surname>Hoekstra</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>CH</given-names></name><name><surname>Huttula</surname><given-names>M</given-names></name><name><surname>Mokso</surname><given-names>R</given-names></name><name><surname>Juusola</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Binocular mirror-symmetric microsaccadic sampling enables <italic>Drosophila</italic> hyperacute 3D vision</article-title><source>PNAS</source><volume>119</volume><elocation-id>e2109717119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2109717119</pub-id><pub-id pub-id-type="pmid">35298337</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kral</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Side-to-side head movements to obtain motion depth cues: a short review of research on the praying mantis</article-title><source>Behavioural Processes</source><volume>43</volume><fpage>71</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/s0376-6357(98)00007-2</pub-id><pub-id pub-id-type="pmid">24897642</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kral</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Behavioural-analytical studies of the role of head movements in depth perception in insects, birds and mammals</article-title><source>Behavioural Processes</source><volume>64</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/s0376-6357(03)00054-8</pub-id><pub-id pub-id-type="pmid">12914988</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuang</surname><given-names>X</given-names></name><name><surname>Poletti</surname><given-names>M</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Rucci</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Temporal encoding of spatial information during active visual fixation</article-title><source>Current Biology</source><volume>22</volume><fpage>510</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.01.050</pub-id><pub-id pub-id-type="pmid">22342751</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Motion and vision: why animals move their eyes</article-title><source>Journal of Comparative Physiology A</source><volume>185</volume><fpage>341</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1007/s003590050393</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name><name><surname>Nilsson</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Animal Eyes</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199581139.001.0001</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langridge</surname><given-names>KV</given-names></name><name><surname>Wilke</surname><given-names>C</given-names></name><name><surname>Riabinina</surname><given-names>O</given-names></name><name><surname>Vorobyev</surname><given-names>M</given-names></name><name><surname>Hempel de Ibarra</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Approach direction prior to landing explains patterns of colour learning in bees</article-title><source>Frontiers in Physiology</source><volume>12</volume><elocation-id>697886</elocation-id><pub-id pub-id-type="doi">10.3389/fphys.2021.697886</pub-id><pub-id pub-id-type="pmid">34955870</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehrer</surname><given-names>M</given-names></name><name><surname>Wehner</surname><given-names>R</given-names></name><name><surname>Srinivasan</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Visual scanning behaviour in honeybees</article-title><source>Journal of Comparative Physiology. A, Sensory, Neural, and Behavioral Physiology</source><volume>157</volume><fpage>405</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1007/BF00615140</pub-id><pub-id pub-id-type="pmid">3837094</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehrer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Spatial vision in the honeybee: the use of different cues in different tasks</article-title><source>Vision Research</source><volume>34</volume><fpage>2363</fpage><lpage>2385</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)90282-8</pub-id><pub-id pub-id-type="pmid">7975277</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehrer</surname><given-names>M</given-names></name><name><surname>Srinivasan</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Active vision in honeybees: task-oriented suppression of an innate behaviour</article-title><source>Vision Research</source><volume>34</volume><fpage>511</fpage><lpage>516</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)90164-3</pub-id><pub-id pub-id-type="pmid">8303834</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehrer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Looking all around: honeybees use different cues in different eye regions</article-title><source>The Journal of Experimental Biology</source><volume>201 (Pt 24)</volume><fpage>3275</fpage><lpage>3292</lpage><pub-id pub-id-type="doi">10.1242/jeb.201.24.3275</pub-id><pub-id pub-id-type="pmid">9817826</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lent</surname><given-names>DD</given-names></name><name><surname>Graham</surname><given-names>P</given-names></name><name><surname>Collett</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visual scene perception in navigating wood ants</article-title><source>Current Biology</source><volume>23</volume><fpage>684</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.03.016</pub-id><pub-id pub-id-type="pmid">23583550</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Letzkus</surname><given-names>P</given-names></name><name><surname>Boeddeker</surname><given-names>N</given-names></name><name><surname>Wood</surname><given-names>JT</given-names></name><name><surname>Zhang</surname><given-names>SW</given-names></name><name><surname>Srinivasan</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Lateralization of visual learning in the honeybee</article-title><source>Biology Letters</source><volume>4</volume><fpage>16</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1098/rsbl.2007.0466</pub-id><pub-id pub-id-type="pmid">18029300</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>H</given-names></name><name><surname>Galpayage Dona</surname><given-names>HS</given-names></name><name><surname>Gatto</surname><given-names>E</given-names></name><name><surname>Loukola</surname><given-names>OJ</given-names></name><name><surname>Buckley</surname><given-names>E</given-names></name><name><surname>Onoufriou</surname><given-names>PD</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Bumblebees use sequential scanning of countable items in visual patterns to solve numerosity tasks</article-title><source>Integrative and Comparative Biology</source><volume>60</volume><fpage>929</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1093/icb/icaa025</pub-id><pub-id pub-id-type="pmid">32369562</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Honeybees solve a multi-comparison ranking task by probability matching</article-title><source>Proceedings. Biological Sciences</source><volume>287</volume><elocation-id>20201525</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2020.1525</pub-id><pub-id pub-id-type="pmid">32873200</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MaBouDi</surname><given-names>HaDi</given-names></name><name><surname>Marshall</surname><given-names>JAR</given-names></name><name><surname>Dearden</surname><given-names>N</given-names></name><name><surname>Barron</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>How honey bees make fast and accurate decisions</article-title><source>eLife</source><volume>12</volume><elocation-id>e86176</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.86176</pub-id><pub-id pub-id-type="pmid">37365884</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maddess</surname><given-names>T</given-names></name><name><surname>Yang</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Orientation-sensitive neurons in the brain of the honey bee (Apis mellifera)</article-title><source>Journal of Insect Physiology</source><volume>43</volume><fpage>329</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1016/s0022-1910(96)00111-4</pub-id><pub-id pub-id-type="pmid">12769894</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mamiya</surname><given-names>A</given-names></name><name><surname>Straw</surname><given-names>AD</given-names></name><name><surname>Tómasson</surname><given-names>E</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Active and passive antennal movements during visually guided steering in flying <italic>Drosophila</italic></article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>6900</fpage><lpage>6914</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0498-11.2011</pub-id><pub-id pub-id-type="pmid">21543620</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Conde</surname><given-names>S</given-names></name><name><surname>Macknik</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Fixational eye movements across vertebrates: comparative dynamics, physiology, and perception</article-title><source>Journal of Vision</source><volume>8</volume><elocation-id>28</elocation-id><pub-id pub-id-type="doi">10.1167/8.14.28</pub-id><pub-id pub-id-type="pmid">19146329</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martinez-Conde</surname><given-names>S</given-names></name><name><surname>Otero-Millan</surname><given-names>J</given-names></name><name><surname>Macknik</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The impact of microsaccades on vision: towards a unified theory of saccadic function</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>83</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1038/nrn3405</pub-id><pub-id pub-id-type="pmid">23329159</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najemnik</surname><given-names>J</given-names></name><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Optimal eye movement strategies in visual search</article-title><source>Nature</source><volume>434</volume><fpage>387</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1038/nature03390</pub-id><pub-id pub-id-type="pmid">15772663</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nath</surname><given-names>T</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>AC</given-names></name><name><surname>Patel</surname><given-names>A</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Using DeepLabCut for 3D markerless pose estimation across species and behaviors</article-title><source>Nature Protocols</source><volume>14</volume><fpage>2152</fpage><lpage>2176</lpage><pub-id pub-id-type="doi">10.1038/s41596-019-0176-0</pub-id><pub-id pub-id-type="pmid">31227823</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nityananda</surname><given-names>V</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Can bees see at a glance?</article-title><source>The Journal of Experimental Biology</source><volume>217</volume><fpage>1933</fpage><lpage>1939</lpage><pub-id pub-id-type="doi">10.1242/jeb.101394</pub-id><pub-id pub-id-type="pmid">24625647</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odenthal</surname><given-names>L</given-names></name><name><surname>Doussot</surname><given-names>C</given-names></name><name><surname>Meyer</surname><given-names>S</given-names></name><name><surname>Bertrand</surname><given-names>OJN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Analysing head-thorax choreography during free-flights in bumblebees</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>14</volume><elocation-id>610029</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2020.610029</pub-id><pub-id pub-id-type="pmid">33510626</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Phillips-Portillo</surname><given-names>J</given-names></name><name><surname>Dacks</surname><given-names>AM</given-names></name><name><surname>Fellous</surname><given-names>JM</given-names></name><name><surname>Gronenberg</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The processing of color, motion, and stimulus timing are anatomically segregated in the bumblebee brain</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>6319</fpage><lpage>6332</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1196-08.2008</pub-id><pub-id pub-id-type="pmid">18562602</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Dacks</surname><given-names>AM</given-names></name><name><surname>Phillips-Portillo</surname><given-names>J</given-names></name><name><surname>Fellous</surname><given-names>JM</given-names></name><name><surname>Gronenberg</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Visual processing in the central bee brain</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>9987</fpage><lpage>9999</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1325-09.2009</pub-id><pub-id pub-id-type="pmid">19675233</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Philippides</surname><given-names>A</given-names></name><name><surname>de Ibarra</surname><given-names>NH</given-names></name><name><surname>Riabinina</surname><given-names>O</given-names></name><name><surname>Collett</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Bumblebee calligraphy: the design and control of flight motifs in the learning and return flights of Bombus terrestris</article-title><source>The Journal of Experimental Biology</source><volume>216</volume><fpage>1093</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1242/jeb.081455</pub-id><pub-id pub-id-type="pmid">23447668</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roper</surname><given-names>M</given-names></name><name><surname>Fernando</surname><given-names>C</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Insect bio-inspired neural network provides new evidence on how simple feature detectors can enable complex visual generalization and stimulus location invariance in the miniature brain of honeybees</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005333</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005333</pub-id><pub-id pub-id-type="pmid">28158189</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rucci</surname><given-names>M</given-names></name><name><surname>Poletti</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Control and functions of fixational eye movements</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>499</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035742</pub-id><pub-id pub-id-type="pmid">27795997</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rugani</surname><given-names>R</given-names></name><name><surname>Vallortigara</surname><given-names>G</given-names></name><name><surname>Priftis</surname><given-names>K</given-names></name><name><surname>Regolin</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Animal cognition. Number-space mapping in the newborn chick resembles humans’ mental number line</article-title><source>Science</source><volume>347</volume><fpage>534</fpage><lpage>536</lpage><pub-id pub-id-type="doi">10.1126/science.aaa1379</pub-id><pub-id pub-id-type="pmid">25635096</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Photoreceptor spectral sensitivity in the bumblebee, Bombus impatiens (Hymenoptera: Apidae)</article-title><source>PLOS ONE</source><volume>5</volume><elocation-id>e12049</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0012049</pub-id><pub-id pub-id-type="pmid">20711523</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><source>Active Vision: A Broader Comparative Perspective Is Needed</source><publisher-name>ChittkaLab</publisher-name></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sobel</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>The locust’s use of motion parallax to measure distance</article-title><source>Journal of Comparative Physiology. A, Sensory, Neural, and Behavioral Physiology</source><volume>167</volume><fpage>579</fpage><lpage>588</lpage><pub-id pub-id-type="doi">10.1007/BF00192653</pub-id><pub-id pub-id-type="pmid">2074547</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spaethe</surname><given-names>J</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Interindividual variation of eye optics and single object resolution in bumblebees</article-title><source>The Journal of Experimental Biology</source><volume>206</volume><fpage>3447</fpage><lpage>3453</lpage><pub-id pub-id-type="doi">10.1242/jeb.00570</pub-id><pub-id pub-id-type="pmid">12939375</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spaethe</surname><given-names>J</given-names></name><name><surname>Tautz</surname><given-names>J</given-names></name><name><surname>Chittka</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Do honeybees detect colour targets using serial or parallel visual search?</article-title><source>The Journal of Experimental Biology</source><volume>209</volume><fpage>987</fpage><lpage>993</lpage><pub-id pub-id-type="doi">10.1242/jeb.02124</pub-id><pub-id pub-id-type="pmid">16513924</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Lehrer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Temporal acuity of honeybee vision: behavioural studies using moving stimuli</article-title><source>Journal of Comparative Physiology A</source><volume>155</volume><fpage>297</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1007/BF00610583</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Lehrer</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Spatial acuity of honeybee vision and its spectral properties</article-title><source>Journal of Comparative Physiology A</source><volume>162</volume><fpage>159</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1007/BF00606081</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Pattern recognition in the honeybee: Recent progress</article-title><source>Journal of Insect Physiology</source><volume>40</volume><fpage>183</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/0022-1910(94)90041-8</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Zhang</surname><given-names>SW</given-names></name><name><surname>Witney</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Visual discrimination of pattern orientation by honeybees: performance and implications for “cortical” processing</article-title><source>Philosophical Transactions of the Royal Society of London. Series B</source><volume>343</volume><fpage>199</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1098/rstb.1994.0021</pub-id><pub-id pub-id-type="pmid">7800707</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Honey bees as a model for vision, perception, and cognition</article-title><source>Annual Review of Entomology</source><volume>55</volume><fpage>267</fpage><lpage>284</lpage><pub-id pub-id-type="doi">10.1146/annurev.ento.010908.164537</pub-id><pub-id pub-id-type="pmid">19728835</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stach</surname><given-names>S</given-names></name><name><surname>Benard</surname><given-names>J</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Local-feature assembling in visual pattern recognition and generalization in honeybees</article-title><source>Nature</source><volume>429</volume><fpage>758</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1038/nature02594</pub-id><pub-id pub-id-type="pmid">15201910</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stach</surname><given-names>S</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The influence of training length on generalization of visual feature assemblies in honeybees</article-title><source>Behavioural Brain Research</source><volume>161</volume><fpage>8</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2005.02.008</pub-id><pub-id pub-id-type="pmid">15904705</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stowers</surname><given-names>JR</given-names></name><name><surname>Hofbauer</surname><given-names>M</given-names></name><name><surname>Bastien</surname><given-names>R</given-names></name><name><surname>Griessner</surname><given-names>J</given-names></name><name><surname>Higgins</surname><given-names>P</given-names></name><name><surname>Farooqui</surname><given-names>S</given-names></name><name><surname>Fischer</surname><given-names>RM</given-names></name><name><surname>Nowikovsky</surname><given-names>K</given-names></name><name><surname>Haubensak</surname><given-names>W</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name><name><surname>Tessmar-Raible</surname><given-names>K</given-names></name><name><surname>Straw</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Virtual reality for freely moving animals</article-title><source>Nature Methods</source><volume>14</volume><fpage>995</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1038/nmeth.4399</pub-id><pub-id pub-id-type="pmid">28825703</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>LM</given-names></name><name><surname>Troje</surname><given-names>NF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Head stabilization in the pigeon: role of vision to correct for translational and rotational disturbances</article-title><source>Frontiers in Neuroscience</source><volume>11</volume><elocation-id>551</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2017.00551</pub-id><pub-id pub-id-type="pmid">29051726</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>CH</given-names></name></person-group><year iso-8601-date="1911">1911</year><article-title>Experiments on pattern-vision of the honey bee</article-title><source>The Biological Bulletin</source><volume>21</volume><fpage>249</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.2307/1536017</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Wait</surname><given-names>PB</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Pattern recognition in bees: orientation discrimination</article-title><source>Journal of Comparative Physiology A</source><volume>167</volume><fpage>649</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1007/BF00192658</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wakakuwa</surname><given-names>M</given-names></name><name><surname>Kurasawa</surname><given-names>M</given-names></name><name><surname>Giurfa</surname><given-names>M</given-names></name><name><surname>Arikawa</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Spectral heterogeneity of honeybee ommatidia</article-title><source>Die Naturwissenschaften</source><volume>92</volume><fpage>464</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1007/s00114-005-0018-5</pub-id><pub-id pub-id-type="pmid">16136295</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1967">1967</year><article-title>Pattern Recognition in Bees</article-title><source>Nature</source><volume>215</volume><fpage>1244</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.1038/2151244a0</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Werner</surname><given-names>A</given-names></name><name><surname>Stürzl</surname><given-names>W</given-names></name><name><surname>Zanker</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Object recognition in flight: how do bees distinguish between 3D shapes?</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0147106</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0147106</pub-id><pub-id pub-id-type="pmid">26886006</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>TI</given-names></name><name><surname>Chiao</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Number sense and state-dependent valuation in cuttlefish</article-title><source>Proceedings. Biological Sciences</source><volume>283</volume><elocation-id>20161379</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2016.1379</pub-id><pub-id pub-id-type="pmid">27559063</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yarbus</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Eye Movements and Vision</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4899-5379-7</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106332.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Desplan</surname><given-names>Claude</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>This useful study revisits a challenging visual learning task in bumblebees. By analyzing the flight trajectory and body orientation in a learning paradigm, the authors discovered an active vision strategy. This contributes to our knowledge of the bumblebee flight behavior. The convincing work will be of interest to researchers working in neuroethology.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106332.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Desplan</surname><given-names>Claude</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting the paper &quot;Exploring active vision of bees in a simple pattern discrimination task&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor.</p><p>Comments to the Authors:</p><p>We are sorry to say that, after consultation with the reviewers, we have decided that this work will not be considered further for publication by <italic>eLife</italic>.</p><p>Several aspects of this potentially useful study are novel and interesting. The observation of active vision is exciting and the use of affordable technological solutions was valued by the reviewers. These qualities did however not mitigate the lack of thorough analyses. More details about the paradigm should be provided to ensure that the work is reproducible and more rigorous experiments are necessary to characterize the mechanism of active vision. As a result, the evidence supporting the claims is incomplete and the study is largely inconclusive: it leaves more questions than answers. Prior to the publication of the work, preliminary observations ought to be generalized. At a minimum, the authors should analyze behavior in the transfer test when the animals are either presented with top and bottom half of the x and plus signs.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The authors present their findings and analysis on the flight behaviour of bumblebees as they perform a visual discrimination task when presented with a rewarding and aversive stimulus. This is a descriptive study where the behaviour of the bees is well explained but I am not sure about the extent of the novelty of the work or the overall impact on the field in general. However, the authors make some interesting observations of the scanning behaviour of the bees and highlight the value of analysing the flight behaviour in these choice experiments. The discussion on nascent and readily available technologies such as cameras on phones and moderate computing power to analyse flight videos to gain deeper insight into animal behaviour is particularly good.</p><p>From a neuroethology standpoint, the depth of inference on the bees' visually guided behaviour that the data presented here can provide is difficult to gauge. I say this because while the scanning behaviour is interesting and well described, it is unclear if this is generalizable. For example, there is no data on the scanning behaviour when only the top or bottom half is presented in the transfer test. Do the bees still scan the bottom when only the top half is presented only to find no information and then choose randomly? It is unclear if the scanning behaviour is indeed general. In the same way, if the bees were trained on only the top half and then in transfer presented with the x and plus signs – would they scan only the top section and then would the scanning strategy change? Such questions naturally arise since the data presented in the experiments here cannot clarify if the scanning behaviour observed is indeed general.</p><p>If we were to consider approaching the experiment as a &quot;minimization of information redundancy&quot; task where the task is solved by learning the minimum salient information necessary to discriminate the two choices – in this case it would be the part of the pattern in the lower left. The rest of the stimuli is basically redundant information and unnecessary to learn. The bees' behaviour appears to conform to this reasoning, right? If so then another experiment would be needed to support or disprove this and suggest that the scanning strategy is indeed general.</p><p>At a minimum it would be good to include analysis of the bees' flight behaviour during the choice in the transfer test when they were either presented with top and bottom half of the x and plus signs.</p><p>I would also like the authors to clarify if the bees were able to discriminate if only the top halves of the patterns are presented? I ask because all the figures (Figure 1and 2) indicate that only the top half was presented in the transfer test but then in the text (line 200) it is mentioned that the bees were unable to discriminate for this case and managed to make the correct choice when only the bottom half was presented. If this was indeed the case, then it would be easier to follow if the figures presented the transfer test with the bottom half.</p><p>The authors make a statement in the abstract about serial and parallelization and neural processing however it seems like a stretch given the relatively narrow experiment scope performed here. I suggest the authors remove or significantly reduce emphasis to this.</p><p>The authors mention flight &quot;dynamics&quot; this should be corrected across the manuscript to flight &quot;trajectories&quot;. The former implies analysis of forces which was not performed here.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>This study revisits a challenging visual learning task in bumblebees. By analyzing the flight trajectory and body orientation in a learning paradigm, the authors discovered an active vision strategy.</p><p>1) More details about the paradigm should be provided, either in the method section or in the main text. For example, how long does a training bout take on average? How long are two training bouts separated? Were the training bouts and tests conducted within a certain time on the same day? Further, analyses of the learning performance in each training bout and how the behavior changes over training bouts would be highly informative in revealing the task-solving strategies of bees.</p><p>2) Although bees' ability to use active vision to solve a task is a fascinating discovery, the study leaves more questions than answers. For example, what would happen if the transfer experiment used only the lower half? What would happen if a single tilted bar or a vertical bar was used? What strategy would bees use if the location of the reward port is placed in other locations of the visual patterns? Does the number of bars in each pattern matter?</p><p>This is an exciting discovery and has a huge potential. But the study seemed prematurely wrapped up. More rigorous follow-up experiments are strongly desired.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>This paper largely repeats the findings in Langridge et al. (2021). I am not sure what this paper brings that's new, but maybe I missed something. The paper is difficult to follow, as the structure is not streamlined and the references to figures are chaotic. In addition, large parts of the Methods appear in the Results or figure Legends.</p><p>The experiments need to be redone to bring in some novel ideas and to be differentiated from the Langridge study. The paper should also be re-written in a logical and clear manner, that highlights the novelty of the results.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106332.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Several aspects of this potentially useful study are novel and interesting. The observation of active vision is exciting and the use of affordable technological solutions was valued by the reviewers. These qualities did however not mitigate the lack of thorough analyses. More details about the paradigm should be provided to ensure that the work is reproducible and more rigorous experiments are necessary to characterize the mechanism of active vision. As a result, the evidence supporting the claims is incomplete and the study is largely inconclusive: it leaves more questions than answers. Prior to the publication of the work, preliminary observations ought to be generalized. At a minimum, the authors should analyze behavior in the transfer test when the animals are either presented with top and bottom half of the x and plus signs.</p></disp-quote><p>These additional experiments have now been performed and new analyses added – see our detailed replies below.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>The authors present their findings and analysis on the flight behaviour of bumblebees as they perform a visual discrimination task when presented with a rewarding and aversive stimulus. This is a descriptive study where the behaviour of the bees is well explained but I am not sure about the extent of the novelty of the work or the overall impact on the field in general. However, the authors make some interesting observations of the scanning behaviour of the bees and highlight the value of analysing the flight behaviour in these choice experiments. The discussion on nascent and readily available technologies such as cameras on phones and moderate computing power to analyse flight videos to gain deeper insight into animal behaviour is particularly good.</p></disp-quote><p>Thank you for your thoughtful and detailed feedback. We appreciate your recognition of the value of analysing bumblebee scanning behaviour using camera technologies and the significance of our findings on their visual discrimination strategies. We believe our study provides novel insights into bumblebee visual sampling and decision-making and highlights cost-effective methods for behavioural analysis. We clarified the novelty and impact in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>From a neuroethology standpoint, the depth of inference on the bees' visually guided behaviour that the data presented here can provide is difficult to gauge. I say this because while the scanning behaviour is interesting and well described, it is unclear if this is generalizable. For example, there is no data on the scanning behaviour when only the top or bottom half is presented in the transfer test. Do the bees still scan the bottom when only the top half is presented only to find no information and then choose randomly? It is unclear if the scanning behaviour is indeed general. In the same way, if the bees were trained on only the top half and then in transfer presented with the x and plus signs – would they scan only the top section and then would the scanning strategy change? Such questions naturally arise since the data presented in the experiments here cannot clarify if the scanning behaviour observed is indeed general.</p></disp-quote><p>Thank you for pointing this out. We have conducted a new experiment and performed additional video analysis to address the ‘generalisation’ framework. The updated manuscript has been revised to address your concern. Please see below for more details.</p><disp-quote content-type="editor-comment"><p>If we were to consider approaching the experiment as a &quot;minimization of information redundancy&quot; task where the task is solved by learning the minimum salient information necessary to discriminate the two choices – in this case it would be the part of the pattern in the lower left. The rest of the stimuli is basically redundant information and unnecessary to learn. The bees' behaviour appears to conform to this reasoning, right? If so then another experiment would be needed to support or disprove this and suggest that the scanning strategy is indeed general.</p></disp-quote><p>Thank you for the suggestion. We agree that further experiments were necessary to test whether the scanning strategy reflects a general mechanism for minimising information redundancy. Below, we address your concerns about the novelty, generalisability, and depth of inference provided by our study, with a specific focus on the additional data and analyses included in the revised manuscript.</p><p>1. Novelty and Impact of the Study</p><p>Our study advances the understanding of bumblebee visual processing by demonstrating their use of an active, feature-based scanning strategy. The novelty lies in showing that bumblebees prioritise specific, task-relevant features of visual stimuli—particularly those in the bottom half of the patterns—rather than uniformly processing the entire pattern. This behaviour reflects a strategy to reduce cognitive load while maximising efficiency in decision-making. The use of bottom-half and top-half transfer tests further underscores this selective strategy, as bees successfully discriminated patterns using cues in the bottom halves but failed to generalise when only the top halves were presented (New figures 2 and 5). This work contributes to the broader field of neuroethology by providing empirical evidence of how bumblebees adapt their scanning behaviours to the most informative regions of their visual environment.</p><p>2. Generalisation of Scanning Behaviour</p><p>To address your concerns about generalisability, we included additional transfer tests and detailed trajectory analyses. The results show that bees exhibit a strong preference for scanning the lower regions of the stimuli during both training and transfer tests. In the bottom-half test, bees successfully discriminated patterns based on the available cues, demonstrating the robustness of their learned scanning strategy. However, in the top-half test, despite the absence of cues in the lower regions, bees persisted in scanning the bottom half, highlighting the reliance on pre-learned spatial regions for decision-making. These findings suggest that while the scanning strategy is consistent and robust under familiar conditions. It also demonstrates limited flexibility when faced with configurations where the learned visual cues are absent from their previously reinforced regions.</p><p>3. Relevance of &quot;Minimisation of Information Redundancy&quot;</p><p>While our findings align with aspects of the &quot;minimisation of information redundancy&quot; framework, they also challenge its sufficiency in explaining bumblebee behaviour. The results suggest that bees actively focus on specific, pre-learned features rather than simply reducing redundant information across the entire pattern. The failure to discriminate in the top-half test indicates that their decision-making depends on the availability of key visual features rather than a uniform simplification strategy. Furthermore, analyses of flight trajectories, hovering times, and velocity distributions provide evidence of deliberate and task-specific adjustments in scanning behaviour, reinforcing the role of active feature selection over passive redundancy minimisation.</p><p>4. Depth of Inference on Visually Guided Behaviour</p><p>Our study provides detailed insights into the mechanisms of bumblebee pattern discrimination through advanced video analyses. Heatmaps and trajectory data reveal targeted scanning and hovering behaviour concentrated on the most informative regions of the patterns. These findings support the hypothesis that bumblebees employ a spatially selective, feature-based strategy to guide their decision-making. While the current experiments focus on specific visual patterns, they establish a foundation for future investigations into the generality and adaptability of these behaviours across different stimuli and environmental conditions.</p><p>In summary, we believe that our new data and the additional detailed analyses offer valuable insights into bumblebees' visual scanning behaviour. We sincerely thank you again for your constructive comments, which have significantly enhanced the quality of the manuscript.</p><disp-quote content-type="editor-comment"><p>At a minimum it would be good to include analysis of the bees' flight behaviour during the choice in the transfer test when they were either presented with top and bottom half of the x and plus signs.</p></disp-quote><p>Thank you for your suggestion. We have included an analysis of the bees' flight behaviour during the transfer tests where they were presented with either the top or bottom halves of the stimuli. This analysis provides insights into how bees interacted with these patterns and confirms that their ability to discriminate is primarily driven by the bottom halves of the patterns. The results are now included in the revised manuscript and are supported by updated figures to ensure clarity and consistency.</p><disp-quote content-type="editor-comment"><p>I would also like the authors to clarify if the bees were able to discriminate if only the top halves of the patterns are presented? I ask because all the figures (Figure 1and 2) indicate that only the top half was presented in the transfer test but then in the text (line 200) it is mentioned that the bees were unable to discriminate for this case and managed to make the correct choice when only the bottom half was presented. If this was indeed the case, then it would be easier to follow if the figures presented the transfer test with the bottom half.</p></disp-quote><p>Thank you for pointing this out. We conducted an additional experiment to clarify this question. Consequently, we have refined the text and updated the figures to ensure consistency and provide a clearer representation of the findings.</p><disp-quote content-type="editor-comment"><p>The authors make a statement in the abstract about serial and parallelization and neural processing however it seems like a stretch given the relatively narrow experiment scope performed here. I suggest the authors remove or significantly reduce emphasis to this.</p></disp-quote><p>Thank you for your feedback. We have revised the abstract accordingly.</p><disp-quote content-type="editor-comment"><p>The authors mention flight &quot;dynamics&quot; this should be corrected across the manuscript to flight &quot;trajectories&quot;. The former implies analysis of forces which was not performed here.</p></disp-quote><p>The term &quot;flight dynamics&quot; has been replaced with &quot;flight trajectories&quot; throughout the manuscript to ensure accuracy and avoid any misinterpretation</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>This study revisits a challenging visual learning task in bumblebees. By analyzing the flight trajectory and body orientation in a learning paradigm, the authors discovered an active vision strategy.</p></disp-quote><p>Thank you for highlighting the discovery of an active vision strategy in bumblebees. In the revised manuscript, we have expanded on this finding by including additional analyses of bees' selective scanning behaviours during pattern discrimination. Specifically, we have conducted a new experiment with 20 bees and provided new data from transfer tests, including the top-half test and single-bar orientation tests, which further confirm the role of active vision in bumblebee decision-making. These results strengthen our conclusion that bumblebees actively employ targeted scanning strategies to enhance their visual learning.</p><disp-quote content-type="editor-comment"><p>1) More details about the paradigm should be provided, either in the method section or in the main text. For example, how long does a training bout take on average? How long are two training bouts separated? Were the training bouts and tests conducted within a certain time on the same day? Further, analyses of the learning performance in each training bout and how the behavior changes over training bouts would be highly informative in revealing the task-solving strategies of bees.</p></disp-quote><p>Thank you for the suggestion. The methods section has been improved with additional details about the experimental paradigm. Specifically, we now include information on the duration of training bouts, the intervals between consecutive bouts, and the timing of training and tests within the same day. Additionally, we have analysed the learning performance in each training bout and how behaviour changes across bouts, providing further insights into the task-solving strategies of bumblebees. These updates ensure greater clarity and reproducibility.</p><disp-quote content-type="editor-comment"><p>2) Although bees' ability to use active vision to solve a task is a fascinating discovery, the study leaves more questions than answers. For example, what would happen if the transfer experiment used only the lower half? What would happen if a single tilted bar or a vertical bar was used? What strategy would bees use if the location of the reward port is placed in other locations of the visual patterns? Does the number of bars in each pattern matter?</p></disp-quote><p>Thank you for raising these important points. In the revised manuscript, we have addressed these questions through additional experiments. Specifically:</p><p>1. Transfer experiment with the lower half of the patterns: We conducted a transfer test using only the bottom halves of the patterns. The results show that bumblebees were able to successfully discriminate between patterns based on the bottom halves, confirming that this region contains critical visual cues for their decision-making.</p><p>2. Transfer experiment with the top half of the patterns: Similarly, we performed a transfer test using only the top halves of the patterns. The results demonstrate that bumblebees could not discriminate between patterns based solely on the top halves, suggesting that they learn to associate the bottom region with relevant visual cues during training.</p><p>3. Transfer experiment with single bars: To explore how bees respond to isolated pattern elements, we conducted a single-bar orientation test. The findings reveal that bees exhibit a clear preference for specific bar orientations associated with the rewarded pattern during training.</p><p>While we did not explicitly test the impact of changing the reward port's location in this study; our previous observations indicate that relocating the reward port can confuse bees and disrupt their learned associations. We believe these additional experiments and insights provide a clearer understanding of the mechanisms underlying bumblebee visual discrimination and address your concerns effectively.</p><disp-quote content-type="editor-comment"><p>This is an exciting discovery and has a huge potential. But the study seemed prematurely wrapped up. More rigorous follow-up experiments are strongly desired.</p></disp-quote><p>Thank you for highlighting this point. We have revised the text to clarify that the approach paths are consistent for individual chickens but vary across different individuals</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>This paper largely repeats the findings in Langridge et al. (2021). I am not sure what this paper brings that's new, but maybe I missed something. The paper is difficult to follow, as the structure is not streamlined and the references to figures are chaotic. In addition, large parts of the Methods appear in the Results or figure Legends.</p><p>The experiments need to be redone to bring in some novel ideas and to be differentiated from the Langridge study. The paper should also be re-written in a logical and clear manner, that highlights the novelty of the results.</p></disp-quote><p>Thank you for your feedback. We have carefully addressed them in the revised manuscript to clarify the unique contributions of our study compared to Langridge et al. (2021). Specifically, our work goes beyond the scope of colour learning to focus on pattern recognition and investigates the role of active vision strategies in bumblebees. This includes advanced analyses of flight paths, body orientations, time hovering and selective scanning behaviours over several tests, offering novel insights into how bumblebees process and distinguish visual patterns. Additionally, to differentiate this work from the Langridge study, we conducted additional experiments, including transfer tests with the top and bottom halves of patterns and single-bar orientation tests. These experiments provide new evidence on how bumblebees use selective scanning strategies to solve visual discrimination tasks, emphasising the novel contributions of this study.</p><p>We have also streamlined the manuscript by improving its clarity and organisation. All sections have been restructured to avoid overlap with figure legends, and the overall presentation has been rewritten in a clear and logical sequence to emphasise the novelty and implications of the findings. We believe these changes significantly enhance the manuscript's impact and further differentiate it from prior work.</p><p>We hope the following summary highlights the differences and the novelty of our study:</p><p>Main Differences Between Langridge et al. (2021) and Our Work:</p><p>1. Focus of Study:</p><p>Langridge et al. (2021): <italic>Focused on colour learning, investigating how bees use</italic> approach direction prior to landing to inform colour preferences/location during floral pattern recognition.</p><p>Current Work: <italic>Focuses on pattern recognition, specifically examining how bumblebees</italic> use active vision strategies to discriminate between geometrical patterns (e.g., a plus sign versus a multiplication sign). Unlike Langridge et al., our study does not involve colour cues but instead highlights shape discrimination.</p><p>2. Methodology:</p><p>Langridge et al. (2021): Relied on analysing approach direction by extracting limited parameters, such as search time, approach angles, and bee positions. This approach does not provide a detailed understanding of individual bees' inspection strategies or their movements over time.</p><p>Current Work: Utilises advanced video analysis tools to track and quantify bee movements, body orientations, hovering time, and scanning behaviours during flight. Our frame-by-frame analysis offers a highly detailed understanding of how bees inspect and interact with visual stimuli.</p><p>3. Focus on Active Vision:</p><p>Langridge et al. (2021): <italic>While the study indirectly touches on the role of scanning,</italic> it does not explicitly explore active vision or how bees dynamically adjust their visual input. Further experiments would be required to confirm the use of active vision strategies in their setup.</p><p>Current Work: <italic>Directly investigates active vision strategies, such as selective scanning</italic> and dynamic visual sampling, which enable bees to identify and focus on specific features of visual patterns. These findings provide novel insights into sensory-motor processes during visual discrimination tasks.</p><p>4. Learning Paradigm:</p><p>Langridge et al. (2021): <italic>Primarily focused on colour learning, a simpler visual</italic> processing task.</p><p>Current Work: <italic>Investigates pattern-based learning and generalisation, exploring how</italic> bees discriminate between geometrical features and transfer learned strategies to novel situations. This represents a significant advancement in understanding bumblebee visual cognition and decision-making.</p></body></sub-article></article>