<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">63055</article-id><article-id pub-id-type="doi">10.7554/eLife.63055</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A new model of decision processing in instrumental learning tasks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-206753"><name><surname>Miletić</surname><given-names>Steven</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7399-2926</contrib-id><email>s.miletic@uva.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-206754"><name><surname>Boag</surname><given-names>Russell J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7689-0682</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-206755"><name><surname>Trutti</surname><given-names>Anne C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0044-4846</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-218790"><name><surname>Stevenson</surname><given-names>Niek</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3206-7544</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-17433"><name><surname>Forstmann</surname><given-names>Birte U</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1005-1675</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-206756"><name><surname>Heathcote</surname><given-names>Andrew</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4324-5537</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>University of Amsterdam, Department of Psychology</institution><addr-line><named-content content-type="city">Amsterdam</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution>Leiden University, Department of Psychology</institution><addr-line><named-content content-type="city">Leiden</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution>University of Newcastle, School of Psychology</institution><addr-line><named-content content-type="city">Newcastle</named-content></addr-line><country>Australia</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><role>Reviewing Editor</role><aff><institution>École normale supérieure, PSL University, INSERM</institution><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>27</day><month>01</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e63055</elocation-id><history><date date-type="received" iso-8601-date="2020-09-15"><day>15</day><month>09</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-01-26"><day>26</day><month>01</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Miletić et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Miletić et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-63055-v2.pdf"/><abstract><p>Learning and decision-making are interactive processes, yet cognitive modeling of error-driven learning and decision-making have largely evolved separately. Recently, evidence accumulation models (EAMs) of decision-making and reinforcement learning (RL) models of error-driven learning have been combined into joint RL-EAMs that can in principle address these interactions. However, we show that the most commonly used combination, based on the diffusion decision model (DDM) for binary choice, consistently fails to capture crucial aspects of response times observed during reinforcement learning. We propose a new RL-EAM based on an advantage racing diffusion (ARD) framework for choices among two or more options that not only addresses this problem but captures stimulus difficulty, speed-accuracy trade-off, and stimulus-response-mapping reversal effects. The RL-ARD avoids fundamental limitations imposed by the DDM on addressing effects of absolute values of choices, as well as extensions beyond binary choice, and provides a computationally tractable basis for wider applications.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>value-based decision making</kwd><kwd>reinforcement learning</kwd><kwd>computational modelling</kwd><kwd>evidence accumulation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>016.vici.185.052</award-id><principal-award-recipient><name><surname>Forstmann</surname><given-names>Birte U</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>DP150100272</award-id><principal-award-recipient><name><surname>Heathcote</surname><given-names>Andrew</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000923</institution-id><institution>Australian Research Council</institution></institution-wrap></funding-source><award-id>DP160101891</award-id><principal-award-recipient><name><surname>Heathcote</surname><given-names>Andrew</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001827</institution-id><institution>University of Amsterdam</institution></institution-wrap></funding-source><award-id>VIP Grant</award-id><principal-award-recipient><name><surname>Heathcote</surname><given-names>Andrew</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A new model is presented to simultaneously study the latent cognitive processes underlying decision making and instrumental learning.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Learning and decision-making are mutually influential cognitive processes. Learning processes refine the internal preferences and representations that inform decisions, and the outcomes of decisions underpin feedback-driven learning (<xref ref-type="bibr" rid="bib13">Bogacz and Larsen, 2011</xref>). Although this relation between learning and decision-making has been acknowledged (<xref ref-type="bibr" rid="bib13">Bogacz and Larsen, 2011</xref>; <xref ref-type="bibr" rid="bib27">Dayan and Daw, 2008</xref>), the study of cognitive processes underlying feedback-driven learning on the one hand, and of perceptual and value-based decision-making on the other, have progressed as largely separate scientific fields. In the study of error-driven learning (<xref ref-type="bibr" rid="bib68">O'Doherty et al., 2017</xref>; <xref ref-type="bibr" rid="bib95">Sutton and Barto, 2018</xref>), the decision process is typically simplified to soft-max, a descriptive model that offers no process-level understanding of how decisions arise from representations, and ignores choice response times (RTs). In contrast, evidence-accumulation models (EAMs; <xref ref-type="bibr" rid="bib31">Donkin and Brown, 2018</xref>; <xref ref-type="bibr" rid="bib36">Forstmann et al., 2016</xref>; <xref ref-type="bibr" rid="bib81">Ratcliff et al., 2016</xref>) provide a detailed process account of decision-making but are typically applied to tasks that minimize the influence of learning, and residual variability caused by learning is treated as noise.</p><p>Recent advances have emphasized how both modeling traditions can be combined in joint models of reinforcement learning (RL) and evidence-accumulation decision-making processes, providing mutual benefits for both fields (<xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref>; <xref ref-type="bibr" rid="bib35">Fontanesi et al., 2019b</xref>; <xref ref-type="bibr" rid="bib56">Luzardo et al., 2017</xref>; <xref ref-type="bibr" rid="bib58">McDougle and Collins, 2020</xref>; <xref ref-type="bibr" rid="bib61">Miletić et al., 2020</xref>; <xref ref-type="bibr" rid="bib63">Millner et al., 2018</xref>; <xref ref-type="bibr" rid="bib72">Pedersen et al., 2017</xref>; <xref ref-type="bibr" rid="bib73">Pedersen and Frank, 2020</xref>; <xref ref-type="bibr" rid="bib89">Sewell et al., 2019</xref>; <xref ref-type="bibr" rid="bib90">Sewell and Stallman, 2020</xref>; <xref ref-type="bibr" rid="bib91">Shahar et al., 2019</xref>; <xref ref-type="bibr" rid="bib103">Turner, 2019</xref>). Combined models generally propose that value-based decision-making and learning interact as follows: For each decision, a subject gradually accumulates evidence for each choice option by sampling from a running average of the subjective value (or <italic>expected reward</italic>) associated with each choice option (known as <italic>Q-values</italic>). Once a threshold level of evidence is reached, they commit to the decision and initiate a corresponding motor process. The response triggers feedback, which is used to update the internal representation of subjective values. The next time the subject encounters the same choice options this updated internal representation changes evidence accumulation.</p><p>The RL-EAM framework has many benefits (<xref ref-type="bibr" rid="bib61">Miletić et al., 2020</xref>). It allows for studying a rich set of behavioral data simultaneously, including entire RT distributions and trial-by-trial dependencies in choices and RTs. It posits a theory of evidence accumulation that assumes a memory representation of rewards is the source of evidence, and it formalizes how these memory representations change due to learning. It complements earlier work connecting theories of reinforcement learning and decision-making (<xref ref-type="bibr" rid="bib13">Bogacz and Larsen, 2011</xref>; <xref ref-type="bibr" rid="bib27">Dayan and Daw, 2008</xref>) and their potential neural implementation in basal ganglia circuits (<xref ref-type="bibr" rid="bib13">Bogacz and Larsen, 2011</xref>), by presenting a measurement model that can be fit to, and makes predictions about, behavioral data. Adding to benefits in terms of theory building, the RL-EAM framework also has potential to improve parameter recovery properties compared to standard RL models (<xref ref-type="bibr" rid="bib91">Shahar et al., 2019</xref>), and allows for the estimation of single-trial parameters of the decision model, which can be crucial in the analysis of neuroimaging data.</p><p>An important challenge of this framework is the number of modeling options in both the fields of reinforcement learning and decision-making. Even considering only model-free (as opposed to model-based, see <xref ref-type="bibr" rid="bib26">Daw and Dayan, 2014</xref>) reinforcement learning, there exists a variety of learning rules (e.g. <xref ref-type="bibr" rid="bib70">Palminteri et al., 2015</xref>; <xref ref-type="bibr" rid="bib86">Rescorla and Wagner, 1972</xref>; <xref ref-type="bibr" rid="bib87">Rummery and Niranjan, 1994</xref>; <xref ref-type="bibr" rid="bib94">Sutton, 1988</xref>), as well as the possibility of multiple learning rates for positive and negative prediction errors (<xref ref-type="bibr" rid="bib18">Christakou et al., 2013</xref>; <xref ref-type="bibr" rid="bib24">Daw et al., 2002</xref>; <xref ref-type="bibr" rid="bib39">Frank et al., 2009</xref>; <xref ref-type="bibr" rid="bib42">Gershman, 2015</xref>; <xref ref-type="bibr" rid="bib38">Frank et al., 2007</xref>; <xref ref-type="bibr" rid="bib67">Niv et al., 2012</xref>), and many additional concepts, such as eligibility traces to allow for updating of previously visited states (<xref ref-type="bibr" rid="bib4">Barto et al., 1981</xref>; <xref ref-type="bibr" rid="bib11">Bogacz et al., 2007</xref>). Similarly, in the decision-making literature, there exists a wide range of evidence-accumulation models, including most prominently the diffusion decision model (DDM; <xref ref-type="bibr" rid="bib78">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib81">Ratcliff et al., 2016</xref>) and race models such as the linear ballistic accumulator model (LBA; <xref ref-type="bibr" rid="bib17">Brown and Heathcote, 2008</xref>) and racing diffusion (RD) models (<xref ref-type="bibr" rid="bib14">Boucher et al., 2007</xref>; <xref ref-type="bibr" rid="bib44">Hawkins and Heathcote, 2020</xref>; <xref ref-type="bibr" rid="bib54">Leite and Ratcliff, 2010</xref>; <xref ref-type="bibr" rid="bib55">Logan et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Purcell et al., 2010</xref>; <xref ref-type="bibr" rid="bib80">Ratcliff et al., 2011</xref>; <xref ref-type="bibr" rid="bib99">Tillman et al., 2020</xref>).</p><p>The existence of this wide variety of modeling options is a double-edged sword. On the one hand, it highlights the success of the general principles underlying both modeling traditions (i.e. learning from prediction errors and accumulate-to-threshold decisions) in explaining behavior, and it allows for studying specific learning/decision-making phenomena. On the other hand, it constitutes a bewildering combinatorial explosion of potential RL-EAMs; here, we provide empirical grounds to navigate this problem with respect to EAMs.</p><p>The DDM is the dominant EAM as currently used in reinforcement learning (<xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref>; <xref ref-type="bibr" rid="bib35">Fontanesi et al., 2019b</xref>; <xref ref-type="bibr" rid="bib63">Millner et al., 2018</xref>; <xref ref-type="bibr" rid="bib72">Pedersen et al., 2017</xref>; <xref ref-type="bibr" rid="bib73">Pedersen and Frank, 2020</xref>; <xref ref-type="bibr" rid="bib89">Sewell et al., 2019</xref>; <xref ref-type="bibr" rid="bib90">Sewell and Stallman, 2020</xref>; <xref ref-type="bibr" rid="bib91">Shahar et al., 2019</xref>), but this choice is without experimental justification. Furthermore, the DDM has several theoretical drawbacks, such as its inability to explain multi-alternative decision-making and its strong commitment to the accumulation of the evidence <italic>difference</italic>, which leads to difficulties in explaining behavioral effects of absolute stimulus and reward magnitudes without additional mechanisms (<xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref>; <xref ref-type="bibr" rid="bib82">Ratcliff et al., 2018</xref>; <xref ref-type="bibr" rid="bib96">Teodorescu et al., 2016</xref>). Here, we compare the performance of different decision-making models in explaining choice behavior in a variety of instrumental learning tasks. Models that fail to capture crucial aspects of performance run the risk of producing misleading psychological inferences. For EAMs, the full RT distribution (i.e. its level of variability and skew) have proven to be crucial. Hence, it is important to assess which RL-EAMs are able to capture not only learning-related changes in choice probabilities and mean RT, but also the general shape of the entire RT distribution and how it changes with learning. Further, in order to be held forth as a general modeling framework, it is important to capture how all these measures interact with key phenomena in the decision-making and learning literature.</p><p>We compare the RL-DDM with two RL-EAMs based on a racing accumulator architecture (<xref ref-type="fig" rid="fig1">Figure 1</xref>). All the RL-EAMs assume evidence accumulation is driven by Q-values, which change based on error-driven learning as governed by the classical delta update rule. Rather than a two-sided DDM process (<xref ref-type="fig" rid="fig1">Figure 1A</xref>), the alternative models adopt a neurally plausible RD architecture (<xref ref-type="bibr" rid="bib79">Ratcliff et al., 2007</xref>), which conceptualize decision-making as a statistically independent race between single-sided diffusive accumulators, each collecting evidence for a different choice option. The first accumulator to reach its threshold triggers motor processes that execute the corresponding decision. The alternative models differ in how the mean values of evidence are constituted. The first model, the RL-RD (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), postulates accumulators are driven by the expected reward for their choice, plus a stimulus-independent baseline (c.f. an <italic>urgency</italic> signal; <xref ref-type="bibr" rid="bib62">Miletić and van Maanen, 2019</xref>). The second model, the RL-ARD (advantage racing diffusion), uses the recently proposed <italic>advantage</italic> framework (<xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>), assuming that each accumulator is driven by weighted combination of three terms: the <italic>difference</italic> (‘advantage’) in mean reward expectancy of one choice option over the other, the <italic>sum</italic> of the mean reward expectancies, and the urgency signal. In perceptual choice, the advantage term consistently dominates the sum term by an order of magnitude (<xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>), but the sum term is necessary to explain the effects of absolute stimulus magnitude. We also fit a limited version of this model, RL-lARD, with the weight of the sum term set to zero to test whether accounting for the influence of the sum is necessary even when reward magnitude is not manipulated, as was the case in our first experiments. The importance of sum and advantage terms is also quantified by their weights as estimated in full RL-ARD model fits.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Comparison of the decision-making models.</title><p>Bottom graphs visualize how Q-values are linked to accumulation rates. Top panel illustrates the evidence-accumulation process of the DDM (panel <bold>A</bold>) and racing diffusion (RD) models (panels <bold>B</bold> and <bold>C</bold>). Note that in the race models there is no lower bound. <xref ref-type="disp-formula" rid="equ5 equ6 equ7">Equations 2–4</xref> formally link Q-values to evidence-accumulation rates. In the RL-DDM, the difference <inline-formula><mml:math id="inf1"><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> in Q-values is accumulated, weighted by free parameter <inline-formula><mml:math id="inf2"><mml:mi>w</mml:mi></mml:math></inline-formula>, plus additive within-trial white noise W with standard deviation <inline-formula><mml:math id="inf3"><mml:mi>s</mml:mi></mml:math></inline-formula>. In the RL-RD, the (weighted) Q-values for both choice options are independently accumulated. An evidence-independent baseline urgency term, <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> (equal for all accumulators), further drives evidence accumulation. In the RL-ARD models, the advantages <inline-formula><mml:math id="inf5"><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> in Q-values are accumulated as well, plus the evidence-independent baseline term <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. The gray icons indicate the influence of the Q-value <italic>sum</italic> <inline-formula><mml:math id="inf7"><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> on evidence accumulation, which is not included in the limited variant of the RL-ARD. In all panels, bold-italic faced characters indicate parameters. Q1 and Q<sub>2</sub> are Q-values for both choice options, which are updated according to a delta learning rule (<xref ref-type="disp-formula" rid="equ2">Equation 1</xref> at the bottom of the graph), with learning rate <inline-formula><mml:math id="inf8"><mml:mi>α</mml:mi></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig1-v2.tif"/></fig><p>For all models, we first test how well they account for RT distributions (central tendency, variability, and skewness of RTs), accuracies, and learning-related changes in RT distributions and accuracies in a typical instrumental learning task (<xref ref-type="bibr" rid="bib37">Frank et al., 2004</xref>). In this experiment, we also manipulated difficulty, that is, the magnitude of the difference in average reward between pairs of options. In two further experiments, we test the ability of the RL-EAMs to capture key behavioral phenomena in the decision-making and reinforcement-learning literatures, respectively, speed-accuracy trade-off (SAT), and reversals in reward contingencies, again in binary choice. In a final experiment, we show that the RL-ARD extends beyond binary choice, successfully explaining accuracy and full RT distributions from a three-alternative instrumental learning task that manipulates reward magnitude.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In the first experiment, participants made decisions between four sets of two abstract choice stimuli, each associated with a fixed reward probability (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). On each trial, one choice option always had a higher expected reward than the other; we refer to this choice as the ‘correct’ choice. After each choice, participants received feedback in the form of points. Reward probabilities, and therefore choice difficulty, differed between the four sets (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). In total, data from 55 subjects were included in the analysis, each performing 208 trials (see Materials and methods).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Paradigms for experiments 1–3.</title><p>(<bold>A</bold>) Example trial for experiments 1 and 3. Each trial starts with a fixation cross, followed by the presentation of the stimulus (until choice is made or 2.5 s elapses), a brief highlight of the chosen option, and probabilistic feedback. Reward probabilities are summarized in (<bold>B</bold>). Percentages indicate the probabilities of receiving +100 points for a choice (with 0 otherwise). The actual symbols used differed between experiments and participants. In experiment 3, the acquisition phase lasted 61–68 trials (uniformly sampled each block), after which the reward contingencies for each stimulus set reversed. (<bold>C</bold>) Example trial for experiment 2, which added a cue prior to each trial (‘SPD’ or ‘ACC’), and had feedback contingent on both the choice and choice timing. In the SPD condition, RTs under 600 ms were considered in time, and too slow otherwise. In the ACC condition, choices were in time as long as they were made in the stimulus window of 1.5 s. Positive feedback ‘Outcome: +100’ and ‘Reward: +100’ were shown in green letters, negative feedback (‘Outcome: 0’, ‘Reward: 0’, and ‘Too slow!”) were shown in red letters.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig2-v2.tif"/></fig><p>Throughout, we summarize RT distributions by calculating the 10th, 50th (median), and 90th percentiles separately for correct and error responses. The median summarizes central tendency, the difference between 10th and 90th percentiles summarizes variability and the larger difference between the 90th and 50th percentiles than between the 50th and 10th percentiles summarizes the positive skew that is always observed in RT distributions. To visualize the effect of learning, we divided all trials in 10 bins (approximately 20 trials each), and calculated accuracy and the RT percentiles per bin. Note that model fitting was not based on these data summaries. Instead, we used hierarchical Bayesian methods to fit models to the data from every trial and participant simultaneously. We compared model fits informally using posterior predictive distributions—calculating the same summary statistics on data generated from the fitted model as we did for the empirical data—and formally using the Bayesian Predictive Information Criterion (BPIC; <xref ref-type="bibr" rid="bib2">Ando, 2007</xref>). The former method allows us to assess the absolute quality of fit (<xref ref-type="bibr" rid="bib71">Palminteri et al., 2017</xref>) and detect misfits; the latter provides a model-selection criterion that trades off quality of fit with model complexity (lower BPICs are preferred), ensuring that a better fit is not only due to greater model flexibility.</p><p>We first examine results aggregated over difficulty conditions. The posterior predictives of all four RL-EAMs are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, with the top row showing accuracies, and the middle and bottom rows correct and error RT distributions (parameter estimates for all models can be found in <xref ref-type="table" rid="table1">Table 1</xref>). The RL-DDM generally explains the learning-related increase in accuracy well, and if only the central tendency were relevant it might be considered to provide an adequate account of RT, although correct median RT is systematically under-estimated. However, RT variability and skew are severely over-estimated. The RL-RD largely overcomes the RT distribution misfit, but it overestimates RTs in the first trial bins, and while capturing an increase in accuracy over trials, it is systematically underestimated. The RL-ARD models provide the best explanation of all key aspects of the data: except for a slight underestimation of accuracy in early trial bins (largely shared with the RL-DDM), they capture accuracy well, and like the RL-RD, they capture the RT distributions well, but without overpredicting the RTs in the early trials. The two RL-ARD models do not differ greatly in fit, except that the limited version slightly underestimates the decrease in RT with learning.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Comparison of posterior predictive distributions of the four RL-EAMs.</title><p>Data (black) and posterior predictive distribution (blue) of the RL-DDM (left column), RL-RD, RL-lARD, and RL-ARD (right column). Top row depicts accuracy over trial bins. Middle and bottom row show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas correspond to the 95% credible interval of the posterior predictive distributions. All data are collapsed across participants and difficulty conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Comparison of posterior predictive distributions of four additional RL-DDMs.</title><p>Data are black dots and lines, posterior predictive distribution are blue. Top row depicts accuracy over trial bins. Middle and bottom row illustrate 10<sup>th</sup>, 50th, and 90th quantile RT for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas correspond to the 95% credible interval of the posterior predictive distributions. All data are collapsed across participants and difficulty conditions. The summed BPICs were 7717 (RL-DDM A1), 7636 (RL-DDM A2), 4844 (RL-DDM A3) and 4884 (RL-DDM A4). Hence, the largest improvement of quality of fit of the RL-DDM was obtained by adding <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Parameter recovery of the RL-ARD model, using the experimental paradigm of experiment 1.</title><p>Parameter recovery was done by first fitting the RL-ARD model to the empirical data, and then simulating the exact same experimental paradigm (208 trials, 55 subjects, four difficulty conditions) using the median parameter estimates obtained from the model fit. Subsequently, the RL-ARD was fit to the simulated data. The recovered median posterior estimates (y-axis) are plotted against the data-generating values (x-axis). Pearson’s correlation coefficient <italic>r</italic> and the root mean square error (RMSE) are shown in each panel. Diagonal lines indicate the identity x = y.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Confusion matrices showing model separability.</title><p>Here, we first fit the RLDDM and RL-ARD to the empirical data of experiment 1, and then simulating 50 full datasets with both models using the exact same experimental paradigm (208 trials, 55 subjects, 4 difficulty conditions), and using the median parameter estimates obtained from the model fits. Hence, in total 100 full datasets (5500 subjects) were simulated. We then fit both the RL-DDM and RL-ARD to all 100 simulated datasets. The matrices visualize the model confusability when using the BPIC (left column) or minimum deviance (right column) as a model comparison metric. The minimum deviance is a measure of quality of fit without a penalty for model complexity. The top row shows the model comparisons per dataset (using the summed BPIC / minimum deviances), bottom row shows the model comparisons per subject. Model comparisons using the BPIC perfectly identified the data-generating model when summing BPICs across subjects. By subject individually, the RL-ARD incorrectly won model comparisons for 82 subjects (3%); the RL-DDM incorrectly won for 4 subjects (0.1%). Interestingly, when using the summed minimum deviances as a model comparison metric (thus not penalizing for model complexity), the across-subject model comparisons perfectly identified the data-generating model. By subject individually, the RL-ARD incorrectly won comparisons for 333 subjects (12%). Combined, this indicates that the RL-ARD, while more complex, is generally not sufficiently flexible to outperform RL-DDM in terms of the quality of fit on data generated by the RL-DDM.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Empirical (black) and posterior predictive (blue) defective probability densities of the RT distributions estimated using kernel density approximation.</title><p>The error RT distributions are shown as negative RTs for visualization. Blue lines represent 100 posterior predictive RT distributions from the RL-ARD model. The grand average is the RT distribution across all trials and subjects, subject-wise RT distributions are across all trials per subject for the first ten subjects, for which the quality of fit was representative for the entire dataset.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig3-figsupp4-v2.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Posterior parameter estimates (across-subject mean and SD of the median of the posterior distributions) for all models and experiments.</title><p>For models including <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, the non-decision time is assumed to be uniformly distributed with bounds <inline-formula><mml:math id="inf11"><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mi>t</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="10" valign="top">Experiment 1</th></tr></thead><tbody><tr><td valign="top">RL-DDM</td><td><inline-formula><mml:math id="inf12"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf13"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf14"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf15"><mml:mi>w</mml:mi></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">BPIC</td></tr><tr><td valign="top"/><td valign="top">0.14 (0.11)</td><td valign="top">1.48 (0.19)</td><td valign="top">0.30 (0.06)</td><td valign="top">3.21 (1.11)</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">7673</td></tr><tr><td valign="top">RL-RD</td><td><inline-formula><mml:math id="inf16"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf17"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf18"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf20"><mml:mi>w</mml:mi></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.08)</td><td valign="top">2.16 (0.27)</td><td valign="top">0.10 (0.04)</td><td valign="top">1.92 (0.42)</td><td valign="top">3.09 (1.32)</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">5613</td></tr><tr><td valign="top">RL-lARD</td><td><inline-formula><mml:math id="inf21"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf22"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf23"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.13 (0.12)</td><td valign="top">2.05 (0.24)</td><td valign="top">0.12 (0.05)</td><td valign="top">2.48 (0.43)</td><td valign="top">2.36 (0.95)</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">4849</td></tr><tr><td valign="top">RL-ARD</td><td><inline-formula><mml:math id="inf26"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf27"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf28"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.13 (0.11)</td><td valign="top">2.14 (0.26)</td><td valign="top">0.11 (0.04)</td><td valign="top">2.46 (0.59)</td><td valign="top">2.25 (0.78)</td><td valign="top">0.36 (0.79)</td><td valign="top"/><td valign="top"/><td valign="top">4577</td></tr><tr><td valign="top">RL-DDM A1</td><td><inline-formula><mml:math id="inf32"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf33"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf34"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf35"><mml:mi>w</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.14 (0.12)</td><td valign="top">1.49 (0.20)</td><td valign="top">0.30 (0.06)</td><td valign="top">3.01 (0.66)</td><td valign="top">2.81 (0.72)</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">7717</td></tr><tr><td valign="top">RL-DDM A2</td><td><inline-formula><mml:math id="inf37"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf38"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf39"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf40"><mml:mi>w</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.14 (0.11)</td><td valign="top">1.48 (0.19)</td><td valign="top">0.30 (0.06)</td><td valign="top">3.21 (1.12)</td><td valign="top">1.79e<sup>−3</sup> (0.4e<sup>−3</sup>)</td><td valign="top">1.8e<sup>−3</sup> (0.4e-<sup>3</sup>)</td><td valign="top"/><td valign="top"/><td valign="top">7637</td></tr><tr><td valign="top">RL-DDM A3</td><td><inline-formula><mml:math id="inf43"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf44"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf45"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf46"><mml:mi>w</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf48"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf49"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.13 (0.12)</td><td valign="top">1.13 (0.19)</td><td valign="top">0.27 (0.06)</td><td valign="top">5.31 (2.04)</td><td valign="top">0.00 (0.00)</td><td valign="top">0.31 (0.13)</td><td valign="top">0.37 (0.13)</td><td valign="top"/><td valign="top">4844</td></tr><tr><td valign="top">RL-DDM A4</td><td><inline-formula><mml:math id="inf50"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf51"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf52"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf53"><mml:mi>w</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf54"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.13 (0.12)</td><td valign="top">1.15 (0.17)</td><td valign="top">0.27 (0.06)</td><td valign="top">2.02 (0)</td><td valign="top">5.16 (1.18)</td><td valign="top">0.55 (0.24)</td><td valign="top">1.57e<sup>−3</sup> (0)</td><td valign="top">0.36 (0.13)</td><td valign="top">4884</td></tr><tr><td valign="top">RL-ALBA</td><td><inline-formula><mml:math id="inf58"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf59"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf60"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top">A</td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.13 (0.11)</td><td valign="top">3.53 (0.53)</td><td valign="top">0.03 (0.00)</td><td valign="top">3.03 (0.57)</td><td valign="top">2.03 (0.59)</td><td valign="top">0.33 (0.78)</td><td valign="top">1.73 (0.43)</td><td valign="top"/><td valign="top">4836</td></tr><tr><td colspan="9" valign="top">Experiment 2</td><td valign="top"/></tr><tr><td valign="top">RL-DDM 1</td><td><inline-formula><mml:math id="inf64"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf65"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf66"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf67"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf68"><mml:mi>w</mml:mi></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.13 (0.06)</td><td valign="top">1.11 (0.18)/1.42 (0.23)</td><td valign="top">0.26 (0.06)</td><td valign="top">3.28 (0.66)</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">979</td></tr><tr><td valign="top">RL-DDM 2</td><td><inline-formula><mml:math id="inf69"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf70"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf71"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf72"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf73"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.13 (0.05)</td><td valign="top">3.01 (0.63)</td><td valign="top">0.26 (0.06)</td><td valign="top">3.46 (0.79)/3.01 (0.63)</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">1518</td></tr><tr><td valign="top">RL-DDM 3</td><td><inline-formula><mml:math id="inf74"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf75"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf76"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf77"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf78"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf79"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.13 (0.06)</td><td valign="top">1.10 (0.18)/1.44 (0.23)</td><td valign="top">0.26 (0.06)</td><td valign="top">3.11 (0.68)/3.48 (0.72)</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">999</td></tr><tr><td valign="top">RL-ARD 1</td><td><inline-formula><mml:math id="inf80"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf82"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf83"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf84"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf85"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">1.45 (0.35)/1.82 (0.35)</td><td valign="top">0.15 (0.07)</td><td valign="top">2.59 (0.50)</td><td valign="top">2.24 (0.53)</td><td valign="top">0.47 (0.34)</td><td valign="top"/><td valign="top"/><td valign="top">−1044</td></tr><tr><td valign="top">RL-ARD 2</td><td><inline-formula><mml:math id="inf86"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf87"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf88"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf89"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf91"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">1.83 (0.36)</td><td valign="top">0.12 (0.07)</td><td valign="top">2.52 (0.53)</td><td valign="top">1.83 (0.56)</td><td valign="top">0.32 (0.26)</td><td valign="top">1.31 (0.20)</td><td valign="top"/><td valign="top">−827</td></tr><tr><td valign="top">RL-ARD 3</td><td><inline-formula><mml:math id="inf93"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf94"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf95"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf96"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf97"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf98"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf99"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">1.83 (0.35)</td><td valign="top">0.12 (0.07)</td><td valign="top">3.37 (0.84)/3.37 (0.54)</td><td valign="top">2.11 (0.52)</td><td valign="top">0.39 (0.30)</td><td valign="top"/><td valign="top"/><td valign="top">−934</td></tr><tr><td valign="top">RL-ARD 4</td><td><inline-formula><mml:math id="inf100"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf101"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf102"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf103"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf104"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf106"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">1.04 (0.14)/1.82 (0.35)</td><td valign="top">0.15 (0.07)</td><td valign="top">2.59 (0.52)</td><td valign="top">2.21 (0.51)</td><td valign="top">0.44 (0.38)</td><td valign="top">1.04 (0.14)</td><td valign="top"/><td valign="top">−1055</td></tr><tr><td valign="top">RL-ARD 5</td><td><inline-formula><mml:math id="inf107"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf110"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf111"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf112"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf113"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf114"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">1.59 (0.40)/1.83 (0.32)</td><td valign="top">0.14 (0.06)</td><td valign="top">2.92 (0.65)/2.52 (0.50)</td><td valign="top">2.21 (0.50)</td><td valign="top">0.43 (0.33)</td><td valign="top"/><td valign="top"/><td valign="top">−1071</td></tr><tr><td valign="top">RL-ARD 6</td><td><inline-formula><mml:math id="inf115"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf116"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf117"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf118"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf119"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf120"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf122"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">1.86 (0.35)</td><td valign="top">0.12 (0.07)</td><td valign="top">4.13 (0.98)/2.40 (0.54)</td><td valign="top">2.28 (0.53)</td><td valign="top">0.44 (0.33)</td><td valign="top">0.84 (0.03)</td><td valign="top"/><td valign="top">−897</td></tr><tr><td valign="top">RL-ARD 7</td><td><inline-formula><mml:math id="inf123"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf124"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf125"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf126"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf131"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">1.61 (0.40)/1.87 (0.32)</td><td valign="top">0.14 (0.06)</td><td valign="top">3.66 (0.74)/2.52 (0.50)</td><td valign="top">2.41 (0.53)</td><td valign="top">0.48 (0.38)</td><td valign="top">0.82 (0.08)</td><td valign="top"/><td valign="top">−1060</td></tr><tr><td valign="top">RL-DDM A3 1</td><td><inline-formula><mml:math id="inf132"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf133"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf134"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf135"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf136"><mml:mi>w</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf137"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf138"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf139"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">0.81 (0.16)/1.14 (0.17)</td><td valign="top">0.23 (0.06)</td><td valign="top">4.46 (0.79)</td><td valign="top">0.10 (0.01)</td><td valign="top">0.18 (0.05)</td><td valign="top">0.26 (0.09)</td><td valign="top"/><td valign="top">−862</td></tr><tr><td valign="top">RL-DDM A3 2</td><td><inline-formula><mml:math id="inf140"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf141"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf142"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf143"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf144"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf145"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf146"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf147"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">1.03 (0.14)</td><td valign="top">0.24 (0.06)</td><td valign="top">18.4 (23.34)/4.44 (0.84)</td><td valign="top">0.26 (0.07)</td><td valign="top">0.61 (0.50)</td><td valign="top">0.28 (0.10)</td><td valign="top"/><td valign="top">−325</td></tr><tr><td valign="top">RL-DDM A3 3</td><td><inline-formula><mml:math id="inf148"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf149"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf150"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf151"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="inf152"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>/<inline-formula><mml:math id="inf153"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf154"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf155"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf156"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.12 (0.05)</td><td valign="top">0.81 (0.16)/1.14 (0.17)</td><td valign="top">0.23 (0.06)</td><td valign="top">4.45 (0.83)/4.45 (0.83)</td><td valign="top">0.07 (0.00)</td><td valign="top">0.17 (0.04)</td><td valign="top">0.26 (0.09)</td><td valign="top"/><td valign="top">−849</td></tr><tr><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top">Experiment 3</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top">Soft-max</td><td><inline-formula><mml:math id="inf157"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf158"><mml:mi>β</mml:mi></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.40 (0.14)</td><td valign="top">2.82 (1.1)</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">23,727</td></tr><tr><td valign="top">RL-DDM</td><td><inline-formula><mml:math id="inf159"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf160"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf161"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.38 (0.14)</td><td valign="top">1.37 (0.24)</td><td valign="top">0.24 (0.07)</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top">15,599</td></tr><tr><td valign="top">RL-ARD</td><td><inline-formula><mml:math id="inf162"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf163"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf164"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf165"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf166"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf167"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.35 (0.15)</td><td valign="top">1.48 (0.34)</td><td valign="top">0.13 (0.08)</td><td valign="top">1.86 (0.51)</td><td valign="top">1.52 (0.63)</td><td valign="top">0.23 (0.25)</td><td valign="top"/><td valign="top"/><td valign="top">11,548</td></tr><tr><td valign="top">RL-DDM A3</td><td><inline-formula><mml:math id="inf168"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf169"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf170"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf171"><mml:mi>w</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf172"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf173"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf174"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.38 (0.14)</td><td valign="top">1.15 (0.22)</td><td valign="top">0.22 (0.07)</td><td valign="top">2.72 (1.16)</td><td valign="top">0.21 (0.09)</td><td valign="top">0.28 (0.15)</td><td valign="top">0.27 (0.17)</td><td valign="top"/><td valign="top">11,659</td></tr><tr><td valign="top">Experiment 4</td><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top">RL-ARD (Win-All)</td><td><inline-formula><mml:math id="inf175"><mml:mi>α</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf176"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf177"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf178"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf179"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf180"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td valign="top"/><td valign="top"/><td valign="top"/></tr><tr><td valign="top"/><td valign="top">0.10 (0.04)</td><td valign="top">1.6 (0.33)</td><td valign="top">0.07 (0.05)</td><td valign="top">1.14 (0.22)</td><td valign="top">1.6 (0.36)</td><td valign="top">0.15 (0.26)</td><td valign="top"/><td valign="top"/><td valign="top">36,512</td></tr></tbody></table></table-wrap><p><xref ref-type="fig" rid="fig4">Figure 4</xref> shows the data and RL-ARD model fit separated by difficulty (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for equivalent RL-DDM fits, which again fail to capture RT distributions). The RL-ARD model displays the same excellent fit as to data aggregated over difficulty, except that it underestimates accuracy in early trials in the easiest condition (<xref ref-type="fig" rid="fig4">Figure 4</xref>, bottom right panel). Further inspections of the data revealed that 17 participants (31%) reached perfect accuracy in the first bin in this condition. Likely, they guessed correctly on the first occurrence of the easiest choice pair, repeated their choice, and received too little negative feedback in the next repetitions to change their choice strategy. <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> shows that, with these 17 participants removed, the overestimation is largely mitigated. The delta rule assumes learning from feedback, and so cannot explain such high early accuracies. Working memory processes could have aided performance in the easiest condition, since the total number of stimuli pairs was limited and feedback was quite reliable, making it relatively easy to remember correct-choice options (<xref ref-type="bibr" rid="bib22">Collins and Frank, 2018</xref>; <xref ref-type="bibr" rid="bib21">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib58">McDougle and Collins, 2020</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Data (black) and posterior predictive distribution of the RL-ARD (blue), separately for each difficulty condition.</title><p>Column titles indicate the reward probabilities, with 0.6/0.4 being the most difficult, and 0.8/0.2 the easiest condition. Top row depicts accuracy over trial bins. Middle and bottom rows show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas correspond to the 95% credible interval of the posterior predictive distributions. All data and fits are collapsed across participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Data (black) and posterior predictive distribution of the RL-DDM (blue), separately for each difficulty condition.</title><p>Row titles indicate the reward probabilities, with 0.6/0.4 being the most difficult, and 0.8/0.2 the easiest condition. Top row depicts accuracy over trial bins. Middle and bottom row illustrate 10th, 50th, and 90th quantile RT for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas correspond to the 95% credible interval of the posterior predictive distributions. All data are collapsed across participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Data (black) and posterior predictive distribution of the RL-ARD (blue), separately for each difficulty condition, excluding 17 subjects which had perfect accuracy in the first bin of the easiest condition.</title><p>Row titles indicate the reward probabilities, with 0.6/0.4 being the most difficult, and 0.8/0.2 the easiest condition. Top row depicts accuracy over trial bins. Middle and bottom row illustrate 10th, 50th, and 90th quantile RT for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas correspond to the 95% credible interval of the posterior predictive distributions. All data are collapsed across participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Posterior predictive distribution of the RL-ALBA model on the data of experiment 1, with one column per difficulty condition.</title><p>The LBA assumes that, on every trial, two accumulators race deterministically toward a common bound <italic>b</italic>. Each accumulator <italic>i</italic> starts at a start point sampled from a uniform distribution [0, A], and with a speed of evidence accumulation sampled from a normal distribution <inline-formula><mml:math id="inf181"><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula>. In the RL-ALBA model, we used <xref ref-type="disp-formula" rid="equ7">Equation 4</xref> to link Q-values to LBA drift rates <inline-formula><mml:math id="inf182"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf183"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> (excluding the <inline-formula><mml:math id="inf184"><mml:mi>s</mml:mi><mml:mi>W</mml:mi> <mml:mi/></mml:math></inline-formula> term, since the LBA assumes no within-trial noise). Instead of directly estimating threshold <italic>b</italic>, we estimated the difference <italic>B</italic> = <italic>b-A</italic> (which simplifies enforcing <italic>b</italic>&gt;<italic>A</italic>). We used the following mildly informed priors for the hypermeans: <inline-formula><mml:math id="inf185"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="inf186"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>9</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> truncated at lower bound 0, <inline-formula><mml:math id="inf187"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>3</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="inf188"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="inf189"><mml:mi>A</mml:mi><mml:mo>~</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="inf190"><mml:mi>B</mml:mi><mml:mo>~</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> truncated at lower bound 0, and <inline-formula><mml:math id="inf191"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>0.5</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, truncated at lower bound 0.025 and upper bound 1. For the hyperSDs, all priors were <inline-formula><mml:math id="inf192"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo> <mml:mi mathvariant="normal"/><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. The summed BPIC was 4836, indicating that the RL-ALBA performs slightly better than the RL-DDM with between-trial variabilities (BPIC = 4844), and better than the RL-lARD (BPIC = 4849), but not as well as the RL-ARD (BPIC = 4577).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig4-figsupp3-v2.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Data (black) and posterior predictive distribution of the RL-ARD (blue), separately for each difficulty condition.</title><p>Column titles indicate the reward probabilities, with 0.6/0.4 being the most difficult, and 0.8/0.2 the easiest condition. Top row depicts accuracy over trial bins. Middle and bottom rows show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas correspond to the 95% credible interval of the posterior predictive distributions. All data and fits are collapsed across participants. Error bars depict standard errors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig4-figsupp4-v2.tif"/></fig></fig-group><sec id="s2-1"><title>Reward magnitude and Q-value evolution</title><p>Q-values represent the participants’ internal beliefs about how rewarding each choice option is. The RL-lARD and RL-DDM assume drift rates are driven only by the difference in Q-values (<xref ref-type="fig" rid="fig5">Figure 5</xref>), and both underestimate the learning-related decrease in RTs. Similar RL-DDM underestimation has been detected before (<xref ref-type="bibr" rid="bib72">Pedersen et al., 2017</xref>), with the proposed remedy being a decrease in the decision bound with time (but with no account of RT distributions). The RL-ARD explains the additional speed-up through the increasing <italic>sum</italic> of Q-values over trials (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), which in turn increases drift rates (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). In line with observations in perceptual decision-making (<xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>), the effect of the expected reward magnitude on drift rate is smaller (on average, <inline-formula><mml:math id="inf193"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.36</mml:mn></mml:math></inline-formula>) than that of the Q-value difference (<inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.25</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the urgency signal (<inline-formula><mml:math id="inf195"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.45</mml:mn></mml:math></inline-formula>). Earlier work using an RL-DDM (<xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref>) showed that higher reward magnitudes decrease RTs in reinforcement learning paradigms. There, the reward magnitude effect on RT was accounted for by allowing the threshold to change as a function of magnitude. However, this requires participants to rapidly adjust their threshold based on the identity of the stimuli, something that is usually not considered possible in EAMs (<xref ref-type="bibr" rid="bib30">Donkin et al., 2011</xref>; <xref ref-type="bibr" rid="bib78">Ratcliff, 1978</xref>). The RL-ARD avoids this problem, with magnitude effects entirely mediated by drift rates, and our results show that the expected reward magnitudes influence RTs due to learning even in the absence of a reward magnitude manipulation. Because the sum affects each accumulator equally, it changes RT with little effect on accuracy.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The evolution of Q-values and their effect on drift rates in the RL-ARD.</title><p>A depicts raw Q-values, separate for each difficulty condition (colors). B and C depict the Q-value differences and the Q-value sums over time. The drift rates (<bold>D</bold>) are a weighted sum of the Q-value differences and Q-value sums, plus an intercept.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig5-v2.tif"/></fig></sec><sec id="s2-2"><title>Speed-accuracy trade-off</title><p>Speed-accuracy trade-off (SAT) refers to the ability to strategically trade-off decision speed for decision accuracy (<xref ref-type="bibr" rid="bib12">Bogacz et al., 2010</xref>; <xref ref-type="bibr" rid="bib69">Pachella and Pew, 1968</xref>; <xref ref-type="bibr" rid="bib84">Ratcliff and Rouder, 1998</xref>). As participants can voluntarily trade speed for accuracy, RT and accuracy are not independent variables, so analysis methods considering only one of these variables while ignoring the other can be misleading. EAMs simultaneously consider RTs and accuracy and allow for estimation of SAT settings. The classical explanation in the DDM framework (<xref ref-type="bibr" rid="bib84">Ratcliff and Rouder, 1998</xref>) holds that participants adjust their SAT by changing the decision threshold: increasing thresholds require a participant to accumulate more evidence, leading to slower but more accurate responses.</p><p>Empirical work draws a more complex picture. Several papers suggest that in addition to thresholds, drift rates (<xref ref-type="bibr" rid="bib3">Arnold et al., 2015</xref>; <xref ref-type="bibr" rid="bib48">Heathcote and Love, 2012</xref>; <xref ref-type="bibr" rid="bib49">Ho et al., 2012</xref>; <xref ref-type="bibr" rid="bib77">Rae et al., 2014</xref>; <xref ref-type="bibr" rid="bib90">Sewell and Stallman, 2020</xref>) and sometimes even non-decision times (<xref ref-type="bibr" rid="bib3">Arnold et al., 2015</xref>; <xref ref-type="bibr" rid="bib109">Voss et al., 2004</xref>) can be affected. Increases in drift rates in a race model could indicate an urgency signal, implemented by drift gain modulation, with qualitatively similar effects to collapsing thresholds over the course of a decision (<xref ref-type="bibr" rid="bib20">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib43">Hawkins et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Miletić, 2016</xref>; <xref ref-type="bibr" rid="bib62">Miletić and van Maanen, 2019</xref>; <xref ref-type="bibr" rid="bib66">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib98">Thura and Cisek, 2016</xref>; <xref ref-type="bibr" rid="bib101">Trueblood et al., 2021</xref>; <xref ref-type="bibr" rid="bib106">van Maanen et al., 2019</xref>). In cognitively demanding tasks, it has been shown that two distinct components of evidence accumulation (quality and quantity of evidence) are affected by SAT manipulations, with quantity of evidence being analogous to an urgency signal (<xref ref-type="bibr" rid="bib9">Boag et al., 2019b</xref>; <xref ref-type="bibr" rid="bib8">Boag et al., 2019a</xref>). Recent evidence suggests that different SAT manipulations can affect different psychological processes: cue-based manipulations that instruct participants to be fast or accurate lead to overall threshold adjustments, whereas deadline-based manipulations can lead to a collapse of thresholds (<xref ref-type="bibr" rid="bib52">Katsimpokis et al., 2020</xref>).</p><p>Here, we apply an SAT manipulation to an instrumental learning task (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The paradigm differs from experiment one by the inclusion of a cue-based instruction to either stress response <italic>speed</italic> (‘SPD’) or response <italic>accuracy</italic> (‘ACC’) prior to each choice (randomly interleaved). Furthermore, on speed trials, participants had to respond within 0.6 s to receive a reward. Feedback was determined based on both the choice’s probabilistic outcome (‘+100’ or ‘+0’) and the RT: On trials where participants responded too late, they were additionally informed of the reward associated with their choice, had they been in time, so that they always received the feedback required to learn from their choices. After exclusions (see Materials and methods), data from 19 participants (324 trials each) were included in the analyses.</p><p>We used two mixed effects models to confirm the effect of the manipulation. A linear model predicting RT confirmed an interaction between trial bin and cue (b = 0.014, SE = 1.53*10<sup>-3</sup>, 95% CI [0.011, 0.017], <italic>p</italic> &lt; 10<sup>-16</sup>), a main effect of cue (b = -0.189, SE = 9.5*10<sup>-3</sup>, 95% CI [-0.207, -0.170], <italic>p</italic> &lt; 10<sup>-16</sup>) and a main effect of trial bin (b = -0.015, SE = 1.08*10<sup>-3</sup>, 95% CI [-0.018, -0.013], <italic>p</italic> &lt; 10<sup>-16</sup>). Thus, RTs decreased with trial bin, were faster for the speed condition, but the effect of the cue was smaller for later trial bins. A logistic mixed effects model of choice accuracy showed a main effect of the cue (b = -0.39, SE = 0.13, 95% CI [-0.65, -0.13], <italic>p</italic> = 0.003) and trial bin (b = 0.42, SE = 0.06, 95% CI [0.30, 0.53], <italic>p = </italic>3.1*10<sup>-12</sup>), but not for an interaction (b = 0.115, SE = 0.08, 95% CI [-0.05, 0.28], <italic>p</italic> = 0.165). Hence, participants were more often correct in the accuracy condition, and their accuracy increased over trial bins, but there was no evidence for a difference in the increase (on a logit scale) between SAT conditions.</p><p>Next, we compared the RL-DDM and RL-ARD. In light of the multiple psychological mechanisms potentially affected by the SAT manipulation, we allowed different combinations of threshold, drift rate, and for the RL-ARD urgency, to vary with the SAT manipulation. We fit three RL-DDM models, varying either threshold, the Q-value weighting on the drift rates parameter (<xref ref-type="bibr" rid="bib90">Sewell and Stallman, 2020</xref>), or both. For the RL-ARD, we fit all seven possible models with different combinations of the threshold, urgency, and drift rate parameters free to vary between SAT conditions.</p><p>Formal model comparison (see <xref ref-type="table" rid="table1">Table 1</xref> for all BPIC values) indicates that the RL-ARD model combining response caution and urgency effects provides the best explanation of the data, in line with earlier research in non-learning contexts (<xref ref-type="bibr" rid="bib52">Katsimpokis et al., 2020</xref>; <xref ref-type="bibr" rid="bib62">Miletić and van Maanen, 2019</xref>; <xref ref-type="bibr" rid="bib77">Rae et al., 2014</xref>; <xref ref-type="bibr" rid="bib98">Thura and Cisek, 2016</xref>). The advantage for the RL-ARD was substantial; the best RL-DDM (with only a threshold effect) performed worse than the worst RL-ARD model. The data and posterior predictive distributions of the best RL-DDM model and the winning RL-ARD model are shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>. As in experiment 1, the RL-DDM failed to capture the shape of RT distributions, although it fit the SAT effect on accuracy and median RTs. The RL-ARD model provides a much better account of the RT distributions, including the differences between SAT conditions. In <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, we show that adding non-decision time variability to the RL-DDM mitigates some of the misfit of the RT distributions, although it still consistently under-predicted the 10th percentile in the accuracy condition. Further, this model was still substantially outperformed by the RL-ARD in formal model selection (ΔBPIC = 209), and non-decision time variability was estimated as much greater than what is found in non-learning contexts, raising the question of its psychological plausibility.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Data (black) and posterior predictive distributions (blue) of the best-fitting RL-DDM (left columns) and the winning RL-ARD model (right columns), separate for the speed and accuracy emphasis conditions.</title><p>Top row depicts accuracy over trial bins. Middle and bottom row show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas in the middle and right column correspond to the 95% credible interval of the posterior predictive distribution.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Data (black) of experiment 2 and posterior predictive distribution (blue) of the RL-DDM A3 with separate thresholds for the SAT conditions, and between-trial variabilities in drift rates, start points, and non-decision times.</title><p>The corresponding summed BPIC was -861, an improvement over the RL-DDM, but outperformed by the RL-ARD (<inline-formula><mml:math id="inf196"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>B</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>232</mml:mn></mml:math></inline-formula> in favor of the RL-ARD). Top row depicts accuracy over trial bins. Middle and bottom row illustrate 10th, 50th, and 90th quantile RT for the correct (middle row) and error (bottom row) response over trial bins. Left and right column are speed and accuracy emphasis condition, respectively. Shaded areas correspond to the 95% credible interval of the posterior predictive distributions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Parameter recovery of the RL-ARD model, using the experimental paradigm of experiment 2.</title><p>Parameter recovery was done by first fitting the RL-ARD model to the empirical data, and then simulating the exact same experimental paradigm (19 subjects, three difficulty conditions, 2 SAT conditions, 312 trials) using the median parameter estimates obtained from the model fit. Subsequently, the RL-ARD was fit to the simulated data. The median posterior estimates (y-axis) are plotted against the data-generating values (x-axis). Pearson’s correlation coefficient <italic>r</italic> and the root mean square error (RMSE) are shown in each panel. Diagonal lines indicate the identity x = y.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig6-figsupp2-v2.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Mean RT (left column) and choice accuracy (right column) across trial bins (x-axis) for experiments 2 and 3 (rows).</title><p>Block numbers are color-coded. Error bars are 1 SE. Mixed effects models indicated that in experiment 2, RTs decreased with block number (b = −0.04, SE = 6.15*10<sup>−3</sup>, 95% CI [−0.05,–0.03], p = 6.61*10<sup>−10</sup>) as well as with trial bin (b = −0.02, SE = 2.11*10<sup>−3</sup>, 95% CI [−0.02,–0.01], p = 1.68*10<sup>−13</sup>), and there was an interaction between trial bin and block number (b = 3.61*10<sup>−3</sup>, SE = 9.86*10<sup>−4</sup>, 95% CI [0.00, 0.01], p = 2.52*10<sup>−4</sup>). There was a main effect of (log-transformed) trial bin on accuracy (on a logit scale; b = 0.36, SE = 0.11, 95% CI [0.15, 0.57], p = 7.99*10<sup>−4</sup>), but no effect of block number, nor an interaction between block number and trial bin on accuracy. In experiment 3, response times increased with block number (b = 0.02, SE = 3.10*10<sup>−3</sup>, 95% CI [0.01, 0.02], p = 1.21*10<sup>−7</sup>), decreased with trial bin (b = −4.24*10<sup>−3</sup>, SE = 1.37*10<sup>−3</sup>, 95% CI [−6.92*10<sup>−3</sup>, −1.56*10<sup>−3</sup>], p = 0.002), but there was no interaction between trial bin and block number (b = −9.15*10<sup>−4</sup>, SE = 5*10<sup>−4</sup>, 95% CI [0.00, 0.00], p = 0.067). The bottom left panel suggests that the main effect of block number on RT is largely caused by an increase in RT after the first block. Accuracy decreased with (log-transformed) trial bin (on a logit scale: b = −0.12, SE = 0.05, 95% CI [−0.22,–0.02], p = 0.02), decreased with block number (b = −0.08, SE = 0.03, 95% CI [−0.14,–0.02], p = 0.009), but there was no interaction (b = 0.02, SE = 0.02, 95% CI [−0.02, 0.06], p = 0.276). The decrease in accuracy with trial bin is expected due to the presence of reversals. The combination of an increase in RT and a decrease in accuracy after the first block could indicate that participants learnt the structure of the task (i.e. the presence of reversals) in the first block, and adjusted their behavior accordingly. In line with this speculation, the accuracy in trial bin 6 (in which the reversal occurred) was lowest in the first block, which suggests that participants adjusted to the reversal faster in the later blocks. In experiment 4, response times decreased with block number (b = −0.04, SE = 9.08*10<sup>−3</sup>, 95% CI [−0.06,–0.02], p = 3.19*10<sup>−3</sup>) and there was an interaction between block number and trial bin (b = −4.31*10<sup>−3</sup>, SE = 1.45*10<sup>−3</sup>, 95% CI = [−0.01, 0.00], p = 0.003), indicating that the decrease of RTs over trial bins was larger for the later blocks. There was no main effect of trial bin on RTs. There was a main effect of (log-transformed) trial bin on accuracy (on a logit scale: b = 0.60, SE = 0.07, 95% CI [0.47, 0.73], p &lt; 10<sup>-16</sup>), but no main effect of block and no interaction between block and trial bin.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig6-figsupp3-v2.tif"/></fig><fig id="fig6s4" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 4.</label><caption><title>Empirical (black) and posterior predictive (blue) defective probability densities of the RT distributions of experiment 2, estimated using kernel density approximation.</title><p>Negative RTs correspond to error RTs. Blue lines represent 100 posterior predictive RT distributions from the RL-ARD model. The grand average is the RT distribution across all trials and subjects, subject-wise RT distributions are across all trials per subject for the first 10 subjects, for which the quality of fit was representative for the entire dataset.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig6-figsupp4-v2.tif"/></fig><fig id="fig6s5" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 5.</label><caption><title>Data (black) and posterior predictive distributions (blue) of the best-fitting RL-DDM (left columns) and the winning RL-ARD model (right columns), separate for the speed and accuracy emphasis conditions.</title><p>Top row depicts accuracy over trial bins. Middle and bottom row show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. Shaded areas in the middle and right column correspond to the 95% credible interval of the posterior predictive distribution. Error bars depict standard errors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig6-figsupp5-v2.tif"/></fig></fig-group><p>Both RL-DDM and RL-ARD models tended to underestimate RTs and choice accuracy in the early trial bins in the accuracy emphasis condition. As in experiment 1, working memory may have contributed to the accurate but slow responses in the first trial bin for the accuracy condition (<xref ref-type="bibr" rid="bib22">Collins and Frank, 2018</xref>; <xref ref-type="bibr" rid="bib21">Collins and Frank, 2012</xref>; <xref ref-type="bibr" rid="bib58">McDougle and Collins, 2020</xref>).</p></sec><sec id="s2-3"><title>Reversal learning</title><p>Next, we tested whether the RL-ARD can capture changes in accuracy and RTs caused by a perturbation in the learning process due to reversals in reward contingencies. In the reversal learning paradigm (<xref ref-type="bibr" rid="bib7">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="bib23">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib50">Izquierdo et al., 2017</xref>) participants first learn a contingency between choice options and probabilistic rewards (the acquisition phase) that is then suddenly reversed without any warning (the reversal phase). If the link between Q-values and decision mechanisms as proposed by the RL-ARD underlies decisions, the model should be able to account for the behavioral consequences (RT distributions and decisions) of Q-value changes induced by the reversal.</p><p>Our reversal learning task had the same general structure as experiment 1 (<xref ref-type="fig" rid="fig1">Figure 1</xref>), except for the presence of reversals. Forty-seven participants completed four blocks of 128 trials each. Within each block, two pairs of stimuli were randomly interleaved. Between trials 61 and 68 (uniformly sampled) in each block, the reward probability switched between stimuli, such that stimuli that were correct during acquisition were incorrect after reversal (and vice versa). Participants were not informed of the reversals prior to the experiment, but many reported noticing them.</p><p>Data and the posterior predictive distributions of the RL-DDM and the RL-ARD models are shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. Both models captured the change in choice proportions after the reversal reasonably well, although they underestimate the speed of change. In <xref ref-type="fig" rid="fig7s1">Figure 7—figure supplement 1</xref>, we show that the same is true for a standard soft-max model, suggesting that the learning rule is the cause of this problem. Recent evidence indicates that, instead of only estimating expected values of both choice options by error-driven learning, participants may additionally learn the task structure, estimate the probability of a reversal occurring and adjust choice behavior accordingly. Such a model-based learning strategy could increase the speed with which choice behavior changes after a reversal (<xref ref-type="bibr" rid="bib23">Costa et al., 2015</xref>; <xref ref-type="bibr" rid="bib50">Izquierdo et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Jang et al., 2015</xref>), but as yet a learning rule that implements this strategy has not been developed.</p><fig-group><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Experiment three data (black) and posterior predictive distributions (blue) for the RL-DDM (left) and RL-ARD (right).</title><p>Top row: choice proportions over trials, with choice option A defined as the high-probability choice before the reversal in reward contingencies. Bottom row: 10th, 50th, and 90th RT percentiles. The data are ordered relative to the trial at which the reversal first occurred (trial 0, with negative trial numbers indicated trials prior to the reversal). Shaded areas correspond to the 95% credible interval of the posterior predictive distributions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig7-v2.tif"/></fig><fig id="fig7s1" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 1.</label><caption><title>Data (black) of experiment 3 and posterior predictive of a standard soft-max learning model (blue).</title><p>As priors, we used <inline-formula><mml:math id="inf197"><mml:mi>β</mml:mi><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>1,5</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> truncated at 0 for the hypermean and <inline-formula><mml:math id="inf198"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1,1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> for the hyperSD. Left panel depicts choice proportions for option over trial bins, where choice A is defined as the high-probability reward choice prior to the reversal. Right column depicts choice proportion over trials, aligned to the trial at which the reversal occurred (trial 0). Shaded areas correspond to the 95% credible interval of the posterior predictive distributions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig7-figsupp1-v2.tif"/></fig><fig id="fig7s2" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 2.</label><caption><title>Data (black) of experiment 3 and posterior predictive distribution (blue) of the RL-DDM A3 (with between-trial variabilities in drift rates, start points, and non-decision times).</title><p>The summed BPIC was 11659. This is better compared to the RL-DDM (<inline-formula><mml:math id="inf199"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>B</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>3940</mml:mn></mml:math></inline-formula>) but did not outperform the RL-ARD (<inline-formula><mml:math id="inf200"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>B</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>112</mml:mn></mml:math></inline-formula> in favor of the RL-ARD).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig7-figsupp2-v2.tif"/></fig><fig id="fig7s3" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 3.</label><caption><title>Parameter recovery of the RL-ARD model, using the experimental paradigm of experiment 3.</title><p>Parameter recovery was done by first fitting the RL-ARD model to the empirical data, and then simulating the exact same experimental paradigm (49 subjects, 2 difficulty conditions, 512 trials, including reversals) using the median parameter estimates obtained from the model fit. Subsequently, the RL-ARD was fit to the simulated data. The median posterior estimates (y-axis) are plotted against the data-generating values (x-axis). Pearson’s correlation coefficient <italic>r</italic> and the root mean square error (RMSE) are shown in each panel. Diagonal lines indicate the identity x = y.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig7-figsupp3-v2.tif"/></fig><fig id="fig7s4" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 4.</label><caption><title>Empirical (black) and posterior predictive (blue) defective probability densities of the RT distributions of experiment 3, estimated using kernel density approximation.</title><p>Negative RTs correspond to choices for the option that was correct after the reversal. Blue lines represent 100 posterior predictive RT distributions from the RL-ARD model. The grand average is the RT distribution across all trials and subjects, subject-wise RT distributions are across all trials per subject for the first 10 subjects, for which the quality of fit was representative for the entire dataset.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig7-figsupp4-v2.tif"/></fig><fig id="fig7s5" position="float" specific-use="child-fig"><label>Figure 7—figure supplement 5.</label><caption><title>Experiment three data (black) and posterior predictive distributions (blue) for the RL-DDM (left) and RL-ARD (right).</title><p>Top row: choice proportions over trials, with choice option A defined as the high-probability choice before the reversal in reward contingencies. Bottom row: 10th, 50th, and 90th RT percentiles. The data are ordered relative to the trial at which the reversal first occurred (trial 0, with negative trial numbers indicated trials prior to the reversal). Shaded areas correspond to the 95% credible interval of the posterior predictive distributions. Error bars depict standard errors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig7-figsupp5-v2.tif"/></fig></fig-group><p>The change in RT around the reversal was less marked than the change in choice probability. Once again, the RL-DDM overestimates variability and skew. Both models fit the effects of learning and reversal similarly, but the fastest responses for the RL-DDM decrease much too quickly during initial learning and the reduction in speed for the slowest responses due to the reversal is strongly overestimated. The RL-ARD provides a much better account of the shape of the RT distributions, and furthermore captures the increase in entire RT <italic>distributions</italic> (instead of only the median) after the reversal point. Formal model comparison also very strongly favors the RL-ARD over the RL-DDM (<inline-formula><mml:math id="inf201"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>B</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>4051</mml:mn></mml:math></inline-formula>). <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref> provides model comparisons to RL-DDMs with between-trial variability parameters, which lead to the same conclusion.</p><p>A notable aspect of the data is that choice behavior stabilizes approximately 20 trials after the reversal, whereas RTs remain high compared to just prior to the reversal point for up to ~40 trials. The RL-ARD explains this behavior through relatively high Q-values for the choice option that was correct during the acquisition (but not reversal) phase (i.e. choice A). <xref ref-type="fig" rid="fig8">Figure 8</xref> depicts the evolution of Q-values, Q-value differences and sums, and drift rates in the RL-ARD model. The Q-values for both choice options increase until the reversal (<xref ref-type="fig" rid="fig8">Figure 8A</xref>), with a much faster increase for Q<sub>A</sub>. At the reversal Q<sub>A</sub> decreases and Q<sub>B</sub> increases, but as Q<sub>A</sub> decreases faster than Q<sub>B</sub> increases there is a temporary decrease in Q-value sums (<xref ref-type="fig" rid="fig8">Figure 8C</xref>). After approximately 10 trials post-reversal, Q<sub>B</sub> is higher than for Q<sub>A</sub>, which flips the sign of the Q-value differences (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). However, Q<sub>A</sub> <italic>after</italic> the reversal remains higher than the Q<sub>B</sub> <italic>before</italic> the reversal, which causes the (absolute) Q-value differences to be lower after the reversal than before. As a consequence, the drift rates for B after the reversal remain lower than the drift rates for A before the reversal, which increases RT. Clearly, it is important to take account of the sum of inputs to accumulators as well as the difference between them in order to provide an accurate account of the effects of learning.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>The evolution of Q-values and their effect on drift rates in the RL-ARD in experiment 3, aggregated across participants.</title><p>Left panel depicts raw Q-values, separate for each difficulty condition (colors). The second and third panel depict the Q-value differences and the Q-value sums over time. The drift rates (right panel) are a weighted sum of the Q-value differences and Q-value sums, plus an intercept. Choice A (solid lines) refers to the option that had the high probability of reward during the acquisition phase, and choice B (dashed lines) to the option that had the high probability of reward after the reversal.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig8-v2.tif"/></fig></sec><sec id="s2-4"><title>Multi-alternative choice</title><p>Finally, we again drew on the advantage framework (<xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>) to extend the RL-ARD to multi-alternative choice tasks, a domain where the RL-DDM cannot be applied. As in the two-choice case, the multi-alternative RL-ARD assumes one accumulator per pairwise difference between choice options. With three choice options (e.g. 1, 2, 3), there are six (directional) pairwise differences (1-2, 1-3, 2-1, 2-3, 3-1, 3-2), and therefore six accumulators (see <xref ref-type="fig" rid="fig9">Figure 9</xref>). All accumulators are assumed to race toward a common threshold, with their drift rates determined by the advantage framework’s combination of an urgency, an advantage, and a sum term. Since each response is associated with two accumulators (e.g. for option 1, one accumulating the advantage 1–2, and the other accumulating the advantage 1–3), a stopping rule is required to determine when a commitment to a response is made and evidence accumulation stops. Following Van Ravenzwaaij et al., we used the Win-All stopping rule, which proposes that the first response option for which <italic>both</italic> accumulators have reached their thresholds wins. RT is the first passage time of the <italic>slowest</italic> of these two winning accumulators, plus non-decision time.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Architecture of the three-alternative RL-ARD.</title><p>In three-choice settings, there are three Q-values. The multi-alternative RL-ARD has one accumulator per directional pairwise difference, hence there are six accumulators. The bottom graph visualizes the connections between Q-values and drift rates (<inline-formula><mml:math id="inf202"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is left out to improve readability). The equations formalize the within-trial dynamics of each accumulator. Top panels illustrate one example trial, in which both accumulators corresponding to response option 1 reached their thresholds. In this example trial, the model chose option 1, with the RT determined by the slowest of the winning accumulators (here, the leftmost accumulator). Decision-related parameters <inline-formula><mml:math id="inf203"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula> are all identical across the six accumulators.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig9-v2.tif"/></fig><p>To test how well the Win-All RL-ARD can explain empirical data, we performed a fourth experiment in which participants were required to repeatedly make decisions between three choice options (<xref ref-type="fig" rid="fig10">Figure 10</xref>). Within each of the four blocks, there were four randomly interleaved stimulus triplets that differed in difficulty (defined as the difference in reward probability between target stimulus and distractors) and reward magnitude (defined as the average reward probability): 0.8/0.25/0.25 (easy, high magnitude), 0.7/0.3/0.3 (hard, high magnitude), 0.7/0.15/0.15 (easy, low magnitude), and 0.6/0.2/0.2 (hard, low magnitude). This enabled us to simultaneously test whether the RL-ARD can account for a manipulation of difficulty and mean reward magnitude. Furthermore, we predicted that the requirement to learn 12 individual stimuli (per block) would interfere with the participants’ ability to rely on working memory (<xref ref-type="bibr" rid="bib21">Collins and Frank, 2012</xref>), and therefore expected that the RL-ARD would provide a better account of accuracy in the early trial bins compared to experiments 1 and 2. After exclusions (see Materials and methods), data from 34 participants (432 trials each) were included in the analyses.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Experimental paradigm of experiment 4.</title><p>(<bold>A</bold>) Example trial of experiment 4. Each trial started with a fixation cross, followed by the stimulus (three choice options; until the subject made a choice, up to 3 s), a brief highlight of the choice, and the choice’s reward was shown. (<bold>B</bold>) Reward contingencies for the target stimulus and two distractors per condition. Percentages indicate the probability of receiving +100 points (+0 otherwise). Presented symbols are examples, the actual symbols differed per block and participant (counterbalanced to prevent potential item effects from confounding the learning process).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig10-v2.tif"/></fig><p>Data and posterior predictive distributions of the RL-ARD are shown in <xref ref-type="fig" rid="fig11">Figure 11</xref>. The top row represents accuracy, the middle row the RT quantiles corresponding to the correct (target) choice option, and the bottom row the RT quantiles of the incorrect choices (collapsed across the two distractor response options). Compared to experiments 1–3, the task was substantially more difficult, as evidenced by the relatively low accuracies. The RL-ARD model was able to account for all patterns in the data, including the increase in accuracy and decrease in RTs due to learning, the shape of the full RT distributions, as well as the difficulty and magnitude effects. Furthermore, there was a decrease in variance in the error RTs due to learning (the 10th quantile RTs even mildly <italic>increased</italic>), which was also captured by the model. Note finally that, contrary to experiments 1 and 2, the model did not underestimate the accuracy in early bins in the easy conditions, which suggests that the influence of working memory was, as predicted, more limited than in earlier experiments. A parameter recovery study (<xref ref-type="fig" rid="fig11s1">Figure 11—figure supplement 1</xref>) demonstrated that the model’s parameters could be recovered accurately.</p><fig-group><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Data (black) and posterior predictive distribution of the RL-ARD (blue), separately for each difficulty condition.</title><p>Column titles indicate the magnitude and difficulty condition. Top row depicts accuracy over trial bins. Middle and bottom rows show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. The error responses are collapsed across distractors. Shaded areas correspond to the 95% credible interval of the posterior predictive distributions. All data and fits are collapsed across participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig11-v2.tif"/></fig><fig id="fig11s1" position="float" specific-use="child-fig"><label>Figure 11—figure supplement 1.</label><caption><title>Parameter recovery of the multi-alterative Win-All RL-ARD model, using the experimental paradigm of experiment 4.</title><p>Parameter recovery was done by first fitting the RL-ARD model to the empirical data, and then simulating the exact same experimental paradigm (34 subjects, 4 conditions, 432 trials) using the median parameter estimates obtained from the model fit. Subsequently, the RL-ARD was fit to the simulated data. The median posterior estimates (y-axis) are plotted against the data-generating values (x-axis). Pearson’s correlation coefficient <italic>r</italic> and the root mean square error (RMSE) are shown in each panel. Diagonal lines indicate the identity x = y.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig11-figsupp1-v2.tif"/></fig><fig id="fig11s2" position="float" specific-use="child-fig"><label>Figure 11—figure supplement 2.</label><caption><title>Empirical (black) and posterior predictive (blue) defective probability densities of the RT distributions of experiment 4, estimated using kernel density approximation.</title><p>Negative RTs correspond to error choices, collapsed across the two distractor choice options. Blue lines represent 100 posterior predictive RT distributions from the RL-ARD model. The grand average is the RT distribution across all trials and subjects, subject-wise RT distributions are across all trials per subject for the first 10 subjects, for which the quality of fit was representative for the entire dataset.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig11-figsupp2-v2.tif"/></fig><fig id="fig11s3" position="float" specific-use="child-fig"><label>Figure 11—figure supplement 3.</label><caption><title>Data (black) and posterior predictive distribution of the RL-ARD (blue), separately for each difficulty condition of experiment 4.</title><p>Column titles indicate the magnitude and difficulty condition. Top row depicts accuracy over trial bins. Middle and bottom rows show 10th, 50th, and 90th RT percentiles for the correct (middle row) and error (bottom row) response over trial bins. The error responses are collapsed across distractors. Shaded areas correspond to the 95% credible interval of the posterior predictive distributions. All data and fits are collapsed across participants. Error bars depict standard errors.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig11-figsupp3-v2.tif"/></fig></fig-group><p>Notably, <xref ref-type="fig" rid="fig11">Figure 11</xref> suggests that the effects of the magnitude manipulation were larger in the hard than in the easy condition. As in previous experiments, we inspected the Q-value evolution (<xref ref-type="fig" rid="fig12">Figure 12</xref>) to understand how this interaction arose. As expected, the high magnitude condition led to higher Q-values (<xref ref-type="fig" rid="fig12">Figure 12A</xref>) than the low magnitude condition, increasing the Q-value sums (<xref ref-type="fig" rid="fig12">Figure 12C</xref>). However, there was a second effect of the increased magnitude: even though the true reward probability differences were equal between magnitude conditions, the Q-value differences for the response accumulators (<inline-formula><mml:math id="inf204"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi mathvariant="normal">Q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>; <xref ref-type="fig" rid="fig12">Figure 12B</xref>) were larger in the high compared to the low magnitude condition, particularly for harder choices. As a consequence, <italic>both</italic> the Q-value sum (weighted by median <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), and the smaller changes in the Q-value difference (weighted by median <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.6</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), increased the drift rates for the response accumulators (<inline-formula><mml:math id="inf207"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>; <xref ref-type="fig" rid="fig12">Figure 12D</xref>), which led to higher accuracy and faster responses.</p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Q-value evolution in experiment 4.</title><p>Top row corresponds to the low magnitude condition, bottom to the high magnitude condition. Colors indicate choice difficulty. (<bold>A</bold>) Q-values for target (<inline-formula><mml:math id="inf208"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and distractor stimuli (<inline-formula><mml:math id="inf209"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). (<bold>B</bold>) Difference in Q-values, for target – distractor (<inline-formula><mml:math id="inf210"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and between the two distractors (<inline-formula><mml:math id="inf211"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). The Q-value difference <inline-formula><mml:math id="inf212"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is omitted from the graph to aid readability (but <inline-formula><mml:math id="inf213"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). (<bold>C</bold>) Sum of Q-values. (<bold>D</bold>) Resulting drift rates for target response accumulators (<inline-formula><mml:math id="inf214"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), and accumulators for the distractor choice options (<inline-formula><mml:math id="inf215"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>. Note that within each condition, there is a single Q-value trace per choice option, but since there are two distractors, there are two overlapping traces for <inline-formula><mml:math id="inf216"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf217"><mml:mi mathvariant="normal">Σ</mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and for all drift rates.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-63055-fig12-v2.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We compared combinations of different evidence-accumulations models with a simple delta reinforcement learning rule (RL-EAMs). The comparison tested the ability of the RL-EAMs to provide a comprehensive account of behavior in learning contexts, not only in terms of the choices made but also the full distribution of the times to make them (RT). We examined a standard instrumental learning paradigm (<xref ref-type="bibr" rid="bib37">Frank et al., 2004</xref>) that manipulated the difference in rewards between binary options (i.e. decision difficulty). We also examined two elaborations of that paradigm testing key phenomena from the decision-making and learning literatures, speed-accuracy trade-offs (SAT), and reward reversals, respectively. Our benchmark was the dual threshold Diffusion Decision Model (DDM; <xref ref-type="bibr" rid="bib78">Ratcliff, 1978</xref>), which has been used in almost all previous RL-EAM research, but has not been compared to other RL-EAMs, and has not been thoroughly evaluated on its ability to account for RT distributions in learning tasks. Our comparison used several different racing diffusion (RD) models, where decisions depend on the winner of a race between single barrier diffusion processes.</p><p>The RL-DDM provided a markedly inferior account to the other models, consistently overestimating RT variability and skew. As these aspects of behavior are considered critical in evaluating models in decision-making literature (<xref ref-type="bibr" rid="bib36">Forstmann et al., 2016</xref>; <xref ref-type="bibr" rid="bib83">Ratcliff and McKoon, 2008</xref>; <xref ref-type="bibr" rid="bib110">Voss et al., 2013</xref>), our results question whether the RL-DDM provides an adequate model of instrumental learning. Furthermore, the DDM carries with it two important theoretical limitations. First, it can only address binary choice. This is unfortunate given that perhaps the most widely used clinical application of reinforcement learning, the Iowa gambling task (<xref ref-type="bibr" rid="bib6">Bechara et al., 1994</xref>), requires choices among four options. Second, the input to the DDM combines the evidence for each choice (i.e., ‘Q’ values determined by the learning rule) into a single difference, and so requires extra mechanisms to account for known effects of overall reward magnitude (<xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref>). Although there are potential ways that the RL-DDM might be modified to account for magnitude effects, such as increasing between-trial drift rate variability in proportion to the mean rate (<xref ref-type="bibr" rid="bib82">Ratcliff et al., 2018</xref>), its inability to extend beyond binary choice remains an enduring impediment.</p><p>The best alternative model that we tested, the RL-ARD (advantage racing diffusion), which is based on the recently proposed advantage accumulation framework (<xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>), remedied all of these problems. The input to each accumulator is the weighted sum of three components: stimulus independent ‘urgency’, the difference between evidence for the choice corresponding to the accumulator and the alternative (the advantage), and the sum of the two evidence values. The urgency component had a large effect in all fits and played a key role in explaining the effect of speed-accuracy trade-offs. Although an urgency mechanism such as collapsing bounds (<xref ref-type="bibr" rid="bib10">Boehm et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Bowman et al., 2012</xref>; <xref ref-type="bibr" rid="bib43">Hawkins et al., 2015</xref>; <xref ref-type="bibr" rid="bib64">Milosavljevic et al., 2010</xref>) or gain modulation (<xref ref-type="bibr" rid="bib10">Boehm et al., 2016</xref>; <xref ref-type="bibr" rid="bib19">Churchland et al., 2008</xref>; <xref ref-type="bibr" rid="bib28">Ditterich, 2006</xref>; <xref ref-type="bibr" rid="bib43">Hawkins et al., 2015</xref>) could potentially improve the fits of the RL-DDM, fitting DDMs with such mechanisms is computationally very expensive, usually requiring the researcher to use simulation-based approximations (e.g. <xref ref-type="bibr" rid="bib104">Turner and Sederberg, 2014</xref>). This expense becomes infeasible in the case of RL-EAMs since these models assume different drift rates per trial, requiring the simulation of an entire dataset <italic>per trial</italic> in the empirical data (for each iteration of the MCMC sampling process). Furthermore, the origin of the concept of urgency lies in studies using racing accumulator models (<xref ref-type="bibr" rid="bib28">Ditterich, 2006</xref>; <xref ref-type="bibr" rid="bib57">Mazurek et al., 2003</xref>; <xref ref-type="bibr" rid="bib85">Reddi and Carpenter, 2000</xref>), which was only later incorporated in the DDM (<xref ref-type="bibr" rid="bib64">Milosavljevic et al., 2010</xref>); the implementation in the RL-ARD remains conceptually close to the early proposals.</p><p>The advantage component of the RL-ARD, which is similar to the input to the DDM, was strongly supported over a model in which each accumulator only receives evidence favoring its own choice. The sum component provides a simple and theoretically transparent way to deal with reward magnitude effects in instrumental learning. Despite having the weakest effect among the three components, the sum was clearly necessary to provide an accurate fit to our data. It also played an important role in explaining the effect of reward reversals.</p><p>In all our models, we assumed a linear function to link Q-value differences to drift rates. This may not be adequate in all settings. For example, <xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref> showed (within an RL-DDM) that a non-linear linking function provided a better fit to their data. This could be caused by a non-linear mapping between objective values and subjective values; the account of perceptual choice using the advantage framework (<xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>) relied on a logarithmic mapping between (objective) luminance and (subjective) brightness magnitudes. Prospect Theory (e.g. <xref ref-type="bibr" rid="bib105">Tversky and Kahneman, 1992</xref>) also assumes that increasingly large objective values relative to a reference point lead to increasingly small increases in subjective value. Such non-linear effects only become evident for sufficiently large differences over an appropriate range. Although in our experiments the RL-ARD was able to explain the data well using a simple linear function, future applications may need to explicitly incorporate a non-linear value function.</p><p>Finally, we showed that the RL-ARD can be extended to multi-alternative choice. In a three-alternative instrumental learning experiment, it accurately predicted the learning curves and full RT distributions for four conditions that differed in difficulty and reward magnitude. Furthermore, examination of Q-value evolution clarified how the reward magnitude manipulation led to the observed behavioral effects. Notably, the number of choice options only changes the architecture of the RL-ARD while the number of parameters remains constant, and all accumulators in the model remain driven by the same three components: an urgency signal, an advantage, and a sum component. As a consequence, parametric complexity does not increase with number of choices and the model remained fully recoverable despite the relatively low number of trials.</p><p>It is perhaps surprising that the RL-DDM consistently overestimated RT variability and skewness given that the DDM typically provides much better fits to data from perceptual decision-making tasks without learning. The inclusion of between-trial variability in non-decision times partially mitigated the misfit but required an implausibly high non-decision time variability, and model comparisons still favored the RL-ARD. Previous work on the RL-DDM did not investigate this issue. In many RL-DDM papers, RT distributions are either not visualized at all, or are plotted using (defective) probability density functions on top of a histogram of RT data, making it hard to detect misfit, particularly with respect to skew due to the slow tail of the distribution. One exception is <xref ref-type="bibr" rid="bib73">Pedersen and Frank, 2020</xref>, whose quantile-based plots show the same pattern that we found here of over-estimated variability and skewness for more difficult choice conditions, despite including between-trial variability in non-decision times. In a non-learning context, it has been shown that the DDM overestimates skewness in high-risk preferential choice data (<xref ref-type="bibr" rid="bib32">Dutilh and Rieskamp, 2016</xref>). Together these results suggest that decision processes in value-based decision in general, and instrumental learning tasks in particular, may be fundamentally different from a two-sided diffusion process, and instead better captured by a race model such as the RL-ARD.</p><p>In the current work, we chose to use racing diffusion processes over the more often used LBA models for reasons of parsimony: error-driven learning introduces between-trial variability in accumulation rates, which are explicitly modeled in the RL-EAM framework. As the LBA includes between-trial variability in drift rates as a free parameter, multiple parameters can account for the same variance. Nonetheless, exploratory fits (see <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>) confirmed our expectation that an RL-ALBA (Advantage LBA) model fit the data of experiment one well, although formal model comparisons preferred the RL-ARD. Future work might consider completely replacing one or more sources of between trial variability in the LBA with structured fluctuations due to learning and adaption mechanisms.</p><p>The parametrization of the ARD model used in the current paper followed the ALBA model proposed by <xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>. This parametrization interprets the influence on drift rates in terms of advantages and magnitudes. However, as both the weights on Q-value differences and sums (<inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) are freely estimated parameters, the equations that define the drift rates can be rearranged as follows:<disp-formula id="equ1"><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in the parametrization of <xref ref-type="disp-formula" rid="equ7">Equation 4</xref>, and <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This re-parametrization shows that each drift rate is determined by an excitatory influence <inline-formula><mml:math id="inf222"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the Q-value associated with the accumulator, and an inhibitory influence <inline-formula><mml:math id="inf223"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mi/></mml:math></inline-formula> of the Q-value associated with the other accumulator. <xref ref-type="bibr" rid="bib103">Turner, 2019</xref> proposed that inhibition plays an important role in learning tasks. Although the locus of inhibition is different in the two models, there are clear parallels that bear further investigation.</p><p>A limitation of the current work is that we collapsed across blocks in analyzing the data of experiments 2, 3, and 4. However, in more detailed explorations (see <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>) there were indications of second-order changes across blocks. In experiment 2, participants were faster in the first trial bin of the second and third block compared to the first block, suggesting additional practice or adaptation effects at the beginning of the experiment. In experiment 3, participants slowed down, and learned the reversal faster, after the first block. This suggests they learned about the presence of reversals in the first block and applied a different strategy in the later blocks. Although it is known that participants increase their learning rates in volatile environments (<xref ref-type="bibr" rid="bib7">Behrens et al., 2007</xref>), this by itself does not explain a decrease in response speed. Potentially, if participants understood the task structure after the first block, model-based strategies, such as estimating the probability of a reversal having occurred, also slowed down responses. In experiment 4, participants were slower (but equally accurate) in the first block compared to the later blocks, suggesting again additional practice or adaptation effects. Future experiments should investigate the nature of these additional adaptation effects.</p><p>Although the account of data provided by the RL-ARD model was generally quite accurate, some elements of misfit suggest the need for further model development. RT and accuracy were underestimated in the initial trials of the easiest condition in experiment 1, in the accuracy emphasis condition in experiment 2, and prior to reversals in experiment 3. Furthermore, the RL-ARD model underestimated the speed with which choice probability changed after reversal of stimulus-response mappings. These misfits point to a limited ability to capture the learning-related changes in behavior. This is to some degree unsurprising, since we used a very simple model of error-driven learning. Future work might explore more sophisticated mechanisms, such as multiple learning rates (<xref ref-type="bibr" rid="bib24">Daw et al., 2002</xref>; <xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref>; <xref ref-type="bibr" rid="bib42">Gershman, 2015</xref>; <xref ref-type="bibr" rid="bib72">Pedersen et al., 2017</xref>) or different learning rules (<xref ref-type="bibr" rid="bib35">Fontanesi et al., 2019b</xref>; <xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref>). Furthermore, there is clearly a role for working memory in some reinforcement learning tasks (<xref ref-type="bibr" rid="bib22">Collins and Frank, 2018</xref>; <xref ref-type="bibr" rid="bib21">Collins and Frank, 2012</xref>), likely explaining the accurate but slow responses we observed in the early trial bins for easy conditions in experiments 1–3. The multi-alternative choice experiment, with 12 individual stimuli per block, hampered participants’ ability to employ working memory strategies, so even early-trial performance was explained by purely error-driven learning.</p><p>In summary, we believe that the ARD decision mechanism provides a firm basis for further explorations of the mutual benefits that arise from the combination of reinforcement learning and evidence-accumulation models, providing constraint that is based on a more comprehensive account of data than has been possible in the past. As it stands, the RL-ARD’s parameter recovery properties are good even with relatively low trial numbers, making it a suitable measurement model for simultaneously studying learning and decision-making processes, and inter-individual differences therein. Further, the advantage framework extends to multiple choice while maintaining analytical tractability and addressing key empirical phenomena in that domain, such as Hick’s Law and response-competition effects (<xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>), enabling future applications to clinical settings, such as in the Iowa gambling task (<xref ref-type="bibr" rid="bib6">Bechara et al., 1994</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experiment 1</title><sec id="s4-1-1"><title>Participants</title><p>61 participants (mean age 21 years old [SD 2.33], 47 women, 56 right handed) were recruited from the subject pool of the department of Psychology, University of Amsterdam, and participated for course credits. All participants had normal or corrected-to-normal vision and gave written informed consent prior to the experiment onset. They did not participate in the other experiments. The study was approved by the local ethics committee.</p></sec><sec id="s4-1-2"><title>Task</title><p>The task was an instrumental probabilistic learning task (<xref ref-type="bibr" rid="bib37">Frank et al., 2004</xref>). On each trial, the subject was presented with two abstract symbols (a ‘stimulus pair’) representing two choice options (see <xref ref-type="fig" rid="fig2">Figure 2A</xref> for an example trial). Each choice option had a fixed probability of being rewarded with points when chosen, with one choice option always having a higher probability of being rewarded than the other. The task is to discover, by trial and error, which choice options are most likely to lead to rewards, and thereby to collect as many points as possible.</p><p>After a short practice block to get familiar with the task, participants completed one block of 208 trials. Four different pairs of abstract symbols were included, each presented 52 times. Stimulus pairs differed in their associated reward probabilities: 0.8/0.2, 0.7/0.3, 0.65/0.35, and 0.6/0.4. The size of the reward, if obtained, was always the same: ‘+100’ (or ‘+0’ otherwise). Reward probabilities were chosen such that they differed only in the between-choice difference in reward probability, leading to varying choice difficulties while keeping the mean reward magnitude fixed.</p><p>Participants were instructed to earn as many points as possible, and to always respond before the deadline of 2 s. Feedback consisted of two parts: an ‘outcome’ and a ‘reward’. The outcome corresponded to the probabilistic outcome of the choice, whereas the reward corresponded to the actual number of earned points. When participants responded before the deadline, the reward was equal to the outcome. If they were too late, the outcome was shown to allow participants to learn from their choice, but the reward they received was set to 0 to encourage responding in time. Participants received a bonus depending on the number of points earned (maximum +0.5 course credits, mean received +0.24). The task was coded in PsychoPy (<xref ref-type="bibr" rid="bib74">Peirce et al., 2019</xref>). After this block, participants performed two more blocks of the same task with different manipulations, which are not of current interest.</p></sec><sec id="s4-1-3"><title>Exclusion</title><p>Six participants were excluded from analysis: One reported, after the experiment, not to have understood the task, one reported a technical issue, and four did not reach an above-chance accuracy level as determined by a binomial test (accuracy cut-off 0.55, corresponding to p&lt;0.05). The final sample thus consisted of 55 subjects (14 men, mean age 21 years old [SD 2.39], 51 right-handed).</p></sec><sec id="s4-1-4"><title>Cognitive modeling</title><p>The main analysis consists of fitting four RL-EAMs to the data and comparing the quality of the fits penalized by model complexity. We compared four different decision models: the DDM (<xref ref-type="bibr" rid="bib78">Ratcliff, 1978</xref>), a racing diffusion (<xref ref-type="bibr" rid="bib14">Boucher et al., 2007</xref>; <xref ref-type="bibr" rid="bib55">Logan et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Purcell et al., 2010</xref>; <xref ref-type="bibr" rid="bib103">Turner, 2019</xref>) model, and two Advantage Racing Diffusion (ARD; <xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>) models (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for an overview). Although the former is a two-sided diffusion process, the latter three models employ a race architecture.</p><p>For all models we used the simple delta update rule as a learning model:<disp-formula id="equ2"><label>(1)</label><mml:math id="m2"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow> <mml:mi/><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="inf224"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the value representation of choice option <inline-formula><mml:math id="inf225"><mml:mi>i</mml:mi></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf226"><mml:mi>t</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf227"><mml:mi>α</mml:mi></mml:math></inline-formula> the learning rate, and <inline-formula><mml:math id="inf228"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the reward on trial <inline-formula><mml:math id="inf229"> <mml:mi/><mml:mi>t</mml:mi></mml:math></inline-formula>. The difference between the actual reward and the value representation of the chosen stimulus, <inline-formula><mml:math id="inf230"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow> <mml:mi/><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, is known as the reward prediction error. The learning rate controls the speed at which Q-values change in response to the reward prediction error, with larger learning rates leading to stronger fluctuations. In this model, only the Q-value of the chosen option is updated.</p></sec><sec id="s4-1-5"><title>RL-EAM 1: RL-DDM</title><p>In the first RL-EAM, we use the DDM (<xref ref-type="bibr" rid="bib78">Ratcliff, 1978</xref>) as a choice model (<xref ref-type="fig" rid="fig1">Figure 1</xref>, left column). The DDM assumes that evidence accumulation is governed by:<disp-formula id="equ3"><mml:math id="m3"><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:math></disp-formula><inline-formula><mml:math id="inf231"><mml:mi>v</mml:mi></mml:math></inline-formula> is the mean speed of evidence accumulation (the <italic>drift rate</italic>), and <inline-formula><mml:math id="inf232"><mml:mi>s</mml:mi></mml:math></inline-formula> is the standard deviation of the within-trial accumulation white noise (W). The RL-DDM assumes that the drift rate depends linearly on the difference of value representations:<disp-formula id="equ4"><mml:math id="m4"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></disp-formula><inline-formula><mml:math id="inf233"><mml:mi>w</mml:mi></mml:math></inline-formula> is a weighting variable, and <inline-formula><mml:math id="inf234"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf235"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the <italic>Q-</italic>values for both choice options per trial, which change each trial according to <xref ref-type="disp-formula" rid="equ2">Equation 1</xref>. Hence,<disp-formula id="equ5"><label>(2)</label><mml:math id="m5"><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>w</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo> <mml:mi/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:math></disp-formula></p><p>The starting point of evidence accumulation, <italic>z</italic>, lies between decision boundaries <inline-formula><mml:math id="inf236"><mml:mi>a</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf237"><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:math></inline-formula>. Here, as in earlier RL-DDM work (<xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref>; <xref ref-type="bibr" rid="bib35">Fontanesi et al., 2019b</xref>; <xref ref-type="bibr" rid="bib72">Pedersen et al., 2017</xref>), we assume an unbiased start of the decision process (i.e., <italic>z =</italic> 0). Evidence accumulation finishes when threshold <inline-formula><mml:math id="inf238"><mml:mi>a</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="inf239"><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:math></inline-formula> is reached, and the decision for the choice corresponding to <inline-formula><mml:math id="inf240"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf241"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, respectively, is made. The response time is the time required for the evidence-accumulation process to reach the bound, plus an intercept called the non-decision time (<inline-formula><mml:math id="inf242"><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula>). The non-decision time is the sum of the time required for perceptual encoding and the time required for the execution of the motor response. Parameter <italic>s</italic> was fixed to 1 to satisfy scaling constraints (<xref ref-type="bibr" rid="bib29">Donkin et al., 2009</xref>; <xref ref-type="bibr" rid="bib107">van Maanen and Miletić, 2020</xref>). In total, this specification of the RL-DDM has 4 free parameters (<inline-formula><mml:math id="inf243"><mml:mi>α</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>w</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>a</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:math></inline-formula>).</p><p>Furthermore, we fit four additional RL-DDMs (RL-DDM A1-A4) with between-trial variabilities in start point, drift rate, and non-decision time, as well as a non-linear link function between Q-values and drift rates (<xref ref-type="bibr" rid="bib34">Fontanesi et al., 2019a</xref>). RL-DDM A1 uses the non-linear function <inline-formula><mml:math id="inf244"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to link Q-values to drift rates (<xref ref-type="bibr" rid="bib35">Fontanesi et al., 2019b</xref>). For the new <inline-formula><mml:math id="inf245"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> parameter, <inline-formula><mml:math id="inf246"><mml:mi mathvariant="script">𝒩</mml:mi><mml:mo>(</mml:mo><mml:mn>2,5</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> (truncated at 0) and <inline-formula><mml:math id="inf247"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1,1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> were used as priors for the hypermean and hyperSD, respectively. RL-DDM A2 includes between-trial variabilities in both drift rate <inline-formula><mml:math id="inf248"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub> <mml:mi/></mml:math></inline-formula> and start point <inline-formula><mml:math id="inf249"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, with <inline-formula><mml:math id="inf250"><mml:mi mathvariant="script">𝒩</mml:mi><mml:mo>(</mml:mo><mml:mn>0.1,0.1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="inf251"><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0.1,0.1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> as priors for hypermeans (respectively, both truncated at 0) and <inline-formula><mml:math id="inf252"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1,1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> for the hyperSD. Drift rate variability was estimated as a proportion of the current drift rate, such that <inline-formula><mml:math id="inf253"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (which allows for higher variability terms for higher Q-value differences, but retains the ratio <inline-formula><mml:math id="inf254"><mml:mi>v</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>. RL-DDM A3 included <inline-formula><mml:math id="inf255"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf256"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and also between-trial variability in non-decision time <inline-formula><mml:math id="inf257"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, for which <inline-formula><mml:math id="inf258"><mml:mi mathvariant="script">𝒩</mml:mi><mml:mo>(</mml:mo><mml:mn>0.1,0.1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> (truncated at 0) and <inline-formula><mml:math id="inf259"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1,1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> were used as priors for the hypermean and hyperSD, respectively. RL-DDM A4 used all three between-trial variabilities as well as the non-linear link function. The quality of fits of these additional models can be found in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. Foreshadowing the results, the RL-DDM A3 improved the quality of fit compared to the RL-DDM, but required an implausibly high non-decision time variability: The across-subject mean of the median posterior estimates of the t0 and <inline-formula><mml:math id="inf260"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> parameters indicate a non-decision time distribution of [0.27 s, 0.64 s]. The range of 0.37 s is very high in light of the literature (<xref ref-type="bibr" rid="bib100">Tran et al., 2021</xref>), raising the question of its psychological plausibility. For this reason, as well as since the RL-DDM is used most often without <inline-formula><mml:math id="inf261"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, we focus on the RL-DDM (without between-trial variabilities) in the main text.</p></sec><sec id="s4-1-6"><title>RL-EAM 2: RL-RD</title><p>The RL-RD (<xref ref-type="fig" rid="fig1">Figure 1</xref>, middle panel) assumes that two evidence accumulators independently accrue evidence for one choice option each, both racing toward a common threshold <inline-formula><mml:math id="inf262"><mml:mi>a</mml:mi></mml:math></inline-formula> (assuming no response bias). The first accumulator to hit the bound wins, and the corresponding decision is made. For each choice option <italic>i</italic>, the dynamics of accumulation are governed by:<disp-formula id="equ6"><label>(3)</label><mml:math id="m6"><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf263"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is a parameter specifying the drift rate in the absence of any evidence, <inline-formula><mml:math id="inf264"><mml:mi>w</mml:mi></mml:math></inline-formula> a weighting parameter, and <inline-formula><mml:math id="inf265"><mml:mi>s</mml:mi></mml:math></inline-formula> the standard deviation of within-trial noise. As such, the mean speed of accumulation (the drift rate <inline-formula><mml:math id="inf266"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) is the sum of two independent factors: an evidence-independent baseline speed <inline-formula><mml:math id="inf267"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, and an evidence-dependent weighted Q-value, <inline-formula><mml:math id="inf268"><mml:mi>w</mml:mi><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Since <inline-formula><mml:math id="inf269"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is assumed to be identical across accumulators, and governs the speed of accumulation unrelated to the amount of evidence, we interpret this parameter as an additive urgency signal (<xref ref-type="bibr" rid="bib62">Miletić and van Maanen, 2019</xref>), with conceptually similar behavioral effects as collapsing bounds (<xref ref-type="bibr" rid="bib43">Hawkins et al., 2015</xref>). Similar to the DDM, a non-decision time parameter accounts for the time for perceptual encoding and the motor response time. Parameter <inline-formula><mml:math id="inf270"><mml:mi>s</mml:mi></mml:math></inline-formula> was fixed to 1 to satisfy scaling constraints (<xref ref-type="bibr" rid="bib29">Donkin et al., 2009</xref>; <xref ref-type="bibr" rid="bib107">van Maanen and Miletić, 2020</xref>). In total, the RL-RD has 5 free parameters <inline-formula><mml:math id="inf271"><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>w</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>a</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:mi>v</mml:mi><mml:mn>0</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mi>t</mml:mi><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>.</p><p>Each accumulator’s first passage times are Wald (also known as inverted Gaussian) distributed (<xref ref-type="bibr" rid="bib1">Anders et al., 2016</xref>). In an independent race model, each accumulator’s first passage time distribution is normalized to the probability of the response with which it is associated (<xref ref-type="bibr" rid="bib17">Brown and Heathcote, 2008</xref>; <xref ref-type="bibr" rid="bib103">Turner, 2019</xref>).</p></sec><sec id="s4-1-7"><title>RL-EAM 3 and 4: RL-ARD</title><p>Thirdly, we fit two racing diffusion models based on an advantage race architecture (<xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>). An advantage race model using an LBA has been shown to provide a natural account for multi-alternative choice phenomena such as Hick’s law, as well as stimulus magnitude effects in perceptual decision-making. As in the RL-RD, accumulators race toward a common bound, but the speed of evidence accumulation <inline-formula><mml:math id="inf272"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mi/></mml:math></inline-formula> depends on multiple factors: first, as in the RL-RD, the evidence-independent speed of accumulation <inline-formula><mml:math id="inf273"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>; second, the <italic>advantage</italic> of the evidence for one choice option over the other (c.f. the DDM, where the difference between evidence for both choice options is accumulated); and third, the <italic>sum</italic> of the total available evidence. Combined, for two accumulators in the RL-EAM framework, this leads to:<disp-formula id="equ7"><label>(4)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In the original work proposing the advantage accumulation framework (<xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>), it was shown that the <inline-formula><mml:math id="inf274"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> parameter had a much stronger influence on evidence-accumulation rates than the <inline-formula><mml:math id="inf275"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> parameter. Therefore, we first fixed the <inline-formula><mml:math id="inf276"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> parameter to 0, to test whether the accumulation of <italic>differences</italic> is sufficient to capture all trends in the data. We term this model the RL-lARD (l = limited), which we compare to the RL-ARD in which we fit <inline-formula><mml:math id="inf277"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as a free parameter.</p><p>As previously, parameter <inline-formula><mml:math id="inf278"><mml:mi>s</mml:mi></mml:math></inline-formula> was fixed to 1 to satisfy scaling constraints (<xref ref-type="bibr" rid="bib29">Donkin et al., 2009</xref>; <xref ref-type="bibr" rid="bib107">van Maanen and Miletić, 2020</xref>). The RL-ARD also has a threshold, non-decision time, and learning rate parameter, totaling five <inline-formula><mml:math id="inf279"><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:mi>a</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:mi>t</mml:mi><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> and 6 free parameters <inline-formula><mml:math id="inf280"><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:mi>a</mml:mi><mml:mo>,</mml:mo> <mml:mi/><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo> <mml:mi/><mml:mi>t</mml:mi><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> for the RL-lARD and RL-ARD, respectively. A parameter recovery study (e.g. <xref ref-type="bibr" rid="bib46">Heathcote et al., 2015</xref>; <xref ref-type="bibr" rid="bib60">Miletić et al., 2017</xref>; <xref ref-type="bibr" rid="bib65">Moran, 2016</xref>; <xref ref-type="bibr" rid="bib92">Spektor and Kellen, 2018</xref>) was performed to confirm that data-generating parameters can be recovered using the experimental paradigm at hand. The results are shown in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>.</p></sec><sec id="s4-1-8"><title>Bayesian hierarchical parameter estimation, posterior predictive distributions, model comparisons</title><p>We estimated group-level and subject-level posterior distributions of each model’s parameter using a combination of differential evolution (DE) and Markov-chain Monte Carlo sampling (MCMC) with Metropolis-Hastings (<xref ref-type="bibr" rid="bib97">Ter Braak, 2006</xref>; <xref ref-type="bibr" rid="bib102">Turner et al., 2013</xref>). Sampling settings were default as implemented in the Dynamic Models of Choice <italic>R</italic> software (<xref ref-type="bibr" rid="bib47">Heathcote et al., 2019</xref>): The number of chains, D, was three times the number of free parameters. Cross-over probability was set to <inline-formula><mml:math id="inf281"><mml:mn>2.38</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt> <mml:mi/></mml:math></inline-formula> at the subject level and <inline-formula><mml:math id="inf282"><mml:mi>U</mml:mi><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo> <mml:mi/><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula> at the group level. Migration probability was set to 0.05 during burn-in only. Convergence was assessed using visual inspection of the chain traces and Gelman-Rubin diagnostic (<xref ref-type="bibr" rid="bib16">Brooks and Gelman, 1998</xref>; <xref ref-type="bibr" rid="bib41">Gelman and Rubin, 1992</xref>) (individual and ﻿multivariate potential scale factors &lt; 1.03 in all cases).</p><p>Hierarchical models were fit assuming independent normal population (“hyper”) distributions for each parameter. For all models, we estimated the learning rate on a probit scale (mapping [0, 1] onto the real domain), with a normal prior <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mtext> </mml:mtext><mml:mo>∼</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1.6</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib92">Spektor and Kellen, 2018</xref>). Prior distributions for all estimated hyper-mean decision-related parameters were vague. RL-EAMs, the threshold parameter <inline-formula><mml:math id="inf284"><mml:mi mathvariant="normal">a</mml:mi> <mml:mi/><mml:mo>~</mml:mo> <mml:mi/><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo> <mml:mi mathvariant="normal"/><mml:mn>5</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> truncated at 0, and <inline-formula><mml:math id="inf285"><mml:mi mathvariant="normal">t</mml:mi><mml:mn>0</mml:mn> <mml:mi/><mml:mo>~</mml:mo> <mml:mi/><mml:mi mathvariant="script">𝒩</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo> <mml:mi mathvariant="normal"/><mml:mn>0.5</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> truncated at 0.025 s and 1 s (all estimation was carried out on the seconds scale). For the RL-DDM, <inline-formula><mml:math id="inf286"><mml:mi mathvariant="normal">w</mml:mi> <mml:mi/><mml:mo>~</mml:mo> <mml:mi/><mml:mi mathvariant="script">𝒩</mml:mi><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo> <mml:mi mathvariant="normal"/><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. For the RL-RD, <inline-formula><mml:math id="inf287"><mml:mi mathvariant="normal">w</mml:mi> <mml:mi/><mml:mo>~</mml:mo> <mml:mi/><mml:mi mathvariant="script">𝒩</mml:mi><mml:mo>(</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo> <mml:mi mathvariant="normal"/><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>, and for the RL-ARD models, <inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>∼</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>∼</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the hyperSD, a <inline-formula><mml:math id="inf290"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1,1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> distribution was used as prior. Plots of superimposed prior and posterior hyper-distributions confirmed that these prior settings were not influential.</p><p>In initial explorations, we also freely estimated the Q-values at trial 0. However, in the RL-EAMs, the posterior distributions for these Q-values consistently converged on 0, which was therefore subsequently used as a fixed value for all results reported in this paper.</p><p>To visualize the quality of model fit, we took 100 random samples from the estimated parameter posteriors and simulated the experimental design with these parameters. For each behavioral measure (e.g. RT quantiles, accuracy), credible intervals were estimated by taking the range between the 2.5% and 97.5% quantiles of the averages over participants.</p><p>To quantitatively compare the fit of different models, penalized by their complexity, we used the Bayesian predictive information criterion (BPIC; <xref ref-type="bibr" rid="bib2">Ando, 2007</xref>). The BPIC is an analogue of the Bayesian information criterion (BIC), but (unlike the BIC) suitable for models estimated using Bayesian methods. Compared to the deviance information criterion (<xref ref-type="bibr" rid="bib93">Spiegelhalter et al., 2002</xref>), the BPIC penalizes model complexity more strongly to prevent over-fitting (c.f. AIC vs. BIC). Lower BPIC values indicate better trade-offs between fit quality and model complexity.</p></sec></sec><sec id="s4-2"><title>Experiment 2</title><sec id="s4-2-1"><title>Participants</title><p>23 participants (mean age 19 years old [SD 1.06 years], 7 men, 23 right-handed) were recruited from the subject pool of the Department of Psychology of the University of Amsterdam and participated for course credits. Participants did not participate in the other experiments. All participants had normal or corrected-to-normal vision and gave written informed consent prior to the experiment onset. The study was approved by the local ethics committee.</p></sec><sec id="s4-2-2"><title>Task</title><p>Participants performed the same task as in experiment 1, with the addition of an SAT manipulation (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The SAT manipulation included both an instructional cue and a response deadline. Prior to each trial, a cue instructed participants to emphasize either decision speed (‘SPD’) or decision accuracy (‘ACC’) in the upcoming trial, and in speed trials, participants did not earn points if they were too late (&gt;600 ms). As in experiment 1, after each choice participants received feedback consisting of two components: an outcome and a reward. The outcome refers to the outcome of the probabilistic gamble, whereas the reward refers to the number of points participants actually received. If participants responded in time, the reward was equal to the outcome. In speed trials, participants did not earn points if they responded later than 600 ms after stimulus onset, even if the outcome was +100. On trials where participants responded too late, they were additionally informed of the reward that was associated with their choice, had they been in time. This way, even when participants are too late, they still receive the feedback that can be used to learn from their choices.</p><p>The deadline manipulation was added because we hypothesized that instructional cues alone would not be sufficient to persuade participants to change their behavior in the instrumental learning task, since that task specifically requires them to accumulate points. If the received number of points was independent of response times, the optimal strategy to collect most points would be to ignore the cue and focus on accuracy only.</p><p>Participants performed 324 trials divided over three blocks. Within each block, three pairs of stimuli were shown, with associated reward probabilities of 0.8/0.2, 0.7/0.3, and 0.6/0.4. Speed and accuracy trials were randomly interleaved. <xref ref-type="fig" rid="fig2">Figure 2C</xref> depicts the sequence of events in each trial. As this experiment also served as a pilot for an fMRI experiment, we added fixation crosses between each phase of the trial, with jittered durations. A pre-stimulus fixation cross lasted 0.5, 1, 1.5, or 2 s; fixation crosses between cue and stimulus, between stimulus and highlight, and between highlight and feedback lasted 0, 0.5, 1, or 1.5 s; and an inter-trial interval fixation cross lasted 0.5, 1, 1.5, 2, 2.5 s. Each trial took 7.5 s. The experiment took approximately 45 min.</p></sec><sec id="s4-2-3"><title>Exclusion</title><p>Four participants did not reach above-chance performance as indicated by a binomial test (cut-off 0.55, p&lt;0.05), and were excluded from further analyses. The final sample thus consisted of 19 participants (mean age 19 years old [SD 1.16 years], 6 men, 19 right-handed). For one additional participant, a technical error occurred after the first block. This participant was included in the analyses, since the Bayesian estimation framework naturally down-weighs the influence of participants with fewer trials.</p></sec><sec id="s4-2-4"><title>Manipulation check and across-block differences in behavior</title><p>We expected an interaction between SAT conditions and learning. In the early trials, participants have not yet learned the reward contingencies, causing a low evidence accumulation rate compared to later trials. With low rates it takes longer to reach the decision threshold, and small changes in the threshold settings or drift rates (by means of an additive urgency signal) can cause large behavioral effects. Therefore, we expected the behavioral effects of the SAT manipulation to become smaller over the course of learning.</p><p>To formally test for the behavioral effects of the SAT manipulation in experiment 2, we fit two mixed effects models (<xref ref-type="bibr" rid="bib40">Gelman and Hill, 2007</xref>): A linear model with RT as dependent variable, and a logistic model with accuracy as the dependent variable. As fixed effects, trial bin and SAT condition were included. Trial bins were obtained by splitting all trials in ten bins (approximately 20 trials each) per participant. As random effects, only participant was included. For the logistic model using accuracy as a dependent variable, we log-transformed trial bin numbers (<xref ref-type="bibr" rid="bib33">Evans et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Heathcote et al., 2000</xref>), to account for the non-linear relation between accuracy and trial bin (<xref ref-type="fig" rid="fig6">Figure 6</xref>, top row). Mixed effects analyses were done using <italic>lme4</italic> (<xref ref-type="bibr" rid="bib5">Bates et al., 2015</xref>). For all mixed effects models, we report parameter estimates of the fixed effects, their standard error and confidence interval, as well as a p-value obtained from a <italic>t</italic>-distribution with the denominator degrees of freedom approximated using Satterthwaite's method (<xref ref-type="bibr" rid="bib88">Satterthwaite, 1941</xref>), as implemented in the <italic>lmerTest</italic> package (<xref ref-type="bibr" rid="bib53">Kuznetsova et al., 2017</xref>) for the <italic>R</italic> programming language (<xref ref-type="bibr" rid="bib76">R Development Core Team, 2017</xref>).</p><p>Next, we tested for the across block stability of behavior using two mixed effects models. One linear mixed effects model was used to predict RT with block number, trial bin, and their interaction, with a random intercept for participant. A second, logistic mixed effects model was used to test the effect of block, trial bin (log-transformed, as above), and their interaction on choice accuracy. In both models, trial bin is expected to influence the outcome variables, but the assumption of across block stability in behavior is violated if there are main effects of block number and/or interaction effects between block number and trial bin. Mean RT and accuracy by block is shown together with the formal test results in <xref ref-type="fig" rid="fig6s3">Figure 6—figure supplement 3</xref>.</p></sec><sec id="s4-2-5"><title>Cognitive modeling</title><p>We fit three RL-DDMs and seven RL-ARDs. The three RL-DDM models varied either threshold, the Q-value weighting on the drift rates parameter (<xref ref-type="bibr" rid="bib90">Sewell and Stallman, 2020</xref>), or both. The seven RL-ARD allowed all unique combinations of the threshold, urgency, and drift rate parameters free to vary between the speed and accuracy conditions.</p><p>For the accuracy condition, we used the same priors as in experiment 1. In the speed condition, the parameters that were free to vary were estimated as proportional differences from the accuracy conditions; specifically: <inline-formula><mml:math id="inf291"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The prior used was <inline-formula><mml:math id="inf294"><mml:mi mathvariant="script">𝒩</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo> <mml:mi mathvariant="normal"/><mml:mn>5</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> for the hypermean and <inline-formula><mml:math id="inf295"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo> <mml:mi mathvariant="normal"/><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> for the hyperSD of all parameters <inline-formula><mml:math id="inf296"><mml:mi>m</mml:mi></mml:math></inline-formula>, truncated at -1.</p><p>As in experiment 1, we performed a parameter recovery study to confirm that the data-generating parameters can be recovered, using the winning model and a simulation of the paradigm of experiment 2. The results are shown in <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>.</p><p>Additionally, we performed a second model comparison using three variants of RL-DDM A3 (i.e., including between-trial variabilities in start point, drift rate, and non-decision time), which varied the threshold, the Q-value weighting on the drift rate parameters, and both. The results are shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, and lead to the same conclusions as the RL-DDM.</p></sec></sec><sec id="s4-3"><title>Experiment 3</title><sec id="s4-3-1"><title>Participants</title><p>Forty-seven participants (mean age 21 years old [SD 2.81 years], 16 men, 40 right-handed) were recruited from the subject pool of the Department of Psychology of the University of Amsterdam and participated for course credits. Participants did not participate in the other experiments. All participants had normal or corrected-to-normal vision and gave written informed consent prior to the experiment onset. The study was approved by the local ethics committee.</p></sec><sec id="s4-3-2"><title>Task</title><p>The reversal learning task had the same general task structure as experiment 1. Participants completed four blocks of 128 trials each, totaling 512 trials. Within each block, two pairs of stimuli were randomly interleaved, with associated reward probabilities of 0.8/0.2 and 0.7/0.3. Between trials 61 and 68 (uniformly sampled) of each block, the reward probability switched between stimuli, such that the stimulus with a pre-reversal reward probability of 0.8/0.7 had a post-reversal reward probability of 0.2/0.3 (and vice versa). Participants were not informed of the reversals prior to the experiment, but many reported noticing them.</p><p>In addition to the reversal learning task, the experimental session also contained a working memory task that is not of current interest. Thirty participants performed the reversal learning task before the working memory task, and 17 participants afterwards. The entire experiment took approximately one hour.</p></sec><sec id="s4-3-3"><title>Cognitive modeling</title><p>We first tested whether a standard soft-max model is able to capture the quick change in choice behavior after the occurrence of a reversal. Soft-max is given by:<disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mtext> </mml:mtext><mml:mi>β</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mtext> </mml:mtext><mml:mi>β</mml:mi><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf297"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability of choosing option <inline-formula><mml:math id="inf298"><mml:mi>i</mml:mi></mml:math></inline-formula> on trial <inline-formula><mml:math id="inf299"><mml:mi>t</mml:mi></mml:math></inline-formula><italic>, J</italic> is the total number of choice options, and <inline-formula><mml:math id="inf300"><mml:mi>β</mml:mi></mml:math></inline-formula> is a free parameter often called the inverse temperature. The inverse temperature is often interpreted in terms of the exploration/exploitation trade-off (<xref ref-type="bibr" rid="bib25">Daw et al., 2006</xref>), with higher values indicating more exploitation. In two-choice settings, <xref ref-type="disp-formula" rid="equ8">Equation</xref> 5 can be re-written as:<disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>which highlights that the choice probability is driven by the <italic>difference</italic> in Q-values, weighted by the inverse temperature parameter. We hierarchically fit the soft-max model using the same parameter estimation methods as in experiment 1. Priors for the hypermean were set to <inline-formula><mml:math id="inf301"><mml:mi>β</mml:mi><mml:mo>~</mml:mo> <mml:mi/><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>1,5</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> truncated at 0, and for the hyperSD <inline-formula><mml:math id="inf302"><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1,1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>.</p><p>Then, the RL-DDM and RL-ARD were fit to the data using the same methods as in experiment 1. Again, we performed a parameter recovery study, of which the results are shown in <xref ref-type="fig" rid="fig7s3">Figure 7—figure supplement 3</xref>. Similarly, we also fit RL-DDM A3 to the data. In an initial fit, the MCMC chains for 11 (out of 47) participants got stuck in values for <inline-formula><mml:math id="inf303"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of 1 (i.e. <inline-formula><mml:math id="inf304"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> covered the entire range between both thresholds), which are implausibly high and moreover led to convergence problems. We re-fit this model with the prior on <inline-formula><mml:math id="inf305"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">𝒩</mml:mi><mml:mo>(</mml:mo><mml:mn>0.1,0.1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> truncated at 0 and 0.5 (i.e. setting the maximum range of between-trial start point variability to be half the range between the lower and upper threshold), which did converge. The posterior predictives are shown in <xref ref-type="fig" rid="fig7s2">Figure 7—figure supplement 2</xref>. This model led to the same overall conclusions as the standard RL-DDM.</p></sec></sec><sec id="s4-4"><title>Experiment 4</title><sec id="s4-4-1"><title>Participants</title><p>Forty-three participants (mean age 20 years old [SD 4.29 years], 5 men, 36 right-handed) were recruited from the subject pool of the Department of Psychology of the University of Amsterdam and participated for course credits. They did not participate in the other experiments. All participants had normal or corrected-to-normal vision and gave written informed consent prior to the experiment onset. The study was approved by the local ethics committee. Participants performed the task online.</p></sec><sec id="s4-4-2"><title>Task</title><p>The three-alternative instrumental learning task had the same general task structure as before. Participants completed three blocks of 144 trials each, totaling 432 trials. Within each block, four triplets of stimuli were randomly interleaved. We used a factorial design to manipulate both difficulty (defined as the difference between the target and distractor reward probability) and reward magnitude (defined as the mean probability of reward per triplet): 0.8/0.25/0.25 (easy, high magnitude), 0.7/0.3/0.3 (hard, high magnitude), 0.7/0.15/0.15 (easy, low magnitude), and 0.6/0.2/0.2 (hard, low magnitude). The entire experiment took approximately 30 min.</p></sec><sec id="s4-4-3"><title>Exclusion</title><p>Nine subjects did not reach above-chance accuracy and were excluded from analysis (accuracy cut-off 0.37, corresponding to p&lt;0.05 in a binomial test). The final sample thus consisted of 34 subjects (three men, mean age 20 years old [SD 4.68 years], 28 right-handed).</p></sec><sec id="s4-4-4"><title>Cognitive modeling</title><p>A three-alternative RL-ARD with the Win-All stopping rule was fit to the data. The multi-alternative version of the RL-ARD includes one accumulator per directional pairwise difference between choice options, leading to a total of six accumulators (1-2, 1-3, 2-1, 2-3, 3-1, 3-2) for the case of three choice options. Each accumulator is governed by the same evidence-accumulation dynamics as in the two-alternative RL-ARD:<disp-formula id="equ10"><label>(7)</label><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>W</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Hence, each choice option (e.g. 1) is associated with two accumulators that collect evidence for the <italic>advantage</italic> of that option over the other two options (1-2, 1-3). The Win-All stopping rule proposes that a final choice is made when two conditions are satisfied: (1) <italic>both</italic> accumulators corresponding to that choice reached their thresholds, <italic>and</italic> (2) for each other choice option <italic>at least one</italic> accumulator has not yet reached the threshold. The corresponding response time is the decision time of the slowest of the two winning accumulators, plus the non-decision time. The probability of response one at time <italic>t</italic> is then given by <xref ref-type="bibr" rid="bib108">van Ravenzwaaij et al., 2020</xref>; derivation in their Appendix:<disp-formula id="equ11"><mml:math id="m11"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>≠</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>J</mml:mi><mml:mo>≠</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mi>C</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>J</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>≠</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>≠</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>C</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where I are choice options {2,3}, J is an option in {2,3} that is not I, and K is an option in {1,2,3} that is not I. PDF and CDF refer to the probability density and cumulative density function of the Wald distribution.</p><p>Despite the visual complexity of <xref ref-type="fig" rid="fig9">Figure 9</xref>, the three-alternative RL-ARD remains highly constrained by the linking functions of <xref ref-type="disp-formula" rid="equ10">Equation 7</xref> and the Q-value evolution. As only the architecture of the linking between Q-values and accumulators was generalized to accommodate the third choice option, there remains the same number of parameters as in the two-alternative case. Note that the three-alternative Win-All RL-ARD naturally reduces to the two-choice RL-ARD when one of the choice options is removed.</p><p>We fit the three-alternative RL-ARD using the same fitting methods (including the same priors) as in experiment 1. We also performed a parameter recovery study using the same methods as in earlier experiments, the results of which are shown in <xref ref-type="fig" rid="fig11s1">Figure 11—figure supplement 1</xref>.</p></sec></sec><sec id="s4-5"><title>Code availability statement</title><p>All analysis codes are available on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/ygrve/">https://osf.io/ygrve/</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Barbara Mathiopoulou and Chris Riddell for their help collecting the data. This work was supported by an NWO-VICI grant (BUF), an ABC VIP grant and ARC DP150100272 and DP160101891 grants (AH).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf2"><p>Reviewing editor, <italic>eLife</italic></p></fn><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Data curation, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Data curation, Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Supervision, Funding acquisition, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other" id="fn1"><p>Human subjects: Informed consent was obtained in all experiments prior to the experiment onset. The local ethics board of the Department of Psychology, University of Amsterdam, approved the study, with reference numbers 2018-BC-9620 (experiment 1), 2019-BC-10672 (experiment 2), 2019-BC-10250 (experiment 3), and 2020-BC-12788 (experiment 4).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-63055-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data analysed in this study are available from <ext-link ext-link-type="uri" xlink:href="https://osf.io/ygrve/">https://osf.io/ygrve/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Miletic</surname><given-names>S</given-names></name><name><surname>Boag</surname><given-names>RJ</given-names></name><name><surname>Trutti</surname><given-names>AC</given-names></name><name><surname>Stevenson</surname><given-names>N</given-names></name><name><surname>Forstmann</surname><given-names>BU</given-names></name><name><surname>Heathcote</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>A new model of decision processing in instrumental learning tasks</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/ygrve/">ygrve</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anders</surname> <given-names>R</given-names></name><name><surname>Alario</surname> <given-names>FX</given-names></name><name><surname>Van Maanen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The shifted wald distribution for response time data analysis</article-title><source>Psychological Methods</source><volume>21</volume><fpage>309</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1037/met0000066</pub-id><pub-id pub-id-type="pmid">26867155</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ando</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Bayesian predictive information criterion for the evaluation of hierarchical bayesian and empirical bayes models</article-title><source>Biometrika</source><volume>94</volume><fpage>443</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1093/biomet/asm017</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnold</surname> <given-names>NR</given-names></name><name><surname>Bröder</surname> <given-names>A</given-names></name><name><surname>Bayen</surname> <given-names>UJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Empirical validation of the diffusion model for recognition memory and a comparison of parameter-estimation methods</article-title><source>Psychological Research</source><volume>79</volume><fpage>882</fpage><lpage>898</lpage><pub-id pub-id-type="doi">10.1007/s00426-014-0608-y</pub-id><pub-id pub-id-type="pmid">25281426</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barto</surname> <given-names>AG</given-names></name><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Brouwer</surname> <given-names>PS</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Associative search network: a reinforcement learning associative memory</article-title><source>Biological Cybernetics</source><volume>40</volume><fpage>201</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1007/BF00453370</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname> <given-names>D</given-names></name><name><surname>Mächler</surname> <given-names>M</given-names></name><name><surname>Bolker</surname> <given-names>B</given-names></name><name><surname>Walker</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fitting linear Mixed-Effects models using lme4</article-title><source>Journal of Statistical Software</source><volume>67</volume><elocation-id>jss.v067.i01</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bechara</surname> <given-names>A</given-names></name><name><surname>Damasio</surname> <given-names>AR</given-names></name><name><surname>Damasio</surname> <given-names>H</given-names></name><name><surname>Anderson</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Insensitivity to future consequences following damage to human prefrontal cortex</article-title><source>Cognition</source><volume>50</volume><fpage>7</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(94)90018-3</pub-id><pub-id pub-id-type="pmid">8039375</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname> <given-names>TE</given-names></name><name><surname>Woolrich</surname> <given-names>MW</given-names></name><name><surname>Walton</surname> <given-names>ME</given-names></name><name><surname>Rushworth</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="doi">10.1038/nn1954</pub-id><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boag</surname> <given-names>RJ</given-names></name><name><surname>Strickland</surname> <given-names>L</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Neal</surname> <given-names>A</given-names></name><name><surname>Loft</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Cognitive control and capacity for prospective memory in complex dynamic environments</article-title><source>Journal of Experimental Psychology: General</source><volume>148</volume><fpage>2181</fpage><lpage>2206</lpage><pub-id pub-id-type="doi">10.1037/xge0000599</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boag</surname> <given-names>RJ</given-names></name><name><surname>Strickland</surname> <given-names>L</given-names></name><name><surname>Loft</surname> <given-names>S</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Strategic attention and decision control support prospective memory in a complex dual-task environment</article-title><source>Cognition</source><volume>191</volume><elocation-id>103974</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2019.05.011</pub-id><pub-id pub-id-type="pmid">31234118</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehm</surname> <given-names>U</given-names></name><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Brown</surname> <given-names>S</given-names></name><name><surname>van Rijn</surname> <given-names>H</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Of monkeys and men: impatience in perceptual decision-making</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>738</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0958-5</pub-id><pub-id pub-id-type="pmid">26518307</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>McClure</surname> <given-names>SM</given-names></name><name><surname>Li</surname> <given-names>J</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Short-term memory traces for action Bias in human reinforcement learning</article-title><source>Brain Research</source><volume>1153</volume><fpage>111</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2007.03.057</pub-id><pub-id pub-id-type="pmid">17459346</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Nieuwenhuis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The neural basis of the speed-accuracy tradeoff</article-title><source>Trends in Neurosciences</source><volume>33</volume><fpage>10</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2009.09.002</pub-id><pub-id pub-id-type="pmid">19819033</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Larsen</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Integration of reinforcement learning and optimal decision-making theories of the basal ganglia</article-title><source>Neural Computation</source><volume>23</volume><fpage>817</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00103</pub-id><pub-id pub-id-type="pmid">21222528</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boucher</surname> <given-names>L</given-names></name><name><surname>Palmeri</surname> <given-names>TJ</given-names></name><name><surname>Logan</surname> <given-names>GD</given-names></name><name><surname>Schall</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Inhibitory control in mind and brain: an interactive race model of countermanding saccades</article-title><source>Psychological Review</source><volume>114</volume><fpage>376</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.114.2.376</pub-id><pub-id pub-id-type="pmid">17500631</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowman</surname> <given-names>NE</given-names></name><name><surname>Kording</surname> <given-names>KP</given-names></name><name><surname>Gottfried</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Temporal integration of olfactory perceptual evidence in human orbitofrontal cortex</article-title><source>Neuron</source><volume>75</volume><fpage>916</fpage><lpage>927</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.06.035</pub-id><pub-id pub-id-type="pmid">22958830</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brooks</surname> <given-names>SP</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>General methods for monitoring convergence of iterative simulations</article-title><source>Journal of Computational and Graphical Statistics</source><volume>7</volume><fpage>434</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1080/10618600.1998.10474787</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The simplest complete model of choice response time: linear ballistic accumulation</article-title><source>Cognitive Psychology</source><volume>57</volume><fpage>153</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2007.12.002</pub-id><pub-id pub-id-type="pmid">18243170</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christakou</surname> <given-names>A</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Simmons</surname> <given-names>A</given-names></name><name><surname>Brammer</surname> <given-names>M</given-names></name><name><surname>Rubia</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural and psychological maturation of decision-making in adolescence and young adulthood</article-title><source>Journal of Cognitive Neuroscience</source><volume>25</volume><fpage>1807</fpage><lpage>1823</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00447</pub-id><pub-id pub-id-type="pmid">23859647</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision-making with multiple alternatives</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>693</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/nn.2123</pub-id><pub-id pub-id-type="pmid">18488024</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname> <given-names>P</given-names></name><name><surname>Puskas</surname> <given-names>GA</given-names></name><name><surname>El-Murr</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decisions in changing conditions: the urgency-gating model</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>11560</fpage><lpage>11571</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1844-09.2009</pub-id><pub-id pub-id-type="pmid">19759303</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AGE</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis</article-title><source>European Journal of Neuroscience</source><volume>35</volume><fpage>1024</fpage><lpage>1035</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07980.x</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname> <given-names>AGE</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Within- and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory</article-title><source>PNAS</source><volume>115</volume><fpage>2502</fpage><lpage>2507</lpage><pub-id pub-id-type="doi">10.1073/pnas.1720963115</pub-id><pub-id pub-id-type="pmid">29463751</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname> <given-names>VD</given-names></name><name><surname>Tran</surname> <given-names>VL</given-names></name><name><surname>Turchi</surname> <given-names>J</given-names></name><name><surname>Averbeck</surname> <given-names>BB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reversal learning and dopamine: a bayesian perspective</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>2407</fpage><lpage>2416</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1989-14.2015</pub-id><pub-id pub-id-type="pmid">25673835</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Kakade</surname> <given-names>S</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Opponent interactions between serotonin and dopamine</article-title><source>Neural Networks</source><volume>15</volume><fpage>603</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1016/S0893-6080(02)00052-7</pub-id><pub-id pub-id-type="pmid">12371515</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Seymour</surname> <given-names>B</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Cortical substrates for exploratory decisions in humans</article-title><source>Nature</source><volume>441</volume><fpage>876</fpage><lpage>879</lpage><pub-id pub-id-type="doi">10.1038/nature04766</pub-id><pub-id pub-id-type="pmid">16778890</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The algorithmic anatomy of model-based evaluation</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><elocation-id>20130478</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0478</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision theory, reinforcement learning, and the brain</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>8</volume><fpage>429</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.3758/CABN.8.4.429</pub-id><pub-id pub-id-type="pmid">19033240</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ditterich</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Evidence for time-variant decision making</article-title><source>European Journal of Neuroscience</source><volume>24</volume><fpage>3628</fpage><lpage>3641</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2006.05221.x</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donkin</surname> <given-names>C</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The overconstraint of response time models: rethinking the scaling problem</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>1129</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.3758/PBR.16.6.1129</pub-id><pub-id pub-id-type="pmid">19966267</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donkin</surname> <given-names>C</given-names></name><name><surname>Brown</surname> <given-names>S</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Drawing conclusions from choice response time models: a tutorial using the linear ballistic accumulator</article-title><source>Journal of Mathematical Psychology</source><volume>55</volume><fpage>140</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2010.10.001</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Donkin</surname> <given-names>C</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name></person-group><year iso-8601-date="2018">2018</year><chapter-title>Response Times and Decision-Making</chapter-title><person-group person-group-type="editor"><name><surname>Donkin</surname> <given-names>C</given-names></name></person-group><source>Stevens’ Handbook of Experimental Psychology and Cognitive Neuroscience</source><publisher-name> John Wiley &amp; Sons, Inc</publisher-name><fpage>1</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1002/9781119170174.epcn509</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dutilh</surname> <given-names>G</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comparing perceptual and preferential decision making</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>723</fpage><lpage>737</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0941-1</pub-id><pub-id pub-id-type="pmid">26432714</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>NJ</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Mewhort</surname> <given-names>DJK</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Refining the law of practice</article-title><source>Psychological Review</source><volume>125</volume><fpage>592</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1037/rev0000105</pub-id><pub-id pub-id-type="pmid">29952624</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontanesi</surname> <given-names>L</given-names></name><name><surname>Gluth</surname> <given-names>S</given-names></name><name><surname>Spektor</surname> <given-names>MS</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>A reinforcement learning diffusion decision model for value-based decisions</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>26</volume><fpage>1099</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1554-2</pub-id><pub-id pub-id-type="pmid">30924057</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontanesi</surname> <given-names>L</given-names></name><name><surname>Palminteri</surname> <given-names>S</given-names></name><name><surname>Lebreton</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Decomposing the effects of context Valence and feedback information on speed and accuracy during reinforcement learning: a meta-analytical approach using diffusion decision modeling</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>19</volume><fpage>490</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.3758/s13415-019-00723-1</pub-id><pub-id pub-id-type="pmid">31175616</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Sequential sampling models in cognitive neuroscience: advantages, applications, and extensions</article-title><source>Annual Review of Psychology</source><volume>67</volume><fpage>641</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122414-033645</pub-id><pub-id pub-id-type="pmid">26393872</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Seeberger</surname> <given-names>LC</given-names></name><name><surname>O'reilly</surname> <given-names>RC</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>By carrot or by stick: cognitive reinforcement learning in parkinsonism</article-title><source>Science</source><volume>306</volume><fpage>1940</fpage><lpage>1943</lpage><pub-id pub-id-type="doi">10.1126/science.1102941</pub-id><pub-id pub-id-type="pmid">15528409</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Moustafa</surname> <given-names>AA</given-names></name><name><surname>Haughey</surname> <given-names>HM</given-names></name><name><surname>Curran</surname> <given-names>T</given-names></name><name><surname>Hutchison</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Genetic triple dissociation reveals multiple roles for dopamine in reinforcement learning</article-title><source>PNAS</source><volume>104</volume><fpage>16311</fpage><lpage>16316</lpage><pub-id pub-id-type="doi">10.1073/pnas.0706111104</pub-id><pub-id pub-id-type="pmid">17913879</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Doll</surname> <given-names>BB</given-names></name><name><surname>Oas-Terpstra</surname> <given-names>J</given-names></name><name><surname>Moreno</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Prefrontal and striatal dopaminergic genes predict individual differences in exploration and exploitation</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>1062</fpage><lpage>1068</lpage><pub-id pub-id-type="doi">10.1038/nn.2342</pub-id><pub-id pub-id-type="pmid">19620978</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Hill</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Data Analysis Using Regression and Multilevel/Hierarchical Models</source><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Rubin</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Inference from iterative simulation using multiple sequences</article-title><source>Statistical Science</source><volume>7</volume><fpage>457</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1214/ss/1177011136</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Do learning rates adapt to the distribution of rewards?</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>22</volume><fpage>1320</fpage><lpage>1327</lpage><pub-id pub-id-type="doi">10.3758/s13423-014-0790-3</pub-id><pub-id pub-id-type="pmid">25582684</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Revisiting the evidence for collapsing boundaries and urgency signals in perceptual decision-making</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>2476</fpage><lpage>2484</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2410-14.2015</pub-id><pub-id pub-id-type="pmid">25673842</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Racing against the clock: evidence-based vs Time-Based decisions</article-title><source>Psychological Review</source><volume>2020</volume><elocation-id>m4uh7</elocation-id><pub-id pub-id-type="doi">10.31234/osf.io/m4uh7</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Brown</surname> <given-names>S</given-names></name><name><surname>Mewhort</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The power law repealed: the case for an exponential law of practice</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>7</volume><fpage>185</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.3758/BF03212979</pub-id><pub-id pub-id-type="pmid">10909131</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>An Introduction to Good Practices in Cognitive ModelingAn Introduction to Model-Based Cognitive Neuroscience</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4939-2236-9_2</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Lin</surname> <given-names>YS</given-names></name><name><surname>Reynolds</surname> <given-names>A</given-names></name><name><surname>Strickland</surname> <given-names>L</given-names></name><name><surname>Gretton</surname> <given-names>M</given-names></name><name><surname>Matzke</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dynamic models of choice</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>961</fpage><lpage>985</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-1067-y</pub-id><pub-id pub-id-type="pmid">29959755</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Love</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Linear deterministic accumulator models of simple choice</article-title><source>Frontiers in Psychology</source><volume>3</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00292</pub-id><pub-id pub-id-type="pmid">22936920</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname> <given-names>T</given-names></name><name><surname>Brown</surname> <given-names>S</given-names></name><name><surname>van Maanen</surname> <given-names>L</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Serences</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The optimality of sensory processing during the speed-accuracy tradeoff</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>7992</fpage><lpage>8003</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0340-12.2012</pub-id><pub-id pub-id-type="pmid">22674274</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izquierdo</surname> <given-names>A</given-names></name><name><surname>Brigman</surname> <given-names>JL</given-names></name><name><surname>Radke</surname> <given-names>AK</given-names></name><name><surname>Rudebeck</surname> <given-names>PH</given-names></name><name><surname>Holmes</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The neural basis of reversal learning: an updated perspective</article-title><source>Neuroscience</source><volume>345</volume><fpage>12</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2016.03.021</pub-id><pub-id pub-id-type="pmid">26979052</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jang</surname> <given-names>AI</given-names></name><name><surname>Costa</surname> <given-names>VD</given-names></name><name><surname>Rudebeck</surname> <given-names>PH</given-names></name><name><surname>Chudasama</surname> <given-names>Y</given-names></name><name><surname>Murray</surname> <given-names>EA</given-names></name><name><surname>Averbeck</surname> <given-names>BB</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The role of frontal cortical and Medial-Temporal lobe brain Areas in learning a bayesian prior belief on reversals</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>11751</fpage><lpage>11760</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1594-15.2015</pub-id><pub-id pub-id-type="pmid">26290251</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katsimpokis</surname> <given-names>D</given-names></name><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>van Maanen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Not all Speed-Accuracy Trade-Off manipulations have the same psychological effect</article-title><source>Computational Brain &amp; Behavior</source><volume>3</volume><fpage>252</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1007/s42113-020-00074-y</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuznetsova</surname> <given-names>A</given-names></name><name><surname>Brockhoff</surname> <given-names>PB</given-names></name><name><surname>Christensen</surname> <given-names>RHB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>lmerTest Package: Tests in Linear Mixed Effects Models</article-title><source>Journal of Statistical Software</source><volume>82</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v082.i13</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leite</surname> <given-names>FP</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modeling reaction time and accuracy of multiple-alternative decisions</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>72</volume><fpage>246</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.3758/APP.72.1.246</pub-id><pub-id pub-id-type="pmid">20045893</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logan</surname> <given-names>GD</given-names></name><name><surname>Van Zandt</surname> <given-names>T</given-names></name><name><surname>Verbruggen</surname> <given-names>F</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On the ability to inhibit thought and action: general and special theories of an act of control</article-title><source>Psychological Review</source><volume>121</volume><fpage>66</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1037/a0035230</pub-id><pub-id pub-id-type="pmid">24490789</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luzardo</surname> <given-names>A</given-names></name><name><surname>Alonso</surname> <given-names>E</given-names></name><name><surname>Mondragón</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Rescorla-Wagner drift-diffusion model of conditioning and timing</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005796</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005796</pub-id><pub-id pub-id-type="pmid">29095819</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazurek</surname> <given-names>ME</given-names></name><name><surname>Roitman</surname> <given-names>JD</given-names></name><name><surname>Ditterich</surname> <given-names>J</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A role for neural integrators in perceptual decision making</article-title><source>Cerebral Cortex</source><volume>13</volume><fpage>1257</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhg097</pub-id><pub-id pub-id-type="pmid">14576217</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDougle</surname> <given-names>SD</given-names></name><name><surname>Collins</surname> <given-names>AGE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modeling the influence of working memory, reinforcement, and action uncertainty on reaction time and choice during instrumental learning</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>19</volume><elocation-id>01774</elocation-id><pub-id pub-id-type="doi">10.3758/s13423-020-01774-z</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miletić</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neural evidence for a role of urgency in the Speed-Accuracy Trade-off in perceptual Decision-Making</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>5909</fpage><lpage>5910</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0894-16.2016</pub-id><pub-id pub-id-type="pmid">27251612</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miletić</surname> <given-names>S</given-names></name><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>van Maanen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parameter recovery for the leaky competing accumulator model</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>25</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.12.001</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miletić</surname> <given-names>S</given-names></name><name><surname>Boag</surname> <given-names>RJ</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mutual benefits: combining reinforcement learning with sequential sampling models</article-title><source>Neuropsychologia</source><volume>136</volume><elocation-id>107261</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2019.107261</pub-id><pub-id pub-id-type="pmid">31733237</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miletić</surname> <given-names>S</given-names></name><name><surname>van Maanen</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Caution in decision-making under time pressure is mediated by timing ability</article-title><source>Cognitive Psychology</source><volume>110</volume><fpage>16</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2019.01.002</pub-id><pub-id pub-id-type="pmid">30735843</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Millner</surname> <given-names>AJ</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Nock</surname> <given-names>MK</given-names></name><name><surname>den Ouden</surname> <given-names>HEM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pavlovian control of escape and avoidance</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1379</fpage><lpage>1390</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01224</pub-id><pub-id pub-id-type="pmid">29244641</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milosavljevic</surname> <given-names>M</given-names></name><name><surname>Malmaud</surname> <given-names>J</given-names></name><name><surname>Huth</surname> <given-names>A</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Rangel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The drift diffusion model can account for the accuracy and reaction time of Value-Based choices under high and low time pressure</article-title><source>Judgment and Decision Making</source><volume>5</volume><fpage>437</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.2139/ssrn.1901533</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moran</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Thou shalt identify! the identifiability of two high-threshold models in confidence-rating recognition (and super-recognition) paradigms</article-title><source>Journal of Mathematical Psychology</source><volume>73</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.03.002</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>PR</given-names></name><name><surname>Boonstra</surname> <given-names>E</given-names></name><name><surname>Nieuwenhuis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Global gain modulation generates time-dependent urgency during perceptual choice in humans</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13526</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13526</pub-id><pub-id pub-id-type="pmid">27882927</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Edlund</surname> <given-names>JA</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>O'Doherty</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural prediction errors reveal a risk-sensitive reinforcement-learning process in the human brain</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>551</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5498-10.2012</pub-id><pub-id pub-id-type="pmid">22238090</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Doherty</surname> <given-names>JP</given-names></name><name><surname>Cockburn</surname> <given-names>J</given-names></name><name><surname>Pauli</surname> <given-names>WM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning, reward, and decision making</article-title><source>Annual Review of Psychology</source><volume>68</volume><fpage>73</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010416-044216</pub-id><pub-id pub-id-type="pmid">27687119</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachella</surname> <given-names>RG</given-names></name><name><surname>Pew</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Speed-Accuracy tradeoff in reaction time: effect of discrete criterion times</article-title><source>Journal of Experimental Psychology</source><volume>76</volume><fpage>19</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1037/h0021275</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname> <given-names>S</given-names></name><name><surname>Khamassi</surname> <given-names>M</given-names></name><name><surname>Joffily</surname> <given-names>M</given-names></name><name><surname>Coricelli</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Contextual modulation of value signals in reward and punishment learning</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8096</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9096</pub-id><pub-id pub-id-type="pmid">26302782</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname> <given-names>S</given-names></name><name><surname>Wyart</surname> <given-names>V</given-names></name><name><surname>Koechlin</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The importance of falsification in computational cognitive modeling</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.03.011</pub-id><pub-id pub-id-type="pmid">28476348</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedersen</surname> <given-names>ML</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Biele</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The drift diffusion model as the choice rule in reinforcement learning</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>24</volume><fpage>1234</fpage><lpage>1251</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1199-y</pub-id><pub-id pub-id-type="pmid">27966103</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedersen</surname> <given-names>ML</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simultaneous hierarchical bayesian parameter estimation for reinforcement learning and drift diffusion models: a tutorial and links to neural data</article-title><source>Computational Brain &amp; Behavior</source><volume>3</volume><fpage>458</fpage><lpage>471</lpage><pub-id pub-id-type="doi">10.1007/s42113-020-00084-w</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>J</given-names></name><name><surname>Gray</surname> <given-names>JR</given-names></name><name><surname>Simpson</surname> <given-names>S</given-names></name><name><surname>MacAskill</surname> <given-names>M</given-names></name><name><surname>Höchenberger</surname> <given-names>R</given-names></name><name><surname>Sogo</surname> <given-names>H</given-names></name><name><surname>Kastman</surname> <given-names>E</given-names></name><name><surname>Lindeløv</surname> <given-names>JK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PsychoPy2: experiments in behavior made easy</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id><pub-id pub-id-type="pmid">30734206</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purcell</surname> <given-names>BA</given-names></name><name><surname>Heitz</surname> <given-names>RP</given-names></name><name><surname>Cohen</surname> <given-names>JY</given-names></name><name><surname>Schall</surname> <given-names>JD</given-names></name><name><surname>Logan</surname> <given-names>GD</given-names></name><name><surname>Palmeri</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neurally constrained modeling of perceptual decision making</article-title><source>Psychological Review</source><volume>117</volume><fpage>1113</fpage><lpage>1143</lpage><pub-id pub-id-type="doi">10.1037/a0020311</pub-id><pub-id pub-id-type="pmid">20822291</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="software"><person-group person-group-type="author"><collab>R Development Core Team</collab></person-group><year iso-8601-date="2017">2017</year><data-title>R: A Language and Environment for Statistical Computing</data-title><publisher-loc>Vienna, Austria</publisher-loc><publisher-name>R Foundation for Statistical Computing</publisher-name><ext-link ext-link-type="uri" xlink:href="http://www.r-project.org">http://www.r-project.org</ext-link></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rae</surname> <given-names>B</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Donkin</surname> <given-names>C</given-names></name><name><surname>Averell</surname> <given-names>L</given-names></name><name><surname>Brown</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The hare and the tortoise: emphasizing speed can change the evidence used to make decisions</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>40</volume><fpage>1226</fpage><lpage>1243</lpage><pub-id pub-id-type="doi">10.1037/a0036801</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Hasegawa</surname> <given-names>YT</given-names></name><name><surname>Hasegawa</surname> <given-names>RP</given-names></name><name><surname>Smith</surname> <given-names>PL</given-names></name><name><surname>Segraves</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Dual diffusion model for single-cell recording data from the superior colliculus in a brightness-discrimination task</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>1756</fpage><lpage>1774</lpage><pub-id pub-id-type="doi">10.1152/jn.00393.2006</pub-id><pub-id pub-id-type="pmid">17122324</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Hasegawa</surname> <given-names>YT</given-names></name><name><surname>Hasegawa</surname> <given-names>RP</given-names></name><name><surname>Childers</surname> <given-names>R</given-names></name><name><surname>Smith</surname> <given-names>PL</given-names></name><name><surname>Segraves</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inhibition in superior colliculus neurons in a brightness discrimination task?</article-title><source>Neural Computation</source><volume>23</volume><fpage>1790</fpage><lpage>1820</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00135</pub-id><pub-id pub-id-type="pmid">21492006</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Smith</surname> <given-names>PL</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>McKoon</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Diffusion decision model: current issues and history</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>260</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.01.007</pub-id><pub-id pub-id-type="pmid">26952739</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Voskuilen</surname> <given-names>C</given-names></name><name><surname>Teodorescu</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Modeling 2-alternative forced-choice tasks: accounting for both magnitude and difference effects</article-title><source>Cognitive Psychology</source><volume>103</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2018.02.002</pub-id><pub-id pub-id-type="pmid">29501775</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>McKoon</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The diffusion decision model: theory and data for two-choice decision tasks</article-title><source>Neural Computation</source><volume>20</volume><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id><pub-id pub-id-type="pmid">18085991</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Rouder</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Modeling response times for Two-Choice decisions</article-title><source>Psychological Science</source><volume>9</volume><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00067</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddi</surname> <given-names>BA</given-names></name><name><surname>Carpenter</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The influence of urgency on decision time</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>827</fpage><lpage>830</lpage><pub-id pub-id-type="doi">10.1038/77739</pub-id><pub-id pub-id-type="pmid">10903577</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rescorla</surname> <given-names>RA</given-names></name><name><surname>Wagner</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>A theory of pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement</article-title><source>Class Cond II Curr Res Theory</source><volume>21</volume><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib87"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rummery</surname> <given-names>GA</given-names></name><name><surname>Niranjan</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>On-Line Q-Learning Using Connectionist Systems</source></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Satterthwaite</surname> <given-names>FE</given-names></name></person-group><year iso-8601-date="1941">1941</year><article-title>Synthesis of variance</article-title><source>Psychometrika</source><volume>6</volume><fpage>309</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1007/BF02288586</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sewell</surname> <given-names>DK</given-names></name><name><surname>Jach</surname> <given-names>HK</given-names></name><name><surname>Boag</surname> <given-names>RJ</given-names></name><name><surname>Van Heer</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Combining error-driven models of associative learning with evidence accumulation models of decision-making</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>26</volume><fpage>868</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.3758/s13423-019-01570-4</pub-id><pub-id pub-id-type="pmid">30719625</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sewell</surname> <given-names>DK</given-names></name><name><surname>Stallman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modeling the effect of speed emphasis in probabilistic category learning</article-title><source>Computational Brain &amp; Behavior</source><volume>3</volume><fpage>129</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1007/s42113-019-00067-6</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahar</surname> <given-names>N</given-names></name><name><surname>Hauser</surname> <given-names>TU</given-names></name><name><surname>Moutoussis</surname> <given-names>M</given-names></name><name><surname>Moran</surname> <given-names>R</given-names></name><name><surname>Keramati</surname> <given-names>M</given-names></name><collab>NSPN consortium</collab><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Improving the reliability of model-based decision-making estimates in the two-stage decision task with reaction-times and drift-diffusion modeling</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006803</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006803</pub-id><pub-id pub-id-type="pmid">30759077</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spektor</surname> <given-names>MS</given-names></name><name><surname>Kellen</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The relative merit of empirical priors in non-identifiable and sloppy models: applications to models of learning and decision-making : empirical priors</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>2047</fpage><lpage>2068</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1446-5</pub-id><pub-id pub-id-type="pmid">29589289</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spiegelhalter</surname> <given-names>DJ</given-names></name><name><surname>Best</surname> <given-names>NG</given-names></name><name><surname>Carlin</surname> <given-names>BP</given-names></name><name><surname>van der Linde</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Bayesian measures of model complexity and fit</article-title><source>Journal of the Royal Statistical Society: Series B</source><volume>64</volume><fpage>583</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1111/1467-9868.00353</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Learning to predict by the methods of temporal differences</article-title><source>Machine Learning</source><volume>3</volume><fpage>9</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1007/BF00115009</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><edition>2nd ed</edition><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teodorescu</surname> <given-names>AR</given-names></name><name><surname>Moran</surname> <given-names>R</given-names></name><name><surname>Usher</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Absolutely relative or relatively absolute: violations of value invariance in human decision making</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>23</volume><fpage>22</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0858-8</pub-id><pub-id pub-id-type="pmid">26022836</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ter Braak</surname> <given-names>CJF</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A markov chain monte carlo version of the genetic algorithm differential evolution: easy bayesian computing for real parameter spaces</article-title><source>Statistics and Computing</source><volume>16</volume><fpage>239</fpage><lpage>249</lpage><pub-id pub-id-type="doi">10.1007/s11222-006-8769-1</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname> <given-names>D</given-names></name><name><surname>Cisek</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Modulation of premotor and primary motor cortical activity during volitional adjustments of Speed-Accuracy Trade-Offs</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>938</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2230-15.2016</pub-id><pub-id pub-id-type="pmid">26791222</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tillman</surname> <given-names>G</given-names></name><name><surname>Van Zandt</surname> <given-names>T</given-names></name><name><surname>Logan</surname> <given-names>GD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sequential sampling models without random between-trial variability: the racing diffusion model of speeded decision making</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>27</volume><fpage>911</fpage><lpage>936</lpage><pub-id pub-id-type="doi">10.3758/s13423-020-01719-6</pub-id><pub-id pub-id-type="pmid">32424622</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tran</surname> <given-names>H</given-names></name><name><surname>Van Maanen</surname> <given-names>L</given-names></name><name><surname>Matzke</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Systematic parameter reviews in cognitive modeling: towards robust and cumulative models of psychological processes</article-title><source>Frontiers in Psychology</source><volume>11</volume><elocation-id>608287</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2020.608287</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trueblood</surname> <given-names>JS</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Evans</surname> <given-names>NJ</given-names></name><name><surname>Holmes</surname> <given-names>WR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Urgency, leakage, and the relative nature of information processing in decision-making</article-title><source>Psychological Review</source><volume>128</volume><fpage>160</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1037/rev0000255</pub-id><pub-id pub-id-type="pmid">32852976</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Steyvers</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A method for efficiently sampling from distributions with correlated dimensions</article-title><source>Psychological Methods</source><volume>18</volume><fpage>368</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1037/a0032222</pub-id><pub-id pub-id-type="pmid">23646991</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Toward a common representational framework for adaptation</article-title><source>Psychological Review</source><volume>126</volume><fpage>660</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1037/rev0000148</pub-id><pub-id pub-id-type="pmid">30973248</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>BM</given-names></name><name><surname>Sederberg</surname> <given-names>PB</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A generalized, likelihood-free method for posterior estimation</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>21</volume><fpage>227</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.3758/s13423-013-0530-0</pub-id><pub-id pub-id-type="pmid">24258272</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tversky</surname> <given-names>A</given-names></name><name><surname>Kahneman</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Advances in prospect theory: cumulative representation of uncertainty</article-title><source>Journal of Risk and Uncertainty</source><volume>5</volume><fpage>297</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1007/BF00122574</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Maanen</surname> <given-names>L</given-names></name><name><surname>van der Mijn</surname> <given-names>R</given-names></name><name><surname>van Beurden</surname> <given-names>M</given-names></name><name><surname>Roijendijk</surname> <given-names>LMM</given-names></name><name><surname>Kingma</surname> <given-names>BRM</given-names></name><name><surname>Miletić</surname> <given-names>S</given-names></name><name><surname>van Rijn</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Core body temperature speeds up temporal processing and choice behavior under deadlines</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>10053</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-46073-3</pub-id><pub-id pub-id-type="pmid">31296893</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Maanen</surname> <given-names>L</given-names></name><name><surname>Miletić</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The interpretation of behavior-model correlations in unidentified cognitive models</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>32</volume><elocation-id>01783</elocation-id><pub-id pub-id-type="doi">10.3758/s13423-020-01783-y</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Marley</surname> <given-names>AAJ</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Accumulating advantages: a new conceptualization of rapid multiple choice</article-title><source>Psychological Review</source><volume>127</volume><fpage>186</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1037/rev0000166</pub-id><pub-id pub-id-type="pmid">31580104</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Rothermund</surname> <given-names>K</given-names></name><name><surname>Voss</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Interpreting the parameters of the diffusion model: an empirical validation</article-title><source>Memory &amp; Cognition</source><volume>32</volume><fpage>1206</fpage><lpage>1220</lpage><pub-id pub-id-type="doi">10.3758/BF03196893</pub-id><pub-id pub-id-type="pmid">15813501</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Nagler</surname> <given-names>M</given-names></name><name><surname>Lerche</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Diffusion models in experimental psychology: a practical introduction</article-title><source>Experimental Psychology</source><volume>60</volume><fpage>385</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1027/1618-3169/a000218</pub-id><pub-id pub-id-type="pmid">23895923</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63055.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><role>Reviewing Editor</role><aff><institution>École normale supérieure, PSL University, INSERM</institution><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>de Gee</surname><given-names>Jan Willem</given-names> </name><role>Reviewer</role><aff><institution>Baylor College of Medicine</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Your responses to the reviewers' comments have convincingly addressed earlier concerns as to how the advantage racing model fares in reinforcement problems with more than two alternatives – a situation where its superiority over standard drift-diffusion models is clearest. The additional experiment described in the revised manuscript, and the formal derivation of the advantage racing model for reinforcement problems with more than two alternatives, constitute two important additions to the study which now provides a qualitative theoretical advance over the existing literature. The revised manuscript describes a convincing set of experiments and analyses that supports advantage racing as a good model of the interplay between learning and decision-making in reinforcement problems varying stimulus difficulty, speed-accuracy trade-off and stimulus-response mapping. Congratulations for an insightful study of human behavior.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A new model of decision processing in instrumental learning tasks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Joshua Gold as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Jan Willem de Gee (Reviewer #1).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the editors have judged that your manuscript is of interest, but as described below that additional experiments are required before it is published, we would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is &quot;in revision at <italic>eLife</italic>&quot;. Please let us know if you would like to pursue this option.</p><p>Summary:</p><p>This cognitive modeling study on a timely topic investigates the combination of reinforcement learning and decision-making for modeling choice and reaction-time data in sequential reinforcement problems (e.g., bandit tasks). The central claim of the paper is that the often-used combination of reinforcement learning with the drift-diffusion model (which decides based on the difference between option values) does not provide an adequate model of instrumental learning. Instead, the authors propose an &quot;advantage racing&quot; model which provides better fits to choice and reaction-time data in different variants of two-alternative forced-choice tasks. Furthermore, the authors emphasize that their advantage racing model allows for fitting decision problems with more than two alternatives – something which the standard drift-diffusion model cannot do. These findings can be of interest for researchers investigating learning and decision-making.</p><p>The study asks an important question for understanding the interaction between reinforcement learning and decision-making, the methods appear sound, and the manuscript is clearly written. The superiority of the advantage racing model is key to the novelty of the study, which otherwise relies on a canonical task studied in several recent papers on the same issue. However, the reviewers feel that the framing of the study and its conclusions would require additional analyses experiments to transform the manuscript from a modest quantitative improvement into a qualitative theoretical advance. In particular, as described in the paragraphs below, the authors should test how their advantage racing model fares in reinforcement problems with more than two alternatives. This is, from their own account throughout the paper, a situation where their model could show most clearly its superiority over standard drift-diffusion models used in the recent literature. The section below describes the essential revisions that the authors should address in a point-by-point fashion in a revised version of their manuscript.</p><p>Essential revisions:</p><p>1) Starting from the Abstract, the authors allude to reinforcement problems with more than 2 alternatives (1) and to the role of absolute choice values (2) in characterizing the limitations of the standard DDM. However, in the current manuscript, point 1 is not addressed (and actually does not appear to be amenable to the current model implementation, see the next point), and point 2 is addressed via modest quantitative improvements to model fits rather than true qualitative divergence between their model and other models' ability to capture specific behavioral effects.</p><p>The authors could greatly strengthen their conclusions if they extend their model to RL data sets with (1) more than two alternatives, and (2) variations in the absolute mean reward across blocks of learning trials. These experimental variants, which would put the superiority of the advantage racing model to a real test, would afford to test qualitative predictions possibly not shared by existing alternatives. For instance, does their model predict set size effects during instrumental learning? Does their model predict qualitative shifts in choice and reaction times when different task blocks have different mean rewards? At the moment the primary results rely solely on improved fits, but it would be important to show their model's unique ability to capture more salient qualitative behavioral effects. This point requires either the reanalysis of existing data from reinforcement problems with more than two alternatives and/or with block-wise variations in mean reward, or the collection of additional data. We understand the difficulty of collecting data in the current context, but this additional data is very important to support the qualitative superiority of the advantage racing model.</p><p>2) It is not clear how the advantage racing model would easily transfer to problems with more than two alternatives. As depicted in Equation 1, the model depends on the difference between two unique Q-values (weighted by <italic>w<sub>D</sub></italic>). How would this be implemented with more than two alternatives? There are several possibilities of implementations (e.g., the current Q-value relative to the top Q-value, the current Q-value minus the average Q-value, etc.), but they seem to require additional heuristics that are not described in the current implementation of the advantage racing model. The authors could also incorporate modulation of drift rates by policies, or use an actor-critic approach. In any case, the model in its current form cannot straightforwardly transfer to more than two alternatives. Given that the authors stress the benefits of their model for sequential reinforcement problems with more than two alternatives, they should provide an implementation of their model that can transfer to more than two alternatives (and also test this implementation against human data in such problems, see the previous point).</p><p>3) All three racing models (including the authors' preferred model) implement an urgency signal. It is unclear why the authors did not consider a similar mechanism within the DDM framework. Within this framework, urgency could possibly be implemented either as (linearly or hyperbolically) collapsing bounds, or as self-excitation (inverse of leak). Both implementations require only one extra parameter, as in the racing models used by the authors. The lack of explicit urgency needs to be addressed, because the current superiority of the advantage racing model could be due in part to its urgency component which is absent from the drift-diffusion model. And the importance of urgency in such problems has already been reported in earlier studies.</p><p>4) The authors appreciate the rigorous parameter recovery analyses in the supplement, but it would be very useful to perform also a model “separability” analysis – i.e., plot a confusion matrix between alternative models. Indeed, it seems like several of the tested models are relatively similar qualitatively speaking and it would thus be very useful to know how confusable they are. The fact that parameters from one known ground-truth model can be recovered accurately does not speak to the confusability between different models.</p><p>5) It is unclear that the authors are implementing SARSA (Rummery and Niranjan, 1994). Indeed, SARSA is: Q(s,a)[t+1] = Q(s,a)[t] + lr(r[t+1] + discount(Q(s,a)[t+1]) – Q(s,a)[t]. However, this task is a “single-step” problem. So it seems like the authors are rather implementing SAR – i.e., the standard Rescorla-Wagner “delta” rule. Please clarify this modeling aspect.</p><p>6) The basic aggregate behavior and model fitting quality should be described and illustrated beyond percentile distributions of reaction-times for correct and error trials. The authors should plot the grand-average RT distributions for subjects and best-fitting model predictions (pooled across subjects and trials). How variable were subjects' behavior? The reviewers understand that the model was fit hierarchically, but it would be nice (possibly in the supplement) to see a distribution of fit quality across subjects (1), to see RT distributions of a couple of good and bad fits (2), and to check whether the results hold after excluding the subjects with worst fits (if there are any outliers, 3). Related, in the RT percentile plots (Figures 3 and 4), it would be useful to see some measure of variability between subjects.</p><p>7) The authors write that RL-AEMs assume that &quot;[...] a subject gradually accumulates evidence for each choice option by sampling from a distribution of memory representations of the subjective value (or expected reward) associated with each choice option (known as Q-values).&quot; It is unclear what the authors mean by &quot;a distribution of memory representations&quot;. Either the authors could replace this by &quot;a running average of rewards&quot;, or they truly mean something like sampling from memory – like recent papers e.g., Bornstein et al., 2017, Nat. Comm. Sampling from (a distribution of) memory representations is a relatively new idea, and I think it would help if the authors would be more circumscribed in the interpretation of these results, and also provide more context and rationale both in the Introduction and Discussion. For example, an interesting Discussion paragraph would be on how such a memory-sampling process might actually be implemented in the brain.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.63055.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Starting from the Abstract, the authors allude to reinforcement problems with more than 2 alternatives (1) and to the role of absolute choice values (2) in characterizing the limitations of the standard DDM. However, in the current manuscript, point 1 is not addressed (and actually does not appear to be amenable to the current model implementation, see the next point), and point 2 is addressed via modest quantitative improvements to model fits rather than true qualitative divergence between their model and other models' ability to capture specific behavioral effects. The authors could greatly strengthen their conclusions if they extend their model to RL data sets with (1) more than two alternatives, and (2) variations in the absolute mean reward across blocks of learning trials. These experimental variants, which would put the superiority of the advantage racing model to a real test, would afford to test qualitative predictions possibly not shared by existing alternatives. For instance, does their model predict set size effects during instrumental learning? Does their model predict qualitative shifts in choice and reaction times when different task blocks have different mean rewards? At the moment the primary results rely solely on improved fits, but it would be important to show their model's unique ability to capture more salient qualitative behavioral effects. This point requires either the reanalysis of existing data from reinforcement problems with more than two alternatives and/or with block-wise variations in mean reward, or the collection of additional data. We understand the difficulty of collecting data in the current context, but this additional data is very important to support the qualitative superiority of the advantage racing model.</p></disp-quote><p>We followed the reviewers’ advice to perform an additional experiment. Specifically, we designed an instrumental learning task in which the participants had to repeatedly choose between three choice alternatives. The RL-DDM cannot be applied in settings beyond two-choice alternatives, providing a strong qualitative divergence between the models’ predictions and capabilities. Within this multi-alternative experiment, we included a reward magnitude manipulation to test whether the RL-ARD is able to capture the behavioral effects of changes in overall reward magnitude.</p><p>We describe the new experiment as well as the multi-alternative RL-ARD in the main text of the revised manuscript. For convenience, we copied the relevant text below:</p><p><bold>“</bold>Multi-alternative choice</p><p>Finally, we again drew on Van Ravenzwaaij et al.’s (Van Ravenzwaaij et al., 2020) advantage framework to extend the RL-ARD to multi-alternative choice tasks, a domain where the RL-DDM cannot be applied. […] As a consequence, <italic>both</italic> the Q-value sum (weighted by median <inline-formula><mml:math id="inf306"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:math></inline-formula>), and the smaller changes in the Q-value difference (weighted by median <inline-formula><mml:math id="inf307"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1.6</mml:mn></mml:mrow></mml:math></inline-formula>), increased the drift rates for the response accumulators (<inline-formula><mml:math id="inf308"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>; Figure 12D), which led to higher accuracy and faster responses.”</p><p>Furthermore, the Materials and methods section provides more detail on experiment 4 and on the implementation of the RL-ARD:</p><p><bold>“</bold>Experiment 4</p><p>Participants</p><p>43 participants (mean age 20 years old [SD 4.29 years], 5 men, 36 right-handed) were recruited from the subject pool of the Department of Psychology of the University of Amsterdam and participated for course credits. […] We also performed a parameter recovery study using the same methods as in earlier experiments, the results of which are shown in Figure 11—figure supplement 1.”</p><disp-quote content-type="editor-comment"><p>2) It is not clear how the advantage racing model would easily transfer to problems with more than two alternatives. As depicted in Equation 1, the model depends on the difference between two unique Q-values (weighted by w<sub>D</sub>). How would this be implemented with more than two alternatives? There are several possibilities of implementations (e.g., the current Q-value relative to the top Q-value, the current Q-value minus the average Q-value, etc.), but they seem to require additional heuristics that are not described in the current implementation of the advantage racing model. The authors could also incorporate modulation of drift rates by policies, or use an actor-critic approach. In any case, the model in its current form cannot straightforwardly transfer to more than two alternatives. Given that the authors stress the benefits of their model for sequential reinforcement problems with more than two alternatives, they should provide an implementation of their model that can transfer to more than two alternatives (and also test this implementation against human data in such problems, see the previous point).</p></disp-quote><p>We addressed the implementation of the multi-alternative RL-ARD in response to comment 1 above.</p><disp-quote content-type="editor-comment"><p>3) All three racing models (including the authors' preferred model) implement an urgency signal. It is unclear why the authors did not consider a similar mechanism within the DDM framework. Within this framework, urgency could possibly be implemented either as (linearly or hyperbolically) collapsing bounds, or as self-excitation (inverse of leak). Both implementations require only one extra parameter, as in the racing models used by the authors. The lack of explicit urgency needs to be addressed, because the current superiority of the advantage racing model could be due in part to its urgency component which is absent from the drift-diffusion model. And the importance of urgency in such problems has already been reported in earlier studies.</p></disp-quote><p>We agree with the reviewer that including an urgency mechanism in the RL-DDM could potentially improve the model’s quality of fit. However, there is no known analytical solution to the likelihood of the DDM with collapsing bounds or an urgency gain signal. Researchers interested in fitting the DDM with an urgency mechanism therefore must resort to simulation-based approximations of the likelihood function. In stationary experiments, the urgency-DDM must be simulated for <italic>N</italic> trials (commonly in the range of 10,000-50,000; e.g., Miletić et al., 2017; Trueblood et al., 2020) per condition per subject in each iteration of the MCMC sampling process. While computationally expensive, with good computational facilities this can still be feasible. However, the RL-DDM has single-trial drift rates, implying that the likelihood distribution needs to be approximated for each individual trial separately, hence requiring the simulation of <italic>N</italic> trials <italic>per trial</italic> per subject, providing a 208-fold increase in the number of trials that need to be simulated for experiment 1 alone (and more for the other experiments). The explosion in the number of simulations becomes computationally infeasible.</p><p>Secondly, it is important to point out that the origin of the concept of urgency lies in racing accumulator models, similar to the implementation we used in the RL-ARD. The development of DDMs with collapsing bounds and urgency gain signals was only later. As such, our model’s urgency mechanism remains close to the original concept.</p><p>In the revised manuscript, we address this point in the Discussion:</p><p>“Although an urgency mechanism such as collapsing bounds (Boehm et al., 2016; Bowman et al., 2012; Hawkins et al., 2015; Milosavljevic et al., 2010) or gain modulation (Boehm et al., 2016; Churchland et al., 2008; Ditterich, 2006; Hawkins et al., 2015) could potentially improve the fits of the RL-DDM, fitting DDMs with such mechanisms is computationally very expensive, usually requiring the researcher to use simulation-based approximations (e.g., Turner and Sederberg, 2014). […] Furthermore, the origin of the concept of urgency lies in studies using racing accumulator models (Ditterich, 2006; Mazurek et al., 2003; Reddi and Carpenter, 2000), which was only later incorporated in the DDM (Milosavljevic et al., 2010); the implementation in the RL-ARD remains conceptually close to the early proposals.”</p><disp-quote content-type="editor-comment"><p>4) The authors appreciate the rigorous parameter recovery analyses in the supplement, but it would be very useful to perform also a model “separability” analysis – i.e., plot a confusion matrix between alternative models. Indeed, it seems like several of the tested models are relatively similar qualitatively speaking and it would thus be very useful to know how confusable they are. The fact that parameters from one known ground-truth model can be recovered accurately does not speak to the confusability between different models.</p></disp-quote><p>In the revised manuscript, we included confusion matrices as Figure 3—figure supplement 3. Since our main focus is on the comparison between the RL-ARD and the RL-DDM (and the generation of the confusion matrices is computationally intensive), we only compared these two models. These results show that the BPIC nearly always correctly identified the data-generating models.</p><disp-quote content-type="editor-comment"><p>5) It is unclear that the authors are implementing SARSA (Rummery and Niranjan, 1994). Indeed, SARSA is: Q(s,a)[t+1] = Q(s,a)[t] + lr(r[t+1] + discount(Q(s,a)[t+1]) – Q(s,a)[t]. However, this task is a “single-step” problem. So it seems like the authors are rather implementing SAR – i.e., the standard Rescorla-Wagner “delta” rule. Please clarify this modeling aspect.</p></disp-quote><p>We apologize for this confusion, and removed all references to “SARSA”, which we substituted with the “delta rule”.</p><disp-quote content-type="editor-comment"><p>6) The basic aggregate behavior and model fitting quality should be described and illustrated beyond percentile distributions of reaction-times for correct and error trials. The authors should plot the grand-average RT distributions for subjects and best-fitting model predictions (pooled across subjects and trials). How variable were subjects' behavior? The reviewers understand that the model was fit hierarchically, but it would be nice (possibly in the supplement) to see a distribution of fit quality across subjects (1), to see RT distributions of a couple of good and bad fits (2), and to check whether the results hold after excluding the subjects with worst fits (if there are any outliers, 3). Related, in the RT percentile plots (Figures 3 and 4), it would be useful to see some measure of variability between subjects.</p></disp-quote><p>We included the following additional plots:</p><p>Figure 3—figure supplement 4, showing grand average RT distributions (both data and posterior predictive distributions), as well as the RT distributions for the first 10 subjects. From visual inspection of similar plots for all subjects, we found no subjects for which the model clearly misfit the empirical RT distributions; the quality of fit of the first 10 subjects was representative of the quality of fit of all other subjects. For experiments 2-4, similar plots are now provided as Figure 6—figure supplement 4, Figure 7—figure supplement 4, Figure 11—figure supplement 2, respectively.</p><p>Figure 4—figure supplement 4, which replicates Figure 4 but includes error bars (standard error) on points of empirical data to visualize between-subject variability. Since this plot combines two different approaches to the concept of uncertainty (a frequentist approach, where uncertainty is assumed to lie in the data, and a Bayesian approach, where the uncertainty is assumed to lie in the model), we prefer to keep this figure supplementary instead of replacing the main figure to retain consistency in our Bayesian approach in the main text. For experiments 2-4, similar plots are now provided as Figure 6—figure supplement 5, Figure 7—figure supplement 5, and Figure 11—figure supplement 3, respectively.</p><disp-quote content-type="editor-comment"><p>7) The authors write that RL-AEMs assume that &quot;[…] a subject gradually accumulates evidence for each choice option by sampling from a distribution of memory representations of the subjective value (or expected reward) associated with each choice option (known as Q-values).&quot; It is unclear what the authors mean by &quot;a distribution of memory representations&quot;. Either the authors could replace this by &quot;a running average of rewards&quot;, or they truly mean something like sampling from memory – like recent papers e.g., Bornstein et al., 2017, Nat. Comm. Sampling from (a distribution of) memory representations is a relatively new idea, and I think it would help if the authors would be more circumscribed in the interpretation of these results, and also provide more context and rationale both in the Introduction and Discussion. For example, an interesting Discussion paragraph would be on how such a memory-sampling process might actually be implemented in the brain.</p></disp-quote><p>We apologize for the confusion raised by our original phrasing. We intended that drift rates are sampled from a running average of rewards. We rephrased the sentence accordingly.</p></body></sub-article></article>