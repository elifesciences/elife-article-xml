<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">80547</article-id><article-id pub-id-type="doi">10.7554/eLife.80547</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Medicine</subject></subj-group></article-categories><title-group><article-title>Early stage NSCLS patients’ prognostic prediction with multi-information using transformer and graph neural network model</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-282124"><name><surname>Lian</surname><given-names>Jie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2351-2570</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-282125"><name><surname>Deng</surname><given-names>Jiajun</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6834-0322</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-282126"><name><surname>Hui</surname><given-names>Edward S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1761-0169</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-282130"><name><surname>Koohi-Moghadam</surname><given-names>Mohamad</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7286-0427</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-282128"><name><surname>She</surname><given-names>Yunlang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7673-9846</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-282129"><name><surname>Chen</surname><given-names>Chang</given-names></name><email>changchenc@tongji.edu.cn</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-281023"><name><surname>Vardhanabhuti</surname><given-names>Varut</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6677-3194</contrib-id><email>varv@hku.hk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02zhqgq86</institution-id><institution>Department of Diagnostic Radiology, Li Ka Shing Faculty of Medicine, The University of Hong Kong</institution></institution-wrap><addr-line><named-content content-type="city">Hong Kong</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03rc6as71</institution-id><institution>Department of Thoracic Surgery, Shanghai Pulmonary Hospital, Tongji University School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00t33hh48</institution-id><institution>Department of Imaging and Interventional Radiology, The Chinese University of Hong Kong</institution></institution-wrap><addr-line><named-content content-type="city">Hong Kong</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00t33hh48</institution-id><institution>Department of Psychiatry, The Chinese University of Hong Kong</institution></institution-wrap><addr-line><named-content content-type="city">Hong Kong</named-content></addr-line><country>China</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02zhqgq86</institution-id><institution>Division of Applied Oral Sciences and Community Dental Care, Faculty of Dentistry, The University of Hong Kong</institution></institution-wrap><addr-line><named-content content-type="city">Hong Kong</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Liu</surname><given-names>Caigang</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00v408z34</institution-id><institution>Shengjing Hospital of China Medical University</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Liu</surname><given-names>Caigang</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00v408z34</institution-id><institution>Shengjing Hospital of China Medical University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>04</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e80547</elocation-id><history><date date-type="received" iso-8601-date="2022-05-24"><day>24</day><month>05</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-09-21"><day>21</day><month>09</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-06-16"><day>16</day><month>06</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.06.14.22276385"/></event></pub-history><permissions><copyright-statement>© 2022, Lian, Deng et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Lian, Deng et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-80547-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-80547-figures-v1.pdf"/><abstract><sec id="abs1"><title>Background:</title><p>We proposed a population graph with Transformer-generated and clinical features for the purpose of predicting overall survival (OS) and recurrence-free survival (RFS) for patients with early stage non-small cell lung carcinomas and to compare this model with traditional models.</p></sec><sec id="abs2"><title>Methods:</title><p>The study included 1705 patients with lung cancer (stages I and II), and a public data set for external validation (n=127). We proposed a graph with edges representing non-imaging patient characteristics and nodes representing imaging tumour region characteristics generated by a pretrained Vision Transformer. The model was compared with a TNM model and a ResNet-Graph model. To evaluate the models' performance, the area under the receiver operator characteristic curve (ROC-AUC) was calculated for both OS and RFS prediction. The Kaplan–Meier method was used to generate prognostic and survival estimates for low- and high-risk groups, along with net reclassification improvement (NRI), integrated discrimination improvement (IDI), and decision curve analysis. An additional subanalysis was conducted to examine the relationship between clinical data and imaging features associated with risk prediction.</p></sec><sec id="abs3"><title>Results:</title><p>Our model achieved AUC values of 0.785 (95% confidence interval [CI]: 0.716–0.855) and 0.695 (95% CI: 0.603–0.787) on the testing and external data sets for OS prediction, and 0.726 (95% CI: 0.653–0.800) and 0.700 (95% CI: 0.615–0.785) for RFS prediction. Additional survival analyses indicated that our model outperformed the present TNM and ResNet-Graph models in terms of net benefit for survival prediction.</p></sec><sec id="abs4"><title>Conclusions:</title><p>Our Transformer-Graph model was effective at predicting survival in patients with early stage lung cancer, which was constructed using both imaging and non-imaging clinical features. Some high-risk patients were distinguishable by using a similarity score function defined by non-imaging characteristics such as age, gender, histology type, and tumour location, while Transformer-generated features demonstrated additional benefits for patients whose non-imaging characteristics were non-discriminatory for survival outcomes.</p></sec><sec id="abs5"><title>Funding:</title><p>The study was supported by the National Natural Science Foundation of China (91959126, 8210071009), and Science and Technology Commission of Shanghai Municipality (20XD1403000, 21YF1438200).</p></sec></abstract><kwd-group kwd-group-type="author-keywords"><kwd>lung cancer</kwd><kwd>medical imaging</kwd><kwd>transformer cnn</kwd><kwd>computed tomography</kwd><kwd>survival</kwd><kwd>prognostic model</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>91959126</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Chang</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>8210071009</award-id><principal-award-recipient><name><surname>She</surname><given-names>Yunlang</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003399</institution-id><institution>Science and Technology Commission of Shanghai Municipality</institution></institution-wrap></funding-source><award-id>20XD1403000</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Chang</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003399</institution-id><institution>Science and Technology Commission of Shanghai Municipality</institution></institution-wrap></funding-source><award-id>21YF1438200</award-id><principal-award-recipient><name><surname>She</surname><given-names>Yunlang</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>It provides evidence for the feasibility of using combined graph convolutional neural network and transformers’ based systems with patient demographics, and CT-derived imaging features as inputs into the transformer model for survival prediction in early stage lung cancer patients.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Lung cancer is expected to account for more than 1.80 million deaths worldwide in 2021, making it the top cause of cancer-related mortality (<xref ref-type="bibr" rid="bib26">Siegel et al., 2021</xref>). In early stage (stages I and II) non-small cell lung carcinomas (NSCLC), surgical resection remains the therapy of choice. However, almost 40–55% of these tumours recur following surgery (<xref ref-type="bibr" rid="bib1">Ambrogi et al., 2011</xref>). The clinical care of lung cancer patients would substantially benefit from accurate prognostic evaluation. Currently, TNM staging system of lung cancer based on the anatomic extent of disease is well recognised and widely adopted, which allows tumours of comparable anatomic extent to be grouped together (<xref ref-type="bibr" rid="bib14">Goldstraw et al., 2016</xref>). Staging guides treatment and provides a broad prediction of prognosis, however individual characteristics, histology, and/or therapy characteristics may impact survival results, as seen by variation within stage groups. In the refinement of the staging system, non-anatomical predictors such as gene mutations and biomarker profiles were proposed to be incorporated (<xref ref-type="bibr" rid="bib13">Giroux et al., 2018</xref>). However, the gene profiling approach relies on tissue sampling, and in addition, may not fully explain the intratumoural heterogeneity seen in NSCLC. Besides, such tests have barriers in deploying to routine oncology workflows due to high turnaround time, complexity, and cost (<xref ref-type="bibr" rid="bib22">Malone et al., 2020</xref>).</p><p>To predict the patient’s prognosis and to optimise individual clinical management, prognostic predictors such as TNM system and imaging-based high throughput quantitative biomarkers, radiomics, have been widely used to describe tumours (<xref ref-type="bibr" rid="bib11">Du et al., 2019</xref>; <xref ref-type="bibr" rid="bib2">Aonpong et al., 2020</xref>; <xref ref-type="bibr" rid="bib4">Bera et al., 2022</xref>; <xref ref-type="bibr" rid="bib6">Carmody et al., 1980</xref>; <xref ref-type="bibr" rid="bib9">Chirra et al., 2019</xref>; <xref ref-type="bibr" rid="bib23">Mirsadraee et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">van Griethuysen et al., 2017</xref>). Artificial intelligence (AI) methods, especially some deep learning (DL) models, have recently been regarded as potentially valuable tools (<xref ref-type="bibr" rid="bib8">Chilamkurthy et al., 2018</xref>; <xref ref-type="bibr" rid="bib24">Nabulsi et al., 2021</xref>; <xref ref-type="bibr" rid="bib30">Xu et al., 2019</xref>). DL models generated multiple quantitative assessments for tumour characteristics, which have the potential to describe tumour phenotypes with more predictive power than the clinical model (<xref ref-type="bibr" rid="bib30">Xu et al., 2019</xref>). While the anatomical structures in a medical image are functionally and mechanically related, most AI-based methods do not take these interdependencies and relationships into account. This leads to instability and poor generalisation of performance (<xref ref-type="bibr" rid="bib32">Zhou et al., 2021</xref>). With recent advancements in AI technology, several novel models have been proposed. Notably, the Transformer (<xref ref-type="bibr" rid="bib28">Vaswani et al., 2017</xref>) model permits exceptional capabilities in natural language processing fields such as language translation and was later applied to the computer vision field and outperformed all state-of-the-art models given large amounts of training data (<xref ref-type="bibr" rid="bib10">Dosovitskiy et al., 2020</xref>). This provides an intuitive reason to apply the Transformer model to the medical image to generate additional meaning for tumour features, as images were processed in sequence with inherent interdependencies (<xref ref-type="bibr" rid="bib33">Zhou et al., 2022</xref>).</p><p>The majority of current prognostic prediction methods have focused mainly either specific to their own domains, such as focusing solely on imaging data, whereas in clinical practice non-imaging clinical data such as sex, age, and disease history all play critical roles in disease prognosis prediction (<xref ref-type="bibr" rid="bib16">Holzinger et al., 2019</xref>). Although some researchers have used multi-modal techniques (<xref ref-type="bibr" rid="bib31">Xue et al., 2018</xref>) to combine that information, it is not easy to explain how the various types of data interacted and how they contributed to the final prediction. Due to their lack of explanatory power, those models may not be easily applied in clinical practice (<xref ref-type="bibr" rid="bib21">London, 2019</xref>). Another type of neural network, called a graph neural network (GNN) (<xref ref-type="bibr" rid="bib19">Kipf and Welling, 2016</xref>), which deals with data that has a graph structure, enables researchers to create more flexible ways to embed various types of data. For example, nodes and edges in a graph might represent a variety of different types of data (imaging and clinical demographics information), and analysing these entities reveals the role of various data sources.</p><p>In this study, we proposed a GNN-based model that leverages imaging and non-imaging data for the prediction of the survival of patients with early stage NSCLC. Patients were represented as a population graph, whereby each patient corresponded to a graph node and was associated with a tumour feature vector that was learnt from the Transformer model, and graph edge weights between patients were derived from a similarity score that was derived from phenotypic data, such as demographics, tumour location, cancer type, and TNM staging. This population graph was used to train a GraphSAGE (<xref ref-type="bibr" rid="bib15">Hamilton et al., 2017</xref>) model for classifying individual patient’s risk of overall survival (OS) and recurrence. Additionally, we attempted to determine the relative importance of imaging and non-imaging features within this model. The proposed model was trained and tested on a large data set, followed by external validation using a publicly available data set.</p></sec><sec id="s2" sec-type="methods"><title>Methods</title><sec id="s2-1"><title>Participants</title><p>The study included consecutive patients who received surgery for early stage NSCLC between January 2011 and December 2013 who matched the criteria. Inclusion criteria included: (1) pathologically proven stage I or stage II NSCLC; (2) preoperative thin-section CT image data; and (3) complete follow-up survival data. Patients undergoing neoadjuvant therapy were excluded from the study. This retrospective study protocol was approved by the Shanghai Pulmonary Hospital’s Institutional Review Board (ref: L21-022-2) and informed consent was waived owing to retrospective nature. Additionally, patients who met our criteria were retrieved from the NSCLC Radiogenomics (<xref ref-type="bibr" rid="bib3">Bakr et al., 2018</xref>) data set as an external validation set (see <xref ref-type="fig" rid="fig1">Figure 1</xref> for the internal and external inclusion criteria flowchart).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overall flow of the study in both internal and external data set.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80547-fig1-v1.tif"/></fig><p>We only used patients’ initial CT scans in this study. For the main cohort, all CT scans were acquired using Somatom Definition AS+ (Siemens Medical Systems, Germany) and iCT256 (Siemens Medical Systems, Germany; Philips Medical Systems, Netherlands). All image data were rebuilt using a 1-mm slice thickness and a 512×512 mm<sup>2</sup> matrix. Intravenous contrast was administered in accordance with institutional clinical practice. Clinical data in this study were manually collected from medical records and were anonymised. Outpatient records and telephone interviews were used to collect follow-up data. The period between the date of surgery and the date of death or the final follow-up was defined as OS. Recurrence-free survival (RFS) was calculated from the date of surgery to the date of recurrence, death, or last follow-up. (More details about internal scan parameters and follow-up strategies can be found in Appendix 1).</p></sec><sec id="s2-2"><title>Image annotation and pre-processing</title><p>Patients’ tumour region was manually labelled by experienced radiologists using 3D Slicer (<xref ref-type="bibr" rid="bib12">Fedorov et al., 2012</xref>), with a centre seed point defining a bounding box. The regions of interest (ROIs) were first annotated by two junior thoracic surgeons (Y.S. and J.D. with 5 and 3 years of experience, respectively), then the consensus on ROI was obtained by a discussion with a senior radiologist (with more than 25 years of experience).</p><p>For image pre-processing, we first normalised all CT images and removed the surrounding noises such as bones by manual thresholding. The size of all tumour segments was fixed to 128×128×64 mm<sup>3</sup>. Small tumours were zero-padded. To reduce the computational cost, we resized the padded segments into 64×64×36 mm<sup>3</sup> and subsequently resized them as 2D square images (each row contained six tumour slices) with the size of 384×384 mm<sup>2</sup> as shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Tumour image processing and feature generation.</title><p>(<bold>A</bold>) Tumour images normalisation, reshaping, and padding to standard sizes, then re-arranged into 2D images. (<bold>B</bold>) Generating 1D Transformer survival features from pretrained Transformer model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80547-fig2-v1.tif"/></fig></sec><sec id="s2-3"><title>Tumour Transformer feature generator</title><p>When pretrained on a large data set and transferred to image recognition benchmarks, it has been shown that Vision Transformer (ViT) can achieve excellent results while requiring significantly less computational resources to train than state-of-the-art convolutional models (<xref ref-type="bibr" rid="bib10">Dosovitskiy et al., 2020</xref>). To this end, we reasoned that by replacing the traditional CNN feature generator architecture with a Transformer structure could be an approach to produce meaningful survival-relevant features. In this study, we used a ViT pretrained on a large-scale data set (ImageNet-21k; <xref ref-type="bibr" rid="bib25">Ridnik et al., 2021</xref>) as the feature generator, which takes 2D tumour segments as inputs. To meet the standard requirements of the sequence model, the input images were divided into 36 ordered patches and position embedding in the first step, followed by a linear projection function before entering the Transformer Encoder. We replaced the original classification layer with a fully connected layer to generate a 1D feature vector. The detailed implementation is illustrated in <xref ref-type="fig" rid="fig2">Figure 2B</xref>. The 1D feature vector was then assigned as the node feature for the individual patient in the graph network.</p></sec><sec id="s2-4"><title>Patient survival graph network</title><p>A population graph method was used to leverage imaging and non-imaging data. Each patient was regarded as a node in a graph and its edge with neighbour was derived from a similarity score which was determined by the product between four component scores, namely demographics (gender and age), tumour location, cancer type (histology), and TNM staging (for more detail, refer to the Appendix 1 for a detailed explanation of similarity scores). Two patients would be connected to each other if they shared similar component scores. The features of an individual patient (node feature) were obtained from the Transformer Encoder trained on the tumour images mentioned above.</p></sec><sec id="s2-5"><title>Graph-based neural network structure</title><p>We applied a graph-based deep neural network structure called GraphSAGE in this study. The proposed network took the whole population graph, along with the edge and node features as the input and generated a risk score in the last layer for each patient node as the output (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). Within the network, every node feature was updated by an aggregation of information from its neighbours and itself, while the importance of different neighbours varied by the corresponding edges’ weight.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Population graph building and model prediction pipeline.</title><p>(<bold>A</bold>) Each patient was regarded as a node and the Transformer-generated feature was regarded as node features. (<bold>B</bold>) Graph edges and the relevant weights were defined by their similarity scores. (<bold>C</bold>) We then put the whole population graph to train the GraphSAGE network in order to make a prediction for each patient (pink indicates high risk and blue indicates low risk). (<bold>D</bold>) Node updating inside the GraphSAGE network.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80547-fig3-v1.tif"/></fig><p>We applied a two-layer GraphSAGE and global meaning pooling structure, aiming to allow each patient’s information to be updated, first from its second neighbours and then its neighbours and itself consequentially. In order to emphasise the target of survival prediction, we specifically replaced the cross-entropy loss with Cox proportional hazards loss function (<xref ref-type="bibr" rid="bib17">Katzman et al., 2018</xref>) which both considered the survival time and events when training the network. The proposed network was implemented in Python, using the Deep Graph Library with Pytorch backend.</p></sec><sec id="s2-6"><title>Statistics analysis</title><p>All patients from the main data set were randomly separated into training, validation, and testing sets with the proportion of 75%, 12.5%, and 12.5% separately. We also tested the model on the external validation data set. The proposed model was compared with the TNM staging system which was generally used in clinical practice and a ResNet-Graph model which has the same graph structure (using clinical features to define the similarity score) as our proposed model while the node feature was generated by a pretrained ResNet-18 model (<xref ref-type="bibr" rid="bib18">Khanna et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Chen et al., 2019</xref>) (using imaging features).</p><p>To evaluate whether there were statistically significant variations in survival between positive and negative groups, the area under the receiver operator characteristic curve (AUC) was determined for OS and RFS prediction to compare the models' performance. The Kaplan–Meier (KM) method was used to generate prognostic and survival estimates for groups with low and high risk (both for OS and RFS), which were stratified according to the training set’s median prediction probability, with the log-rank test employed to establish statistical significance. To quantify the net benefits of survival prediction, we quantified the net reclassification improvement (NRI) and integrated discrimination improvement (IDI), as well as performed a decision curve analysis (DCA). All of the analyses above were performed in Python using the Lifelines package.</p><p>An additional subanalysis was performed on the test data set to explore the relationship between patients’ clinical information and imaging features contributing to risk prediction. We generated a sub-graph visualisation using PyVis and a KM analysis was used for several sub-graphs to evaluate our model’s ability to separate high-risk patients. Finally, as a proof of concept, we plotted two patients’ nodes feature changes before and after one layer processing using a correlation heatmap, along with its neighbours’ edge weights analysis to try to understand the inner workings of our model.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><sec id="s3-1"><title>Data description</title><p>In the main cohort, we initially enrolled 2309 patients and after exclusion based on our criteria, a total of 1705 NSCLC patients were included in the study. The median age was 61 (interquartile range, 55–66 years). There were 1010 males (59.2%) and 695 women (40.8%). Tumours were more frequently located in the upper lobes (1018, 59.7%). A total of 1235 patients (72.4%) had adenocarcinoma, while 391 patients (22.9%) had squamous cell carcinoma. The distribution of pathologic stages was as follows: stage IA was present in 791 patients (46.4%), stage IB was present in 607 patients (35.6%), stage IIA was present in 133 patients (7.8%), and stage IIB was present in 174 patients (10.2%). The OS and RFS rates were 78·2% (95% confidence interval [CI]: 76.2–80.2%) and 74.2% (70.8–77.6%), respectively. The external validation data set included a total of 127 patients of which 32 (25.2%) were females and 95 (74.8%) males, with a median age of 69 (interquartile range, 46–87 years). Upper lobe tumours were also more prevalent (76 patients, 59.8%). Among them 95 patients were diagnosed with adenocarcinoma and 30 with squamous cell carcinoma. The OS and RFS rates were 68.5% (95% CI: 60.4–77.7 %) and 59·1% (95% CI: 50.4–67.8 %), respectively. Please refer to <xref ref-type="table" rid="table1">Table 1</xref> for more detailed information.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Feature distribution in the total patient cohorts, training and validation cohorts and the test cohorts.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" colspan="2"/><th align="left" valign="top">TRAIN and VAL(n=1492)</th><th align="left" valign="top">TEST(n=213)</th><th align="left" valign="top"/><th align="left" valign="top">EXTERNAL(n=127)</th><th align="left" valign="top"/></tr></thead><tbody><tr><td align="left" valign="top">Feature</td><td align="left" valign="top">Content</td><td align="left" valign="top" colspan="2"><bold>Mean, SD, 95%</bold> CI/<bold>Count, %</bold></td><td align="left" valign="top">p</td><td align="left" valign="top">Mean, SD, 95% CI/Count, %</td><td align="left" valign="top">p</td></tr><tr><td align="left" valign="top">Age</td><td align="left" valign="top">Age</td><td align="left" valign="top">60.6, 8.7, (CI: 60.1, 61.0)</td><td align="left" valign="top">60.7, 9.5, (CI: 59.4, 62.0)</td><td align="left" valign="top">&gt;0.05</td><td align="left" valign="top">68.7, 9.1, (CI: 67.2, 70.1)</td><td align="left" valign="top">&lt;0.01**</td></tr><tr><td align="left" valign="top">Sex</td><td align="left" valign="top">Female no. (%);<break/>Male no. (%)</td><td align="left" valign="top">602 (33.3); 890 (66.7)</td><td align="left" valign="top">93 (33.3); 120 (66.7)</td><td align="left" valign="top">&gt;0.05</td><td align="left" valign="top">32 (25.2); 95 (74.8)</td><td align="left" valign="top">&lt;0.01**</td></tr><tr><td align="left" valign="top">Resection</td><td align="left" valign="top">Sublobar resection no. (%);<break/>Lobectomy no. (%);<break/>Bilobectomy no. (%); Pneumonectomy no. (%)</td><td align="left" valign="top">123 (8.2);<break/>1292 (86.6);<break/>59 (3.95);<break/>18 (1.2)</td><td align="left" valign="top">23 (10.8);<break/>180 (84.5);<break/>7 (3.3);<break/>3 (1.4)</td><td align="left" valign="top">&gt;0.05</td><td align="char" char="." valign="top">/</td><td align="char" char="." valign="top">/</td></tr><tr><td align="left" valign="top">Histology</td><td align="left" valign="top">Adenocarcinoma no. (%);<break/>Squamous Cell Carcinoma no. (%);<break/>Others no. (%)</td><td align="left" valign="top">1072 (71.4);<break/>351 (23.5);<break/>69 (4.6)</td><td align="left" valign="top">163 (76.5);<break/>40 (18.8);<break/>10 (4.7)</td><td align="left" valign="top">&gt;0.05</td><td align="left" valign="top">95 (74.8);<break/>30 (23.6);<break/>2 (1.6)</td><td align="left" valign="top">&gt;0.05</td></tr><tr><td align="left" valign="top">Tumour<break/>location</td><td align="left" valign="top">LUL no. (%);<break/>LLL no. (%);<break/>RUL no. (%);<break/>RML no. (%);<break/>RLL no. (%)</td><td align="left" valign="top">384 (25.7);<break/>211 (14.1);<break/>504 (33.8);<break/>146 (9.8);<break/>247 (16.6)</td><td align="left" valign="top">51 (23.9);<break/>37 (17.4);<break/>79 (37.1);<break/>15 (7.0)<break/>31 (14.6)</td><td align="left" valign="top">&gt;0.05</td><td align="left" valign="top">30 (23.6);<break/>22 (17.3);<break/>46 (36.2);<break/>15 (11.8);<break/>14 (11.0).</td><td align="left" valign="top">&gt;0.05</td></tr><tr><td align="left" valign="top">Tumour size</td><td align="left" valign="top">Tumour size</td><td align="left" valign="top">2.68, 1.38,<break/>(CI: 2.61, 2.75)</td><td align="left" valign="top">2.55, 1.25,<break/>(CI: 2.38, 2.71)</td><td align="left" valign="top">&gt;0.05</td><td align="char" char="." valign="top">/</td><td align="char" char="." valign="top">/</td></tr><tr><td align="left" valign="top">pTNM stage</td><td align="left" valign="top">Stage I no. (%);<break/>Stage II no. (%);</td><td align="left" valign="top">1219 (81.7);<break/>273 (18.3)</td><td align="left" valign="top">179 (84.0);<break/>34 (16.0)</td><td align="left" valign="top">&gt;0.05</td><td align="left" valign="top">97 (76.3);<break/>30 (23.7)</td><td align="left" valign="top">&lt;0.01**</td></tr><tr><td align="left" valign="top">RFS status</td><td align="left" valign="top">RFS no. (%)</td><td align="left" valign="top">1089 (73.0)</td><td align="left" valign="top">154 (72.3)</td><td align="left" valign="top">&gt;0.05</td><td align="left" valign="top">75 (59.1)</td><td align="left" valign="top">&gt;0.05</td></tr><tr><td align="left" valign="top">RFS month</td><td align="left" valign="top">RFS month</td><td align="left" valign="top">57.5, 24.5,<break/>(CI: 56.2, 58.7)</td><td align="left" valign="top">58.4, 23.4,<break/>(CI: 55.2, 61.5)</td><td align="left" valign="top">&gt;0.05</td><td align="left" valign="top">39.5, 26.9,<break/>(CI: 34.8, 44.2)</td><td align="left" valign="top">&lt;0.01**</td></tr><tr><td align="left" valign="top">OS status</td><td align="left" valign="top">OS no. (survival %)</td><td align="left" valign="top">1166 (78.2)</td><td align="left" valign="top">167 (78.4)</td><td align="left" valign="top">&gt;0.05</td><td align="left" valign="top">87 (68.5)</td><td align="left" valign="top">&gt;0.05</td></tr><tr><td align="left" valign="top">OS month</td><td align="left" valign="top">OS month</td><td align="left" valign="top">62.4, 19.9,<break/>(CI: 61.4, 63.4)</td><td align="left" valign="top">63.4, 18.4,<break/>(CI: 60.9, 65.9)</td><td align="left" valign="top">&gt;0.05</td><td align="left" valign="top">44.8, 27.8,<break/>(CI: 40.9, 50.0)</td><td align="left" valign="top">&lt;0.01**</td></tr></tbody></table></table-wrap></sec><sec id="s3-2"><title>Model performance</title><p>To develop deep transformer graph learning-based biomarkers for OS prediction, we trained on the main cohorts, separated into training and validation data sets and then evaluated them separately on the testing set (213 patients) and the external set (127 patients). For OS prediction, our model achieved AUC values of 0.785 (95% CI: 0.716–0.855) and 0.695 (95% CI: 0.603–0.787) on the testing and external data sets, respectively, compared to 0.690 (95% CI: 0.600–0.780) and 0.634 (95% CI: 0.544–0.724) for the TNM model, and 0.730 (95% CI: 0.640–0.820) and 0.626 (95% CI: 0.530–0.722) for ResNet-Graph model. For RFS prediction, our model achieved AUC values of 0.726 (95% CI: 0.653–0.800) and 0.700 (95% CI: 0.615–0.785) on the testing and external data sets, respectively, compared to 0.628 (95% CI: 0.542–0.713) and 0.650 (95% CI: 0.561–0.732) for the TNM model, and 0.681 (95% CI: 0.598–0.764) and 0.595 (95% CI: 0.615–0.785) for ResNet-Graph model (<xref ref-type="fig" rid="fig4">Figure 4A and B</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model performance: (<bold>A</bold>) ROC-AUC curve on test data and external set for OS and (<bold>B</bold>) RFS prediction and (<bold>C</bold>) KM curve on test data set for OS and (<bold>D</bold>) RFS prediction.</title><p>(<bold>E</bold>) Decision curve on test data set for OS and RFS prediction. KM, Kaplan–Meier; OS, overall survival; RFS, recurrence-free survival; ROC-AUC, area under the receiver operator characteristic curve.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80547-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Kaplan-Meier survival analysis.</title><p>(<bold>A</bold>) KM curve on external data set for OS and (<bold>B</bold>) RFS prediction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80547-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Additional survival analyses were performed using KM estimates for groups with low and high risk of mortality and recurrence, respectively, based on the median stratification of patient prediction scores (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>). All three models showed statistically significant differences in 5-year OS. For RFS prediction, the ResNet-Graph model was unable to distinguish between individuals at low and high risk (p&gt;0.05), while both Transformer-Graph and TNM models were able to separate high and low risk of RFS groups (p&lt;0.05). The KM plots for the external set were reported in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p><p>Additionally, the DCA (<xref ref-type="fig" rid="fig4">Figure 4E</xref>) and net benefit analysis (IDI, NRI) indicated that the Transformer-Graph model significantly outperformed the present TNM and ResNet-Graph models in terms of net benefit for both OS and RFS survival prediction. As for detailed net benefit analysis, Transformer-Graph model outperformed the present TNM and ResNet-Graph models in terms of IDI and NRI. Our proposed model improved the survival prediction significantly compared with TNM regarding NRI (OS: 0.284, 95% CI: –0.112 to 0.519, p&lt;0.0001; RFS: 0.175, 95% CI: –0.115 to 0.486, p&lt;0.0001) and IDI (OS: 0.159, 95% CI: 0.103–0.214, p=0.00032; RFS: 0.137, 95% CI: 0.086–0.189, p=0.00074). The results comparing with ResNet-Graph were reported in Appendix 1.</p></sec><sec id="s3-3"><title>Patients’ clinical-based graph analysis</title><p>We visualised the whole internal set (<xref ref-type="fig" rid="fig5">Figure 5A</xref>) along with the testing cohort’s subplot (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) and analysed two challenging cases to better understand the population-based graph structure and how clinical data was integrated with node attributes (i.e., patients' tumour images). The testing subplot showed that while the graph structure (specified by the similarity score) was capable of broadly separating at-risk patients, several clusters had both high- and low-risk patients intermingled together, making them difficult to separate using traditional clinical information (see <xref ref-type="fig" rid="fig5">Figures 5C</xref> and <xref ref-type="fig" rid="fig4">4D</xref>). The subsequent KM analysis indicated that by using Transformer-generated tumour attributes, high- and low-risk patients could be significantly discriminated.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Testing set graph analysis.</title><p>(<bold>A</bold>) A visual representation of the whole cohort population graph of 1705 patients. (<bold>B</bold>) A visual representation of the testing sub-graph of 213 patients. (<bold>C</bold>) and (<bold>D</bold>) two sub-graphs containing challenging cases where the graphs contained both high- and low-risk patients. (<bold>E</bold>) Node features’ correlation heatmaps and edge weights distribution of patient No. 44: Each square represents a neighbour’s node features’ correlation coefficient, higher values (red colour) reveal closer relation with the target node; the box plot of 42 neighbours indicates that the high-risk neighbours (blue box) have higher edge weights median. (<bold>F</bold>) Node features’ correlation heatmaps and edge weights distribution of patient No. 182: The box plot of 25 neighbours indicates that the low-risk neighbours (orange box) have higher edge weights median.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-80547-fig5-v1.tif"/></fig><p>Additionally, we analysed specifically as an example, patient No. 44 (high-risk node), and surrounding neighbours’ edge weights distribution, as well as the initial and subsequent one layer node features. This patient was a high-risk patient who died after 38 months, with 42 neighbours. Initially, we analysed the correlation coefficient between neighbours' node features in order to determine the role that Transformer-generated image features played prior to graph training. As illustrated in <xref ref-type="fig" rid="fig5">Figure 5E</xref>, the correlation matrix of Transformer-generated features revealed that almost all of patient No. 44’s high-risk (dashed box nodes) and low-risk neighbours were highly correlated, implying that image features did not contain directly discriminative survival information before learning. We next then examined the distribution of neighbours' edge weights. As illustrated in <xref ref-type="fig" rid="fig5">Figure 5E</xref>, despite the fact that there were only five high-risk neighbours, the median value of similarity scores was slightly higher than that of low-risk neighbours (2.50 vs. 2.00), indicating that the high-risk neighbour group was more closely connected to the target nodes from non-imaging information aspects. After one layer of GraphSAGE updating, we discovered that the high-risk neighbours were more correlated with patient No. 44 (see <xref ref-type="fig" rid="fig5">Figure 5E</xref> GraphSAGE Layer 1, nodes in the dash boxes showed higher coefficient values), revealing that within our model, both neighbours’ nodes and edge features contained survival-related information, and they contributed together to efficiently provide information for the target node learning.</p><p>Then, we analysed the correlation between patient No. 182 (a patient with low risk) and its 25 neighbours (see <xref ref-type="fig" rid="fig5">Figure 5F</xref>). The majority of patient No. 182’s neighbours were low-risk patients (22 out of 25), and the median value of their similarity scores was significantly higher than that of their high-risk neighbours (3.00 vs. 1.00). With the help of edge weights (clinical data) updated on node features, high-risk neighbours could be separated after only one layer (see <xref ref-type="fig" rid="fig5">Figure 5</xref>, three nodes in GraphSAGE Layer 1’s dashed boxes had lower coefficient values).</p></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>The patient’s prognosis is relevant for both treatment estimation and future treatment planning. In practice, clinical information and some hand-crafted medical imaging features were used to predict outcomes (<xref ref-type="bibr" rid="bib11">Du et al., 2019</xref>). With the advent of AI technologies, methods that incorporate deep-learning-based features have been developed, generating more medical imaging-related features from a variety of perspectives (<xref ref-type="bibr" rid="bib30">Xu et al., 2019</xref>), resulting in improving the prediction performance. In previous studies (<xref ref-type="bibr" rid="bib2">Aonpong et al., 2020</xref>; <xref ref-type="bibr" rid="bib24">Nabulsi et al., 2021</xref>; <xref ref-type="bibr" rid="bib30">Xu et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Wang et al., 2018</xref>), convolutional models such as ResNet were commonly used, whereas ViT, which outperformed novel convolutional-based DL models in computer vision tasks on nature image data sets, could generate medical imaging features in a different manner. Besides, the design of the combination of imaging and non-imaging data is always a challenge. In the past, linear models were commonly used (<xref ref-type="bibr" rid="bib20">Liao et al., 2019</xref>), treating imaging and non-imaging equally, which may have resulted in inefficient information utilization. The proposal of multimodal data integration allows for effective information fusion over different modalities (<xref ref-type="bibr" rid="bib5">Brown et al., 2018</xref>), while the relation between modalities is not well-explained.</p><p>In this project, we demonstrated the feasibility of using ViT on CT images of lung tumours to generate features for cancer survival analysis. Additionally, we used a graph structure to embed patients' imaging and non-imaging clinical data separately in the GNN and attempted to explain how clinical data communicates with Transformer-generated imaging features for survival analysis. While Transformer and GNN models have been widely used in computer vision, their application in the medical field, particularly for survival prediction, is still evolving due to the complexity and unbalanced nature of medical data (high dimension, multiple data formats, including non-imaging data). In our study, we combined these two methods and created a specially designed graph structure to handle a variety of data formats, demonstrating the utility of Transformer-generated features in survival analysis and emphasising the extent to which clinical data and imaging features contribute to the prediction. To our knowledge, this is the first work to demonstrate the feasibility of using Transformer in survival prediction using a graph data structure and exploratory analysis of the models’ intuitions in an attempt to explain these state-of-the-art methods.</p><p>Our experiments indicated that the proposed model outperformed the commonly used TNM model in predicting survival not only on the testing data set but also on the external dataset, despite the fact that the data distributions were significantly different (refer to <xref ref-type="table" rid="table1">Table 1</xref>, the survival distribution on the external data set is significantly different from the internal data set), demonstrating the model’s generalisability for unseen data. The model also outperformed the generally regarded current state-of-the-art model, the Res-Net model which in our study incorporated both imaging and non-imaging data when we performed survival analyses based on KM estimates. The model’s good performance indicated that both the Transformer-generated imaging features and the structure of our population graph (i.e., using graph edges and nodes to combine non-imaging clinical data and imaging data) contained useful information for survival. Additionally, the subplot graph on the testing data set (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) indicated that our graph structure was capable of approximate clustering high- and low-risk groups and segregating the majority of the high-risk patients. Meanwhile, when patients were similar in terms of demographic information and it was hard to determine the risk patients by traditional clinical methods (refer to <xref ref-type="fig" rid="fig5">Figure 5C and D</xref> the dense graphs containing both pink and blue nodes), the Transformer-generated image features and edge weights had more roles to play in determining the differences between neighbours. More specifically, the Transformer-generated features did not contain directly discriminative survival information before learning, while with edge weights together, effective information from neighbours’ node features could be determined. In this case, all patient node features could be effectively updated, and high-risk patients could be better discriminated as in <xref ref-type="fig" rid="fig5">Figure 5E</xref>.</p><p>Our study contains several strengths. First, our data set is relatively large, encompassing both contrast and non-contrast CT scans. This not only aided in the model’s generalisation learning but also allows for flexibility in the imaging standards in clinical settings. Second, our graph model demonstrated the ability to combine non-imaging clinical features with imaging features in an understandable manner, implying a new direction of embedding multi-data with DL models. Finally, we sought to understand the roles of imaging and non-imaging features in determining high-risk nodes within the GNN, which could aid clinicians in comprehending the internal workings of the neural networks.</p><p>There are some limitations worth noting. First, whilst the proposed model significantly outperformed the TNM model on the external data set (OS prediction AUC 0.693 vs. 0.633, RFS prediction RFS 0.700 vs. 0.650), the model’s performance on the external set was below that of the testing set (AUC 0.783 and 0.726 for OS and RFS). One reason could be that the patients' demographics were different, particularly in terms of age (the external group’s average age was 10 years older than the main cohort), cancer staging (84.0% stage I in the main cohort while 76.3% in the external testing set), and gender (male percentage 66.7% vs. 74.8%). The fact that the two data sets originate from distinct countries, as well as the differences in ethnicity, treatment, and follow-up strategies (see <xref ref-type="table" rid="table1">Table 1</xref>, especially the mean follow-up time) may also have an impact on the prediction performance (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> for ethnicity and smoking information of external set). Second, smoking history played important role in the development of lung cancer, which may help to improve the model’s performance and reduce the performance difference between the testing set and the external set, despite the fact that relevant information was not collected in the main set. Besides, we only found one applicable public external in this project, whereas more external data can improve the convince of our model’s generalization ability. (We tried to search on the Cancer Imaging Archive, and only 2 of 40 lung cancer data sets meet our requirements. We used the first as our current external set; the second has a death rate of 95.88% for early stage lung cancer patients with no explanation, so we did not include it.) Finally, the initial step requires the human observer to identify the tumour and draw a bounding box which in our study was still a manual procedure. As the pipeline for automatic tumour detection and segmentation becomes more mature, this step can potentially be automated allowing for ease of translation into the clinics.</p><p>In conclusion, the population graph DL model constructed using Transformer-generated imaging and non-imaging clinical features was proven to be effective at predicting survival in patients with early stage lung cancer. The subanalysis concluded that by developing a meaningful similarity score function and comparing patients' non-imaging characteristics such as age, gender, histology type, and tumour location, the majority of high-risk patients can already be separated. Additionally, when high- and low-risk patients shared very similar demographic information, TNM information provided additional information for survival prediction when combined with tumour imaging features.</p></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Software, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Resources, Data curation, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Data curation, Supervision, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Data curation, Formal analysis, Supervision, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: This retrospective study protocol was approved by the Shanghai Pulmonary Hospital's Institutional Review Board (ref: L21-022-2) and informed consent was waived owing to retrospective nature.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Ethnicity and Smoking information table of the external dataset.</title></caption><media xlink:href="elife-80547-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-80547-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. To aid reproducibility of research, our codes are published on the Github repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/SereneLian/TransGNN-Lung">https://github.com/SereneLian/TransGNN-Lung</ext-link> Copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:52c6dd4c3b76d2345a4a2224360989a842820cd4;origin=https://github.com/SereneLian/TransGNN-Lung;visit=swh:1:snp:d8d58241f13fafc549049c4276837eefe4e77aab;anchor=swh:1:rev:a8a47fa9f47040b83f42ada9dfd053d47281ae74">swh:1:rev:a8a47fa9f47040b83f42ada9dfd053d47281ae74</ext-link>.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Bakr</surname><given-names>S</given-names></name><name><surname>Gevaert</surname><given-names>O</given-names></name><name><surname>Echegaray</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>NSCLC Radiogenomics</data-title><source>TCIA</source><pub-id pub-id-type="accession" xlink:href="https://wiki.cancerimagingarchive.net/display/Public/NSCLC+Radiogenomics">Radiogenomics</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ambrogi</surname><given-names>MC</given-names></name><name><surname>Fanucchi</surname><given-names>O</given-names></name><name><surname>Cioni</surname><given-names>R</given-names></name><name><surname>Dini</surname><given-names>P</given-names></name><name><surname>De Liperi</surname><given-names>A</given-names></name><name><surname>Cappelli</surname><given-names>C</given-names></name><name><surname>Davini</surname><given-names>F</given-names></name><name><surname>Bartolozzi</surname><given-names>C</given-names></name><name><surname>Mussi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Long-Term results of radiofrequency ablation treatment of stage I non-small cell lung cancer: a prospective intention-to-treat study</article-title><source>Journal of Thoracic Oncology</source><volume>6</volume><fpage>2044</fpage><lpage>2051</lpage><pub-id pub-id-type="doi">10.1097/JTO.0b013e31822d538d</pub-id><pub-id pub-id-type="pmid">22052222</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aonpong</surname><given-names>P</given-names></name><name><surname>Iwamoto</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Lin</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>YW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Hand-crafted and deep learning-based radiomics models for recurrence prediction of non-small cells lung cancers</article-title><source>Innovation in Medicine and Healthcare</source><volume>1</volume><fpage>135</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1007/978-981-15-5852-8_13</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakr</surname><given-names>S</given-names></name><name><surname>Gevaert</surname><given-names>O</given-names></name><name><surname>Echegaray</surname><given-names>S</given-names></name><name><surname>Ayers</surname><given-names>K</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Shafiq</surname><given-names>M</given-names></name><name><surname>Zheng</surname><given-names>H</given-names></name><name><surname>Benson</surname><given-names>JA</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Leung</surname><given-names>ANC</given-names></name><name><surname>Kadoch</surname><given-names>M</given-names></name><name><surname>Hoang</surname><given-names>CD</given-names></name><name><surname>Shrager</surname><given-names>J</given-names></name><name><surname>Quon</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DL</given-names></name><name><surname>Plevritis</surname><given-names>SK</given-names></name><name><surname>Napel</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A radiogenomic dataset of non-small cell lung cancer</article-title><source>Scientific Data</source><volume>5</volume><elocation-id>180202</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2018.202</pub-id><pub-id pub-id-type="pmid">30325352</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bera</surname><given-names>K</given-names></name><name><surname>Braman</surname><given-names>N</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Velcheti</surname><given-names>V</given-names></name><name><surname>Madabhushi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Predicting cancer outcomes with radiomics and artificial intelligence in radiology</article-title><source>Nature Reviews. Clinical Oncology</source><volume>19</volume><fpage>132</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1038/s41571-021-00560-7</pub-id><pub-id pub-id-type="pmid">34663898</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>EN</given-names></name><name><surname>Pavone</surname><given-names>KJ</given-names></name><name><surname>Naranjo</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multimodal general anesthesia: theory and practice</article-title><source>Anesthesia and Analgesia</source><volume>127</volume><fpage>1246</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1213/ANE.0000000000003668</pub-id><pub-id pub-id-type="pmid">30252709</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carmody</surname><given-names>DP</given-names></name><name><surname>Nodine</surname><given-names>CF</given-names></name><name><surname>Kundel</surname><given-names>HL</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>An analysis of perceptual and cognitive factors in radiographic interpretation</article-title><source>Perception</source><volume>9</volume><fpage>339</fpage><lpage>344</lpage><pub-id pub-id-type="doi">10.1068/p090339</pub-id><pub-id pub-id-type="pmid">7454514</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>K</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Med3D: Transfer Learning for 3D Medical Image Analysis</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1904.00625</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Ghosh</surname><given-names>R</given-names></name><name><surname>Tanamala</surname><given-names>S</given-names></name><name><surname>Biviji</surname><given-names>M</given-names></name><name><surname>Campeau</surname><given-names>NG</given-names></name><name><surname>Venugopal</surname><given-names>VK</given-names></name><name><surname>Mahajan</surname><given-names>V</given-names></name><name><surname>Rao</surname><given-names>P</given-names></name><name><surname>Warier</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep learning algorithms for detection of critical findings in head CT scans: a retrospective study</article-title><source>Lancet</source><volume>392</volume><fpage>2388</fpage><lpage>2396</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(18)31645-3</pub-id><pub-id pub-id-type="pmid">30318264</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chirra</surname><given-names>P</given-names></name><name><surname>Leo</surname><given-names>P</given-names></name><name><surname>Yim</surname><given-names>M</given-names></name><name><surname>Bloch</surname><given-names>BN</given-names></name><name><surname>Rastinehad</surname><given-names>AR</given-names></name><name><surname>Purysko</surname><given-names>A</given-names></name><name><surname>Rosen</surname><given-names>M</given-names></name><name><surname>Madabhushi</surname><given-names>A</given-names></name><name><surname>Viswanath</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Multisite evaluation of radiomic feature reproducibility and discriminability for identifying peripheral zone prostate tumors on MRI</article-title><source>Journal of Medical Imaging</source><volume>6</volume><elocation-id>024502</elocation-id><pub-id pub-id-type="doi">10.1117/1.JMI.6.2.024502</pub-id><pub-id pub-id-type="pmid">31259199</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Beyer</surname><given-names>L</given-names></name><name><surname>Kolesnikov</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2010.11929</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>R</given-names></name><name><surname>Lee</surname><given-names>VH</given-names></name><name><surname>Yuan</surname><given-names>H</given-names></name><name><surname>Lam</surname><given-names>K-O</given-names></name><name><surname>Pang</surname><given-names>HH</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Lam</surname><given-names>EY</given-names></name><name><surname>Khong</surname><given-names>P-L</given-names></name><name><surname>Lee</surname><given-names>AW</given-names></name><name><surname>Kwong</surname><given-names>DL</given-names></name><name><surname>Vardhanabhuti</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Radiomics model to predict early progression of nonmetastatic nasopharyngeal carcinoma after intensity modulation radiation therapy: a multicenter study</article-title><source>Radiology. Artificial Intelligence</source><volume>1</volume><elocation-id>e180075</elocation-id><pub-id pub-id-type="doi">10.1148/ryai.2019180075</pub-id><pub-id pub-id-type="pmid">33937796</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorov</surname><given-names>A</given-names></name><name><surname>Beichel</surname><given-names>R</given-names></name><name><surname>Kalpathy-Cramer</surname><given-names>J</given-names></name><name><surname>Finet</surname><given-names>J</given-names></name><name><surname>Fillion-Robin</surname><given-names>J-C</given-names></name><name><surname>Pujol</surname><given-names>S</given-names></name><name><surname>Bauer</surname><given-names>C</given-names></name><name><surname>Jennings</surname><given-names>D</given-names></name><name><surname>Fennessy</surname><given-names>F</given-names></name><name><surname>Sonka</surname><given-names>M</given-names></name><name><surname>Buatti</surname><given-names>J</given-names></name><name><surname>Aylward</surname><given-names>S</given-names></name><name><surname>Miller</surname><given-names>JV</given-names></name><name><surname>Pieper</surname><given-names>S</given-names></name><name><surname>Kikinis</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>3D slicer as an image computing platform for the quantitative imaging network</article-title><source>Magnetic Resonance Imaging</source><volume>30</volume><fpage>1323</fpage><lpage>1341</lpage><pub-id pub-id-type="doi">10.1016/j.mri.2012.05.001</pub-id><pub-id pub-id-type="pmid">22770690</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giroux</surname><given-names>DJ</given-names></name><name><surname>Van Schil</surname><given-names>P</given-names></name><name><surname>Asamura</surname><given-names>H</given-names></name><name><surname>Rami-Porta</surname><given-names>R</given-names></name><name><surname>Chansky</surname><given-names>K</given-names></name><name><surname>Crowley</surname><given-names>JJ</given-names></name><name><surname>Rusch</surname><given-names>VW</given-names></name><name><surname>Kernstine</surname><given-names>K</given-names></name><collab>International Association for the Study of Lung Cancer Staging and Prognostic Factors Committee</collab></person-group><year iso-8601-date="2018">2018</year><article-title>The IASLC lung cancer staging project: a renewed call to participation</article-title><source>Journal of Thoracic Oncology</source><volume>13</volume><fpage>801</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1016/j.jtho.2018.02.012</pub-id><pub-id pub-id-type="pmid">29476906</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldstraw</surname><given-names>P</given-names></name><name><surname>Chansky</surname><given-names>K</given-names></name><name><surname>Crowley</surname><given-names>J</given-names></name><name><surname>Rami-Porta</surname><given-names>R</given-names></name><name><surname>Asamura</surname><given-names>H</given-names></name><name><surname>Eberhardt</surname><given-names>WEE</given-names></name><name><surname>Nicholson</surname><given-names>AG</given-names></name><name><surname>Groome</surname><given-names>P</given-names></name><name><surname>Mitchell</surname><given-names>A</given-names></name><name><surname>Bolejack</surname><given-names>V</given-names></name><collab>International Association for the Study of Lung Cancer Staging and Prognostic Factors Committee, Advisory Boards, and Participating Institutions</collab><collab>International Association for the Study of Lung Cancer Staging and Prognostic Factors Committee Advisory Boards and Participating Institutions</collab></person-group><year iso-8601-date="2016">2016</year><article-title>The IASLC lung cancer staging project: proposals for revision of the TNM stage groupings in the forthcoming (eighth) edition of the TNM classification for lung cancer</article-title><source>Journal of Thoracic Oncology</source><volume>11</volume><fpage>39</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.jtho.2015.09.009</pub-id><pub-id pub-id-type="pmid">26762738</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>WL</given-names></name><name><surname>Ying</surname><given-names>R</given-names></name><name><surname>Leskovec</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inductive representation learning on large graphs</article-title><conf-name>Proceedings of the 31st International Conference on Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holzinger</surname><given-names>A</given-names></name><name><surname>Haibe-Kains</surname><given-names>B</given-names></name><name><surname>Jurisica</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Why imaging data alone is not enough: AI-based integration of imaging, omics, and clinical data</article-title><source>European Journal of Nuclear Medicine and Molecular Imaging</source><volume>46</volume><fpage>2722</fpage><lpage>2730</lpage><pub-id pub-id-type="doi">10.1007/s00259-019-04382-9</pub-id><pub-id pub-id-type="pmid">31203421</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katzman</surname><given-names>JL</given-names></name><name><surname>Shaham</surname><given-names>U</given-names></name><name><surname>Cloninger</surname><given-names>A</given-names></name><name><surname>Bates</surname><given-names>J</given-names></name><name><surname>Jiang</surname><given-names>T</given-names></name><name><surname>Kluger</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepSurv: personalized treatment recommender system using a COX proportional hazards deep neural network</article-title><source>BMC Medical Research Methodology</source><volume>18</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1186/s12874-018-0482-1</pub-id><pub-id pub-id-type="pmid">29482517</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khanna</surname><given-names>A</given-names></name><name><surname>Londhe</surname><given-names>ND</given-names></name><name><surname>Gupta</surname><given-names>S</given-names></name><name><surname>Semwal</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A deep residual U-net convolutional neural network for automated lung segmentation in computed tomography images</article-title><source>Biocybernetics and Biomedical Engineering</source><volume>40</volume><fpage>1314</fpage><lpage>1327</lpage><pub-id pub-id-type="doi">10.1016/j.bbe.2020.07.007</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kipf</surname><given-names>TN</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Semi-Supervised Classification with Graph Convolutional Networks</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1609.02907</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Zhong</surname><given-names>P</given-names></name><name><surname>Yin</surname><given-names>G</given-names></name><name><surname>Fan</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A nomogram for the prediction of overall survival in patients with stage II and III non-small cell lung cancer using a population-based study</article-title><source>Oncology Letters</source><volume>18</volume><fpage>5905</fpage><lpage>5916</lpage><pub-id pub-id-type="doi">10.3892/ol.2019.10977</pub-id><pub-id pub-id-type="pmid">31788064</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>London</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Artificial intelligence and black-box medical decisions: accuracy versus explainability</article-title><source>The Hastings Center Report</source><volume>49</volume><fpage>15</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1002/hast.973</pub-id><pub-id pub-id-type="pmid">30790315</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malone</surname><given-names>ER</given-names></name><name><surname>Oliva</surname><given-names>M</given-names></name><name><surname>Sabatini</surname><given-names>PJB</given-names></name><name><surname>Stockley</surname><given-names>TL</given-names></name><name><surname>Siu</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Molecular profiling for precision cancer therapies</article-title><source>Genome Medicine</source><volume>12</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1186/s13073-019-0703-1</pub-id><pub-id pub-id-type="pmid">31937368</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirsadraee</surname><given-names>S</given-names></name><name><surname>Oswal</surname><given-names>D</given-names></name><name><surname>Alizadeh</surname><given-names>Y</given-names></name><name><surname>Caulo</surname><given-names>A</given-names></name><name><surname>van Beek</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The 7th lung cancer TNM classification and staging system: review of the changes and implications</article-title><source>World Journal of Radiology</source><volume>4</volume><fpage>128</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.4329/wjr.v4.i4.128</pub-id><pub-id pub-id-type="pmid">22590666</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nabulsi</surname><given-names>Z</given-names></name><name><surname>Sellergren</surname><given-names>A</given-names></name><name><surname>Jamshy</surname><given-names>S</given-names></name><name><surname>Lau</surname><given-names>C</given-names></name><name><surname>Santos</surname><given-names>E</given-names></name><name><surname>Kiraly</surname><given-names>AP</given-names></name><name><surname>Ye</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Pilgrim</surname><given-names>R</given-names></name><name><surname>Kazemzadeh</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Kalidindi</surname><given-names>SR</given-names></name><name><surname>Etemadi</surname><given-names>M</given-names></name><name><surname>Garcia-Vicente</surname><given-names>F</given-names></name><name><surname>Melnick</surname><given-names>D</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Peng</surname><given-names>L</given-names></name><name><surname>Eswaran</surname><given-names>K</given-names></name><name><surname>Tse</surname><given-names>D</given-names></name><name><surname>Beladia</surname><given-names>N</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>P-HC</given-names></name><name><surname>Shetty</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Deep learning for distinguishing normal versus abnormal chest radiographs and generalization to two unseen diseases tuberculosis and COVID-19</article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>15523</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-93967-2</pub-id><pub-id pub-id-type="pmid">34471144</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ridnik</surname><given-names>T</given-names></name><name><surname>Ben-Baruch</surname><given-names>E</given-names></name><name><surname>Noy</surname><given-names>A</given-names></name><name><surname>Zelnik-Manor</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Imagenet-21k Pretraining for the Masses</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2104.10972</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>RL</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Fuchs</surname><given-names>HE</given-names></name><name><surname>Statistics</surname><given-names>JAC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>CA: a cancer Journal for clinicians</article-title><source>Cancer Statistics</source><volume>71</volume><fpage>7</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.3322/caac.21654</pub-id><pub-id pub-id-type="pmid">33433946</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Griethuysen</surname><given-names>JJM</given-names></name><name><surname>Fedorov</surname><given-names>A</given-names></name><name><surname>Parmar</surname><given-names>C</given-names></name><name><surname>Hosny</surname><given-names>A</given-names></name><name><surname>Aucoin</surname><given-names>N</given-names></name><name><surname>Narayan</surname><given-names>V</given-names></name><name><surname>Beets-Tan</surname><given-names>RGH</given-names></name><name><surname>Fillion-Robin</surname><given-names>J-C</given-names></name><name><surname>Pieper</surname><given-names>S</given-names></name><name><surname>Aerts</surname><given-names>HJWL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Computational radiomics system to decode the radiographic phenotype</article-title><source>Cancer Research</source><volume>77</volume><fpage>e104</fpage><lpage>e107</lpage><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-17-0339</pub-id><pub-id pub-id-type="pmid">29092951</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Attention Is All You Need</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Tang</surname><given-names>Z</given-names></name><name><surname>Wei</surname><given-names>W</given-names></name><name><surname>Dong</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Tian</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unsupervised deep learning features for lung cancer overall survival analysis</article-title><conf-name>Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference</conf-name><fpage>2583</fpage><lpage>2586</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2018.8512833</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Hosny</surname><given-names>A</given-names></name><name><surname>Zeleznik</surname><given-names>R</given-names></name><name><surname>Parmar</surname><given-names>C</given-names></name><name><surname>Coroller</surname><given-names>T</given-names></name><name><surname>Franco</surname><given-names>I</given-names></name><name><surname>Mak</surname><given-names>RH</given-names></name><name><surname>Aerts</surname><given-names>HJWL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep learning predicts lung cancer treatment response from serial medical imaging</article-title><source>Clinical Cancer Research</source><volume>25</volume><fpage>3266</fpage><lpage>3275</lpage><pub-id pub-id-type="doi">10.1158/1078-0432.CCR-18-2495</pub-id><pub-id pub-id-type="pmid">31010833</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xue</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>T</given-names></name><name><surname>Long</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multimodal recurrent model with attention for automated radiology report generation</article-title><source>Springer Nature</source><volume>1</volume><fpage>457</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-00928-1_52</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>SK</given-names></name><name><surname>Greenspan</surname><given-names>H</given-names></name><name><surname>Davatzikos</surname><given-names>C</given-names></name><name><surname>Duncan</surname><given-names>JS</given-names></name><name><surname>Van Ginneken</surname><given-names>B</given-names></name><name><surname>Madabhushi</surname><given-names>A</given-names></name><name><surname>Prince</surname><given-names>JL</given-names></name><name><surname>Rueckert</surname><given-names>D</given-names></name><name><surname>Summers</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A review of deep learning in medical imaging: imaging traits, technology trends, case studies with progress highlights, and future promises</article-title><source>Proceedings of the IEEE</source><volume>109</volume><fpage>820</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2021.3054390</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Bae</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>J</given-names></name><name><surname>Samaras</surname><given-names>D</given-names></name><name><surname>Prasanna</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Self Pre-Training with Masked Autoencoders for Medical Image Analysis</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.2203.05573</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Scanner parameter and follow-up strategies</title><p>CT scans ranged from thoracic inlet to subcostal plane and were obtained before surgical resection from two CT machines: Brilliance (Philips Medical Systems Inc, Cleveland, OH) and SOMATOM Definition AS (Siemens Aktiengesell-schaft, Munich, Germany).</p><p>CT parameters of Brilliance (Philips Medical Systems Inc) were as follows: 64×1 mm<sup>2</sup> acquisition; 0.75 s rotation time; slice width 1 mm; tube voltage, 120 kVp; tube current, 150–200 mA; lung window centre: –700 Hounsfield units (HU) and window width: 1200 HU; mediastinal window centre: 60 HU and window width: 450 HU level; pitch: 0.906; and field of view (FOV): 350 mm.</p><p>CT parameters of the SOMATOM Definition AS (Siemens Aktiengesell-schaft) were as follows: 128×1 mm<sup>2</sup> acquisition; 0.5 s rotation time; slice width: 1 mm; tube voltage: 120 kVp; tube current: 150–200 mA; lung window centre: –700 HU and window width: 1200 HU; mediastinal window centre: 60 HU and window width: 450 HU level; FOV: 300 mm; pitch: 1.2; and FOV: 350 mm. CT images were reconstructed into 0.67 to 1.25 mm section thicknesses according to a high-resolution algorithm.</p><p>Follow-up was conducted through outpatient examinations or telephone calls.</p><p>Chest CT scan and abdominal ultrasound/CT were performed on follow-up visits within a duration of 3, 6, and 12 months after operation and annually thereafter for 5 years. Magnetic resonance imaging for brain and bone scan were annually performed for 5 years or when the patient had signs or symptoms of recurrence.</p></sec><sec sec-type="appendix" id="s9"><title>Similarity score definition</title><p>Similarity score for patient <inline-formula><mml:math id="inf1"><mml:mi>x</mml:mi></mml:math></inline-formula> and patient <inline-formula><mml:math id="inf2"><mml:mi>y</mml:mi><mml:mo>:</mml:mo></mml:math></inline-formula><disp-formula id="equ1"><mml:math id="m1"><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf3"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> : if <inline-formula><mml:math id="inf4"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> have same gender, get 1 point; if <inline-formula><mml:math id="inf6"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf7"><mml:mi>y</mml:mi></mml:math></inline-formula>’s age difference is within 5 year, get another 1 point.</p><p><inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> : if <inline-formula><mml:math id="inf9"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>’s tumours locate at the same lung lobes, get 1 point.</p><p><inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> : if <inline-formula><mml:math id="inf12"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf13"><mml:mi>y</mml:mi></mml:math></inline-formula>’s histology of tumours is the same type, get 1 point.</p><p><inline-formula><mml:math id="inf14"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> : if <inline-formula><mml:math id="inf15"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:mi>y</mml:mi></mml:math></inline-formula> have the same T stage, get 1 point; if <inline-formula><mml:math id="inf17"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:mi>y</mml:mi></mml:math></inline-formula> have the same N stage, get another point; if <inline-formula><mml:math id="inf19"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf20"><mml:mi>y</mml:mi></mml:math></inline-formula> have the same M stage, get another 1 point.</p><p>When  <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, patient <inline-formula><mml:math id="inf22"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf23"><mml:mi>y</mml:mi></mml:math></inline-formula> can be connected.</p></sec><sec sec-type="appendix" id="s10"><title>ResNet-Graph NRI and IDI results</title><p>Transformer-Graph comparing with ResNet-Graph, regarding NRI (OS: 0.240, 95% CI: –0.325 to 0.600, p&lt;0.001; RFS: 0.104, 95% CI: –0.41 to 0.389, p&lt;0.001) and IDI (OS: 0.075, 95% CI: 0.068–0.082, p&lt;0.05; RFS: 0.063, 95% CI: 0.027–0.098, p&lt;0.05).</p></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80547.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Caigang</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00v408z34</institution-id><institution>Shengjing Hospital of China Medical University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.06.14.22276385" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.14.22276385"/></front-stub><body><p>This work constructed a population graph deep learning model using machine learning-generated imaging features and non-imaging clinical characteristics that were proven to be effective at predicting the survival of patients with early-stage NSCLC, which help us understand the imaging and non-imaging features in determining NSCLC populations with a high risk of recurrence, and the high predictive accuracy proves its novelty and significance.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80547.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Liu</surname><given-names>Caigang</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00v408z34</institution-id><institution>Shengjing Hospital of China Medical University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.06.14.22276385">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.medrxiv.org/content/10.1101/2022.06.14.22276385v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Early-Stage NSCLC Patients' Prognostic Prediction with Multi-information Using Transformer and Graph Neural Network Model&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Caigang Liu as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Additional information regarding the external validation cohort requires to be supplemented, and the scale of the external independent cohort would better be enriched if possible;</p><p>2) The current situation of this field, along with the limitations of this study could be discussed in more detail in the Discussion.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>While I think something like deep learning is needed in the medical field, I think the authors could do substantially more to validate that the Transformer-Graph model is the right choice and this work is appropriately placed in the larger context of ongoing research in the field. Although this approach is of great significance, the manuscript contains a number of weaknesses. In order to improve the current manuscript, I have the following critical points which need to be addressed before publication.</p><p>1) In the statistical analysis on page 8 line 180, I find it hard to understand &quot;some code&quot;.</p><p>Please clarify this and add the missing part.</p><p>2) For Figures 3C and D, only KM curves on the test data set for OS and RFS prediction are presented. The author should also supply KM curves on external data sets for OS and RFS prediction if it is possible.</p><p>3) The authors stated this manuscript is the first work to demonstrate the feasibility of using Transformer in survival prediction using a graph data structure and exploratory analysis of the models' intuitions in an attempt to explain these state-of-the-art methods, but the authors still need to discuss other key papers and more explicitly state their contribution.</p><p>4) In the discussion, the authors do a very good job to highlight the advantages and limitations of their study, but the following limitations should be expanded:</p><p>a. The authors think that the Transformer-Graph model's performance on the external set was below that of the testing set, maybe because of the patients' different demographics (e.g. age, cancer staging, gender), datasets originate (e.g. ethnicity, treatment, follow-up strategies) and the manual initial step. Smoking history is well known to play an important role in the development of lung cancer. If possible, the non-imaging clinical data should include smoking habits.</p><p>b. On page 14, the authors showed the male percentage is 66.7% and 78.3% in the testing set and external set. It is not clear why this is inconsistent with the data in Table 1. Please the authors clarify this.</p><p>5) Only one external dataset is not enough to evaluate the Transformer-Graph model. If possible, the authors should analyze more datasets.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I have a few concerns that I believe need to be addressed before publication, but overall, I am enthusiastic about this work being in <italic>eLife</italic>.</p><p>1) The authors analyzed specifically as an example, patient No 44. 1 or 2 more examples could be helpful to understand the benefit of this model for survival prediction.</p><p>2) The model was constructed using both imaging and non-imaging clinical features. More discussion of the benefit of this model compared to most current prognostic prediction methods could be helpful.</p><p>3) If possible, increase the clinical case to decrease the different patients' demographics.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.80547.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Additional information regarding the external validation cohort requires to be supplemented, and the scale of the external independent cohort would better be enriched if possible;</p></disp-quote><p>We have added the KM survival plots for the external set as Figure 4—figure supplement 1. Other additional information regarding the external validation cohort has been added as Supplementary File 1.</p><p>We have endeavored to try and find another external dataset. We searched extensively over 40 lung cancer datasets on the TCIA and found only 2 of them met our requirements. The first dataset is already used as our current external dataset. The other dataset we found that the demographics, treatment and follow-up periods were vastly different. For example, the dataset has death rate of 95.88% for early-stage lung cancer patients which is very high, but this could be due to the fact that the dataset is very old (different treatment strategies, etc). We even proceeded to test a model with the second dataset, but the results were not satisfactory which we think is due to differences between the training cohorts and the testing cohorts. To this end, we feel that we are only able to include one external dataset. We have listed “only one external set” as one of our limitations.</p><disp-quote content-type="editor-comment"><p>2) The current situation of this field, along with the limitations of this study could be discussed in more detail in the Discussion.</p></disp-quote><p>Thank you for your suggestion. We have added and elaborated further with respect to the limitations of this study.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>While I think something like deep learning is needed in the medical field, I think the authors could do substantially more to validate that the Transformer-Graph model is the right choice and this work is appropriately placed in the larger context of ongoing research in the field. Although this approach is of great significance, the manuscript contains a number of weaknesses. In order to improve the current manuscript, I have the following critical points which need to be addressed before publication.</p><p>1) In the statistical analysis on page 8 line 180, I find it hard to understand &quot;some code&quot;.</p><p>Please clarify this and add the missing part.</p></disp-quote><p>This is an error caused by Endnote citation generation, we have removed it.</p><disp-quote content-type="editor-comment"><p>2) For Figures 3C and D, only KM curves on the test data set for OS and RFS prediction are presented. The author should also supply KM curves on external data sets for OS and RFS prediction if it is possible.</p></disp-quote><p>We have added the KM curves on external data sets for OS and RFS prediction set as Figure 4—figure supplement 1. Thank you for your suggestion.</p><disp-quote content-type="editor-comment"><p>3) The authors stated this manuscript is the first work to demonstrate the feasibility of using Transformer in survival prediction using a graph data structure and exploratory analysis of the models' intuitions in an attempt to explain these state-of-the-art methods, but the authors still need to discuss other key papers and more explicitly state their contribution.</p></disp-quote><p>Thank you for your suggestion. We have further elaborated and summarized some related papers and explained our contribution in the Discussion section.</p><disp-quote content-type="editor-comment"><p>4) In the discussion, the authors do a very good job to highlight the advantages and limitations of their study, but the following limitations should be expanded:</p><p>a. The authors think that the Transformer-Graph model's performance on the external set was below that of the testing set, maybe because of the patients' different demographics (e.g. age, cancer staging, gender), datasets originate (e.g. ethnicity, treatment, follow-up strategies) and the manual initial step. Smoking history is well known to play an important role in the development of lung cancer. If possible, the non-imaging clinical data should include smoking habits.</p></disp-quote><p>Thank you for your suggestion. It is unfortunate that we do not have the smoking history information in the main set. This is an inherent limitation of a retrospective study. We agree that this would have been important information to include. We have listed this in the limitation.</p><disp-quote content-type="editor-comment"><p>b. On page 14, the authors showed the male percentage is 66.7% and 78.3% in the testing set and external set. It is not clear why this is inconsistent with the data in Table 1. Please the authors clarify this.</p></disp-quote><p>Sorry, this was an erroneous typo. We have corrected it.</p><disp-quote content-type="editor-comment"><p>5) Only one external dataset is not enough to evaluate the Transformer-Graph model. If possible, the authors should analyze more datasets.</p></disp-quote><p>We have endeavored to try and find another external dataset. We searched extensively over 40 lung cancer datasets on the TCIA and found only 2 of them met our requirements. The first dataset is already used as our current external dataset. The other dataset we found that the demographics, treatment and follow-up periods were vastly different. For example, the dataset has death rate of 95.88% for early-stage lung cancer patients which is very high, but this could be due to the fact that the dataset is very old (different treatment strategies, etc). For more details pleaes refer to here:</p><p>(https://wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiomics#1605685483aa3937478c4031873e85766dfdde48).</p><p>We even proceeded to test a model with the second dataset, but the results were not satisfactory which we think is due to differences between the training cohorts and the testing cohorts. To this end, we feel that we are only able to include one external dataset. We have listed “only one external set” as one of our limitations.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I have a few concerns that I believe need to be addressed before publication, but overall, I am enthusiastic about this work being in eLife.</p><p>1) The authors analyzed specifically as an example, patient No 44. 1 or 2 more examples could be helpful to understand the benefit of this model for survival prediction.</p></disp-quote><p>Thank you for your great suggestion. We have added another patient No.182 in the subanalysis part. As No 44 is a high-risk patient, we then analyzed a low-risk patient (No. 182) as a comparison to help understand the model.</p><disp-quote content-type="editor-comment"><p>2) The model was constructed using both imaging and non-imaging clinical features. More discussion of the benefit of this model compared to most current prognostic prediction methods could be helpful.</p></disp-quote><p>We have elaborated more in the discussion with respect to the TNM and the ResNet-Graph model.</p><p>We have now also made it more clear that the ResNet-Graph model used both imaging and non-imaging data in the previous version with additional information and comparison in the supplementary section.</p><disp-quote content-type="editor-comment"><p>3) If possible, increase the clinical case to decrease the different patients' demographics.</p></disp-quote><p>We have endeavored to try and find another external dataset. We searched extensively over 40 lung cancer datasets on the TCIA and found only 2 of them met our requirements. The first dataset is already used as our current external dataset. The other dataset we found that the demographics, treatment and follow-up periods were vastly different. For example, the dataset has death rate of 95.88% for early-stage lung cancer patients which is very high, but this could be due to the fact that the dataset is very old (different treatment strategies, etc). For more details pleaes refer to here:</p><p>(https://wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiomics#1605685483aa3937478c4031873e85766dfdde48).</p><p>We even proceeded to test a model with the second dataset but the results were not satisfactory which we think is due to differences between the training cohorts and the testing cohorts. To this end, we feel that we are only able to include one external dataset. We have listed “only one external set” as one of our limitations.</p></body></sub-article></article>