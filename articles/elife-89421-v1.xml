<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">89421</article-id><article-id pub-id-type="doi">10.7554/eLife.89421</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89421.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>An emerging view of neural geometry in motor cortex supports high-performance decoding</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Perkins</surname><given-names>Sean M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9456-4648</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Amematsro</surname><given-names>Elom A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4843-4513</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author"><name><surname>Cunningham</surname><given-names>John</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Qi</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Churchland</surname><given-names>Mark M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9123-6526</contrib-id><email>mc3502@columbia.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Biomedical Engineering, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01esghr10</institution-id><institution>Department of Neuroscience, Columbia University Medical Center</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Statistics, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01esghr10</institution-id><institution>Center for Theoretical Neuroscience, Columbia University Medical Center</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Grossman Center for the Statistics of Mind, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01esghr10</institution-id><institution>Kavli Institute for Brain Science, Columbia University Medical Center</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>École Polytechnique Fédérale de Lausanne</institution></institution-wrap><country>Switzerland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>03</day><month>02</month><year>2025</year></pub-date><volume>12</volume><elocation-id>RP89421</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-06-08"><day>08</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-04-29"><day>29</day><month>04</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.04.05.535396"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-10-10"><day>10</day><month>10</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89421.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-27"><day>27</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89421.2"/></event></pub-history><permissions><copyright-statement>© 2023, Perkins et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Perkins et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-89421-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-89421-figures-v1.pdf"/><abstract><p>Decoders for brain-computer interfaces (BCIs) assume constraints on neural activity, chosen to reflect scientific beliefs while yielding tractable computations. Recent scientific advances suggest that the true constraints on neural activity, especially its geometry, may be quite different from those assumed by most decoders. We designed a decoder, MINT, to embrace statistical constraints that are potentially more appropriate. If those constraints are accurate, MINT should outperform standard methods that explicitly make different assumptions. Additionally, MINT should be competitive with expressive machine learning methods that can implicitly learn constraints from data. MINT performed well across tasks, suggesting its assumptions are well-matched to the data. MINT outperformed other interpretable methods in every comparison we made. MINT outperformed expressive machine learning methods in 37 of 42 comparisons. MINT’s computations are simple, scale favorably with increasing neuron counts, and yield interpretable quantities such as data likelihoods. MINT’s performance and simplicity suggest it may be a strong candidate for many BCI applications.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>brain-computer interface</kwd><kwd>motor control</kwd><kwd>decoding</kwd><kwd>population activity</kwd><kwd>state estimation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>1R01NS135240</award-id><principal-award-recipient><name><surname>Churchland</surname><given-names>Mark M</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>SCGB</award-id><principal-award-recipient><name><surname>Cunningham</surname><given-names>John</given-names></name><name><surname>Churchland</surname><given-names>Mark M</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>Grossman Center for the Statistics of Mind</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Churchland</surname><given-names>Mark M</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>Graduate Fellowship</award-id><principal-award-recipient><name><surname>Amematsro</surname><given-names>Elom A</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural activity in motor cortex displays unexpected properties, making it possible to accurately decode behavior using a straightforward and interpretable approach.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Brain-computer interfaces (BCIs) seek to estimate target variables, in real time, from observations of neural spiking activity. Target variables may correspond to prosthetic motion (<xref ref-type="bibr" rid="bib10">Carmena et al., 2003</xref>; <xref ref-type="bibr" rid="bib20">Collinger et al., 2013</xref>; <xref ref-type="bibr" rid="bib42">Hochberg et al., 2012</xref>; <xref ref-type="bibr" rid="bib41">Hochberg et al., 2006</xref>; <xref ref-type="bibr" rid="bib105">Velliste et al., 2008</xref>; <xref ref-type="bibr" rid="bib116">Wodlinger et al., 2015</xref>), muscle activity (<xref ref-type="bibr" rid="bib2">Ajiboye et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Bouton et al., 2016</xref>; <xref ref-type="bibr" rid="bib27">Ethier et al., 2012</xref>; <xref ref-type="bibr" rid="bib61">Moritz et al., 2008</xref>), cursor control (<xref ref-type="bibr" rid="bib36">Gilja et al., 2012</xref>; <xref ref-type="bibr" rid="bib43">Jarosiewicz et al., 2015</xref>; <xref ref-type="bibr" rid="bib63">Nuyujukian et al., 2015</xref>; <xref ref-type="bibr" rid="bib67">Pandarinath et al., 2017</xref>; <xref ref-type="bibr" rid="bib91">Serruya et al., 2002</xref>; <xref ref-type="bibr" rid="bib92">Shanechi et al., 2017</xref>; <xref ref-type="bibr" rid="bib99">Taylor et al., 2002</xref>), navigation (<xref ref-type="bibr" rid="bib54">Libedinsky et al., 2016</xref>; <xref ref-type="bibr" rid="bib75">Rajangam et al., 2016</xref>; <xref ref-type="bibr" rid="bib86">Schroeder et al., 2022</xref>), speech (<xref ref-type="bibr" rid="bib4">Anumanchipalli et al., 2019</xref>; <xref ref-type="bibr" rid="bib113">Willett et al., 2023</xref>; <xref ref-type="bibr" rid="bib115">Wilson et al., 2020</xref>; <xref ref-type="bibr" rid="bib9">Card et al., 2024</xref>; <xref ref-type="bibr" rid="bib58">Metzger et al., 2023</xref>; <xref ref-type="bibr" rid="bib107">Wairagkar et al., 2023</xref>), handwriting (<xref ref-type="bibr" rid="bib113">Willett et al., 2023</xref>), or cognitive states (<xref ref-type="bibr" rid="bib62">Musallam et al., 2004</xref>; <xref ref-type="bibr" rid="bib74">Provenza et al., 2019</xref>; <xref ref-type="bibr" rid="bib82">Sani et al., 2018</xref>; <xref ref-type="bibr" rid="bib108">Wallis, 2018</xref>; <xref ref-type="bibr" rid="bib120">Yousefi et al., 2019</xref>). Alternatively, one may estimate the state of the brain, a latent variable, for clinical monitoring (<xref ref-type="bibr" rid="bib11">Chari et al., 2020</xref>), data visualization (<xref ref-type="bibr" rid="bib21">Cowley et al., 2013</xref>), or closed-loop neurally contingent experiments (<xref ref-type="bibr" rid="bib71">Peixoto et al., 2021</xref>). Because neural spiking is noisy, target variables must be estimated from patterns of spikes that were unobserved during training. Doing so requires assumptions.</p><p><xref ref-type="fig" rid="fig1">Figure 1a</xref> illustrates a set of common assumptions. In this view, the primary objects are neural firing rates (which determine the probability of spiking) and a manifold (<xref ref-type="bibr" rid="bib31">Gallego et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Gallego et al., 2018</xref>; <xref ref-type="bibr" rid="bib38">Golub et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Sadtler et al., 2014</xref>) that constrains the possible values of the ‘neural state’: a vector containing the rate of every neuron. While that manifold is presumed to be nonlinear, a linear approximation – i.e. a subspace – may be acceptable for practical purposes (<xref ref-type="bibr" rid="bib31">Gallego et al., 2017</xref>). Within this subspace, there exist neural dimensions where activity correlates with target variables such as hand velocity, providing a basis for estimating those variables. Generalization (e.g. <xref ref-type="bibr" rid="bib36">Gilja et al., 2012</xref>; <xref ref-type="bibr" rid="bib43">Jarosiewicz et al., 2015</xref>; <xref ref-type="bibr" rid="bib110">Weiss et al., 2019</xref>) is thought to rely on two features. First, if distributions of behavioral variables overlap across tasks, neural-state distributions will also overlap (<xref ref-type="bibr" rid="bib32">Gallego et al., 2018</xref>). Second, out-of-distribution generalization can leverage the reliable correlation between behavioral variables and neural activity.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Two perspectives on the structure of neural activity during motor tasks.</title><p>(<bold>a</bold>) Illustration of a traditional perspective. Each neural state (<italic>colored dots</italic>) is an <italic>N</italic>-dimensional vector of firing rates. States are limited to a manifold that can be usefully approximated by a subspace. In this illustration, neural states are largely limited to a two-dimensional subspace within the full three-dimensional firing-rate space. Within that restricted subspace, there exist neural dimensions where neural activity correlates with to-be-decoded variables. BCI decoding leverages knowledge of the subspace and of those correlations. If two tasks have similar motor outputs at two particular moments, the corresponding neural states will also be similar, aiding generalization. (<bold>b</bold>) An emerging perspective. Neural activity is summarized by neural factors (one per axis), with each neuron’s firing rate being a simple function of the factors. Factors may be numerous (many dozens or even hundreds), resulting in a high-dimensional subspace of neural activity. Yet most of that space is unoccupied; factor-trajectories are heavily constrained by the underlying dynamics (<italic>gray arrows</italic>), such that most states are never visited. The order in which states are visited is also heavily sculpted by dynamics; e.g. trajectories rarely ‘swim upstream’. The primary constraint is thus not a subspace, but a sparse manifold where activity largely flows in specific directions. Different tasks may often (but not always) employ different dynamics and thus different regions of the manifold. Because most aspects of neural trajectories fall in the null space of outgoing commands, there are no directions in neural state space that consistently correlate with kinematic variables, and distant states may correspond to similar motor outputs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig1-v1.tif"/></fig><p>Decoding methods frequently assume many or all of these features. For example, the classic population vector (<xref ref-type="bibr" rid="bib35">Georgopoulos et al., 1986</xref>; <xref ref-type="bibr" rid="bib87">Schwartz, 1994</xref>; <xref ref-type="bibr" rid="bib99">Taylor et al., 2002</xref>) assumes a two- or three-dimensional manifold (for reaches in two or three physical dimensions) and leverages neural dimensions where activity correlates with hand velocity. Similarly, nearly all ‘interpretable’ methods (those that make explicit assumptions rather than learning them from training data) assume neural dimensions where activity correlates – perhaps incidentally, but at least usefully – with target variables. Such methods often assume additional constraints on neural activity related to structure in behavior (<xref ref-type="bibr" rid="bib8">Brockwell et al., 2004</xref>; <xref ref-type="bibr" rid="bib46">Kao et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Kemere et al., 2003</xref>; <xref ref-type="bibr" rid="bib49">Kemere et al., 2004a</xref>; <xref ref-type="bibr" rid="bib50">Kemere et al., 2004b</xref>; <xref ref-type="bibr" rid="bib47">Kemere et al., 2002</xref>; <xref ref-type="bibr" rid="bib67">Pandarinath et al., 2017</xref>; <xref ref-type="bibr" rid="bib92">Shanechi et al., 2017</xref>; <xref ref-type="bibr" rid="bib121">Yu et al., 2007</xref>). E.g. a Kalman filter can embody the assumption that velocity and position are dynamically linked (<xref ref-type="bibr" rid="bib36">Gilja et al., 2012</xref>; <xref ref-type="bibr" rid="bib117">Wu et al., 2003</xref>), while <xref ref-type="bibr" rid="bib83">Sani et al., 2021</xref> leveraged the assumption that only a subspace within a low-dimensional neural manifold is relevant to behavior.</p><p>Although the perspective in <xref ref-type="fig" rid="fig1">Figure 1a</xref> has been useful, its assumptions may be true only locally (<xref ref-type="bibr" rid="bib29">Fortunato et al., 2024</xref>) and perhaps not even then. Under an alternative perspective (<xref ref-type="fig" rid="fig1">Figure 1b</xref>), the primary objects are neural factors (which determine each neuron’s instantaneous probability of spiking) and a flow-field (gray arrows) governing factor-state trajectories. Within the space of potential factor states, the flow-field ensures that few are visited. The resulting manifold is complex, and may or may not be locally flat in any useful sense. For example, when cycling at different speeds, the manifold is an (at least) seven-dimensional tube (as in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, green) whose long-axis corresponds to cycling speed (<xref ref-type="bibr" rid="bib84">Saxena et al., 2022</xref>). Many locations in factor-space are never visited, including the void within the tube. The manifold is thus sparse – a minority of factor-states are observable – even though individual-neuron responses are rarely sparse in the traditional sense (most neurons show time-varying activity during most movements). Different tasks (or subtasks) may require different computations, and thus employ different regions of factor space with different local flow-fields. This may result in a complex and extremely sparse manifold. Some factors may, in some regions of the manifold, correlate incidentally with external variables such as velocity. Yet this is not expected to be consistently true.</p><p>The assumptions in <xref ref-type="fig" rid="fig1">Figure 1</xref> accord with the hypothesis of factor-level dynamics (<xref ref-type="bibr" rid="bib19">Churchland and Shenoy, 2024</xref>; <xref ref-type="bibr" rid="bib24">DePasquale et al., 2023</xref>; <xref ref-type="bibr" rid="bib106">Vyas et al., 2020</xref>) that sculpt computation-specific neural geometries (<xref ref-type="bibr" rid="bib18">Churchland et al., 2012</xref>; <xref ref-type="bibr" rid="bib56">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib84">Saxena et al., 2022</xref>; <xref ref-type="bibr" rid="bib97">Sussillo et al., 2015</xref>; <xref ref-type="bibr" rid="bib77">Remington et al., 2018</xref>; <xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>; <xref ref-type="bibr" rid="bib80">Russo et al., 2020</xref>; <xref ref-type="bibr" rid="bib94">Sohn et al., 2019</xref>). As a simple example, <xref ref-type="bibr" rid="bib24">DePasquale et al., 2023</xref> constructed a network of spiking neurons to generate muscle activity during the cycling task (<xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>). Network dynamics produced a limit cycle in a 12-dimensional factor space, with any deviations being swiftly corrected by the flow-field. Thus, although the data occupy a sizable subspace, the manifold consists of only those states near the limit cycle. When the network was trained to both cycle and reach, it did so using task-specific dynamics in different neural dimensions, resulting in a sparse, complex manifold with task-specific sub-regions.</p><p><xref ref-type="fig" rid="fig1">Figure 1b</xref> also accords with analyses that presume high-dimensional neural trajectories (<xref ref-type="bibr" rid="bib12">Chaudhuri et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">Goudar and Buonomano, 2018</xref>; <xref ref-type="bibr" rid="bib60">Mishne et al., 2016</xref>; <xref ref-type="bibr" rid="bib95">Stopfer et al., 2003</xref>), with methods that assume strongly nonlinear mappings between neural activity and behavioral variables (<xref ref-type="bibr" rid="bib85">Schneider et al., 2023</xref>; <xref ref-type="bibr" rid="bib124">Zhou and Wei, 2020</xref>), and with the finding that locally linear relationships between activity and behavior are prominent during some tasks but not others (<xref ref-type="bibr" rid="bib86">Schroeder et al., 2022</xref>). It agrees with the proposal that factors (though less numerous than neurons) are plentiful (<xref ref-type="bibr" rid="bib16">Churchland and Shenoy, 2007</xref>; <xref ref-type="bibr" rid="bib33">Gao and Ganguli, 2015</xref>; <xref ref-type="bibr" rid="bib34">Gao et al., 2017</xref>; <xref ref-type="bibr" rid="bib57">Marshall et al., 2022</xref>; <xref ref-type="bibr" rid="bib90">Seely et al., 2016</xref>), with different tasks and task-epochs often using different factors (<xref ref-type="bibr" rid="bib3">Ames and Churchland, 2019</xref>; <xref ref-type="bibr" rid="bib40">Heming et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">DePasquale et al., 2023</xref>; <xref ref-type="bibr" rid="bib26">Elsayed et al., 2016</xref>; <xref ref-type="bibr" rid="bib59">Miri et al., 2017</xref>; <xref ref-type="bibr" rid="bib86">Schroeder et al., 2022</xref>; <xref ref-type="bibr" rid="bib109">Warriner et al., 2022</xref>; <xref ref-type="bibr" rid="bib118">Xing et al., 2022</xref>) (the ‘flexibility-via-subspace’ hypothesis; <xref ref-type="bibr" rid="bib19">Churchland and Shenoy, 2024</xref>), and with the ability of training to ‘open up’ previously unused degrees of freedom (<xref ref-type="bibr" rid="bib64">Oby et al., 2019</xref>). The assumption of a strong flow-field agrees with empirical limitations on BCI-generated trajectories (<xref ref-type="bibr" rid="bib5">Athalye et al., 2023</xref>; <xref ref-type="bibr" rid="bib65">Oby et al., 2024</xref>) and with the finding that decoding is aided by assuming neural-state dynamics (<xref ref-type="bibr" rid="bib45">Kao et al., 2015</xref>; <xref ref-type="bibr" rid="bib68">Pandarinath et al., 2018</xref>). More broadly, low trajectory tangling (<xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>; <xref ref-type="bibr" rid="bib84">Saxena et al., 2022</xref>) – a feature of motor cortex activity in most tasks – constrains neural trajectories in ways that imply many of the features in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, and thus argues for approximating the manifold using collections of trajectories rather than a subspace (<xref ref-type="bibr" rid="bib7">Brennan et al., 2023</xref>; <xref ref-type="bibr" rid="bib39">Goudar and Buonomano, 2018</xref>).</p><p>There is thus a potential mismatch between the structure of the data and the assumptions made by traditional interpretable decoders. This may be one reason why interpretable methods are often outperformed by expressive machine-learning methods (<xref ref-type="bibr" rid="bib37">Glaser et al., 2020</xref>; <xref ref-type="bibr" rid="bib88">Schwemmer et al., 2018</xref>; <xref ref-type="bibr" rid="bib113">Willett et al., 2023</xref>). Expressive methods (<xref ref-type="bibr" rid="bib1">Ahmadi et al., 2021</xref>; <xref ref-type="bibr" rid="bib4">Anumanchipalli et al., 2019</xref>; <xref ref-type="bibr" rid="bib9">Card et al., 2024</xref>; <xref ref-type="bibr" rid="bib37">Glaser et al., 2020</xref>; <xref ref-type="bibr" rid="bib55">Makin et al., 2018</xref>; <xref ref-type="bibr" rid="bib58">Metzger et al., 2023</xref>; <xref ref-type="bibr" rid="bib68">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="bib88">Schwemmer et al., 2018</xref>; <xref ref-type="bibr" rid="bib96">Sussillo et al., 2012</xref>; <xref ref-type="bibr" rid="bib98">Sussillo et al., 2016</xref>; <xref ref-type="bibr" rid="bib103">Tseng et al., 2019</xref>; <xref ref-type="bibr" rid="bib107">Wairagkar et al., 2023</xref>; <xref ref-type="bibr" rid="bib112">Willett et al., 2021</xref>; <xref ref-type="bibr" rid="bib113">Willett et al., 2023</xref>; <xref ref-type="bibr" rid="bib119">Ye and Pandarinath, 2021</xref>) may be able to implicitly learn, during training, many of the constraints illustrated in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. Motivated by these considerations, we constructed a novel algorithm, Mesh of Idealized Neural Trajectories (MINT), that explicitly leverages those constraints. MINT takes a trajectory-centric view, where a complicated manifold is approximated using previously observed trajectories and interpolations between them. MINT abandons any notion of neural dimensions that reliably correlate with behavioral variables. Instead, MINT creates a direct correspondence between neural and behavioral trajectories, allowing it to capture highly nonlinear relationships. These relationships can be task-specific if the data so argue. One might have expected that mathematical tractability would be compromised by embracing the unusual assumptions in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. Yet the requisite computations are surprisingly simple. It also becomes straightforward to decode a variety of behavioral variables – even those that do not correlate with neural activity – and to estimate the neural state.</p><p>MINT’s performance confirms that there are gains to be made by building decoders whose assumptions match a different, possibly more accurate view of population activity. At the same time, our results suggest fundamental limits on decoder generalization. Under the assumptions in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, it will sometimes be difficult or impossible for decoders to generalize to not-yet-seen tasks. We found that this was true regardless of whether one uses MINT or a more traditional method. This finding has implications regarding when and how generalization should be attempted.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We applied MINT to a total of nine datasets, recorded from primates performing a variety of motor, sensory, and cognitive tasks. Each dataset consisted of simultaneous neural recordings and behavioral variables. We used MINT to decode, based on spiking observations, behavioral variables and the neural state. A central goal was to compare MINT’s decoding performance with existing ‘interpretable’ methods (methods such as the Kalman filter that make explicit assumptions regarding data constraints) and with ‘expressive’ methods (e.g. neural networks that can learn constraints from training data). Current interpretable methods make assumptions largely aligned with <xref ref-type="fig" rid="fig1">Figure 1a</xref>. Thus, if <xref ref-type="fig" rid="fig1">Figure 1b</xref> better describes the data, MINT should consistently outperform other interpretable methods. Highly expressive methods can, given enough training data, implicitly learn a broad range of constraints, including those in <xref ref-type="fig" rid="fig1">Figure 1a, b</xref>, or some other yet-to-be imagined scenario. This provides a strong test of the assumptions in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, because MINT should perform well if those assumptions are correct. Indeed, if <xref ref-type="fig" rid="fig1">Figure 1b</xref> is correct, MINT’s performance should be similar to that of highly expressive methods, and may even be better in some cases because MINT can begin with good assumptions rather than having to learn them from data.</p><p>We also document properties of the data – directly and during decoding – that bear on the scientific question of whether <xref ref-type="fig" rid="fig1">Figure 1a or b</xref> makes better assumptions. A comprehensive characterization of neural trajectory geometry is beyond the scope of this study. Yet, whenever possible, we examine features of neural trajectories relevant to MINT’s assumptions. To do so, we begin by focusing on one dataset (MC_Cycle) recorded during a task in which a primate grasps a hand-pedal and moves it cyclically forward or backward to navigate a virtual environment.</p><sec id="s2-1"><title>Neural trajectories are locally stereotyped and sparsely distributed</title><p>During the cycling task, empirical neural trajectories are compatible with solutions found by neural networks trained to produce muscle commands as an output (<xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>). Those trajectories have features that tend to argue for the assumptions in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. Many of these features follow from the property of ‘low trajectory tangling’: during movement, it is never the case that very different factor-trajectory directions are associated with similar factor states. Trajectories are thus locally stereotyped: two trajectories that pass near one another are moving in similar directions. When trajectories travel in dissimilar directions, maintaining low tangling requires that they avoid one another, spreading out into additional dimensions and becoming more sparse in factor space. For example, the elliptical trajectories during forward and backward cycling (<xref ref-type="fig" rid="fig2">Figure 2a</xref>) appear to overlap in the dominant two dimensions. Yet despite this appearance, the corresponding neural trajectories never come close to crossing. For example, at the apparent crossing-point indicated by the gray disk, forward and backward trajectories are well-separated in dimension 3 (all dimensions use the same scale). This is true at all apparent crossing points, leading to forward-cycling and backward-cycling trajectories that are well-separated. Additionally, both limit cycles contain a void in their center that is never occupied. One might expect that such voids would fill in as additional behaviors are considered. For example, perhaps the interior of the limit cycle becomes occupied when stationary, or at slower speeds? Yet when stationary, neural states are far from the limit-cycle center (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Cycling at different speeds involves neural trajectories that spread out in additional ‘new’ dimensions, and thus remain sparse (<xref ref-type="bibr" rid="bib84">Saxena et al., 2022</xref>). This illustrates a key difference between the notion of a manifold in <xref ref-type="fig" rid="fig1">Figure 1a and b</xref>. In <xref ref-type="fig" rid="fig1">Figure 1a</xref>, if two distant neural states are both likely to be observed, then the state halfway between them is also reasonably likely to be observed. In <xref ref-type="fig" rid="fig1">Figure 1b</xref>, this will frequently not be true.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Properties of neural trajectories in motor cortex, illustrated for data recorded during the cycling task (MC_Cycle dataset).</title><p>(<bold>a</bold>) Low tangling implies separation between trajectories that might otherwise be close. <italic>Top</italic>. Neural trajectories for forward (<italic>purple</italic>) and backward (<italic>orange</italic>) cycling. Trajectories begin 600 ms before movement onset and end 600 ms after movement offset. Trajectories are based on trial-averaged firing rates, projected onto two dimensions. Dimensions were selected to highlight apparent crossings of the cyclic trajectories during forward and backward cycling, while also capturing considerable variance (11.5%, comparable to the 11.6% captured by PCs 3 and 4). <italic>Gray region</italic> highlights one set of apparent crossings. <italic>Bottom</italic>. Trajectories during the restricted range of times in the gray region, but projected onto different dimensions. The same scale is used in top and bottom subpanels. (<bold>b</bold>) Examples of well-separated neural states (<italic>main panel</italic>) corresponding to similar behavioral states (<italic>inset</italic>). Colored trajectory tails indicate the previous 150 ms of states. Data from 7-cycle forward condition. (<bold>c</bold>) Joint distribution of pairwise distances for muscle and neural trajectories. Analysis considered all states across all conditions. For both muscle and neural trajectories, we computed all possible pairwise distances across all states. Each muscle state has a corresponding neural state, from the same time within the same condition. Thus, each pairwise muscle-state distance has a corresponding neural-state distance. The color of each pixel indicates how common it was to observe a particular combination of muscle-state distance and neural-state distance. Muscle trajectories are based on 7 z-scored intramuscular EMG recordings. Correspondence between neural and muscle state pairs included a 50 ms lag to account for physiological latency. Results were not sensitive to the presence or size of this lag. Neural and muscle distances were normalized (separately) by average pairwise distance. (<bold>d</bold>) Same analysis for neural and kinematic distances (based on phase and angular velocity). Correspondence between neural and kinematic state pairs included a 150 ms lag. Results were not sensitive to the presence or size of this lag. (<bold>e</bold>) Control analysis to assess the impact of sampling error. If two sets of trajectories (e.g. neural and kinematic) are isometric and can be estimated perfectly, their joint distribution should fall along the diagonal. To estimate the impact of sampling error, we repeated the above analysis comparing neural distances across two data partitions, each containing 15–18 trials/condition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig2-v1.tif"/></fig></sec><sec id="s2-2"><title>Neural and behavioral trajectories are non-isometric</title><p>A potential consequence of low-tangled trajectories is that similar behavioral outputs can be associated with distant neural states. We found that this was common. As a simple example, there exist many moments when two-dimensional velocity is matched between forward and backward cycling: e.g. at a 45° phase when cycling forward versus 225° when cycling backward. Yet forward and backward trajectories remain distant at all moments. One might suspect that this is simply because angular position is different, even though velocity is matched. Yet neural states can be quite different even when both position and velocity are matched. This point is illustrated (<xref ref-type="fig" rid="fig2">Figure 2b</xref>) by the neural trajectory that unfolds when transitioning from stationary, to cycling forward seven times, to stationary again. This projection was chosen to highlight dimensions where the neural trajectory resembles the behavioral trajectory (inset). Nevertheless, neural and behavioral trajectories are far from isometric. Red circles indicate two neural states where the corresponding positions and angular velocities are nearly identical: cycling at ∼1.7 Hz with the hand approaching the cycle’s bottom. Despite this behavioral match, the neural states are distant. Gray squares indicate neural states that are distant even though, in both cases, the hand is stopped at the cycle’s bottom.</p><p>These observations suggest a many-to-one mapping from a neural manifold with one geometry to a behavioral manifold with a very different geometry. To quantify, we leveraged the fact that each neural distance (between a pair of neural states) has a corresponding behavioral distance, obtained for the same pair of conditions and times (allowing for a latency shift). If behavioral and neural geometries are similar, behavioral and neural distances should be strongly correlated. If geometries differ, a given behavioral distance could be associated with a broad range of neural distances. To establish a baseline that incorporates sampling error, we compared neural distances across two partitions. As expected, the joint distribution was strongly diagonal (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). In contrast, the joint distribution was non-diagonal when comparing neural versus muscle-population trajectories (<xref ref-type="fig" rid="fig2">Figure 2c</xref>) and neural versus kinematic trajectories (<xref ref-type="fig" rid="fig2">Figure 2d</xref>). Each behavioral distance (on the x-axis) was associated with a range of different neural distances (on the y-axis). This was true even when behavioral distances were small: for kinematic states with normalized distance &lt;0.1, the corresponding neural states were as close as 0.02 and as far as 1.46. Thus, dissimilar neural states frequently correspond to similar behavioral states, as suggested by <xref ref-type="fig" rid="fig1">Figure 1b</xref> (see states indicated by arrows).</p><p>Importantly, the converse was not true. It was never the case that similar neural states corresponded to dissimilar behavioral states. Once past small values on the x-axis, the white regions in <xref ref-type="fig" rid="fig2">Figure 2c, d</xref> never extend all the way to the x-axis. For example, when kinematic distances were ∼2, neural distances were always at least 0.53. Thus, accurate decoding should be possible as long as the decoder can handle the many-to-one mapping of neural states to behavioral variables. In principle this might be accomplished linearly. Suppose neural trajectories mirrored behavioral trajectories but with additional non-behavior-encoding dimensions. A linear decoder could simply ignore those additional dimensions. Yet, this will be suboptimal if the ignored dimensions capture the majority of response variance. The above observations thus argue that one may desire quite nonlinear decoding.</p><p>A potential concern regarding the analyses in <xref ref-type="fig" rid="fig2">Figure 2c, d</xref> is that they require explicit choices of behavioral variables: muscle population activity in <xref ref-type="fig" rid="fig2">Figure 2c</xref> and angular phase and velocity in <xref ref-type="fig" rid="fig2">Figure 2d</xref>. Perhaps these choices were misguided. Might neural and behavioral geometries become similar if one chooses ‘the right’ set of behavioral variables? This concern relates to the venerable search for movement parameters that are reliably encoded by motor cortex activity (<xref ref-type="bibr" rid="bib16">Churchland and Shenoy, 2007</xref>; <xref ref-type="bibr" rid="bib28">Fetz, 1992</xref>; <xref ref-type="bibr" rid="bib76">Reimer and Hatsopoulos, 2009</xref>; <xref ref-type="bibr" rid="bib89">Scott, 2008</xref>; <xref ref-type="bibr" rid="bib101">Todorov, 2000</xref>). If one chooses the wrong set of parameters (e.g. chooses muscle activity when one should have chosen joint angles) then of course neural and behavioral geometries will appear non-isometric. There are two reasons why this ‘wrong parameter choice’ explanation is unlikely to account for the results in <xref ref-type="fig" rid="fig2">Figure 2c, d</xref>. First, consider the implications of the left-hand side of <xref ref-type="fig" rid="fig2">Figure 2d</xref>. A small kinematic distance implies that angular pedal position and velocity are both nearly identical for the two moments being compared. Yet the corresponding pair of neural states can be quite distant. Under the concern above, this distance would be due to other encoded behavioral variables – perhaps joint angle and joint velocity – differing between those two moments. However, there are not enough degrees of freedom in this task to make this plausible. The shoulder remains at a fixed position (because the head is fixed) and the wrist has limited mobility due to the pedal design (<xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>). Thus, shoulder and elbow angles are almost completely determined by cycle phase. More generally, ‘external variables’ (positions, angles, and their derivatives) are unlikely to differ more than slightly when pedal phase and angular velocity are matched. Muscle activity could be different because many muscles act on each joint, creating redundancy. However, as illustrated in <xref ref-type="fig" rid="fig2">Figure 2c</xref>, the key effect is just as clear when analyzing muscle activity. Thus, the above concern seems unlikely even if it can’t be ruled out entirely. A broader reason to doubt the ‘wrong parameter choice’ proposition is that it provides a vague explanation for a phenomenon that already has a straightforward explanation. A lack of isometry between the neural population response and behavior is expected when neural-trajectory tangling is low and output-null factors are plentiful (<xref ref-type="bibr" rid="bib19">Churchland and Shenoy, 2024</xref>; <xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>). For example, in networks that generate muscle activity, neural and muscle-activity trajectories are far from isometric (<xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>; <xref ref-type="bibr" rid="bib84">Saxena et al., 2022</xref>; <xref ref-type="bibr" rid="bib97">Sussillo et al., 2015</xref>). Given this straightforward explanation, and given repeated failures over decades to find the ‘correct’ parameters (muscle activity, movement direction, etc.) that create neural-behavior isometry, it seems reasonable to conclude that no such isometry exists.</p><p>We further explored the topic of isometry by considering pairs of distances. To do so, we chose two random neural states and computed their distance, yielding <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We repeated this process, yielding <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. We then computed the corresponding pair of distances in muscle space (<inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and kinematic space (<inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). We considered cases where <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was meaningfully larger than (or smaller than) <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and asked whether the behavioral variables had the same relationship; for example was <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> also larger than <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>? For kinematics, this relationship was weak: across 100,000 comparisons, the sign of <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> agreed with <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> only 67.3% of the time (with 50% being chance). The relationship was much stronger for muscles: the sign of <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> agreed with <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> 79.2% of the time, which is far more than expected by chance yet also far from what is expected given isometry (e.g. the sign agrees 99.7% of the time for the truly isometric control data in <xref ref-type="fig" rid="fig2">Figure 2e</xref>). Indeed there were multiple moments during this task when <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was much larger than <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, yet <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was smaller than <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. These observations are consistent with the proposal that neural trajectories resemble muscle trajectories in some dimensions, but with additional output-null dimensions that break the isometry (<xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>).</p></sec><sec id="s2-3"><title>Leveraging trajectories to estimate the manifold</title><p>The results above, along with other recent results, argue for the perspective in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. In this view, the manifold of observable states is complicated and sparse. We thus designed MINT to approximate that manifold using the neural trajectories themselves, rather than their covariance matrix or corresponding subspace. Unlike a covariance matrix, neural trajectories indicate not only which states are likely, but also which state-derivatives are likely. If a neural state is near previously observed states, it should be moving in a similar direction. MINT leverages this directionality.</p><p>Training-set trajectories can take various forms, depending on what is convenient to collect. Most simply, training data might include one neural trajectory per condition, with each condition corresponding to a distinct movement type. Alternatively, one might employ one long trajectory spanning many movements. Another option is to employ many sub-trajectories, each briefer than a whole movement. The goal is simply for training-set trajectories to act as a scaffolding, outlining the manifold that might be occupied during decoding and the directions in which decoded trajectories are likely to be traveling.</p><p>Because training-set trajectories are unlikely to sample the manifold with sufficient density (e.g. one may train using eight reach directions but wish to decode any direction), we designed MINT to interpolate during decoding. We use the term ‘mesh’ to describe the scaffolding created by the training-set trajectories and the interpolated states that arise at runtime. The term mesh is apt because, if MINT’s assumptions are correct, interpolation will almost always be local. If so, the set of decodable states will resemble a mesh, created by line segments connecting nearby training-set trajectories. However, this mesh-like structure is not enforced by MINT’s operations. Interpolation could, in principle, create state-distributions that depart from the assumption of a sparse manifold. For example, interpolation could fill in the center of the green tube in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, resulting in a solid manifold rather than a mesh around its outer surface. However, this would occur only if spiking observations argued for it. As will be documented below, we find that essentially all interpolation is local. This is a useful fact, because it implies that the estimated neural state (during decoding) will rarely be far from previously observed neural states for which the corresponding behavioral states are known. MINT finds those behavioral states using a direct (and highly nonlinear) mapping between neural and behavioral states. MINT then decodes behavior using local linear interpolation.</p><p>Although the mesh is formed of stereotyped trajectories, decoded trajectories can move along the mesh in non-stereotyped ways as long as they generally obey the flow-field implied by the training data. This flexibility supports many types of generalization, including generalization that is compositional in nature. Other types of generalization – for example from the green trajectories to the orange trajectories in <xref ref-type="fig" rid="fig1">Figure 1b</xref> – are unavailable when using MINT and are expected to be challenging for any method (as will be documented in a later section).</p></sec><sec id="s2-4"><title>Training and decoding using MINT</title><p>Training MINT requires computing a library of neural trajectories (<inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and a corresponding library of behavioral trajectories (<inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). For ease of subsequent computation, neural trajectories are expressed in firing-rate space. MINT is agnostic regarding how neural trajectories are found. For example, one can estimate factor-trajectories, then convert to firing rates via a rectifying nonlinearity (<xref ref-type="bibr" rid="bib24">DePasquale et al., 2023</xref>; <xref ref-type="bibr" rid="bib68">Pandarinath et al., 2018</xref>). The simplest approach, when training data contain repeated trials per condition, is to compute each neuron’s rate via trial-averaging. We use this approach to illustrate MINT’s operations during a center-out reaching task. Neural and behavioral trajectories (<xref ref-type="fig" rid="fig3">Figure 3a</xref>) were learned from a training set containing repeated reaches (trials) to each target location. Each moment in that training data corresponds to a condition (<inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and an index (<inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) indicating progress through the movement. For example, <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 3 corresponds to the purple reach condition and <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 240 indicates a moment 240 ms into that movement. There were many trials for this condition, and thus many measurements of neural activity and behavior for <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 3 and <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 240. By temporally filtering spikes and averaging across trials, one obtains a neural state <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> containing each neuron’s estimated rate. If desired, firing-rate estimation can be further improved by a variety of means (see Methods). A similar process yields a behavioral state <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, which may contain position, velocity, or any other variables of interest.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Example training (<italic>top panel</italic>) and decoding (<italic>bottom panels</italic>) procedures for MINT, illustrated using four conditions from a reaching task.</title><p>(<bold>a</bold>) Libraries of neural and behavioral trajectories are learned such that each neural state <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to a behavioral state <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>b</bold>) Spiking observations are binned spike counts (20 ms bins for all main analyses). <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> contains the spike count of each neuron for the present bin, <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> bins in the past. (<bold>c</bold>) At each time step during decoding, the log-likelihood of observing <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is computed for each and every state in the library of neural trajectories. Log-likelihoods decompose across neurons and time bins into Poisson log-likelihoods that can be queried from a precomputed lookup table. A recursive procedure (not depicted) further improves computational efficiency. (<bold>d</bold>) Two candidate neural states (<italic>purple</italic> and <italic>blue</italic>) are identified. The first is the state within the library of trajectories that maximizes the log-likelihood of the observed spikes. The second state similarly maximizes that log-likelihood, with the restriction that the second state must not come from the same trajectory as the first (i.e. must be from a different condition). Interpolation identifies an intermediate state that maximizes log-likelihood. (<bold>e</bold>) The optimal interpolation is applied to candidate neural states – yielding the final neural-state estimate – and their corresponding behavioral states – yielding decoded behavior. Despite utilizing binned spiking observations, neural and behavioral states can be updated at millisecond resolution (Methods).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig3-v1.tif"/></fig><p>As expected given results above, neural and behavioral trajectories differ during reaching: neural trajectories are stacked loops while position trajectories are arranged radially (velocity trajectories are also radial but return to center as the reach ends). These different neural and behavioral geometries might seem like an impediment to decoding, or to argue for a different choice of target behavioral variables. Yet because neural and behavioral trajectories share the same indices <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, decoding is straightforward regardless of which variables one wishes to decode.</p><p>The parameters of MINT are the libraries of neural trajectories (<inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) and behavioral trajectories (<inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). The trajectories in <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> form a scaffolding that outlines the expected neural manifold during decoding. <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> also specifies the directionality with which decoded states are expected to traverse that manifold. On long timescales, decoded trajectories may be very different from any library trajectory. Yet decoded trajectories will typically be composed of events resembling those from the library, reflecting the local stereotypy described above. Prior work has utilized stereotyped behavioral trajectories (<xref ref-type="bibr" rid="bib50">Kemere et al., 2004b</xref>; <xref ref-type="bibr" rid="bib47">Kemere et al., 2002</xref>) or a mixture of condition-specific behavioral trajectory models that capture trial-by-trial movement variability (<xref ref-type="bibr" rid="bib121">Yu et al., 2007</xref>). For tractability, those methods assumed low-dimensional neural states that are roughly isometric to arm velocities. With MINT, neural trajectories can have any geometry, are learned empirically, and are related to behavioral trajectories via <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> rather than by an assumed geometric isometry.</p><p>During decoding, spike-based observations are binned and counted (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 20 ms for all analyses unless otherwise specified). Suppose we are at time-bin <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> within a decoding session. Recent spiking observations are denoted by <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> specifies the number of previous time bins retained in memory. The foundation of MINT is the computation of the log-likelihood of the data, <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, for a variety of neural states. The decoded state is chosen with the goal of maximizing that log-likelihood. Not all decoders involve data likelihood computations, but likelihoods are desirable when it is possible to obtain them. One typically wishes to decode the state that maximizes data likelihood, or maximizes an objective function that involves likelihoods across multiple states. Knowledge of likelihoods can also indicate the trustworthiness of decoding, as will be discussed in a subsequent section.</p><p>To decode, we first compute the log-likelihood that we would have observed <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for each neural state in the library (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). Importantly, this computation is performed for relatively few states. Even with 1000 samples for each of the neural trajectories in <xref ref-type="fig" rid="fig3">Figure 3</xref>, there are only 4000 possible neural states for which log-likelihoods must be computed (in practice it is fewer still, see Methods). This is far fewer than if one were to naively consider all possible neural states in a typical rate- or factor-based subspace. It thus becomes tractable to compute log-likelihoods using a Poisson observation model. A Poisson observation model is usually considered desirable, yet can pose tractability challenges for methods that utilize a continuous model of neural states. For example, when using a Kalman filter, one is often restricted to assuming a Gaussian observation model to maintain computational tractability.</p><p>The library, <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, makes it simple to compute data likelihoods that take spike-history into account. Without the constraint of a flow-field, a great many past trajectories could have led to the present neural state. Computing the likelihood of <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, given all these possible histories, may be impractical during real-time decoding. Yet given a strong flow-field, such histories will be similar on short timescales. For example, if we are presently 240 ms into the purple trajectory in <xref ref-type="fig" rid="fig3">Figure 3a</xref> (i.e. <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 3 and <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 240) then we know (approximately) the rate of every neuron over the last few hundred milliseconds. One can thus compute the log-likelihood of the observed pattern of spikes, over the last few hundred milliseconds, given that we are presently at <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 3 and <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> = 240. This computation is aided by the fact that, under a Poisson model, spike counts in non-overlapping bins are conditionally independent given knowledge of the underlying rate. Similarly, spike counts for different neurons are conditionally independent given knowledge of their rates. Thus, the log-likelihood of <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, for a particular current neural state, is simply the sum of many individual log-likelihoods (one per neuron and time-bin). Each individual log-likelihood depends on only two numbers: the firing rate at that moment and the spike count in that bin. To simplify online computation, one can precompute the log-likelihood, under a Poisson model, for every plausible combination of rate and spike-count. For example, a lookup table of size 2001 × 21 is sufficient when considering rates that span 0-200 spikes/s in increments of 0.1 spikes/s, and considering 20 ms bins that contain at most 20 spikes (only one lookup table is ever needed, so long as its firing-rate range exceeds that of the most-active neuron at the most active moment in <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Now suppose we are observing a population of 200 neurons, with a 200 ms history divided into ten 20 ms bins. For each library state, the log-likelihood of the observed spike-counts is simply the sum of 200 × 10 = 2000 individual log-likelihoods, each retrieved from the lookup table. In practice, computation is even simpler because many terms can be reused from the last time bin using a recursive solution (Methods). This procedure is lightweight and amenable to real-time applications. The assumption of locally stereotyped trajectories also enables neural states (and decoded behaviors) to be updated between time bins. While awaiting the next set of spiking observations, MINT simply assumes that each neural state advances deterministically according to the flow-field implied by the trajectories.</p><p>To decode stereotyped trajectories, one could simply obtain the maximum-likelihood neural state from the library, then render a behavioral decode based on the behavioral state with the same values of <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This would be appropriate for applications in which conditions are categorical, such as typing or handwriting. Yet in most cases we wish for the trajectory library to serve not as an exhaustive set of possible states, but as a scaffolding for the mesh of possible states. MINT’s operations are thus designed to estimate any neural trajectory – and any corresponding behavioral trajectory – that moves along the mesh in a manner generally consistent with the trajectories in <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. To illustrate, suppose the subject made a reach that split the difference between the purple and blue reach conditions in <xref ref-type="fig" rid="fig3">Figure 3a</xref>. MINT identifies two candidate states with high likelihoods, along different neural trajectories (<xref ref-type="fig" rid="fig3">Figure 3d</xref>) in the library. MINT then interpolates between these high-likelihood states, and determines whether there exists a higher-likelihood state between them. MINT does so by considering a line segment of neural states parameterized by <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, ranging from 0 (at the purple state) to 1 (at the blue state). Because the log-likelihood of the observed spikes is a concave function of <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the optimal neural state can be rapidly identified online using Newton’s method. The decoded behavioral output is then produced by interpolation between the corresponding two behavioral states in <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, using the same <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> associated with the optimal neural state (<xref ref-type="fig" rid="fig3">Figure 3e</xref>).</p><p>The value of <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> may change with time. For example, this could occur if the true behavioral trajectory began close to the purple trajectory but became progressively more similar to the blue trajectory. The library trajectories that MINT interpolates between can also change with time. Thus, interpolation allows considerable flexibility. Not only is one not ‘stuck’ on a trajectory from <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, one is also not stuck on trajectories created by weighted averaging of trajectories in <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For example, if cycling speed increases, the decoded neural state could move steadily up a scaffolding like that illustrated in <xref ref-type="fig" rid="fig1">Figure 1b</xref> (green). In such cases, the decoded trajectory might be very different in duration from any of the library trajectories. Thus, one should not think of the library as a set of possible trajectories that are selected from, but rather as providing a mesh-like scaffolding that defines where neural states are likely to live and the likely direction of their local motion. The decoded trajectory may differ considerably from any trajectory within <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Nevertheless, individual decoded states are (empirically) close to states within <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, as will be discussed in a subsequent section. That proximity makes it reasonable to use the linear interpolation step when decoding behavior.</p></sec><sec id="s2-5"><title>Behavioral decoding when the ground-truth is known</title><p>We begin by assessing MINT’s performance when applied to the activity of an artificial recurrent network of spiking neurons, trained to generate the empirical activity of the deltoid (<xref ref-type="bibr" rid="bib24">DePasquale et al., 2023</xref>). Doing so provides a situation where the ground truth is known: we know the network’s true output and that spiking is approximately Poisson, resembling that of cortical neurons (<xref ref-type="bibr" rid="bib24">DePasquale et al., 2023</xref>). The network performed two tasks – reaching and cycling – governed by different local flow-fields in different regions of factor-space (as proposed by the flexibility-via-subspace hypothesis; <xref ref-type="bibr" rid="bib19">Churchland and Shenoy, 2024</xref>). Indeed, the neural dimensions occupied during cycling are orthogonal to those occupied during reaching (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). There are also moments when behavioral output – instantaneous muscle activity – is identical across tasks even though the underlying neural state is very different, in agreement with the empirical many-to-one mapping (<xref ref-type="fig" rid="fig2">Figure 2c, d</xref>). These features provide a test of whether MINT can handle geometry similar to that proposed in <xref ref-type="fig" rid="fig1">Figure 1b</xref>.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Examples of behavioral decoding provided by MINT for one simulated dataset and four empirically recorded datasets.</title><p>All decoding is causal; only spikes from before the decoded moment are used. (<bold>a</bold>) MINT was applied to spiking data from an artificial spiking network. That network was trained to generate posterior deltoid activity and to switch between reaching and cycling tasks. Based on spiking observations, MINT approximately decoded the true network output at each moment. ‘R3’, ‘R4’, and ‘R6’ indicate three different reach conditions. ‘C’ indicates a cycling bout. MINT used no explicit task-switching, but simply tracked neural trajectories across tasks as if they were conditions within a task. (<bold>b</bold>) Illustration of the challenging nature, from a decoding perspective, of network trajectories. Trajectories are shown for two dimensions that are strongly occupied during cycling. Trajectories for the 8 reaching conditions (<italic>pink</italic>) are all nearly orthogonal to the trajectory for cycling (<italic>brown</italic>) and thus appear compressed in this projection. (<bold>c</bold>) Decoded behavioral variables (<italic>green</italic>) compared to actual behavioral variables (<italic>black</italic>) across four empirical datasets. MC_Cycle and MC_RTT show 10 seconds of continuous decoding. MC_Maze and Area2_Bump show randomly selected trials, demarcated by <italic>vertical dashed lines</italic>. (<bold>d</bold>) Distribution of distances between each decoded neural state and the nearest state in the library <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<italic>green</italic>), for the MC_Cycle dataset. To provide a reference, we drew neural states from a Gaussian distribution whose mean and covariance matched the library states, and computed distances to the library states (<italic>purple</italic>). This emulates what would occur if the manifold were defined by the data covariance and associated neural subspace. The <italic>green</italic> distribution mostly contains non-zero values, but is much closer to zero than the <italic>purple</italic> distribution. Thus, MINT rarely decodes a neural state that exactly matches a library state, but most decoded states are close to the scaffolding provided by the library states. Neural distances are normalized by the average pairwise distance between library states.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig4-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-89421-fig4-video1.mp4" id="fig4video1"><label>Figure 4—video 1.</label><caption><title>Video demonstrating causal neural state estimation and behavioral decoding from MINT on the MC_Cycle dataset.</title><p>In this dataset, a monkey moved a hand pedal cyclically forward or backward in response to visual cues. The raster plot of spiking activity (112 neurons, <italic>bottom right subpanel</italic>) and the actual and decoded angular velocities of the pedal (<italic>top right subpanel</italic>) are animated with 10 seconds of trailing history. Decoding was causal; the decode of the present angular velocity (right hand edge of scrolling traces) was based only on present and past spiking. Cycling speed was ∼2 Hz. The underlying neural state estimate (<italic>green sphere in left subpanel</italic>) is plotted in a 3D neural subspace, with a 2D projection below. The present neural state is superimposed on top of the library of 8 neural trajectories used by MINT. The state estimate always remained on or near (via state interpolation) the neural trajectories. <italic>Purple</italic> and <italic>orange</italic> trajectories correspond to forward and backward cycling conditions, respectively. The lighter-to-darker color gradients differentiate between trajectories corresponding to 1-, 2-, 4-, and 7-cycle conditions for each cycling direction. Neural state estimate corresponds in time to the right edge of the scrolling raster/velocity plot. The three-dimensional neural subspace was hand-selected to capture a large amount of neural variance (59.6%; close to the 62.9% captured by the top 3 PCs) while highlighting the dominant translational and rotational structure in the trajectories.</p></caption></media></fig-group><p>Decoded muscle activity (<xref ref-type="fig" rid="fig4">Figure 4</xref>) was virtually identical to network output (black). This was true during reaches (R3, R4, R6) and bouts of cycling (C). Decoding seamlessly transitioned between tasks. Decoding <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> was .968 over ∼7.1 min of test trials based on ∼4.4 min of training data. This demonstrates that MINT performs well when data match its motivating assumptions, and also that it may be particularly useful in situations where decoding must automatically switch amongst tasks.</p><p>We leveraged this simulated data to compare MINT with a biomimetic decoder whose operations (unlike MINT) mimic the manner in which the true output is produced. The network’s true output is simply a weighted sum of the spikes of 1200 neurons. We thus employed a linear readout (learned via ridge regression) leveraging all 1200 neurons. The resulting decode was very good: <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.982 (and would have been unity if readout weights were inferred from connectivity rather than learned from activity). However, the decoding dimension – just like the actual output dimension – captured exceedingly little (∼2%) of the total variance of network activity, a feature that is likely true of many real networks as well (<xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>). Biomimetic decoding may therefore suffer when not all neurons are recorded. Indeed, when only using 5% of network neurons, performance of the biomimetic decoder dropped from <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.982 to <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.773. In contrast, MINT’s performance declined only slightly from <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.968 to <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.967. This illustrates that, even when biomimetic decoding is possible, it may be outperformed by methods that leverage all neural dimensions rather than just output dimensions. MINT can also decode variables – e.g. kinematics – that are not literally encoded by network activity. These observations call into question an often-implicit assumption: that the activity-to-behavior ‘model’ used during decoding should attempt to approximate the true activity-to-behavior mapping. This assumption is presumably correct if a great many neurons can be recorded and the true output of the neural population is the quantity one wishes to decode – yet that is often not the case in real-world BCI applications.</p></sec><sec id="s2-6"><title>Behavioral decoding across multiple datasets</title><p>We investigated MINT’s decoding performance across four datasets in which primates performed different motor tasks. All datasets included simultaneously collected spiking activity and behavioral measurements. The MC_Cycle dataset involved cycling a hand-held pedal and provided the motor cortex data in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The MC_Maze dataset involved recordings from motor cortex during straight and curved reaches (<xref ref-type="bibr" rid="bib17">Churchland et al., 2010</xref>). The Area2_Bump dataset involved recordings from somatosensory cortex (area 2) during active and passive center-out-reaches (<xref ref-type="bibr" rid="bib14">Chowdhury et al., 2020</xref>). The MC_RTT dataset involved motor cortex recordings during point-to-point reaches with no condition structure (<xref ref-type="bibr" rid="bib55">Makin et al., 2018</xref>). All decoding was offline, to facilitate comparison across methods using matched data. Many of these datasets are suspected to involve trajectories with properties matching those in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. However, unlike for the network above, this is an empirical inference and is not known to be true for all datasets. It was thus unclear, a priori, how well MINT would perform. All decoding was causal; i.e. was based on spikes from before the decoded time. All decoding examples and performance quantifications utilized held-out test sets. Selection of MINT’s (few) hyperparameters is documented in a later section.</p><p>For the MC_Cycle dataset (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, top row), MINT’s decode tracked pedal phase and angular velocity (also see <xref ref-type="video" rid="fig4video1">Figure 4—video 1</xref>). When the pedal was stationary, erroneous deviations from non-zero angular velocity sometimes occurred (e.g. <xref ref-type="fig" rid="fig4">Figure 4c</xref>, just before the middle cycling bout) but were brief and rare: decoded angular speed was &lt;0.1Hz for ∼99% of non-moving times. Transitions between rest and cycling occurred with near-zero latency. During movement, angular velocity was accurately decoded and angular phase was decoded with effectively no net drift over time. This is noteworthy because angular velocity on test trials never perfectly matched any of the trajectories in <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, if decoding were restricted to a library trajectory, one would expect growing phase discrepancies. Yet decoded trajectories only need to locally (and approximately) follow the flow-field defined by the library trajectories. Based on incoming spiking observations, decoded trajectories speed up or slow down (within limits).</p><p>This decoding flexibility presumably relates to the fact that the decoded neural state is allowed to differ from the nearest state in <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. To explore, we computed, for each moment in the MC_Cycle dataset, the distance between the decoded neural state and the closest library state. Most distances were non-zero (<xref ref-type="fig" rid="fig4">Figure 4d</xref>, green distribution). At the same time, distances were much smaller than expected if decoded states obeyed a Gaussian with the empirical covariance (purple). These small distances are anticipated under the view that the empirical manifold (which produces the spiking observations that drive decoding) differs considerably in its shape from the distribution defined by the data’s covariance matrix.</p><p>For the MC_Maze dataset (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, second row), decoded velocity tracked actual reach velocity across a variety of straight and curved reaches (108 conditions). Decoded and actual velocities were highly correlated (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, middle row, <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.963 ± 0.001, <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.950 ± 0.002; ranges indicate standard errors, computed by resampling test trials with replacement). For comparison with the other reach-like datasets below, we computed the mean absolute error (MAE) between decoded and actual velocity. Absolute errors were low (MAE<sub>x</sub> = 4.5 ± 0.1 cm/s, MAE<sub>y</sub> = 4.7 ± 0.1 cm/s), and constituted only ∼2% of x- and y-velocity ranges (224.8 and 205.7 cm/s, respectively).</p><p>For the Area2_Bump dataset (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, third row), correlations with horizontal and vertical hand velocity were also high: <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.939 ± 0.007, <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.895 ± 0.015. Mean absolute errors were small (MAE<sub>x</sub> = 4.1 ± 0.2 cm/s, MAE<sub>y</sub> = 4.4 ± 0.2 cm/s) and similar to those for the MC_Maze dataset. In relative terms, errors were slightly larger for Area2_Bump versus MC_Maze because the range of velocities was smaller (95.8 and 96.8 cm/s for x- and y-velocity). Errors are thus slightly clearer in the traces plotted in <xref ref-type="fig" rid="fig4">Figure 4c</xref> (compare second and third rows).</p><p>Decoding was acceptable, but noticeably worse, for the MC_RTT dataset (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, bottom row). In part this can be attributed to this dataset’s slower reaches (86.6 and 61.3 cm/s x- and y-velocity ranges). When analyzing only movement periods (reach speed &gt;1 cm/s), the median speed across all movement times was 38.1, 16.8, and 5.9 cm/s for MC_Maze, Area2_Bump, and MC_RTT, respectively. Thus, despite being modest in absolute terms (MAE<sub>x</sub> = 2.7 ± 0.1 cm/s, MAE<sub>y</sub> = 2.1 ± 0.1 cm/s), errors resulted in noticeably weakened correlations between decoder output and behavior: <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.785 ± 0.013, <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.843 ± 0.013. As will be discussed below, every decode method achieved its worst estimates of velocity for the MC_RTT dataset. In addition to the impact of slower reaches, MINT was likely impacted by training data that made it challenging to accurately estimate library trajectories. Due to the lack of repeated trials, MINT used AutoLFADS to estimate the neural state during training. In principle this should work well. In practice AutoLFADS may have been limited by having only ∼10 min of training data. Because the random-target task involved more variable reaches, it may also have stressed the ability of all methods to generalize, perhaps for the reasons illustrated in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. To aid interpolation amongst library trajectories (and possibly improve generalization), for this dataset we allowed MINT to consider multiple pairwise interpolations when finding the maximum-likelihood state. This aided performance, but only very modestly. We return to the issue of generalization in a later section.</p></sec><sec id="s2-7"><title>Comparison to other decoders</title><p>We compared MINT with four other decode algorithms: the Kalman filter, Wiener filter, feedforward neural network, and recurrent neural network (GRU). The Kalman filter and Wiener filter are historically popular BCI algorithms (<xref ref-type="bibr" rid="bib10">Carmena et al., 2003</xref>; <xref ref-type="bibr" rid="bib117">Wu et al., 2003</xref>) that are computationally simple and make linear assumptions. The feedforward neural network and GRU are expressive nonlinear function approximators that have been proposed for neural decoding (<xref ref-type="bibr" rid="bib37">Glaser et al., 2020</xref>; <xref ref-type="bibr" rid="bib111">Wessberg et al., 2000</xref>) and can be implemented causally. At each decoding time, both the feedforward neural network and the GRU leverage recent spiking history. The primary difference between the two is whether that history is processed all at once (feedforward network) or sequentially (GRU).</p><p>Each method was evaluated on each of the four datasets and was asked to decode multiple behavioral variables on held-out test sets (<xref ref-type="fig" rid="fig5">Figure 5</xref>). All decoders were provided with a trailing window of spiking observations, binned in 20 millisecond increments. The length of this trailing window was optimized separately for each decoder and dataset (and, like all hyperparameters, was chosen using validation data). For non-MINT decoders, hyperparameters were tuned, using Bayesian optimization, to maximize performance (<xref ref-type="bibr" rid="bib93">Snoek et al., 2012</xref>). MINT’s few hyperparameters were less extensively optimized. For example, window length was optimized just once for a given dataset for MINT, rather than separately for each set of behavioral variables as was done for most other decoders (excepting the Kalman filter). Minimal hyperparameter optimization embraces an unusual and useful aspect of MINT: there is no need to retrain if one wishes to decode a different behavioral variable. Once the neural state has been estimated, all behavioral variables are readily decodable. In principle, less-extensive optimization could have put MINT at a relative disadvantage. In practice, MINT is robust across reasonable hyperparameter choices (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>); any disadvantage is thus likely very slight. By necessity and desire, all comparisons were made offline, enabling benchmarked performance across a variety of tasks and decoded variables, where each decoder had access to the exact same data and recording conditions.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Comparison of decoding performance, for MINT and four additional algorithms, for four datasets and multiple decoded variables.</title><p>On a given trial, MINT decodes all behavioral variables in a unified way based on the same inferred neural state. For non-MINT algorithms, separate decoders were trained for each behavioral group (with the exception of the Kalman filter, which used the same model to decode position and velocity). E.g. separate GRUs were trained to output position and velocity in MC_Maze. Parentheticals indicate the number of behavioral variables within a group. E.g. ‘position (2)’ has two components: x- and y-position. <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is averaged across behavioral variables within a group. ‘Overall’ plots performance averaged across all behavioral groups. <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> values for feedforward networks and GRUs are additionally averaged across runs for 10 random seeds. The Kalman filter is traditionally utilized for position- and velocity-based decoding and was therefore only used to predict these behavioral groups. Accordingly, the ‘overall’ category excludes the Kalman filter for datasets in which the Kalman filter did not contribute predictions for every behavioral group. Results are based on the following numbers of training / test trials: MC_Cycle (174 train, 99 test), MC_Maze (1721 train, 574 test), Area2_Bump (272 train, 92 test), MC_RTT (810 train, 268 test). Vertical offsets and vertical ticks are used to increase visibility of data when symbols overlap.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>MINT’s decoding performance is robust to the choice of hyperparameters.</title><p>MINT was run on the MC_Maze dataset with systematic perturbations to two of MINT’s hyperparameters: bin width (ms) and window length (ms). Bin width is the size of the bin in which spikes are counted: 20 ms for all analyses in the main figures. Window length is the length, in milliseconds, of spiking history that is considered. For all main analyses of the MC_Maze dataset, this was 300 ms, i.e. MINT considered the spike count in the present bin and in 14 previous bins. Perturbations were also made to two hyperparameters related to learning neural trajectories: temporal smoothing width (standard deviation of Gaussian filter applied to spikes) and condition-smoothing dimensionality (see Methods). These two hyperparameters describe how aggressively the trial-averaged data are smoothed (across time and conditions, respectively) when learning rates. Baseline decoding performance (<italic>black circles</italic>) was computed using the same hyperparameters that were used with the MC_Maze dataset in the analyses from <xref ref-type="fig" rid="fig4">Figures 4</xref> and <xref ref-type="fig" rid="fig5">5</xref>. Then, decoding performance was computed by perturbing each of the four hyperparameters twice (<italic>colored circles</italic>): once to ∼50% of the hyperparameter’s baseline value and once to ∼150%. Trials were bootstrapped (1000 resamples) to generate 95% confidence intervals (<italic>error bars</italic>). Perturbations of hyperparameters had little impact on performance. Altering bin width had essentially no impact, nor did altering temporal smoothing. Shortening window length had a negative impact, presumably because MINT had to estimate the neural state using fewer observations. However, the drop in performance was minimal: <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> dropped by .011 for position decoding only. Reducing the number of dimensions used for across-condition smoothing, and consequently over-smoothing the data, had a negative impact on both position and velocity decoding. Yet again this was small: e.g. velocity <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> dropped by .010. These results demonstrate that MINT can achieve high performance using hyperparameter values that span a large range. Thus, they do not need to be meticulously optimized to ensure good performance. In general, optimization may not be needed at all, as MINT’s hyperparameters can often be set based on first principles. For example, in this study, bin width was never optimized either for specific datasets or in general. We chose to always count spikes in 20 ms bins (except in the perturbations shown here) because this duration is long enough to reduce computation time yet short relative to the timescales over which rates change. Additionally, window length can be optimized (as we did for decoding analyses), but it could also simply be chosen to roughly match the timescale over which past behavior tends to predict future behavior. Temporal smoothing of trajectories when building the library can simply use the same values commonly used when analyzing such data. For example, in prior studies, we have used smoothing kernels of width 20–30 ms when computing trial-averaged rates, and these values also support excellent decoding. Condition smoothing is optional and need not be applied at all, but may be useful if one wishes to record fewer trials for more conditions. For example, rather than record 15 trials for 8 reach directions, one might wish to record 5 trials for 24 conditions, then use condition smoothing to reduce sampling error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Impact of different modeling and preprocessing choices on performance of MINT.</title><p>For most applications we anticipate MINT will employ direct decoding that leverages the correspondence between neural and behavioral trajectories, but one could also choose to linearly decode behavior from the estimated neural state. For most applications, we anticipate MINT will use interpolation amongst candidate neural states on neighboring trajectories, but one could also restrict decoding to states within the trajectory library. For real-time applications we anticipate MINT will be run causally, but acausal decoding (using both past and future spiking observations) could be used offline or even online by introducing a lag. We anticipate MINT may be used both in situations where spike events have been sorted based on neuron identity, and situations where decoding simply uses channel-specific unsorted threshold crossings. Panels (<bold>a-c</bold>) explore the first three choices. Performance was quantified for all eight combinations of: direct MINT readout vs. linear MINT readout, interpolation vs. no interpolation, causal decoding vs. acausal decoding. This was done for 121 behavioral variables across four datasets for a total of 964 <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> values. The ‘phase’ behavioral variable in MC_Cycle was excluded from ‘linear MINT readout’ variants because its circularity makes it a trivially poor target for linear decoding. (<bold>a</bold>) MINT’s direct neural-to-behavioral-state association outperforms a linear readout based on MINT’s estimated neural state. Performance was significantly higher using the direct readout (<inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>=0.061 ± 0.002 SE; p&lt;0.001, paired t-test). Note that the linear readout still benefits from the ability of MINT to estimate the neural state using all neural dimensions, not just those that correlate with kinematics. (<bold>b</bold>) Decoding with interpolation significantly outperformed decoding without interpolation (<inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>=0.018 ± 0.001 SE; p&lt;0.001, paired t-test). (<bold>c</bold>) Running acausally significantly improved performance relative to causal decoding (<inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>=0.051 ± 0.002 SE; p&lt;0.001, paired t-test). Although causal decoding is required for real-time applications, this result suggests that (when tolerable) introducing a small decoding lag could improve performance. For example, a decoder using 200 ms of spiking activity could introduce a 50 ms lag such that the decode at time <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is rendered by generating the best estimate of the behavioral state at time <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> using spiking data from <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> through <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>d</bold>) Decoding performance for 13 behavioral variables in the MC_Cycle dataset when sorted spikes were used (112 neurons, <italic>pink</italic>) versus ‘good’ threshold crossings from electrodes for which the signal-to-noise ratio of the firing rates exceeded a threshold (93 electrodes, SNR &gt;2, <italic>cyan</italic>). The loss in performance when using threshold crossings was small (<inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>=–0.014 ± 0.002 SE). SE refers to standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig5-figsupp2-v1.tif"/></fig></fig-group><p>There were 15 total ‘behavioral groups’ across the four datasets. For example, MC_Maze yielded two behavioral groups: position and velocity. We computed performance, for each group, by averaging across group members (e.g. across horizontal and vertical velocity). For every behavioral group, across all datasets, MINT’s performance improved upon the ‘interpretable’ methods: the Kalman and Wiener filters. MINT’s performance was typically comparable to, and often better than, that of the expressive GRU and feedforward neural network. MINT yielded the best performance, of all methods, for 11 of 15 behavioral groups. For 3 of the 4 behavioral groups where MINT’s performance was not the highest, the deficit relative to the best method was negligible: <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> &lt; 0.004. Thus, in only 1 of 15 cases was MINT at a disadvantage relative to another decoder.</p><p>When decoding velocity, MINT provided a small but noticeable improvement over the expressive methods for two datasets (MC_Maze and Area2_Bump), was modestly worse for one dataset (MC_RTT), and was involved in essentially a three-way tie for one dataset (MC_Cycle). For all datasets where position was a behavioral group, MINT and both expressive methods provided extremely similar and very accurate performance, considerably better than that of the Wiener and Kalman filters. MINT tended to display the largest improvements, relative to the next-best decoder, when decoding muscle-related variables (EMG, force, muscle velocity, and muscle length) and phase. This is noteworthy because MINT used the same hyperparameters in all cases and was not re-optimized or re-trained across variables (unlike the other methods). Thus, the same MINT operations that produce a velocity decode naturally produce a position decode and a muscle-length decode.</p><p>For each dataset, we computed overall performance by averaging across behavioral groups. MINT had the best overall performance for three of four datasets: MC_Cycle, MC_Maze, and Area2_Bump. For two datasets (MC_Cycle and MC_Maze), MINT considerably outperformed the Kalman and Wiener filters, and had a tiny advantage over the expressive feedforward network and GRU. For the third (Area2_Bump), MINT’s performance was noticeably better than all other methods.</p><p>The only dataset where MINT did not perform the best overall was the MC_RTT dataset, where it was outperformed by the feedforward network and GRU. As noted above, this may relate to the need for MINT to learn neural trajectories from training data that lacked repeated trials of the same movement (a design choice one might wish to avoid). Alternatively, the less-structured MC_RTT dataset may strain the capacity to generalize; all methods experienced a drop in velocity-decoding <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for this dataset compared to the others. MINT generalizes somewhat differently than other methods, and may have been at a modest disadvantage for this dataset. A strong version of this possibility is that perhaps the perspective in <xref ref-type="fig" rid="fig1">Figure 1a</xref> is correct, in which case MINT might struggle because it cannot use forms of generalization that are available to other methods (e.g. generalization based on neuron-velocity correlations). This strong version seems unlikely; MINT continued to significantly outperform the Wiener and Kalman filters, which make assumptions aligned with <xref ref-type="fig" rid="fig1">Figure 1a</xref>. Additionally, as will be described below, MINT’s neural-state estimates for the MC_RTT dataset were not poor when compared with other methods (indeed, MINT excelled in this regard).</p><p>MINT’s generally high level of decoding performance rests upon Bayesian computations: the estimation of data likelihoods and the selection of a decoded state that maximizes likelihood. A natural question is thus whether a simpler Bayesian decoder would have yielded similar results. We explored this possibility by testing a Naive Bayes regression decoder (<xref ref-type="bibr" rid="bib37">Glaser et al., 2020</xref>) using the MC_Maze dataset. This decoder performed poorly, especially when decoding velocity (<inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.688 and 0.093 for hand position and velocity, respectively), indicating that the modeling assumptions that differentiate MINT from a naive Bayesian decoder are important drivers of MINT’s performance. We also investigated the possibility that MINT gained its performance advantage simply by having access to trial-averaged neural trajectories during training, while all other methods were trained on single-trial data. This difference arises from the fundamental requirements of the decoder architectures: MINT needs to estimate typical trajectories while other methods don’t. Yet it might still be the case that other methods would benefit from including trial-averaged data in the training set, in addition to single-trial data. Alternatively, this might harm performance by creating a mismatch, between training and decoding, in the statistics of decoder inputs. We found that the latter was indeed the case: all non-MINT methods performed better when trained purely on single-trial data.</p></sec><sec id="s2-8"><title>How challenging is generalization?</title><p>The contrasting assumptions in <xref ref-type="fig" rid="fig1">Figure 1a and b</xref> yield different implications regarding generalization, illustrated in <xref ref-type="fig" rid="fig6">Figure 6a and b</xref>. In both views, generalization is possible when a newly observed neural trajectory is ‘composed’ of previously observed elements. However, the nature of those elements is very different. Under the view in <xref ref-type="fig" rid="fig6">Figure 6a</xref>, those elements are neural states within the manifold. New movements involve novel neural trajectories, but those trajectories are composed of states within the previously observed manifold. Because those neural states are assumed to correlate with behavioral variables (e.g. velocity) those variables can be decoded during novel movements.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Implications of neural geometry for decoder generalization, in principle and in practice.</title><p>(<bold>a</bold>) Under a traditional perspective, training that explores the full range of to-be-decoded variables will also explore the relevant neural manifold, allowing future generalization. <italic>Gray dots</italic> indicate neural states observed during training. <italic>Colored traces</italic> indicate newly observed trajectories. Because these trajectories traverse the previously defined manifold, they can be decoded by leveraging the established correlations between manifold dimensions and kinematics. (<bold>b</bold>) Under an emerging perspective, generalization is possible when new trajectories lie within a similar region of factor-space as training-set trajectories. <italic>Gray-dashed traces</italic> indicate training-set neural trajectories. <italic>Colored traces</italic> indicate newly observed trajectories. <italic>Green</italic> and <italic>blue</italic> trajectories largely follow the flow-field implied by training-set trajectories, allowing generalization. In contrast, the <italic>orange</italic> trajectory evolves far from any previously explored region, challenging generalization. This may occur even when behavioral outputs overlap with those observed during training. (<bold>c</bold>) Basic decoding performance on the MC_PacMan task for MINT (<italic>green</italic>) and a Wiener filter (<italic>red</italic>). Test sets used held-out trials from the same conditions as training sets. Force-decoding was excellent both when considering all conditions and when considering only ramps (fast and slow, increasing and decreasing). Results are based on the following numbers of training and test trials: All (234 train, 128 test), Ramps (91 train, 57 test). (<bold>d</bold>) Generalization when training employed the four ramp conditions and testing employed a sine or chirp. Results are based on 91 training trials (all cases) and the following numbers of test trials: Ramps (57), 0.25 Hz Sine (19), 1 Hz Sine (12), 3 Hz Sine (15), Chirp (25). (<bold>e</bold>) Data likelihoods indicate strained generalization. We computed the log-likelihood of the spiking observations for each neural state decoded by MINT. Rightward values indicate that those observations were unlikely even for the state selected as most likely to have generated those observations. Training employed the four ramp conditions. Distributions are shown when test data employed ramps, a 0.25 Hz sine, or a 3 Hz sine. (<bold>f</bold>) Examples of force decoding traces (for held-out trials) when decoding using the Wiener filter. <italic>Top row:</italic> all conditions were used during training. <italic>Bottom row:</italic> training employed the four ramp conditions. Horizontal and vertical scale bars correspond to 2 s and 16 Newtons, respectively. (<bold>g</bold>) As above, but when decoding using MINT. Trials and scale bars are matched across (<bold>f</bold>) and (<bold>g</bold>). Representative trials were selected as those that achieved median decoding performance by MINT within each condition.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Illustration of why decoding will typically fail to generalize across tasks when neural trajectories occupy orthogonal subspaces.</title><p>In theory, learning the output-potent dimensions in motor cortex would be an effective strategy for biomimetic decoding that generalizes to novel tasks. In practice, it is difficult (and often impossible) to statistically infer these dimensions without observing the subject’s full behavioral repertoire (at which point generalization is no longer needed because all the relevant behaviors were directly observed). (<bold>a</bold>) In this toy example, two neural trajectories occupy fully orthogonal subspaces. In the ‘blue task’, the neural trajectory occupies dimensions 1 and 2. In the ‘red task’, the neural trajectory occupies dimensions 3 and 4. Trajectories in both subspaces have a non-zero projection onto <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, enabling neural activity from each task to drive the appropriate output. The output at the right is simply the projection of the blue-task and red-task trajectories onto <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>b</bold>) Illustration of the difficulty of inferring <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from only one task. In this example, <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is estimated using data from the blue task, by linearly regressing the observed output against the neural trajectory for the blue task. The resulting estimate, <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, correctly translates neural activity in the blue task into the appropriate time-varying output, but fails to generalize to the red task. This failure to generalize is a straightforward consequence of the fact that <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was learned from data that didn’t explore dimensions 3 and 4. Note that this same phenomenon would occur if <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> were estimated by regressing intended output versus neural activity (as might occur in a paralyzed patient). (<bold>c</bold>) Estimating the output-potent dimension based only on the red task yields an estimate, <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, that fails to generalize to the blue task. This phenomenon is illustrated here for a linear readout, but would apply to most nonlinear methods as well, unless some additional form of knowledge allows inferences regarding neural trajectories in previously unseen dimensions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig6-figsupp1-v1.tif"/></fig></fig-group><p>While this scenario would be advantageous, empirical observations question its assumptions. The constraint of low tangling implies that it is not possible to traverse previously observed states in entirely new ways. Indeed, even when encouraged to do so under BCI control, monkeys are incapable of producing new neural trajectories that conflict with the flow-field implied by previously seen trajectories (<xref ref-type="bibr" rid="bib5">Athalye et al., 2023</xref>; <xref ref-type="bibr" rid="bib65">Oby et al., 2024</xref>). This suggests that newly observed trajectories will fall into two categories. First, as illustrated by the green and blue trajectories in <xref ref-type="fig" rid="fig6">Figure 6b</xref>, new trajectories may be composed of trajectory segments that largely obey the flow-field implied by previously observed trajectories. For example, the green trajectory climbs the scaffolding of previously observed single-speed trajectories (dashed gray traces), mimicking what might occur in a new situation where cycling speed increases steadily (<xref ref-type="bibr" rid="bib84">Saxena et al., 2022</xref>). Second, newly observed trajectories may sometimes evolve in previously unobserved portions of state-space (<xref ref-type="fig" rid="fig6">Figure 6b</xref>, orange trajectories) where the flow-field has never been estimated. This could occur if the internal computation must change considerably (<xref ref-type="bibr" rid="bib102">Trautmann et al., 2022</xref>), a scenario that may be difficult to anticipate externally.</p><p>Under <xref ref-type="fig" rid="fig6">Figure 6a</xref>, MINT would be at a disadvantage relative to traditional decode methods. In particular, the standard Wiener filter should generalize well because it leverages neuron-behavior correlations and is not limited by any assumptions about a flow-field. In contrast, MINT might often generalize poorly because it cannot leverage neuron-behavior correlations and is constrained by previously observed trajectories. For example, if the green trajectory in <xref ref-type="fig" rid="fig6">Figure 6a</xref> were in the library, then MINT would be predisposed against decoding the blue trajectory during generalization because the two trajectories approach one another with opposing derivatives. The Wiener filter would have no such limitation.</p><p>If the scenario in <xref ref-type="fig" rid="fig6">Figure 6b</xref> holds, generalization will hinge less on the decoder itself and more on how new neural trajectories relate to the previously observed scaffolding. When newly observed trajectories hew close to that scaffolding (e.g. the green and blue trajectories) any good decode method should generalize well. When newly observed trajectories are far from the scaffolding (e.g. orange), all methods will struggle. MINT will struggle because it has no scaffolding in this region. Other methods will struggle because previously observed neuron-behavior correlations are no longer valid.</p><p>We desired a dataset where the predictions of <xref ref-type="fig" rid="fig6">Figure 6a and b</xref> are likely to be different. With the possible exception of MC_RTT, the datasets above lack this property; generalization to held-out trials is not particularly challenging and could be subserved by the features in <xref ref-type="fig" rid="fig6">Figure 6a</xref> or by decoding something like the green trajectory in <xref ref-type="fig" rid="fig6">Figure 6b</xref>. One way to challenge generalization, using existing data, is to restrict training to fewer conditions while ensuring that training still employs a broad range of values for each decoded variable. In the MC_Cycle dataset, the same set of x- and y-velocities occur during forward and backward cycling, just at different phases. If neural activity were dominated by correlations with velocity, traditional methods that leverage those correlations should generalize across cycling directions. Instead, we found that all tested methods failed to generalize when trained on one cycling direction and tested on the other. Most <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> values were close to zero or even negative. The likely reason is that neural trajectories for forward and backward cycling occupy nearly orthogonal subspaces (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), especially for this particular dataset (<xref ref-type="bibr" rid="bib86">Schroeder et al., 2022</xref>). These observations are consistent with the view in <xref ref-type="fig" rid="fig6">Figure 6b</xref>. Of course, this did not have to be the case; the empirical neural trajectories during cycling could have obeyed the hypothesis in <xref ref-type="fig" rid="fig6">Figure 6a</xref> and thus supported generalization.</p><p>Whether one finds the above test conclusive depends upon which behavioral variables one considers primary. If only velocity (or only position) matters, then generalization should have been easy under <xref ref-type="fig" rid="fig6">Figure 6a</xref> because the same range of velocities (and positions) was present in training and test sets. Yet one may have different views regarding which variables are central. One may thus suspect that generalization failed because those variables (or their combinations) were insufficiently sampled during training. To address this concern, we explored generalization using a dataset where it is simple to confirm that the relevant behavioral variables are fully explored during training.</p><p>The MC_PacMan dataset involved Neuropixels-based recordings of motor cortex spiking activity and simultaneous measurements of isometric force applied to an immovable handle. All forces were generated in a forward direction (away from the body) and the arm did not move. Force is thus the central behavioral variable in this task; all other behavioral variables (including muscle activity and cursor height) mirror force or its derivative. Every condition explores a similar range of forces, yet involves very different temporal force profiles. Under the scenario in <xref ref-type="fig" rid="fig6">Figure 6a</xref>, generalization to new force profiles should be straightforward (for traditional methods) because neural states associated with each force level have already been observed. New force profiles are simply new trajectories through previously observed states. This scenario thus predicts that a simple traditional decoder, such as a Wiener filter, will generalize well but that MINT will not. In contrast, under the scenario in <xref ref-type="fig" rid="fig6">Figure 6b</xref>, some forms of generalization should be possible (for any method) while others will be intrinsically challenging (for any method).</p><p>To test these predictions, we trained both MINT and a Wiener filter to decode force. Training data included fast and slow increasing and decreasing ramps, and thus spanned the full range of forces used in this task. Generalization was tested using sinusoids (0.25, 2, and 3 Hz) and a slow-to-fast chirp. In agreement with <xref ref-type="fig" rid="fig6">Figure 6b</xref>, generalization performance was strongly condition-specific and only weakly decoder-specific (<xref ref-type="fig" rid="fig6">Figure 6d, f, g</xref>). Both MINT and the Wiener filter generalized well to the 0.25 Hz sinusoid: performance was almost as good as when training included all conditions. Yet both decoders generalized poorly at higher frequencies, especially the 3 Hz sinusoid. This was true even though the training data contained swiftly increasing and decreasing forces.</p><p>These empirical results make little sense under <xref ref-type="fig" rid="fig6">Figure 6a</xref>, because training data should have explored the relevant manifold. However, they do make sense under <xref ref-type="fig" rid="fig6">Figure 6b</xref>. Under this perspective, neural activity is not dominated by correlations with behavioral variables, but is shaped by the flow-field performing the underlying computation. Two conditions can involve different flow-fields, even if their behavioral outputs span similar ranges. Generalization will be strained in such situations, which may be difficult to anticipate ‘from the outside’.</p><p>In instances where generalization is strained, and performance is poor overall, different decoders may experience performance drops of different, and perhaps difficult-to-predict, magnitudes. For example, MINT outperformed the Wiener filter when generalizing from ramps to the chirp, while the Wiener filter outperformed MINT when generalizing from ramps to the 1 Hz sine. This observation does not suggest a meaningful advantage for either decoder. These advantages were idiosyncratic and, more importantly, overall performance was so poor that either decoder would have been essentially unusable without retraining that included additional conditions. The MC_RTT task may also contain examples of this effect: the two expressive methods outperformed MINT but all decoders achieved their worst velocity-decode performance for this dataset (<xref ref-type="fig" rid="fig5">Figure 5</xref>, bottom), as did methods for estimating the neural state (see below).</p><p>For these reasons, one may wish to know, during decoding, if generalization is becoming strained. Usefully, MINT computes the log-likelihood of spiking observations for its estimated neural state (which is the most-probable state it can find). A prediction of <xref ref-type="fig" rid="fig6">Figure 6b</xref> is that, when generalization is strained, these likelihoods should become unusually low because the true neural state (which generated the spiking observations) will often be far from any state that MINT can estimate. To test this prediction, we computed the distribution of negative log-likelihoods (<xref ref-type="fig" rid="fig6">Figure 6e</xref>) for all estimated neural states during the test of generalization described above. The distribution for 0.25 Hz sinusoids (purple) was similar to that for ramps (blue). This is consistent with the set of true neural states, during the 0.25 Hz sinusoids, being similar to states that are ‘reachable’ by MINT’s interpolation step. In contrast, the distribution for 3 Hz sinusoids (orange) showed a second mode of spiking observations that were particularly unlikely. This is expected if the true neural state, at these moments, is far from any state that can be reached by MINT’s interpolation (as would be true for the orange trace in <xref ref-type="fig" rid="fig6">Figure 6b</xref>). Data log-likelihoods could thus potentially be used to indicate – without having to ask the participant – when decoding is strained because neural states are far from those observed during training. Training could then be modified to include additional conditions.</p></sec><sec id="s2-9"><title>Modeling and preprocessing choices</title><p>MINT uses a direct mapping from neural states to behavioral states to instantiate highly nonlinear decoding, then uses locally linear interpolation to support decoding of behavioral states not observed during training. To determine the impact of these choices on performance, we ran a full factorial analysis that used neural state estimates generated by MINT but compared the direct neural-to-behavioral-state mapping to a linear neural-to-behavioral-state mapping (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a</xref>) and also assessed the impact of interpolation (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2b</xref>). Both choices did indeed improve performance. We also assessed the impact of acausal versus causal decoding (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2c</xref>). Causal decoding is important for real-time BCIs, and is thus employed for all our main decoding analyses. However, future applications could potentially introduce a small lag into online decoding, effectively allowing some ‘acausal decoding’ at the expense of longer-latency responses. As expected, acausal decoding provided modest performance increases, raising the possibility of an interesting tradeoff. Decoding can be zero-lag (causal), positive-lag (acausal), or even negative-lag. The latter would result in a very ‘snappy’ decoder that anticipated actions at the expense of some decoding accuracy. The lag hyperparameter wouldn’t need to be specified prior to training and could be freely adjusted at any time.</p><p>All datasets were curated to contain sorted spikes. Yet during online performance, decoding must typically rely on unsorted threshold crossings. Prior studies have found that moving from sorted spikes to threshold crossings produces a modest but tolerable drop in performance (<xref ref-type="bibr" rid="bib15">Christie et al., 2015</xref>). We similarly found that moving from sorted spikes to threshold crossings (selected to be from ‘good’ channels with reasonable signal-to-noise) produced only a small drop in performance (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d</xref>). The operations of MINT highlight why such robustness is expected. When spikes from two neurons are combined, the resulting ‘unit’ acts statistically as if it were a high firing-rate neuron. Spiking is still approximately Poisson, because the sum of Poisson processes is a Poisson process. Thus, MINT’s statistical assumptions remain appropriate.</p></sec><sec id="s2-10"><title>Neural state estimation</title><p>MINT’s success at behavioral decoding suggests underlying success in neural-state estimation. However, that would not necessarily have to be true. For example, the GRU also typically supported excellent behavioral decodes but doesn’t provide neural state estimation. MINT does (by its very construction) but whether those estimates are accurate remains to be evaluated.</p><p>To evaluate neural-state estimates, we submitted firing-rate predictions to the Neural Latents Benchmark (<xref ref-type="bibr" rid="bib69">Pei et al., 2021</xref>) an online benchmark for latent variable models that compares acausal neural-state estimates across methods and datasets. The four primary datasets for the benchmark are MC_Maze, Area2_Bump, MC_RTT, and an additional dataset, DMFC_RSG, that contains neural recordings from dorsomedial frontal cortex while a monkey performed a cognitive timing task (<xref ref-type="bibr" rid="bib94">Sohn et al., 2019</xref>). Three secondary datasets (MC_Maze-L, MC_Maze-M, and MC_Maze-S) contain additional sessions of the maze task with progressively fewer training trials (500, 250, 100, respectively).</p><p>By submitting to the Neural Latents Benchmark, we can ask how MINT performs relative to a set of ‘baseline’ methods. One baseline method (smoothed spikes) is traditional and simple. Other baseline methods, Gaussian Process Factor Analysis (GPFA; <xref ref-type="bibr" rid="bib122">Yu et al., 2009</xref>) and Switching Linear Dynamical System (SLDS; <xref ref-type="bibr" rid="bib30">Fox et al., 2008</xref>), are modern yet well-established. Finally, two baseline methods are quite new and cutting-edge: Neural Data Transformers (NDT; <xref ref-type="bibr" rid="bib119">Ye and Pandarinath, 2021</xref>) and AutoLFADS (<xref ref-type="bibr" rid="bib51">Keshtkaran et al., 2022</xref>). These are expected to provide a high bar by which to judge the performance of other approaches.</p><p>Although the neural state is a well-defined quantity in spiking models (<xref ref-type="bibr" rid="bib24">DePasquale et al., 2023</xref>), experimental data does not typically provide a ground-truth neural state against which estimates can be compared (unlike for behavioral-state decoding). Yet one can define attributes that an accurate neural-state decode should possess. A critical attribute is prediction of spiking activity. This attribute is assessed via bits per spike, which is closely related to the log-likelihood of spiking observations given a set of firing rates (assuming Poisson spiking). MINT performed similarly to AutoLFADS and was consistently slightly better than NDT on the four primary datasets (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, left). MINT performed slightly better than AutoLFADS and NDT on the largest of the secondary datasets. That advantage grew as dataset-size shrank (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, right). MINT outperformed the other three baseline methods by modest-to-sizeable margins for all seven datasets.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Evaluation of neural state estimates for seven datasets.</title><p>(<bold>a</bold>) Performance quantified using bits per spike. The benchmark’s baseline methods have colored markers. All other submissions have gray markers. Vertical offsets and vertical ticks are used to increase visibility of data when symbols are close due to similar values. Results with negative values are designated by markers to the left of zero, but their locations don’t reflect the magnitude of the negative values. Data from Neural Latents Benchmark (<ext-link ext-link-type="uri" xlink:href="https://neurallatents.github.io/">https://neurallatents.github.io/</ext-link>). (<bold>b</bold>) Performance quantified using PSTH <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>c</bold>) Performance quantified using velocity <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, after velocity was decoded linearly from the neural state estimate. A linear decode is used simply as a way of evaluating the quality of neural state estimates, especially in dimensions relevant to behavior.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig7-v1.tif"/></fig><p>MINT was not designed to maximize bits per spike – good performance in this regard simply results from the primary goal of estimating a neural state containing firing rates. This matters because there exist spiking features – for example, synchrony beyond that due to correlated rates – that some methods may be able to leverage when predicting spike probabilities. Doing so is reasonable but separate from MINT’s goal of estimating a rate-based neural state. It is thus worth considering other attributes expected of good neural-state estimates. The Neural Latents Benchmark includes two such attributes. First, if state estimation is accurate, trial-averaged neural state estimates should resemble empirical trial-averaged firing rates (computed for test trials), an attribute assessed by PSTH <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. MINT achieved higher PSTH <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> values than every other method on every dataset (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). This is in some ways unsurprising: MINT estimates neural states that tend to resemble (at least locally) trajectories ‘built’ from training-set-derived rates, which presumably resemble test-set rates. Yet strong performance is not a trivial consequence of MINT’s design. MINT does not ‘select’ whole library trajectories; PSTH <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> will be high only if condition (<inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), index (<inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), and the interpolation parameter (<inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) are accurately estimated for most moments.</p><p>A second attribute is that, in sensorimotor areas during motor tasks, behavior should be decodable from the neural state. The benchmark thus measured <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for a linear decode of velocity from the neural state. MINT outperformed all baseline approaches (<xref ref-type="fig" rid="fig7">Figure 7c</xref>) for all datasets. (MINT could of course have achieved even better decoding using its typical direct association, but that would have undermined the utility of this comparison).</p><p>In addition to the five baseline methods, the Neural Latents Benchmark solicited submissions from other methods and many were submitted. These submissions spanned a broad range in terms of performance (gray vertical lines in <xref ref-type="fig" rid="fig7">Figure 7</xref>), highlighting the challenging nature of this problem. Some submissions involved ensemble approaches, some of which outperformed both MINT and the baseline methods in terms of bits per spike. For the primary datasets, the highest bits per spike overall was achieved by an ensemble of SpatioTemporal Neural Data Transformers (<xref ref-type="bibr" rid="bib53">Le and Shlizerman, 2022</xref>), achieving 0.386 bits per spike on the MC_Maze dataset, compared to 0.330 for MINT. For the smaller secondary datasets, the very best submissions provided minimal-to-modest improvement over MINT in terms of bits per spike (<xref ref-type="fig" rid="fig7">Figure 7a</xref>, right), and the majority performed less well.</p><p>The ability of some expressive neural networks (and some ensembles of these networks) to improve the bits per spike metric indicates there exists variability in the neural recordings that MINT does not capture. That variability could arise from behaviorally irrelevant fluctuations in the neural state, from synchrony based effects, or from instabilities in the neural recordings. These are ‘real’ things and it is thus valid for a method to leverage them to predict spiking when that is the goal. Yet as noted above, they may often be incidental to one’s goals in estimating the neural state, highlighting the need to consider additional attributes such as PSTH <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and velocity <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. For PSTH <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7b</xref>), MINT outperformed all other methods for all six datasets. For velocity <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig7">Figure 7c</xref>), MINT had the best performance for three out of six datasets and was very close to the highest <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> values for the remaining three datasets (the largest deficit was <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.012).</p><p>In summary, MINT’s strong decode performance (<xref ref-type="fig" rid="fig5">Figure 5</xref>) does indeed follow from good neural-state estimation. MINT provided estimates that were competitive with the best present methods. This includes existing well-documented methods, as well as a bevy of additional submissions. This implies that MINT could be used in situations where neural-state estimation is the goal, possibly including monitoring of the neural state for clinical purposes. MINT uses simple operations and is readily implemented causally – indeed this is typically how we would expect to see it used. Yet MINT’s performance is typically similar to – and often better than – that of complex methods that would likely have to be limited to offline situations.</p></sec><sec id="s2-11"><title>Practical considerations</title><sec id="s2-11-1"><title>Training and execution times</title><p>For MINT, ‘training’ simply means computation of standard quantities (e.g. firing rates) rather than parameter optimization. MINT is thus typically very fast to train (<xref ref-type="table" rid="table1">Table 1</xref>), on the order of seconds using generic hardware (no GPUs). This speed reflects the simple operations involved in constructing the library of neural-state trajectories: filtering of spikes and averaging across trials. At the same time we stress that MINT is a method for leveraging a trajectory library, not a method for constructing it. One may sometimes wish to use alternatives to trial-averaging, either of necessity or because they improve trajectory estimates. For example, for the MC_RTT task we used AutoLFADS to infer the library. Training was consequently much slower (hours rather than seconds) because of the time taken to estimate rates. Training time could be reduced back to seconds using a different approach – grouping into pseudo-conditions and averaging – but performance was reduced. Thus, training will typically be very fast, but one may choose computationally intensive methods when appropriate.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>MINT training times and average execution times (average time it took to decode a 20 ms bin of spiking observations).</title><p>To be appropriate for real-time applications, execution times will ideally be shorter than the bin width. Note that the 20 ms bin width does not prevent MINT from decoding every millisecond; MINT updates the inferred neural state and associated behavioral state between bins, using neural and behavioral trajectories that are sampled every millisecond. For all table entries (except MC_RTT training time), means and standard deviations were computed across 10 train/test runs for each dataset. Training times exclude loading datasets into memory and any hyperparameter optimization. Timing measurements taken on a Macbook Pro (on CPU) with 32GB RAM and a 2.3 GHz 8-Core Intel Core i9 processor. Training and execution code used for timing measurements was written in MATLAB (with the core recursion implemented as a MEX file). For MC_RTT, training involved running AutoLFADS twice (and averaging the resulting rates) to generate neural trajectories. This training procedure utilized 10 GPUs and took ∼1.6 hr per run. For AutoLFADS, hyperparameter optimization and model fitting procedures are intertwined. Thus, the training time reported includes time spent optimizing hyperparameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Training Time (s)</th><th align="left" valign="bottom">Execution Time (ms)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Area2_Bump</td><td align="left" valign="bottom">4.8 ± 0.2</td><td align="left" valign="bottom">0.31 ± 0.03</td></tr><tr><td align="left" valign="bottom">MC_Cycle</td><td align="left" valign="bottom">20.3 ± 2.9</td><td align="left" valign="bottom">0.55 ± 0.03</td></tr><tr><td align="left" valign="bottom">MC_Maze</td><td align="left" valign="bottom">42.8 ± 0.6</td><td align="left" valign="bottom">2.26 ± 0.09</td></tr><tr><td align="left" valign="bottom">MC_Maze-L</td><td align="left" valign="bottom">10.8 ± 0.4</td><td align="left" valign="bottom">0.83 ± 0.02</td></tr><tr><td align="left" valign="bottom">MC_Maze-M</td><td align="left" valign="bottom">8.0 ± 0.4</td><td align="left" valign="bottom">0.80 ± 0.04</td></tr><tr><td align="left" valign="bottom">MC_Maze-S</td><td align="left" valign="bottom">5.8 ± 0.2</td><td align="left" valign="bottom">0.59 ± 0.07</td></tr><tr><td align="left" valign="bottom">MC_RTT</td><td align="left" valign="bottom">∼3.2 hr</td><td align="left" valign="bottom">6.99 ± 0.28</td></tr></tbody></table></table-wrap><p>Execution times were well below the threshold for real-time applications (<xref ref-type="table" rid="table1">Table 1</xref>), even using a laptop computer. State estimation in MINT is <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the total number of times (across conditions) in the library of neural trajectories. This is <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> when making the simplifying assumption that all conditions are of equal length. Memory requirements are similarly <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of behavioral variables (a value that will typically remain quite small). Thus, both execution times and memory requirements grow only linearly with neurons or with conditions. Additionally, much of the estimation procedure in MINT is condition-specific and therefore parallelizable. This makes MINT a good candidate for decoding in BCI applications that involve large neuron counts and/or large behavioral repertoires.</p></sec></sec><sec id="s2-12"><title>MINT performs well on small training sets</title><p>To investigate robustness to training with small trial-counts, we leveraged the fact that the four maze-reaching datasets – one primary (MC_Maze) and three secondary (MC_Maze-L, MC_Maze-M, and MC_Maze-S) – contained training sets of decreasing size. Because training involved computing trial-averaged rates, performance is expected to decline when fewer trials are available for averaging. For comparison, we chose the GRU and the feedforward neural network because of their generally good position and velocity decoding performance in the MC_Maze task. We assessed performance in terms of position and velocity <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig8">Figure 8a</xref>). MINT always performed at least as well as the two expressive methods. This gap was often (though not always) larger for smaller training sets. More broadly, MINT was quite robust to small training sets: an approximately fivefold reduction in training data led to a decline in <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> of only 0.037 (position) and 0.050 (velocity).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Decoding robustness in the face of small training sets and reduced neuron counts.</title><p>(<bold>a</bold>) <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> values for MINT and two neural network decoders (GRU and the feedforward network) when decoding position and velocity for four maze datasets with progressively fewer training trials per condition. Results are based on the following numbers of training and testing trials: MC_Maze-S (75 train, 25 test), MC_Maze-M (188 train, 62 test), MC_Maze-L (375 train, 125 test), MC_Maze (1721 train, 574 test). MC_Maze contains 108 conditions and the other maze datasets each contain 27 conditions. (<bold>b</bold>) Decoding performance in the face of undetected (<italic>dashed traces</italic>) and known (<italic>solid traces</italic>) loss of neurons. <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> values are shown for MINT (<italic>green</italic>) and the Wiener filter (<italic>red</italic>) when decoding position and velocity from the MC_Maze-L dataset (same train/test trials as in <bold>a</bold>). To simulate undetected neuron loss, we chose <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons (randomly, with <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> being the number of lost neurons out of 162 total) and eliminated their spikes without adjusting the decoders. This procedure was repeated 50 times for each value of <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Traces show the mean and standard deviation of the sampling distribution (equivalent to the standard error). To simulate known neuron loss, we followed the above procedure but altered the decoder to account for the known loss. For MINT, this simply involved ignoring the lost neurons when computing data likelihoods. In contrast, the Wiener filter had to be retrained using training data with those neurons eliminated (as would be true for most methods). This was done separately for each set of lost neurons.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>The Kalman filter’s relative performance would improve if neural data had different statistical properties.</title><p>We compared MINT to the Kalman filter across one empirical dataset (MC_Maze) and four simulated datasets (same behavior as MC_Maze, but simulated spikes). Simulated firing rates were linear functions of hand position, velocity, and acceleration (as is assumed by the Kalman filter). The means and standard deviations (across time and conditions) of the simulated firing rates were matched to actual neural data (with additional rate scaling in two cases). The Kalman filter assumes that observation noise is stationary and Gaussian. Although spiking variability cannot be Gaussian (spike counts must be positive integers), spiking variability can be made more stationary by letting that variability depend less on rate. Thus, although the first simulation generated spikes via a Poisson process, subsequent simulations utilized gamma-interval spiking (gamma distribution with <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is firing rate). Gamma-interval spiking variability of this form is closer to stationary at higher rates. Thus, the third and fourth simulations scaled up firing rates to further push spiking variability into a more stationary regime at the expense of highly unrealistic rates (in the fourth simulation, a firing rate briefly exceeded 1800 Hz). Overall, as the simulated neural data better accorded with the assumptions of the Kalman filter, decoding performance for the Kalman filter improved. Interestingly, MINT continued to perform well even on the simulated data, likely because MINT can exploit a linear relationship between neural and behavioral states when the data argue for it and higher rates benefit both algorithms. These results demonstrate that algorithms like MINT and the Kalman filter are not intrinsically good or bad. Rather, they are best suited to data that match their assumptions. When simulated data approximate the assumptions of the Kalman filter, both methods perform similarly. However, MINT shows much higher performance for the empirical data, suggesting that its assumptions are a better match for the statistical properties of the data.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig8-figsupp1-v1.tif"/></fig><fig id="fig8s2" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 2.</label><caption><title>MINT is a modular algorithm amenable to a variety of modifications and extensions.</title><p>The flowchart illustrates the standard MINT algorithm in <italic>black</italic> and lists potential changes to the algorithm in <italic>red</italic>. For example, the library of neural trajectories is typically learned via standard trial-averaging or a single-trial rate estimation technique like AutoLFADS. However, transfer learning could be utilized to learn the library of trajectories based on trajectories from a different subject or session. The trajectories could also be modified online while MINT is running to reflect changes in spiking activity that relate to recording instabilities. A potential modification to the method also occurs when likelihoods are converted into posterior probabilities. Typically, we assume a uniform prior over states in the library. However, that prior could be set to reflect the relative frequency of different behaviors and could even incorporate time-varying information from external sensors (e.g. state of a prosthetic limb, eye tracking, etc.) that include information about how probable each behavior is at a given moment. Another potential extension of MINT occurs at the stage where candidate neural states are selected. Typically, these states are selected based solely on spike count likelihoods. However, one could use a utility function that reflects a user’s values (e.g. the user may dislike some decoding mistakes more than others) in conjunction with the likelihoods to maximize expected utility. Lastly, the behavioral estimate that MINT returns could be post-processed (e.g. temporally smoothed). MINT’s modularity largely derives from the fact that the library of neural trajectories is finite. This assumption enables posterior probabilities to be directly computed for each state, rather than analytically derived. Thus, choices like how to learn the library of trajectories, which observation model to use (e.g. Poisson vs. generalized Poisson), and which (if any) state priors to use, can be made independently from one another. These choices all interact to impact performance. Yet they will not interact to impact tractability, as would have been the case if one analytically derived a continuous posterior probability distribution.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89421-fig8-figsupp2-v1.tif"/></fig></fig-group></sec><sec id="s2-13"><title>Performance when neurons are lost</title><p>It is desirable for a decoder to be robust to the unexpected loss of the ability to detect spikes from some neurons. Such loss might occur while decoding, without being immediately detected. Additionally, one desires robustness to a known loss of neurons / recording channels. For example, there may have been channels that were active one morning but are no longer active that afternoon. At least in principle, MINT makes it very easy to handle this second situation: there is no need to retrain the decoder, one simply ignores the lost neurons when computing likelihoods. This is in contrast to nearly all other methods, which require retraining because the loss of one neuron alters the optimal parameters associated with every other neuron.</p><p>To evaluate, we performed a neuron-dropping analysis using the MC_Maze-L dataset (<xref ref-type="fig" rid="fig8">Figure 8b</xref>). For comparison, we also evaluated a Wiener filter. The Wiener filter provides a useful comparison because (like MINT) it is easy to retrain quickly, with reliable results, across many training sets (which was critical in the present situation). To simulate undetected neuron loss, we chose a random subset of neurons and removed all their spikes, without making any adjustment to the decoders. This simulates a neuron (or channel) falling silent. This was repeated many times with different random draws. To simulate known neuron loss, we did the same but allowed the decoders to adjust: by ignoring the lost neurons in the case of MINT, and by retraining all parameters in the case of the Wiener filter. We evaluated both position (<xref ref-type="fig" rid="fig8">Figure 8b</xref>, top) and velocity (bottom) decoding.</p><p>For MINT, undetected loss of fewer than 20 (of 162) neurons caused essentially no performance deficit. Average position and velocity decoding <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> remained above 95% of peak performance until fewer than 113 and 109 neurons remained, respectively. Large undetected losses – for example of half the neurons – caused major deficits in decoding. Yet decoding remained accurate if those losses were known, with the lost neurons ignored during decoding. MINT was extremely robust in this situation: MINT’s average <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for position decoding remained above 95% of peak performance until fewer than 58 neurons (36%) remained. Results were similar for velocity decoding: performance was &gt;95% of its peak until fewer than 62 neurons (38%) remained. The Wiener filter was also fairly robust, but less so than MINT. To remain above 95% of peak performance, the retrained Wiener filter needed approximately twice as many remaining neurons: 118 for position and 126 for velocity. When not retrained, Wiener-filter performance dropped steadily without a clear plateau for modest neuron loss.</p><p>These results illustrate that MINT is reasonably robust to undetected neuron loss. In practice, the loss of many neurons would likely be detected. In such cases, MINT is very robust. Because there is no need for any true retraining, adjustment to known neuron-loss can occur on the fly with no need to pause decoding.</p></sec><sec id="s2-14"><title>Interpretability</title><p>One form of interpretability relates to whether an algorithm uses ‘black box’ optimization to learn a solution. For example, neural networks can be highly expressive and learn constraints directly from data. However, when constraints are known, one may prefer to rely more on explicit assumptions and less on expressivity. Interpretable methods, such as the Kalman filter, rely on explicit assumptions. Such assumptions may (if they match the data) improve performance. They may also provide other potential advantages. For example, the Kalman filter is a canonical ‘interpretable’ algorithm, with a probabilistic graphical model that makes an explicit Gaussian assumption regarding distributions of spike-count measurements. This explicitness may be helpful in understanding successes and failures (e.g. the Gaussian assumption can break down during low-rate periods).</p><p>The performance of interpretable methods should tend to reflect the extent to which their assumptions match the statistics of the data. To investigate, we considered the performance of both the Kalman filter and MINT on the MC_Maze reaching dataset. The Kalman filter produces a relatively poor decode (<inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> = 0.609 for hand velocity, versus 0.914 for MINT). This does not imply the Kalman filter is a poor algorithm, simply that its assumptions may not be well-aligned with the data. We confirmed this by generating synthetic data whose properties are closer to those assumed by the Kalman filter. The Kalman filter then performed quite competitively (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). This result underscores that MINT does not outperform the Kalman filter because it is intrinsically a better method. Instead, MINT performs well because its assumptions are presumably a better match to the underlying properties of the data. If <xref ref-type="fig" rid="fig1">Figure 1a</xref> were the correct perspective, the Kalman filter would have been expected to perform at least as well as MINT. Under <xref ref-type="fig" rid="fig1">Figure 1b</xref> the Kalman filter is predicted to perform less well, which is indeed what we found.</p><p>MINT is also interpretable regarding why each prediction is made. Suppose that, at a given moment during decoding, MINT selects neural state <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as the most likely state and renders its decode using the associated behavioral state <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. As analyzed in <xref ref-type="fig" rid="fig6">Figure 6e</xref> above, selection of <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> comes with an associated likelihood that conveys information regarding how well the spiking data matched that neural state. Likelihoods are known not only for this ‘selected’ state, but for all non-selected states on all trajectories in the library. These likelihoods can be analyzed to understand why the selected state was chosen over the others, how close the algorithm was to selecting a different state, and which neurons contributed most strongly to state selection. More generally, because of its probability-based approach, MINT is readily modifiable and extensible in a variety of ways (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Implications regarding neural geometry</title><p>Our results argue that the empirical structure of motor-cortex activity is closer to the hypothesis in <xref ref-type="fig" rid="fig1">Figure 1b</xref>. We began with the prediction that, under that hypothesis, MINT should consistently outperform existing interpretable methods such as the Kalman and Wiener filters. This was indeed the case: MINT always performed better than these two methods, often by sizable margins. In contrast, MINT and the Kalman filter performed comparably on simulated data that better approximated the assumptions in <xref ref-type="fig" rid="fig1">Figure 1a</xref>. Thus, MINT is not a ‘better’ algorithm – simply better aligned with the empirical properties of motor cortex data. This highlights an important caveat. Although MINT performs well when decoding from motor areas, its assumptions may be a poor match in other areas (e.g. the hippocampus). MINT performed well on two non-motor-cortex datasets – Area2_Bump (S1) and DMFC_RSG (dorsomedial frontal cortex) – yet there will presumably be other brain areas and/or contexts where one would prefer a different method that makes assumptions appropriate for that area.</p><p>We also began with the prediction that, under the hypothesis in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, MINT should be competitive with highly expressive approaches that can learn their constraints from data. MINT may even perform better in some cases because MINT can begin with appropriate constraints rather than having to learn them from training data. At the same time, MINT might sometimes be at a disadvantage if there are additional constraints or features that it does not take into account (a simple example would be drift in firing rates over time – one would desire a drift stabilization module to correct for this, as in <xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>). Empirically, decoding performance was often exceedingly similar for MINT and both expressive methods (the feedforward network and GRU). Consider velocity, which is probably the most commonly decoded parameter for motor BCIs. Across the four datasets in <xref ref-type="fig" rid="fig5">Figure 5</xref>, the average <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for the velocity decode was 0.841 (MINT), 0.830 (GRU), and 0.837 (feedforward network). These values are all considerably higher than for the Kalman filter (0.630) and Wiener filter (0.719). There were multiple cases where MINT performed noticeably better than either expressive method, and one case (the MC_RTT dataset) where MINT performed noticeably worse. These results are consistent with the set of expectations outlined above: MINT should be highly competitive with expressive methods, often performing modestly better and sometimes performing modestly worse.</p><p>The hypothesis that MINT’s assumptions are well-aligned with the data is further supported by its performance during neural-state estimation. Of the well-established baseline methods for state-estimation, the highest performing were AutoLFADS and a Neural Data Transformer (NDT). Across a variety of datasets and performance metrics, MINT either performed similarly to these methods or performed noticeably better. MINT consistently outperformed simple lightweight state estimators, despite being a simple lightweight estimator itself. We find it noteworthy that MINT naturally performs very well as a neural-state estimator, despite being designed as a decoder of behavior. MINT does so with no need for modification. This relates to the observation that MINT can readily switch from decoding one parameter (e.g. cycling phase) to a very different one (muscle activity) with no need for additional training, an option not available with any other standard method.</p></sec><sec id="s3-2"><title>Practical considerations for use of MINT as an online decoder</title><p>Across the four datasets we tested, the best decoders (MINT and the two expressive methods) were all very close in terms of performance. Given this, decoder-choice would likely be based on considerations beyond raw performance. Here MINT has many advantages. Because MINT’s computations are so simple, it is typically fast to train and always very fast to run. Its interpretability supports principled extensions (discussed below) and the ability to investigate the source of any failures (as in <xref ref-type="fig" rid="fig6">Figure 6e</xref>). These considerations, combined with the fact that MINT often yielded the best behavioral decode of all methods, makes MINT a compelling candidate for online decoding in scientific or clinical applications. With that goal in mind, there exist three important practical considerations. First, some decode algorithms experience a performance drop when used online. One presumed reason is that, when decoding is imperfect, the participant alters their strategy which in turn alters the neural responses upon which decoding is based. Because MINT produces particularly accurate decoding, this effect may be minimized, but this cannot be known in advance. If a performance drop does occur, one could adapt the known solution of retraining using data collected during online decoding (<xref ref-type="bibr" rid="bib36">Gilja et al., 2012</xref>). Another presumed reason (for a gap between offline and online decoding) is that offline decoders can overfit the temporal structure in training data (<xref ref-type="bibr" rid="bib22">Deo et al., 2023</xref>). This concern is somewhat mitigated by MINT’s use of a short spike-count history, but MINT may nevertheless benefit from data augmentation strategies such as including time-dilated versions of learned trajectories in the libraries.</p><p>A second practical consideration is that, for a participant with limited ability to move, training would involve asking subjects to observe and/or attempt movements. Both strategies engage neurons in motor cortex (<xref ref-type="bibr" rid="bib104">Vargas-Irwin et al., 2018</xref>), but can also create uncertainty regarding the exact behavioral label for each moment (a challenge any decode method must face). MINT’s decoding performance benefits from good estimates of <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and will decline if those estimates are poor or misaligned. Fortunately, methods exist for temporally aligning neural data based on spiking activity alone (<xref ref-type="bibr" rid="bib114">Williams et al., 2020</xref>) and the performance of such methods should only improve as more neurons are simultaneously recorded. Such methods would allow computation of trial-averaged rates (or single-trial rates) that could then be aligned with a canonical movement trajectory (e.g. the usual bell-shaped velocity profile). Conveniently, any alterations to that alignment are trivial to accomplish, including at runtime. For example, to shift velocity earlier, one would simply shift MINT’s library indices. More generally, MINT is amenable to a variety of methods – simple or sophisticated – for inferring the trajectory library. Possible future approaches include leveraging the typical neural geometry for a particular task. This could allow a participant-specific library to be learned using fewer trials. This might even be done in an unsupervised way (similar to <xref ref-type="bibr" rid="bib25">Dyer et al., 2017</xref>): a trajectory library could be built without behavioral labels, then appropriately linked to a ‘reference’ behavioral library via prior knowledge of task-specific neural geometry. As a simple example, cycling across different speeds produces a tube-shaped manifold with different velocities at each end (<xref ref-type="bibr" rid="bib84">Saxena et al., 2022</xref>). An empirically observed tube might be given behavioral labels based on this knowledge.</p><p>A final practical consideration concerns generalization. Because MINT uses a library of canonical trajectories, it is tempting to suppose that its decoded trajectories will simply be replicas of a limited set of stereotyped behaviors. By removing the interpolation step, MINT can indeed approximate this mode, which would be desirable in situations where the to-be-decoded behavior is itself stereotyped (e.g. producing keystrokes or handwritten characters). Yet one often wishes to decode behaviors beyond those observed in the training set. MINT can do so if the empirical neural trajectory mostly (and locally) obeys the flow-field implied by the library. This provides considerable, but not unlimited, flexibility. Might MINT thus be at a disadvantage, relative to other methods, with respect to generalization? This is predicted to be true under the hypothesis in <xref ref-type="fig" rid="fig1">Figures 1a</xref> and <xref ref-type="fig" rid="fig6">6a</xref>. However, it is not expected to be true under the hypothesis in <xref ref-type="fig" rid="fig1">Figures 1b</xref> and <xref ref-type="fig" rid="fig6">6b</xref>. Under that hypothesis, the forms of generalization that are unavailable to MINT will also be unavailable to standard methods. Consistent with that prediction, generalization success for the MC_PacMan dataset was condition-specific, not decoder-specific.</p></sec><sec id="s3-3"><title>Multi-task decoding</title><p>The fact that generalization may often be challenging does not imply that decoding across multiple tasks is a challenge. Indeed MINT is ideally suited for this situation. For example, MINT successfully decoded motor output for both tasks performed by the network in <xref ref-type="fig" rid="fig4">Figure 4a</xref>, and across all conditions in the MC_PacMan dataset (when trained on all conditions, <xref ref-type="fig" rid="fig6">Figure 6c</xref>). As long as trajectories relevant to both tasks are present in the library, ‘task switching’ occurs naturally and implicitly. Because MINT does not rely on correlations between neural activity and behavioral variables, task switching will not be impaired if those correlations change across tasks, as is anticipated under <xref ref-type="fig" rid="fig1">Figures 1b</xref> and <xref ref-type="fig" rid="fig6">6b</xref>. As the number of situations decoders are expected to handle increases, this may be a useful property.</p></sec><sec id="s3-4"><title>Future avenues</title><p>MINT’s probability-based framework is amenable to principled extensions. Rather than simply decode the state with the highest data likelihood, one could incorporate priors. Priors could be fixed or guided by incoming information (e.g. an eye tracker). Decoding could also focus on utility functions rather than probabilities. This would respect the fact that a participant may value avoiding certain types of errors more than others. For example, erroneous transitions from stationary to moving may be jarring or unsafe, and could be dissuaded via an appropriately designed utility function. These extensions are natural because MINT computes data likelihoods for a broad range of candidate states before choosing the maximum-likelihood state. Converting likelihoods to probabilities (based on a prior) or to an estimated cost can thus be done in a principled manner.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>MINT (Part I): Estimating candidate states</title><sec id="s4-1-1"><title>Model of idealized trajectories</title><p>Let <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">S</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> be the measured spiking activity from <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons at time sample <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Spikes are associated with some underlying neural state <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and a corresponding behavioral state <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of behavioral variables (e.g. <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>- and <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-velocity of the hand). We bin spiking activity in time such that <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Given recent spiking history <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (spike counts from the most recently completed bin <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> previous bins), we are interested in inferring posterior distributions over neural states <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and over behavioral states <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We can then perform <italic>maximum</italic> a posteriori estimation to generate candidate estimates for both the neural state and behavior. We introduce a model of the form:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Unif</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Pois</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">→</mml:mo><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a state-transition lookup table, <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a lookup table that associates neural states with behavioral states, <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of samples elapsed since the last time bin completed, and <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates the extent of the deterministic history for <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (matching the extent of recent spiking history). This model has parameters <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. These parameters are described below, along with definitions for <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p><inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is a set (library) of <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> neural trajectories, with each trajectory consisting of an ordered set of neural states. Each neural state in the library is notated as <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf175"><mml:mi>c</mml:mi></mml:math></inline-formula> indexes trajectories and <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> indexes locations along each trajectory (e.g. <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates the 100th neural state along the second trajectory in the library). Each trajectory is presumed to contain the sequence of neural states that are traversed for a particular behavior. Thus, <inline-formula><mml:math id="inf179"><mml:mi>k</mml:mi></mml:math></inline-formula> increasing along a neural trajectory corresponds to the passage of time during the execution of a behavior. <xref ref-type="disp-formula" rid="equ2 equ3">Equations 2 and 3</xref> indicate that each <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> has a history of <inline-formula><mml:math id="inf181"><mml:mi>τ</mml:mi></mml:math></inline-formula> states that are responsible for generating recent spike count observations. However, the first <inline-formula><mml:math id="inf182"><mml:mi>τ</mml:mi></mml:math></inline-formula> states along each neural trajectory lack a complete state history. Thus, we learn the full <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, but in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> we only consider a reduced set of neural states, <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>∣</mml:mo><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, because only these states have the necessary state histories. The state-transition lookup table is defined as follows,<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>simply indicating that each neural state in the library has a recent history determined by the stereotyped ordering of neural states within each neural trajectory.</p><p><inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a set (library) of <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> behavioral trajectories, where each trajectory consists of an ordered set of behavioral states. Each <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is associated with a behavioral state, <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, via<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>In other words, each neural state in the library is paired with a behavioral state for the same <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Once the libraries of neural and behavioral trajectories have been learned, MINT’s parameters are fully learned. <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will be determined by the ordering of states within each trajectory and <inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> will be determined by each state’s <inline-formula><mml:math id="inf193"><mml:mi>c</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> indices. There are a variety of methods one can employ for learning these trajectories. The methods used in this paper are described in the “Learning idealized trajectories” section. These trajectories can often be learned by averaging neural and behavioral data across repeated trials of the same movement, yielding <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> that are representative of the neural and behavioral states expected for a particular movement.</p><p>We assume <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> if and only if <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. (We similarly assume <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> if and only if <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.) This can be made trivially true by letting <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> be the first two behavioral variables. Thus, <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is invertible. This trick is mathematically convenient for subsequent derivations—it does not change the fact that multiple neural states can be associated with the same behavior as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></sec><sec id="s4-1-2"><title>Estimating candidate states</title><p>The prior in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> ensures that <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∉</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Every element of <inline-formula><mml:math id="inf208"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> can be written as <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> for some <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf211"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the prior in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is uniform over the states in <inline-formula><mml:math id="inf212"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> indicate that each <inline-formula><mml:math id="inf213"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> has only one possible recent history, a history that is specified by the stereotyped ordering of neural states in the <inline-formula><mml:math id="inf214"><mml:mi>c</mml:mi></mml:math></inline-formula>-th trajectory. Thus, the posterior probabilities over neural states in <inline-formula><mml:math id="inf215"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> do not need to marginalize over multiple potential state histories. Rather, the probability associated with each <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is proportional to the likelihood of recently observed spikes, <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, given the unique history of neural states associated with <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the posterior probabilities over neural states in <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> can be written<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∣</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∏</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mover><mml:mi>x</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf220"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> (i.e. Poisson likelihoods, as dictated by <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>), and <inline-formula><mml:math id="inf221"><mml:msub><mml:menclose notation="top" class="tml-overline"><mml:mi>s</mml:mi></mml:menclose><mml:mrow><mml:mi>n</mml:mi><mml:mo separator="true">,</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf222"><mml:msubsup><mml:menclose notation="top" class="tml-overline"><mml:mi>x</mml:mi></mml:menclose><mml:mrow><mml:mi>n</mml:mi><mml:mo separator="true">,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:math></inline-formula> are the <italic>n</italic>-th components of <inline-formula><mml:math id="inf223"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. Recall that <italic>δ</italic> is the number of time samples elapsed since the completion of the most recent time bin. A normalizing constant can convert <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> into full posterior probabilities. Computing this constant is straightforward because <inline-formula><mml:math id="inf225"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is finite.</p><p><xref ref-type="disp-formula" rid="equ4 equ6">Equations 4 and 6</xref> allow the posterior over behavioral states to be written in terms of the posterior over neural states. <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo>∉</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, but for behaviors in <inline-formula><mml:math id="inf228"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> the posterior probabilities are<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∣</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We can perform <italic>maximum</italic> a posteriori estimation on the log-transformed neural state posterior and read out the behavioral estimate directly.<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mi>argmax</mml:mi><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-1-3"><title>Lookup table of log-likelihoods</title><p>One could proceed with querying <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> for all <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, selecting the most probable neural state, then reading out the associated behavioral state with <xref ref-type="disp-formula" rid="equ11">Equation 11</xref>. However, one may often wish to speed up this process, e.g. to deploy in a real-time application. In this section (and the following two sections), we describe a procedure for approximating <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> with considerably reduced computational burden.</p><p>Notice that the log-likelihood term in <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> only varies as a function of spike count and rate. Spike counts are finite and firing rates have limited dynamic ranges, which can be discretized, so it is possible to precompute and store these log-likelihoods in a lookup table in memory. Suppose the dynamic range of rates is given by <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and we sample this range with <inline-formula><mml:math id="inf231"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> values such that <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mi>v</mml:mi><mml:mi>V</mml:mi></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf233"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">V</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf234"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi class="mathcal" mathvariant="script">V</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Every rate <inline-formula><mml:math id="inf235"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> can now be approximated by <inline-formula><mml:math id="inf236"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for some <inline-formula><mml:math id="inf237"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">V</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that minimizes the approximation error. If we define a lookup table with entries <inline-formula><mml:math id="inf238"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>λ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, then <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> can be rewritten<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:munder><mml:mi>argmax</mml:mi><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus, during training we can compute <inline-formula><mml:math id="inf239"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf240"><mml:mi>s</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf241"><mml:mi>v</mml:mi></mml:math></inline-formula>. Similarly, we can compute <inline-formula><mml:math id="inf242"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf244"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf245"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. This formulation ensures the costly process of computing log-likelihoods only needs to be performed once, during training.</p></sec><sec id="s4-1-4"><title>Recursive solution</title><p>The estimation procedure in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> can be made faster still by exploiting recursive structure. Although the notation used in this section to carefully document this recursion is verbose to avoid any ambiguity, the concept is quite simple: the model of stereotyped trajectories reuses many log-likelihood terms across decoding timesteps that needn’t be recomputed each time. Consider the following definitions.<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munderover><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ13">Equation 13</xref> states that <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the log-likelihood of the data given one is presently at moment <inline-formula><mml:math id="inf247"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in condition <inline-formula><mml:math id="inf248"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, can be decomposed into a sum of time-bin-specific log-likelihoods (<xref ref-type="disp-formula" rid="equ14">Equation 14</xref>). Those time-bin-specific log-likelihoods can again be decomposed into neuron-specific log-likelihoods that can be queried from a lookup table. The added case for non-positive <inline-formula><mml:math id="inf249"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is a minor implementation detail that covers edge cases (computing <inline-formula><mml:math id="inf251"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> shortly after spiking data becomes available or for a <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> near the beginning of a trajectory). <xref ref-type="disp-formula" rid="equ13">Equation 13</xref> can be restated recursively,<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>δ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with initial conditions <inline-formula><mml:math id="inf253"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. In other words, when new spiking data arrives (corresponding to <inline-formula><mml:math id="inf254"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), each new log-likelihood <inline-formula><mml:math id="inf255"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> can be computed by taking a previously computed log-likelihood associated with a nearby state, <inline-formula><mml:math id="inf256"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and updating it to remove a time-bin-specific log-likelihood associated with old data and add a time-bin-specific log-likelihood associated with new data. Given that <inline-formula><mml:math id="inf257"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is simply shifted by one index relative to the previous time step when <inline-formula><mml:math id="inf258"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (i.e. no new spiking data is available, so state estimates are advanced deterministically using the model of stereotyped trajectories), the state estimate can be written<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:munder><mml:mi>argmax</mml:mi><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>δ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf259"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf260"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are the indices such that <inline-formula><mml:math id="inf261"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Note that <inline-formula><mml:math id="inf262"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> cannot be computed until <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (when sufficient spiking history has been collected).</p></sec><sec id="s4-1-5"><title>Querying fewer states</title><p>The approach described in <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> is efficient insofar as it renders a new estimate at each time <inline-formula><mml:math id="inf264"><mml:mi>t</mml:mi></mml:math></inline-formula> while only requiring substantial computations every <inline-formula><mml:math id="inf265"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> time samples. Nevertheless, when <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the recursion requires computing new <inline-formula><mml:math id="inf267"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo separator="true">,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf268"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo lspace="0em" rspace="0em" class="tml-prime">′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo separator="true">,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:math></inline-formula> for every element in <inline-formula><mml:math id="inf269"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="normal">Ω</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> to ensure that <inline-formula><mml:math id="inf270"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined for every <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, we typically assume some smoothness in neural trajectories over time. Thus, for reasonably small <inline-formula><mml:math id="inf272"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, we can approximate the most likely neural state when <inline-formula><mml:math id="inf273"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> by only considering a subset of neural states defined by<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mover><mml:mi mathvariant="bold">x</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtext> </mml:mtext><mml:mfrac><mml:mi>k</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi></mml:mfrac><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">Z</mml:mi></mml:mrow></mml:mrow><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which is equivalent to downsampling the neural trajectories in <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with <inline-formula><mml:math id="inf275"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> spacing between samples. Letting <inline-formula><mml:math id="inf276"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>k</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, this choice yields a new expression for the neural state estimate,<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:munder><mml:mi>argmax</mml:mi><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>δ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf277"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The recursion can now be performed over fewer states.<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>This simplification reduces memory and execution time. Furthermore, by using interpolation (described in the next section) we can still estimate neural states outside of <inline-formula><mml:math id="inf278"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-1-6"><title>Generating estimates at higher-than-bin-width resolution</title><p><xref ref-type="disp-formula" rid="equ18">Equation 18</xref> describes a procedure for generating new neural state estimates at every time sample. When <inline-formula><mml:math id="inf279"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the estimate is updated using new spiking observations. When <inline-formula><mml:math id="inf280"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the estimate is updated using the model of stereotyped trajectories, which specifies which neural state is likely to come next even in the absence of new spiking observations. However, to implement these equations in real time requires that the most time-consuming computations (those performed when <inline-formula><mml:math id="inf281"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) be performed within one time sample (e.g. 1 ms). MINT is very fast and in many cases will be capable of generating estimates in under a millisecond (<xref ref-type="table" rid="table1">Table 1</xref>). When this isn’t possible, a small decoding lag can be introduced (e.g. 5 ms) such that the computations required when <inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> can begin prior to needing the result.</p></sec></sec><sec id="s4-2"><title>MINT (Part II): Interpolation</title><p>The algorithm described above leverages a library of neural trajectories composed of discrete sequences of neural states typically observed for a discrete set of conditions. These trajectories are presumed to sample an underlying neural manifold that is fundamentally continuous. The library of neural trajectories will more finely sample that manifold if one uses a large <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> such that small variations in behavior each get their own neural and behavioral trajectories. Additionally, recall from the “Querying fewer states” section that, while decoding, only a subset of neural states (spaced apart by <inline-formula><mml:math id="inf284"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) are considered. The larger <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is, the more likely it is that the neural state estimate will make a small error in time (estimating a neural state that is slightly ahead or slightly behind the true state along a trajectory). Thus, neural state estimation can be more accurate in time and reflect more behavioral variability if one uses a small <inline-formula><mml:math id="inf286"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and a large <inline-formula><mml:math id="inf287"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, the computational burden of the algorithm grows as <inline-formula><mml:math id="inf288"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> shrinks or <inline-formula><mml:math id="inf289"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> grows. (The training data requirement also increases with <inline-formula><mml:math id="inf290"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). This creates a potential trade-off between performance and computational burden. However, MINT side-steps this trade-off by using linear interpolation between states.</p><p>When the neural manifold is presumed to smoothly vary between neural states (across time or conditions) and there is also smooth variation between the corresponding behavioral states, it is unnecessary to use an overly small <inline-formula><mml:math id="inf291"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and/or large <inline-formula><mml:math id="inf292"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Instead, one can identify candidate states along a small set of neural trajectories that coarsely sample the manifold, and then interpolate between those candidate states to find an intermediate state that is more consistent with the observed spikes. This two-step procedure (identify candidate states and then interpolate) allows the neural state estimate (and corresponding behavioral state estimate) to be accurate in time and capture variability between conditions (effectively as though a smaller <inline-formula><mml:math id="inf293"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and larger <inline-formula><mml:math id="inf294"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> had been used) while preserving the computational benefits of a reasonable <inline-formula><mml:math id="inf295"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (e.g. 20 ms) and small <inline-formula><mml:math id="inf296"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><sec id="s4-2-1"><title>Model of interpolated states</title><p>Suppose we identify two neural states, <inline-formula><mml:math id="inf297"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and we’d like to consider the possibility that the actual neural state is somewhere between them. We can use a model of the following form:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Unif</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>Pois</mml:mtext></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>This model assumes that as the neural state smoothly varies from <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf300"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, the corresponding behavior smoothly varies from <inline-formula><mml:math id="inf301"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-2-2"><title>Interpolating between states</title><p>Explicitly evaluating the probabilities associated with all <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in this model would be computationally inefficient. Given that the prior on <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is uniform over the range [0, 1], we can simply select the <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that maximizes the log-likelihood of the observed spikes. The log-likelihood of the observed spikes is given by the equation:<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo>˘</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Differentiating <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo>˘</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> twice with respect to <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> yields<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo>˘</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mover><mml:mi>s</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Notice that <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo>˘</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> always,  <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>q</mml:mi><mml:mo>˘</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a concave function of <inline-formula><mml:math id="inf310"><mml:mi>α</mml:mi></mml:math></inline-formula>. Thus, we can rapidly compute <inline-formula><mml:math id="inf311"><mml:mover><mml:mi>α</mml:mi><mml:mo stretchy="false" class="tml-xshift" style="math-style:normal;math-depth:0;">^</mml:mo></mml:mover></mml:math></inline-formula> using Newton’s method and then estimate <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> via <xref ref-type="disp-formula" rid="equ21 equ23">Equations 21 and 23</xref>. This procedure will yield the same <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>α</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf315"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> associated with the same <inline-formula><mml:math id="inf316"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the interpolation only needs to be performed once per time bin, when <inline-formula><mml:math id="inf317"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. In this paper, we used the following stopping criteria for Newton’s method: (1) the estimate of <inline-formula><mml:math id="inf318"><mml:mi>α</mml:mi></mml:math></inline-formula> is within 0.01 of the estimate at the previous iteration, (2) the estimate of <inline-formula><mml:math id="inf319"><mml:mi>α</mml:mi></mml:math></inline-formula> saturates at 0 or 1, or (3) optimization runs for 10 iterations.</p></sec><sec id="s4-2-3"><title>Interpolating across indices</title><p><xref ref-type="disp-formula" rid="equ17">Equation 17</xref> restricts the neural states under consideration to be spaced apart by <inline-formula><mml:math id="inf320"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> indices. Thus, a straightforward use of the interpolation scheme described above is to interpolate between nearby neural states along the same trajectory to restore the precision in the estimate that was lost by this restriction. First, a candidate neural state, which we’ll denote <inline-formula><mml:math id="inf321"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, is identified using <xref ref-type="disp-formula" rid="equ18">Equation 18</xref>. Then, a second candidate neural state is identified, which we’ll denote <inline-formula><mml:math id="inf322"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. This second state is whichever of the two adjacent states (either <inline-formula><mml:math id="inf323"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf324"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>) has a larger log-likelihood of the observed spikes. Then, interpolation between <inline-formula><mml:math id="inf325"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf326"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is performed as in the “Interpolating between states” section to yield neural and behavioral state estimates <inline-formula><mml:math id="inf327"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf328"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This form of interpolation is less about generalization and more about enabling <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> to improve computational efficiency without loss of precision in the neural state estimates along each trajectory.</p></sec><sec id="s4-2-4"><title>Interpolating across conditions</title><p>To interpolate across conditions, we also identify candidate neural states. The first candidate neural state, <inline-formula><mml:math id="inf329"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, is again identified using <xref ref-type="disp-formula" rid="equ18">Equation 18</xref>. The second candidate neural state, <inline-formula><mml:math id="inf330"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, is identified as the neural state that maximizes the log-likelihood of the observed spikes but is not a state along the same trajectory as <inline-formula><mml:math id="inf331"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> (i.e. <inline-formula><mml:math id="inf332"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). Next, both <inline-formula><mml:math id="inf333"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are each interpolated across indices with their adjacent states to yield improved candidate neural and behavioral state estimates. Then, an additional interpolation is performed between the improved candidate state estimates to yield the condition-interpolated neural and behavioral state estimates <inline-formula><mml:math id="inf335"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf336"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This allows for generalization between similar behaviors.</p></sec><sec id="s4-2-5"><title>Other forms of interpolation</title><p>The exact manner in which interpolation is applied during MINT may vary by task or dataset. Often, interpolation across indices and conditions will be sufficient. However, in this section, we will review some alternative interpolation implementations that were used for various analyses throughout this paper.</p><p>First, a dataset like MC_RTT that lacks explicit condition structure will not necessarily have one trajectory per behavior. We used AutoLFADS to learn neural trajectories for MC_RTT, which would have yielded a single neural trajectory for the entire training set were it not for recording gaps that broke up that trajectory. Thus, different portions of the same trajectory frequently corresponded to similar movements, such that it made sense to consider interpolations between them. We therefore modified interpolation such that the second candidate state could be on the same trajectory as the first candidate state, but it had to be at least 1000 milliseconds away from that first state. Additionally, in a task with many degrees of behavioral freedom, there may be many trajectory fragments near a candidate neural state that could be worthwhile to interpolate to. Thus, for the MC_RTT dataset we expanded to multiple candidate neural states (6 for decoding analyses, 5 for the neural state estimation analysis; these values were optimized as hyperparameters). Interpolation occurred between all pairs of candidate neural states. Then, we proceeded to only use the interpolation that yielded the highest spike count log-likelihoods. These modifications ensured that utilizing long trajectories spanning multiple behaviors in a reaching task with many degrees of behavioral freedom would not limit the ability of interpolation to generalize between similar behaviors.</p><p>Second, in DMFC_RSG we computed multiple libraries of neural trajectories, each library corresponding to a different section of the training data in which the overall firing rate statistics differed due to recording instabilities throughout the session. For this dataset, we wished to expand interpolation to occur across indices, conditions, <italic>and</italic> libraries. We simply extended the approach for interpolating across conditions one step further. The first candidate neural state was selected as the most likely neural state across all libraries. The second candidate neural state was selected as the most likely neural state in any library except the one in which the first candidate state resided. For each of these two candidate states, within-library interpolation across indices and conditions proceeded to improve the estimates. Then, a final interpolation occurred between the best estimate in the first library with the best estimate in the second library. This approach enabled MINT to robustly estimate the neural state even when fed test data from various sections of the dataset in which different recording instabilities were present.</p><p>Third, in MC_Cycle we decoded a circular variable: phase. To accommodate this circularity, phase interpolations were modified such that they always occurred across the lesser angle between the two phases. For example, if two candidate states corresponding to phases of 10 degrees and 350 degrees were identified, interpolation occurred across the 20 degree difference rather than the 340 degree difference.</p></sec></sec><sec id="s4-3"><title>Datasets</title><p>We analyzed 9 empirical datasets (Area2_Bump, DMFC_RSG, MC_Cycle, MC_Maze, MC_Maze-L, MC_Maze-M, MC_Maze-S, MC_RTT, MC_PacMan), several simulated maze datasets, and an artificial multitask spiking network. For the MC_Cycle and MC_PacMan datasets, animal protocols were approved by the Columbia University Institutional Animal Care and Use Committee (Protocol number AC-AABE3550). For other datasets, protocols were approved by the relevant institutional committee for that institution. All empirical datasets contained spike times from offline or online sorted units. Detailed descriptions of all empirical datasets (except MC_Cycle and MC_PacMan) can be found in <xref ref-type="bibr" rid="bib69">Pei et al., 2021</xref>. Here, we briefly describe each of the datasets.</p><sec id="s4-3-1"><title>Area2_Bump</title><p>This dataset contains neural recordings (96-electrode Utah array) from Brodmann’s area 2 of S1 (a proprioceptive area) while a monkey used a manipulandum to make center-out-reaches to one of eight targets. On some trials, the monkey volitionally reached to the targets (‘active’ trials). On other trials, the manipulandum perturbed the monkey’s arm out toward one of the targets (‘passive’ trials). Thus, right after movement onset the ‘active’ trials involved predictable sensory feedback (the monkey planned the movement) and the ‘passive’ trials involved unexpected sensory feedback (the monkey was not planning to move). Between the eight targets and two trial types (‘active’ and ‘passive’), there were a total of 16 conditions. The behavioral data for this dataset includes: hand position (x- and y-components), hand velocity (x- and y-components), forces and torques applied to the manipulandum (x- y- and z-forces; x- y- and z-torques), 7 estimated joint angles, 7 estimated joint velocities, 39 estimated muscle lengths, and 39 estimated muscle velocities. Position data was zeroed at movement onset for each trial to ensure position data was relative to a pre-movement baseline. More information on the task and data can be found in <xref ref-type="bibr" rid="bib14">Chowdhury et al., 2020</xref>.</p></sec><sec id="s4-3-2"><title>DMFC_RSG</title><p>This dataset contains neural recordings (three Plexon probes) from dorsomedial frontal cortex while a monkey performed a time-interval reproduction task (‘Ready-Set-Go’). In this task, the monkey received two visual cues, ‘Ready’ and ‘Set’, separated in time by a sample interval. The monkey was required to estimate the sample interval and report this estimate by performing an action (‘Go’) that followed the ‘Set’ cue by a production interval. The monkey was rewarded depending on how close the production interval was to the sample interval on a given trial. The sampling interval was selected on each trial from one of two prior distributions. The short prior distribution contained sampling intervals of 480, 560, 640, 720, and 800 ms. The long prior distribution contained sampling intervals of 800, 900, 1000, 1100, and 1200 ms. Trials were selected from prior distributions in blocks (i.e. many consecutive trials were drawn from the same prior distribution). Depending on the trial, the action could be reported in one of four ways: a joystick movement or an eye saccade to either the left or right. Between the two prior distributions, five sampling intervals per prior distribution, and four actions, there were a total of 40 conditions. More information on the task can be found in <xref ref-type="bibr" rid="bib94">Sohn et al., 2019</xref>.</p></sec><sec id="s4-3-3"><title>MC_Cycle</title><p>This dataset contains neural recordings (two 96-electrode Utah arrays) from motor cortex (M1 and PMd) while a monkey performed a cycling task to navigate a virtual environment. Offline sorting of spike waveforms (using Kilosort: <xref ref-type="bibr" rid="bib66">Pachitariu et al., 2024</xref>) yielded single-neuron and high-quality multi-neuron isolations. The dataset also includes threshold crossings that were detected online. On each trial, the monkey grasped a hand-pedal and moved it cyclically forward or backward to navigate the virtual environment. The color of the virtual landscape cued the monkey as to whether forward or backward cycling would advance their location in the virtual environment (‘green’ landscape: forward cycling advanced their location; ‘tan’ landscape: backward cycling advanced their location). Cycling distance was cued by a virtual target that appeared a fixed distance away in the virtual environment. The number of complete cycles of pedaling required to reach the target was either 1, 2, 4, or 7 cycles. Between the two cycling directions and four cycling distances, there were a total of 8 conditions. The behavioral data for this dataset includes: pedal phase, angular velocity of the pedal, x- and y- pedal position, x- and y- pedal velocity, and 7 intramuscular EMG recordings. The dataset includes both sorted spikes and threshold crossings. More information on the task can be found in <xref ref-type="bibr" rid="bib86">Schroeder et al., 2022</xref>.</p></sec><sec id="s4-3-4"><title>MC_Maze</title><p>This dataset contains neural recordings (two 96-electrode Utah arrays) from motor cortex (M1 and PMd) while a monkey performed straight and curved reaches. The monkey was presented with a maze on a vertical screen with a cursor that tracked the motion of the monkey’s hand. Targets appeared on the screen and the monkey had to make reaches that would move the cursor onto the target. Virtual barriers were presented that the cursor could not pass through, requiring curved reaches that avoided the barriers. Each maze configuration had three variants. The first variant had a single target with no barriers (straight reach). The second variant had a single target with barriers (curved reach). The third variant had a target with barriers (curved reach) as well as two unreachable distractor targets elsewhere in the maze. There were 36 maze configurations for a total of 108 conditions. The behavioral data for this dataset includes x- and y-components of hand position and velocity. Position data was zeroed at movement onset for each trial to ensure position data was relative to a pre-movement baseline. More information on the task can be found in <xref ref-type="bibr" rid="bib17">Churchland et al., 2010</xref>.</p></sec><sec id="s4-3-5"><title>MC_Maze-(L,M,S)</title><p>These three datasets were collected from the same monkey as in MC_Maze, performing the same task, but on different days with different conditions. The same arrays were used to collect neural recordings (though different numbers of sorted units were identified in each session) and the same behavioral data was collected. These datasets differ from MC_Maze primarily in that they have fewer conditions (nine maze configurations with three variants, for a total of 27 conditions) and fewer trials.</p></sec><sec id="s4-3-6"><title>MC_RTT</title><p>This dataset contains neural recordings (96-electrode Utah array) from motor cortex while a monkey made reaches in a horizontal plane to targets in an 8 x 8 grid. The task had no explicit trial structure nor did it require that individual movements be repeated throughout the session. Rather, a target would appear in one of the 64 grid locations and the monkey would reach to that target. Then, a new target would appear at a different random grid location and the monkey would reach there. This process continued throughout the session, leading to a collection of reaches that varied in terms of reach direction, reach distance, and reach speed. Despite lacking meaningful trial structure, the session was nevertheless broken up into 600 ms ‘trial’ segments (hence the trial count listed in <xref ref-type="table" rid="table2">Table 2</xref>). The behavioral data for this dataset includes x- and y-components of finger position and velocity. Due to the structure of the training data, neural trajectories were learned in long stretches spanning multiple movements. Thus, the zeroing of position data at movement onset that was employed for other datasets would have led to discontinuities in the behavioral trajectories for this task. We could have decoded position with no zeroing – however, similar movements (with similar corresponding neural activity) could have very different starting and ending positions in this dataset. Thus, we decided to focus decoding analyses on velocity only. More information on the task can be found in <xref ref-type="bibr" rid="bib55">Makin et al., 2018</xref>.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Neuron, condition, and trial counts for each dataset.</title><p>For some datasets, there are additional trials that are excluded from these counts. These trials are excluded because they were only usable for Neural Latents Benchmark submissions due to hidden behavioral data and partially hidden spiking data (see “Neural Latents Benchmark” section).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Neurons</th><th align="left" valign="bottom">Conditions</th><th align="left" valign="bottom">Trials</th></tr></thead><tbody><tr><td align="left" valign="bottom">Area2_Bump</td><td align="left" valign="bottom">65</td><td align="left" valign="bottom">16</td><td align="left" valign="bottom">364</td></tr><tr><td align="left" valign="bottom">DMFC_RSG</td><td align="left" valign="bottom">54</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">1006</td></tr><tr><td align="left" valign="bottom">MC_Cycle</td><td align="left" valign="bottom">112</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">273</td></tr><tr><td align="left" valign="bottom">MC_Maze</td><td align="left" valign="bottom">182</td><td align="left" valign="bottom">108</td><td align="left" valign="bottom">2295</td></tr><tr><td align="left" valign="bottom">MC_Maze-L</td><td align="left" valign="bottom">162</td><td align="left" valign="bottom">27</td><td align="left" valign="bottom">500</td></tr><tr><td align="left" valign="bottom">MC_Maze-M</td><td align="left" valign="bottom">152</td><td align="left" valign="bottom">27</td><td align="left" valign="bottom">250</td></tr><tr><td align="left" valign="bottom">MC_Maze-S</td><td align="left" valign="bottom">142</td><td align="left" valign="bottom">27</td><td align="left" valign="bottom">100</td></tr><tr><td align="left" valign="bottom">MC_RTT</td><td align="left" valign="bottom">130</td><td align="left" valign="bottom">N/A</td><td align="left" valign="bottom">1080</td></tr><tr><td align="left" valign="bottom">MC_PacMan</td><td align="left" valign="bottom">128</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">362</td></tr><tr><td align="left" valign="bottom">Maze Simulations</td><td align="left" valign="bottom">182</td><td align="left" valign="bottom">108</td><td align="left" valign="bottom">2295</td></tr><tr><td align="left" valign="bottom">Multitask Network</td><td align="left" valign="bottom">1200</td><td align="left" valign="bottom">9</td><td align="left" valign="bottom">270</td></tr></tbody></table></table-wrap></sec><sec id="s4-3-7"><title>MC_PacMan</title><p>This dataset contains neural recordings (128 electrodes recorded simultaneously from a passive pre-production version of the Neuropixels 1.0-NHP probe) from motor cortex while a monkey generated forward isometric force against an immovable handle. The monkey was presented with a PacMan icon on a screen. PacMan’s vertical position corresponded to the force applied to the handle. A path of scrolling dots moved horizontally across the screen and the monkey was rewarded when PacMan intercepted the dots. The pattern of scrolling dots was used to cue various dynamic force profiles. The task contained eight conditions that all spanned approximately the same range of forces: four ramping force profiles (fast and slow, ascending and descending), three sinusoidal force profiles (0.25 Hz, 1 Hz, 3 Hz), and a chirp profile in which the frequency steadily increased throughout the trial from 0-3 Hz. More information on the task can be found in <xref ref-type="bibr" rid="bib57">Marshall et al., 2022</xref>.</p></sec><sec id="s4-3-8"><title>Maze task simulations</title><p>Several synthetic datasets were generated based on the maze task. These simulations were based on the MC_Maze dataset and matched the neuron, condition, and trial counts from MC_Maze. In all cases, the actual behavior from MC_Maze was used — spiking activity was the only component that was simulated.</p><p>To simulate firing rates, we first z-scored hand position, velocity, and acceleration (x- and y-components). Then, we simulated rates for each neuron as a random weighted sum of these z-scored kinematics with a constant offset. The weights and offsets were scaled such that the means and standard deviations of firing rates for the simulated neurons matched the means and standard deviations of the empirical trial-averaged rates in MC_Maze. In some simulations, we increased simulated firing rates by a factor of 5 or 10.</p><p>Simulated spiking activity was generated from simulated rates in one of two ways. In one simulation, spiking activity was simulated as a Poisson process, which corresponds to exponential-interval spiking. In other simulations, spiking activity was simulated with gamma-interval spiking. More specifically, we let the interval between spikes be drawn from a gamma distribution with a shape parameter <inline-formula><mml:math id="inf337"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and a rate parameter <inline-formula><mml:math id="inf338"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf339"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the simulated firing rate. For reference, exponential-interval spiking corresponds to a gamma distribution with <inline-formula><mml:math id="inf340"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf341"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The overall rate of spiking was matched regardless of whether exponential-interval or gamma-interval spiking was simulated, but spiking occurred more regularly given the same rate for the gamma-interval simulations.</p></sec><sec id="s4-3-9"><title>Multitask spiking network</title><p>A simulated multitask dataset was generated using the method described in <xref ref-type="bibr" rid="bib24">DePasquale et al., 2023</xref>. A spiking network was trained to generate posterior deltoid activity during reaching (eight reach conditions) and cycling (one cycling condition). In the network, there were 39 latent factors related to reaching and 12 latent factors related to cycling. The factors, and therefore the neural trajectories, for reaching and cycling were roughly orthogonal to one another. The network simulated 500 consecutive trials. A 135-trial stretch of this simulated dataset was designated as the test set (∼7.1 min of data). The training set was constructed by randomly selecting 15 trials per condition (135 training trials total) from the remaining trials.</p></sec></sec><sec id="s4-4"><title>Learning idealized trajectories</title><p>When training data includes repeated trials of the same behavior, neural and behavioral trajectories can be learned via standard trial-averaging. This procedure begins with filtering spikes to yield single-trial rates at millisecond resolution. Rates and behavioral variables are then aligned to behaviorally relevant events (e.g. movement onset). Single-trial rates are averaged across trials of the same condition to yield firing rates. The vector of firing rates at a given time defines a neural state. The collection of neural states within each condition defines a neural trajectory. Single-trial behavioral variables are similarly averaged across trials to yield behavioral states and trajectories.</p><p>While the above process is standard – it commonly forms the basis of scientific analyses – it may also require dataset-specific operations that reflect task structure and timing (e.g. one may need to align movements of slightly different durations). There also exist pre-processing and post-processing steps that can improve rate-estimates and have been used for this purpose in scientific analyses. These steps (which involve smoothing across conditions and neurons) can also improve decoding, and we thus employed them as appropriate. The sections below give complete dataset-specific details regarding how we computed firing rates. The broader point is that one should, for a given task-type, estimate rates in the way that makes the most sense given how the training data was collected, much as one would when analyzing the data scientifically.</p><sec id="s4-4-1"><title>Filtering, extracting, and temporally aligning data on each trial</title><p>Spiking activity for each neuron on each trial was temporally filtered with a Gaussian to yield single-trial rates. <xref ref-type="table" rid="table3">Table 3</xref> reports the Gaussian standard deviations <inline-formula><mml:math id="inf342"><mml:mi>σ</mml:mi></mml:math></inline-formula> (in milliseconds) used for each dataset. The optimal <inline-formula><mml:math id="inf343"><mml:mi>σ</mml:mi></mml:math></inline-formula> for a dataset may depend on neuron count, typical spike rate, number of trials, and the timescale with which rates change in any given task or brain area. Given these factors vary across datasets, the optimal <inline-formula><mml:math id="inf344"><mml:mi>σ</mml:mi></mml:math></inline-formula> is expected to vary as well. Thus, <inline-formula><mml:math id="inf345"><mml:mi>σ</mml:mi></mml:math></inline-formula> was treated as a hyperparameter and optimized per-dataset.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Hyperparameters for learning neural trajectories via standard trial-averaging.</title><p><inline-formula><mml:math id="inf346"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula> is the standard deviation of the Gaussian kernel used to temporally filter spikes. Trial-averaging Type I and Type II procedures are described in the “Averaging across trials” section. <inline-formula><mml:math id="inf347"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf348"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are the neural- and condition-dimensionalities described in the “Smoothing across neurons and/or conditions” section. ‘Full’ means that no dimensionality reduction was performed. Condition smoothing could not be performed for MC_Cycle or the multitask network because different conditions in these datasets are of different lengths (i.e. <inline-formula><mml:math id="inf349"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>K</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is not the same for all <inline-formula><mml:math id="inf350"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>c</mml:mi></mml:mstyle></mml:math></inline-formula>). (C) and (R) refer to the cycling and reaching trajectories, respectively, in the multitask network. <inline-formula><mml:math id="inf351"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf352"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf353"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> correspond to movement onset, movement offset, and the ‘go’ time in the ready-set-go task, respectively. In MC_PacMan, each force profile is padded with static target forces at the beginning and end. <inline-formula><mml:math id="inf354"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf355"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> mark the beginning and end of the non-padded force profile on each trial.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Trajectory Start</th><th align="left" valign="bottom">Trajectory End</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf356"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom">Trial Averaging</th><th align="left" valign="bottom"><inline-formula><mml:math id="inf357"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf358"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="middle">Area2_Bump</td><td align="left" valign="middle"><inline-formula><mml:math id="inf359"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 350</td><td align="left" valign="middle"><inline-formula><mml:math id="inf360"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 750</td><td align="left" valign="middle">25</td><td align="left" valign="middle">Type II</td><td align="left" valign="middle">Full</td><td align="left" valign="middle">Full</td></tr><tr><td align="left" valign="middle">DMFC_RSG</td><td align="left" valign="middle"><inline-formula><mml:math id="inf361"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 1950</td><td align="left" valign="middle"><inline-formula><mml:math id="inf362"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 750</td><td align="left" valign="middle">55</td><td align="left" valign="middle">Type I</td><td align="left" valign="middle">49</td><td align="left" valign="middle">17</td></tr><tr><td align="left" valign="middle">MC_Cycle</td><td align="left" valign="middle"><inline-formula><mml:math id="inf363"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 600</td><td align="left" valign="middle"><inline-formula><mml:math id="inf364"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 600</td><td align="left" valign="middle">30</td><td align="left" valign="middle">Type I</td><td align="left" valign="middle">Full</td><td align="left" valign="middle">N/A</td></tr><tr><td align="left" valign="middle">MC_Maze</td><td align="left" valign="middle"><inline-formula><mml:math id="inf365"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 500</td><td align="left" valign="middle"><inline-formula><mml:math id="inf366"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 700</td><td align="left" valign="middle">30</td><td align="left" valign="middle">Type II</td><td align="left" valign="middle">Full</td><td align="left" valign="middle">21</td></tr><tr><td align="left" valign="middle">MC_Maze-L</td><td align="left" valign="middle"><inline-formula><mml:math id="inf367"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 500</td><td align="left" valign="middle"><inline-formula><mml:math id="inf368"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 700</td><td align="left" valign="middle">35</td><td align="left" valign="middle">Type II</td><td align="left" valign="middle">Full</td><td align="left" valign="middle">16</td></tr><tr><td align="left" valign="middle">MC_Maze-M</td><td align="left" valign="middle"><inline-formula><mml:math id="inf369"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 500</td><td align="left" valign="middle"><inline-formula><mml:math id="inf370"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 700</td><td align="left" valign="middle">60</td><td align="left" valign="middle">Type II</td><td align="left" valign="middle">Full</td><td align="left" valign="middle">20</td></tr><tr><td align="left" valign="middle">MC_Maze-S</td><td align="left" valign="middle"><inline-formula><mml:math id="inf371"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 500</td><td align="left" valign="middle"><inline-formula><mml:math id="inf372"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 700</td><td align="left" valign="middle">85</td><td align="left" valign="middle">Type II</td><td align="left" valign="middle">120</td><td align="left" valign="middle">10</td></tr><tr><td align="left" valign="middle">MC_PacMan</td><td align="left" valign="middle"><inline-formula><mml:math id="inf373"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 1000</td><td align="left" valign="middle"><inline-formula><mml:math id="inf374"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 1000</td><td align="left" valign="middle">35</td><td align="left" valign="middle">Type II</td><td align="left" valign="middle">Full</td><td align="left" valign="middle">Full</td></tr><tr><td align="left" valign="middle">Maze Simulations</td><td align="left" valign="middle"><inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 500</td><td align="left" valign="middle"><inline-formula><mml:math id="inf376"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 700</td><td align="left" valign="middle">30</td><td align="left" valign="middle">Type II</td><td align="left" valign="middle">Full</td><td align="left" valign="middle">21</td></tr><tr><td align="left" valign="top">Multitask Network</td><td align="left" valign="top">(C) <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 1000<break/>(R) <inline-formula><mml:math id="inf378"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 800</td><td align="left" valign="top">(C) <inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 1000<break/>(R) <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 1000<break/></td><td align="left" valign="top">15</td><td align="left" valign="top">Type I</td><td align="left" valign="top">Full</td><td align="left" valign="top">N/A</td></tr></tbody></table></table-wrap><p>Next, single-trial rates and behavioral data were extracted on each trial relative to key trial events. For example, in the maze datasets, data was extracted beginning 500 ms prior to movement onset and ending 700 ms after movement onset. These extraction boundaries (listed in <xref ref-type="table" rid="table3">Table 3</xref>) determine when the neural trajectories begin and end. These boundaries should be set to ensure that the behaviors one is interested in decoding are represented in the library of trajectories. It is also important to have extra data at the beginning of each trajectory because the first <inline-formula><mml:math id="inf381"><mml:mi>τ</mml:mi></mml:math></inline-formula> indices on each trajectory cannot be selected as candidate states (due to lack of sufficient state history).</p><p>In the maze datasets, Area2_Bump, and MC_PacMan, there was no need to warp time – all movements of the same condition unfolded over similar timescales relative to movement onset. However, in the cycling dataset this was not the case. The duration between movement onset and offset was variable across trials of the same condition (e.g. a mere 5% difference in cycling speed can lead to a ~180 ms discrepancy in the time it takes to complete 7 cycles). Thus, data in MC_Cycle was time-warped using the procedure described in <xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>. In the DMFC_RSG dataset, the duration between the ‘set’ cue and ‘go’ action was variable across trials. Thus, uniform time-warping to match the average duration for each condition was used. More generally, one may find the time-warping technique in <xref ref-type="bibr" rid="bib114">Williams et al., 2020</xref> to be a useful tool. It is important that warping take place <italic>after</italic> filtering spikes, because warping raw spiking activity would change the neurons’ rates.</p><p>The result of filtering, extracting, and warping is that, for each condition <inline-formula><mml:math id="inf382"><mml:mi>c</mml:mi></mml:math></inline-formula>, single-trial rates can be formatted into a tensor <inline-formula><mml:math id="inf383"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf384"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of neurons, <inline-formula><mml:math id="inf385"><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the number of extracted time points on each trial, and <inline-formula><mml:math id="inf386"><mml:mi>R</mml:mi></mml:math></inline-formula> is the number of trials. Similarly, behavioral data can be formatted into tensors <inline-formula><mml:math id="inf387"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf388"><mml:mi>M</mml:mi></mml:math></inline-formula> is the number of behavioral variables.</p></sec><sec id="s4-4-2"><title>Averaging across trials</title><p>It is straightforward to average each <inline-formula><mml:math id="inf389"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> across trials yielding matrices <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We refer to this as Type I averaging. However, we found that trial averages (and decoding) were often improved if we first used a preprocessing step that minimized the contribution of outliers. We refer to this as Type II averaging.</p><p>For Type II averaging, all single-trial rates (<inline-formula><mml:math id="inf391"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf392"><mml:mi>c</mml:mi></mml:math></inline-formula>) are mean-centered and soft-normalized as described in <xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>. The offset and scaling factor for this step are computed based on Type I averaged rates. These rates are formatted into matrices <inline-formula><mml:math id="inf393"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. PCA is used to reduce the trial-dimensionality of the rates to 1 for each <inline-formula><mml:math id="inf394"><mml:mi>c</mml:mi></mml:math></inline-formula> via<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf395"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∈</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>R</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is a column-vector corresponding to the first principal component of <inline-formula><mml:math id="inf396"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then, we reverse the soft-normalization and mean-centering operations on <inline-formula><mml:math id="inf397"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and average across trials to get <inline-formula><mml:math id="inf398"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> (Type II). Average rates were very similar regardless of how they were computed, but Type II rates have the potential advantage that the preprocessing step tends to minimize the contribution of outliers, such as a single neuron having an unusually high rate on a single trial, at a time when all other neurons had typical rates. <xref ref-type="table" rid="table3">Table 3</xref> indicates for each dataset which trial-averaging procedure was used.</p><p>We average each <inline-formula><mml:math id="inf399"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> across trials yielding matrices <inline-formula><mml:math id="inf400"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Typically, no modified trial-averaging procedure is required for behavioral data. However, there are cases where a behavioral variable should not be averaged without some pre- and/or post-processing. For example, in MC_Cycle, phase is a circular variable and therefore can’t be linearly averaged. To accommodate this, we unwrapped phase, averaged across trials, then re-wrapped it.</p></sec><sec id="s4-4-3"><title>Smoothing across neurons and/or conditions</title><p>Scientific analyses sometimes improve estimates of firing rates by using dimensionality reduction to smooth across neurons or across conditions. Doing so reduces the impact of idiosyncratic events that are specific to one neuron or one condition. Smoothing across conditions can be particularly useful because it mitigates the typical tradeoff between collecting more conditions versus collecting more trials per condition.</p><p>To smooth across neurons, trial-averaged rates are mean-centered and soft-normalized, then concatenated across conditions into a matrix <inline-formula><mml:math id="inf401"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf402"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="true">∑</mml:mo><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. PCA is used to reduce the neural-dimensionality of the rates via<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf403"><mml:mrow><mml:mi>W</mml:mi><mml:mo>∈</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the top <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> principal components of <inline-formula><mml:math id="inf405"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>To smooth across conditions, <inline-formula><mml:math id="inf406"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is reformatted into a new <inline-formula><mml:math id="inf407"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf408"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the same for all <inline-formula><mml:math id="inf409"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (this form of smoothing cannot be applied when this is not the case). PCA is used to reduce the condition-­ dimensionality of the rates via<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf410"><mml:mrow><mml:mi>W</mml:mi><mml:mo>∈</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the top <inline-formula><mml:math id="inf411"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> principal components of <inline-formula><mml:math id="inf412"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Following the above, we reverse the soft-normalization and mean-centering operations on <inline-formula><mml:math id="inf413"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, rectify to ensure non-negative rates, and reformat the smoothed rates back into matrices <inline-formula><mml:math id="inf414"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Not all datasets benefited from neuron smoothing and/or condition smoothing. The smoothing dimensionalities used for each dataset are reported in <xref ref-type="table" rid="table3">Table 3</xref>.</p><p>After completing the previous steps (some combination of smoothing rates over time, trials, neurons, and/or conditions), there will be matrices of firing rates <inline-formula><mml:math id="inf415"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and behavioral data <inline-formula><mml:math id="inf416"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for each <inline-formula><mml:math id="inf417"><mml:mi>c</mml:mi></mml:math></inline-formula>. The neural and behavioral states comprising the idealized neural and behavioral trajectories are then directly defined by these matrices.<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula><disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-4-4"><title>Learning trajectories for MC_RTT</title><p>Neural trajectories were learned for MC_RTT using AutoLFADS (<xref ref-type="bibr" rid="bib51">Keshtkaran et al., 2022</xref>). Following the procedure described in <xref ref-type="bibr" rid="bib51">Keshtkaran et al., 2022</xref>, spiking activity in the training data was formatted into 600 ms segments with spike counts binned every 5 ms. Each segment overlapped with the previous segment by 200 ms and with the subsequent segment by 200 ms. AutoLFADS was then run twice on this formatted data (using 80/20 training and validation splits), yielding two sets of rate estimates that we then averaged across. Each AutoLFADS run yields slightly different results. Thus, we reasoned that averaging across runs would improve performance by reducing the impact of idiosyncratic rate variability that doesn’t show up consistently across runs. We chose not to average more than two runs due to the computational burden this would accrue for training.</p><p>After averaging across runs, the rates from each segment were stitched back together (via the weighted average described in <xref ref-type="bibr" rid="bib51">Keshtkaran et al., 2022</xref>) into long neural trajectories spanning many movements. The trajectories were then upsampled to 1 kHz via linear interpolation. If there had been no recording gaps, this procedure would have yielded a single neural trajectory. For the decoding analyses, there were 2 recording gaps and therefore 3 neural trajectories. For the neural state estimation analysis (which utilized a larger training set), there were 3 recording gaps and therefore 4 neural trajectories. In all cases, each trajectory contained ∼2.7 minutes of data. Although one might prefer trajectory boundaries to begin and end at behaviorally relevant moments (e.g. a stationary state), rather than at recording gaps, the exact boundary points are unlikely to be consequential for trajectories of this length that span multiple movements. If MINT estimates a state near the end of a long trajectory, its estimate will simply jump to another likely state on a different trajectory (or earlier along the same trajectory) in subsequent moments. Clipping the end of each trajectory to an earlier behaviorally-relevant moment would only remove potentially useful states from the libraries.</p><p>When running AutoLFADS for the neural state estimation analysis, one of the runs returned a highly oscillatory set of rates (∼25-40 Hz oscillations) that did not by eye resemble the output of the other AutoLFADS runs. Although we confirmed this set of rates performed similarly for neural state estimation (0.200 bits/spike using the oscillatory rates alone, compared to 0.201 bits/spike when averaging across two non-oscillatory runs), we nevertheless chose to discard this run and average across two non-oscillatory AutoLFADS runs because the oscillatory solution was not one that AutoLFADS consistently returns for this particular training set.</p><p>See the “Other forms of interpolation” section for details on how these long neural trajectories are used during interpolation to improve neural state estimation and decoding.</p></sec><sec id="s4-4-5"><title>Learning trajectories for DMFC_RSG</title><p>Learning neural trajectories for the DMFC_RSG dataset involved two challenges related to the specifics of this dataset and the nature of the comparisons being made amongst methods. First, we needed to accommodate that each trial contained multiple trial events requiring separate alignment. Second, we desired a procedure for creating multiple libraries of neural trajectories corresponding to different portions of the session in which different recording instabilities were present. The solutions we developed for each of these challenges are described below in full detail for completeness. The solution for addressing recording instabilities described in this section was sufficient for facilitating fair comparison between MINT and other methods in the Neural Latents Benchmark that can implicitly model recording instabilities. However, the approach used here is rather specific to this desired comparison, and to the DMFC_RSG dataset. If MINT were deployed for online decoding, a different method to correct for recording instabilities would likely be desired (<xref ref-type="fig" rid="fig8s2">Figure 8—figure supplement 2</xref>).</p><p>Each trial contained five main trial events: the ‘fixation’ time when the monkey began looking at the center of the screen, the ‘target onset’ time when a white target appeared in one of two locations on the screen, the ‘ready’ cue, the ‘set’ cue, and the ‘go’ time at which the monkey initiated a joystick movement or saccade (depending on the condition). Thus, when learning neural trajectories, we needed to compute rates for five trial epochs: fixation to target onset, target onset to ready cue, ready cue to set cue, set cue to go time, and go time to the end of the trial. The first three epochs could not be averaged across trials in the standard way (because the durations varied from one trial to the next), but it was also not appropriate to warp them. Warping should be performed when the monkey is executing a computation or behavior at a slightly variable speed. In this case, the monkey was simply waiting for the next trial event without knowledge of when it would arrive. Thus, we chose to align to the initial event in each epoch, average across trials (ignoring missing data associated with the variable epoch durations), and then trim the averaged rates to the median epoch duration. In this task, certain conditions are not disambiguated for the monkey until the set cue arrives (e.g. the monkey doesn’t know if they are in an 800 ms or 900 ms interval trial until the set cue arrives). Thus, averages for a given condition in these first three epochs included all trials from the given condition plus all trials from other conditions that could not be distinguished from the given condition at this stage of the trial. In the set cue to go time epoch, rates were uniformly warped to match the average duration within each condition and then averaged across trials. All data after the go time were aligned to the go time and averaged. After computing these epoch-specific rate averages, the rates were concatenated across epochs to yield neural trajectories for each condition. They were then trimmed according to the trajectory start and end times listed in <xref ref-type="table" rid="table3">Table 3</xref>.</p><p>With access to a very large dataset, computing multiple libraries of neural trajectories corresponding to different session epochs (e.g. beginning of session vs. end of session) is straightforward: simply break the training data up in time into different sections and separately compute a library of trajectories for each. These libraries can then reflect how rates vary throughout a session due to recording instabilities. However, given limited training data, we sought to implement a similar strategy while not limiting the trajectories in each section to be based only on the small amount of spiking data available in that section. Thus, we first learned a library of neural trajectories based on the entire session (following the procedure described above) and smoothed across neurons and conditions to improve this estimate (yielding <inline-formula><mml:math id="inf418"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for each condition <inline-formula><mml:math id="inf419"><mml:mi>c</mml:mi></mml:math></inline-formula>). Then, we broke up the session into 6 consecutive sections and proceeded to learn a complete set of neural trajectories for each section with no smoothing across neurons or conditions (yielding <inline-formula><mml:math id="inf420"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for each condition <inline-formula><mml:math id="inf421"><mml:mi>c</mml:mi></mml:math></inline-formula> and section <inline-formula><mml:math id="inf422"><mml:mi>d</mml:mi></mml:math></inline-formula>). Finally, we learned a linear transformation of <inline-formula><mml:math id="inf423"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for each section <inline-formula><mml:math id="inf424"><mml:mi>d</mml:mi></mml:math></inline-formula> that would better match the transformed firing rates for each section to those in <inline-formula><mml:math id="inf425"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> while preserving some of the neural geometry learned at the level of the session. This occurred via the equation<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi mathvariant="normal">Λ</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf426"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>d</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf427"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>d</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf428"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow><mml:mo>∈</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf429"><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow></mml:math></inline-formula> is simply a diagonal matrix whose action on <inline-formula><mml:math id="inf430"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is to soft-normalize. The values in <inline-formula><mml:math id="inf431"><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi></mml:mrow></mml:math></inline-formula> are learned from <inline-formula><mml:math id="inf432"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> via the same soft-normalization procedure described in <xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref> and used previously in this paper. The linear transformation was formulated in <xref ref-type="disp-formula" rid="equ31">Equation 31</xref> to ensure that all regularization when learning the weights only applied to the deviation between the session-wide rates and the section-specific transformed rates. <inline-formula><mml:math id="inf433"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>d</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf434"><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo form="prefix" stretchy="false" lspace="0em" rspace="0em">(</mml:mo><mml:mi>d</mml:mi><mml:mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> were learned via an L2-regularized weighted least squares regression,<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>S</mml:mi><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula><disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf435"><mml:mi>S</mml:mi></mml:math></inline-formula> is a diagonal matrix of observation weights, <inline-formula><mml:math id="inf436"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean of <inline-formula><mml:math id="inf437"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> across columns, and <inline-formula><mml:math id="inf438"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the mean of <inline-formula><mml:math id="inf439"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Λ</mml:mi><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> across columns. In <xref ref-type="disp-formula" rid="equ34 equ35">Equations 34 and 35</xref>, the subtractions of <inline-formula><mml:math id="inf440"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf441"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are applied to each column of the matrices they subtract from. The diagonal entries of <inline-formula><mml:math id="inf442"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> weight how much each observation should matter when learning <inline-formula><mml:math id="inf443"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Given that the epoch between the set cue and the go time is when the monkey is performing an internal computation (keeping track of elapsed time internally) and this epoch is a large fraction of the evaluated trial period in the Neural Latents Benchmark, we decided <inline-formula><mml:math id="inf444"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> should prioritize fitting this epoch well. Thus, diagonal entries of <inline-formula><mml:math id="inf445"><mml:mi>S</mml:mi></mml:math></inline-formula> corresponding to samples in the set-go epoch were given a weight <inline-formula><mml:math id="inf446"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> whereas all other diagonal entries of <inline-formula><mml:math id="inf447"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> were set to 1. Both the set-go epoch weight and the L2 regularization term were treated as hyperparameters and optimized on a validation set, yielding <inline-formula><mml:math id="inf448"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf449"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. To summarize, <inline-formula><mml:math id="inf450"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> transforms the mean-centered, soft-normalized session-wide rates into a set of rate residuals that can be added to the session-wide rates to better match the firing rate statistics in each section of the data. The result of this procedure is a complete library of neural trajectories for each section of the training data <inline-formula><mml:math id="inf451"><mml:mi>d</mml:mi></mml:math></inline-formula>.<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mover><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>See the “Other forms of interpolation” section for details on how these multiple libraries are used during interpolation to improve the neural state estimate. There are no behavioral trajectories for this dataset (it was only used for neural state estimation analyses).</p></sec></sec><sec id="s4-5"><title>Visualizing neural trajectories</title><p>Neural trajectories have dimensionality matching the number of recorded neurons (or multi-units), but it is often useful to visualize them in a 2D state space. Throughout this paper, PCA is used to project neural trajectories into a low-dimensional subspace. When this is done, firing rates are preprocessed with mean-centering and soft-normalization (as in <xref ref-type="bibr" rid="bib79">Russo et al., 2018</xref>). In some cases, it is useful to rotate the data within the top PCs to find a perspective that highlights certain properties of the trajectories. When this is done, we report the neural variance captured by the plotted dimensions (along with the neural variance captured by the top PCs) to contextualize the scale of the neural dimensions. Percent neural variance captured was computed in the standard way, as described in <xref ref-type="bibr" rid="bib86">Schroeder et al., 2022</xref>.</p></sec><sec id="s4-6"><title>Distance analyses</title><p>In <xref ref-type="fig" rid="fig2">Figure 2c-e</xref>, several distance metrics were used to analyze the MC_Cycle dataset. Neural distance is defined as the Euclidean distance between two neural states in the full-dimensional space. Muscle distance is defined by z-scoring the ‘muscle state’ (the portion of the behavioral state corresponding to muscle activity), then computing the Euclidean distance between two normalized muscle states. The muscle state consisted of seven intramuscular EMG recordings (long head of the biceps, anterior deltoid, lateral deltoid, posterior deltoid, trapezius, and lateral and long heads of the triceps). Kinematic distance is defined using the phases and angular velocities associated with two behavioral states. Both phase and angular velocity are z-scored (for phase, this utilizes the circular standard deviation). Then, the phase distance (<inline-formula><mml:math id="inf452"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) is computed as the circular distance between the two normalized phases. The angular velocity distance (<inline-formula><mml:math id="inf453"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) is computed as the absolute difference between the two normalized angular velocities. Then, the kinematic distance is computed as <inline-formula><mml:math id="inf454"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. All trajectories (neural and behavioral) were binned at 10 ms resolution prior to computing pairwise distances to keep the analysis computationally manageable. In <xref ref-type="fig" rid="fig2">Figure 2e</xref>, data was partitioned into two trial sets (‘Partition A’ and ‘Partition B’) for a control analysis. To keep the number of trials used to compute trajectories matched across all analyses in <xref ref-type="fig" rid="fig2">Figure 2c-e</xref>, only trials from ‘Partition A’ were utilized in the neural distance vs. muscle distance and neural distance vs. kinematic distance analyses. When plotting pairwise distances, all distances were normalized (separately for each plotting axis) such that 1 corresponded to the average pairwise distance.</p><p>The neural distances reported in <xref ref-type="fig" rid="fig4">Figure 4d</xref> are on the same scale as those in <xref ref-type="fig" rid="fig2">Figure 2c-e</xref>, i.e. they are Euclidean distances between neural states normalized by the average pairwise distance between neural states in the library of trajectories.</p></sec><sec id="s4-7"><title>Decoding analyses</title><p>All decoding analyses utilized the train-test trial splits listed in <xref ref-type="table" rid="table4">Table 4</xref>. Neural and behavioral trajectories were learned from the training set as described in the “Learning idealized trajectories” section. Then, MINT was provided with spiking activity on test trials with which to decode behavior. The trial epochs over which performance was evaluated were set to match the evaluation epochs from the Neural Latents Benchmark (for datasets that were used in the benchmark) and are listed in <xref ref-type="table" rid="table4">Table 4</xref>.</p><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Details for decoding analyses.</title><p>The number of training and testing trials for each dataset are provided along with the evaluation period over which performance was computed. Generalization analyses used subsets of these training and testing trials. The window lengths refer to the amount of spiking history MINT used for decoding (e.g. when <inline-formula><mml:math id="inf455"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>14</mml:mn></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf456"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mstyle></mml:math></inline-formula> the window length is <inline-formula><mml:math id="inf457"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mstyle></mml:math></inline-formula> ms). <inline-formula><mml:math id="inf458"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> corresponds to movement onset, <inline-formula><mml:math id="inf459"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> corresponds to movement offset, <inline-formula><mml:math id="inf460"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> refers to the beginning of a trial, and <inline-formula><mml:math id="inf461"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> refers to the end of a trial. There is no defined condition structure for MC_RTT to use for defining trial boundaries. Thus, each trial is simply a 600 ms segment of data, with no alignment to movement. Although 270 of these segments were available for testing, the first 2 segments lacked sufficient spiking history for all decoders to be evaluated and were therefore excluded, leaving 268 test trials. In MC_PacMan, each force profile is padded with static target forces at the beginning and end. <inline-formula><mml:math id="inf462"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf463"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> mark the beginning and end of the non-padded force profile on each trial. For the multitask network, performance was evaluated on a continuous stretch of 135 trials spanning ∼7.1 minutes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Training Trials</th><th align="left" valign="bottom">Test Trials</th><th align="left" valign="bottom">Evaluation Start</th><th align="left" valign="bottom">Evaluation End</th><th align="left" valign="bottom">Window Length (ms)</th></tr></thead><tbody><tr><td align="left" valign="middle">Area2_Bump</td><td align="left" valign="middle">272</td><td align="left" valign="middle">92</td><td align="left" valign="middle"><inline-formula><mml:math id="inf464"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 100</td><td align="left" valign="middle"><inline-formula><mml:math id="inf465"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 500</td><td align="left" valign="middle">240</td></tr><tr><td align="left" valign="middle">MC_Cycle</td><td align="left" valign="middle">174</td><td align="left" valign="middle">99</td><td align="left" valign="middle"><inline-formula><mml:math id="inf466"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 250</td><td align="left" valign="middle"><inline-formula><mml:math id="inf467"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 250</td><td align="left" valign="middle">200</td></tr><tr><td align="left" valign="middle">MC_Maze</td><td align="left" valign="middle">1721</td><td align="left" valign="middle">574</td><td align="left" valign="middle"><inline-formula><mml:math id="inf468"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 250</td><td align="left" valign="middle"><inline-formula><mml:math id="inf469"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 450</td><td align="left" valign="middle">300</td></tr><tr><td align="left" valign="middle">MC_Maze-L</td><td align="left" valign="middle">375</td><td align="left" valign="middle">125</td><td align="left" valign="middle"><inline-formula><mml:math id="inf470"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 250</td><td align="left" valign="middle"><inline-formula><mml:math id="inf471"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 450</td><td align="left" valign="middle">300</td></tr><tr><td align="left" valign="middle">MC_Maze-M</td><td align="left" valign="middle">188</td><td align="left" valign="middle">62</td><td align="left" valign="middle"><inline-formula><mml:math id="inf472"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 250</td><td align="left" valign="middle"><inline-formula><mml:math id="inf473"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 450</td><td align="left" valign="middle">300</td></tr><tr><td align="left" valign="middle">MC_Maze-S</td><td align="left" valign="middle">75</td><td align="left" valign="middle">25</td><td align="left" valign="middle"><inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 250</td><td align="left" valign="middle"><inline-formula><mml:math id="inf475"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 450</td><td align="left" valign="middle">340</td></tr><tr><td align="left" valign="middle">MC_RTT</td><td align="left" valign="middle">810</td><td align="left" valign="middle">268</td><td align="left" valign="middle"><inline-formula><mml:math id="inf476"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="middle"><inline-formula><mml:math id="inf477"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 599</td><td align="left" valign="middle">480</td></tr><tr><td align="left" valign="middle">MC_PacMan</td><td align="left" valign="middle">234</td><td align="left" valign="middle">128</td><td align="left" valign="middle"><inline-formula><mml:math id="inf478"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 500</td><td align="left" valign="middle"><inline-formula><mml:math id="inf479"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 1000</td><td align="left" valign="middle">300</td></tr><tr><td align="left" valign="middle">Maze Simulations</td><td align="left" valign="middle">1721</td><td align="left" valign="middle">574</td><td align="left" valign="middle"><inline-formula><mml:math id="inf480"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> – 250</td><td align="left" valign="middle"><inline-formula><mml:math id="inf481"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> + 450</td><td align="left" valign="middle">300</td></tr><tr><td align="left" valign="middle">Multitask Network</td><td align="left" valign="middle">135</td><td align="left" valign="middle">135</td><td align="left" valign="middle"><inline-formula><mml:math id="inf482"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (first trial)</td><td align="left" valign="middle"><inline-formula><mml:math id="inf483"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (last trial)</td><td align="left" valign="middle">300</td></tr></tbody></table></table-wrap><p>MINT used a bin size of Δ = 20 ms. The amount of spiking history provided to MINT at each decoding time step varied by dataset and is listed in <xref ref-type="table" rid="table4">Table 4</xref>. Decoding was always causal (i.e. only utilizing spiking history prior to the decoded moment), with one exception described below in the “MINT variants” section. MINT utilized interpolation for all datasets. The lookup table of log-likelihoods utilized a minimum rate, <inline-formula><mml:math id="inf484"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, corresponding to 1 spike/second. Log-likelihoods in the table were also clipped so as not to drop below <inline-formula><mml:math id="inf485"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. These two choices regularize decoding such that a spurious spike from a neuron whose rate is close to 0 cannot dominate the decision of which neural state is most probable.</p><sec id="s4-7-1"><title>Decoding R<sup>2</sup></title><p>Decoded behavioral variables were compared to ground truth behavioral variables over the evaluation epoch of all test trials. Performance was reported as the coefficient of determination (<inline-formula><mml:math id="inf486"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>) for the decoded behavioral variables, averaged across behavioral variables within a behavioral group. For example, <inline-formula><mml:math id="inf487"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> for x-velocity and y-velocity were averaged to yield ‘velocity <inline-formula><mml:math id="inf488"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>’. When computing <inline-formula><mml:math id="inf489"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> for phase, circular distances and circular means were substituted for traditional distances and means in the <inline-formula><mml:math id="inf490"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> definition. For the neural networks, the training/testing procedure was repeated 10 times with different random seeds. For most behavioral variables, there was very little variability in performance across repetitions. However, there were a few outliers for which variability was larger. Reported performance for each behavioral group is the average performance across the 10 repetitions to ensure results were not sensitive to any specific random initialization of each network. Decoding <inline-formula><mml:math id="inf491"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> was always computed at 5 ms resolution (set to match the resolution used by the Neural Latents Benchmark).</p></sec><sec id="s4-7-2"><title>Decoder comparison</title><p>MINT was primarily compared to four other decode algorithms: Kalman filter, Wiener filter, feedforward neural network, and a recurrent neural network (GRU). An additional comparison to a Naive Bayes regression decoder was made for the MC_Maze dataset. All decoders were provided with the same training/testing data and used the same evaluation epochs. Complete details on their implementations are provided in the Appendix.</p></sec><sec id="s4-7-3"><title>Neuron dropping</title><p>The neuron dropping analyses in <xref ref-type="fig" rid="fig8">Figure 8b</xref> were performed on the MC_Maze-L dataset, which has 162 neurons. The analysis of decoding performance for known neuron loss consisted of training and testing the decoders with reduced sets of neurons. The analysis of performance for undetected neuron loss consisted of training and testing the decoder with all 162 neurons, but in the test set a subset of those neurons were artificially set to never spike. The analyses were performed for the following neuron counts: [1, 2, 3, 4, 5, 10, 15, ..., 150, 155, 160, 162]. At each neuron count, the known loss and undetected loss analyses were each repeated 50 times (with different neurons randomly dropped at each repetition). Linear interpolation between these neuron counts was then applied to generate <inline-formula><mml:math id="inf492"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> values for every neuron count between 1 and 162.</p></sec><sec id="s4-7-4"><title>MINT variants</title><p>The analysis in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a-c</xref> required training and testing several variations on MINT for Area2_Bump, MC_Cycle, MC_Maze, and MC_RTT. These variations are described here.</p><p>The first variation determines how behavior is decoded from the neural state estimate. The ‘Direct MINT Readout’ utilizes the direct association between neural and behavioral states that is standard for MINT. The ‘Linear MINT Readout’ begins by estimating the neural state using MINT, but behavior is then decoded as a weighted sum of the rates comprising the neural state estimate. The weights are learned from training data using ridge regression, with the L2 regularization term in the regression optimized via a grid search with 5-fold cross validation. To improve the quality of the fit, a lag between neural and behavioral states was included (lags set to match the values used in <xref ref-type="bibr" rid="bib69">Pei et al., 2021</xref>; MC_Cycle set to 100 ms).</p><p>The second variation considered whether interpolation was used. When interpolation was used, we employed the same methodology used for the analyses in <xref ref-type="fig" rid="fig5">Figure 5</xref> (6 candidate states for MC_RTT; 2 candidate states for other datasets). When interpolation was not used, the neural state was selected from the library of neural trajectories as the state that maximized the log-likelihood of the observed spikes.</p><p>The third variation determined whether decoding occurred causally or acausally. When decoding causally, the spiking observations all came prior to the decoded moment. When decoding acausally, the spiking observations were centered on the decoding moment. For the analysis in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2a-c</xref>, the extent of spiking observations (window length) was optimized separately for causal vs. acausal decoding to give each variant the opportunity to perform as well as possible. The causal window lengths are reported in <xref ref-type="table" rid="table4">Table 4</xref>. The acausal window lengths were 560 ms for Area2_Bump, 400 ms for MC_Cycle, 580 ms for MC_Maze, and 660 ms for MC_RTT.</p></sec><sec id="s4-7-5"><title>SNR criteria for threshold crossings</title><p>When decoding based on threshold crossings in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2d</xref>, a signal-to-noise ratio (SNR) was computed for each electrode channel. Then, only electrode channels with SNR &gt; 2 were utilized for decoding. The SNR was computed as the ratio of the range of firing rates (determined from trial-averaged rates) and the standard deviation of the firing rate residuals. The firing rate residuals on each trial were computed as the difference between the filtered spikes and the corresponding trial-averaged rates. If the standard deviation was less than 5 spikes/second, it was set to 5 spikes/second in the SNR computation. This ensured that channels with low variability in the residuals due to saturation at small or large firing rates were rejected. For example, an electrode channel that almost never spikes will have very low variability in the residuals simply because the firing rate is typically 0.</p></sec><sec id="s4-7-6"><title>Training non-MINT decoders with trial-averaged data</title><p>In the “Comparison to other decoders” section, it is reported that non-MINT decoders were trained with access to trial-averaged data and this did not improve decoding performance. This result comes from an analysis of the MC_Maze dataset. The training set was augmented to include two copies of each trial: the original trial, and an augmented trial in which spiking activity was replaced with trial-averaged rates for that trial’s condition (appropriately normalized such that binned rates matched the scale of binned spike counts).</p><p>Thus, during training, each method was exposed both to the original trials (whose neural data match the statistics of neural data expected during testing) and the augmented trials that, in theory, might benefit decoding by exposing the decoder to denoised rates (i.e. providing each method with access to the same trial-averaged data that MINT leverages). In practice, decoding performance declined for each of the non-MINT decoders when trained this way, presumably due to the mismatch it created between the statistics of inputs during training and testing.</p></sec></sec><sec id="s4-8"><title>Neural Latents Benchmark</title><p>All neural state estimation results in <xref ref-type="fig" rid="fig7">Figure 7</xref> came from the Neural Latents Benchmark (<ext-link ext-link-type="uri" xlink:href="https://neurallatents.github.io/">https://neurallatents.github.io/</ext-link>), a neural latent variable modeling competition released through the 2021 NeurIPS Datasets &amp; Benchmarks Track (<xref ref-type="bibr" rid="bib69">Pei et al., 2021</xref>). For each of seven datasets, the benchmark provided training data containing simultaneous spiking activity and behavioral measurements. The neurons in the training data were divided into two sets: held-in neurons and held-out neurons. For the test data, the spiking activity of the held-in neurons was provided, but the spiking activity of the held-out neurons and the behavioral data was kept private by the benchmark curators. These splits are provided in <xref ref-type="table" rid="table5">Table 5</xref>. The evaluation epochs used are the same as those used in <xref ref-type="table" rid="table4">Table 4</xref>. (For DMFC_RSG, the evaluation epoch was the 1500 ms leading up to the ‘go’ time.) In the test data, the held-in spiking activity was only provided for time points within the evaluation epochs. For each dataset, submissions consisted of rate estimates for every trial in the training and test sets (held-in rates and held-out rates). The benchmark was fundamentally acausal: rate estimates at a given moment could leverage spiking activity from the entire trial. The data in <xref ref-type="fig" rid="fig7">Figure 7</xref> reflect all benchmark submissions up through Feb. 24th, 2023.</p><sec id="s4-8-1"><title>MINT submissions</title><p>MINT first learned idealized neural trajectories during training as described in the “Learning idealized trajectories” section. Then, the neural trajectories were partitioned by neurons into two libraries: a held-in library of trajectories and a held-out library of trajectories. The held-in library contained neural states for the held-in neurons only. The held-out library similarly contained neural states for the held-out neurons only. MINT was run using the held-in library of trajectories to estimate a neural state in the held-in neural state space. A direct association between held-in neural states and held-out neural states (the same direct association typically used to map neural states to behavioral states) was then used to generate rate estimates for the held-out neurons. Rate estimates were saturated such that they could not be lower than 0.1 spikes/second. Interpolation was utilized across indices and conditions (with modifications for DMFC_RSG and MC_RTT as described in the “Other forms of interpolation” section).</p><p>The window length used for acausal neural state estimation on each dataset is provided in <xref ref-type="table" rid="table5">Table 5</xref>. Note that test set data was not provided outside of the evaluation epochs (an idiosyncracy of the benchmark). Thus, despite estimating rates acausally, the spiking observations often could not be centered on the estimated time. For example, when estimating the neural state for Area2_Bump, the window length was 500 ms, but the held-in spiking activity only spanned a 600 ms period. Thus, only neural states in the 250-350 ms portion of this 600 ms period could be estimated based on 500 ms of centered spiking activity. The neural states outside of this 250-350 ms zone were estimated by either propagating the earliest estimate backward or the latest estimate forward. This propagation is possible because each neural state estimate either occurs on a trajectory with a unique past and future or is an interpolation between trajectories that each have a unique past and future (i.e. the interpolation parameters are frozen and the states being interpolated are propagated backward or forward along their trajectories).</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Details for neural state estimation results.</title><p>Note that the training trial counts match the total number of trials reported in <xref ref-type="table" rid="table2">Table 2</xref>. This reflects that the Neural Latents Benchmark utilized an additional set of test trials not reflected in the <xref ref-type="table" rid="table2">Table 2</xref> trial counts. The test trials used for this analysis have ground truth behavior hidden by the benchmark creators and are therefore only suitable for this analysis.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Training Trials</th><th align="left" valign="bottom">Test Trials</th><th align="left" valign="bottom">Held-in Neurons</th><th align="left" valign="bottom">Held-out Neurons</th><th align="left" valign="bottom">Window Length (ms)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Area2_Bump</td><td align="left" valign="bottom">364</td><td align="left" valign="bottom">98</td><td align="left" valign="bottom">49</td><td align="left" valign="bottom">16</td><td align="left" valign="bottom">500</td></tr><tr><td align="left" valign="bottom">DMFC_RSG</td><td align="left" valign="bottom">1006</td><td align="left" valign="bottom">283</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">14</td><td align="left" valign="bottom">1500</td></tr><tr><td align="left" valign="bottom">MC_Maze</td><td align="left" valign="bottom">2295</td><td align="left" valign="bottom">574</td><td align="left" valign="bottom">137</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">500</td></tr><tr><td align="left" valign="bottom">MC_Maze-L</td><td align="left" valign="bottom">500</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">122</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">500</td></tr><tr><td align="left" valign="bottom">MC_Maze-M</td><td align="left" valign="bottom">250</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">114</td><td align="left" valign="bottom">38</td><td align="left" valign="bottom">500</td></tr><tr><td align="left" valign="bottom">MC_Maze-S</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">107</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">500</td></tr><tr><td align="left" valign="bottom">MC_RTT</td><td align="left" valign="bottom">1080</td><td align="left" valign="bottom">271</td><td align="left" valign="bottom">98</td><td align="left" valign="bottom">32</td><td align="left" valign="bottom">500</td></tr></tbody></table></table-wrap><p>In addition to requiring test set rate estimates, the benchmark required training set rate estimates (these were needed for the velocity <inline-formula><mml:math id="inf493"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> metric, see “Evaluation metrics” section). Each moment in each trial of the training data corresponded to a particular condition <inline-formula><mml:math id="inf494"><mml:mi>c</mml:mi></mml:math></inline-formula> and a time within the execution of that condition <inline-formula><mml:math id="inf495"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (with the exception of MC_RTT, which will be discussed in a moment). Thus, training rate estimates were simply constructed by assigning each moment within each training trial a vector of rates <inline-formula><mml:math id="inf496"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> for the corresponding <inline-formula><mml:math id="inf497"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf498"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For MC_RTT, the training rate estimates were simply the single-trial rates learned via AutoLFADS.</p></sec><sec id="s4-8-2"><title>Evaluation metrics</title><p>The evaluation metrics used in this paper are briefly described below. Detailed explanations can be found in <xref ref-type="bibr" rid="bib69">Pei et al., 2021</xref>.</p><p>Bits per spike was used to assess how well the rate estimates for the held-out neurons on the test data matched the actual held-out spiking activity. The metric is computed by first computing the log-likelihood of the observed spikes, given rate estimates, across all neurons. Then, the log-likelihood that would have been obtained by letting the rate estimates be the neurons’ mean rates is subtracted off. Finally, the metric is normalized. Positive values indicate that the held-out rate estimates are more predictive of held-out spiking activity than the mean rates.</p><p>PSTH <inline-formula><mml:math id="inf499"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> was computed by collecting all rate estimates on the test set, sorting them by condition, and averaging rate estimates across trials of the same condition. Then the <inline-formula><mml:math id="inf500"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> was computed between these rate-estimate-derived PSTHs and the empirical PSTHs (computed by smoothing spikes and averaging across trials within conditions). Empirical PSTHs were computed using trials from both the training and test sets. A separate <inline-formula><mml:math id="inf501"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> value was generated for each neuron — the reported values are the average <inline-formula><mml:math id="inf502"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> across all neurons. This metric could not be computed for MC_RTT due to lack of condition structure.</p><p>Velocity <inline-formula><mml:math id="inf503"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> was computed by first regressing lagged x- and y-velocity of the hand against estimated rates in the training data (ridge regression). Then, the linear mapping learned via regression is applied to estimated rates in the test data to generate estimated x- and y-velocity of the hand. <inline-formula><mml:math id="inf504"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is computed between estimated velocities and actual velocities on the test data and averaged across the x- and y-components. This metric was not computed for DMFC_RSG because the evaluated portion of the trials did not involve motion of the monkey’s arm.</p></sec></sec><sec id="s4-9"><title>Hyperparameter optimization</title><p>MINT has very few hyperparameters, all of which can be readily set by hand. These hyperparameters typically relate straightforwardly to properties of the task or data, and performance is robust to their exact values (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). For example, we did not optimize bin size (<inline-formula><mml:math id="inf505"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). Rather, we let <inline-formula><mml:math id="inf506"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> ms for all analyses, reasoning that this value would reduce computation time while remaining considerably shorter than the timescale over which rates change. MINT’s only other relevant hyperparameters are window length (duration of spiking history considered at each moment) and the number of candidate states to use for interpolation. The number of candidate states was simply set to two for all datasets except MC_RTT. For MC_RTT, the nature of the training data argued for considering more candidate states during the interpolation stage. Thus, the number of states was optimized using a grid search with 10-fold cross validation on the training set (using velocity decoding <inline-formula><mml:math id="inf507"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> as an objective function to maximize). Window length was also optimized via this grid search procedure, separately for each dataset. In some cases (e.g. the maze datasets), we still chose to standardize window length across datasets because optimization yielded similar values. Because neural state estimation was acausal for the Neural Latents Benchmark, whereas decoding analyses were typically causal (with one exception described in the “MINT variants” section), window length was also optimized separately across these analyses. For the simulated datasets, we chose not to optimize window length at all – the maze simulations and multitask spiking network were simply set to use the same window length as MC_Maze.</p><p>This approach to hyperparameter selection contrasts with our approach for the non-MINT decoders. As described in the Appendix, the non-MINT decoders utilized a more sophisticated technique to select hyperparameters: Bayesian optimization (<xref ref-type="bibr" rid="bib93">Snoek et al., 2012</xref>). Additionally, the non-MINT decoders were allowed the flexibility of using a different window length for each behavioral group. The choice not to provide MINT with the same flexibility was not arbitrary. Rather, this choice emphasizes a particularly useful aspect of MINT: any relevant behavioral variable can be read out from the same neural state estimate. There is no need to use separate training or decoding procedures for different behavioral variables.</p><p>The methods we used for learning neural trajectories to use with MINT also contained hyperparameters (e.g. <xref ref-type="table" rid="table3">Table 3</xref>). For all datasets involved in the Neural Latents Benchmark (except MC_RTT), these hyperparameters were optimized using the grid search procedure (using bits per spike as an objective function to maximize). These hyperparameters were then re-used for decoding analyses with no additional optimization. For MC_RTT, we utilized AutoLFADS to learn neural trajectories, which contains a built-in procedure for optimizing hyperparameters. MC_Cycle, MC_PacMan, the maze simulations, and the multitask network were not part of the Neural Latents Benchmark and therefore needed their trajectory-learning hyperparameters set in a different way. For MC_Cycle and MC_PacMan, these hyperparameters were optimized using the grid search procedure with velocity and force decoding <inline-formula><mml:math id="inf508"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively, as the objective functions. The maze simulations simply used the same hyperparameters as MC_Maze. For the multitask network, the trajectory learning hyperparameters were set very conservatively by hand to only perform very light temporal smoothing on spikes along with standard trial-averaging.</p><p>The example decoding results in <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="video" rid="fig4video1">Figure 4—video 1</xref> used the same hyperparameters that were used in generating the quantitative decoding results in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>holds a patent pertaining to this work. The patent has been licensed to Blackrock Neurotech. The authors declare no additional competing interests</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con4"><p>Supervision, Investigation</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Supervision, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing, Funding acquisition</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>For the MC_Cycle and MC_PacMan datasets, animal protocols were approved by the Columbia University Institutional Animal Care and Use Committee (Protocol number AC-AABE3550). For other datasets, protocols were approved by the relevant institutional committee for that institution.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-89421-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>MINT is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/seanmperkins/mint">GitHub</ext-link> (copy archived at <xref ref-type="bibr" rid="bib73">Perkins, 2025</xref>) (MATLAB). Other decoders are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/seanmperkins/bci-decoders">GitHub</ext-link> (<xref ref-type="bibr" rid="bib72">Perkins, 2023</xref>) (Python). Multitask spiking network was simulated with code from <xref ref-type="bibr" rid="bib24">DePasquale et al., 2023</xref> that is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/briandepasquale/factor-based-spiking-nets/">GitHub</ext-link> (<xref ref-type="bibr" rid="bib23">DePasquale, 2022</xref>) (MATLAB). Empirical datasets (except MC_Cycle and MC_PacMan) were curated by the Neural Latents Benchmark team and made publicly available on DANDI Archive. The MC_Cycle dataset is available at <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/14769761">Zenodo</ext-link>. The MC_PacMan dataset is available at <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/14769889">Zenodo</ext-link>. Useful functions for loading the DANDI datasets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/neurallatents/nlb_tools">GitHub</ext-link> (<xref ref-type="bibr" rid="bib70">Pei et al., 2024</xref>).</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Perkins</surname><given-names>S</given-names></name><name><surname>Schroeder</surname><given-names>K</given-names></name><name><surname>Churchland</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>MC_Cycle: macaque motor cortex spiking activity during cycling</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.14769761</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Amematsro</surname><given-names>E</given-names></name><name><surname>Marshall</surname><given-names>N</given-names></name><name><surname>Trautmann</surname><given-names>E</given-names></name><name><surname>Churchland</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>MC_PacMan: macaque motor cortex spiking activity during isometric force production</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.14769889</pub-id></element-citation></p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset3"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Chowdhury</surname><given-names>R</given-names></name><name><surname>Miller</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Area2_Bump: macaque somatosensory area 2 spiking activity during reaching with perturbations</data-title><source>DANDI Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000127/0.220113.0359</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset4"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>DMFC_RSG: macaque dorsomedial frontal cortex spiking activity during time interval reproduction task</data-title><source>DANDI Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000130/0.220113.0407</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset5"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Churchland</surname><given-names>M</given-names></name><name><surname>Kaufman</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>MC_Maze: macaque primary motor and dorsal premotor cortex spiking activity during delayed reaching</data-title><source>DANDI Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000128/0.220113.0400</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset6"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Churchland</surname><given-names>M</given-names></name><name><surname>Kaufman</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>MC_Maze_Large: macaque primary motor and dorsal premotor cortex spiking activity during delayed reaching</data-title><source>DANDI Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000138/0.220113.0407</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset7"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Churchland</surname><given-names>M</given-names></name><name><surname>Kaufman</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>MC_Maze_Medium: macaque primary motor and dorsal premotor cortex spiking activity during delayed reaching</data-title><source>DANDI Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000139/0.220113.0408</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset8"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Churchland</surname><given-names>M</given-names></name><name><surname>Kaufman</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>MC_Maze_Small: macaque primary motor and dorsal premotor cortex spiking activity during delayed reaching</data-title><source>DANDI Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000140/0.220113.0408</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset9"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>O'Doherty</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>MC_RTT: macaque motor cortex spiking activity during self-paced reaching</data-title><source>DANDI Archive</source><pub-id pub-id-type="doi">10.48324/dandi.000129/0.241017.1444</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Felix Pei, Chethan Pandarinath, and the rest of the Neural Latents Benchmark team for curating many of the datasets used in this paper and releasing them publicly. We thank those who originally collected those datasets: Raeed Chowdhury (Area2_Bump), Hansem Sohn (DMFC_RSG), Matt Kaufman and Mark Churchland (MC_Maze, MC_Maze-L,M,S), and Joseph O’Doherty (MC_RTT). We additionally thank Felix and Chethan for providing AutoLFADS rates for the MC_RTT dataset. We thank Karen Schroeder for collecting and preprocessing the MC_Cycle dataset in collaboration with the first and last authors. We thank Najja Marshall, Eric Trautmann, and Andrew Zimnik for their central roles (with Elom Amematsro) in designing the PacMan task and collecting data using Neuropixels probes. We thank Yana Pavlova for expert animal care while collecting the MC_Cycle and MC_PacMan datasets. We thank Brian DePasquale for providing code for simulating the artificial multitask spiking network. We thank Andrew Zimnik and Eric Trautmann for helping the second author present this work at NCM 2022 when the first author had COVID. This work was supported by the Simons Foundation and the Grossman Center for the Statistics of Mind. Aspects of analysis (e.g. generalization performance during the PacMan task) and manuscript preparation were supported by NIH 1R01NS135240. Elom Amematsro was supported by an NSF graduate fellowship.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadi</surname><given-names>N</given-names></name><name><surname>Constandinou</surname><given-names>TG</given-names></name><name><surname>Bouganis</surname><given-names>C-S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Robust and accurate decoding of hand kinematics from entire spiking activity using deep learning</article-title><source>Journal of Neural Engineering</source><volume>18</volume><elocation-id>026011</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/abde8a</pub-id><pub-id pub-id-type="pmid">33477128</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ajiboye</surname><given-names>AB</given-names></name><name><surname>Willett</surname><given-names>FR</given-names></name><name><surname>Young</surname><given-names>DR</given-names></name><name><surname>Memberg</surname><given-names>WD</given-names></name><name><surname>Murphy</surname><given-names>BA</given-names></name><name><surname>Miller</surname><given-names>JP</given-names></name><name><surname>Walter</surname><given-names>BL</given-names></name><name><surname>Sweet</surname><given-names>JA</given-names></name><name><surname>Hoyen</surname><given-names>HA</given-names></name><name><surname>Keith</surname><given-names>MW</given-names></name><name><surname>Peckham</surname><given-names>PH</given-names></name><name><surname>Simeral</surname><given-names>JD</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Kirsch</surname><given-names>RF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Restoration of reaching and grasping movements through brain-controlled muscle stimulation in a person with tetraplegia: a proof-of-concept demonstration</article-title><source>Lancet</source><volume>389</volume><fpage>1821</fpage><lpage>1830</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(17)30601-3</pub-id><pub-id pub-id-type="pmid">28363483</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ames</surname><given-names>KC</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Motor cortex signals for each arm are mixed across hemispheres and neurons yet partitioned within the population response</article-title><source>eLife</source><volume>8</volume><elocation-id>e46159</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46159</pub-id><pub-id pub-id-type="pmid">31596230</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anumanchipalli</surname><given-names>GK</given-names></name><name><surname>Chartier</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Speech synthesis from neural decoding of spoken sentences</article-title><source>Nature</source><volume>568</volume><fpage>493</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1119-1</pub-id><pub-id pub-id-type="pmid">31019317</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Athalye</surname><given-names>VR</given-names></name><name><surname>Khanna</surname><given-names>P</given-names></name><name><surname>Gowda</surname><given-names>S</given-names></name><name><surname>Orsborn</surname><given-names>AL</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name><name><surname>Carmena</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Invariant neural dynamics drive commands to control different movements</article-title><source>Current Biology</source><volume>33</volume><fpage>2962</fpage><lpage>2976</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2023.06.027</pub-id><pub-id pub-id-type="pmid">37402376</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouton</surname><given-names>CE</given-names></name><name><surname>Shaikhouni</surname><given-names>A</given-names></name><name><surname>Annetta</surname><given-names>NV</given-names></name><name><surname>Bockbrader</surname><given-names>MA</given-names></name><name><surname>Friedenberg</surname><given-names>DA</given-names></name><name><surname>Nielson</surname><given-names>DM</given-names></name><name><surname>Sharma</surname><given-names>G</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name><name><surname>Glenn</surname><given-names>BC</given-names></name><name><surname>Mysiw</surname><given-names>WJ</given-names></name><name><surname>Morgan</surname><given-names>AG</given-names></name><name><surname>Deogaonkar</surname><given-names>M</given-names></name><name><surname>Rezai</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Restoring cortical control of functional movement in a human with quadriplegia</article-title><source>Nature</source><volume>533</volume><fpage>247</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1038/nature17435</pub-id><pub-id pub-id-type="pmid">27074513</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>C</given-names></name><name><surname>Aggarwal</surname><given-names>A</given-names></name><name><surname>Pei</surname><given-names>R</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Proekt</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>One dimensional approximations of neuronal dynamics reveal computational strategy</article-title><source>PLOS Computational Biology</source><volume>19</volume><elocation-id>e1010784</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010784</pub-id><pub-id pub-id-type="pmid">36607933</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brockwell</surname><given-names>AE</given-names></name><name><surname>Rojas</surname><given-names>AL</given-names></name><name><surname>Kass</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Recursive bayesian decoding of motor cortical signals by particle filtering</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>1899</fpage><lpage>1907</lpage><pub-id pub-id-type="doi">10.1152/jn.00438.2003</pub-id><pub-id pub-id-type="pmid">15010499</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Card</surname><given-names>NS</given-names></name><name><surname>Wairagkar</surname><given-names>M</given-names></name><name><surname>Iacobacci</surname><given-names>C</given-names></name><name><surname>Hou</surname><given-names>X</given-names></name><name><surname>Singer-Clark</surname><given-names>T</given-names></name><name><surname>Willett</surname><given-names>FR</given-names></name><name><surname>Kunz</surname><given-names>EM</given-names></name><name><surname>Fan</surname><given-names>C</given-names></name><name><surname>Nia</surname><given-names>MV</given-names></name><name><surname>Deo</surname><given-names>DR</given-names></name><name><surname>Srinivasan</surname><given-names>A</given-names></name><name><surname>Choi</surname><given-names>EY</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shahlaie</surname><given-names>K</given-names></name><name><surname>Brandman</surname><given-names>DM</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>An accurate and rapidly calibrating speech neuroprosthesis</article-title><source>medRxiv</source><ext-link ext-link-type="uri" xlink:href="https://www.medrxiv.org/content/10.1101/2023.12.26.23300110v2">https://www.medrxiv.org/content/10.1101/2023.12.26.23300110v2</ext-link></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carmena</surname><given-names>JM</given-names></name><name><surname>Lebedev</surname><given-names>MA</given-names></name><name><surname>Crist</surname><given-names>RE</given-names></name><name><surname>O’Doherty</surname><given-names>JE</given-names></name><name><surname>Santucci</surname><given-names>DM</given-names></name><name><surname>Dimitrov</surname><given-names>DF</given-names></name><name><surname>Patil</surname><given-names>PG</given-names></name><name><surname>Henriquez</surname><given-names>CS</given-names></name><name><surname>Nicolelis</surname><given-names>MAL</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Learning to control a brain-machine interface for reaching and grasping by primates</article-title><source>PLOS Biology</source><volume>1</volume><elocation-id>E42</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0000042</pub-id><pub-id pub-id-type="pmid">14624244</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chari</surname><given-names>A</given-names></name><name><surname>Thornton</surname><given-names>RC</given-names></name><name><surname>Tisdall</surname><given-names>MM</given-names></name><name><surname>Scott</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Microelectrode recordings in human epilepsy: A case for clinical translation</article-title><source>Brain Communications</source><volume>2</volume><elocation-id>fcaa082</elocation-id><pub-id pub-id-type="doi">10.1093/braincomms/fcaa082</pub-id><pub-id pub-id-type="pmid">32954332</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaudhuri</surname><given-names>R</given-names></name><name><surname>Gerçek</surname><given-names>B</given-names></name><name><surname>Pandey</surname><given-names>B</given-names></name><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Fiete</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1512</fpage><lpage>1520</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0460-x</pub-id><pub-id pub-id-type="pmid">31406365</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>van Merrienboer</surname><given-names>B</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Bahdanau</surname><given-names>D</given-names></name><name><surname>Bougares</surname><given-names>F</given-names></name><name><surname>Schwenk</surname><given-names>H</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning phrase representations using RNN encoder–decoder for statistical machine translation</article-title><conf-name>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</conf-name><pub-id pub-id-type="doi">10.3115/v1/D14-1179</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Glaser</surname><given-names>JI</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Area 2 of primary somatosensory cortex encodes kinematics of the whole arm</article-title><source>eLife</source><volume>9</volume><elocation-id>e48198</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48198</pub-id><pub-id pub-id-type="pmid">31971510</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christie</surname><given-names>BP</given-names></name><name><surname>Tat</surname><given-names>DM</given-names></name><name><surname>Irwin</surname><given-names>ZT</given-names></name><name><surname>Gilja</surname><given-names>V</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Foster</surname><given-names>JD</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Thompson</surname><given-names>DE</given-names></name><name><surname>Chestek</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Comparison of spike sorting and thresholding of voltage waveforms for intracortical brain-machine interface performance</article-title><source>Journal of Neural Engineering</source><volume>12</volume><elocation-id>016009</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/12/1/016009</pub-id><pub-id pub-id-type="pmid">25504690</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Temporal complexity and heterogeneity of single-neuron activity in premotor and motor cortex</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>4235</fpage><lpage>4257</lpage><pub-id pub-id-type="doi">10.1152/jn.00095.2007</pub-id><pub-id pub-id-type="pmid">17376854</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cortical preparatory activity: representation of movement or first cog in a dynamical machine?</article-title><source>Neuron</source><volume>68</volume><fpage>387</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.015</pub-id><pub-id pub-id-type="pmid">21040842</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Foster</surname><given-names>JD</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><volume>487</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature11129</pub-id><pub-id pub-id-type="pmid">22722855</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Preparatory activity and the expansive null-space</article-title><source>Nature Reviews. Neuroscience</source><volume>25</volume><fpage>213</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/s41583-024-00796-z</pub-id><pub-id pub-id-type="pmid">38443626</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collinger</surname><given-names>JL</given-names></name><name><surname>Wodlinger</surname><given-names>B</given-names></name><name><surname>Downey</surname><given-names>JE</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Weber</surname><given-names>DJ</given-names></name><name><surname>McMorland</surname><given-names>AJC</given-names></name><name><surname>Velliste</surname><given-names>M</given-names></name><name><surname>Boninger</surname><given-names>ML</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>High-performance neuroprosthetic control by an individual with tetraplegia</article-title><source>Lancet</source><volume>381</volume><fpage>557</fpage><lpage>564</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(12)61816-9</pub-id><pub-id pub-id-type="pmid">23253623</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowley</surname><given-names>BR</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Butler</surname><given-names>ZS</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>DataHigh: graphical user interface for visualizing and interacting with high-dimensional neural activity</article-title><source>Journal of Neural Engineering</source><volume>10</volume><elocation-id>066012</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/10/6/066012</pub-id><pub-id pub-id-type="pmid">24216250</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Deo</surname><given-names>DR</given-names></name><name><surname>Willett</surname><given-names>FR</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Avansino</surname><given-names>DT</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Translating deep learning to neuroprosthetic control</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.04.21.537581</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>DePasquale</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Factor-based-spiking-nets</data-title><version designator="fac6eae">fac6eae</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/briandepasquale/factor-based-spiking-nets/">https://github.com/briandepasquale/factor-based-spiking-nets/</ext-link></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DePasquale</surname><given-names>B</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The centrality of population-level factors to network computation is demonstrated by a versatile approach for training spiking networks</article-title><source>Neuron</source><volume>111</volume><fpage>631</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.12.007</pub-id><pub-id pub-id-type="pmid">36630961</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyer</surname><given-names>EL</given-names></name><name><surname>Gheshlaghi Azar</surname><given-names>M</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Fernandes</surname><given-names>HL</given-names></name><name><surname>Naufel</surname><given-names>S</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Körding</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A cryptography-based approach for movement decoding</article-title><source>Nature Biomedical Engineering</source><volume>1</volume><fpage>967</fpage><lpage>976</lpage><pub-id pub-id-type="doi">10.1038/s41551-017-0169-7</pub-id><pub-id pub-id-type="pmid">31015712</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsayed</surname><given-names>GF</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reorganization between preparatory and movement population responses in motor cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13239</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13239</pub-id><pub-id pub-id-type="pmid">27807345</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethier</surname><given-names>C</given-names></name><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Bauman</surname><given-names>MJ</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Restoration of grasp following paralysis through brain-controlled stimulation of muscles</article-title><source>Nature</source><volume>485</volume><fpage>368</fpage><lpage>371</lpage><pub-id pub-id-type="doi">10.1038/nature10987</pub-id><pub-id pub-id-type="pmid">22522928</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetz</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Are movement parameters recognizably coded in the activity of single neurons?</article-title><source>Behavioral and Brain Sciences</source><volume>15</volume><fpage>679</fpage><lpage>690</lpage><pub-id pub-id-type="doi">10.1017/S0140525X00072599</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Fortunato</surname><given-names>C</given-names></name><name><surname>Bennasar-Vázquez</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>JC</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Dudman</surname><given-names>JT</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Gallego</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Nonlinear Manifolds Underlie Neural Population Activity during Behaviour</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.07.18.549575</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>E</given-names></name><name><surname>Sudderth</surname><given-names>EB</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name><name><surname>Willsky</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Bayesian nonparametric inference of switching dynamic linear models</article-title><source>Advances in Neural Information Processing Systems</source><volume>59</volume><fpage>1569</fpage><lpage>1585</lpage><pub-id pub-id-type="doi">10.1109/TSP.2010.2102756</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural manifolds for the control of movement</article-title><source>Neuron</source><volume>94</volume><fpage>978</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id><pub-id pub-id-type="pmid">28595054</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Naufel</surname><given-names>SN</given-names></name><name><surname>Ethier</surname><given-names>C</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Cortical population activity within a preserved neural manifold underlies multiple motor behaviors</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4233</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06560-z</pub-id><pub-id pub-id-type="pmid">30315158</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>P</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>On simplicity and complexity in the brave new world of large-scale neuroscience</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>148</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.04.003</pub-id><pub-id pub-id-type="pmid">25932978</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>P</given-names></name><name><surname>Trautmann</surname><given-names>E</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A theory of multineuronal dimensionality, dynamics and measurement</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/214262</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>AP</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Kettner</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Neuronal population coding of movement direction</article-title><source>Science</source><volume>233</volume><fpage>1416</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1126/science.3749885</pub-id><pub-id pub-id-type="pmid">3749885</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilja</surname><given-names>V</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Chestek</surname><given-names>CA</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Fan</surname><given-names>JM</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A high-performance neural prosthesis enabled by control algorithm design</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1752</fpage><lpage>1757</lpage><pub-id pub-id-type="doi">10.1038/nn.3265</pub-id><pub-id pub-id-type="pmid">23160043</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glaser</surname><given-names>JI</given-names></name><name><surname>Benjamin</surname><given-names>AS</given-names></name><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Machine learning for neural decoding</article-title><source>eNeuro</source><volume>7</volume><elocation-id>ENEURO.0506-19.2020</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0506-19.2020</pub-id><pub-id pub-id-type="pmid">32737181</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Sadtler</surname><given-names>PT</given-names></name><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Quick</surname><given-names>KM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning by neural reassociation</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>607</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0095-3</pub-id><pub-id pub-id-type="pmid">29531364</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goudar</surname><given-names>V</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Encoding sensory and motor patterns as time-invariant trajectories in recurrent neural networks</article-title><source>eLife</source><volume>7</volume><elocation-id>e31134</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31134</pub-id><pub-id pub-id-type="pmid">29537963</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heming</surname><given-names>EA</given-names></name><name><surname>Cross</surname><given-names>KP</given-names></name><name><surname>Takei</surname><given-names>T</given-names></name><name><surname>Cook</surname><given-names>DJ</given-names></name><name><surname>Scott</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Independent representations of ipsilateral and contralateral limbs in primary motor cortex</article-title><source>eLife</source><volume>8</volume><elocation-id>e48190</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48190</pub-id><pub-id pub-id-type="pmid">31625506</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Serruya</surname><given-names>MD</given-names></name><name><surname>Friehs</surname><given-names>GM</given-names></name><name><surname>Mukand</surname><given-names>JA</given-names></name><name><surname>Saleh</surname><given-names>M</given-names></name><name><surname>Caplan</surname><given-names>AH</given-names></name><name><surname>Branner</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Penn</surname><given-names>RD</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neuronal ensemble control of prosthetic devices by a human with tetraplegia</article-title><source>Nature</source><volume>442</volume><fpage>164</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1038/nature04970</pub-id><pub-id pub-id-type="pmid">16838014</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Bacher</surname><given-names>D</given-names></name><name><surname>Jarosiewicz</surname><given-names>B</given-names></name><name><surname>Masse</surname><given-names>NY</given-names></name><name><surname>Simeral</surname><given-names>JD</given-names></name><name><surname>Vogel</surname><given-names>J</given-names></name><name><surname>Haddadin</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>van der Smagt</surname><given-names>P</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Reach and grasp by people with tetraplegia using a neurally controlled robotic arm</article-title><source>Nature</source><volume>485</volume><fpage>372</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1038/nature11076</pub-id><pub-id pub-id-type="pmid">22596161</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarosiewicz</surname><given-names>B</given-names></name><name><surname>Sarma</surname><given-names>AA</given-names></name><name><surname>Bacher</surname><given-names>D</given-names></name><name><surname>Masse</surname><given-names>NY</given-names></name><name><surname>Simeral</surname><given-names>JD</given-names></name><name><surname>Sorice</surname><given-names>B</given-names></name><name><surname>Oakley</surname><given-names>EM</given-names></name><name><surname>Blabe</surname><given-names>C</given-names></name><name><surname>Pandarinath</surname><given-names>C</given-names></name><name><surname>Gilja</surname><given-names>V</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>Eskandar</surname><given-names>EN</given-names></name><name><surname>Friehs</surname><given-names>G</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Virtual typing by people with tetraplegia using a self-calibrating intracortical brain-computer interface</article-title><source>Science Translational Medicine</source><volume>7</volume><elocation-id>313ra179</elocation-id><pub-id pub-id-type="doi">10.1126/scitranslmed.aac7328</pub-id><pub-id pub-id-type="pmid">26560357</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>A new approach to linear filtering and prediction problems</article-title><source>Journal of Basic Engineering</source><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Single-trial dynamics of motor cortex and their applications to brain-machine interfaces</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>7759</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms8759</pub-id><pub-id pub-id-type="pmid">26220660</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A high-performance neural prosthesis incorporating discrete state selection with hidden markov models</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>64</volume><fpage>935</fpage><lpage>945</lpage><pub-id pub-id-type="doi">10.1109/TBME.2016.2582691</pub-id><pub-id pub-id-type="pmid">27337709</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kemere</surname><given-names>CT</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Meng</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Decoding of plan and peri-movement neural signals in prosthetic systems</article-title><conf-name>IEEE Workshop on Signal Processing Systems</conf-name><fpage>276</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1109/SIPS.2002.1049722</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Meng</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Robust neural decoding of reaching movements for prosthetic systems</article-title><conf-name>25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name><pub-id pub-id-type="doi">10.1109/IEMBS.2003.1280146</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Meng</surname><given-names>T</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2004">2004a</year><article-title>Model-based decoding of reaching movements for prosthetic systems</article-title><conf-name>26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name><pub-id pub-id-type="doi">10.1109/IEMBS.2004.1404256</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Meng</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2004">2004b</year><article-title>Model-based neural decoding of reaching movements: a maximum likelihood approach</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>51</volume><fpage>925</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1109/TBME.2004.826675</pub-id><pub-id pub-id-type="pmid">15188860</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshtkaran</surname><given-names>MR</given-names></name><name><surname>Sedler</surname><given-names>AR</given-names></name><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Tandon</surname><given-names>R</given-names></name><name><surname>Basrai</surname><given-names>D</given-names></name><name><surname>Nguyen</surname><given-names>SL</given-names></name><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Pandarinath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A large-scale neural network training framework for generalized estimation of single-trial population dynamics</article-title><source>Nature Methods</source><volume>19</volume><fpage>1572</fpage><lpage>1577</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01675-0</pub-id><pub-id pub-id-type="pmid">36443486</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1412.6980">https://doi.org/10.48550/arXiv.1412.6980</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Le</surname><given-names>T</given-names></name><name><surname>Shlizerman</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>STNDT: Modeling Neural Population Activity with a Spatiotemporal Transformer</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2206.04727">https://arxiv.org/abs/2206.04727</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Libedinsky</surname><given-names>C</given-names></name><name><surname>So</surname><given-names>R</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Kyar</surname><given-names>TK</given-names></name><name><surname>Ho</surname><given-names>D</given-names></name><name><surname>Lim</surname><given-names>C</given-names></name><name><surname>Chan</surname><given-names>L</given-names></name><name><surname>Chua</surname><given-names>Y</given-names></name><name><surname>Yao</surname><given-names>L</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Vishal</surname><given-names>KV</given-names></name><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>ZN</given-names></name><name><surname>Lim</surname><given-names>LK</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Zou</surname><given-names>X</given-names></name><name><surname>Ang</surname><given-names>KK</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Ng</surname><given-names>WH</given-names></name><name><surname>Han</surname><given-names>BS</given-names></name><name><surname>Chng</surname><given-names>K</given-names></name><name><surname>Guan</surname><given-names>C</given-names></name><name><surname>Je</surname><given-names>M</given-names></name><name><surname>Yen</surname><given-names>S-C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Independent mobility achieved through a wireless brain-machine interface</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0165773</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0165773</pub-id><pub-id pub-id-type="pmid">27802344</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makin</surname><given-names>JG</given-names></name><name><surname>O’Doherty</surname><given-names>JE</given-names></name><name><surname>Cardoso</surname><given-names>MMB</given-names></name><name><surname>Sabes</surname><given-names>PN</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Superior arm-movement decoding from cortex with a new, unsupervised-learning algorithm</article-title><source>Journal of Neural Engineering</source><volume>15</volume><elocation-id>026010</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa9e95</pub-id><pub-id pub-id-type="pmid">29192609</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>NJ</given-names></name><name><surname>Glaser</surname><given-names>JI</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Amematsro</surname><given-names>EA</given-names></name><name><surname>Perkins</surname><given-names>SM</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Flexible neural control of motor units</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>1492</fpage><lpage>1504</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01165-8</pub-id><pub-id pub-id-type="pmid">36216998</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metzger</surname><given-names>SL</given-names></name><name><surname>Littlejohn</surname><given-names>KT</given-names></name><name><surname>Silva</surname><given-names>AB</given-names></name><name><surname>Moses</surname><given-names>DA</given-names></name><name><surname>Seaton</surname><given-names>MP</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Dougherty</surname><given-names>ME</given-names></name><name><surname>Liu</surname><given-names>JR</given-names></name><name><surname>Wu</surname><given-names>P</given-names></name><name><surname>Berger</surname><given-names>MA</given-names></name><name><surname>Zhuravleva</surname><given-names>I</given-names></name><name><surname>Tu-Chan</surname><given-names>A</given-names></name><name><surname>Ganguly</surname><given-names>K</given-names></name><name><surname>Anumanchipalli</surname><given-names>GK</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A high-performance neuroprosthesis for speech decoding and avatar control</article-title><source>Nature</source><volume>620</volume><fpage>1037</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06443-4</pub-id><pub-id pub-id-type="pmid">37612505</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miri</surname><given-names>A</given-names></name><name><surname>Warriner</surname><given-names>CL</given-names></name><name><surname>Seely</surname><given-names>JS</given-names></name><name><surname>Elsayed</surname><given-names>GF</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Behaviorally selective engagement of short-latency effector pathways by motor cortex</article-title><source>Neuron</source><volume>95</volume><fpage>683</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.042</pub-id><pub-id pub-id-type="pmid">28735748</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishne</surname><given-names>G</given-names></name><name><surname>Talmon</surname><given-names>R</given-names></name><name><surname>Meir</surname><given-names>R</given-names></name><name><surname>Schiller</surname><given-names>J</given-names></name><name><surname>Lavzin</surname><given-names>M</given-names></name><name><surname>Dubin</surname><given-names>U</given-names></name><name><surname>Coifman</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Hierarchical coupled-geometry analysis for neuronal structure and activity pattern discovery</article-title><source>IEEE Journal of Selected Topics in Signal Processing</source><volume>10</volume><fpage>1238</fpage><lpage>1253</lpage><pub-id pub-id-type="doi">10.1109/JSTSP.2016.2602061</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moritz</surname><given-names>CT</given-names></name><name><surname>Perlmutter</surname><given-names>SI</given-names></name><name><surname>Fetz</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Direct control of paralysed muscles by cortical neurons</article-title><source>Nature</source><volume>456</volume><fpage>639</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1038/nature07418</pub-id><pub-id pub-id-type="pmid">18923392</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musallam</surname><given-names>S</given-names></name><name><surname>Corneil</surname><given-names>BD</given-names></name><name><surname>Greger</surname><given-names>B</given-names></name><name><surname>Scherberger</surname><given-names>H</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Cognitive control signals for neural prosthetics</article-title><source>Science</source><volume>305</volume><fpage>258</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1126/science.1097938</pub-id><pub-id pub-id-type="pmid">15247483</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Fan</surname><given-names>JM</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A high-performance keyboard neural prosthesis enabled by task optimization</article-title><source>IEEE Transactions on Bio-Medical Engineering</source><volume>62</volume><fpage>21</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1109/TBME.2014.2354697</pub-id><pub-id pub-id-type="pmid">25203982</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Hennig</surname><given-names>JA</given-names></name><name><surname>Degenhart</surname><given-names>AD</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>New neural activity patterns emerge with long-term learning</article-title><source>PNAS</source><volume>116</volume><fpage>15210</fpage><lpage>15215</lpage><pub-id pub-id-type="doi">10.1073/pnas.1820296116</pub-id><pub-id pub-id-type="pmid">31182595</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Degenhart</surname><given-names>AD</given-names></name><name><surname>Grigsby</surname><given-names>EM</given-names></name><name><surname>Motiwala</surname><given-names>A</given-names></name><name><surname>McClain</surname><given-names>NT</given-names></name><name><surname>Marino</surname><given-names>PJ</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Dynamical Constraints on Neural Population Activity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2024.01.03.573543</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Sridhar</surname><given-names>S</given-names></name><name><surname>Pennington</surname><given-names>J</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Spike sorting with Kilosort4</article-title><source>Nature Methods</source><volume>21</volume><fpage>914</fpage><lpage>921</lpage><pub-id pub-id-type="doi">10.1038/s41592-024-02232-7</pub-id><pub-id pub-id-type="pmid">38589517</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname><given-names>C</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Blabe</surname><given-names>CH</given-names></name><name><surname>Sorice</surname><given-names>BL</given-names></name><name><surname>Saab</surname><given-names>J</given-names></name><name><surname>Willett</surname><given-names>FR</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>High performance communication by people with paralysis using an intracortical brain-computer interface</article-title><source>eLife</source><volume>6</volume><elocation-id>e18554</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18554</pub-id><pub-id pub-id-type="pmid">28220753</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname><given-names>C</given-names></name><name><surname>O’Shea</surname><given-names>DJ</given-names></name><name><surname>Collins</surname><given-names>J</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title><source>Nature Methods</source><volume>15</volume><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id><pub-id pub-id-type="pmid">30224673</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Ye</surname><given-names>J</given-names></name><name><surname>Zoltowski</surname><given-names>D</given-names></name><name><surname>Wu</surname><given-names>A</given-names></name><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>O’Doherty</surname><given-names>JE</given-names></name><name><surname>Shenoy,</surname><given-names>KV</given-names></name><name><surname>Kaufman,</surname><given-names>MT</given-names></name><name><surname>Churchland,</surname><given-names>M</given-names></name><name><surname>Jazayeri,</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Pillow</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>M</given-names></name><name><surname>Dyer</surname><given-names>EL</given-names></name><name><surname>Pandarinath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural Latents Benchmark 21: Evaluating Latent Variable Models of Neural Population Activity</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2109.04463">https://arxiv.org/abs/2109.04463</ext-link><pub-id pub-id-type="doi">10.48550/arxiv.2109.04463</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>F</given-names></name><name><surname>Sedler</surname><given-names>A</given-names></name><name><surname>Ye</surname><given-names>J</given-names></name><name><surname>Ghosh</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Nlb_tools</data-title><version designator="1ddc15f">1ddc15f</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/neurallatents/nlb_tools">https://github.com/neurallatents/nlb_tools</ext-link></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peixoto</surname><given-names>D</given-names></name><name><surname>Verhein</surname><given-names>JR</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Chandrasekaran</surname><given-names>C</given-names></name><name><surname>Brown</surname><given-names>J</given-names></name><name><surname>Fong</surname><given-names>S</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Decoding and perturbing decision states in real time</article-title><source>Nature</source><volume>591</volume><fpage>604</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-03181-9</pub-id><pub-id pub-id-type="pmid">33473215</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Perkins</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Bci-decoders</data-title><version designator="19e4802">19e4802</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/seanmperkins/bci-decoders">https://github.com/seanmperkins/bci-decoders</ext-link></element-citation></ref><ref id="bib73"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Perkins</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Mint</data-title><version designator="swh:1:rev:347206a402ac71ea528752365eebea17e3c770da">swh:1:rev:347206a402ac71ea528752365eebea17e3c770da</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:f985194eb95e6cd917c52069336ab40e13ad08cd;origin=https://github.com/seanmperkins/mint;visit=swh:1:snp:9cad3085b4ffed50eeb85c76052255aa90060c3a;anchor=swh:1:rev:347206a402ac71ea528752365eebea17e3c770da">https://archive.softwareheritage.org/swh:1:dir:f985194eb95e6cd917c52069336ab40e13ad08cd;origin=https://github.com/seanmperkins/mint;visit=swh:1:snp:9cad3085b4ffed50eeb85c76052255aa90060c3a;anchor=swh:1:rev:347206a402ac71ea528752365eebea17e3c770da</ext-link></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Provenza</surname><given-names>NR</given-names></name><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Peled</surname><given-names>N</given-names></name><name><surname>Restrepo</surname><given-names>MI</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>Dougherty</surname><given-names>DD</given-names></name><name><surname>Eskandar</surname><given-names>EN</given-names></name><name><surname>Borton</surname><given-names>DA</given-names></name><name><surname>Widge</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Decoding task engagement from distributed network electrophysiology in humans</article-title><source>Journal of Neural Engineering</source><volume>16</volume><elocation-id>056015</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/ab2c58</pub-id><pub-id pub-id-type="pmid">31419211</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajangam</surname><given-names>S</given-names></name><name><surname>Tseng</surname><given-names>P-H</given-names></name><name><surname>Yin</surname><given-names>A</given-names></name><name><surname>Lehew</surname><given-names>G</given-names></name><name><surname>Schwarz</surname><given-names>D</given-names></name><name><surname>Lebedev</surname><given-names>MA</given-names></name><name><surname>Nicolelis</surname><given-names>MAL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Wireless cortical brain-machine interface for whole-body navigation in primates</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>22170</elocation-id><pub-id pub-id-type="doi">10.1038/srep22170</pub-id><pub-id pub-id-type="pmid">26938468</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Hatsopoulos</surname><given-names>NG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Progress in motor control, a multidisciplinary perspective</article-title><source>Advances in Experimental Medicine and Biology</source><volume>629</volume><fpage>243</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1007/978-0-387-77064-2</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remington</surname><given-names>ED</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible sensorimotor computations through rapid reconfiguration of cortical dynamics</article-title><source>Neuron</source><volume>98</volume><fpage>1005</fpage><lpage>1019</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.020</pub-id><pub-id pub-id-type="pmid">29879384</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1961">1961</year><source>Principles of neurodynamics: perceptrons and the theory of brain mechanisms</source><publisher-name>Safari Research Group</publisher-name></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>AA</given-names></name><name><surname>Bittner</surname><given-names>SR</given-names></name><name><surname>Perkins</surname><given-names>SM</given-names></name><name><surname>Seely</surname><given-names>JS</given-names></name><name><surname>London</surname><given-names>BM</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Miri</surname><given-names>A</given-names></name><name><surname>Marshall</surname><given-names>NJ</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Motor cortex embeds muscle-like commands in an untangled population response</article-title><source>Neuron</source><volume>97</volume><fpage>953</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.004</pub-id><pub-id pub-id-type="pmid">29398358</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>AA</given-names></name><name><surname>Khajeh</surname><given-names>R</given-names></name><name><surname>Bittner</surname><given-names>SR</given-names></name><name><surname>Perkins</surname><given-names>SM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural trajectories in the supplementary motor area and motor cortex exhibit distinct geometries, compatible with different classes of computation</article-title><source>Neuron</source><volume>107</volume><fpage>745</fpage><lpage>758</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.05.020</pub-id><pub-id pub-id-type="pmid">32516573</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadtler</surname><given-names>PT</given-names></name><name><surname>Quick</surname><given-names>KM</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural constraints on learning</article-title><source>Nature</source><volume>512</volume><fpage>423</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.1038/nature13665</pub-id><pub-id pub-id-type="pmid">25164754</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sani</surname><given-names>OG</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Lee</surname><given-names>MB</given-names></name><name><surname>Dawes</surname><given-names>HE</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name><name><surname>Shanechi</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mood variations decoded from multi-site intracranial human brain activity</article-title><source>Nature Biotechnology</source><volume>36</volume><fpage>954</fpage><lpage>961</lpage><pub-id pub-id-type="doi">10.1038/nbt.4200</pub-id><pub-id pub-id-type="pmid">30199076</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sani</surname><given-names>OG</given-names></name><name><surname>Abbaspourazad</surname><given-names>H</given-names></name><name><surname>Wong</surname><given-names>YT</given-names></name><name><surname>Pesaran</surname><given-names>B</given-names></name><name><surname>Shanechi</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>140</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-00733-0</pub-id><pub-id pub-id-type="pmid">33169030</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxena</surname><given-names>S</given-names></name><name><surname>Russo</surname><given-names>AA</given-names></name><name><surname>Cunningham</surname><given-names>J</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Motor cortex activity across movement speeds is predicted by network-level strategies for generating muscle activity</article-title><source>eLife</source><volume>11</volume><elocation-id>e67620</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.67620</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Learnable latent embeddings for joint behavioural and neural analysis</article-title><source>Nature</source><volume>617</volume><fpage>360</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06031-6</pub-id><pub-id pub-id-type="pmid">37138088</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>KE</given-names></name><name><surname>Perkins</surname><given-names>SM</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Cortical control of virtual self-motion using task-specific subspaces</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>220</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2687-20.2021</pub-id><pub-id pub-id-type="pmid">34716229</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Direct cortical representation of drawing</article-title><source>Science</source><volume>265</volume><fpage>540</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1126/science.8036499</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwemmer</surname><given-names>MA</given-names></name><name><surname>Skomrock</surname><given-names>ND</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name><name><surname>Ting</surname><given-names>JE</given-names></name><name><surname>Sharma</surname><given-names>G</given-names></name><name><surname>Bockbrader</surname><given-names>MA</given-names></name><name><surname>Friedenberg</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Meeting brain-computer interface user performance expectations using a deep neural network decoding framework</article-title><source>Nature Medicine</source><volume>24</volume><fpage>1669</fpage><lpage>1676</lpage><pub-id pub-id-type="doi">10.1038/s41591-018-0171-y</pub-id><pub-id pub-id-type="pmid">30250141</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Inconvenient truths about neural processing in primary motor cortex</article-title><source>The Journal of Physiology</source><volume>586</volume><fpage>1217</fpage><lpage>1224</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2007.146068</pub-id><pub-id pub-id-type="pmid">18187462</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seely</surname><given-names>JS</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tensor analysis reveals distinct population structure that parallels the different computational roles of areas M1 and V1</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005164</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005164</pub-id><pub-id pub-id-type="pmid">27814353</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serruya</surname><given-names>MD</given-names></name><name><surname>Hatsopoulos</surname><given-names>NG</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Fellows</surname><given-names>MR</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Instant neural control of a movement signal</article-title><source>Nature</source><volume>416</volume><fpage>141</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1038/416141a</pub-id><pub-id pub-id-type="pmid">11894084</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shanechi</surname><given-names>MM</given-names></name><name><surname>Orsborn</surname><given-names>AL</given-names></name><name><surname>Moorman</surname><given-names>HG</given-names></name><name><surname>Gowda</surname><given-names>S</given-names></name><name><surname>Dangi</surname><given-names>S</given-names></name><name><surname>Carmena</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Rapid control and feedback rates enhance neuroprosthetic control</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>13825</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13825</pub-id><pub-id pub-id-type="pmid">28059065</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Snoek</surname><given-names>J</given-names></name><name><surname>Hugo</surname><given-names>L</given-names></name><name><surname>Ryan</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Practical bayesian optimization of machine learning algorithms</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1206.2944">https://arxiv.org/abs/1206.2944</ext-link></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Meirhaeghe</surname><given-names>N</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian computation through cortical latent dynamics</article-title><source>Neuron</source><volume>103</volume><fpage>934</fpage><lpage>947</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.012</pub-id><pub-id pub-id-type="pmid">31320220</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stopfer</surname><given-names>M</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Intensity versus identity coding in an olfactory system</article-title><source>Neuron</source><volume>39</volume><fpage>991</fpage><lpage>1004</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2003.08.011</pub-id><pub-id pub-id-type="pmid">12971898</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Fan</surname><given-names>JM</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Shenoy</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A recurrent neural network for closed-loop intracortical brain-machine interface decoders</article-title><source>Journal of Neural Engineering</source><volume>9</volume><elocation-id>026027</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/9/2/026027</pub-id><pub-id pub-id-type="pmid">22427488</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A neural network that finds A naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id><pub-id pub-id-type="pmid">26075643</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Making brain-machine interfaces robust to future neural variability</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13749</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13749</pub-id><pub-id pub-id-type="pmid">27958268</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>DM</given-names></name><name><surname>Tillery</surname><given-names>SIH</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Direct cortical control of 3D neuroprosthetic devices</article-title><source>Science</source><volume>296</volume><fpage>1829</fpage><lpage>1832</lpage><pub-id pub-id-type="doi">10.1126/science.1070291</pub-id><pub-id pub-id-type="pmid">12052948</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tieleman</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Lecture 6.5-rmsprop: divide the gradient by a running average of its recent magnitude</chapter-title><person-group person-group-type="editor"><name><surname>Tieleman</surname><given-names>T</given-names></name></person-group><source>COURSERA: Neural Networks for Machine Learning</source><publisher-name>COURSERA</publisher-name><fpage>26</fpage><lpage>30</lpage></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Direct cortical control of muscle activation in voluntary arm movements: a model</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>391</fpage><lpage>398</lpage><pub-id pub-id-type="doi">10.1038/73964</pub-id><pub-id pub-id-type="pmid">10725930</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Trautmann</surname><given-names>E</given-names></name><name><surname>Amematsro</surname><given-names>E</given-names></name><name><surname>Escola</surname><given-names>S</given-names></name><name><surname>Wolpert</surname><given-names>D</given-names></name><name><surname>Marshall</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Aliyari</surname><given-names>E</given-names></name><name><surname>Sacadura</surname><given-names>F</given-names></name><name><surname>Shadlen</surname><given-names>M</given-names></name><name><surname>Churchland</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><source>Motor Cortex Isolates Skill-Specific Dynamics in a Context Switching Task</source><publisher-name>Computational and Systems Neuroscience (COSYNE)</publisher-name><pub-id pub-id-type="doi">10.1016/j.neuroscience.2017.09.010</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tseng</surname><given-names>P-H</given-names></name><name><surname>Urpi</surname><given-names>NA</given-names></name><name><surname>Lebedev</surname><given-names>M</given-names></name><name><surname>Nicolelis</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Decoding movements from cortical ensemble activity using a long short-term memory recurrent network</article-title><source>Neural Computation</source><volume>31</volume><fpage>1085</fpage><lpage>1113</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01189</pub-id><pub-id pub-id-type="pmid">30979355</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vargas-Irwin</surname><given-names>CE</given-names></name><name><surname>Feldman</surname><given-names>JM</given-names></name><name><surname>King</surname><given-names>B</given-names></name><name><surname>Simeral</surname><given-names>JD</given-names></name><name><surname>Sorice</surname><given-names>BL</given-names></name><name><surname>Oakley</surname><given-names>EM</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>Eskandar</surname><given-names>EN</given-names></name><name><surname>Friehs</surname><given-names>GM</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Watch, imagine, attempt: motor cortex single-unit activity reveals context-dependent movement encoding in humans with tetraplegia</article-title><source>Frontiers in Human Neuroscience</source><volume>12</volume><elocation-id>450</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2018.00450</pub-id><pub-id pub-id-type="pmid">30524258</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Velliste</surname><given-names>M</given-names></name><name><surname>Perel</surname><given-names>S</given-names></name><name><surname>Spalding</surname><given-names>MC</given-names></name><name><surname>Whitford</surname><given-names>AS</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical control of a prosthetic arm for self-feeding</article-title><source>Nature</source><volume>453</volume><fpage>1098</fpage><lpage>1101</lpage><pub-id pub-id-type="doi">10.1038/nature06996</pub-id><pub-id pub-id-type="pmid">18509337</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vyas</surname><given-names>S</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Computation through neural population dynamics</article-title><source>Annual Review of Neuroscience</source><volume>43</volume><fpage>249</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-092619-094115</pub-id><pub-id pub-id-type="pmid">32640928</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wairagkar</surname><given-names>M</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Brandman</surname><given-names>DM</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Synthesizing speech by decoding intracortical neural activity from dorsal motor cortex</article-title><conf-name>2023 11th International IEEE/EMBS Conference on Neural Engineering (NER)</conf-name><conf-loc>Baltimore, MD, USA</conf-loc><pub-id pub-id-type="doi">10.1109/NER52421.2023.10123880</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decoding cognitive processes from neural ensembles</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>1091</fpage><lpage>1102</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.09.002</pub-id><pub-id pub-id-type="pmid">30279136</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warriner</surname><given-names>CL</given-names></name><name><surname>Fageiry</surname><given-names>S</given-names></name><name><surname>Saxena</surname><given-names>S</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name><name><surname>Miri</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Motor cortical influence relies on task-specific activity covariation</article-title><source>Cell Reports</source><volume>40</volume><elocation-id>111427</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.111427</pub-id><pub-id pub-id-type="pmid">36170841</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>JM</given-names></name><name><surname>Gaunt</surname><given-names>RA</given-names></name><name><surname>Franklin</surname><given-names>R</given-names></name><name><surname>Boninger</surname><given-names>ML</given-names></name><name><surname>Collinger</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Demonstration of a portable intracortical brain-computer interface</article-title><source>Brain-Computer Interfaces</source><volume>6</volume><fpage>106</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1080/2326263X.2019.1709260</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wessberg</surname><given-names>J</given-names></name><name><surname>Stambaugh</surname><given-names>CR</given-names></name><name><surname>Kralik</surname><given-names>JD</given-names></name><name><surname>Beck</surname><given-names>PD</given-names></name><name><surname>Laubach</surname><given-names>M</given-names></name><name><surname>Chapin</surname><given-names>JK</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Biggs</surname><given-names>SJ</given-names></name><name><surname>Srinivasan</surname><given-names>MA</given-names></name><name><surname>Nicolelis</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Real-time prediction of hand trajectory by ensembles of cortical neurons in primates</article-title><source>Nature</source><volume>408</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/35042582</pub-id><pub-id pub-id-type="pmid">11099043</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willett</surname><given-names>FR</given-names></name><name><surname>Avansino</surname><given-names>DT</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>High-performance brain-to-text communication via handwriting</article-title><source>Nature</source><volume>593</volume><fpage>249</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03506-2</pub-id><pub-id pub-id-type="pmid">33981047</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willett</surname><given-names>FR</given-names></name><name><surname>Kunz</surname><given-names>EM</given-names></name><name><surname>Fan</surname><given-names>C</given-names></name><name><surname>Avansino</surname><given-names>DT</given-names></name><name><surname>Wilson</surname><given-names>GH</given-names></name><name><surname>Choi</surname><given-names>EY</given-names></name><name><surname>Kamdar</surname><given-names>F</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A high-performance speech neuroprosthesis</article-title><source>Nature</source><volume>620</volume><fpage>1031</fpage><lpage>1036</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06377-x</pub-id><pub-id pub-id-type="pmid">37612500</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>AH</given-names></name><name><surname>Poole</surname><given-names>B</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Fisher</surname><given-names>T</given-names></name><name><surname>Wilson</surname><given-names>CD</given-names></name><name><surname>Brann</surname><given-names>DH</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Shusterman</surname><given-names>R</given-names></name><name><surname>Rinberg</surname><given-names>D</given-names></name><name><surname>Ölveczky</surname><given-names>BP</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Discovering precise temporal patterns in large-scale neural recordings through robust and interpretable time warping</article-title><source>Neuron</source><volume>105</volume><fpage>246</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.020</pub-id><pub-id pub-id-type="pmid">31786013</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>GH</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Willett</surname><given-names>FR</given-names></name><name><surname>Avansino</surname><given-names>DT</given-names></name><name><surname>Kelemen</surname><given-names>JN</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Decoding spoken English from intracortical electrode arrays in dorsal precentral gyrus</article-title><source>Journal of Neural Engineering</source><volume>17</volume><elocation-id>066007</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/abbfef</pub-id><pub-id pub-id-type="pmid">33236720</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wodlinger</surname><given-names>B</given-names></name><name><surname>Downey</surname><given-names>JE</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Boninger</surname><given-names>ML</given-names></name><name><surname>Collinger</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Ten-dimensional anthropomorphic arm control in a human brain-machine interface: difficulties, solutions, and limitations</article-title><source>Journal of Neural Engineering</source><volume>12</volume><elocation-id>016011</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/12/1/016011</pub-id><pub-id pub-id-type="pmid">25514320</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>W</given-names></name><name><surname>Black</surname><given-names>MJ</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Bienenstock</surname><given-names>E</given-names></name><name><surname>Serruya</surname><given-names>M</given-names></name><name><surname>Shaikhoun</surname><given-names>A</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neural decoding of cursor motion using a kalman filter</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xing</surname><given-names>D</given-names></name><name><surname>Truccolo</surname><given-names>W</given-names></name><name><surname>Borton</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Emergence of distinct neural subspaces in motor cortical dynamics during volitional adjustments of ongoing locomotion</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>9142</fpage><lpage>9157</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0746-22.2022</pub-id><pub-id pub-id-type="pmid">36283830</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>J</given-names></name><name><surname>Pandarinath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Representation learning for neural population activity with neural data transformers</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.01.16.426955</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yousefi</surname><given-names>A</given-names></name><name><surname>Basu</surname><given-names>I</given-names></name><name><surname>Paulk</surname><given-names>AC</given-names></name><name><surname>Peled</surname><given-names>N</given-names></name><name><surname>Eskandar</surname><given-names>EN</given-names></name><name><surname>Dougherty</surname><given-names>DD</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>Widge</surname><given-names>AS</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Decoding hidden cognitive states from behavior and physiology using a bayesian approach</article-title><source>Neural Computation</source><volume>31</volume><fpage>1751</fpage><lpage>1788</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01196</pub-id><pub-id pub-id-type="pmid">31335292</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Afshar</surname><given-names>A</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Meng</surname><given-names>TH</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Mixture of trajectory models for neural decoding of goal-directed movements</article-title><source>Journal of Neurophysiology</source><volume>97</volume><fpage>3763</fpage><lpage>3780</lpage><pub-id pub-id-type="doi">10.1152/jn.00482.2006</pub-id><pub-id pub-id-type="pmid">17329627</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>614</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1152/jn.90941.2008</pub-id><pub-id pub-id-type="pmid">19357332</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Ginzburg</surname><given-names>I</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>1017</fpage><lpage>1044</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.2.1017</pub-id><pub-id pub-id-type="pmid">9463459</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>D</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Learning Identifiable and Interpretable Latent Models of High-Dimensional Neural Activity Using Pi-VAE</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2011.04798">https://arxiv.org/abs/2011.04798</ext-link></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Decoder implementations</title><p>All decoders described in this section were provided with the same training and testing data as MINT. When decoding, spiking activity was binned every 20 ms for each neuron (with the exception of the Naive Bayes regression decoder, which benefited from a larger bin size). Each decoder was given access to recent binned spike counts to use for decoding. The following sections will: (1) provide an overview of the notation and definitions needed to understand the subsequent equations (notation <italic>will not</italic> in general match the notation used for MINT), (2) describe the hyperparameter optimization technique used for all decoders in this appendix, and (3) describe each decoder’s implementation in detail.</p><sec sec-type="appendix" id="s8-1"><title>Definitions</title><sec sec-type="appendix" id="s8-1-1"><title>Time Bins</title><p>Each decoder received binned spiking activity and decoded behavior at time-bin resolution. These time bins are indexed by <inline-formula><mml:math id="inf509"><mml:mi>k</mml:mi></mml:math></inline-formula>. When higher-resolution estimates were required (e.g. for evaluating decoding performance), the decoded variables were simply upsampled with a zero-order-hold.</p></sec><sec sec-type="appendix" id="s8-1-2"><title>Observations</title><p>The vector of spike counts at time bin <inline-formula><mml:math id="inf510"><mml:mi>k</mml:mi></mml:math></inline-formula> is denoted by the column vector <inline-formula><mml:math id="inf511"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf512"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of neurons. When decoding, each decoder was given access to the current time bin of spike counts and <inline-formula><mml:math id="inf513"><mml:mi>κ</mml:mi></mml:math></inline-formula> previous time bins of spike counts. <inline-formula><mml:math id="inf514"><mml:mi>κ</mml:mi></mml:math></inline-formula> was not a fixed value and varied depending on the method, dataset, and behavioral group being decoded.</p></sec><sec sec-type="appendix" id="s8-1-3"><title>Target Variables</title><p>The vector of behavioral variables at time bin <inline-formula><mml:math id="inf515"><mml:mi>k</mml:mi></mml:math></inline-formula> is denoted by the column vector <inline-formula><mml:math id="inf516"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf517"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of behavioral variables. This corresponds to the values of the behavioral variables at the end of the time bin.</p></sec><sec sec-type="appendix" id="s8-1-4"><title>Decoded variables</title><p>The vector of decoded behavioral variables at time bin <inline-formula><mml:math id="inf518"><mml:mi>k</mml:mi></mml:math></inline-formula> is denoted <inline-formula><mml:math id="inf519"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s8-1-5"><title>Training data</title><p>Observations and target variables at time bin <inline-formula><mml:math id="inf520"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of trial <inline-formula><mml:math id="inf521"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the training data are denoted <inline-formula><mml:math id="inf522"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf523"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec></sec><sec sec-type="appendix" id="s8-2"><title>Hyperparameter optimization</title><p>The hyperparameters for each decoder were set using Bayesian optimization (<xref ref-type="bibr" rid="bib93">Snoek et al., 2012</xref>). The training set was split into a reduced training set (80% of the trials from the full training set) and a validation set (the remaining 20% of trials). The method is provided with a range of hyperparameter values to search. The method then seeks to learn a set of hyperparameters that, when trained on the reduced training set, will lead to maximal decoding <inline-formula><mml:math id="inf524"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> on the validation set. The optimization occurs via an iterative process that involves exploring the space of hyperparameters and exploiting knowledge of how certain hyperparameter sets performed in previous iterations. In all cases, we set the method to initially perform 10 iterations of random exploration of hyperparameter space, followed by 10 iterations of Bayesian optimization. The optimization was performed separately for each behavioral group (e.g. position vs. velocity) within each dataset. In the decoder-specific sections below, the exact hyperparameter values learned for each dataset and behavioral group are reported along with the hyperparameter ranges that were searched.</p></sec><sec sec-type="appendix" id="s8-3"><title>Wiener filter</title><p>The Wiener filter (<xref ref-type="bibr" rid="bib1">Ahmadi et al., 2021</xref>) uses a model<disp-formula id="equ39"><label>(S1)</label><mml:math id="m39"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf525"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf526"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and the residuals are <inline-formula><mml:math id="inf527"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. Decoding occurs via the equation<disp-formula id="equ40"><label>(S2)</label><mml:math id="m40"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mpadded width="0"><mml:mphantom><mml:mi>P</mml:mi></mml:mphantom></mml:mpadded></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>for some matrix <inline-formula><mml:math id="inf528"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> that minimizes the squared error of the residuals in the training data.</p><sec sec-type="appendix" id="s8-3-1"><title>Parameter fitting</title><p>For each trial <inline-formula><mml:math id="inf529"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the training data, we construct matrices<disp-formula id="equ41"><label>(S3)</label><mml:math id="m41"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ42"><label>(S4)</label><mml:math id="m42"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf530"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of time bins in trial <inline-formula><mml:math id="inf531"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then, we concatenate across <inline-formula><mml:math id="inf532"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> trials to get<disp-formula id="equ43"><label>(S5)</label><mml:math id="m43"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ44"><label>(S6)</label><mml:math id="m44"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p>can then be fit via a regularized regression<disp-formula id="equ45"><label>(S7)</label><mml:math id="m45"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Hyperparameters used for the Wiener filter.</title><p>The L2 regularization term <inline-formula><mml:math id="inf533"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> was optimized in the range [0, 2000], with the optimized values rounded to the closest multiple of 10. Window lengths were optimized (in 20 ms increments) in the range [200, 600] for Area2_Bump, [200, 1000] for MC_Cycle, [200, 1200] for MC_RTT, [200, 500] for MC_PacMan, and [200, 700] for MC_Maze, MC_Maze-L, MC_Maze-M, and MC_Maze-S. These ranges were determined by the structure of each dataset (e.g. Area2_Bump couldn’t look back more than 600 ms from the beginning of the evaluation epoch without entering the previous trial). Window lengths are directly related to <inline-formula><mml:math id="inf534"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>κ</mml:mi></mml:mstyle></mml:math></inline-formula> via <inline-formula><mml:math id="inf535"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (e.g. <inline-formula><mml:math id="inf536"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>14</mml:mn></mml:mstyle></mml:math></inline-formula> would correspond to a window length of <inline-formula><mml:math id="inf537"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mn>14</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>300</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula> ms).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Behavioral Group</th><th align="left" valign="bottom">Window Length (ms)</th><th align="left" valign="bottom">L2 Regularization (λ)</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="7">Area2_Bump</td><td align="left" valign="bottom">Position</td><td align="char" char="." valign="bottom">560</td><td align="char" char="." valign="bottom">350</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="char" char="." valign="bottom">320</td><td align="char" char="." valign="bottom">320</td></tr><tr><td align="left" valign="bottom">Force</td><td align="left" valign="bottom">600</td><td align="left" valign="bottom">900</td></tr><tr><td align="left" valign="bottom">Joint Angles</td><td align="left" valign="bottom">300</td><td align="left" valign="bottom">250</td></tr><tr><td align="left" valign="bottom">Joint Velocities</td><td align="left" valign="bottom">400</td><td align="left" valign="bottom">1410</td></tr><tr><td align="left" valign="bottom">Muscle Lengths</td><td align="left" valign="bottom">380</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">Muscle Velocities</td><td align="left" valign="bottom">460</td><td align="left" valign="bottom">940</td></tr><tr><td align="left" valign="bottom" rowspan="5">MC_Cycle</td><td align="left" valign="bottom">Phase</td><td align="left" valign="bottom">1000</td><td align="left" valign="bottom">1730</td></tr><tr><td align="left" valign="bottom">Angular Velocity</td><td align="left" valign="bottom">960</td><td align="left" valign="bottom">1470</td></tr><tr><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">920</td><td align="left" valign="bottom">1710</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">420</td><td align="left" valign="bottom">600</td></tr><tr><td align="left" valign="bottom">EMG</td><td align="left" valign="bottom">560</td><td align="left" valign="bottom">1990</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze</td><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">660</td><td align="left" valign="bottom">530</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">540</td><td align="left" valign="bottom">210</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze-L</td><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">540</td><td align="left" valign="bottom">510</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">540</td><td align="left" valign="bottom">440</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze-M</td><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">660</td><td align="left" valign="bottom">440</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">480</td><td align="left" valign="bottom">310</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze-S</td><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">680</td><td align="left" valign="bottom">130</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">420</td><td align="left" valign="bottom">270</td></tr><tr><td align="left" valign="bottom">MC_RTT</td><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">1180</td><td align="left" valign="bottom">1610</td></tr><tr><td align="left" valign="bottom">MC_PacMan</td><td align="left" valign="bottom">Force</td><td align="left" valign="bottom">480</td><td align="left" valign="bottom">470</td></tr></tbody></table></table-wrap><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Hyperparameters used for the Kalman filter.</title><p>The lag (in increments of 20 ms time bins) between neural activity and behavior was optimized in the range [2, 8], corresponding to 40–160 ms, for all datasets except Area2_Bump. For Area2_Bump the lag was not optimized and was simply set to 0 due to the fact that, in a sensory area, movement precedes sensory feedback. Given that <inline-formula><mml:math id="inf538"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> aggregates spikes across the whole time bin, but <inline-formula><mml:math id="inf539"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> corresponds to the behavioral variables at the end of the time bin, the effective lag is actually half a bin (10 ms) longer — i.e. the effective range of lags considered for the non-sensory datasets was 50–170 ms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Behavioral Group</th><th align="left" valign="bottom">Lag (bins)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Area2_Bump</td><td align="left" valign="bottom">Position &amp; Velocity</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">MC_Cycle</td><td align="left" valign="bottom">Position &amp; Velocity</td><td align="left" valign="bottom">2</td></tr><tr><td align="left" valign="bottom">MC_Maze</td><td align="left" valign="bottom">Position &amp; Velocity</td><td align="left" valign="bottom">4</td></tr><tr><td align="left" valign="bottom">MC_Maze-L</td><td align="left" valign="bottom">Position &amp; Velocity</td><td align="left" valign="bottom">4</td></tr><tr><td align="left" valign="bottom">MC_Maze-M</td><td align="left" valign="bottom">Position &amp; Velocity</td><td align="left" valign="bottom">4</td></tr><tr><td align="left" valign="bottom">MC_Maze-S</td><td align="left" valign="bottom">Position &amp; Velocity</td><td align="left" valign="bottom">4</td></tr><tr><td align="left" valign="bottom">MC_RTT</td><td align="left" valign="bottom">Position &amp; Velocity</td><td align="left" valign="bottom">2</td></tr></tbody></table></table-wrap></sec></sec><sec sec-type="appendix" id="s8-4"><title>Kalman filter</title><p>The Kalman filter (<xref ref-type="bibr" rid="bib44">Kalman, 1960</xref>) uses a model<disp-formula id="equ46"><label>(S8)</label><mml:math id="m46"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ47"><label>(S9)</label><mml:math id="m47"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf540"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf541"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi class="mathcal" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Decoding occurs via the standard update equations:<disp-formula id="equ48"><label>(S10)</label><mml:math id="m48"><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">_</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ49"><label>(S11)</label><mml:math id="m49"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">_</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">_</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ50"><label>(S12)</label><mml:math id="m50"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">_</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ51"><label>(S13)</label><mml:math id="m51"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ52"><label>(S14)</label><mml:math id="m52"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>The state vector <inline-formula><mml:math id="inf542"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> includes 7 elements: position, velocity, and acceleration (x- and y-components of each) as well as a constant 1. The 1 was included to allow firing rates to have a constant offset. Acceleration was included as in <xref ref-type="bibr" rid="bib117">Wu et al., 2003</xref> to improve the position and velocity estimates.</p><sec sec-type="appendix" id="s8-4-1"><title>Parameter fitting</title><p>For each trial in the training set, we construct matrices<disp-formula id="equ53"><label>(S15)</label><mml:math id="m53"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ54"><label>(S16)</label><mml:math id="m54"><mml:mrow><mml:msup><mml:mi mathvariant="bold">Y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>…</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ55"><label>(S17)</label><mml:math id="m55"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ56"><label>(S18)</label><mml:math id="m56"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf543"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of time bins in trial <inline-formula><mml:math id="inf544"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Then, we concatenate across <inline-formula><mml:math id="inf545"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> trials to get<disp-formula id="equ57"><label>(S19)</label><mml:math id="m57"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ58"><label>(S20)</label><mml:math id="m58"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ59"><label>(S21)</label><mml:math id="m59"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ60"><label>(S22)</label><mml:math id="m60"><mml:mrow><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The parameters can then be fit as follows:<disp-formula id="equ61"><label>(S23)</label><mml:math id="m61"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ62"><label>(S24)</label><mml:math id="m62"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ63"><label>(S25)</label><mml:math id="m63"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ64"><label>(S26)</label><mml:math id="m64"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>T</mml:mi></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ65"><label>(S27)</label><mml:math id="m65"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>J</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ66"><label>(S28)</label><mml:math id="m66"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>J</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf546"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and are the number of columns in <inline-formula><mml:math id="inf547"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf548"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively</p></sec></sec><sec sec-type="appendix" id="s8-5"><title>Feedforward Neural Network</title><p>The feedforward neural network (<xref ref-type="bibr" rid="bib78">Rosenblatt, 1961</xref>) we use takes spike count observations (current and previous time bins) and flattens them into a long vector,<disp-formula id="equ67"><label>(S29)</label><mml:math id="m67"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>centers and normalizes that vector,<disp-formula id="equ68"><label>(S30)</label><mml:math id="m68"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⊘</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>and then feeds it through multiple hidden network layers<disp-formula id="equ69"><label>(S31)</label><mml:math id="m69"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>ReLU</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ70"><label>(S32)</label><mml:math id="m70"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mtext>ReLU</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>before finally reading out behavior<disp-formula id="equ71"><label>(S33)</label><mml:math id="m71"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf549"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf550"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of units per hidden layer, and <inline-formula><mml:math id="inf551"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of hidden layers.</p><sec sec-type="appendix" id="s8-5-1"><title>Parameter fitting</title><p>For each trial <inline-formula><mml:math id="inf552"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the training set, we create flattened observations<disp-formula id="equ72"><label>(S34)</label><mml:math id="m72"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>κ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>for all <inline-formula><mml:math id="inf553"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (sufficient spike count history doesn’t exist for <inline-formula><mml:math id="inf554"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). We then let <inline-formula><mml:math id="inf555"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf556"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> be the element-wise mean and standard deviation of <inline-formula><mml:math id="inf557"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> across all observations in the training set (i.e. across all time bins in all <inline-formula><mml:math id="inf558"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> trials for which <inline-formula><mml:math id="inf559"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>). The parameters <inline-formula><mml:math id="inf560"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf561"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf562"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf563"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf564"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are then learned by training the network with the Adam optimization routine (<xref ref-type="bibr" rid="bib52">Kingma and Ba, 2014</xref>) and a mean-squared error loss function. Training utilized dropout on the outputs of each hidden layer.</p><table-wrap id="app1table3" position="float"><label>Appendix 1—table 3.</label><caption><title>Hyperparameters used for the feedforward neural network.</title><p>The number of hidden layers (<inline-formula><mml:math id="inf565"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi></mml:mstyle></mml:math></inline-formula>) was optimized in the range [1, 15]. The number of units per hidden layer (<inline-formula><mml:math id="inf566"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi></mml:mstyle></mml:math></inline-formula>) was optimized in the range [50, 1000], with the optimized values rounded to the closest multiple of 10. The dropout rate was optimized in the range [0,0.5] and the number of training epochs was optimized in the range [2, 100]. Window lengths were optimized (in 20 ms increments) in the range [200, 600] for Area2_Bump, [200, 1000] for MC_Cycle, [200, 1200] for MC_RTT, and [200, 700] for MC_Maze, MC_Maze-L, MC_Maze-M, and MC_Maze-S.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Behavioral Group</th><th align="left" valign="bottom">Window Length (ms)</th><th align="left" valign="bottom">Hidden Layers (<italic>L</italic>)</th><th align="left" valign="bottom">Units/Layer (<italic>D</italic>)</th><th align="left" valign="bottom">Dropout Rate</th><th align="left" valign="bottom">Epochs</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="7">Area2_Bump</td><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">440</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">690</td><td align="left" valign="bottom">0.20</td><td align="left" valign="bottom">58</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">520</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">260</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">62</td></tr><tr><td align="left" valign="bottom">Force</td><td align="left" valign="bottom">240</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">560</td><td align="left" valign="bottom">0.01</td><td align="left" valign="bottom">54</td></tr><tr><td align="left" valign="bottom">Joint Angles</td><td align="left" valign="bottom">320</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">480</td><td align="left" valign="bottom">0.15</td><td align="left" valign="bottom">59</td></tr><tr><td align="left" valign="bottom">Joint Velocities</td><td align="left" valign="bottom">460</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">660</td><td align="left" valign="bottom">0.12</td><td align="left" valign="bottom">36</td></tr><tr><td align="left" valign="bottom">Muscle Lengths</td><td align="left" valign="bottom">240</td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">840</td><td align="left" valign="bottom">0.19</td><td align="left" valign="bottom">81</td></tr><tr><td align="left" valign="bottom">Muscle Velocities</td><td align="left" valign="bottom">500</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">480</td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">24</td></tr><tr><td align="left" valign="bottom" rowspan="5">MC_Cycle</td><td align="left" valign="bottom">Phase</td><td align="left" valign="bottom">740</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">180</td><td align="left" valign="bottom">0.11</td><td align="left" valign="bottom">24</td></tr><tr><td align="left" valign="bottom">Angular Velocity</td><td align="left" valign="bottom">700</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">710</td><td align="left" valign="bottom">0.01</td><td align="left" valign="bottom">43</td></tr><tr><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">360</td><td align="left" valign="bottom">9</td><td align="left" valign="bottom">470</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">45</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">460</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">330</td><td align="left" valign="bottom">0.25</td><td align="left" valign="bottom">55</td></tr><tr><td align="left" valign="bottom">EMG</td><td align="left" valign="bottom">840</td><td align="left" valign="bottom">5</td><td align="left" valign="bottom">970</td><td align="left" valign="bottom">0.07</td><td align="left" valign="bottom">43</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze</td><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">680</td><td align="left" valign="bottom">7</td><td align="left" valign="bottom">340</td><td align="left" valign="bottom">0.15</td><td align="left" valign="bottom">65</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">700</td><td align="left" valign="bottom">14</td><td align="left" valign="bottom">760</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">89</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze-L</td><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">380</td><td align="left" valign="bottom">6</td><td align="left" valign="bottom">160</td><td align="left" valign="bottom">0.05</td><td align="left" valign="bottom">44</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">600</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">380</td><td align="left" valign="bottom">0.13</td><td align="left" valign="bottom">43</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze-M</td><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">520</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">860</td><td align="left" valign="bottom">0.26</td><td align="left" valign="bottom">79</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">420</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">360</td><td align="left" valign="bottom">0.09</td><td align="left" valign="bottom">59</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze-S</td><td align="left" valign="bottom">Position</td><td align="left" valign="bottom">580</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">340</td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">74</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">520</td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">210</td><td align="left" valign="bottom">0.07</td><td align="left" valign="bottom">94</td></tr><tr><td align="left" valign="bottom">MC_RTT</td><td align="left" valign="bottom">Velocity</td><td align="left" valign="bottom">1040</td><td align="left" valign="bottom">2</td><td align="left" valign="bottom">700</td><td align="left" valign="bottom">0.15</td><td align="left" valign="bottom">17</td></tr></tbody></table></table-wrap></sec></sec><sec sec-type="appendix" id="s8-6"><title>GRU</title><p>The GRU neural network (<xref ref-type="bibr" rid="bib13">Cho et al., 2014</xref>) we use takes spike count observations (current and previous time bins), centers and normalizes those observations,<disp-formula id="equ73"><label>(S35)</label><mml:math id="m73"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⊘</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>κ</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and then feeds the observations sequentially into the network (initializing with <inline-formula><mml:math id="inf567"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>κ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>)<disp-formula id="equ74"><label>(S36)</label><mml:math id="m74"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext>sigmoid</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ75"><label>(S37)</label><mml:math id="m75"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext>sigmoid</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ76"><label>(S38)</label><mml:math id="m76"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext>tanh</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ77"><label>(S39)</label><mml:math id="m77"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>ultimately reading out behavior from the final state<disp-formula id="equ78"><label>(S40)</label><mml:math id="m78"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf568"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf569"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf570"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf571"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf572"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of GRU units. The GRU hidden states do not persist from one decoding time bin to the next. Rather, at each decoding time bin, the GRU is re-initialized with <inline-formula><mml:math id="inf573"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>κ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and run sequentially over recent history to generate an estimate of the behavior at the current time bin.</p><sec sec-type="appendix" id="s8-6-1"><title>Parameter fitting</title><p><italic>μ</italic> and σ (both column vectors of length <inline-formula><mml:math id="inf574"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) are computed as the mean and standard deviation, respectively, of the observed spike counts for each neuron in the training set. The parameters <inline-formula><mml:math id="inf575"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf576"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf577"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf578"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf579"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf580"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf581"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf582"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf583"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf584"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf585"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are then learned by training the network with the RMSProp optimization routine (<xref ref-type="bibr" rid="bib100">Tieleman, 2012</xref>) and a mean-squared error loss function. Training utilized dropout both on the linear transformation of inputs and on the linear transformation of the recurrent state.</p><table-wrap id="app1table4" position="float"><label>Appendix 1—table 4.</label><caption><title>Hyperparameters used for the GRU network.</title><p>The number of units (<inline-formula><mml:math id="inf586"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>) was optimized in the range [500, 1000], with the optimized values rounded to the closest multiple of 10. The dropout rate was optimized in the range [0,0.5] and the number of training epochs was optimized in the range [2, 50]. Window lengths were optimized (in 20 ms increments) in the range [200, 600] for Area2_Bump, [200, 1000] for MC_Cycle, [200, 1200] for MC_RTT, and [200, 700] for MC_Maze, MC_Maze-L, MC_Maze-M, and MC_Maze-S.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">Behavioral Group</th><th align="left" valign="bottom">Window Length (ms)</th><th align="left" valign="bottom">Units (<italic>D</italic>)<break/></th><th align="left" valign="bottom">Dropout Rate</th><th align="left" valign="bottom">Epochs</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="7">Area2_Bump</td><td align="left" valign="bottom">Position</td><td align="char" char="." valign="bottom">560</td><td align="char" char="." valign="bottom">700</td><td align="char" char="." valign="bottom">0.06</td><td align="char" char="." valign="bottom">12</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="char" char="." valign="bottom">340</td><td align="char" char="." valign="bottom">820</td><td align="char" char="." valign="bottom">0.32</td><td align="char" char="." valign="bottom">3</td></tr><tr><td align="left" valign="bottom">Force</td><td align="char" char="." valign="bottom">380</td><td align="char" char="." valign="bottom">380</td><td align="char" char="." valign="bottom">0.27</td><td align="char" char="." valign="bottom">26</td></tr><tr><td align="left" valign="bottom">Joint Angles</td><td align="char" char="." valign="bottom">480</td><td align="char" char="." valign="bottom">630</td><td align="char" char="." valign="bottom">0.31</td><td align="char" char="." valign="bottom">8</td></tr><tr><td align="left" valign="bottom">Joint Velocities</td><td align="char" char="." valign="bottom">260</td><td align="char" char="." valign="bottom">390</td><td align="char" char="." valign="bottom">0.27</td><td align="char" char="." valign="bottom">9</td></tr><tr><td align="left" valign="bottom">Muscle Lengths</td><td align="char" char="." valign="bottom">460</td><td align="char" char="." valign="bottom">990</td><td align="char" char="." valign="bottom">0.34</td><td align="char" char="." valign="bottom">45</td></tr><tr><td align="left" valign="bottom">Muscle Velocities</td><td align="char" char="." valign="bottom">420</td><td align="char" char="." valign="bottom">870</td><td align="char" char="." valign="bottom">0.19</td><td align="char" char="." valign="bottom">47</td></tr><tr><td align="left" valign="bottom" rowspan="5">MC_Cycle</td><td align="left" valign="bottom">Phase</td><td align="char" char="." valign="bottom">460</td><td align="char" char="." valign="bottom">580</td><td align="char" char="." valign="bottom">0.18</td><td align="char" char="." valign="bottom">49</td></tr><tr><td align="left" valign="bottom">Angular Velocity</td><td align="char" char="." valign="bottom">480</td><td align="char" char="." valign="bottom">390</td><td align="char" char="." valign="bottom">0.12</td><td align="char" char="." valign="bottom">27</td></tr><tr><td align="left" valign="bottom">Position</td><td align="char" char="." valign="bottom">920</td><td align="char" char="." valign="bottom">760</td><td align="char" char="." valign="bottom">0.06</td><td align="char" char="." valign="bottom">42</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="char" char="." valign="bottom">520</td><td align="char" char="." valign="bottom">170</td><td align="char" char="." valign="bottom">0.43</td><td align="char" char="." valign="bottom">45</td></tr><tr><td align="left" valign="bottom">EMG</td><td align="char" char="." valign="bottom">840</td><td align="char" char="." valign="bottom">250</td><td align="char" char="." valign="bottom">0.40</td><td align="char" char="." valign="bottom">36</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze</td><td align="left" valign="bottom">Position</td><td align="char" char="." valign="bottom">640</td><td align="char" char="." valign="bottom">740</td><td align="char" char="." valign="bottom">0.27</td><td align="char" char="." valign="bottom">4</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="char" char="." valign="bottom">520</td><td align="char" char="." valign="bottom">800</td><td align="char" char="." valign="bottom">0.34</td><td align="char" char="." valign="bottom">11</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze-L</td><td align="left" valign="bottom">Position</td><td align="char" char="." valign="bottom">620</td><td align="char" char="." valign="bottom">640</td><td align="char" char="." valign="bottom">0.40</td><td align="char" char="." valign="bottom">49</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="char" char="." valign="bottom">580</td><td align="char" char="." valign="bottom">420</td><td align="char" char="." valign="bottom">0.36</td><td align="char" char="." valign="bottom">31</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze-M</td><td align="left" valign="bottom">Position</td><td align="char" char="." valign="bottom">620</td><td align="char" char="." valign="bottom">820</td><td align="char" char="." valign="bottom">0.41</td><td align="char" char="." valign="bottom">48</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="char" char="." valign="bottom">660</td><td align="char" char="." valign="bottom">840</td><td align="char" char="." valign="bottom">0.29</td><td align="char" char="." valign="bottom">30</td></tr><tr><td align="left" valign="bottom" rowspan="2">MC_Maze-S</td><td align="left" valign="bottom">Position</td><td align="char" char="." valign="bottom">460</td><td align="char" char="." valign="bottom">310</td><td align="char" char="." valign="bottom">0.39</td><td align="char" char="." valign="bottom">27</td></tr><tr><td align="left" valign="bottom">Velocity</td><td align="char" char="." valign="bottom">680</td><td align="char" char="." valign="bottom">500</td><td align="char" char="." valign="bottom">0.44</td><td align="char" char="." valign="bottom">45</td></tr><tr><td align="left" valign="bottom">MC_RTT</td><td align="left" valign="bottom">Velocity</td><td align="char" char="." valign="bottom">540</td><td align="char" char="." valign="bottom">890</td><td align="char" char="." valign="bottom">0.40</td><td align="char" char="." valign="bottom">8</td></tr></tbody></table></table-wrap></sec></sec><sec sec-type="appendix" id="s8-7"><title>Naive Bayes regression</title><p>The Naive Bayes regression decoder (<xref ref-type="bibr" rid="bib123">Zhang et al., 1998</xref>, <xref ref-type="bibr" rid="bib37">Glaser et al., 2020</xref>) is fully described with code provided in <xref ref-type="bibr" rid="bib37">Glaser et al., 2020</xref>. The hyperparameters for this decoder include the density of the kinematic grid, bin size, and the number of previous bins available to the decoder. This decoder was only evaluated for the MC_Maze dataset. The best performance was achieved for a 50 x 50 grid of kinematics. For both position and velocity, hyperparameter optimization indicated use of only a single bin of spike counts (220 ms and 320 ms bin sizes for position and velocity, respectively).</p></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89421.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>École Polytechnique Fédérale de Lausanne</institution><country>Switzerland</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This paper presents a new method called MINT that is effective at BCI-style decoding tasks. The authors show <bold>convincing</bold> evidence to support their claims regarding how MINT is a new method that produces excellent decoding performance relative to the state-of-the-art. This work is <bold>important</bold> and will be of broad interest to neuroscientists and neuroengineers.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89421.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This paper presents an innovative decoding approach for brain-computer interfaces (BCIs), introducing a new method named MINT. The authors develop a trajectory-centric approach to decode behaviors across several different datasets, including eight empirical datasets from the Neural Latents Benchmark. Overall, the paper is well written and their method shows impressive performance compared to more traditional decoding approaches that use a simpler approach. While there are some concerns (see below), the paper's strengths, particularly its emphasis on a trajectory-centric approach and the simplicity of MINT, provide a compelling contribution to the field.</p><p>Strengths:</p><p>The adoption of a trajectory-centric approach that utilizes statistical constraints presents a substantial shift in methodology, potentially revolutionizing the way BCIs interpret and predict neural behaviour. This is one of the strongest aspects of the paper.</p><p>The thorough evaluation of the method across various datasets serves as an assurance that the superior performance of MINT is not a result of overfitting. The comparative simplicity of the method in contrast to many neural network approaches is refreshing and should facilitate broader applicability.</p><p>Weaknesses:</p><p>Scope: Despite the impressive performance of MINT across multiple datasets, it seems predominantly applicable to M1/S1 data. Only one of the eight empirical datasets comes from an area outside the motor/somatosensory cortex. It would be beneficial if the authors could expand further on how the method might perform with other brain regions that do not exhibit low tangling or do not have a clear trial structure (e.g. decoding of position or head direction from hippocampus)</p><p>When comparing methods, the neural trajectories of MINT are based on averaged trials, while the comparison methods are trained on single trials. An additional analysis might help in disentangling the effect of the trial averaging. For this, the authors could average the input across trials for all decoders, establishing a baseline for averaged trials. Note that inference should still be done on single trials. Performance can then be visualized across different values of N, which denotes the number of averaged trials used for training.</p><p>Comments on revisions:</p><p>I have looked at the responses and they are thorough and answer all of my questions.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89421.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The goal of this paper is to present a new method, termed MINT, for decoding behavioral states from neural spiking data. MINT is a statistical method which, in addition to outputting a decoded behavioral state, also provides soft information regarding the likelihood of that behavioral state based on the neural data. The innovation in this approach is neural states are assumed to come from sparsely distributed neural trajectories with low tangling, meaning that neural trajectories (time sequences of neural states) are sparse in the high-dimensional space of neural spiking activity and that two dissimilar neural trajectories tend to correspond to dissimilar behavioral trajectories. The authors support these assumptions through analysis of previously collected data, and then validate the performance of their method by comparing it to a suite of alternative approaches. The authors attribute the typically improved decoding performance by MINT to its assumptions being more faithfully aligned to the properties of neural spiking data relative to assumptions made by the alternatives.</p><p>Strengths:</p><p>The paper did an excellent job critically evaluating common assumptions made by neural analytical methods, such as neural state being low-dimensional relative to the number of recorded neurons. The authors made strong arguments, supported by evidence and literature, for potentially high-dimensional neural states and thus the need for approaches that do not rely on an assumption of low dimensionality.</p><p>The paper was thorough in considering multiple datasets across a variety of behaviors, as well as existing decoding methods, to benchmark the MINT approach. This provided a valuable comparison to validate the method. The authors also provided nice intuition regarding why MINT may offer performance improvement in some cases and in which instances MINT may not perform as well.</p><p>In addition to providing a philosophical discussion as to the advantages of MINT and benchmarking against alternatives, the authors also provided a detailed description of practical considerations. This included training time, amount of training data, robustness to data loss or changes in the data, and interpretability. These considerations not only provided objective evaluation of practical aspects but also provided insights to the flexibility and robustness of the method as they relate back to the underlying assumptions and construction of the approach.</p><p>Impact:</p><p>This work is motivated by brain-computer interfaces applications, which it will surely impact in terms of neural decoder design. However, this work is also broadly impactful for neuroscientific analysis to relate neural spiking activity to observable behavioral features. Thus, MINT will likely impact neuroscience research generally. The methods are made publicly available, and the datasets used are all in public repositories, which facilitates adoption and validation of this method within the greater scientific community.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89421.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Perkins</surname><given-names>Sean M</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Amematsro</surname><given-names>Elom A</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Cunningham</surname><given-names>John</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Qi</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Churchland</surname><given-names>Mark M</given-names></name><role specific-use="author">Author</role><aff><institution>Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p>Summary of reviewers’ comments and our revisions:</p><p>We thank the reviewers for their thoughtful feedback. This feedback has motivated multiple revisions and additions that, in our view, have greatly improved the manuscript. This is especially true with regard to a major goal of this study: clearly defining existing scientific perspectives and delineating their decoding implications. In addition to building on this conceptual goal, we have expanded existing analyses and have added a new analysis of generalization using a newly collected dataset. We expect the manuscript will be of very broad interest, both to those interested in BCI development and to those interested in fundamental properties of neural population activity and its relationship with behavior.</p><p>Importantly, all reviewers were convinced that MINT provided excellent performance, when benchmarked against existing methods, across a broad range of standard tasks:</p><p>“their method shows impressive performance compared to more traditional decoding approaches” (R1)</p><p>“The paper was thorough in considering multiple datasets across a variety of behaviors, as well as existing decoding methods, to benchmark the MINT approach. This provided a valuable comparison to validate the method.” (R2)</p><p>“The fact that performance on stereotyped tasks is high is interesting and informative…” (R3)</p><p>This is important. It is challenging to design a decoder that performs consistently across multiple domains and across multiple situations (including both decoding and neural state estimation). MINT does so. MINT consistently outperformed existing lightweight ‘interpretable’ decoders, despite being a lightweight interpretable decoder itself. MINT was very competitive with expressive machine-learning methods, yet has advantages in flexibility and simplicity that more ‘brute force’ methods do not. We made a great many comparisons, and MINT was consistently a strong performer. Of the many comparisons we made, there was only one where MINT was at a modest disadvantage, and it was for a dataset where all methods performed poorly. No other method we tested was as consistent. For example, although the GRU and the feedforward network were often competitive with MINT (and better than MINT in the one case mentioned above), there were multiple other situations where they performed less well and a few situations where they performed poorly. Moreover, no other existing decoder naturally estimates the neural state while also readily decoding, without retraining, a broad range of behavioral variables.</p><p>R1 and R2 were very positive about the broader impacts of the study. They stressed its impact both on decoder design, and on how our field thinks, scientifically, about the population response in motor areas:</p><p>“This paper presents an innovative decoding approach for brain-computer interfaces” (R1)</p><p>“presents a substantial shift in methodology, potentially revolutionizing the way BCIs interpret and predict neural behaviour” (R1)</p><p>“the paper's strengths, particularly its emphasis on a trajectory-centric approach and the simplicity of MINT, provide a compelling contribution to the field” (R1)</p><p>“The authors made strong arguments, supported by evidence and literature, for potentially high-dimensional neural states and thus the need for approaches that do not rely on an assumption of low dimensionality” (R2)</p><p>“This work is motivated by brain-computer interfaces applications, which it will surely impact in terms of neural decoder design.” (R2)</p><p>“this work is also broadly impactful for neuroscientific analysis... Thus, MINT will likely impact neuroscience research generally.” (R2)</p><p>We agree with these assessments, and have made multiple revisions to further play into these strengths. As one example, the addition of Figure 1b (and 6b) makes this the first study, to our knowledge, to fully and concretely illustrate this emerging scientific perspective and its decoding implications. This is important, because multiple observations convince us that the field is likely to move away from the traditional perspective in Figure 1a, and towards that in Figure 1b. We also agree with the handful of weaknesses R1 and R2 noted. The manuscript has been revised accordingly. The major weakness noted by R1 was the need to be explicit regarding when we suspect MINT would (and wouldn’t) work well in other brain areas. In non-motor areas, the structure of the data may be poorly matched with MINT’s assumptions. We agree that this is likely to be true, and thus agree with the importance of clarifying this topic for the reader. The revision now does so. R1 also wished to know whether existing methods might benefit from including trial-averaged data during training, something we now explore and document (see detailed responses below). R2 noted two weaknesses: (1) The need to better support (with expanded analysis) the statement that neural and behavioral trajectories are non-isometric, and (2) The need to more rigorously define the ‘mesh’. We agree entirely with both suggestions, and the revision has been strengthened by following them (see detailed responses below).</p><p>R3 also saw strengths to the work, stating that:</p><p>“This paper is well-structured and its main idea is clear.”</p><p>“The fact that performance on stereotyped tasks is high is interesting and informative, showing that these stereotyped tasks create stereotyped neural trajectories.”</p><p>“The task-specific comparisons include various measures and a variety of common decoding approaches, which is a strength.”</p><p>However, R3 also expressed two sizable concerns. The first is that MINT might have onerous memory requirements. The manuscript now clarifies that MINT has modest memory requirements. These do not scale unfavorably as the reviewer was concerned they might. The second concern is that MINT is:</p><p>“essentially a table-lookup rather than a model.”</p><p>Although we don’t agree, the concern makes sense and may be shared by many readers, especially those who take a particular scientific perspective. Pondering this concern thus gave us the opportunity to modify the manuscript in ways that support its broader impact. Our revisions had two goals: (1) clarify the ways in which MINT is far more flexible than a lookup-table, and (2) better describe the dominant scientific perspectives and their decoding implications.</p><p>The heart of R3’s concern is the opinion that MINT is an effective but unprincipled hack suitable for situations where movements are reasonably stereotyped. Of course, many tasks involve stereotyped movements (e.g. handwriting characters), so MINT would still be useful. Nevertheless, if MINT is not principled, other decode methods would often be preferable because they could (unlike MINT in R3’s opinion) gain flexibility by leveraging an accurate model. Most of R3’s comments flow from this fundamental concern:</p><p>“This is again due to MINT being a lookup table with a library of stereotyped trajectories rather than a model.”</p><p>“MINT models task-dependent neural trajectories, so the trained decoder is very task-dependent and cannot generalize to other tasks.”</p><p>“Unlike MINT, these works can achieve generalization because they model the neural subspace and its association to movement.”</p><p>“given that MINT tabulates task-specific trajectories, it will not generalize to tasks that are not seen in the training data even when these tasks cover the exact same space (e.g., the same 2D computer screen and associated neural space).”</p><p>“For proper training, the training data should explore the whole movement space and the associated neural space, but this does not mean all kinds of tasks performed in that space must be included in the training set (something MINT likely needs while modeling-based approaches do not).”</p><p>The manuscript has been revised to clarify that MINT is considerably more flexible than a lookup table, even though a lookup table is used as a first step. Yet, on its own, this does not fully address R3’s concern. The quotes above highlight that R3 is making a standard assumption in our field: that there exists a “movement space and associated neural space”. Under this perspective, one should, as R3 argues fully explore the movement space. This would perforce fully explore the associated neural subspace. One can then “model the neural subspace and its association to movement”. MINT does not use a model of this type, and thus (from R3’s perspective) does not appear to use a model at all. A major goal of our study is to question this traditional perspective. We have thus added a new figure to highlight the contrast between the traditional (Figure 1a) and new (Figure 1b) scientific perspectives, and to clarify their decoding implications.</p><p>While we favor the new perspective (Figure 1b), we concede that R3 may not share our view. This is fine. Part of the reason we believe this study is timely, and will be broadly read, is that it raises a topic of emerging interest where there is definitely room for debate. If we are misguided – i.e. if Figure 1a is the correct perspective – then many of R3’s concerns would be on target: MINT could still be useful, but traditional methods that make the traditional assumptions in Figure 1a would often be preferable. However, if the emerging perspective in Figure 1b is more accurate, then MINT’s assumptions would be better aligned with the data than those of traditional methods, making it a more (not less) principled choice.</p><p>Our study provides new evidence in support of Figure 1b, while also synthesizing existing evidence from other recent studies. In addition to Figure 2, the new analysis of generalization further supports Figure 1b. Also supporting Figure 1b is the analysis in which MINT’s decoding advantage, over a traditional decoder, disappears when simulated data approximate the traditional perspective in Figure 1a.</p><p>That said, we agree that the present study cannot fully resolve whether Figure 1a or 1b is more accurate. Doing so will take multiple studies with different approaches (indeed we are currently preparing other manuscripts on this topic). Yet we still have an informed scientific opinion, derived from past, present and yet-to-be-published observations. Our opinion is that Figure 1b is the more accurate perspective. This possibility makes it reasonable to explore the potential virtues of a decoding method whose assumptions are well-aligned with that perspective. MINT is such a method. As expected under Figure 1b, MINT outperforms traditional interpretable decoders in every single case we studied.</p><p>As noted above, we have added a new generalization-focused analysis (Figure 6) based on a newly collected dataset. We did so because R3’s comments highlight a deep point: which scientific perspective one takes has strong implications regarding decoder generalization. These implications are now illustrated in the new Figure 6a and 6b. Under Figure 6a, it is possible, as R3 suggests, to explore “the whole movement space and associated neural space” during training. However, under Figure 6b, expectations are very different. Generalization will be ‘easy’ when new trajectories are near the training-set trajectories. In this case, MINT should generalize well as should other methods. In contrast, generalization will be ‘hard’ when new neural trajectories have novel shapes and occupy previously unseen regions / dimensions. In this case, all current methods, including MINT, are likely to fail. R3 points out that traditional decoders have sometimes generalized well to new tasks (e.g. from center-out to ‘pinball’) when cursor movements occur in the same physical workspace. These findings could be taken to support Figure 6a, but are equally consistent with ‘easy’ generalization in Figure 6b. To explore this topic, the new analysis in Figure 6c-g considers conditions that are intended to span the range from easy to hard. Results are consistent with the predictions of Figure 6b.</p><p>We believe the manuscript has been significantly improved by these additions. The revisions help the manuscript achieve its twin goals: (1) introduce a novel class of decoder that performs very well despite being very simple, and (2) describe properties of motor-cortex activity that will matter for decoders of all varieties.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1:</bold></p><p>Summary:</p><p>This paper presents an innovative decoding approach for brain-computer interfaces (BCIs), introducing a new method named MINT. The authors develop a trajectory-centric approach to decode behaviors across several different datasets, including eight empirical datasets from the Neural Latents Benchmark. Overall, the paper is well written and their method shows impressive performance compared to more traditional decoding approaches that use a simpler approach. While there are some concerns (see below), the paper's strengths, particularly its emphasis on a trajectory-centric approach and the simplicity of MINT, provide a compelling contribution to the field.</p></disp-quote><p>We thank the reviewer for these comments. We share their enthusiasm for the trajectory-centric approach, and we are in complete agreement that this perspective has both scientific and decoding implications. The revision expands upon these strengths.</p><disp-quote content-type="editor-comment"><p>Strengths:</p><p>The adoption of a trajectory-centric approach that utilizes statistical constraints presents a substantial shift in methodology, potentially revolutionizing the way BCIs interpret and predict neural behaviour. This is one of the strongest aspects of the paper.</p></disp-quote><p>Again, thank you. We also expect the trajectory-centric perspective to have a broad impact, given its relevance to both decoding and to thinking about manifolds.</p><p>The thorough evaluation of the method across various datasets serves as an assurance that the superior performance of MINT is not a result of overfitting. The comparative simplicity of the method in contrast to many neural network approaches is refreshing and should facilitate broader applicability.</p><p>Thank you. We were similarly pleased to see such a simple method perform so well. We also agree that, while neural-network approaches will always be important, it is desirable to also possess simple ‘interpretable’ alternatives.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>Comment (1) Scope: Despite the impressive performance of MINT across multiple datasets, it seems predominantly applicable to M1/S1 data. Only one of the eight empirical datasets comes from an area outside the motor/somatosensory cortex. It would be beneficial if the authors could expand further on how the method might perform with other brain regions that do not exhibit low tangling or do not have a clear trial structure (e.g. decoding of position or head direction from hippocampus)</p></disp-quote><p>We agree entirely. Population activity in many brain areas (especially outside the motor system) presumably will often not have the properties upon which MINT’s assumptions are built. This doesn’t necessarily mean that MINT would perform badly. Using simulated data, we have found that MINT can perform surprisingly well even when some of its assumptions are violated. Yet at the same time, when MINT’s assumptions don’t apply, one would likely prefer to use other methods. This is, after all, one of the broader themes of the present study: it is beneficial to match decoding assumptions to empirical properties. We have thus added a section on this topic early in the Discussion:</p><p>“In contrast, MINT and the Kalman filter performed comparably on simulated data that better approximated the assumptions in Figure 1a. Thus, MINT is not a ‘better’ algorithm – simply better aligned with the empirical properties of motor cortex data. This highlights an important caveat. Although MINT performs well when decoding from motor areas, its assumptions may be a poor match in other areas (e.g. the hippocampus). MINT performed well on two non-motor-cortex datasets – Area2_Bump (S1) and DMFC_RSG (dorsomedial frontal cortex) – yet there will presumably be other brain areas and/or contexts where one would prefer a different method that makes assumptions appropriate for that area.”</p><disp-quote content-type="editor-comment"><p>Comment (2) When comparing methods, the neural trajectories of MINT are based on averaged trials, while the comparison methods are trained on single trials. An additional analysis might help in disentangling the effect of the trial averaging. For this, the authors could average the input across trials for all decoders, establishing a baseline for averaged trials. Note that inference should still be done on single trials. Performance can then be visualized across different values of N, which denotes the number of averaged trials used for training.</p></disp-quote><p>We explored this question and found that the non-MINT decoders are harmed, not helped, by the inclusion of trial-averaged responses in the training set. This is presumably because the statistics of trialaveraged responses don’t resemble what will be observed during decoding. This statistical mismatch, between training and decoding, hurts most methods. It doesn’t hurt MINT, because MINT doesn’t ‘train’ in the normal way. It simply needs to know rates, and trial-averaging is a natural way to obtain them. To describe the new analysis, we have added the following to the text.</p><p>“We also investigated the possibility that MINT gained its performance advantage simply by having access to trial-averaged neural trajectories during training, while all other methods were trained on single-trial data. This difference arises from the fundamental requirements of the decoder architectures: MINT needs to estimate typical trajectories while other methods don’t. Yet it might still be the case that other methods would benefit from including trial-averaged data in the training set, in addition to single-trial data. Alternatively, this might harm performance by creating a mismatch, between training and decoding, in the statistics of decoder inputs. We found that the latter was indeed the case: all non-MINT methods performed better when trained purely on single-trial data.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2</bold>:</p><p>Summary:</p><p>The goal of this paper is to present a new method, termed MINT, for decoding behavioral states from neural spiking data. MINT is a statistical method which, in addition to outputting a decoded behavioral state, also provides soft information regarding the likelihood of that behavioral state based on the neural data. The innovation in this approach is neural states are assumed to come from sparsely distributed neural trajectories with low tangling, meaning that neural trajectories (time sequences of neural states) are sparse in the high-dimensional space of neural spiking activity and that two dissimilar neural trajectories tend to correspond to dissimilar behavioral trajectories. The authors support these assumptions through analysis of previously collected data, and then validate the performance of their method by comparing it to a suite of alternative approaches. The authors attribute the typically improved decoding performance by MINT to its assumptions being more faithfully aligned to the properties of neural spiking data relative to assumptions made by the alternatives.</p></disp-quote><p>We thank the reviewer for this accurate summary, and for highlighting the subtle but important fact that MINT provides information regarding likelihoods. The revision includes a new analysis (Figure 6e) illustrating one potential way to leverage knowledge of likelihoods.</p><disp-quote content-type="editor-comment"><p>Strengths:</p><p>The paper did an excellent job critically evaluating common assumptions made by neural analytical methods, such as neural state being low-dimensional relative to the number of recorded neurons. The authors made strong arguments, supported by evidence and literature, for potentially high-dimensional neural states and thus the need for approaches that do not rely on an assumption of low dimensionality.</p></disp-quote><p>Thank you. We also hope that the shift in perspective is the most important contribution of the study. This shift matters both scientifically and for decoder design. The revision expands on this strength. The scientific alternatives are now more clearly and concretely illustrated (especially see Figure 1a,b and Figure 6a,b). We also further explore their decoding implications with new data (Figure 6c-g).</p><disp-quote content-type="editor-comment"><p>The paper was thorough in considering multiple datasets across a variety of behaviors, as well as existing decoding methods, to benchmark the MINT approach. This provided a valuable comparison to validate the method. The authors also provided nice intuition regarding why MINT may offer performance improvement in some cases and in which instances MINT may not perform as well.</p></disp-quote><p>Thank you. We were pleased to be able to provide comparisons across so many datasets (we are grateful to the Neural Latents Benchmark for making this possible).</p><disp-quote content-type="editor-comment"><p>In addition to providing a philosophical discussion as to the advantages of MINT and benchmarking against alternatives, the authors also provided a detailed description of practical considerations. This included training time, amount of training data, robustness to data loss or changes in the data, and interpretability. These considerations not only provided objective evaluation of practical aspects but also provided insights to the flexibility and robustness of the method as they relate back to the underlying assumptions and construction of the approach.</p></disp-quote><p>Thank you. We are glad that these sections were appreciated. MINT’s simplicity and interpretability are indeed helpful in multiple ways, and afford opportunities for interesting future extensions. One potential benefit of interpretability is now explored in the newly added Figure 6e.</p><disp-quote content-type="editor-comment"><p>Impact:</p><p>This work is motivated by brain-computer interfaces applications, which it will surely impact in terms of neural decoder design. However, this work is also broadly impactful for neuroscientific analysis to relate neural spiking activity to observable behavioral features. Thus, MINT will likely impact neuroscience research generally. The methods are made publicly available, and the datasets used are all in public repositories, which facilitates adoption and validation of this method within the greater scientific community.</p></disp-quote><p>Again, thank you. We have similar hopes for this study.</p><p>Weaknesses (1 &amp; 2 are related, and we have switched their order in addressing them):</p><disp-quote content-type="editor-comment"><p>Comment (2) With regards to the idea of neural and behavioral trajectories having different geometries, this is dependent on what behavioral variables are selected. In the example for Fig 2a, the behavior is reach position. The geometry of the behavioral trajectory of interest would look different if instead the behavior of interest was reach velocity. The paper would be strengthened by acknowledgement that geometries of trajectories are shaped by extrinsic choices rather than (or as much as they are) intrinsic properties of the data.</p></disp-quote><p>We agree. Indeed, we almost added a section to the original manuscript on this exact topic. We have now done so:</p><p>“A potential concern regarding the analyses in Figure 2c,d is that they require explicit choices of behavioral variables: muscle population activity in Figure 2c and angular phase and velocity in Figure 2d. Perhaps these choices were misguided. Might neural and behavioral geometries become similar if one chooses ‘the right’ set of behavioral variables? This concern relates to the venerable search for movement parameters that are reliably encoded by motor cortex activity [69, 92–95]. If one chooses the wrong set of parameters (e.g. chooses muscle activity when one should have chosen joint angles) then of course neural and behavioral geometries will appear non-isometric. There are two reasons why this ‘wrong parameter choice’ explanation is unlikely to account for the results in Figure 2c,d. First, consider the implications of the left-hand side of Figure 2d. A small kinematic distance implies that angular position and velocity are nearly identical for the two moments being compared. Yet the corresponding pair of neural states can be quite distant. Under the concern above, this distance would be due to other encoded behavioral variables – perhaps joint angle and joint velocity – differing between those two moments. However, there are not enough degrees of freedom in this task to make this plausible. The shoulder remains at a fixed position (because the head is fixed) and the wrist has limited mobility due to the pedal design [60]. Thus, shoulder and elbow angles are almost completely determined by cycle phase. More generally, ‘external variables’ (positions, angles, and their derivatives) are unlikely to differ more than slightly when phase and angular velocity are matched. Muscle activity could be different because many muscles act on each joint, creating redundancy. However, as illustrated in Figure 2c, the key effect is just as clear when analyzing muscle activity. Thus, the above concern seems unlikely even if it can’t be ruled out entirely. A broader reason to doubt the ‘wrong parameter choice’ proposition is that it provides a vague explanation for a phenomenon that already has a straightforward explanation. A lack of isometry between the neural population response and behavior is expected when neural-trajectory tangling is low and output-null factors are plentiful [55, 60]. For example, in networks that generate muscle activity, neural and muscle-activity trajectories are far from isometric [52, 58, 60]. Given this straightforward explanation, and given repeated failures over decades to find the ‘correct’ parameters (muscle activity, movement direction, etc.) that create neural-behavior isometry, it seems reasonable to conclude that no such isometry exists.”</p><disp-quote content-type="editor-comment"><p>Comment (1) The authors posit that neural and behavioral trajectories are non-isometric. To support this point, they look at distances between neural states and distances between the corresponding behavioral states, in order to demonstrate that there are differences in these distances in each respective space. This supports the idea that neural states and behavioral states are non-isometric but does not directly address their point. In order to say the trajectories are non-isometric, it would be better to look at pairs of distances between corresponding trajectories in each space.</p></disp-quote><p>We like this idea and have added such an analysis. To be clear, we like the original analysis too: isometry predicts that neural and behavioral distances (for corresponding pairs of points) should be strongly correlated, and that small behavioral distances should not be associated with large neural distances. These predictions are not true, providing a strong argument against isometry. However, we also like the reviewer’s suggestion, and have added such an analysis. It makes the same larger point, and also reveals some additional facts (e.g. it reveals that muscle-geometry is more related to neural-geometry than is kinematic-geometry). The new analysis is described in the following section:</p><p>“We further explored the topic of isometry by considering pairs of distances. To do so, we chose two random neural states and computed their distance, yielding dneural1. We repeated this process, yielding dneural2. We then computed the corresponding pair of distances in muscle space (dmuscle1 and dmuscle2) and kinematic space (dkin1 and dkin2). We considered cases where dneural1 was meaningfully larger than (or smaller than) dneural2, and asked whether the behavioral variables had the same relationship; e.g. was dmuscle1 also larger than dmuscle2? For kinematics, this relationship was weak: across 100,000 comparisons, the sign of dkin1 − dkin2 agreed with dneural1 − dneural2 only 67.3% of the time (with 50% being chance). The relationship was much stronger for muscles: the sign of dmuscle1 − dmuscle2 agreed with dneural1 − dneural2 79.2% of the time, which is far more than expected by chance yet also far from what is expected given isometry (e.g. the sign agrees 99.7% of the time for the truly isometric control data in Figure 2e). Indeed there were multiple moments during this task when dneural1 was much larger than dneural2, yet dmuscle1 was smaller than dmuscle2. These observations are consistent with the proposal that neural trajectories resemble muscle trajectories in some dimensions, but with additional output-null dimensions that break the isometry [60].”</p><disp-quote content-type="editor-comment"><p>Comment (3) The approach is built up on the idea of creating a &quot;mesh&quot; structure of possible states. In the body of the paper the definition of the mesh was not entirely clear and I could not find in the methods a more rigorous explicit definition. Since the mesh is integral to the approach, the paper would be improved with more description of this component.</p></disp-quote><p>This is a fair criticism. Although MINTs actual operations were well-documented, how those operations mapped onto the term ‘mesh’ was, we agree, a bit vague. The definition of the mesh is a bit subtle because it only emerges during decoding rather than being precomputed. This is part of what gives MINT much more flexibility than a lookup table. We have added the following to the manuscript.</p><p>“We use the term ‘mesh’ to describe the scaffolding created by the training-set trajectories and the interpolated states that arise at runtime. The term mesh is apt because, if MINT’s assumptions are correct, interpolation will almost always be local. If so, the set of decodable states will resemble a mesh, created by line segments connecting nearby training-set trajectories. However, this mesh-like structure is not enforced by MINT’s operations.</p><p>Interpolation could, in principle, create state-distributions that depart from the assumption of a sparse manifold. For example, interpolation could fill in the center of the green tube in Figure 1b, resulting in a solid manifold rather than a mesh around its outer surface. However, this would occur only if spiking observations argued for it. As will be documented below, we find that essentially all interpolation is local”</p><p>We have also added Figure 4d. This new analysis documents the fact that decoded states are near trainingset trajectories, which is why the term ‘mesh’ is appropriate.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>Summary:</p><p>This manuscript develops a new method termed MINT for decoding of behavior. The method is essentially a table-lookup rather than a model. Within a given stereotyped task, MINT tabulates averaged firing rate trajectories of neurons (neural states) and corresponding averaged behavioral trajectories as stereotypes to construct a library. For a test trial with a realized neural trajectory, it then finds the closest neural trajectory to it in the table and declares the associated behavior trajectory in the table as the decoded behavior. The method can also interpolate between these tabulated trajectories. The authors mention that the method is based on three key assumptions: (1) Neural states may not be embedded in a lowdimensional subspace, but rather in a high-dimensional space. (2) Neural trajectories are sparsely distributed under different behavioral conditions. (3) These neural states traverse trajectories in a stereotyped order.</p><p>The authors conducted multiple analyses to validate MINT, demonstrating its decoding of behavioral trajectories in simulations and datasets (Figures 3, 4). The main behavior decoding comparison is shown in Figure 4. In stereotyped tasks, decoding performance is comparable (M_Cycle, MC_Maze) or better (Area 2_Bump) than other linear/nonlinear algorithms</p><p>(Figure 4). However, MINT underperforms for the MC_RTT task, which is less stereotyped (Figure 4).</p><p>This paper is well-structured and its main idea is clear. The fact that performance on stereotyped tasks is high is interesting and informative, showing that these stereotyped tasks create stereotyped neural trajectories. The task-specific comparisons include various measures and a variety of common decoding approaches, which is a strength. However, I have several major concerns. I believe several of the conclusions in the paper, which are also emphasized in the abstract, are not accurate or supported, especially about generalization, computational scalability, and utility for BCIs. MINT is essentially a table-lookup algorithm based on stereotyped task-dependent trajectories and involves the tabulation of extensive data to build a vast library without modeling. These aspects will limit MINT's utility for real-world BCIs and tasks. These properties will also limit MINT's generalizability from task to task, which is important for BCIs and thus is commonly demonstrated in BCI experiments with other decoders without any retraining. Furthermore, MINT's computational and memory requirements can be prohibitive it seems. Finally, as MINT is based on tabulating data without learning models of data, I am unclear how it will be useful in basic investigations of neural computations. I expand on these concerns below.</p></disp-quote><p>We thank the reviewer for pointing out weaknesses in our framing and presentation. The comments above made us realize that we needed to (1) better document the ways in which MINT is far more flexible than a lookup-table, and (2) better explain the competing scientific perspectives at play. R3’s comments also motivated us to add an additional analysis of generalization. In our view the manuscript is greatly improved by these additions. Specifically, these additions directly support the broader impact that we hope the study will have.</p><p>For simplicity and readability, we first group and summarize R3’s main concerns in order to better address them. (These main concerns are all raised above, in addition to recurring in the specific comments below. Responses to each individual specific comment are provided after these summaries.)</p><p>(1) R3 raises concerns about ‘computational scalability.’ The concern is that “MINT's computational and memory requirements can be prohibitive.” This point was expanded upon in a specific comment, reproduced below:</p><disp-quote content-type="editor-comment"><p>I also find the statement in the abstract and paper that &quot;computations are simple, scalable&quot; to be inaccurate. The authors state that MINT's computational cost is O(NC) only, but it seems this is achieved at a high memory cost as well as computational cost in training. The process is described in section &quot;Lookup table of log-likelihoods&quot; on line [978-990]. The idea is to precompute the log-likelihoods for any combination of all neurons with discretization x all delay/history segments x all conditions and to build a large lookup table for decoding. Basically, the computational cost of precomputing this table is O(V^{Nτ} x TC) and the table requires a memory of O(V^{Nτ}), where V is the number of discretization points for the neural firing rates, N is the number of neurons, τ is the history length, T is the trial length, and C is the number of conditions. This is a very large burden, especially the V^{Nτ} term. This cost is currently not mentioned in the manuscript and should be clarified in the main text. Accordingly, computation claims should be modified including in the abstract.</p></disp-quote><p>The revised manuscript clarifies that our statement (that computations are simple and scalable) is absolutely accurate. There is no need to compute, or store, a massive lookup table. There are three tables: two of modest size and one that is tiny. This is now better explained:</p><p>“Thus, the log-likelihood of <inline-formula><mml:math id="sa3m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, for a particular current neural state, is simply the sum of many individual log-likelihoods (one per neuron and time-bin). Each individual log-likelihood depends on only two numbers: the firing rate at that moment and the spike count in that bin. To simplify online computation, one can precompute the log-likelihood, under a Poisson model, for every plausible combination of rate and spike-count. For example, a lookup table of size 2001 × 21 is sufficient when considering rates that span 0-200 spikes/s in increments of 0.1 spikes/s, and considering 20 ms bins that contain at most 20 spikes (only one lookup table is ever needed, so long as its firing-rate range exceeds that of the most-active neuron at the most active moment in Ω). Now suppose we are observing a population of 200 neurons, with a 200 ms history divided into ten 20 ms bins. For each library state, the log-likelihood of the observed spike-counts is simply the sum of 200 × 10 = 2000 individual loglikelihoods, each retrieved from the lookup table. In practice, computation is even simpler because many terms can be reused from the last time bin using a recursive solution (Methods). This procedure is lightweight and amenable to real-time applications.”</p><p>In summary, the first table simply needs to contain the firing rate of each neuron, for each condition, and each time in that condition. This table consumes relatively little memory. Assuming 100 one-second-long conditions (rates sampled every 20 ms) and 200 neurons, the table would contain 100 x 50 x 200 = 1,000,000 numbers. These numbers are typically stored as 16-bit integers (because rates are quantized), which amounts to about 2 MB. This is modest, given that most computers have (at least) tens of GB of RAM. A second table would contain the values for each behavioral variable, for each condition, and each time in that condition. This table might contain behavioral variables at a finer resolution (e.g. every millisecond) to enable decoding to update in between 20 ms bins (1 ms granularity is not needed for most BCI applications, but is the resolution used in this study). The number of behavioral variables of interest for a particular BCI application is likely to be small, often 1-2, but let’s assume for this example it is 10 (e.g. x-, y-, and z-position, velocity, and acceleration of a limb, plus one other variable). This table would thus contain 100 x 1000 x 10 = 1,000,000 floating point numbers, i.e. an 8 MB table. The third table is used to store the probability of s spikes being observed given a particular quantized firing rate (e.g. it may contain probabilities associated with firing rates ranging from 0 – 200 spikes/s in 0.1 spikes/s increments). This table is not necessary, but saves some computation time by precomputing numbers that will be used repeatedly. This is a very small table (typically ~2000 x 20, i.e. 320 KB). It does not need to be repeated for different neurons or conditions, because Poisson probabilities depend on only rate and count.</p><p>(2) R3 raises a concern that MINT “is essentially a table-lookup rather than a model.’ R3 states that MINT</p><disp-quote content-type="editor-comment"><p>“is essentially a table-lookup algorithm based on stereotyped task-dependent trajectories and involves the tabulation of extensive data to build a vast library without modeling.”</p></disp-quote><p>and that,</p><disp-quote content-type="editor-comment"><p>“as MINT is based on tabulating data without learning models of data, I am unclear how it will be useful in basic investigations of neural computations.”</p></disp-quote><p>This concern is central to most subsequent concerns. The manuscript has been heavily revised to address it. The revisions clarify that MINT is much more flexible than a lookup table, even though MINT uses a lookup table as its first step. Because R3’s concern is intertwined with one’s scientific assumptions, we have also added the new Figure 1 to explicitly illustrate the two key scientific perspectives and their decoding implications.</p><p>Under the perspective in Figure 1a, R3 would be correct in saying that there exist traditional interpretable decoders (e.g. a Kalman filter) whose assumptions better model the data. Under this perspective, MINT might still be an excellent choice in many cases, but other methods would be expected to gain the advantage when situations demand more flexibility. This is R3’s central concern, and essentially all other concerns flow from it. It makes sense that R3 has this concern, because their comments repeatedly stress a foundational assumption of the perspective in Figure 1a: the assumption of a fixed lowdimensional neural subspace where activity has a reliable relationship to behavior that can be modeled and leveraged during decoding. The phrases below accord with that view:</p><p>“Unlike MINT, these works can achieve generalization because they model the neural subspace and its association to movement.”</p><p>“it will not generalize… even when these tasks cover the exact same space (e.g., the same 2D computer screen and associated neural space).”</p><p>“For proper training, the training data should explore the whole movement space and the associated neural space”</p><p>“I also believe the authors should clarify the logic behind developing MINT better. From a scientific standpoint, we seek to gain insights into neural computations by making various assumptions and building models that parsimoniously describe the vast amount of neural data rather than simply tabulating the data. For instance, low-dimensional assumptions have led to the development of numerous dimensionality reduction algorithms and these models have led to important interpretations about the underlying dynamics”</p><p>Thus, R3 prefers a model that (1) assumes a low-dimensional subspace that is fixed across tasks and (2) assumes a consistent ‘association’ between neural activity and kinematics. Because R3 believes this is the correct model of the data, they believe that decoders should leverage it. Traditional interpretable method do, and MINT doesn’t, which is why they find MINT to be unprincipled. This is a reasonable view, but it is not our view. We have heavily revised the manuscript to clarify that a major goal of our study is to explore the implications of a different, less-traditional scientific perspective.</p><p>The new Figure 1a illustrates the traditional perspective. Under this perspective, one would agree with R3’s claim that other methods have the opportunity to model the data better. For example, suppose there exists a consistent neural subspace – conserved across tasks – where three neural dimensions encode 3D hand position and three additional neural dimensions encode 3D hand velocity. A traditional method such as a Kalman filter would be a very appropriate choice to model these aspects of the data.</p><p>Figure 1b illustrates the alternative scientific perspective. This perspective arises from recent, present, and to-be-published observations. MINT’s assumptions are well-aligned with this perspective. In contrast, the assumptions of traditional methods e.g. the Kalman filter are not well-aligned with the properties of the data under this perspective. This does not mean traditional methods are not useful. Yet under Figure 1b, it is traditional methods, such as the Kalman filter, that lack an accurate model of the data. Of course, the reviewer may disagree with our scientific perspective. We would certainly concede that there is room for debate. However, we find the evidence for Figure 1b to be sufficiently strong that it is worth exploring the utility of methods that align with this scientific perspective. MINT is such a method. As we document, it performs very well.</p><p>Thus, in our view, MINT is quite principled because its assumptions are well aligned with the data. It is true that the features of the data that MINT models are a bit different from those that are traditionally modeled. For example, R3 is quite correct that MINT does not attempt to use a biomimetic model of the true transformation from neural activity, to muscle activity, and thence to kinematics. We see this as a strength, and the manuscript has been revised accordingly (see paragraph beginning with “We leveraged this simulated data to compare MINT with a biomimetic decoder”).</p><p>(3) R3 raises concerns that MINT cannot generalize. This was a major concern of R3 and is intimately related to concern #2 above. The concern is that, if MINT is “essentially a lookup table” that simply selects pre-defined trajectories, then MINT will not be able to generalize. R3 is quite correct that MINT generalizes rather differently than existing methods. Whether this is good or bad depends on one’s scientific perspective. Under Figure 1a, MINT’s generalization would indeed be limiting because other methods could achieve greater flexibility. Under Figure 1b, all methods will have serious limits regarding generalization. Thus, MINT’s method for generalizing may approximate the best one can presently do. To address this concern, we have made three major changes, numbered i-iii below:</p><p>i) Large sections of the manuscript have been restructured to underscore the ways in which MINT can generalize. A major goal was to counter the impression, stated by R3 above, that:</p><disp-quote content-type="editor-comment"><p>“for a test trial with a realized neural trajectory, [MINT] then finds the closest neural trajectory to it in the table and declares the associated behavior trajectory in the table as the decoded behavior”.</p></disp-quote><p>This description is a reasonable way to initially understand how MINT works, and we concede that we may have over-used this intuition. Unfortunately, it can leave the misimpression that MINT decodes by selecting whole trajectories, each corresponding to ‘a behavior’. This can happen, but it needn’t and typically doesn’t. As an example, consider the cycling task. Suppose that the library consists of stereotyped trajectories, each four cycles long, at five fixed speeds from 0.5-2.5 Hz. If the spiking observations argued for it, MINT could decode something close to one of these five stereotyped trajectories. Yet it needn’t. Decoded trajectories will typically resemble library trajectories locally, but may be very different globally. For example, a decoded trajectory could be thirty cycles long (or two, or five hundred) perhaps speeding up and slowing down multiple times across those cycles.</p><p>Thus, the library of trajectories shouldn’t be thought of as specifying a limited set of whole movements that can be ‘selected from’. Rather, trajectories define a scaffolding that outlines where the neural state is likely to live and how it is likely to be changing over time. When we introduce the idea of library trajectories, we are now careful to stress that they don’t function as a set from which one trajectory is ‘declared’ to be the right one:</p><p>“We thus designed MINT to approximate that manifold using the trajectories themselves, rather than their covariance matrix or corresponding subspace. Unlike a covariance matrix, neural trajectories indicate not only which states are likely, but also which state-derivatives are likely. If a neural state is near previously observed states, it should be moving in a similar direction. MINT leverages this directionality.</p><p>Training-set trajectories can take various forms, depending on what is convenient to collect. Most simply, training data might include one trajectory per condition, with each condition corresponding to a discrete movement. Alternatively, one might instead employ one long trajectory spanning many movements. Another option is to employ many sub-trajectories, each briefer than a whole movement. The goal is simply for training-set trajectories to act as a scaffolding, outlining the manifold that might be occupied during decoding and the directions in which decoded trajectories are likely to be traveling.”</p><p>Later in that same section we stress that decoded trajectories can move along the ‘mesh’ in nonstereotyped ways:</p><p>“Although the mesh is formed of stereotyped trajectories, decoded trajectories can move along the mesh in non-stereotyped ways as long as they generally obey the flow-field implied by the training data. This flexibility supports many types of generalization, including generalization that is compositional in nature. Other types of generalization – e.g. from the green trajectories to the orange trajectories in Figure 1b – are unavailable when using MINT and are expected to be challenging for any method (as will be documented in a later section).”</p><p>The section “Training and decoding using MINT” has been revised to clarify the ways in which interpolation is flexible, allowing decoded movements to be globally very different from any library trajectory.</p><p>“To decode stereotyped trajectories, one could simply obtain the maximum-likelihood neural state from the library, then render a behavioral decode based on the behavioral state with the same values of c and k. This would be appropriate for applications in which conditions are categorical, such as typing or handwriting. Yet in most cases we wish for the trajectory library to serve not as an exhaustive set of possible states, but as a scaffolding for the mesh of possible states. MINT’s operations are thus designed to estimate any neural trajectory – and any corresponding behavioral trajectory – that moves along the mesh in a manner generally consistent with the trajectories in Ω.”</p><p>“…interpolation allows considerable flexibility. Not only is one not ‘stuck’ on a trajectory from Φ, one is also not stuck on trajectories created by weighted averaging of trajectories in Φ. For example, if cycling speed increases, the decoded neural state could move steadily up a scaffolding like that illustrated in Figure 1b (green). In such cases, the decoded trajectory might be very different in duration from any of the library trajectories. Thus, one should not think of the library as a set of possible trajectories that are selected from, but rather as providing a mesh-like scaffolding that defines where future neural states are likely to live and the likely direction of their local motion. The decoded trajectory may differ considerably from any trajectory within Ω.”</p><p>This flexibility is indeed used during movement. One empirical example is described in detail:</p><p>“During movement… angular phase was decoded with effectively no net drift over time. This is noteworthy because angular velocity on test trials never perfectly matched any of the trajectories in Φ. Thus, if decoding were restricted to a library trajectory, one would expect growing phase discrepancies. Yet decoded trajectories only need to locally (and approximately) follow the flow-field defined by the library trajectories. Based on incoming spiking observations, decoded trajectories speed up or slow down (within limits).</p><p>This decoding flexibility presumably relates to the fact that the decoded neural state is allowed to differ from the nearest state in Ω. To explore… [the text goes on to describe the new analysis in Figure 4d, which shows that the decoded state is typically not on any trajectory, though it is typically close to a trajectory].”</p><p>Thus, MINT’s operations allow considerable flexibility, including generalization that is compositional in nature. Yet R3 is still correct that there are other forms of generalization that are unavailable to MINT. This is now stressed at multiple points in the revision. However, under the perspective in Figure 1b, these forms of generalization are unavailable to any current method. Hence we made a second major change in response to this concern… (ii) We explicitly illustrate how the structure of the data determines when generalization is or isn’t possible. The new Figure 1a,b introduces the two perspectives, and the new Figure 6a,b lays out their implications for generalization. Under the perspective in Figure 6a, the reviewer is quite right: other methods can generalize in ways that MINT cannot. Under the perspective in Figure 6b, expectations are very different. Those expectations make testable predictions. Hence the third major change… (iii) We have added an analysis of generalization, using a newly collected dataset. This dataset was collected using Neuropixels Probes during our Pac-Man force-tracking task. This dataset was chosen because it is unusually well-suited to distinguishing the predictions in Figure 6a versus Figure 6b. Finding a dataset that can do so is not simple. Consider R3’s point that training data should “explore the whole movement space and the associated neural space”. The physical simplicity of the Pac-Man task makes it unusually easy to confirm that the behavioral workspace has been fully explored. Importantly, under Figure 6b, this does not mean that the neural workspace has been fully explored, which is exactly what we wish to test when testing generalization. We do so, and compare MINT with a Wiener filter. A Wiener filter is an ideal comparison because it is simple, performs very well on this task, and should be able to generalize well under Figure 1a. Additionally, the Wiener filter (unlike the Kalman Filter) doesn’t leverage the assumption that neural activity reflects the derivative of force. This matters because we find that neural activity does not reflect dforce/dt in this task. The Wiener filter is thus the most natural choice of the interpretable methods whose assumptions match Figure 1a.</p><p>The new analysis is described in Figure 6c-g and accompanying text. Results are consistent with the predictions of Figure 6b. We are pleased to have been motivated to add this analysis for two reasons. First, it provides an additional way of evaluating the predictions of the two competing scientific perspectives that are at the heart of our study. Second, this analysis illustrates an underappreciated way in which generalization is likely to be challenging for any decode method. It can be tempting to think that the main challenge regarding generalization is to fully explore the relevant behavioral space. This makes sense if a behavioral space has “an associated neural space”. However, we are increasingly of the opinion that it doesn’t. Different tasks often involve different neural subspaces, even when behavioral subspaces overlap. We have even seen situations where motor output is identical but neural subspaces are quite different. These facts are relevant to any decoder, something highlighted in the revised Introduction:</p><p>“MINT’s performance confirms that there are gains to be made by building decoders whose assumptions match a different, possibly more accurate view of population activity. At the same time, our results suggest fundamental limits on decoder generalization. Under the assumptions in Figure 1b, it will sometimes be difficult or impossible for decoders to generalize to not-yet-seen tasks. We found that this was true regardless of whether one uses MINT or a more traditional method. This finding has implications regarding when and how generalization should be attempted.”</p><p>We have also added an analysis (Figure 6e) illustrating how MINT’s ability to compute likelihoods can be useful in detecting situations that may strain generalization (for any method). MINT is unusual in being able to compute and use likelihoods in this way.</p><p>Detailed responses to R3: we reproduce each of R3’s specific concerns below, but concentrate our responses on issues not already covered above.</p><disp-quote content-type="editor-comment"><p>Main comments:</p><p>Comment 1. MINT does not generalize to different tasks, which is a main limitation for BCI utility compared with prior BCI decoders that have shown this generalizability as I review below. Specifically, given that MINT tabulates task-specific trajectories, it will not generalize to tasks that are not seen in the training data even when these tasks cover the exact same space (e.g., the same 2D computer screen and associated neural space).</p></disp-quote><p>First, the authors provide a section on generalization, which is inaccurate because it mixes up two fundamentally different concepts: (1) collecting informative training data and (2) generalizing from task to task. The former is critical for any algorithm, but it does not imply the latter. For example, removing one direction of cycling from the training set as the authors do here is an example of generating poor training data because the two behavioral (and neural) directions are non-overlapping and/or orthogonal while being in the same space. As such, it is fully expected that all methods will fail. For proper training, the training data should explore the whole movement space and the associated neural space, but this does not mean all kinds of tasks performed in that space must be included in the training set (something MINT likely needs while modeling-based approaches do not). Many BCI studies have indeed shown this generalization ability using a model. For example, in Weiss et al. 2019, center-out reaching tasks are used for training and then the same trained decoder is used for typing on a keyboard or drawing on the 2D screen. In Gilja et al. 2012, training is on a center-out task but the same trained decoder generalizes to a completely different pinball task (hit four consecutive targets) and tasks requiring the avoidance of obstacles and curved movements. There are many more BCI studies, such as Jarosiewicz et al. 2015 that also show generalization to complex realworld tasks not included in the training set. Unlike MINT, these works can achieve generalization because they model the neural subspace and its association to movement. On the contrary, MINT models task-dependent neural trajectories, so the trained decoder is very task-dependent and cannot generalize to other tasks. So, unlike these prior BCIs methods, MINT will likely actually need to include every task in its library, which is not practical.</p><p>I suggest the authors remove claims of generalization and modify their arguments throughout the text and abstract. The generalization section needs to be substantially edited to clarify the above points. Please also provide the BCI citations and discuss the above limitation of MINT for BCIs.</p><p>As discussed above, R3’s concerns are accurate under the view in Figure 1a (and the corresponding Figure 6a). Under this view, a method such as that in Gilja et al. or Jarosiewicz et al. can find the correct subspace, model the correct neuron-behavior correlations, and generalize to any task that uses “the same 2D computer screen and associated neural space”, just as the reviewer argues. Under Figure 1b things are quite different.</p><p>This topic – and the changes we have made to address it – is covered at length above. Here we simply want to highlight an empirical finding: sometimes two tasks use the same neural subspace and sometimes they don’t. We have seen both in recent data, and it is can be very non-obvious which will occur based just on behavior. It does not simply relate to whether one is using the same physical workspace. We have even seen situations where the patterns of muscle activity in two tasks are nearly identical, but the neural subspaces are fairly different. When a new task uses a new subspace, neither of the methods noted above (Gilja nor Jarosiewicz) will generalize (nor will MINT). Generalizing to a new subspace is basically impossible without some yet-to-be-invented approach. On the other hand, there are many other pairs of tasks (center-out-reaching versus some other 2D cursor control) where subspaces are likely to be similar, especially if the frequency content of the behavior is similar (in our recent experience this is often critical). When subspaces are shared, most methods will generalize, and that is presumably why generalization worked well in the studies noted above.</p><p>Although MINT can also generalize in such circumstances, R3 is correct that, under the perspective in Figure 1a, MINT will be more limited than other methods. This is now carefully illustrated in Figure 6a. In this traditional perspective, MINT will fail to generalize in cases where new trajectories are near previously observed states, yet move in very different ways from library trajectories. The reason we don’t view this is a shortcoming is that we expect it to occur rarely (else tangling would be high). We thus anticipate the scenario in Figure 6b.</p><p>This is worth stressing because R3 states that our discussion of generalization “is inaccurate because it mixes up two fundamentally different concepts: (1) collecting informative training data and (2) generalizing from task to task.” We have heavily revised this section and improved it. However, it was never inaccurate. Under Figure 6b, these two concepts absolutely are mixed up. If different tasks use different neural subspaces, then this requires collecting different “informative training data” for each. One cannot simply count on having explored the physical workspace.</p><disp-quote content-type="editor-comment"><p>Comment 2. MINT is shown to achieve competitive/high performance in highly stereotyped datasets with structured trials, but worse performance on MC_RTT, which is not based on repeated trials and is less stereotyped. This shows that MINT is valuable for decoding in repetitive stereotyped use-cases. However, it also highlights a limitation of MINT for BCIs, which is that MINT may not work well for real-world and/or less-constrained setups such as typing, moving a robotic arm in 3D space, etc. This is again due to MINT being a lookup table with a library of stereotyped trajectories rather than a model. Indeed, the authors acknowledge that the lower performance on MC_RTT (Figure 4) may be caused by the lack of repeated trials of the same type. However, real-world BCI decoding scenarios will also not have such stereotyped trial structure and will be less/un-constrained, in which MINT underperforms. Thus, the claim in the abstract or lines 480-481 that MINT is an &quot;excellent&quot; candidate for clinical BCI applications is not accurate and needs to be qualified. The authors should revise their statements according and discuss this issue. They should also make the use-case of MINT on BCI decoding clearer and more convincing.</p></disp-quote><p>We discussed, above, multiple changes and additions to the revision that were made to address these concerns. Here we briefly expand on the comment that MINT achieves “worse performance on MC_RTT, which is not based on repeated trials and is less stereotyped”. All decoders performed poorly on this task. MINT still outperformed the two traditional methods, but this was the only dataset where MINT did not also perform better (overall) than the expressive GRU and feedforward network. There are probably multiple reasons why. We agree with R3 that one likely reason is that this dataset is straining generalization, and MINT may have felt this strain more than the two machine-learning-based methods. Another potential reason is the structure of the training data, which made it more challenging to obtain library trajectories in the first place. Importantly, these observations do not support the view in Figure 1a. MINT still outperformed the Kalman and Wiener filters (whose assumptions align with Fig. 1a). To make these points we have added the following:</p><p>“Decoding was acceptable, but noticeably worse, for the MC_RTT dataset… As will be discussed below, every decode method achieved its worst estimates of velocity for the MC_RTT dataset. In addition to the impact of slower reaches, MINT was likely impacted by training data that made it challenging to accurate estimate library trajectories. Due to the lack of repeated trials, MINT used AutoLFADS to estimate the neural state during training. In principle this should work well. In practice AutoLFADS may have been limited by having only 10 minutes of training data. Because the random-target task involved more variable reaches, it may also have stressed the ability of all methods to generalize, perhaps for the reasons illustrated in Figure 1b.</p><p>The only dataset where MINT did not perform the best overall was the MC_RTT dataset, where it was outperformed by the feedforward network and GRU. As noted above, this may relate to the need for MINT to learn neural trajectories from training data that lacked repeated trials of the same movement (a design choice one might wish to avoid). Alternatively, the less-structured MC_RTT dataset may strain the capacity to generalize; all methods experienced a drop in velocity-decoding R2 for this dataset compared to the others. MINT generalizes somewhat differently than other methods, and may have been at a modest disadvantage for this dataset. A strong version of this possibility is that perhaps the perspective in Figure 1a is correct, in which case MINT might struggle because it cannot use forms of generalization that are available to other methods (e.g. generalization based on neuron-velocity correlations). This strong version seems unlikely; MINT continued to significantly outperform the Wiener and Kalman filters, which make assumptions aligned with Figure 1a.”</p><disp-quote content-type="editor-comment"><p>Comment 3. Related to 2, it may also be that MINT achieves competitive performance in offline and trial-based stereotyped decoding by overfitting to the trial structure in a given task, and thus may not generalize well to online performance due to overfitting. For example, a recent work showed that offline decoding performance may be overfitted to the task structure and may not represent online performance (Deo et al. 2023). Please discuss.</p></disp-quote><p>We agree that a limitation of our study is that we do not test online performance. There are sensible reasons for this decision:</p><p>“By necessity and desire, all comparisons were made offline, enabling benchmarked performance across a variety of tasks and decoded variables, where each decoder had access to the exact same data and recording conditions.”</p><p>We recently reported excellent online performance in the cycling task with a different algorithm</p><p>(Schroeder et al. 2022). In the course of that study, we consistently found that improvements in our offline decoding translated to improvements in our online decoding. We thus believe that MINT (which improves on the offline performance of our older algorithm) is a good candidate to work very well online. Yet we agree this still remains to be seen. We have added the following to the Discussion:</p><p>“With that goal in mind, there exist three important practical considerations. First, some decode algorithms experience a performance drop when used online. One presumed reason is that, when decoding is imperfect, the participant alters their strategy which in turn alters the neural responses upon which decoding is based. Because MINT produces particularly accurate decoding, this effect may be minimized, but this cannot be known in advance. If a performance drop does indeed occur, one could adapt the known solution of retraining using data collected during online decoding [13]. Another presumed reason (for a gap between offline and online decoding) is that offline decoders can overfit the temporal structure in training data [107]. This concern is somewhat mitigated by MINT’s use of a short spike-count history, but MINT may nevertheless benefit from data augmentation strategies such as including timedilated versions of learned trajectories in the libraries”</p><disp-quote content-type="editor-comment"><p>Comment 4. Related to 2, since MINT requires firing rates to generate the library and simple averaging does not work for this purpose in the MC_RTT dataset (that does not have repeated trials), the authors needed to use AutoLFADS to infer the underlying firing rates. The fact that MINT requires the usage of another model to be constructed first and that this model can be computationally complex, will also be a limiting factor and should be clarified.</p></disp-quote><p>This concern relates to the computational complexity of computing firing-rate trajectories during training. Usually, rates are estimated via trial-averaging, which makes MINT very fast to train. This was quite noticeable during the Neural Latents Benchmark competition. As one example, for the “MC_Scaling 5 ms Phase”, MINT took 28 seconds to train while GPFA took 30 minutes, the transformer baseline (NDT) took 3.5 hours, and the switching nonlinear dynamical system took 4.5 hours.</p><p>However, the reviewer is quite correct that MINT’s efficiency depends on the method used to construct the library of trajectories. As we note, “MINT is a method for leveraging a trajectory library, not a method for constructing it”. One can use trial-averaging, which is very fast. One can also use fancier, slower methods to compute the trajectories. We don’t view this as a negative – it simply provides options. Usually one would choose trial-averaging, but one does not have to. In the case of MC_RTT, one has a choice between LFADS and grouping into pseudo-conditions and averaging (which is fast). LFADS produces higher performance at the cost of being slower. The operator can choose which they prefer. This is discussed in the following section:</p><p>“For MINT, ‘training’ simply means computation of standard quantities (e.g. firing rates) rather than parameter optimization. MINT is thus typically very fast to train (Table 1), on the order of seconds using generic hardware (no GPUs). This speed reflects the simple operations involved in constructing the library of neural-state trajectories: filtering of spikes and averaging across trials. At the same time we stress that MINT is a method for leveraging a trajectory library, not a method for constructing it. One may sometimes wish to use alternatives to trial-averaging, either of necessity or because they improve trajectory estimates. For example, for the MC_RTT task we used AutoLFADS to infer the library. Training was consequently much slower (hours rather than seconds) because of the time taken to estimate rates. Training time could be reduced back to seconds using a different approach – grouping into pseudo-conditions and averaging – but performance was reduced. Thus, training will typically be very fast, but one may choose time-consuming methods when appropriate.”</p><disp-quote content-type="editor-comment"><p>Comment 5. I also find the statement in the abstract and paper that &quot;computations are simple, scalable&quot; to be inaccurate. The authors state that MINT's computational cost is O(NC) only, but it seems this is achieved at a high memory cost as well as computational cost in training. The process is described in section &quot;Lookup table of log-likelihoods&quot; on line [978-990]. The idea is to precompute the log-likelihoods for any combination of all neurons with discretization x all delay/history segments x all conditions and to build a large lookup table for decoding. Basically, the computational cost of precomputing this table is O(V^{Nτ} x TC) and the table requires a memory of O(V^{Nτ}), where V is the number of discretization points for the neural firing rates, N is the number of neurons, τ is the history length, T is the trial length, and C is the number of conditions. This is a very large burden, especially the V^{Nτ} term. This cost is currently not mentioned in the manuscript and should be clarified in the main text. Accordingly, computation claims should be modified including in the abstract.</p></disp-quote><p>As discussed above, the manuscript has been revised to clarify that our statement was accurate.</p><disp-quote content-type="editor-comment"><p>Comment 6. In addition to the above technical concerns, I also believe the authors should clarify the logic behind developing MINT better. From a scientific standpoint, we seek to gain insights into neural computations by making various assumptions and building models that parsimoniously describe the vast amount of neural data rather than simply tabulating the data. For instance, low-dimensional assumptions have led to the development of numerous dimensionality reduction algorithms and these models have led to important interpretations about the underlying dynamics (e.g., fixed points/limit cycles). While it is of course valid and even insightful to propose different assumptions from existing models as the authors do here, they do not actually translate these assumptions into a new model. Without a model and by just tabulating the data, I don't believe we can provide interpretation or advance the understanding of the fundamentals behind neural computations. As such, I am not clear as to how this library building approach can advance neuroscience or how these assumptions are useful. I think the authors should clarify and discuss this point.</p></disp-quote><p>As requested, a major goal of the revision has been to clarify the scientific motivations underlying MINT’s design. In addition to many textual changes, we have added figures (Figures 1a,b and 6a,b) to outline the two competing scientific perspectives that presently exist. This topic is also addressed by extensions of existing analyses and by new analyses (e.g. Figure 6c-g).</p><p>In our view these additions have dramatically improved the manuscript. This is especially true because we think R3’s concerns, expressed above, are reasonable. If the perspective in Figure 1a is correct, then R3 is right and MINT is essentially a hack that fails to model the data. MINT would still be effective in many circumstances (as we show), but it would be unprincipled. This would create limitations, just as the reviewer argues. On the other hand, if the perspective in Figure 1b is correct, then MINT is quite principled relative to traditional approaches. Traditional approaches make assumptions (a fixed subspace, consistent neuron-kinematic correlations) that are not correct under Figure 1b.</p><p>We don’t expect R3 to agree with our scientific perspective at this time (though we hope to eventually convince them). To us, the key is that we agree with R3 that the manuscript needs to lay out the different perspectives and their implications, so that readers have a good sense of the possibilities they should be considering. The revised manuscript is greatly improved in this regard.</p><disp-quote content-type="editor-comment"><p>Comment 7. Related to 6, there seems to be a logical inconsistency between the operations of MINT and one of its three assumptions, namely, sparsity. The authors state that neural states are sparsely distributed in some neural dimensions (Figure 1a, bottom). If this is the case, then why does MINT extend its decoding scope by interpolating known neural states (and behavior) in the training library? This interpolation suggests that the neural states are dense on the manifold rather than sparse, thus being contradictory to the assumption made. If interpolation-based dense meshes/manifolds underlie the data, then why not model the neural states through the subspace or manifold representations? I think the authors should address this logical inconsistency in MINT, especially since this sparsity assumption also questions the low-dimensional subspace/manifold assumption that is commonly made.</p></disp-quote><p>We agree this is an important issue, and have added an analysis on this topic (Figure 4d). The key question is simple and empirical: during decoding, does interpolation cause MINT to violate the assumption of sparsity? R3 is quite right that in principle it could. If spiking observations argue for it, MINT’s interpolation could create a dense manifold during decoding rather than a sparse one. The short answer is that empirically this does not happen, in agreement with expectations under Figure 1b. Rather than interpolating between distant states and filling in large ‘voids’, interpolation is consistently local. This is a feature of the data, not of the decoder (MINT doesn’t insist upon sparsity, even though it is designed to work best in situations where the manifold is sparse).</p><p>In addition to adding Figure 4d, we added the following (in an earlier section):</p><p>“The term mesh is apt because, if MINT’s assumptions are correct, interpolation will almost always be local. If so, the set of decodable states will resemble a mesh, created by line segments connecting nearby training-set trajectories. However, this mesh-like structure is not enforced by MINT’s operations. Interpolation could, in principle, create state-distributions that depart from the assumption of a sparse manifold. For example, interpolation could fill in the center of the green tube in Figure 1b, resulting in a solid manifold rather than a mesh around its outer surface. However, this would occur only if spiking observations argued for it. As will be documented below, we find that essentially all interpolation is local.”</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors</bold>:</p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>I appreciate the detailed methods section, however, more specifics should be integrated into the main text. For example on Line 238, it should additionally be stated how many minutes were used for training and metrics like the MAE which is used later should be reported here.</p></disp-quote><p>Thank you for this suggestion. We now report the duration of training data in the main text:</p><p>“Decoding R^2 was .968 over ~7.1 minutes of test trials based on ~4.4 minutes of training data.”</p><p>We have also added similar specifics throughout the manuscript, e.g. in the Fig. 5 legend:</p><p>“Results are based on the following numbers of training / test trials: MC\_Cycle (174 train, 99 test), MC\_Maze (1721 train, 574 test), Area2\_Bump (272 train, 92 test), MC\_RTT (810 train, 268 test).”</p><p>Similar additions were made to the legends for Fig. 6 and 8. Regarding the request to add MAE for the multitask network, we did not do so for the simple reason that the decoded variable (muscle activity) has arbitrary units. The raw MAE is thus not meaningful. We could of course have normalized, but at this point the MAE is largely redundant with the correlation. In contrast, the MAE is useful when comparing across the MC_Maze, Area2_Bump, and MC_RTT datasets, because they all involve the same scale (cm/s).</p><disp-quote content-type="editor-comment"><p>Regarding the MC_RTT task, AutoLFADS was used to obtain robust spike rates, as reported in the methods. However, the rationale for splitting the neural trajectories after AutoLFADS is unclear. If the trajectories were split based on random recording gaps, this might lead to suboptimal performance? It might be advantageous to split them based on a common behavioural state?</p></disp-quote><p>When learning neural trajectories via AutoLFADS, spiking data is broken into short (but overlapping) segments, rates are estimated for each segment via AutoLFADs, and these rates are then stitched together across segments into long neural trajectories. If there had been no recording gaps, these rates could have been stitched into a single neural trajectory for this dataset. However, the presence of recording gaps left us no choice but to stitch together these rates into more than one trajectory. Fortunately, recording gaps were rare: for the decoding analysis of MC_RTT there were only two recording gaps and therefore three neural trajectories, each ~2.7 minutes in duration.</p><p>We agree that in general it is desirable to learn neural trajectories that begin and end at behaviorallyrelevant moments (e.g. in between movements). However, having these trajectories potentially end midmovement is not an issue in and of itself. During decoding, MINT is never stuck on a trajectory. Thus, if MINT were decoding states near the end of a trajectory that was cut short due to a training gap, it would simply begin decoding states from other trajectories or elsewhere along the same trajectory in subsequent moments. We could have further trimmed the three neural trajectories to begin and end at behaviorallyrelevant moments, but chose not to as this would have only removed a handful of potentially useful states from the library.</p><p>We now describe this in the Methods:</p><p>“Although one might prefer trajectory boundaries to begin and end at behaviorally relevant moments (e.g. a stationary state), rather than at recording gaps, the exact boundary points are unlikely to be consequential for trajectories of this length that span multiple movements. If MINT estimates a state near the end of a long trajectory, its estimate will simply jump to another likely state on a different trajectory (or earlier along the same trajectory) in subsequent moments. Clipping the end of each trajectory to an earlier behaviorally-relevant moment would only remove potentially useful states from the libraries.”</p><disp-quote content-type="editor-comment"><p>Are the training and execution times in Table 1 based on pure Matlab functions or Mex files? If it's Mex files as suggested by the code, it would be good to mention this in the Table caption.</p></disp-quote><p>They are based on a combination of MATLAB and MEX files. This is now clarified in the table caption:</p><p>“Timing measurements taken on a Macbook Pro (on CPU) with 32GB RAM and a 2.3 GHz 8-Core Intel Core i9 processor. Training and execution code used for measurements was written in MATLAB (with the core recursion implemented as a MEX file).”</p><disp-quote content-type="editor-comment"><p>As the method most closely resembles a Bayesian decoder it would be good to compare performance against a Naive Bayes decoder.</p></disp-quote><p>We agree and have now done so. The following has been added to the text:</p><p>“A natural question is thus whether a simpler Bayesian decoder would have yielded similar results. We explored this possibility by testing a Naïve Bayes regression decoder [85] using the MC_Maze dataset. This decoder performed poorly, especially when decoding velocity (R2 = .688 and .093 for hand position and velocity, respectively), indicating that the specific modeling assumptions that differentiate MINT from a naive Bayesian decoder are important drivers of MINT’s performance.”</p><disp-quote content-type="editor-comment"><p>Line 199 Typo: The assumption of stereotypy trajectory also enables neural states (and decoded behaviors) to be updated in between time bins.</p></disp-quote><p>Fixed</p><disp-quote content-type="editor-comment"><p>Table 3: It's unclear why the Gaussian binning varies significantly across different datasets. Could the authors explain why this is the case and what its implications might be?</p></disp-quote><p>We have added the following description in the “Filtering, extracting, and warping data on each trial” subsection of the Methods to discuss how 𝜎 may vary due to the number of trials available for training and how noisy the neural data for those trials is:</p><p>“First, spiking activity for each neuron on each trial was temporally filtered with a Gaussian to yield single-trial rates. Table 3 reports the Gaussian standard deviations σ (in milliseconds) used for each dataset. Larger values of σ utilize broader windows of spiking activity when estimating rates and therefore reduce variability in those rate estimates. However, large σ values also yield neural trajectories with less fine-grained temporal structure. Thus, the optimal σ for a dataset depends on how variable the rate estimates otherwise are.”</p><disp-quote content-type="editor-comment"><p>An implementation of the method in an open-source programming language could further enhance the widespread use of the tool.</p></disp-quote><p>We agree this would be useful, but have yet not implemented the method in any other programming languages. Implementation in Python is still a future goal.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>- Figures 4 and 5 should show the error bars on the horizontal axis rather than portraying them vertically.</p></disp-quote><p>[Note that these are now Figures 5 and 6]</p><p>The figure legend of Figure 5 now clarifies that the vertical ticks are simply to aid visibility when symbols have very similar means and thus overlap visually. We don’t include error bars (for this analysis) because they are very small and would mostly be smaller than the symbol sizes. Instead, to indicate certainty regarding MINT’s performance measurements, the revised text now gives error ranges for the correlations and MAE values in the context of Figure 4c. These error ranges were computed as the standard deviation of the sampling distribution (computed via resampling of trials) and are thus equivalent to SEMs. The error ranges are all very small; e.g. for the MC_Maze dataset the MAE for x-velocity is 4.5 +/- 0.1 cm/s. (error bars on the correlations are smaller still).</p><p>Thus, for a given dataset, we can be quite certain of how well MINT performs (within ~2% in the above case). This is reassuring, but we also don’t want to overemphasize this accuracy. The main sources of variability one should be concerned about are: (1) different methods can perform differentially well for different brain areas and tasks, (2) methods can decode some behavioral variables better than others, and (3) performance depends on factors like neuron-count and the number of training trials, in ways that can differ across decode methods. For this reason, the study examines multiple datasets, across tasks and brain areas, and measures performance for a range of decoded variables. We also examine the impact of training-set-size (Figure 8a) and population size (solid traces in Fig. 8b, see R2’s next comment below).</p><p>There is one other source of variance one might be concerned about, but it is specific to the neuralnetwork approaches: different weight initializations might result in different performance. For this reason, each neural-network approach was trained ten times, with the average performance computed. The variability around this average was very small, and this is now stated in the Methods.</p><p>“For the neural networks, the training/testing procedure was repeated 10 times with different random seeds. For most behavioral variables, there was very little variability in performance across repetitions. However, there were a few outliers for which variability was larger. Reported performance for each behavioral group is the average performance across the 10 repetitions to ensure results were not sensitive to any specific random initialization of each network.”</p><disp-quote content-type="editor-comment"><p>- For Figure 6, it is unclear whether the neuron-dropping process was repeated multiple times. If not, it should be since the results will be sensitive to which particular subsets of neurons were &quot;dropped&quot;. In this case, the results presented in Figure 6 should include error bars to describe the variability in the model performance for each decoder considered.</p></disp-quote><p>A good point. The results in Figure 8 (previously Figure 6) were computed by averaging over the removal of different random subsets of neurons (50 subsets per neuron count), just as the reviewer requests. The figure has been modified to include the standard deviation of performance across these 50 subsets. The legend clarifies how this was done.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>Other comments:</p><p>(1) [Line 185-188] The authors argue that in a 100-dimensional space with 10 possible discretized values, 10^100 potential neural states need to be computed. But I am not clear on this. This argument seems to hold only in the absence of a model (as in MINT). For a model, e.g., Kalman filter or AutoLFADS, information is encoded in the latent state. For example, a simple Kalman filter for a linear model can be used for efficient inference. This 10^100 computation isn't a general problem but seems MINT-specific, please clarify.</p></disp-quote><p>We agree this section was potentially confusing. It has been rewritten. We were simply attempting to illustrate why maximum likelihood computations are challenging without constraints. MINT simplifies this problem by adding constraints, which is why it can readily provide data likelihoods (and can do so using a Poisson model). The rewritten section is below:</p><p>“Even with 1000 samples for each of the neural trajectories in Figure 3, there are only 4000 possible neural states for which log-likelihoods must be computed (in practice it is fewer still, see Methods). This is far fewer than if one were to naively consider all possible neural states in a typical rate- or factor-based subspace. It thus becomes tractable to compute log-likelihoods using a Poisson observation model. A Poisson observation model is usually considered desirable, yet can pose tractability challenges for methods that utilize a continuous model of neural states. For example, when using a Kalman filter, one is often restricted to assuming a Gaussian observation model to maintain computational tractability “</p><disp-quote content-type="editor-comment"><p>(2) [Figure 6b] Why do the authors set the dropped neurons to zero in the &quot;zeroed&quot; results of the robustness analysis? Why not disregard the dropped neurons during the decoding process?</p></disp-quote><p>We agree the terminology we had used in this section was confusing. We have altered the figure and rewritten the text. The following, now at the beginning of that section, addresses the reviewer’s query:</p><p>“It is desirable for a decoder to be robust to the unexpected loss of the ability to detect spikes from some neurons. Such loss might occur while decoding, without being immediately detected. Additionally, one desires robustness to a known loss of neurons / recording channels. For example, there may have been channels that were active one morning but are no longer active that afternoon. At least in principle, MINT makes it very easy to handle this second situation: there is no need to retrain the decoder, one simply ignores the lost neurons when computing likelihoods. This is in contrast to nearly all other methods, which require retraining because the loss of one neuron alters the optimal parameters associated with every other neuron.”</p><p>The figure has been relabeled accordingly; instead of the label ‘zeroed’, we use the label ‘undetected neuron loss’.</p><disp-quote content-type="editor-comment"><p>(3) Authors should provide statistical significance on their results, which they already did for Fig. S3a,b,c but missing on some other figures/places.</p></disp-quote><p>We have added error bars in some key places, including in the text when quantifying MINT’s performance in the context of Figure 4. Importantly, error bars are only as meaningful as the source of error they assess, and there are reasons to be careful given this. The standard method for putting error bars on performance is to resample trials, which is indeed what we now report. These error bars are very small. For example, when decoding horizontal velocity for the MC_Maze dataset, the correlation between MINT’s decode and the true velocity had a mean and SD of the sampling distribution of 0.963 +/- 0.001. This means that, for a given dataset and target variable, we have enough trials/data that we can be quite certain of how well MINT performs. However, we want to be careful not to overstate this certainty. What one really wants to know is how well MINT performs across a variety of datasets, brain areas, target variables, neuron counts, etc. It is for this reason that we make multiple such comparisons, which provides a more valuable view of performance variability.</p><p>For Figure 7, error bars are unavailable. Because this was a benchmark, there was exactly one test-set that was never seen before. This is thus not something that could be resampled many times (that would have revealed the test data and thus invalidated the benchmark, not to mention that some of these methods take days to train). We could, in principle, have added resampling to Figure 5. In our view it would not be helpful and could be misleading for the reasons noted above. If we computed standard errors using different train/test partitions, they would be very tight (mostly smaller than the symbol sizes), which would give the impression that one can be quite certain of a given R^2 value. Yet variability in the train/test partition is not the variability one is concerned about in practice. In practice, one is concerned about whether one would get a similar R^2 for a different dataset, or brain area, or task, or choice of decoded variable. Our analysis thus concentrated on showing results across a broad range of situations. In our view this is a far more relevant way of illustrating the degree of meaningful variability (which is quite large) than resampling, which produces reassuringly small but (mostly) irrelevant standard errors.</p><p>Error bars are supplied in Figure 8b. These error bars give a sense of variability across re-samplings of the neural population. While this is not typically the source of variability one is most concerned about, for this analysis it becomes appropriate to show resampling-based standard errors because a natural concern is that results may depend on which neurons were dropped. So here it is both straightforward, and desirable, to compute standard errors. (The fact that MINT and the Wiener filter can be retrained many times swiftly was also key – this isn’t true of the more expressive methods). Figure S1 also uses resampling-based confidence intervals for similar reasons.</p><disp-quote content-type="editor-comment"><p>(4) [Line 431-437] Authors state that MINT outperforms other methods with the PSTH R^2 metric (trial-averaged smoothed spikes for each condition). However, I think this measure may not provide a fair comparison and is confounded because MINT's library is built using PSTH (i.e., averaged firing rate) but other methods do not use the PSTH. The author should clarify this.</p></disp-quote><p>The PSTH R^2 metric was not created by us; it was part of the Neural Latents Benchmark. They chose it because it ensures that a method cannot ‘cheat’ (on the Bits/Spike measure) by reproducing fine features of spiking while estimating rates badly. We agree with the reviewer’s point: MINT’s design does give it a potential advantage in this particular performance metric. This isn’t a confound though, just a feature. Importantly, MINT will score well on this metric only if MINT’s neural state estimate is accurate (including accuracy in time). Without accurate estimation of the neural state at each time, it wouldn’t matter that the library trajectory is based on PSTHs. This is now explicitly stated:</p><p>“This is in some ways unsurprising: MINT estimates neural states that tend to resemble (at least locally) trajectories ‘built’ from training-set-derived rates, which presumably resemble test-set rates. Yet strong performance is not a trivial consequence of MINT’s design. MINT does not ‘select’ whole library trajectories; PSTH R2 will be high only if condition (c), index (k), and the interpolation parameter (α) are accurately estimated for most moments.”</p></body></sub-article></article>