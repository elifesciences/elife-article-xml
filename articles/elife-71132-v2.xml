<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">71132</article-id><article-id pub-id-type="doi">10.7554/eLife.71132</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>An image reconstruction framework for characterizing initial visual encoding</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-241711"><name><surname>Zhang</surname><given-names>Ling-Qi</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8468-7927</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-244154"><name><surname>Cottaris</surname><given-names>Nicolas P</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf3"/></contrib><contrib contrib-type="author" id="author-115968"><name><surname>Brainard</surname><given-names>David</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9827-543X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf4"/></contrib><aff id="aff1"><institution content-type="dept">Department of Psychology</institution>, <institution>University of Pennsylvania</institution>, <addr-line><named-content content-type="city">Philadelphia</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-3549"><name><surname>Meister</surname><given-names>Markus</given-names></name><role>Reviewing editor</role><aff><institution>California Institute of Technology</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>lingqiz@sas.upenn.edu</email> (LZ);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>17</day><month>01</month><year>2022</year></pub-date><volume>11</volume><elocation-id>e71132</elocation-id><history><date date-type="received"><day>09</day><month>06</month><year>2021</year></date><date date-type="accepted"><day>14</day><month>01</month><year>2022</year></date></history><permissions><copyright-statement>Â© 2022, Zhang et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Zhang et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-71132-v2.pdf"/><abstract><p>We developed an image-computable observer model of the initial visual encoding that operates on natural image input, based on the framework of Bayesian image reconstruction from the excitations of the retinal cone mosaic. Our model extends previous work on ideal observer analysis and evaluation of performance beyond psychophysical discrimination, takes into account the statistical regularities of the visual environment, and provides a unifying framework for answering a wide range of questions regarding the visual front end. Using the error in the reconstructions as a metric, we analyzed variations of the number of different photoreceptor types on human retina as an optimal design problem. In addition, the reconstructions allow both visualization and quantification of information loss due to physiological optics and cone mosaic sampling, and how these vary with eccentricity. Furthermore, in simulations of color deficiencies and interferometric experiments, we found that the reconstructed images provide a reasonable proxy for modeling subjects' percepts. Lastly, we used the reconstruction-based observer for the analysis of psychophysical threshold, and found notable interactions between spatial frequency and chromatic direction in the resulting spatial contrast sensitivity function. Our method is widely applicable to experiments and applications in which the initial visual encoding plays an important role.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Facebook Reality Labs</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Zhang</surname><given-names>Ling-Qi</given-names></name><name><surname>Cottaris</surname><given-names>Nicolas P</given-names></name><name><surname>Brainard</surname><given-names>David</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf2"><p>Ling-Qi Zhang, Funding provided by Facebook Reality Labs.</p></fn><fn fn-type="conflict" id="conf3"><p>Nicolas P Cottaris, Funding provided by Facebook Reality Labs.</p></fn><fn fn-type="conflict" id="conf4"><p>David Brainard, Funding provided by Facebook Reality Labs.</p></fn><fn fn-type="conflict" id="conf1"><p>The other authors declare that no competing interests exist.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>The MATLAB code used for this paper is available at: https://github.com/isetbio/ISETImagePipelineIn addition, the curated RGB and hyperspectral image datasets, parameters used in the simulation including display and cone mosaic setup, as well as the intermediate results such as the learned sparse priors, likelihood functions (i.e., render matrices), are available through: https://tinyurl.com/26r92c8y</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Russakovsky O</collab><collab>Deng J</collab><collab>Su H</collab><collab>Krause J</collab><collab>Satheesh S</collab><collab>Ma S</collab><collab>Huang Z</collab><collab>Karpathy A</collab><collab>Khosla A</collab><collab>Bernstein M</collab><collab>Berg AC</collab></person-group><year iso-8601-date="2015">2015</year><source>Imagenet Large Scale Visual Recognition Challenge</source><ext-link ext-link-type="uri" xlink:href="https://image-net.org/challenges/LSVRC/">https://image-net.org/challenges/LSVRC/</ext-link><comment>N/A</comment></element-citation><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Nascimento SM</collab><collab>Ferreira FP</collab><collab>Foster DH</collab></person-group><year iso-8601-date="2002">2002</year><source>Hyperspectral Images of Natural Scenes 2002</source><ext-link ext-link-type="uri" xlink:href="https://personalpages.manchester.ac.uk/staff/d.h.foster/Hyperspectral_images_of_natural_scenes_02.html">https://personalpages.manchester.ac.uk/staff/d.h.foster/Hyperspectral_images_of_natural_scenes_02.html</ext-link><comment>N/A</comment></element-citation><element-citation id="dataset3" publication-type="data" specific-use="references"><person-group person-group-type="author"><collab>Chakrabarti A</collab><collab>Zickler T</collab></person-group><year iso-8601-date="2011">2011</year><source>Real-World Hyperspectral Images Database</source><ext-link ext-link-type="uri" xlink:href="http://vision.seas.harvard.edu/hyperspec/download.html">http://vision.seas.harvard.edu/hyperspec/download.html</ext-link><comment>N/A</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-71132-supp-v2.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>