<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">93171</article-id><article-id pub-id-type="doi">10.7554/eLife.93171</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.93171.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Mesoscale functional organization and connectivity of color, disparity, and naturalistic texture in human second visual area</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ai</surname><given-names>Hailin</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Lin</surname><given-names>Weiru</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Chengwen</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Chen</surname><given-names>Nihong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0890-3875</contrib-id><email>rainbowcnh@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhang</surname><given-names>Peng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9603-8454</contrib-id><email>zhangpeng@ibp.ac.cn</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03cve4549</institution-id><institution>Department of Psychological and Cognitive Sciences, Tsinghua University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>State Key Laboratory of Brain and Cognitive Science, Institute of Biophysics, Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/053w1zy07</institution-id><institution>University of Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Changsha</named-content></addr-line><country>China</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/034t30j35</institution-id><institution>Department of Psychology and Cognition and Human Behavior Key Laboratory of Hunan Province, Hunan Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Hunan</named-content></addr-line><country>China</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/053w1zy07</institution-id><institution>Center for Mind &amp; Brain Sciences, Hunan Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Changsh</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03cve4549</institution-id><institution>THU-IDG/McGovern Institute for Brain Research, Tsinghua University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff7"><label>7</label><institution>Institute of Artificial Intelligence, Hefei Comprehensive National Science Center</institution><addr-line><named-content content-type="city">Hefei</named-content></addr-line><country>China</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ggpsq73</institution-id><institution>Otto-von-Guericke University Magdeburg</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Stanford University, Howard Hughes Medical Institute</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>20</day><month>03</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP93171</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-10-26"><day>26</day><month>10</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-10-30"><day>30</day><month>10</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.10.26.564178"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-02-02"><day>02</day><month>02</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93171.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-01-28"><day>28</day><month>01</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.93171.2"/></event></pub-history><permissions><copyright-statement>© 2024, Ai, Lin et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Ai, Lin et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-93171-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-93171-figures-v1.pdf"/><abstract><p>Although parallel processing has been extensively studied in the low-level geniculostriate pathway and the high-level dorsal and ventral visual streams, less is known at the intermediate-level visual areas. In this study, we employed high-resolution fMRI at 7T to investigate the columnar and laminar organizations for color, disparity, and naturalistic texture in the human secondary visual cortex (V2), and their informational connectivity with lower- and higher-order visual areas. Although fMRI activations in V2 showed reproducible interdigitated color-selective thin and disparity-selective thick ‘stripe’ columns, we found no clear evidence of columnar organization for naturalistic textures. Cortical depth-dependent analyses revealed the strongest color-selectivity in the superficial layers of V2, along with both feedforward and feedback informational connectivity with V1 and V4. Disparity selectivity was similar across different cortical depths of V2, which showed significant feedforward and feedback connectivity with V1 and V3ab. Interestingly, the selectivity for naturalistic texture was strongest in the deep layers of V2, with significant feedback connectivity from V4. Thus, while local circuitry within cortical columns is crucial for processing color and disparity information, feedback signals from V4 are involved in generating the selectivity for naturalistic textures in area V2.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>V2</kwd><kwd>7T fMRI</kwd><kwd>columns</kwd><kwd> layers</kwd><kwd>color</kwd><kwd>disparity</kwd><kwd>textures</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>National Science and Technology Innovation STI2030-Major Projects</institution></institution-wrap></funding-source><award-id>2022ZD0211900</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Peng</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>National Science and Technology Innovation STI2030-Major Projects</institution></institution-wrap></funding-source><award-id>2021ZD0203600</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Nihong</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution>National Science and Technology Innovation STI2030-Major Projects</institution></institution-wrap></funding-source><award-id>2021ZD0204200</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Peng</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31971031</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Nihong</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31871107</award-id><principal-award-recipient><name><surname>Zhang</surname><given-names>Peng</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31930053</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Nihong</given-names></name><name><surname>Zhang</surname><given-names>Peng</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>Hunan Provincial Natural Science Foundation</institution></institution-wrap></funding-source><award-id>2024JJ6313</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Chengwen</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>V2 exhibits interdigitated columnar organizations for color and disparity that involved feedforward and feedback processing, while the texture selectivity is primarily driven by feedback modulations from V4.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A single glance at the world captures a rich amount of visual information. The initial signals hit on the retina are transformed along the visual hierarchy in a way that different aspects of information are processed in parallel streams (<xref ref-type="bibr" rid="bib61">Nassi and Callaway, 2009</xref>). The retino-geniculate-striate pathway of the primate visual system is primarily segregated into magnocellular (M) and parvocellular (P) streams (<xref ref-type="bibr" rid="bib36">Kaplan et al., 1990</xref>; <xref ref-type="bibr" rid="bib45">Lee, 1996</xref>; <xref ref-type="bibr" rid="bib56">Merigan and Maunsell, 1993</xref>), selectively processing different spatiotemporal frequencies of achromatic and chromatic information. In the primary visual cortex (V1), M and P information from the geniculate are transformed into higher-level visual representations, such as motion, disparity, color, and orientation (<xref ref-type="bibr" rid="bib87">Tootell and Nasr, 2017</xref>). Early studies in nonhuman primates established the functional specializations of area V2 (<xref ref-type="bibr" rid="bib11">Burkhalter and Van Essen, 1986</xref>; <xref ref-type="bibr" rid="bib68">Peterhans and von der Heydt, 1989</xref>; <xref ref-type="bibr" rid="bib69">Poggio and Fischer, 1977</xref>; <xref ref-type="bibr" rid="bib98">Zeki, 1973</xref>). The processing of motion/disparity, color, and orientation information has been found to be organized into ‘stripe’-shaped interdigitated columns (<xref ref-type="bibr" rid="bib16">DeYoe and Van Essen, 1985</xref>; <xref ref-type="bibr" rid="bib30">Hubel and Livingstone, 1985</xref>; <xref ref-type="bibr" rid="bib31">Hubel and Livingstone, 1987</xref>; <xref ref-type="bibr" rid="bib76">Roe and Ts’o, 1995</xref>; <xref ref-type="bibr" rid="bib89">Ts’o et al., 2001</xref>; <xref ref-type="bibr" rid="bib95">Xiao et al., 2003</xref>), corresponding to the ‘thick’, ‘thin’, and ‘pale’ stripes in cytochrome oxidase (CO) staining studies (<xref ref-type="bibr" rid="bib49">Livingstone and Hubel, 1982</xref>; <xref ref-type="bibr" rid="bib86">Tootell et al., 1983</xref>; referred to as CO-stripes in the present study). The interdigitated columnar organizations for color and motion/disparity processing have also been found in human V2 by high-resolution fMRI at 7T (<xref ref-type="bibr" rid="bib39">Kennedy et al., 2023</xref>; <xref ref-type="bibr" rid="bib60">Nasr et al., 2016</xref>).</p><p>More recently, primate V2 was also found to be sensitive to high-order statistical dependencies embedded in naturalistic textures (<xref ref-type="bibr" rid="bib22">Freeman et al., 2013</xref>), a unique type of information critical for surface and material perception. Computationally, these high-order statistical dependencies were composed of correlations across different orientations, spatial scales, and local positions, which can be calculated from the output of orientation filters in V1 (<xref ref-type="bibr" rid="bib22">Freeman et al., 2013</xref>; <xref ref-type="bibr" rid="bib71">Portilla and Simoncelli, 2000</xref>). Although weakly represented in V1 (<xref ref-type="bibr" rid="bib22">Freeman et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Okazawa et al., 2015</xref>; <xref ref-type="bibr" rid="bib99">Ziemba et al., 2016</xref>), the neural selectivity to naturalistic statistics was found to be increasingly stronger in the downstream areas V3 (<xref ref-type="bibr" rid="bib43">Kohler et al., 2016</xref>) and V4 (<xref ref-type="bibr" rid="bib3">Arcizet et al., 2008</xref>; <xref ref-type="bibr" rid="bib63">Okazawa et al., 2015</xref>; <xref ref-type="bibr" rid="bib64">Okazawa et al., 2017</xref>). Electrophysiological recordings have also identified later and much weaker sensitivity to naturalistic texture in the superficial and deep layers of V1 compared to V2 (<xref ref-type="bibr" rid="bib100">Ziemba et al., 2019</xref>), consistent with an effect of corticocortical feedback modulation. However, it remains unclear whether the neural representations of naturalistic textures arise from local processing within V2 or feedback modulation from higher-order visual areas, such as V4.</p><p>While interdigitated stripe-shaped columnar organizations for color and motion/disparity processing have been found in primate V2, it remains unknown whether a columnar organization also exists for texture processing. If local circuits in area V2 are essential for processing high-order naturalistic statistics, specialized cortical columns might develop to enhance computational efficiency (<xref ref-type="bibr" rid="bib57">Mountcastle, 1997</xref>; <xref ref-type="bibr" rid="bib78">Schulte To Brinke et al., 2022</xref>; <xref ref-type="bibr" rid="bib83">Stoop et al., 2013</xref>). A likely candidate for such computational units is the pale stripes, known for their preferential responses to orientation information (<xref ref-type="bibr" rid="bib31">Hubel and Livingstone, 1987</xref>; <xref ref-type="bibr" rid="bib51">Livingstone and Hubel, 1988</xref>; <xref ref-type="bibr" rid="bib53">Lu and Roe, 2008</xref>; <xref ref-type="bibr" rid="bib54">Malach et al., 1994</xref>). Alternatively, if feedback modulations from higher-order visual areas play a prominent role in generating the selectivity in area V2 to high-order statistical dependencies embedded in naturalist textures, a specialized cortical column may not be necessary for such computations. Furthermore, the integration of local elements across various locations, orientations, and spatial scales, which is necessary for processing high-order statistics, might pose a challenge for early visual areas like V2 to develop a specialized computational module.</p><p>To better understand the mesoscale functional organizations and neural circuits of information processing in area V2, the present study investigated laminar (or cortical depth-dependent) and columnar response profiles for color, disparity, and naturalistic texture in human V2 using 7T fMRI at 1 mm isotropic resolution. Cortical depth-dependent fMRI (also called laminar fMRI) allows us to noninvasively measure feedforward, local, and feedback activity in human cerebral cortex (<xref ref-type="bibr" rid="bib32">Huber et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Liu et al., 2021</xref>; <xref ref-type="bibr" rid="bib62">Norris and Polimeni, 2019</xref>; <xref ref-type="bibr" rid="bib65">Olman et al., 2012</xref>; <xref ref-type="bibr" rid="bib70">Polimeni and Uludağ, 2018</xref>). By combining laminar fMRI with informational connectivity methods (<xref ref-type="bibr" rid="bib35">Jia et al., 2020</xref>), we further investigated the feedforward and feedback connectivity between V2 and the lower- and higher-order visual areas. Our results revealed interdigitated stripe-shaped columnar organizations in V2 for color and disparity processing, which involved both feedforward and feedback connectivity with other visual areas in the hierarchy. In contrast, feedback modulations from V4 played a prominent role in processing naturalistic statistics in area V2, which showed no clear evidence of modular organization in CO-stripes.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p><xref ref-type="fig" rid="fig1">Figure 1A</xref> shows the stimuli for the color, disparity, and texture experiments. Color-selective activation was defined as the contrast of fMRI responses between chromatic (Chr) and achromatic (Ach) gratings (first column). Disparity-selective activation was the response difference between disparity-defined sinusoidal gratings (3D) by random dot stereograms (RDSs) and their zero-disparity (2D) counterparts (second column). Texture-selective activation was the response difference between naturalistic textures (T) and spectrally matched noise (N) (third column).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Visual stimuli and model of layer-specific neural circuitry.</title><p>(<bold>A</bold>) Visual stimuli for the fMRI experiments. Left: chromatic and achromatic gratings for the color experiment; Middle: disparity-defined grating and zero-disparity disc from random dots for the disparity experiment; Right: naturalistic texture and spectrally matched noise for the texture experiment. (<bold>B</bold>) Parallel information processing pathways in the early visual areas. (<bold>C</bold>) Layer-specific neural circuitry of feedforward, feedback, and horizontal connections in the early visual areas. S: superficial layers; M: middle layers. D: deep layers.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Results of isoluminance adjustment.</title><p>RGB indices of gray (<bold>A</bold>) and red (<bold>B</bold>) that match the luminance of maximum blue values across three eccentricities. Error bars indicate 1 SEM across ten subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig1-figsupp1-v1.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig1">Figure 1B</xref> illustrates a simple model for the building blocks of parallel processing streams in area V2 and their connections with lower (V1) and higher-order visual areas (V3ab and V4). Previous studies in anesthetized macaques found neural selectivity to binocular disparity in the layer 4B of V1, V2 thick stripes, and V3ab in the dorsal stream (<xref ref-type="bibr" rid="bib31">Hubel and Livingstone, 1987</xref>; <xref ref-type="bibr" rid="bib50">Livingstone and Hubel, 1987</xref>; <xref ref-type="bibr" rid="bib86">Tootell et al., 1983</xref>; <xref ref-type="bibr" rid="bib89">Ts’o et al., 2001</xref>; <xref ref-type="bibr" rid="bib88">Tsao et al., 2003</xref>), color selectivity in the color blobs in the V1 superficial layers, V2 thin stripes, and V4 in the ventral stream (<xref ref-type="bibr" rid="bib31">Hubel and Livingstone, 1987</xref>; <xref ref-type="bibr" rid="bib50">Livingstone and Hubel, 1987</xref>; <xref ref-type="bibr" rid="bib51">Livingstone and Hubel, 1988</xref>; <xref ref-type="bibr" rid="bib53">Lu and Roe, 2008</xref>; <xref ref-type="bibr" rid="bib98">Zeki, 1973</xref>), and strong orientation selectivity in layer 2/3 of V1, V2 pale stripes, and V4 (<xref ref-type="bibr" rid="bib31">Hubel and Livingstone, 1987</xref>; <xref ref-type="bibr" rid="bib76">Roe and Ts’o, 1995</xref>; <xref ref-type="bibr" rid="bib84">Tanigawa et al., 2010</xref>; <xref ref-type="bibr" rid="bib89">Ts’o et al., 2001</xref>). Only major connections are shown here. There are also other connections, such as V1 interblobs projecting to thick stripes (<xref ref-type="bibr" rid="bib17">Federer et al., 2021</xref>; <xref ref-type="bibr" rid="bib29">Hu and Roe, 2022</xref>; <xref ref-type="bibr" rid="bib82">Sincich and Horton, 2005</xref>). Given a strong dependency on the output of orientation filters (<xref ref-type="bibr" rid="bib71">Portilla and Simoncelli, 2000</xref>; <xref ref-type="bibr" rid="bib81">Simoncelli and Olshausen, 2001</xref>), naturalistic textures might be selectively processed by orientation-selective neurons in the pale stripes of V2.</p><p><xref ref-type="fig" rid="fig1">Figure 1C</xref> illustrates a simplified model for the layer-specific neural circuitry in the early visual cortex (<xref ref-type="bibr" rid="bib18">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="bib61">Nassi and Callaway, 2009</xref>). In addition to feedforward and local horizontal connections, feedback modulations may also play important roles in processing visual information, especially in conscious visual perception (<xref ref-type="bibr" rid="bib25">Ge et al., 2020</xref>). Here, we focused on major connections in the early visual areas. There are also other connections such as feedforward connections from V1 to V4 (<xref ref-type="bibr" rid="bib91">Ungerleider et al., 2008</xref>), and feedback connections from the superficial layers to lower-order visual areas (<xref ref-type="bibr" rid="bib10">Briggs, 2020</xref>; <xref ref-type="bibr" rid="bib75">Rockland and Pandya, 1979</xref>). Using cortical depth-dependent fMRI, we aim to investigate the feedforward, feedback, and local processing of color, disparity, and texture information in the human visual system.</p><sec id="s2-1"><title>Functional organizations on the cortical surface of V2</title><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Response selectivity of color, disparity, and naturalistic texture in V2.</title><p>(<bold>A</bold>) Activation maps in a representative subject (S09). The scale bar denotes percent signal change of BOLD response. From left to right: Chr – Ach (color), 3D – 2D (disparity), color – disparity, T – N (texture). The bottom panels show enlarged activations in the black square. The highlighted region in the bottom panels represents area V2. Color-selective and disparity-selective stripe-shaped activations arranged perpendicular to the V1-V2 border. Red arrowheads denote the location of color-selective (thin) stripes and blue arrowheads denote the location of disparity-selective (thick) stripes. Black arrowheads in the fourth column highlight the texture-selective activations in anterior V2 (corresponding to peripheral visual field). The regions of interest (ROIs) for pale stripes were defined as vertices in-between adjacent thin and thick stripes (see ‘Materials and methods’ for details). (<bold>B</bold>) Inter-session correlations for the color- and disparity-selective functional maps in S09. Each blue dot represents one vertex on V2 surface. (<bold>C</bold>) Selectivity indices for color, disparity and naturalistic texture in different types of columns. Error bar indicates 1 SEM across subjects. **p&lt;0.01, ***p&lt;0.001. n.s.: none significance. Circles represent data from individual participants. (<bold>D</bold>) Texture selectivity at different eccentricities. Error bars represent 1 SEM across ten participants. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>The functional maps in V2 for all 10 subjects.</title><p>The scale bar denotes percent signal change of BOLD response. Color-selective thin and disparity-selective thick stripes were denoted by red and blue arrows, respectively. The number of stripes in each category is summarized in <xref ref-type="supplementary-material" rid="fig2s1sdata1">Figure 2—figure supplement 1—source data 1</xref>. LH: left hemisphere; RH: right hemisphere.</p><p><supplementary-material id="fig2s1sdata1"><label>Figure 2—figure supplement 1—source data 1.</label><caption><title>The number of stripes in manually defined ROIs.</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-93171-fig2-figsupp1-data1-v1.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Test-retest activation patterns for color and disparity experiments in a subject (S09).</title><p>Color-selective thin and disparity-selective thick stripes are denoted by red and blue arrows, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Inter-session correlations for the color- and disparity-selective activation maps in four subjects who scanned both color and disparity experiments in 2 days.</title><p>The results for S09 are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>The manually defined ROIs for disparity-selective thick, color-selective thin stripes, and the pale stripes in-between in a representative subject (S09).</title><p>The scale bar denotes percent signal change of BOLD response. The stripes are framed with dashed lines. Black: thin and thick stripes (<xref ref-type="fig" rid="fig2">Figure 2A</xref>); purple: pale stripes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-figsupp4-v1.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>The bootstrapped distributions of stimulus-selectivity indices in different types of column ROIs.</title><p>Dashed lines indicate zero selectivity. Color- and disparity-selective indices both show significant differences across three stripes. Texture-selective index shows non-significant difference across different stripes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-figsupp5-v1.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>Feature selectivity in CO-stripes under different ROI-definition thresholds.</title><p>From left to right: 10%, 20%, and 30% of extreme voxels. We defined the stripe ROIs using different thresholds to investigate whether the results were independent of the threshold for ROI definition. Specifically, we ranked the voxels in manually defined stripe ROIs by the color-disparity response. We then defined the lowest 10% as the thick stripe voxels, the highest 10% as thin stripe voxels, and the middle 10% as pale stripe voxels. Additionally, we adjusted the thresholds to select 20% or 30% of extreme voxels to define the three stripes (with 30% being the least strict threshold). Notably, in all threshold conditions, there was no significant difference in texture selectivity across different stripes. Error bars indicate 1 SEM across ten subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-figsupp6-v1.tif"/></fig><fig id="fig2s7" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 7.</label><caption><title>The response difference between texture and noise in the 3T fMRI experiment.</title><p>The six naturalistic textures were used as stimuli in both 3T and 7T fMRI experiments. MRI data were collected using a 3T Siemens scanner (Prisma) with a 32-channel phased-array coil. Stimuli were back- projected via an MRI-safe projector (spatial resolution: 1024 × 768, refresh rate: 60 Hz). Anatomical volumes were acquired with a T1-weighted MPRAGE sequence at 1 mm isotropic resolution. Functional data were collected with a gradient echo planar imaging sequence (TE = 34 ms; TR = 2 s; FOV = 200 × 200 mm<sup>2</sup>; matrix = 100 × 100; flip angle = 90°; slice thickness = 2 mm; 72 axial slices). Each run consisted of thirty 24 s stimulus blocks, starting and ending with 16 s fixation periods. Blocks of naturalistic texture and spectrally matched noise stimuli were presented alternatively. In each block, a random sequence of images from one texture family (naturalistic texture or spectrally matched noise) were presented at 5 Hz. There were 15 pairs of naturalistic texture and spectrally matched noise in one run. Eight runs were collected for two subjects while four runs for the other three subjects. MRI data analyses were similar as those in the 7T experiment. Error bars indicate 1 SEM across ten subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-figsupp7-v1.tif"/></fig><fig id="fig2s8" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 8.</label><caption><title>Maps showing selectivity for color, disparity and naturalistic texture in V4 and V3ab in a subject (S01).</title><p>Selectivity maps for three experiments were initially thresholded at p&lt;0.05 and then normalized by linearly scaling the values from 0 to 1. A vertex was classified as selective for a particular feature (i.e., either color, disparity or texture) if its normalized value was times greater than the value for any other feature. Inter-session correlations were also calculated for the activation maps showing selectivity for color-selective, disparity-selective, and texture-selective in V4 (left panel) and V3ab (right panel).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-figsupp8-v1.tif"/></fig><fig id="fig2s9" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 9.</label><caption><title>Null distributions of pattern correlation coefficients from Monte Carlo simulation.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig2-figsupp9-v1.tif"/></fig></fig-group><p>As shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref> in a representative subject, color-selective (Chr – Ach, first column, red arrows) and disparity-selective activations (3D – 2D, second column, blue arrows) show stripe-shaped organizations in area V2. These interdigitated stripes can be more clearly seen on the differential map between color and disparity activations [(Chr – Ach) – (3D – 2D), third column]. However, the texture-selective activation map does not exhibit a clear columnar organization (T – N, fourth column). Stronger texture-selective activations can be found from the more anterior part of V2, corresponding to the peripheral visual field. Similar functional organizations can be found from other subjects (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>In 5 out of 10 subjects, both color and disparity experiments were conducted in two sessions. To evaluate the test–retest reliability of the interdigitated columnar organizations, we calculated the inter-session pattern correlations for the color- and disparity-selective functional maps. For the representative subject (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), the correlation coefficients for both color- (<italic>r</italic> = 0.66) and disparity-selective activation maps (<italic>r</italic> = 0.53) were highly significant (both p&lt;0.001, family-wise-error [FWE] corrected; activation patterns from the two sessions were shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). The functional maps from the other four subjects also demonstrate highly significant pattern correlations between sessions (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). These findings demonstrate that the interdigitated columnar organizations for color and disparity processing are highly reproducible.</p><p>To further demonstrate whether there is a difference in texture selectivity within different functional modules of the parallel processing streams in area V2, we performed an ROI analysis with the thick, thin, and pale stripes. The ROIs for disparity-selective thick and color-selective thin stripes were defined by the differential map between color- and disparity-selective activations (color-disparity, the third column in <xref ref-type="fig" rid="fig2">Figure 2A</xref>), while the ROIs for the pale stripes were defined as vertices in-between adjacent thin- and thick-stripe ROIs (see ‘Materials and methods’ for details about ROI definition and <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> for the ROIs in a representative subject). From the columnar response profile (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), thin- and thick-stripe ROIs show the strongest selectivity to color and disparity information, respectively. This is as expected and validated our ROI definition approach. We further conducted repeated-measures ANOVA and Bayesian ANOVA to examine whether there is difference in texture-selectivity index across three different stripes. The results were statistically nonsignificant (F(2,9) = 1.88, p=0.18, BF<sub>10</sub> = 0.65). A nonparametric bootstrap method also revealed no significant difference between the responses to naturalistic texture and spectrally matched noise (see bootstrapped distributions in <xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>). Further analysis shows that the results were independent of the threshold used to define the stripe ROIs (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>).</p><p>Although none of the CO-stripes demonstrated a preference for naturalistic textures, the anterior part of V2 showed some texture-selective activations (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). To further demonstrate this observation, we divided V2 into three parts based on eccentricity: 0–3° (central), 3–6° (parafoveal), and 6–18° (peripheral). Texture selectivity increased significantly with eccentricity (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, F(2,9) = 17.74, p&lt;0.001, BF<sub>10</sub> = 345.83; peripheral vs. parafoveal: <italic>t</italic>(9) = 4.12, p&lt;0.01; parafoveal vs. central: <italic>t</italic>(9) = 3.20, p&lt;0.05). These findings suggest that texture selectivity in V2 is stronger in more peripheral visual fields. However, it is possible that the central visual field might prefer texture patterns with a higher spatial frequency than our stimuli.</p><p>Texture-selective activations in our results are weaker compared to those in the previous study (<xref ref-type="bibr" rid="bib22">Freeman et al., 2013</xref>). There also appears to be residual texture patterns in some of the noise stimuli (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Thus, to demonstrate the efficacy and specificity of our stimuli, we performed a 3T fMRI experiment using the same stimuli as in Freeman et al.’s study. Different texture families and noise stimuli were presented in separate blocks. <xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7</xref> shows the 3T results for stimuli that were also used in the 7T experiment. All texture families showed significantly stronger activation in V2 compared to the corresponding noise patterns, even for those that ‘appeared’ to have residual texture information (e.g., the third texture family). These results demonstrate that our stimuli are effective in activating texture-selective neural populations in area V2. The 3T data also showed a notable increase in texture-selective activations compared to the 7T experiment, likely due to the increased stimulus presentation speed.</p></sec><sec id="s2-2"><title>Cortical depth-dependent response selectivity</title><p>A response selectivity index was calculated for each stimulus contrast (see ‘Materials and methods’ for details). The original BOLD responses are also provided (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Within each ROI, repeated-measures ANOVA was conducted on each type of selectivity index with cortical depth (deep/middle/superficial) as the within-subject factor, followed by post hoc <italic>t</italic>-tests between different depths. Color selectivity was significantly stronger in the superficial cortical depth compared to the middle and deep cortical depths in both V1 (F(2,9) = 15.08, p&lt;0.001, BF<sub>10</sub> = 133.72; S vs. M: <italic>t</italic>(9) = 4.70, p&lt;0.01; S vs. D: <italic>t</italic>(9) = 4.12, p&lt;0.01) and V2 (F(2,9) = 12.93, p&lt;0.001, BF<sub>10</sub> = 64.83; S vs. M: <italic>t</italic>(9) = 3.77, p&lt;0.01; S vs. D: <italic>t</italic>(9) = 3.85, p&lt;0.01). No significant difference was observed across different depths in other visual areas (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). According to the hierarchical model, the strongest color selectivity in the superficial cortical depth is consistent with the fact that color blobs locate in the superficial layers of V1 (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="bibr" rid="bib18">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="bib31">Hubel and Livingstone, 1987</xref>; <xref ref-type="bibr" rid="bib61">Nassi and Callaway, 2009</xref>). The strongest color selectivity in superficial V2 suggests that both local and feedforward connections are involved in processing color information (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Layer-specific response selectivity for color (<bold>A</bold>), disparity (<bold>B</bold>), and naturalistic texture (<bold>C</bold>).</title><p>Error bars indicate 1 SEM across ten subjects. *p&lt;0.05, **p&lt;0.01. See <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for the original BOLD response across cortical depth.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Raw BOLD signal changes for calculating layer-specific selectivity indices in color (<bold>A</bold>), disparity (<bold>B</bold>), and texture experiment (<bold>C</bold>).</title><p>Error bars indicate 1 SEM across ten subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Illustrations of depth map (<bold>A</bold>) and pial vein removal (<bold>B</bold>).</title><p>Red pixels in (B) represent vertices with extremely large signal changes (top 5%) that were excluded.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig3-figsupp2-v1.tif"/></fig></fig-group><p>Disparity selectivity was significantly higher in the superficial cortical depth compared to the middle and deep cortical depths in V3ab (<xref ref-type="fig" rid="fig3">Figure 3B</xref>) (F(2,9) = 9.06, p&lt;0.01, BF<sub>10</sub> = 16.81; S vs. M: <italic>t</italic>(9) = 3.11, p&lt;0.05; S vs. D: <italic>t</italic>(9) = 3.75, p&lt;0.01). No significant difference was found across cortical depths in other ROIs (all F(2,9) &lt; 2.85, p&gt;0.08). The absence of laminar difference in disparity selectivity may suggest that both feedforward, feedback, and local mechanisms are involved in processing disparity information in area V2.</p><p>Response selectivity to naturalistic texture was strongest in the deep cortical depth in both V1 (F(2,9) = 12.91, p&lt;0.001, BF<sub>10</sub> = 63.57; D vs. M: <italic>t</italic>(9) = 2.28, p&lt;0.05; D vs. S: <italic>t</italic>(9) = 4.76, p&lt;0.01) and V2 (F(2,9) = 8.6, BF<sub>10</sub> = 14.08, p&lt;0.01; D vs. M: <italic>t</italic>(9) = 2.49, p&lt;0.05; D vs. S: <italic>t</italic>(9) = 4.29, p&lt;0.01). Texture selectivity showed no significant difference across cortical depths in the higher-level visual areas V4 (F(2,9) = 1.64, p=0.22) or V3ab (F(2,9) = 3.53, p=0.051), which were significantly larger than those in V1 and V2 (all paired comparisons between ROIs, p&lt;0.001, BF<sub>10</sub> &gt; 1106.79). Thus, the depth-dependent effect on feature selectivity does not seem to depend on original BOLD response difference. V1 responses to naturalistic textures were also significantly weaker compared to spectrally matched noise, in line with the top-down feedback hypothesis of predictive coding (<xref ref-type="bibr" rid="bib23">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib59">Murray et al., 2002</xref>; <xref ref-type="bibr" rid="bib74">Rao and Ballard, 1999</xref>). The strongest selectivity in the deep layers of V2 suggests that feedback modulations from higher-level visual areas play a crucial role in processing naturalistic statistical information in this region.</p></sec><sec id="s2-3"><title>Cortical depth-dependent informational connectivity</title><p>To further investigate the information flow in the visual hierarchy, we conducted layer-specific informational connectivity analysis among V1, V2, V3ab, and V4 (<xref ref-type="bibr" rid="bib1">Aly and Turk-Browne, 2016</xref>; <xref ref-type="bibr" rid="bib14">Coutanche and Thompson-Schill, 2014</xref>; <xref ref-type="bibr" rid="bib27">Haxby et al., 2001</xref>; <xref ref-type="bibr" rid="bib34">Huffman and Stark, 2017</xref>; <xref ref-type="bibr" rid="bib35">Jia et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Koster et al., 2018</xref>). For each pair of stimuli, an support vector machine (SVM) classifier was trained to decode the stimulus type (e.g., chromatic or achromatic gratings). Block-by-block multivariate distances to the decision boundary were used to calculate the co-variability of stimulus representations between two brain regions. Feedforward connectivity was defined as the connection between the superficial layer of a lower-level area and the middle layer of a higher-level area, whereas feedback connectivity was defined as the connection between the deep layers of two brain regions.</p><p>As shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, for color-selective processing, significant feedforward (<italic>t</italic>(9) = 5.64, p&lt;0.001) and feedback connections (<italic>t</italic>(9) = 10.39, p&lt;0.001) were found between V1 and V2, and between V2 and V4 (both <italic>t</italic>(9) &gt; 4.96, p&lt;0.01). For disparity-selective processing, significant feedforward (<italic>t</italic>(9) = 3.77, p&lt;0.05) and feedback connections (<italic>t</italic>(9) = 3.06, p&lt;0.05) were found between V1 and V2, and also between V2 and V3ab (both <italic>t</italic>(9) &gt; 2.90, p&lt;0.05). In contrast, for naturalistic texture-selective processing, a significant feedback connection was found from V4 to V2 (<italic>t</italic>(9) = 4.28, p&lt;0.05). No significant correlation was found for other connections (all <italic>t</italic>(9) &lt; 1.7, p&gt;0.25).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Layer-specific feedforward and feedback informational connectivity of color, disparity, and naturalistic texture.</title><p>Numbers denote the mean values of connection (Pearson’s <italic>r</italic>) across all subjects. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001 after false discovery rate correction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-fig4-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Utilizing 7T BOLD fMRI at 1 mm isotropic resolution, we investigated laminar and columnar response profiles for color, disparity, and naturalistic textures in human V2 by presenting three stimulus contrasts. Color- and disparity-selective activations revealed clear stripe-shaped columnar organizations in area V2, oriented perpendicular to the V1-V2 border (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). These columnar patterns were reproducible between different scanning sessions (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>) and are consistent with previous findings from intrinsic optical imaging studies in monkeys (<xref ref-type="bibr" rid="bib53">Lu and Roe, 2008</xref>; <xref ref-type="bibr" rid="bib89">Ts’o et al., 2001</xref>) and 7T fMRI studies in humans (<xref ref-type="bibr" rid="bib39">Kennedy et al., 2023</xref>; <xref ref-type="bibr" rid="bib60">Nasr et al., 2016</xref>). However, we found no clear evidence for modular organization for naturalistic textures. CO-stripes in V2 exhibit similar texture selectivity. Cortical depth-dependent analysis revealed that compared to color and disparity information, the processing of naturalistic statistics in V2 is more dependent on feedback modulation from V4.</p><p>The laminar profiles of response selectivity revealed both inter- and intra-areal hierarchical processing of visual information. Color selectivity was strongest in the superficial depth of V1 and V2. This result is consistent with the findings that color blobs are primarily located in the superficial layers of primate V1 (<xref ref-type="bibr" rid="bib49">Livingstone and Hubel, 1982</xref>), and that the local processing of color information is most prominent in the superficial layers of the early visual cortex (<xref ref-type="bibr" rid="bib31">Hubel and Livingstone, 1987</xref>; <xref ref-type="bibr" rid="bib53">Lu and Roe, 2008</xref>). For disparity processing, no significant difference was identified across cortical depths in the early visual cortex. This result suggests that feedforward, feedback, and local mechanisms all contribute to generating disparity-defined 3D perception as the middle layer is recognized as the primary termination of feedforward inputs, and the superficial and deep layers are considered as the output layers to higher-order areas and the primary recipients of feedback projections, respectively (<xref ref-type="bibr" rid="bib12">Callaway, 2004</xref>; <xref ref-type="bibr" rid="bib18">Felleman and Van Essen, 1991</xref>). Consistent with the laminar response profiles, cortical depth-dependent informational connectivity analyses showed that both feedforward and feedback signals play important roles in color and disparity processing in the ventral (i.e., V2-V4) and dorsal (i.e., V2-V3ab) visual streams, respectively.</p><p>A previous electrophysiological study investigated laminar neural activity in V1 and V2 to naturalistic textures in anesthetized macaques (<xref ref-type="bibr" rid="bib100">Ziemba et al., 2019</xref>). Their findings suggest that the superficial and deep layers of V1 are subject to top-down modulation from higher-order visual areas during processing of naturalistic textures. Consistent with this study, we found the strongest selectivity to naturalistic textures in the deep V1, suggesting feedback modulation from higher-order visual areas. V1 responses to naturalistic textures were also significantly weaker compared to spectrally matched noise, consistent with the framework of predictive coding: top-down hypotheses from higher-level area ‘explain away’ or reduce the prediction error signals in the lower visual area (<xref ref-type="bibr" rid="bib23">Friston, 2005</xref>; <xref ref-type="bibr" rid="bib59">Murray et al., 2002</xref>; <xref ref-type="bibr" rid="bib74">Rao and Ballard, 1999</xref>). This could also explain the strongest signal reduction in the superficial cortical depth in our results since the error signals should be mainly represented in the output layers according to the model of canonical microcircuits for predictive coding (<xref ref-type="bibr" rid="bib5">Bastos et al., 2012</xref>).</p><p>In V2, <xref ref-type="bibr" rid="bib100">Ziemba et al., 2019</xref> found stronger texture selectivity in the superficial and middle layers, which potentially emerged from local processing within this area. In contrast, our data revealed the highest selectivity to naturalistic textures in the deep V2, along with a significant feedback modulation from V4. The amount of texture selectivity in V4 was also stronger compared to V2. This is consistent with previous macaque studies showing stronger texture selectivity in V4 than in V2 (<xref ref-type="bibr" rid="bib64">Okazawa et al., 2017</xref>) and local clustered neurons in V4 that shared preferred image statistics (<xref ref-type="bibr" rid="bib26">Hatanaka et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">Kim et al., 2022</xref>). In addition, our pilot analyses also suggest modular organizations for naturalistic texture, in addition to those for color and disparity in V4 and V3ab (<xref ref-type="fig" rid="fig2s8">Figure 2—figure supplement 8</xref>). Altogether, these findings suggest an important role of feedback modulation from V4 in generating selectivity to naturalistic textures in V2. In the monkey study, it is possible that general anesthesia substantially reduced feedback modulation from high-order brain areas. There is evidence showing that V4 activity is more closely related to conscious visual perception than the early visual areas (<xref ref-type="bibr" rid="bib55">Mehta et al., 2000</xref>; <xref ref-type="bibr" rid="bib85">Tong, 2003</xref>).</p><p>Our fMRI data demonstrate a significant contribution of feedback signals in generating the selectivity for naturalistic textures in area V2. Nonetheless, this does not preclude the possibility that such selectivity might arise from local processing within this area and become progressively stronger along the feedforward pathway. In accordance with the predictive coding hypothesis discussed above, top-down feedback might reduce the neural activity representing prediction errors in the superficial layers of lower-order areas, counteracting the effect of neural response evoked by local processing. Furthermore, recent macaque studies have shown that the cortical processing of naturalistic textures depends on the type of statistical regularities (<xref ref-type="bibr" rid="bib41">Kim et al., 2022</xref>; <xref ref-type="bibr" rid="bib63">Okazawa et al., 2015</xref>). Compared to previous studies (<xref ref-type="bibr" rid="bib22">Freeman et al., 2013</xref>; <xref ref-type="bibr" rid="bib63">Okazawa et al., 2015</xref>; <xref ref-type="bibr" rid="bib99">Ziemba et al., 2016</xref>), our V2 results showed much weaker selectivity to naturalistic textures. This could be due to different textures used in the current study. As suggested by the previous findings, texture selectivity across neurons in V2 and V4 can be highly diversified (Figure 2e in <xref ref-type="bibr" rid="bib22">Freeman et al., 2013</xref>; <xref ref-type="bibr" rid="bib41">Kim et al., 2022</xref>): some texture families are much more or less effective in driving neural activity than others, with distinct temporal dynamics.</p><p>Our data did not show clear evidence of a ‘stripe’-shaped columnar organization for naturalistic textures within area V2. A cortical column is formed by many mini-columns bound together by short-range horizontal connections (<xref ref-type="bibr" rid="bib57">Mountcastle, 1997</xref>), supporting efficient information processing via local circuitry. Thus, the absence of a columnar organization in area V2 is consistent with a dominant role of feedback modulation, rather than local or feedforward processing in generating texture selectivity within this area. Considering the complex computations required for processing naturalistic information, it is likely that V4 neurons are more suitable for this task than those in V2. Higher-order statistics in naturalistic textures are computed via integrating local elements across different locations, orientations, and spatial scales (<xref ref-type="bibr" rid="bib71">Portilla and Simoncelli, 2000</xref>), presenting a challenge for an early visual area such as V2 to develop a specialized computational module. In line with this idea, the neural tunings in V4 are distributed in a way suitable for categorizing textures and predicting texture discrimination abilities (<xref ref-type="bibr" rid="bib26">Hatanaka et al., 2022</xref>; <xref ref-type="bibr" rid="bib41">Kim et al., 2022</xref>; <xref ref-type="bibr" rid="bib63">Okazawa et al., 2015</xref>). Consistently, our results also revealed modular organizations for textures in V4 and V3ab (<xref ref-type="fig" rid="fig2s8">Figure 2—figure supplement 8</xref>). These texture-selective organizations may be related to surface representations in these higher-order visual areas (<xref ref-type="bibr" rid="bib94">Wang et al., 2024</xref>). Although cortical columns in V2 are much larger than those in V1 (<xref ref-type="bibr" rid="bib52">Lu and Roe, 2007</xref>), our results do not rule out the possibility that smaller modules of texture processing might exist beyond our fMRI resolution at 1 mm isotropic voxels, especially at farther eccentricities (<xref ref-type="fig" rid="fig2">Figure 2A and D</xref>).</p><p>Finally, the critical period for the formation of cortical columns in lower-level visual areas might close at an earlier stage during development (<xref ref-type="bibr" rid="bib42">Kiorpes, 2015</xref>; <xref ref-type="bibr" rid="bib46">Levi, 2005</xref>). It is possible that the emergence of selectivity to naturalistic textures requires extensive visual experience with the ability to actively explore the natural environment. After the closure of the critical period in V2 for forming color- and disparity-selective columns, V4 may still be in its critical period with high neural plasticity, allowing it to develop neuronal clusters with strong preference for naturalistic textures (<xref ref-type="bibr" rid="bib26">Hatanaka et al., 2022</xref>). Subsequently, feedback modulations from V4 may further increase the selectivity for naturalistic textures in V2.</p><p>In summary, the present study demonstrated parallel pathways for color, disparity, and texture processing in the human visual cortex. Unlike color and disparity, no clear evidence of columnar organization or response preference across thick, pale, and thin stripes was found for naturalistic texture in area V2. Consistent with this finding, our results further suggest that feedback processing from V4 plays a dominant role in generating texture selectivity within V2. These results underscore the critical involvement of higher-order visual areas in texture processing. Given the diversity of naturalistic textures, different cortical mechanisms may be involved at various processing stages (<xref ref-type="bibr" rid="bib41">Kim et al., 2022</xref>; <xref ref-type="bibr" rid="bib63">Okazawa et al., 2015</xref>). In future studies, it would be important to characterize columnar and laminar fMRI responses using different texture types to obtain a comprehensive picture of naturalistic texture processing along the visual hierarchy.</p><p>Due to the limitations of the T2*w GE-BOLD signal in its sensitivity to large draining veins (<xref ref-type="bibr" rid="bib21">Fracasso et al., 2021</xref>; <xref ref-type="bibr" rid="bib66">Parkes et al., 2005</xref>; <xref ref-type="bibr" rid="bib90">Uludag and Havlicek, 2021</xref>), the original BOLD responses were strongly biased toward the superficial depth in our data (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). Compared to GE-BOLD, VASO-CBV and SE-BOLD fMRI techniques have higher spatial specificity but much lower sensitivity (<xref ref-type="bibr" rid="bib33">Huber et al., 2019</xref>). As suggested by previous studies (<xref ref-type="bibr" rid="bib40">Kim and Fukuda, 2008</xref>; <xref ref-type="bibr" rid="bib73">Qian et al., 2024</xref>; <xref ref-type="bibr" rid="bib97">Yacoub et al., 2008</xref>), differential BOLD signals in a continuous stimulus design exhibit stronger microvascular contribution, which might help improve the laminar specificity of feature selectivity measures in our results (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Compared to the submillimeter voxels, as used in most laminar fMRI studies, our fMRI resolution at 1 mm isotropic voxel may have a stronger partial volume effect in the cortical depth-dependent analysis. However, consistent with our results, previous studies have also shown that 7T fMRI at 1 mm isotropic resolution can resolve cortical depth-dependent signals in human visual cortex (<xref ref-type="bibr" rid="bib77">Roefs et al., 2024</xref>; <xref ref-type="bibr" rid="bib80">Shao et al., 2021</xref>). Finally, the interpretations of cortical depth-dependent results are based on the canonical models of cortical microcircuitry. Alternative connections exist in addition to these major pathways. Therefore, our findings on the cortical microcircuitry in humans would require further support from invasive neuroscience methods, such as laminar electrophysiological recordings in non-human primates.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">FreeSurfer (version 6.0)</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu">https://surfer.nmr.mgh.harvard.edu</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_001847">SCR_001847</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">AFNI</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://afni.nimh.nih.gov/afni/">http://afni.nimh.nih.gov/afni/</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_005927">SCR_005927</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">ANTs</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib4">Avants et al., 2011</xref></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_004757">SCR_004757</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">mripy</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/mripy/">https://pypi.org/project/mripy/</ext-link>; <xref ref-type="bibr" rid="bib28">herrlich10, 2025</xref></td><td align="left" valign="bottom"/><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">MATLAB</td><td align="left" valign="bottom">MathWorks</td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_001622">SCR_001622</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Psychophysics toolbox</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/">http://psychtoolbox.org/</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_002881">SCR_002881</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">JASP</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://jasp-stats.org/">https://jasp-stats.org/</ext-link></td><td align="left" valign="bottom">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_015823">SCR_015823</ext-link></td><td align="left" valign="bottom"/></tr><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Portilla-Simoncelli model</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib71">Portilla and Simoncelli, 2000</xref></td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">Used for synthesizing naturalistic texture</td></tr><tr><td align="left" valign="bottom">Other</td><td align="left" valign="bottom">7T MAGNETOM MRI scanner</td><td align="left" valign="bottom">Siemens Healthineers</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">MRI data collection</td></tr><tr><td align="left" valign="bottom">Other</td><td align="left" valign="bottom">32-channel receive, 4-channel transmit open-face surface coil</td><td align="left" valign="bottom"><xref ref-type="bibr" rid="bib79">Sengupta et al., 2016</xref></td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">Custom-built open-face visual coil</td></tr><tr><td align="left" valign="bottom">Other</td><td align="left" valign="bottom">Custom anaglyph spectacles (red and cyan)</td><td align="left" valign="bottom">This paper</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">Used for disparity experiment</td></tr></tbody></table></table-wrap><sec id="s4-1"><title>Participants</title><p>Ten participants (four females; age range 21–40 years) were recruited for this study. All participants had normal or corrected-to-normal vision and reported no history of neuropsychological or visual disorders. A sample size of 10 participants is relatively large according to the literature on columnar and laminar fMRI studies with long scanning durations (<xref ref-type="bibr" rid="bib60">Nasr et al., 2016</xref>; <xref ref-type="bibr" rid="bib87">Tootell and Nasr, 2017</xref>). The experimental procedures were approved by the ethical review board of Institute of Biophysics, Chinese Academy of Sciences (no. 2012-IRB-011). Written informed consent was obtained from all participants prior to their participation in the study.</p></sec><sec id="s4-2"><title>General procedures</title><p>Each participant underwent three fMRI experiments in the 7T scanner. Six subjects participated in color, disparity, and texture experiments in three daily sessions (five subjects scanned both color and disparity experiments in two sessions, and the texture experiment in a single session; one subject scanned one experiment in each session). For each experiment, 10 runs of fMRI data were collected. The remaining four participants completed all three experiments in a single session, consisting of 12 runs in total (four runs for each experiment). The order of stimulus presentation was counterbalanced both within and across fMRI runs for each participant. Visual stimuli subtended 46.7° × 35.9° in visual angle, with a fixation point (0.3° in diameter) in the center. During fMRI scans, each run started and ended with 16 s fixation periods. In the remaining periods, visual stimuli were presented in 24 s stimulus blocks. Participants were required to maintain fixation and to detect sparsely and randomly presented color changes of the fixation point.</p></sec><sec id="s4-3"><title>Stimuli and apparatus</title><p>Visual stimuli were presented through an MRI-safe projector (1024 × 768 pixel resolution, 60 Hz refresh rate) onto a rear-projection screen. The experiment was conducted using MATLAB 2016a (MathWorks) based on Psychophysics toolbox extensions version 3.0 (<xref ref-type="bibr" rid="bib9">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib67">Pelli, 1997</xref>). Participants viewed the screen via a mirror mounted inside the head coil.</p><sec id="s4-3-1"><title>Color experiment</title><p>The MRI-safe projector was calibrated using a PR-655 photometer to have a linear luminance output. To account for changes in isoluminance at different eccentricities (<xref ref-type="bibr" rid="bib60">Nasr et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Bilodeau and Faubert, 1997</xref>; <xref ref-type="bibr" rid="bib50">Livingstone and Hubel, 1987</xref>; <xref ref-type="bibr" rid="bib58">Mullen, 1985</xref>), we measured blue-red and blue-gray isoluminance for each participant at three eccentricity ranges (0°–3°, 3°–8°, and 8°–16°). Blue was set as the reference color because the project has lower light intensity for blue compared with red and gray. A minimal motion procedure was used to match the perceived luminance between blue and red/gray (<xref ref-type="bibr" rid="bib2">Anstis and Cavanagh, 1983</xref>). During isoluminance adjustment, achromatic and chromatic gratings were presented in alternating frames, with pi/2 phase difference between adjacent frames. Blue luminance was fixed at the maximum level, participants adjusted the match-color luminance until no consistent apparent motion was seen (i.e., bi-stable motion directions with equal durations). For each eccentricity range, the isoluminance adjustment was repeated four times and the results were averaged. <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A and B</xref> illustrates the blue-matched luminance levels (in RGB index) of gray and red, respectively, at the three eccentricity ranges. Consistent with previous findings (<xref ref-type="bibr" rid="bib60">Nasr et al., 2016</xref>; <xref ref-type="bibr" rid="bib8">Bilodeau and Faubert, 1997</xref>; <xref ref-type="bibr" rid="bib50">Livingstone and Hubel, 1987</xref>; <xref ref-type="bibr" rid="bib58">Mullen, 1985</xref>), the isoluminance level varied significantly as eccentricity (blue-gray: F(2,9) = 87.9, p&lt;0.001, FWE corrected; blue-red: F(2,9) = 35.71, p&lt;0.001, FWE corrected). During fMRI scans, chromatic and achromatic gratings (0.2 cycles-per-degree concentric rings, 46.7° × 35.9° in size, <xref ref-type="fig" rid="fig1">Figure 1A</xref>, left panel) moved in either centrifugal or centripetal direction with a speed of 0.8 cycles/s, alternating in 24 s blocks (five blocks per stimulus condition). A 16 s fixation period with uniform gray background was presented at the beginning and the end of each run. Ten runs of fMRI data were collected for six subjects and four runs for the other four subjects.</p></sec><sec id="s4-3-2"><title>Disparity experiment</title><p>The binocular disparity stimulus (46.7° × 35.9°) was random red/green dot stereograms (RDSs) presented against a black background. Subjects viewed the stimulus through custom anaglyph spectacles (red and cyan). The RDSs generated a stereoscopic percept, with the depth of each dot sinusoidally modulating between –2.2°–2.2° in front of and behind the frontoparallel plane of fixation (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, middle panel). In the zero-disparity 2D control condition, randomly moving dots formed a frontoparallel plane intersecting the fixation point (i.e., zero depth at that point). Each run began and ended with a 16 s fixation period. The disparity-defined grating and zero-disparity disc stimuli were presented in alternation every 24 s. Ten runs of fMRI data were collected for six subjects and four runs for the other four subjects.</p></sec><sec id="s4-3-3"><title>Texture experiment</title><p>The naturalistic texture and spectrally matched noise (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, right panel) were synthesized using the Portilla–Simoncelli model (<xref ref-type="bibr" rid="bib71">Portilla and Simoncelli, 2000</xref>). Thirty image pairs were generated. The stimuli (46.7° × 35.9°) were presented in the middle of the screen, centered on the fixation point. Each fMRI run consisted of ten 24 s stimulus blocks, starting and ending with 16 s fixation periods. Each stimulus block consisted of 30 pictures in a random order, with a duration of 0.6 s for each picture. Ten runs of fMRI data were collected for six subjects and four runs for the other four subjects. Although the number of runs was not equal across participants, there were at least four runs (i.e., 20 blocks for each stimulus condition) of data in each experiment, which should be sufficient to investigate within-subject effects. In support of this, the split-half analysis revealed reproducible columnar organizations with five runs of data (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</p></sec></sec><sec id="s4-4"><title>MRI data acquisition</title><p>MRI data were acquired on a 7T MAGNETOM MRI scanner (Siemens Healthineers, Erlangen, Germany) with a custom-built 32-channel receive and 4-channel transmit open-face surface coil (<xref ref-type="bibr" rid="bib79">Sengupta et al., 2016</xref>), in the Beijing MRI center for Brain Research. Functional data were collected with a T2*-weighted 2D GE-EPI sequence (1.0 mm isotropic voxels, 39 slices, TR = 2400 ms, TE = 25 ms, image matrix = 128 × 128, FOV = 128 × 128 mm<sup>2</sup>, GRAPPA acceleration factor = 3, nominal flip angle = 80°, partial Fourier factor = 7/8, phase encoding direction from head to foot, receiver bandwidth = 1148 Hz/Pix). Slices were oriented perpendicular to the calcarine sulcus. After each fMRI run, five EPI images with reversed phase encoding direction (F to H) were also acquired for EPI distortion correction. High-resolution anatomical volumes were acquired with a T1-weighted MP2RAGE sequence at 0.7 mm isotropic resolution (256 sagittal slices, centric phase encoding, acquisition matrix = 320 × 320, FOV = 224 × 224 mm<sup>2</sup>, GRAPPA = 3, TR = 4000 ms, TE = 3.05 ms, TI1=750 ms, flip angle = 4°, TI2 = 2500 ms, flip angle = 5°). A bite-bar was settled for each subject to minimize head motion to ensure high data quality.</p></sec><sec id="s4-5"><title>MRI data analysis</title><sec id="s4-5-1"><title>Preprocessing</title><p>The anatomical data were preprocessed using FreeSurfer version 6.0 (<xref ref-type="bibr" rid="bib19">Fischl, 2012</xref>), which involved the segmentation and reconstruction of inflated and flattened cortical surfaces based on high-resolution anatomical data. We inspected visually and edited manually the surface segmentation to eliminate dura matter, sinus, etc., ensuring correct gray matter boundaries. The functional data were preprocessed and analyzed with AFNI (<xref ref-type="bibr" rid="bib15">Cox, 1996</xref>), ANTs (<xref ref-type="bibr" rid="bib4">Avants et al., 2011</xref>), and the mripy package developed in our lab (<ext-link ext-link-type="uri" xlink:href="https://github.com/herrlich10/mripy">https://github.com/herrlich10/mripy</ext-link>; <xref ref-type="bibr" rid="bib28">herrlich10, 2025</xref>). Preprocessing steps included head motion correction, de-spiking, slice timing correction, EPI distortion correction (nonlinear warping with blip-up/down method), and per-run scaling as percent signal change. All spatial transformations were combined and applied in a single interpolation step (sinc interpolation) to minimize the loss of spatial resolution (<xref ref-type="bibr" rid="bib93">Wang et al., 2022</xref>). No spatial smoothing was applied to the main functional imaging data. We aligned the anatomical volume as well as the reconstructed surfaces to the mean of preprocessed EPI images. General linear models (GLMs) were used to estimate the BOLD responses (β values) to visual stimuli with a canonical hemodynamic response function (BLOCK4 in AFNI). Slow baseline drift and motion parameters were included as nuisance regressors in GLMs.</p></sec><sec id="s4-5-2"><title>Cortical depth definition</title><p>To perform the cortical depth-dependent analysis, we resampled the functional volumes to 0.5 mm isotropic resolution using cubic interpolation (3dresample in AFNI). An equi-volume method was used to calculate the relative cortical depth of each voxel to the white matter and pial surface (0: white matter surface; 1: pial surface, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>), using mripy (<ext-link ext-link-type="uri" xlink:href="https://github.com/herrlich10/mripy">https://github.com/herrlich10/mripy</ext-link>). The voxels in each ROI were sorted and divided into three bins: deep depth (0–0.33), middle depth (0.33–0.67), and superficial depth (0.67–1.00) (<xref ref-type="bibr" rid="bib25">Ge et al., 2020</xref>; <xref ref-type="bibr" rid="bib38">Kemper et al., 2018</xref>).</p></sec><sec id="s4-5-3"><title>ROI definition</title><p>ROIs were defined on the inflated cortical surface. Surface ROIs for V1, V2, V3ab, and V4 were defined based on the polar angle atlas from the 7T retinotopic dataset of Human Connectome Project (<xref ref-type="bibr" rid="bib6">Benson et al., 2014</xref>; <xref ref-type="bibr" rid="bib7">Benson et al., 2018</xref>). Moreover, the boundary of V2 was edited manually based on columnar patterns. All ROIs were constrained to regions where mean activation across all stimulus conditions exceeded 0. In V2, ROIs for the thin and thick ‘stripe’-shaped columns were manually defined in two stages. Firstly, we defined thin stripes by contrast between the chromatic and achromatic stimuli, and thick stripes by contrast between binocular disparity and 2D control stimuli. Secondly, we defined final stripes by contrast between these two, resulting in interdigitated thin and thick stripes distributed without overlap. The pale stripes were defined as the regions located between the thin and thick stripes. We compared the fMRI signal changes elicited by the three stimulus contrasts in each stripe. For the cortical depth-dependent analyses in <xref ref-type="fig" rid="fig3">Figure 3</xref>, we used all voxels in the retinotopic ROI. Pooling all voxels in the ROI avoids the problem of double-dipping and also increases the signal-to-noise ratio of ROI-averaged BOLD responses.</p></sec><sec id="s4-5-4"><title>Stimulus-selectivity index</title><p>The ROI-averaged BOLD responses were calculated for each stimulus condition. We defined a selectivity index (SI) for color, disparity, and texture processing, respectively:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>S</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Here, β<sub>chr</sub>, β<sub>ach,</sub> β<sub>3D,</sub> β<sub>2D</sub>, β<sub>T</sub>, and β<sub>N</sub> represent the beta estimates of BOLD responses for the chromatic, achromatic, binocular disparity, 2D control, naturalistic texture, and spectrally matched noise stimuli, respectively.</p></sec><sec id="s4-5-5"><title>Test–retest reliability of columnar organizations</title><p>For five subjects who participated in both color and disparity experiments across two daily scan sessions, we generated color (β<sub>chr</sub> – β<sub>ach</sub>) and disparity (β<sub>3D</sub> – β<sub>2D</sub>) selective functional maps on the cortical surface in area V2. Pearson’s correlations were computed to evaluate the test–retest reliability of color- and disparity-selective response patterns between the two scan sessions. FWEs of the pattern correlations were controlled by a null distribution generated from Monte Carlo simulation (<xref ref-type="bibr" rid="bib72">Qian et al., 2023</xref>). In this procedure, the first session’s GLM residual volumes were used to estimate the spatial auto-correlation function (3dFWHMx in AFNI), and it was then used to generate a simulated GLM volume for the second session (3dClustSim in AFNI). We then projected the first and second sessions’ GLM volumes onto the cortical surface and calculated the inter-session correlations of color- and disparity-selective response patterns in V2. This process was repeated 10,000 times (<xref ref-type="fig" rid="fig2s9">Figure 2—figure supplement 9</xref>). Finally, the measured correlation coefficients were compared to the critical value of the null distribution.</p></sec><sec id="s4-5-6"><title>Pial vein removal</title><p>To mitigate the strong BOLD effect from large pial veins on layer-specific signals in the gray matter (<xref ref-type="bibr" rid="bib13">Cheng et al., 2001</xref>; <xref ref-type="bibr" rid="bib24">Gati et al., 1997</xref>; <xref ref-type="bibr" rid="bib37">Kay et al., 2019</xref>; <xref ref-type="bibr" rid="bib96">Yacoub et al., 2005</xref>), we excluded vertices with extremely large signal changes and their corresponding voxels in the gray matter. Specifically, the top 5% cortical vertices with large signal changes from baseline (all stimulus conditions vs. fixation) and the corresponding voxels across all cortical depths were excluded from analysis in V1, V2, V4, and V3ab (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>). According to our previous study (<xref ref-type="bibr" rid="bib48">Liu et al., 2021</xref>), this large vein removal approach can effectively reduce the superficial bias in laminar response profiles of the visual cortex.</p></sec><sec id="s4-5-7"><title>Cortical depth-dependent informational connectivity</title><p>To investigate stimulus-specific information flow in the visual processing hierarchy, we calculated informational connectivity between the input and output layers of two brain regions (<xref ref-type="bibr" rid="bib1">Aly and Turk-Browne, 2016</xref>; <xref ref-type="bibr" rid="bib14">Coutanche and Thompson-Schill, 2014</xref>; <xref ref-type="bibr" rid="bib27">Haxby et al., 2001</xref>; <xref ref-type="bibr" rid="bib34">Huffman and Stark, 2017</xref>; <xref ref-type="bibr" rid="bib35">Jia et al., 2020</xref>; <xref ref-type="bibr" rid="bib44">Koster et al., 2018</xref>). This approach is analogous to the functional connectivity method in multivariate pattern analysis, where connectivity is inferred from shared changes (covariation) in decoding accuracy between regions over time (i.e., across blocks). The layer-specific neural circuitry was further used to define the direction of informational connectivity (<xref ref-type="bibr" rid="bib35">Jia et al., 2020</xref>). Specifically, feedforward connectivity was defined as the connection between the superficial layer of the lower visual area and the middle layer of the higher visual area, whereas feedback connectivity was defined as the connection between two deep layers (<xref ref-type="fig" rid="fig1">Figure 1C</xref>).</p><p>In the analysis, a separate GLM regressor was first used to estimate the activation pattern (<italic>t</italic> scores of voxels) for each stimulus block. For each stimulus condition per cortical layer, 50 activation patterns were obtained for six subjects and 20 activation patterns for the other four subjects. We trained linear SVM classifiers (<ext-link ext-link-type="uri" xlink:href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link>) using these patterns and extracted the distance from the hyperplane for each stimulus block, following a leave-one-run-out cross-validation procedure. Before each training, feature selection was performed with K-1 runs of data to select voxels with high visual sensitivity and stimulus selectivity. We first selected the top 20% most visually responsive voxels by the <italic>t</italic> distribution of activations from baseline for a stimulus condition (e.g., Chromatic + Achromatic – Fixation). Then 200 voxels with strong stimulus selectivity were selected from each side of the <italic>t</italic> distribution of differential responses (e.g., Chromatic – Achromatic). The activation patterns of these 400 voxels were normalized to have a unitary Euclidean norm (L2-norm). An SVM classifier of stimulus type (e.g., Chromatic vs. Achromatic) was trained from K-1 runs of data, and the distance to the decision boundary was calculated for each stimulus block from the remaining run. Pearson’s correlation between the block-by-block distance timeseries of two brain regions was calculated to estimate the layer-specific informational connectivity. Then we averaged the correlation coefficients across all folds.</p><p>The correlation coefficients were Fisher z-transformed before statistical analysis. One-sample <italic>t</italic>-test against 0 was conducted on each connectivity value, and the results were submitted to false discovery rate (FDR) correction. The reported correlations are the original <italic>r</italic> values to facilitate interpretation and visualization.</p></sec></sec><sec id="s4-6"><title>Statistical analysis</title><p>Statistical analyses were performed using MATLAB 2021a, JASP (v0.17.1), and custom Python code. Repeated-measures ANOVA and paired <italic>t</italic>-tests were used for most of the statistical analyses of ROI data. An FWE-corrected threshold of p&lt;0.05 was used for each group of ANOVA. We further performed an FWE correction for paired <italic>t</italic>-tests only when the corrected p-value from ANOVA exceeded threshold, according to the (<xref ref-type="bibr" rid="bib20">Fisher, 1936</xref>; <xref ref-type="bibr" rid="bib47">Levin et al., 1994</xref>). We conducted Bayesian repeated-measures ANOVA to complement the classical null-hypothesis test with JASP (<xref ref-type="bibr" rid="bib92">Wagenmakers et al., 2018</xref>). The calculated Bayes factor (BF<sub>10</sub>) falling into 0.33–1 indicates anecdotal evidence for the null hypothesis (H<sub>0</sub>), whereas a value between 10–30 and &gt;100 refers to strong and extreme evidence for the alternative hypothesis (H<sub>1</sub>), respectively. Moreover, we used the nonparametric permutation test to test if selectivity indices differ across stripes. For each selectivity index, we resampled with replacement and computed the mean value within each type of stripe, and calculated the difference between each pair of stripes. This process was repeated 10,000 times to derive the null distribution. The critical value was set to 0.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Formal analysis, Supervision, Investigation, Methodology</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The experimental procedures were approved by the ethical review board of Institute of Biophysics, Chinese Academy of Sciences (No. 2012-IRB-011). Written informed consent was obtained from all participants prior to their participation in the study.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-93171-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data and code to reproduce the main findings of this study can be downloaded from Open Science Framework (OSF, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/VBTQS">https://doi.org/10.17605/OSF.IO/VBTQS</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Ai</surname><given-names>H</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>N</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Mesoscale functional organization and connectivity of color, disparity, and naturalistic texture in human second visual area</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/VBTQS</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This study was funded by STI2030-Major Projects (2022ZD0211900, 2021ZD0203600, 2021ZD0204200), National Natural Science Foundation of China (31971031, 31871107, 31930053), and Hunan Provincial Natural Science Foundation (2024JJ6313).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aly</surname><given-names>M</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Attention promotes episodic encoding by stabilizing hippocampal representations</article-title><source>PNAS</source><volume>113</volume><fpage>E420</fpage><lpage>E429</lpage><pub-id pub-id-type="doi">10.1073/pnas.1518931113</pub-id><pub-id pub-id-type="pmid">26755611</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Anstis</surname><given-names>S</given-names></name><name><surname>Cavanagh</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1983">1983</year><chapter-title>A minimum motion technique for judging equiluminance</chapter-title><person-group person-group-type="editor"><name><surname>Mollon</surname><given-names>J</given-names></name><name><surname>Sharpe</surname><given-names>LT</given-names></name></person-group><source>Colour Vision: Physiology and Psychophysics</source><publisher-name>Academic Press</publisher-name><fpage>155</fpage><lpage>166</lpage></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcizet</surname><given-names>F</given-names></name><name><surname>Jouffrais</surname><given-names>C</given-names></name><name><surname>Girard</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Natural textures classification in area V4 of the macaque monkey</article-title><source>Experimental Brain Research</source><volume>189</volume><fpage>109</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1007/s00221-008-1406-9</pub-id><pub-id pub-id-type="pmid">18506435</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Song</surname><given-names>G</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A reproducible evaluation of ANTs similarity metric performance in brain image registration</article-title><source>NeuroImage</source><volume>54</volume><fpage>2033</fpage><lpage>2044</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.025</pub-id><pub-id pub-id-type="pmid">20851191</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Usrey</surname><given-names>WM</given-names></name><name><surname>Adams</surname><given-names>RA</given-names></name><name><surname>Mangun</surname><given-names>GR</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Canonical microcircuits for predictive coding</article-title><source>Neuron</source><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id><pub-id pub-id-type="pmid">23177956</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Butt</surname><given-names>OH</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Aguirre</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Correction of distortion in flattened representations of the cortical surface allows prediction of V1-V3 functional organization from anatomy</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003538</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003538</pub-id><pub-id pub-id-type="pmid">24676149</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Jamison</surname><given-names>KW</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The Human Connectome Project 7 Tesla retinotopy dataset: Description and population receptive field analysis</article-title><source>Journal of Vision</source><volume>18</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.1167/18.13.23</pub-id><pub-id pub-id-type="pmid">30593068</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bilodeau</surname><given-names>L</given-names></name><name><surname>Faubert</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Isoluminance and chromatic motion perception throughout the visual field</article-title><source>Vision Research</source><volume>37</volume><fpage>2073</fpage><lpage>2081</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(97)00012-6</pub-id><pub-id pub-id-type="pmid">9327055</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Briggs</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Role of feedback connections in central visual processing</article-title><source>Annual Review of Vision Science</source><volume>6</volume><fpage>313</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-121219-081716</pub-id><pub-id pub-id-type="pmid">32552571</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkhalter</surname><given-names>A</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Processing of color, form and disparity information in visual areas VP and V2 of ventral extrastriate cortex in the macaque monkey</article-title><source>The Journal of Neuroscience</source><volume>6</volume><fpage>2327</fpage><lpage>2351</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.06-08-02327.1986</pub-id><pub-id pub-id-type="pmid">3746412</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Feedforward, feedback and inhibitory connections in primate visual cortex</article-title><source>Neural Networks</source><volume>17</volume><fpage>625</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2004.04.004</pub-id><pub-id pub-id-type="pmid">15288888</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>K</given-names></name><name><surname>Waggoner</surname><given-names>RA</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Human ocular dominance columns as revealed by high-field functional magnetic resonance imaging</article-title><source>Neuron</source><volume>32</volume><fpage>359</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00477-9</pub-id><pub-id pub-id-type="pmid">11684004</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coutanche</surname><given-names>MN</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Using informational connectivity to measure the synchronous emergence of fMRI multi-voxel information across time</article-title><source>Journal of Visualized Experiments</source><volume>01</volume><elocation-id>51226</elocation-id><pub-id pub-id-type="doi">10.3791/51226</pub-id><pub-id pub-id-type="pmid">25046335</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical Research, an International Journal</source><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1006/cbmr.1996.0014</pub-id><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeYoe</surname><given-names>EA</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Segregation of efferent connections and receptive field properties in visual area V2 of the macaque</article-title><source>Nature</source><volume>317</volume><fpage>58</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1038/317058a0</pub-id><pub-id pub-id-type="pmid">2412132</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federer</surname><given-names>F</given-names></name><name><surname>Ta’afua</surname><given-names>S</given-names></name><name><surname>Merlin</surname><given-names>S</given-names></name><name><surname>Hassanpour</surname><given-names>MS</given-names></name><name><surname>Angelucci</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Stream-specific feedback inputs to the primate primary visual cortex</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>228</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-20505-5</pub-id><pub-id pub-id-type="pmid">33431862</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>FreeSurfer</article-title><source>NeuroImage</source><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><pub-id pub-id-type="pmid">22248573</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1936">1936</year><article-title>Design of experiments</article-title><source>BMJ</source><volume>1</volume><elocation-id>554</elocation-id><pub-id pub-id-type="doi">10.1136/bmj.1.3923.554-a</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fracasso</surname><given-names>A</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Petridou</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Point-spread function of the BOLD response across columns and cortical depth in human extra-striate cortex</article-title><source>Progress in Neurobiology</source><volume>202</volume><elocation-id>102034</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2021.102034</pub-id><pub-id pub-id-type="pmid">33741401</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Ziemba</surname><given-names>CM</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A functional and perceptual signature of the second visual area in primates</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>974</fpage><lpage>981</lpage><pub-id pub-id-type="doi">10.1038/nn.3402</pub-id><pub-id pub-id-type="pmid">23685719</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>360</volume><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gati</surname><given-names>JS</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Rutt</surname><given-names>BK</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Experimental determination of the BOLD field strength dependence in vessels and tissue</article-title><source>Magnetic Resonance in Medicine</source><volume>38</volume><fpage>296</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1002/mrm.1910380220</pub-id><pub-id pub-id-type="pmid">9256111</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ge</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Qian</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>He</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Adaptation to feedback representation of illusory orientation produced from flash grab effect</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3925</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17786-1</pub-id><pub-id pub-id-type="pmid">32764538</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hatanaka</surname><given-names>G</given-names></name><name><surname>Inagaki</surname><given-names>M</given-names></name><name><surname>Takeuchi</surname><given-names>RF</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Ikezoe</surname><given-names>K</given-names></name><name><surname>Fujita</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Processing of visual statistics of naturalistic videos in macaque visual areas V1 and V4</article-title><source>Brain Structure &amp; Function</source><volume>227</volume><fpage>1385</fpage><lpage>1403</lpage><pub-id pub-id-type="doi">10.1007/s00429-022-02468-z</pub-id><pub-id pub-id-type="pmid">35286478</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Furey</surname><given-names>ML</given-names></name><name><surname>Ishai</surname><given-names>A</given-names></name><name><surname>Schouten</surname><given-names>JL</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="doi">10.1126/science.1063736</pub-id><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="software"><person-group person-group-type="author"><collab>herrlich10</collab></person-group><year iso-8601-date="2025">2025</year><data-title>Mripy</data-title><version designator="a758983">a758983</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://github.com/herrlich10/mripy">https://github.com/herrlich10/mripy</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>JM</given-names></name><name><surname>Roe</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Functionally specific and sparse domain-based micro-networks in monkey V1 and V2</article-title><source>Current Biology</source><volume>32</volume><fpage>2797</fpage><lpage>2809</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2022.04.095</pub-id><pub-id pub-id-type="pmid">35623347</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Complex–unoriented cells in a subregion of primate area 18</article-title><source>Nature</source><volume>315</volume><fpage>325</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1038/315325a0</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Segregation of form, color, and stereopsis in primate area 18</article-title><source>The Journal of Neuroscience</source><volume>7</volume><fpage>3378</fpage><lpage>3415</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-11-03378.1987</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>L</given-names></name><name><surname>Handwerker</surname><given-names>DA</given-names></name><name><surname>Jangraw</surname><given-names>DC</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Hall</surname><given-names>A</given-names></name><name><surname>Stüber</surname><given-names>C</given-names></name><name><surname>Gonzalez-Castillo</surname><given-names>J</given-names></name><name><surname>Ivanov</surname><given-names>D</given-names></name><name><surname>Marrett</surname><given-names>S</given-names></name><name><surname>Guidi</surname><given-names>M</given-names></name><name><surname>Goense</surname><given-names>J</given-names></name><name><surname>Poser</surname><given-names>BA</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>High-resolution cbv-fmri allows mapping of laminar activity and connectivity of cortical input and output in human M1</article-title><source>Neuron</source><volume>96</volume><fpage>1253</fpage><lpage>1263</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.005</pub-id><pub-id pub-id-type="pmid">29224727</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>L</given-names></name><name><surname>Uludağ</surname><given-names>K</given-names></name><name><surname>Möller</surname><given-names>HE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Non-BOLD contrast for laminar fMRI in humans: CBF, CBV, and CMR<sub>O2</sub></article-title><source>NeuroImage</source><volume>197</volume><fpage>742</fpage><lpage>760</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.041</pub-id><pub-id pub-id-type="pmid">28736310</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huffman</surname><given-names>DJ</given-names></name><name><surname>Stark</surname><given-names>CEL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The influence of low-level stimulus features on the representation of contexts, items, and their mnemonic associations</article-title><source>NeuroImage</source><volume>155</volume><fpage>513</fpage><lpage>529</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.019</pub-id><pub-id pub-id-type="pmid">28400264</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>K</given-names></name><name><surname>Zamboni</surname><given-names>E</given-names></name><name><surname>Kemper</surname><given-names>V</given-names></name><name><surname>Rua</surname><given-names>C</given-names></name><name><surname>Goncalves</surname><given-names>NR</given-names></name><name><surname>Ng</surname><given-names>AKT</given-names></name><name><surname>Rodgers</surname><given-names>CT</given-names></name><name><surname>Williams</surname><given-names>G</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Recurrent processing drives perceptual plasticity</article-title><source>Current Biology</source><volume>30</volume><fpage>4177</fpage><lpage>4187</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.08.016</pub-id><pub-id pub-id-type="pmid">32888488</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaplan</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>BB</given-names></name><name><surname>Shapley</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Chapter 7 New views of primate retinal function</article-title><source>Progress in Retinal Research</source><volume>9</volume><fpage>273</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1016/0278-4327(90)90009-7</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Jamison</surname><given-names>KW</given-names></name><name><surname>Vizioli</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Margalit</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A critical assessment of data quality and venous effects in sub-millimeter fMRI</article-title><source>NeuroImage</source><volume>189</volume><fpage>847</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.006</pub-id><pub-id pub-id-type="pmid">30731246</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemper</surname><given-names>VG</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Emmerling</surname><given-names>TC</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High resolution data analysis strategies for mesoscale human functional MRI at 7 and 9.4T</article-title><source>NeuroImage</source><volume>164</volume><fpage>48</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.058</pub-id><pub-id pub-id-type="pmid">28416453</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennedy</surname><given-names>B</given-names></name><name><surname>Bex</surname><given-names>P</given-names></name><name><surname>Hunter</surname><given-names>DG</given-names></name><name><surname>Nasr</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Two fine-scale channels for encoding motion and stereopsis within the human magnocellular stream</article-title><source>Progress in Neurobiology</source><volume>220</volume><elocation-id>102374</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2022.102374</pub-id><pub-id pub-id-type="pmid">36403864</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>SG</given-names></name><name><surname>Fukuda</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Lessons from fMRI about mapping cortical columns</article-title><source>The Neuroscientist</source><volume>14</volume><fpage>287</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1177/1073858407309541</pub-id><pub-id pub-id-type="pmid">17989170</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>T</given-names></name><name><surname>Bair</surname><given-names>W</given-names></name><name><surname>Pasupathy</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Perceptual texture dimensions modulate neuronal response dynamics in visual cortical area V4</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>631</fpage><lpage>642</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0971-21.2021</pub-id><pub-id pub-id-type="pmid">34862189</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiorpes</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Visual development in primates: Neural mechanisms and critical periods</article-title><source>Developmental Neurobiology</source><volume>75</volume><fpage>1080</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1002/dneu.22276</pub-id><pub-id pub-id-type="pmid">25649764</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohler</surname><given-names>PJ</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Yakovleva</surname><given-names>A</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Representation of maximally regular textures in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>714</fpage><lpage>729</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2962-15.2016</pub-id><pub-id pub-id-type="pmid">26791203</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koster</surname><given-names>R</given-names></name><name><surname>Chadwick</surname><given-names>MJ</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Berron</surname><given-names>D</given-names></name><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Düzel</surname><given-names>E</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Big-loop recurrence within the hippocampal system supports integration of information across episodes</article-title><source>Neuron</source><volume>99</volume><fpage>1342</fpage><lpage>1354</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.08.009</pub-id><pub-id pub-id-type="pmid">30236285</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>BB</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Receptive field structure in the primate retina</article-title><source>Vision Research</source><volume>36</volume><fpage>631</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(95)00167-0</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levi</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Perceptual learning in adults with amblyopia: A reevaluation of critical periods in human vision</article-title><source>Developmental Psychobiology</source><volume>46</volume><fpage>222</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1002/dev.20050</pub-id><pub-id pub-id-type="pmid">15772964</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levin</surname><given-names>JR</given-names></name><name><surname>Serlin</surname><given-names>RC</given-names></name><name><surname>Seaman</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>A controlled, powerful multiple-comparison strategy for several situations</article-title><source>Psychological Bulletin</source><volume>115</volume><fpage>153</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.115.1.153</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Guo</surname><given-names>F</given-names></name><name><surname>Qian</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Sun</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>DJ</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Layer-dependent multiplicative effects of spatial attention on contrast responses in human early visual cortex</article-title><source>Progress in Neurobiology</source><volume>207</volume><elocation-id>101897</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2020.101897</pub-id><pub-id pub-id-type="pmid">32818495</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Thalamic inputs to cytochrome oxidase-rich regions in monkey visual cortex</article-title><source>PNAS</source><volume>79</volume><fpage>6098</fpage><lpage>6101</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.19.6098</pub-id><pub-id pub-id-type="pmid">6193514</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Psychophysical evidence for separate channels for the perception of form, color, movement, and depth</article-title><source>The Journal of Neuroscience</source><volume>7</volume><fpage>3416</fpage><lpage>3468</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-11-03416.1987</pub-id><pub-id pub-id-type="pmid">3316524</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Segregation of form, color, movement, and depth: anatomy</article-title><source>Physiology, and Perception. Science</source><volume>240</volume><fpage>740</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1126/science.3283936</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>HD</given-names></name><name><surname>Roe</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Optical imaging of contrast response in Macaque monkey V1 and V2</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2675</fpage><lpage>2695</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl177</pub-id><pub-id pub-id-type="pmid">17264252</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>HD</given-names></name><name><surname>Roe</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Functional organization of color domains in V1 and V2 of macaque monkey revealed by optical imaging</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>516</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm081</pub-id><pub-id pub-id-type="pmid">17576751</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name><name><surname>Malonek</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Relationship between orientation domains, cytochrome oxidase stripes, and intrinsic horizontal connections in squirrel monkey area V2</article-title><source>Cerebral Cortex</source><volume>4</volume><fpage>151</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1093/cercor/4.2.151</pub-id><pub-id pub-id-type="pmid">8038566</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Ulbert</surname><given-names>I</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Intermodal selective attention in monkeys. I: distribution and timing of effects across visual areas</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>343</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.4.343</pub-id><pub-id pub-id-type="pmid">10769247</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merigan</surname><given-names>WH</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>How parallel are the primate visual pathways?</article-title><source>Annual Review of Neuroscience</source><volume>16</volume><fpage>369</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1146/annurev.ne.16.030193.002101</pub-id><pub-id pub-id-type="pmid">8460898</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mountcastle</surname><given-names>VB</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The columnar organization of the neocortex</article-title><source>Brain</source><volume>120</volume><fpage>701</fpage><lpage>722</lpage><pub-id pub-id-type="doi">10.1093/brain/120.4.701</pub-id><pub-id pub-id-type="pmid">9153131</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullen</surname><given-names>KT</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Bornstein changes in brightness matches may have produced artifacts in previous isoluminant</article-title><source>The Journal of Physiology</source><volume>359</volume><fpage>381</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1985.sp015591</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>SO</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Schrater</surname><given-names>P</given-names></name><name><surname>Woods</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Shape perception reduces activity in human primary visual cortex</article-title><source>PNAS</source><volume>99</volume><fpage>15164</fpage><lpage>15169</lpage><pub-id pub-id-type="doi">10.1073/pnas.192579399</pub-id><pub-id pub-id-type="pmid">12417754</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Interdigitated color- and disparity-selective columns within human visual cortical areas V2 and V3</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>1841</fpage><lpage>1857</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3518-15.2016</pub-id><pub-id pub-id-type="pmid">26865609</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassi</surname><given-names>JJ</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Parallel processing strategies of the primate visual system</article-title><source>Nature Reviews. Neuroscience</source><volume>10</volume><fpage>360</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nrn2619</pub-id><pub-id pub-id-type="pmid">19352403</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Laminar (f)MRI: a short history and future prospects</article-title><source>NeuroImage</source><volume>197</volume><fpage>643</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.04.082</pub-id><pub-id pub-id-type="pmid">31059800</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okazawa</surname><given-names>G</given-names></name><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Komatsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Image statistics underlying natural texture selectivity of neurons in macaque V4</article-title><source>PNAS</source><volume>112</volume><fpage>E351</fpage><lpage>E360</lpage><pub-id pub-id-type="doi">10.1073/pnas.1415146112</pub-id><pub-id pub-id-type="pmid">25535362</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okazawa</surname><given-names>G</given-names></name><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Komatsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Gradual development of visual texture-selective properties between macaque areas V2 and V4</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>4867</fpage><lpage>4880</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw282</pub-id><pub-id pub-id-type="pmid">27655929</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olman</surname><given-names>CA</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Feinberg</surname><given-names>DA</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Layer-specific fMRI reflects different neuronal computations at different depths in human V1</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e32536</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0032536</pub-id><pub-id pub-id-type="pmid">22448223</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkes</surname><given-names>LM</given-names></name><name><surname>Schwarzbach</surname><given-names>JV</given-names></name><name><surname>Bouts</surname><given-names>AA</given-names></name><name><surname>Deckers</surname><given-names>RHR</given-names></name><name><surname>Pullens</surname><given-names>P</given-names></name><name><surname>Kerskens</surname><given-names>CM</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Quantifying the spatial resolution of the gradient echo and spin echo BOLD response at 3 Tesla</article-title><source>Magnetic Resonance in Medicine</source><volume>54</volume><fpage>1465</fpage><lpage>1472</lpage><pub-id pub-id-type="doi">10.1002/mrm.20712</pub-id><pub-id pub-id-type="pmid">16276507</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterhans</surname><given-names>E</given-names></name><name><surname>von der Heydt</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Mechanisms of contour perception in monkey visual cortex. II. Contours bridging gaps</article-title><source>The Journal of Neuroscience</source><volume>9</volume><fpage>1749</fpage><lpage>1763</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.09-05-01749.1989</pub-id><pub-id pub-id-type="pmid">2723748</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>GF</given-names></name><name><surname>Fischer</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Binocular interaction and depth sensitivity in striate and prestriate cortex of behaving rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>40</volume><fpage>1392</fpage><lpage>1405</lpage><pub-id pub-id-type="doi">10.1152/jn.1977.40.6.1392</pub-id><pub-id pub-id-type="pmid">411898</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Uludağ</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neuroimaging with ultra-high field MRI: present and future</article-title><source>NeuroImage</source><volume>168</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.072</pub-id><pub-id pub-id-type="pmid">29410013</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portilla</surname><given-names>J</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Parametric texture model based on joint statistics of complex wavelet coefficients</article-title><source>International Journal of Computer Vision</source><volume>40</volume><fpage>49</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1023/A:1026553619983</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>de Hollander</surname><given-names>G</given-names></name><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Hierarchical and fine-scale mechanisms of binocular rivalry for conscious perception</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.02.11.528110</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Hollander</surname><given-names>G</given-names></name><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Mesoscale cortical mechanisms of perceptual conflict resolution in binocular rivalry</article-title><source>Research square</source><pub-id pub-id-type="doi">10.21203/rs.3.rs-4490689/v1</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RPN</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rockland</surname><given-names>KS</given-names></name><name><surname>Pandya</surname><given-names>DN</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Laminar origins and terminations of cortical connections of the occipital lobe in the rhesus monkey</article-title><source>Brain Research</source><volume>179</volume><fpage>3</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(79)90485-2</pub-id><pub-id pub-id-type="pmid">116716</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roe</surname><given-names>AW</given-names></name><name><surname>Ts’o</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Visual topography in primate V2: multiple representation across functional stripes</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>3689</fpage><lpage>3715</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-05-03689.1995</pub-id><pub-id pub-id-type="pmid">7751939</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roefs</surname><given-names>ECA</given-names></name><name><surname>Schellekens</surname><given-names>W</given-names></name><name><surname>Báez-Yáñez</surname><given-names>MG</given-names></name><name><surname>Bhogal</surname><given-names>AA</given-names></name><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>van Osch</surname><given-names>MJP</given-names></name><name><surname>Siero</surname><given-names>JCW</given-names></name><name><surname>Petridou</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>The contribution of the vascular architecture and cerebrovascular reactivity to the BOLD signal formation across cortical depth</article-title><source>Imaging Neuroscience</source><volume>2</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1162/imag_a_00203</pub-id><pub-id pub-id-type="pmid">39411228</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulte To Brinke</surname><given-names>T</given-names></name><name><surname>Duarte</surname><given-names>R</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Characteristic columnar connectivity caters to cortical computation: Replication, simulation, and evaluation of a microcircuit model</article-title><source>Frontiers in Integrative Neuroscience</source><volume>16</volume><elocation-id>923468</elocation-id><pub-id pub-id-type="doi">10.3389/fnint.2022.923468</pub-id><pub-id pub-id-type="pmid">36310713</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sengupta</surname><given-names>S</given-names></name><name><surname>Roebroeck</surname><given-names>A</given-names></name><name><surname>Kemper</surname><given-names>VG</given-names></name><name><surname>Poser</surname><given-names>BA</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Adriany</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A specialized multi-transmit head coil for high resolution fmri of the human visual cortex at 7T</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0165418</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0165418</pub-id><pub-id pub-id-type="pmid">27911950</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shao</surname><given-names>X</given-names></name><name><surname>Guo</surname><given-names>F</given-names></name><name><surname>Shou</surname><given-names>Q</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Jann</surname><given-names>K</given-names></name><name><surname>Yan</surname><given-names>L</given-names></name><name><surname>Toga</surname><given-names>AW</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>DJJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Laminar perfusion imaging with zoomed arterial spin labeling at 7 Tesla</article-title><source>NeuroImage</source><volume>245</volume><elocation-id>118724</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118724</pub-id><pub-id pub-id-type="pmid">34780918</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id><pub-id pub-id-type="pmid">11520932</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sincich</surname><given-names>LC</given-names></name><name><surname>Horton</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The circuitry of V1 and V2: integration of color, form, and motion</article-title><source>Annual Review of Neuroscience</source><volume>28</volume><fpage>303</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.28.061604.135731</pub-id><pub-id pub-id-type="pmid">16022598</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoop</surname><given-names>R</given-names></name><name><surname>Saase</surname><given-names>V</given-names></name><name><surname>Wagner</surname><given-names>C</given-names></name><name><surname>Stoop</surname><given-names>B</given-names></name><name><surname>Stoop</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Beyond scale-free small-world networks: cortical columns for quick brains</article-title><source>Physical Review Letters</source><volume>110</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.110.108105</pub-id><pub-id pub-id-type="pmid">23521304</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanigawa</surname><given-names>H</given-names></name><name><surname>Lu</surname><given-names>HD</given-names></name><name><surname>Roe</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional organization for color and orientation in macaque V4</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1542</fpage><lpage>1548</lpage><pub-id pub-id-type="doi">10.1038/nn.2676</pub-id><pub-id pub-id-type="pmid">21076422</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Cognitive neuroscience: primary visual cortex and visual awareness</article-title><source>Nature Reviews Neuroscience</source><volume>4</volume><fpage>219</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1038/nrn1055</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tootell</surname><given-names>RB</given-names></name><name><surname>Silverman</surname><given-names>MS</given-names></name><name><surname>De Valois</surname><given-names>RL</given-names></name><name><surname>Jacobs</surname><given-names>GH</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Functional organization of the second cortical visual area in primates</article-title><source>Science</source><volume>220</volume><fpage>737</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1126/science.6301017</pub-id><pub-id pub-id-type="pmid">6301017</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tootell</surname><given-names>RBH</given-names></name><name><surname>Nasr</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Columnar segregation of magnocellular and parvocellular streams in human extrastriate cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8014</fpage><lpage>8032</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0690-17.2017</pub-id><pub-id pub-id-type="pmid">28724749</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Sasaki</surname><given-names>Y</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Knutsen</surname><given-names>TA</given-names></name><name><surname>Mandeville</surname><given-names>JB</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Stereopsis activates V3A and caudal intraparietal areas in macaques and humans</article-title><source>Neuron</source><volume>39</volume><fpage>555</fpage><lpage>568</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00459-8</pub-id><pub-id pub-id-type="pmid">12895427</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ts’o</surname><given-names>DY</given-names></name><name><surname>Roe</surname><given-names>AW</given-names></name><name><surname>Gilbert</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A hierarchy of the functional organization for color, form and disparity in primate visual area V2</article-title><source>Vision Research</source><volume>41</volume><fpage>1333</fpage><lpage>1349</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00076-1</pub-id><pub-id pub-id-type="pmid">11322978</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uludag</surname><given-names>K</given-names></name><name><surname>Havlicek</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Determining laminar neuronal activity from BOLD fMRI using a generative model</article-title><source>Progress in Neurobiology</source><volume>207</volume><elocation-id>102055</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2021.102055</pub-id><pub-id pub-id-type="pmid">33930519</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Galkin</surname><given-names>TW</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Gattass</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Cortical connections of area V4 in the macaque</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>477</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm061</pub-id><pub-id pub-id-type="pmid">17548798</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name><name><surname>Love</surname><given-names>J</given-names></name><name><surname>Marsman</surname><given-names>M</given-names></name><name><surname>Jamil</surname><given-names>T</given-names></name><name><surname>Ly</surname><given-names>A</given-names></name><name><surname>Verhagen</surname><given-names>J</given-names></name><name><surname>Selker</surname><given-names>R</given-names></name><name><surname>Gronau</surname><given-names>QF</given-names></name><name><surname>Dropmann</surname><given-names>D</given-names></name><name><surname>Boutin</surname><given-names>B</given-names></name><name><surname>Meerhoff</surname><given-names>F</given-names></name><name><surname>Knight</surname><given-names>P</given-names></name><name><surname>Raj</surname><given-names>A</given-names></name><name><surname>van Kesteren</surname><given-names>E-J</given-names></name><name><surname>van Doorn</surname><given-names>J</given-names></name><name><surname>Šmíra</surname><given-names>M</given-names></name><name><surname>Epskamp</surname><given-names>S</given-names></name><name><surname>Etz</surname><given-names>A</given-names></name><name><surname>Matzke</surname><given-names>D</given-names></name><name><surname>de Jong</surname><given-names>T</given-names></name><name><surname>van den Bergh</surname><given-names>D</given-names></name><name><surname>Sarafoglou</surname><given-names>A</given-names></name><name><surname>Steingroever</surname><given-names>H</given-names></name><name><surname>Derks</surname><given-names>K</given-names></name><name><surname>Rouder</surname><given-names>JN</given-names></name><name><surname>Morey</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bayesian inference for psychology. Part II: Example applications with JASP</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>58</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.3758/s13423-017-1323-7</pub-id><pub-id pub-id-type="pmid">28685272</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Roe</surname><given-names>AW</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Critical factors in achieving fine-scale functional MRI: Removing sources of inadvertent spatial smoothing</article-title><source>Human Brain Mapping</source><volume>43</volume><fpage>3311</fpage><lpage>3331</lpage><pub-id pub-id-type="doi">10.1002/hbm.25867</pub-id><pub-id pub-id-type="pmid">35417073</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Du</surname><given-names>X</given-names></name><name><surname>Yao</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Tanigawa</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Roe</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Mesoscale organization of ventral and dorsal visual pathways in macaque monkey revealed by 7T fMRI</article-title><source>Progress in Neurobiology</source><volume>234</volume><elocation-id>102584</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2024.102584</pub-id><pub-id pub-id-type="pmid">38309458</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Felleman</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A spatially organized representation of colour in macaque cortical area V2</article-title><source>Nature</source><volume>421</volume><fpage>535</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1038/nature01372</pub-id><pub-id pub-id-type="pmid">12556893</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Van De Moortele</surname><given-names>P-F</given-names></name><name><surname>Shmuel</surname><given-names>A</given-names></name><name><surname>Uğurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Signal and noise characteristics of Hahn SE and GE BOLD fMRI at 7 T in humans</article-title><source>NeuroImage</source><volume>24</volume><fpage>738</fpage><lpage>750</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.09.002</pub-id><pub-id pub-id-type="pmid">15652309</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>High-field fMRI unveils orientation columns in humans</article-title><source>PNAS</source><volume>105</volume><fpage>10607</fpage><lpage>10612</lpage><pub-id pub-id-type="doi">10.1073/pnas.0804110105</pub-id><pub-id pub-id-type="pmid">18641121</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeki</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Colour coding in rhesus monkey prestriate cortex</article-title><source>Brain Research</source><volume>53</volume><fpage>422</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(73)90227-8</pub-id><pub-id pub-id-type="pmid">4196224</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziemba</surname><given-names>CM</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Selectivity and tolerance for visual texture in macaque V2</article-title><source>PNAS</source><volume>113</volume><fpage>E3140</fpage><lpage>E3149</lpage><pub-id pub-id-type="doi">10.1073/pnas.1510847113</pub-id><pub-id pub-id-type="pmid">27173899</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziemba</surname><given-names>CM</given-names></name><name><surname>Perez</surname><given-names>RK</given-names></name><name><surname>Pai</surname><given-names>J</given-names></name><name><surname>Kelly</surname><given-names>JG</given-names></name><name><surname>Hallum</surname><given-names>LE</given-names></name><name><surname>Shooner</surname><given-names>C</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Laminar differences in responses to naturalistic texture in macaque V1 and V2</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>9748</fpage><lpage>9756</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1743-19.2019</pub-id><pub-id pub-id-type="pmid">31666355</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93171.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Krug</surname><given-names>Kristine</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Otto-von-Guericke University Magdeburg</institution><country>Germany</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This study builds on previous findings showing modular organization of primate visual cortical areas by presenting <bold>important</bold> results about the cortical processing of color, disparity, and naturalistic textures in the human visual cortex at the spatial scale of cortical layers and columns using state-of-the-art high-resolution fMRI methods at ultra-high magnetic field strength (7T). <bold>Solid</bold> evidence supports an interesting layer-specific informational connectivity analysis to infer information flow across early visual areas for processing disparity and color signals. While the question of how the modularity of representation relates to cortical hierarchical processing is interesting, the findings that texture does not map onto previously established columnar architecture in V2 is suggestive. The successful application of high-resolution fMRI methods to study the functional organization along cortical columns and layers is relevant to a broad readership interested in general neuroscience.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93171.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study examines the cortical modular functional organization of visual texture in comparison with that of color and disparity. While color, disparity, and orientation have been shown to exhibit clear functional organizations within the thin, thick, and thick/pale stripes of V2, whether the feature of texture is also organized within V2 is unknown. Using ultrahigh field 7T fMRI in humans viewing color-, disparity-, and texture-specific visual stimuli, the authors find that, unlike color and disparity, texture does not exhibit stripe-specific organization in V2. Moreover, using laminar imaging methods and calculations of informational connectivity, they find V2 color and disparity stripes exhibit the expected feedforward and feedback relationships with V1 &amp; V4, and with V1 &amp; V3ab, respectively. In contrast, texture activation, found predominantly in the deep layers of V2, is driven preferentially by feedback from V4. Based on these findings, the authors suggest that texture is a visual feature computed in higher-order areas and not generated by local intra-V2 computation.</p><p>Strengths:</p><p>This study poses an interesting and fundamental question regarding the relationship between functional modularity and hierarchical origin of computed properties. This question is thus highly significant and deserves study. The methodology is appropriate for the question and the areal and laminar resolution achieved across 10 subjects is commendable. The combination of high-resolution functional imaging and informational connectivity analysis introduces a useful way for examining feedforward and feedback relationships in mesoscale imaging data.</p><p>Comments on latest version:</p><p>The authors have responded adequately to my comments. The lack of texture organization in V2 is now strengthened by the apparently more clustered texture response in V4 (Fig. S9). The paired results in V2 and V4 make the study stronger. The authors may suggest that texture response, while present at the neural level, may not emerge as a primary organizational cue in V2, based on this texture stimulus paradigm. The negative results should still be presented cautiously. The connectivity inferences are interesting but should also be stated cautiously, as there are multiple assumptions. Overall, this study makes a contribution to emerging views about texture processing in the early visual pathways.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93171.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This study investigates the cortical circuitry at the mesoscopic level of cortical columns in the human secondary visual cortex (V2) using high-resolution fMRI at ultra-high field strength (7T). The findings confirm the columnar organization of color-selective thin and disparity-selective thick stripes, a result previously demonstrated and replicated in human fMRI research. However, this study adds a novel layer of analysis by examining cortical depth, providing insights into feedforward and feedback connections to and from V2. Furthermore, examining texture selectivity in V2 showed no evidence of a columnar structure when compared to color- and disparity-selective activation clusters. Interestingly, texture selectivity in V2 was most pronounced in deeper cortical layers, with significant feedback connectivity from V4. The authors conclude that local columnar circuitry plays a crucial role in color and disparity processing within V2, while texture selectivity is driven by feedback modulation. This research underscores the potential of high-resolution human fMRI to explore the local circuitry of the cortex at the mesoscopic scale.</p><p>However, I still have a few comments that I would like to be addressed:</p><p>(1) In lines 401-403, the authors state that differential BOLD responses can significantly enhance the laminar specificity. Differential contrasts indeed have the potential to reduce macrovascular contributions that are unspecific to both experimental conditions, which was already discussed in the literature (e.g., Yacoub et al., 2008, High-field fMRI unveils orientation columns in humans). This might be especially true for the pial vasculature that drains a larger surface area of the cortex, e.g., multiple columns, which is probably the key factor that enables cortical column mapping using differential BOLD contrasts despite the relatively large spatial point spread function of the BOLD response. However, this may differ for laminar analyses, where neuronal and vascular responses from intracortical and pial veins might be harder to disentangle. It would, therefore, be advisable to tone down this statement somewhat since it could imply that laminar specificity can be readily achieved with GE-BOLD, while this remains an active area of research. This is not to say that the present results are incorrect, but the broader implications of this statement should be cautiously framed.</p><p>(2) Looking at Figure 3, one might also argue (excluding responses from V4) that statistically significant differences in selectivity are only observed where the cortical profiles generally show higher response levels. Could this be simply due to varying signal-to-noise ratios (SNR) achieved by different contrasts (color, disparity, texture)?</p><p>(3) In lines 480-484, the authors state that twenty blocks for each stimulus condition should be sufficient to investigate within-subject effects. It would be helpful if they could elaborate on the basis for this claim. High-resolution fMRI is typically limited by low temporal signal-to-noise ratio (tSNR), and extensive averaging is often required to achieve sufficient signal. Clarifying the rationale behind this assertion would strengthen the argument.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93171.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Ai et al. studied texture, color and disparity selectivity in human visual cortex at mesoscale level using high-resolution fMRI. They reproduced earlier monkey and human studies showing interdigitated color-selective and disparity-selective sub-compartments within area V2, likely corresponding to thin and thick stripes, respectively. At least with the stimuli used, no clear evidence for texture-selective mesoscale activations were observed in area V2. The most interesting and novel part of this study focused on cortical-depth-dependent connectivity analyses across areas. The data suggest feedback and feedforward functional connectivity between V1 and V3A for disparity signals and feedback from V4 to the deep layers of V2 for textures.</p><p>Strengths:</p><p>High-resolution fMRI and highly interesting layer-specific informational connectivity analyses.</p><p>Weaknesses:</p><p>The authors tend to overclaim their results. Too few data to make conclusive inferences.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.93171.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ai</surname><given-names>Hailin</given-names></name><role specific-use="author">Author</role><aff><institution>Tsinghua University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Lin</surname><given-names>Weiru</given-names></name><role specific-use="author">Author</role><aff><institution>Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Chengwen</given-names></name><role specific-use="author">Author</role><aff><institution>Hunan Normal University</institution><addr-line><named-content content-type="city">Changsha</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Nihong</given-names></name><role specific-use="author">Author</role><aff><institution>Tsinghua University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Peng</given-names></name><role specific-use="author">Author</role><aff><institution>Chinese Academy of Sciences</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Public Review:</bold></p><p><bold>Reviewer #1:</bold></p><p>(1) To support the finding that texture is not represented in a modular fashion, additional possibilities must be considered. These include (a) the effectiveness and specificity of the texture stimulus and control stimuli, (b) further analysis of possible structure in images that may have been missed, and (c) limitations of imaging resolution.</p></disp-quote><p>Thank you for your comments. To address your concerns, we have conducted a new 3T fMRI experiment to demonstrate the effectiveness and specificity of our stimuli, performed further analyses to investigate possible structure of texture-selective activation, and discussed the limitations of imaging resolution.</p><p>(a) To demonstrate the effectiveness and specificity of our stimuli, we conducted a new 3T fMRI experiment in five participants using an experimental design and texture families similar to those in Freeman (2013). Six texture stimuli in the 7T experiment were also included. To assess the effectiveness of each stimulus type, different texture families and their corresponding noise patterns were presented in separate blocks for 24 seconds, at a high presentation rate of 5 frames per second. In Figure S7, all texture families showed significantly stronger activation in V2 compared to their corresponding noise patterns, even for those that ‘appeared’ to have residual texture (e.g., the third texture family). These results demonstrate that our texture vs. noise stimuli were effective in producing texture-selective activations in area V2. Compared to the 7T results, the 3T data showed a notable increase in texture-selective activations in V2, likely due to increased stimulus presentation speed (1.25 vs. 5 frames/second). Future studies should use stimuli with faster presentation speed to validate our results in the 7T experiment.</p><p>(b)Thank you for pointing out the possible structures of texture-selective activations in the peripheral visual field (Figure S1). In further analyses, we also found stronger texture selectivity in more peripheral visual fields (Figure 2D), and there were weak but significant correlations in the texture-noise activation patterns during split-half analysis (Author response image 2). Although this is not strong evidence for columnar organization of naturalistic textures, it suggests a possibility for modular organizations in the peripheral visual field.</p><p>(c) Although our fMRI result at 1-mm isotropic resolution did not show strong evidence for modular processing of naturalistic texture in V2 stripe columns, this does not exclude the possibility that smaller modules exist beyond the current fMRI resolution. We have discussed this possibility in the revised manuscript.</p><p>We hope this response clarifies our findings, and we have revised the conclusions in the manuscript accordingly.</p><disp-quote content-type="editor-comment"><p>(2) More in-depth analysis of subject data is needed. The apparent structure in the texture images in peripheral fields of some subjects calls for more detailed analysis. e.g Relationship to eccentricity and the need for a 'modularity index' to quantify the degree of modularity. A possible relationship to eccentricity should also be considered.</p></disp-quote><p>Based on your recommendations, we have performed further analysis and found interesting results regarding the modularity index in relation to eccentricity. As shown in Figure 2D, the texture-selectivity index increased as eccentricity. This may suggest a higher possibility of modular organization for texture representation in the peripheral compared to central visual fields. We have updated our results in Figure 2C, and discussed this possibility in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>(3) Given what is known as a modular organization in V4 and V3 (e.g. for color, orientation, curvature), did images reveal these organizations? If so, connectivity analysis would be improved based on such ROIs. This would further strengthen the hierarchical scheme.</p></disp-quote><p>Following your recommendations, we have conducted further analysis to investigate the potential modular organizations in V4 and V3ab. In Figure S9 (Figure S9), vertices that are most responsive to color, disparity and texture were shown in a representative subject. Indeed, texture-selective patches can be found in both V4 and V3ab, along with the color- and disparity-selective patches. We agree with you that there should be pathway-specific connectivity among the same type of functional modules. In the informational connectivity analyses, we already used highly informative voxels by feature selection, which should mainly represent information from the modular organizations in these higher visual areas.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>(1) In lines 162-163, it is stated that no clear columnar organization exists for naturalistic texture processing in V2. In my opinion, this should be rephrased. As far as I understand, Figure 2B refers to the analysis used to support the conclusion. The left and middle bar plots only show a circular analysis since ROIs were based on the color and disparity contrast used to define thin and thick stripes. The interesting graph is the right plot, which shows no statistically significant overlap of texture processing with thin, thick, and pale stripe ROIs. It should be pointed out that this analysis does not dismiss a columnar organization per se but instead only supports the conclusion of no coincidence with the CO-stripe architecture.</p></disp-quote><p>Thank you for your suggestions. Reviewer #1 also raised a similar concern. We agree that there may be a smaller functional module of textures in area V2 at a finer spatial scale than our fMRI resolution. We have rephrased our conclusions to be more precise.</p><disp-quote content-type="editor-comment"><p>(2) In Figure 3, cortical depth-dependent analyses are presented for color, disparity, and texture processing. I acknowledge that the authors took care of venous effects by excluding outlier voxels. However, the GE-BOLD signal at high magnetic fields is still biased to extravascular contributions from around larger veins. Therefore, the highest color selectivity in superficial layers might also result from the bias to draining veins and might not be of neuronal origin. Furthermore, it is interesting that cortical profiles with the highest selectivity in superficial layers show overall higher selectivity across cortical depth. Could the missing increase toward the pial surface in other profiles result from the ROI definition or overall smaller signal changes (effect size) of selected voxels? At least, a more careful interpretation and discussion would be helpful for the reader.</p></disp-quote><p>We agree with you that there will be residual venous effects even after removing voxels containing large veins. However, calculating the selectivity index largely removed the superficial bias (Figure 3). In the revised manuscript, we discussed the limitations of cortical depth-dependent analysis using GE-BOLD fMRI.</p><p>In Line 397-403: “Due to the limitations of the T2*w GE-BOLD signal in its sensitivity to large draining veins (Fracasso et al., 2021; Parkes et al., 2005; Uludag &amp; Havlicek, 2021), the original BOLD responses were strongly biased towards the superficial depth in our data (Figure S8). Compared to GE-BOLD, VASO-CBV and SE-BOLD fMRI techniques have higher spatial specificity but much lower sensitivity (Huber et al., 2019). As shown in a recent study (Qian et al., 2024), using differential BOLD responses in a continuous­­ stimulus design can significantly enhance the laminar specificity of the feature selectivity measures in our results (Figure 3).”</p><p>It is unlikely that the strongest color selectivity index in the superficial depth is a result of stronger signal change or larger effect size in this condition. As shown by the original BOLD responses in Figure S8, all stimulus conditions produced robust activations that strongly biased to the superficial depth. High texture selectivity was also found in V4 and V3ab across cortical depth, which showed a flat laminar profile.</p><disp-quote content-type="editor-comment"><p>(3) I was slightly surprised that no retinotopy data was acquired. The ROI definition in the manuscript was based on a retinotopy atlas plus manual stripe segmentation of single columns. Both steps have disadvantages because they neglect individual differences and are based on subjective assessment. A few points might be worth discussing: (1) In lines 467-468, the authors state that V2 was defined based on the extent of stripes. This classical definition of area V2 was questioned by a recent publication (Nasr et al., 2016, J Neurosci, 36, 1841-1857), which showed that stripes might extend into V3. Could this have been a problem in the present analysis, e.g., in the connectivity analysis? (2) The manual segmentation depends on the chosen threshold value, which is inevitably arbitrary. Which value was used?</p></disp-quote><p>A previous study showed that the retinotopic atlas of early visual areas (V1-V3) aligned very well across participants on the standard surface after surface-based registration by the anatomical landmarks (Benson 2018). Thus, the group-averaged atlas should be accurate in defining the boundaries of early visual areas. To directly demonstrate the accuracy of this method, retinotopic data were acquired in five participants in a 3T fMRI experiment. A phase-encoded method was used to define the boundaries of early visual areas (black lines in Author response image 1), which were highly consistent with the Benson atlas.</p><p>Although a few feature-selective stripes may extend into V3, these stripe patterns were mainly represented in V2. Thus, the signal contribution from V3 is likely to be small and should not affect the pattern of results. The activation map threshold for manual segmentation was abs(T)&gt;2. We have clarified this in the revised methods.</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><caption><title>Retinotopic ROIs defined by the Benson atlas (left) and the polar angle map (right) of the representative subject.</title><p>Black lines denote the boundaries of early visual areas based on the retinotopic map from the subject.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-sa4-fig1-v1.tif"/></fig><p>Benson, N. C., Jamison, K. W., Arcaro, M. J., Vu, A. T., Glasser, M. F., Coalson, T. S., Van Essen, D. C., Yacoub, E., Ugurbil, K., Winawer, J., &amp; Kay, K. (2018). The Human Connectome Project 7 Tesla retinotopy dataset: Description and population receptive field analysis. J Vis, 18(13), 23. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/18.13.23">https://doi.org/10.1167/18.13.23</ext-link></p><disp-quote content-type="editor-comment"><p>(4) The use of 1-mm isotropic voxels is relatively coarse for cortical depth-dependent analyses, especially in the early visual cortex, which is highly convoluted and has a small cortical thickness. For example, most layer-fMRI studies use a voxel size of around isotropic 0.8 mm, which has half the voxel volume of 1 mm isotropic voxels. With increasing voxel volume, partial volume effects become more pronounced. For example, partial volume with CSF might confound the analysis by introducing pulsatility effects.</p></disp-quote><p>We agree that a 1-mm isotropic voxel is much larger in volume than a 0.8-mm isotropic voxel, but the resolution along the cortical depth is not a big difference. In addition to our study, a previous study showed that fMRI at 1-mm isotropic resolution is capable of resolving cortical depth-dependent signals (Roefs et al., 2024; Shao et al., 2021). We have discussed these issues about fMRI resolution in the revised manuscript.</p><p>In Line 403-408: “Compared to the submillimeter voxels, as used in most laminar fMRI studies, our fMRI resolution at 1-mm isotropic voxel may have a stronger partial volume effect in the cortical depth-dependent analysis. However, consistent with our results, previous studies have also shown that 7T fMRI at 1-mm isotropic resolution can resolve cortical depth-dependent signals in human visual cortex (Roefs et al., 2024; Shao et al., 2021).”</p><p>Shao, X., Guo, F., Shou, Q., Wang, K., Jann, K., Yan, L., Toga, A. W., Zhang, P., &amp; Wang, D. J. J. (2021). Laminar perfusion imaging with zoomed arterial spin labeling at 7 Tesla. NeuroImage, 245, 118724. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2021.118724">https://doi.org/10.1016/j.neuroimage.2021.118724</ext-link></p><p>Roefs, E. C., Schellekens, W., Báez-Yáñez, M. G., Bhogal, A. A., Groen, I. I., van Osch, M. J., ... &amp; Petridou, N. (2024). The Contribution of the Vascular Architecture and Cerebrovascular Reactivity to the BOLD signal Formation across Cortical Depth. <italic>Imaging Neuroscience, 2</italic>, 1–19.</p><disp-quote content-type="editor-comment"><p>(5) The SVM analysis included a feature selection step stated in lines 531-533. Although this step is reasonable for the training of a machine learning classifier, it would be interesting to know if the authors think this step could have reintroduced some bias to draining vein contributions.</p></disp-quote><p>We excluded vertices with extremely large signal change and their corresponding voxels in the gray matter when defining ROIs. The same number of voxels were selected from each cortical depth for the SVM analysis, thus there was no bias in the number of voxels from the superficial layers susceptible to large draining veins.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>The authors tend to overclaim their results.</p></disp-quote><p>Re: Thank you for your comments. We added more control analyses to strengthen our findings, and gave more appropriate discussion of results.</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1:</bold></p><p>(1) Controls: There is a bit more complexity than is expressed in the introduction. The authors hypothesize that the emergence of computational features such as texture may be reflected in specialized columns. That is, if texture is generated in V2, there may be texture columns (perhaps in the pale stripes of V2); but if generated at a higher level, then no texture columns would be needed. This is a very interesting and fundamental hypothesis. While there may be merit to this hypothesis, the demonstration that color and disparity are modular but not texture falls short of making a compelling argument. At a minimum, the finding that texture is not organized in V2 requires additional controls. (a) To boost the texture signal, additional texture stimuli or a sequence of multiple texture stimuli per trial could be considered. (b) Unfortunately, the comparison noise pattern also seems to contain texture; perhaps a less textured control could be designed. (c) It also appears that some of the texture images in Supplementary Figure S1 contain possible structure, e.g. in more peripheral visual fields. (d) Is it possible that the current imaging resolution is not sufficient for revealing texture domains? (e) Note that 'texture' may be a property that defines surfaces and not contours. Thus, while texture may have orientation content, its function may be associated with the surface processing pathways. A control stimulus might contain oriented elements of a texture stimulus that do not elicit texture percept; such a control might activate pale and/or thick stripes (both of which contain orientation domains), while the texture percept stimulus may activate surface-related bands in V4.</p></disp-quote><p>Thank you for your suggestions. They are extremely helpful in improving our manuscript. For the controls you mentioned in (a-d), we discussed them in the public review that we also attached below.</p><p>(a) and (b): To demonstrate the effectiveness and specificity of our stimuli, we conducted a new 3T fMRI experiment in five participants using an experimental design and texture families similar to those in Freeman (2013). All texture stimuli in the 7T experiment were also included. To assess the effectiveness of each stimulus type, different texture families and their corresponding noise patterns were presented in separate blocks for 24 seconds, at a high presentation rate of 5 frames per second. In Figure S7, all texture families showed significantly stronger activation in V2 compared to their corresponding noise patterns, even for those that ‘appeared’ to have residual texture (e.g., the third texture family). These results suggest that our texture stimuli were effective in producing texture-selective activations in area V2 compared to the noise control. Compared to the 7T results, the 3T data showed a notable increase in texture-selective activations in V2, likely due to the increased stimulus presentation speed (1.25 vs. 5 frames/second). Weak texture activations might preclude the detection of columnar representations in the 7T experiment.</p><p>(c) Thank you for pointing out the possible structures of texture-selective activations in the peripheral visual field (Figure S1). In further analyses, we also found stronger texture selectivity in more peripheral visual fields (Figure 2D), and there were weak but significant correlations in the texture-noise activation patterns during split-half analysis (Author response image 2). Although these are not strong evidence for columnar organization of naturalistic textures, it suggests a possibility for such organizations in the peripheral visual field.</p><p>(d) Although our fMRI result at 1-mm isotropic resolution did not show strong evidence for modular processing of naturalistic texture in V2 stripe columns, this does not exclude the possibility that smaller modules exist beyond the current fMRI resolution. We have discussed these limitations in the revised manuscript.</p><p>We fully agree with your explanation in (e). It fits our data very well. Both texture and control stimuli strongly activated the CO-stripes (Figure 2 and Figure 2D), while modular organizations for texture were found in V4 and V3ab (Figure S9). We have discussed this explanation in the revised manuscript.</p><p>In Line 371-374: “Consistently, our pilot results also revealed modular organizations for textures in V4 and V3ab (Figure S9). These texture-selective organizations may be related to surface representations in these higher order visual areas (Wang et al., 2024).”</p><disp-quote content-type="editor-comment"><p>(2) Overly simple description of FF, FB circuitry. The classic anatomical definition of feedforward is output from a 'lower' area, in most cases predominantly arising from superficial layers and projecting to middle layers of a 'higher area' (Felleman and Van Essen 1991). This description holds for V1-to-V2, V2-to-V3, and V2-to-V4. [Note there are also feedforward projections from central 5 degrees of V1-to-V4 (cf. Ungerleider) as well as V3-to-V4.] The definition of feedback can be more varied but is generally considered from cells in superficial and deep layers of 'higher' areas projecting to superficial and deep layers of 'lower' areas. Feedback inputs to V1 heavily innervate Layer 1 and superficial Layer 2, as well as the deep layers. Note that feedback connections from V2 to V1, similar to that from V1 to V2, are functionally specific, i.e. thin-to-blob and pale/thick-to interblob (Federer...Angelucci 2021, Hu...Roe 2022). Thus, current views are moving away from the dogma that feedback is diffuse. Recognition that feedback may be modular introduces new ideas about analysis.</p></disp-quote><p>Thanks for your detailed recommendations. We have expanded the discussion of circuit models of functional connectivity in the introduction. Our model and experiments primarily aim to investigate how higher-level areas provide feedback to the V2 area. While we acknowledge that feedback may indeed be functionally specific, our methodology has some certain advantages: it ensures signal stability and avoids the double-dipping issue. Meanwhile, it also focuses on voxels with high feature selectivity, which may already be included in the modular organizations of early visual areas. In the functional connectivity analysis, we performed feature selection to use the most informative voxels. These voxels with high feature selectivity should already be included in the modular organizations of early visual areas. Identifying functionally specific feedback connections between modular areas will be an important and meaningful work for future research. We have added a discussion of this topic in the revised manuscript.</p><p>In Line 136-138: “Only major connections were shown here. There are also other connections, such as V1 interblobs projecting to thick stripes (Federer et al., 2021; Hu &amp; Roe, 2022; Sincich and Horton, 2005).”</p><disp-quote content-type="editor-comment"><p>(3) Imaging superficial layers: Although removal of the top layer of cortical voxels (top 5% of voxels) is a common method for dealing with surface vascular artifact contribution to BOLD signal, it likely removes a portion of the Layer 1&amp;2 feedback signals. Is this why the authors define feedback and deep layer to deep layer? If so, both superficial and deep-layer data in Figure 4 should be explicitly explained and discussed.</p></disp-quote><p>Thank you for pointing this out. We would like to clarify the surface-based method removing vascular artifact. The vertices influenced by large pial veins were first defined on the cortical surface, and then voxels were removed from the entire columns corresponding to these vertices to avoid sampling bias along the cortical depth. Thus, there should be complete data from all cortical depths for the remaining columns. We defined the feedback connectivity from deep layers to deep layers because it represents strong feedback connections according to literature (Markov et al., 2013; Ullman, 1995) and also avoids confounding the feedforward signals from superficial layers.</p><p>Markov, N. T., Vezoli, J., Chameau, P., Falchier, A., Quilodran, R., Huissoud, C., Lamy, C., Misery, P., Giroud, P., Ullman, S., Barone, P., Dehay, C., Knoblauch, K., &amp; Kennedy, H. (2014). Anatomy of hierarchy: feedforward and feedback pathways in macaque visual cortex. The Journal of comparative neurology, 522(1), 225–259. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/cne.23458">https://doi.org/10.1002/cne.23458</ext-link></p><p>Ullman S. (1995). Sequence seeking and counter streams: a computational model for bidirectional information flow in the visual cortex. Cerebral cortex, 5(1), 1–11. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/5.1.1">https://doi.org/10.1093/cercor/5.1.1</ext-link></p><disp-quote content-type="editor-comment"><p>(4) More detail on other subjects in Figure S1. Ten subjects conducted visual fixation and used a bite bar. Imaging data are illustrated in detail from one subject and the remaining subjects are depicted in graphs and in Supplemental Figure S1. Please provide arrowheads in each image to help guide the reader. Some kind of summary or index of modularity would also be helpful.</p></disp-quote><p>Thanks for your suggestions. There are arrowheads in each image in our original manuscript and we have revised Figure S1 for better illustration. Additionally, we have added a table summarizing the number of stripes to provide a clearer overview.</p><disp-quote content-type="editor-comment"><p>(5) How are ROIs in V3ab and V4 defined? V2 ROIs were defined (thin, thick, and pale stripe), but V3ab and V4 averaged across the whole area. Why not use the most activated &quot;domains&quot; from V3ab and V4? How does this influence connectivity analysis?</p></disp-quote><p>Thank you for your question. We defined V4 and V3ab on the cortical surface using a retinotopic atlas (Benson 2018), which has been shown to be quite accurate in defining ROIs for the early visual areas. Since all ‘domains’ showed robust BOLD activation to our stimuli, we used voxels from the entire ROI in the depth-dependent analysis. In the functional connectivity analysis, we used the most informative voxels by feature selection, which should already be included in the feature domains.</p><disp-quote content-type="editor-comment"><p>Minor:</p><p>English language editing is needed.</p></disp-quote><p>Thank you for your feedback. We have carefully revised the manuscript for clarity and readability.</p><disp-quote content-type="editor-comment"><p>Line 31 &quot;its&quot; should be &quot;their&quot;.</p></disp-quote><p>Thank you. We have corrected &quot;its&quot; to &quot;their&quot;.</p><disp-quote content-type="editor-comment"><p>Replace 'representative subject' with 'subject'.</p></disp-quote><p>We have replaced &quot;representative subject&quot; with &quot;subject&quot; in the manuscript.</p><disp-quote content-type="editor-comment"><p>Replace 'naturalistic texture' with 'texture'.</p></disp-quote><p>Thank you for your suggestion. The textures used in our experiment were generated based on the algorithm by Portilla and Simoncelli (2000), and the term &quot;naturalistic texture&quot; was used to be consistent with literature. The textures used in our study are different from traditional artificial textures, as they contain higher-order statistical dependencies. Following your recommendations, we have replaced ‘naturalistic texture’ with ‘texture’ in some places in the main text to improve readability.</p><disp-quote content-type="editor-comment"><p>Typo: Line 126, Fig 2B should be 1B.</p></disp-quote><p>Thank you. We have corrected &quot;Fig 2B&quot; to &quot;Fig 1B&quot; in Line 128.</p><disp-quote content-type="editor-comment"><p>Fig. 2A: point out where are texture domains in anterior V2.</p></disp-quote><p>The texture-selective activations in anterior V2 (corresponds to peripheral visual field) have been highlighted by arrowheads.</p><disp-quote content-type="editor-comment"><p>Fig 2B, 3 legend: Round symbols are for each subject?</p></disp-quote><p>Yes, the round symbols in Figures 2B represent data for individual participants. We have revised the legend for clarity.</p><disp-quote content-type="editor-comment"><p>Fig. 3: Disparity and texture values do not look different across depth (except may the V2 texture values).</p></disp-quote><p>While the difference in feature selectivity is small across cortical depths, they are highly consistent across participants. We have provided a figure showing the original BOLD responses in the revised manuscript (Figure S8 and Figure S8). Data from individual subjects were also available at Open Science Framework (OSF, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/KSXT8">https://doi.org/10.17605/OSF.IO/KSXT8</ext-link> (‘rawBetaValues.mat’ in the data directory)).</p><disp-quote content-type="editor-comment"><p>Line 57-59 The statement is not strictly accurate. V1 also has color, orientation, and motion representations.</p></disp-quote><p>Thank you for your feedback. Our statement was intended to convey that M and P information from the geniculate input are transformed into representations of color, orientation, disparity, and motion in the primary visual cortex. We have clarified this point in the revised manuscript.</p><p>In Line 58-60: “In the primary visual cortex (V1), the M and P information from the geniculate input are transformed into higher-level visual representations, such as motion, disparity, color, orientation, etc. (Tootell &amp; Nasr, 2017).”</p><disp-quote content-type="editor-comment"><p>Fig. 1B V1 interblobs also project to thick stripes (Sincich and Horton).</p></disp-quote><p>Thank you for the additional information. We appreciate your input. Our figure is intended as a simplified schematic and does not fully represent all the connections. We have discussed this reference in the revised manuscript.</p><p>In Line 136-138: “Only major connections were shown here. There are also other connections, such as V1 interblobs projecting to thick stripes (Federer et al., 2021; Hu &amp; Roe, 2022; Sincich and Horton, 2005).”</p><disp-quote content-type="editor-comment"><p>Line 207 &quot;suggesting that both local and feedforward connections are involved in processing color information in area V2.&quot; Logic? English?</p></disp-quote><p>Thank you for pointing this out. The superficial layers are involved in local intracortical processing by lateral connections and also send output to higher order visual areas along the feedforward pathway. Thus, the strongest color selectivity in the superficial depth of V2 supports that color information was processed in local neural circuits in area V2 and transmitted to higher order areas along the feedforward pathway. We have revised the manuscript for clarity.</p><p>In Line 241-245: “According to the hierarchical model, the strongest color selectivity in the superficial cortical depth is consistent with the fact that color blobs locate in the superficial layers of V1 (Figure 1B, Felleman &amp; Van Essen, 1991; Hubel &amp; Livingstone, 1987; Nassi &amp; Callaway, 2009). The strongest color selectivity in superficial V2 suggests that both local and feedforward connections are involved in processing color information (Figure 1C).”</p><disp-quote content-type="editor-comment"><p>Line 254 &quot;Laminar&quot;. Please use &quot;cortical depth&quot; or explicitly state that 'laminar' refers to superficial, middle, and deep as defined by cortical depth.</p></disp-quote><p>Thank you for your suggestion. We have clarified the term &quot;laminar&quot; in the manuscript as referring to superficial, middle, and deep layers as defined by cortical depth.</p><p>In Line 96-99: “To better understand the mesoscale functional organizations and neural circuits of information processing in area V2, the present study investigated laminar (or cortical depth-dependent) and columnar response profiles for color, disparity, and naturalistic texture in human V2 using 7T fMRI at 1-mm isotropic resolution.”</p><disp-quote content-type="editor-comment"><p>Fig. S5 Please add a unit of isoluminance.</p></disp-quote><p>Thank you for your suggestion. Supplementary Figure S10A and S10B illustrate the blue-matched luminance levels in RGB index. In our isoluminance experiment, blue was set as the reference color (RGB [0 0 255]) to measure the red and gray isoluminance.</p><disp-quote content-type="editor-comment"><p>Line 448-449 To make this rationale clearer, refer to:</p><p>Wang J, Nasr S, Roe AW, Polimeni JR. 2022. Critical factors in achieving fine‐scale functional MRI: Removing sources of inadvertent spatial smoothing. Human Brain Mapping. 43:3311-3331.</p></disp-quote><p>Thank you for your suggestion. We have added this reference to better support the rationale of data analysis.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2:</bold></p><p>(1) Line 126 should refer to Figure 1B.</p></disp-quote><p>Thank you. We have corrected the reference in the revised manuscript as Figure 1B.</p><disp-quote content-type="editor-comment"><p>(2) Even if only one naturalistic texture session was acquired per participant, it might be interesting to see the within-session repeatability by, e.g., splitting the texture runs into two halves.</p></disp-quote><p>Thank you for your suggestion. We performed a split-half correlation analysis for participants who completed 10 runs in the naturalistic texture session. The result from one representative subject was shown in the figure below (for other participants, r = 0.38, 0.38, 0.24, and 0.23, respectively).</p><fig id="sa4fig2" position="float"><label>Author response image 2.</label><caption><title>Split-half correlations for the texture-selective activation maps in a representative subject (S01) in V2.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-sa4-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(3) Unfortunately, Figure S2 only shows the stripe ROIs but not V3ab or V4 ROIs. Including another figure that shows all ROIs in more detail would be interesting.</p></disp-quote><p>Thank you for your suggestion. We have included a figure showing the ROIs for V4 and V3ab (the black dotted lines in Figure S9).</p><disp-quote content-type="editor-comment"><p>(4) It would be helpful for the reader to have a more detailed discussion about methodological limitations, including the unspecificity of the GE-BOLD signal (Engel et al., 1997, Cereb Cortex, 7, 181-192; Parkes et al., 2005, MRM, 54, 1465-1472; Fracasso et al., 2021, Prog Neurobiol, 202, 102187) and the used voxel sizes.</p></disp-quote><p>Thank you for your suggestion. We have added a more detailed discussion about the methodological limitations, including the unspecificity of the GE-BOLD signal and the voxel sizes used.</p><p>In Line 397-408: “Due to the limitations of the T2*w GE-BOLD signal in its sensitivity to large draining veins (Fracasso et al., 2021; Parkes et al., 2005; Uludag &amp; Havlicek, 2021), the original BOLD responses were strongly biased towards the superficial depth in our data (Figure S8). Compared to GE-BOLD, VASO-CBV and SE-BOLD fMRI techniques have higher spatial specificity but much lower sensitivity (Huber et al., 2019). As shown in a recent study (Qian et al., 2024), using differential BOLD responses in a continuous¬¬ stimulus design can significantly enhance the laminar specificity of the feature selectivity measures in our results (Figure 3). Compared to the submillimeter voxels, as used in most laminar fMRI studies, our fMRI resolution at 1-mm isotropic voxel may have a stronger partial volume effect in the cortical depth-dependent analysis. However, consistent with our results, previous studies have also shown that 7T fMRI at 1-mm isotropic resolution can resolve cortical depth-dependent signals in human visual cortex (Roefs et al., 2024; Shao et al., 2021).”</p><disp-quote content-type="editor-comment"><p>(5) If I understand correctly, different numbers of runs/sessions were acquired for different subjects. It would be good to discuss if this could have impacted the results, e.g., different effect sizes could have biased the manual ROI definition.</p></disp-quote><p>Thank you for your suggestion. Although there were differences in the number of runs/sessions acquired for different subjects, there were at least four runs of data for each experiment, which should be enough to examine the within-subject effect. We have discussed this point in the revised manuscript.</p><p>In Line 481-484: “Although the number of runs were not equal across participants, there were at least four runs (twenty blocks for each stimulus condition) of data in each experiment, which should be sufficient to investigate within-subject effects.”</p><disp-quote content-type="editor-comment"><p>(6) It would be good to add the software used for layer definition. Was it Laynii?</p></disp-quote><p>We have provided more details in the revised methods.</p><p>In Line 523-526: “An equi-volume method was used to calculate the relative cortical depth of each voxel to the white matter and pial surface (0: white matter surface, 1: pial surface, Supplementary Figure S11A), using mripy (<ext-link ext-link-type="uri" xlink:href="https://github.com/herrlich10/mripy">https://github.com/herrlich10/mripy</ext-link>).”</p><disp-quote content-type="editor-comment"><p>(7) It would be interesting to see (at least for one subject) the contrasts of color-selective thin stripes and disparity-selective thick stripes from single sessions to demonstrate the repeatability of measurements.</p></disp-quote><p>Thank you for your suggestion. We have shown the test-retest reliability of the response pattern of color-selective thin stripes and disparity-selective thick stripes in a representative subject in Figure S5.</p><disp-quote content-type="editor-comment"><p>(8) By any chance, do the authors also have resting-state data from the same subjects? It would be interesting to see the connectivity analysis between stripes and V3ab, V4 with resting-state data.</p></disp-quote><p>Thank you for your suggestion. Unfortunately, we do not have resting-state data from the same subjects at this time. We agree with you that layer-specific connectivity analysis with resting-state data is very interesting and worth investigating in future studies.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3:</bold></p><p>(1) For investigating information flow across areas, the authors rely on layer-specific informational connectivity analyses, which is an exciting approach. Covariation in decoding accuracy for a specific dependent variable between the superficial layers of a lower area and the middle layer of a higher area is taken as evidence for feedforward connectivity, whereas FB was defined as the connection between the two deep layers. Yet this method is not assumption-free. For example, the canonical idea (Figure 1C) of FF terminals exclusively arriving in layer 4 and FB terminals exclusively terminating in supra-or infragranular layers is not entirely correct. This is not even the case for area V1 - see for example Kathy Rockland's exquisite tractography studies, showing that even single axons with branches terminating in different layers. Also, feedback signals not only arrive in the deep layers of a lower area. Although these informational connectivity analyses can be suggestive of information flow, this reviewer doubts it can be considered as conclusive evidence. Therefore, the authors should drastically tone down their language in this respect, throughout the text. They present suggestive, not conclusive evidence. To obtain truly conclusive evidence, one likely has to perform laminar electrophysiological recordings simultaneously across multiple areas and infer the directionality of information flow using, for example, granger causality.</p></disp-quote><p>Thank you for pointing out this important issue. In our response to a previous question (Reviewer #1, the 2nd comment), we have discussed other possible connections in addition to the canonical feedforward and feedback pathways. In the revised manuscript, the conclusion has been toned down to properly reflect our findings. However, we would also like to emphasize that our conclusion about laminar circuits was supported by converging lines of evidence. For example, in addition to the depth-dependent connectivity results, the role of feedback circuit in processing texture information was also supported by greater selectivity in V4 than V2, and the strongest deep layer selectivity in V2 (Figure 3C).</p><disp-quote content-type="editor-comment"><p>(2) In the same realm, how reproducible are the information connectivity results? In the first part of the study, the authors performed a split-half analyses. This should be also done for Figure 4.</p></disp-quote><p>Thank you for your suggestion. We have performed a split-half analysis for the informational connectivity results. As shown in Author response image 3, the results for the color experiment were robust and reproducible, while the disparity and texture connectivity results were less consistent between the two halves. The results from the second half (Author response image 3, below) are more consistent with the original findings (Figure 4). Overall, the pattern of results were qualitatively similar between the two halves. The inconsistency may be due to the fact that some participants had only four runs of data, which could make the split-half analysis less reliable.</p><fig id="sa4fig3" position="float"><label>Author response image 3.</label><caption><title>Split-half analysis of informational connectivity.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-93171-sa4-fig3-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(3) Most of the other layer-specific claims (not the ones about the flow of information) are based on indices. It is unclear which ROIs contributed to these indices. Was it the entire extent of V1, V2, ...? Or only the visually-driven voxels within these areas? How exactly were the voxels selected? For V2, it would make sense to calculate the selectivity indices independently for the disparity and color-selective (putative) thick and (putative) thin stripe compartments, respectively. Adding voxels of non-selective compartments (e.g. putative thick stripe voxels for calculating the color-index; or adding putative thin-strip voxels for calculating the disparity index), will only add noise.</p></disp-quote><p>In the revised manuscript, we have clarified that we selected the entire ROI in the depth-dependent analysis. Since our study does not have an independent functional localizer, using the entire ROI avoids the problem of double dipping. The processing of visual features is not confined solely to specific stripes. We have also provided a more comprehensive explanation of this issue in the discussion section.</p><p>In Line 541-544: “For the cortical depth-dependent analyses in Figure 3, we used all voxels in the retinotopic ROI. Pooling all voxels in the ROI avoids the problem of double-dipping and also increases the signal-to-noise ratio of ROI-averaged BOLD responses.”</p><disp-quote content-type="editor-comment"><p>(4) It is apparent from Figure 3, that the indices are largely (though not exclusively) driven by 2 subjects. Therefore, this reviewer wishes to see the raw data in addition to a table for calculating the color, disparity, and texture selectivity indices -along with the number of voxels that contributed to it.</p></disp-quote><p>Thank you for your suggestion. We have provided a figure showing the original BOLD responses (Figure S8 and Figure S8). Data from individual subjects were also available at Open Science Framework (OSF, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/KSXT8">https://doi.org/10.17605/OSF.IO/KSXT8</ext-link> (‘rawBetaValues.mat’ in the data directory)).</p><disp-quote content-type="editor-comment"><p>Minor:</p><p>(1) I typically find inferences about 'layer fMRI' vastly overstated. We all know that fMRI does not (yet) provide laminar-specific resolution, i.e., whereby meaningful differences in fMRI signals can be extracted from all 6 individual layers of neocortex, without partial volume effects, or without taking into account pre-and postsynaptic contributions of neurons to the fMRI signal (the cell bodies may very well lay in different layers than the dendritic trees etc.), or without taking into account the vascular anatomy, etc. The authors should use the term cortical depth-dependent fMRI throughout the text -as they do in the abstract and intro.</p></disp-quote><p>Thank you for pointing out this important issue. We have now defined the meaning of layer or laminar as “cortical depth-dependent” in the introduction, to be consistent with the terminology in most published papers on this topic.</p><disp-quote content-type="editor-comment"><p>(2) 1st sentence abstract: I disagree with this statement. The parallel streams in intermediate-level areas are probably equally well studied as the geniculostriate pathway -already starting with the seminal work of Hubel, Livingstone, and more recently by Angelucci and co-workers who looked in detail at the anatomical and functional interactions across sub-compartments of V1 and V2.</p></disp-quote><p>Thank you for your feedback. In the revised manuscript, we have removed the term &quot;much&quot; from the first sentence of the abstract. Although there have been seminal studies of V2 sub-compartments in monkeys, only a few fMRI studies investigated this issue in humans.</p><disp-quote content-type="editor-comment"><p>(3) The authors show inter-session correlations for color and disparity. This reviewer would like to see test-retest images since the explained variance is not terribly good. Also, show the correlation values for the inter-session texture beta values.</p></disp-quote><p>Thank you for your suggestion. We have performed the test-retest reliability analysis of texture-selective patterns in the response to a previous question (Reviewer #2, the 2nd comment, Author response image 2).</p><disp-quote content-type="editor-comment"><p>(4) The stripe definitions are threshold dependent. Please clarify whether the reported results are threshold-independent.</p></disp-quote><p>Thank you for your question. To address your concern, we defined the stripe ROIs using different thresholds, and the results remained consistent. Specifically, we ranked the voxels in manually defined stripe ROIs by the color-disparity response. We then defined the lowest 10% as the thick stripe voxels, the highest 10% as thin stripe voxels, and the middle 10% as pale stripe voxels. Additionally, we adjusted the thresholds to 20% and 30% to define the three stripes (with 30% being the least strict threshold). Feature selectivities at different thresholds were shown in Figure S6 (from left to right: 10%, 20%, 30%). Notably, in all threshold conditions, there was no significant difference in texture selectivity across different stripes.</p><disp-quote content-type="editor-comment"><p>(5) How were the visual areas defined?</p></disp-quote><p>In the revised manuscript, we have provided a detailed description about methods.</p><p>In Line 531-535: “ROIs were defined on the inflated cortical surface. Surface ROIs for V1, V2, V3ab, and V4 were defined based on the polar angle atlas from the 7T retinotopic dataset of Human Connectome Project (Benson et al., 2014, 2018). Moreover, the boundary of V2 was edited manually based on columnar patterns. All ROIs were constrained to regions where mean activation across all stimulus conditions exceeded 0.”</p><disp-quote content-type="editor-comment"><p>(6) &quot;According to the hierarchical model in Figure 1B and 1C, the strongest color selectivity in the superficial cortical depth is consistent with the fact that color blobs mainly locate in the superficial layers of V1, suggesting that both local and feedforward connections are involved in processing color information in area V2.&quot; But color-selective activation within V2 could be also consistent with feedback from other areas (some of which were not covered in the present experiments) -the more since most parts of the brain were not covered (i.e. a slab of 4 cm was covered)?</p></disp-quote><p>Thank you for reminding us about this issue. We have discussed the possibility of feedback influence in explanation of the superficial bias of color selectivity in area V2.</p></body></sub-article></article>